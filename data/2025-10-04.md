<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 3]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Odontoceti: Ultra-Fast DAG Consensus with Two Round Commitment](https://arxiv.org/abs/2510.01216)
*Preston Vander Vos*

Main category: cs.DC

TL;DR: Odontoceti, a DAG-based consensus protocol, improves blockchain scalability with low latency (300ms) and high throughput (10,000 TPS) by trading off fault tolerance (20% instead of 33%). It uses two communication rounds and demonstrates notable performance improvements.


<details>
  <summary>Details</summary>
Motivation: Blockchain users demand faster transaction processing and reduced latency, leading to scalability concerns, which the study addresses through a new DAG-based protocol.

Method: The protocol uses a DAG structure with n = 5f + 1 validators, operating under 20% fault tolerance. It commits blocks in two communication rounds, includes crash-fault optimizations, and measures performance under realistic conditions.

Result: The protocol achieves a median latency of 300 milliseconds and processes 10,000 transactions per second, with a 20-25% latency improvement over existing protocols.

Conclusion: Odontoceti demonstrates the practicality of lower fault tolerance consensus protocols for enhancing blockchain scalability and performance.

Abstract: Users of blockchains value scalability, expecting fast confirmations and
immediate transaction processing. Odontoceti, the latest in DAG-based
consensus, addresses these concerns by prioritizing low latency and high
throughput, making a strategic trade-off in security by operating with a 20%
fault tolerance instead of the established 33% level. It is the first DAG-based
protocol to achieve commitment in just two communication rounds, delivering
median latency of 300 milliseconds while processing 10,000 transactions per
second under realistic network conditions. Odontoceti operates with n = 5f + 1
validators and creates an uncertified DAG with a novel decision rule for
committing blocks. The protocol includes an optimization that advances progress
when participants are slow, benefiting crash fault scenarios which are more
common in practice than Byzantine faults. Evaluation results demonstrate 20-25%
latency improvements compared to an existing production protocol, validating
that reducing wave length from three rounds to two rounds yields meaningful
performance benefits. This paper establishes the practical viability of lower
fault tolerance consensus protocols for blockchains.

</details>


### [2] [Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters](https://arxiv.org/abs/2510.01256)
*Lingling Zeng,Gen Zhang,Jialin Peng,Xiang Xu,Yuan Xu,Lijun Ma*

Main category: cs.DC

TL;DR: This paper introduces Kant, a unified scheduling platform for AI container clusters, which improves resource utilization, efficiency, and reduces fragmentation in training and inference jobs.


<details>
  <summary>Details</summary>
Motivation: To address traditional scheduling systems' inefficiency in large-scale AI workloads.

Method: Developed and implemented the Kant system, employing Backfill and Enhanced Binpack (E-Binpack) strategies to optimize scheduling.

Result: Kant achieved high performance in large AI clusters, improving GPU utilization and reducing resource fragmentation.

Conclusion: The system enables high-performance scheduling for AI workloads and has been successfully deployed in multiple AI data centers.

Abstract: As AI cluster sizes continue to expand and the demand for
large-language-model (LLM) training and inference workloads grows rapidly,
traditional scheduling systems face significant challenges in balancing
resource utilization, scheduling efficiency, and service quality. This paper
presents and evaluates Kant: an efficient unified scheduling platform designed
for large-scale AI container clusters, supporting the co-scheduling of both
training and inference jobs. Based on the practical implementation of the Kant
system, we systematically define a set of key evaluation metrics for AI
clusters, including GPU Allocation Ratio (GAR), Scheduling Occupancy Rate
(SOR), GPU Node Fragmentation Ratio (GFR), Job Waiting Time Distribution
(JWTD), and Job Training Time Estimation Distribution (JTTED), providing a
foundation for quantitative performance analysis. Experimental results
demonstrate that Kant achieves exceptional performance in clusters ranging from
hundreds to tens of thousands of GPUs. By leveraging scheduling strategies such
as Backfill and Enhanced Binpack (E-Binpack), the system significantly improves
resource utilization and scheduling efficiency, while effectively reducing
resource fragmentation and communication overhead in distributed training. The
system has been deployed in multiple AI data center clusters, where it stably
supports large-scale intelligent computing workloads. This work provides a
practical engineering approach for building high-performance, highly available,
AI-native scheduling infrastructure.

</details>


### [3] [IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol](https://arxiv.org/abs/2510.01260)
*Ningyuan Yang,Guanliang Lyu,Mingchen Ma,Yiyi Lu,Yiming Li,Zhihui Gao,Hancheng Ye,Jianyi Zhang,Tingjun Chen,Yiran Chen*

Main category: cs.DC

TL;DR: The paper introduces IoT-MCP, a framework integrating Large Language Models (LLMs) with Internet-of-Things (IoT) via the Model Context Protocol (MCP), achieving 100% task success rate with low resource usage.


<details>
  <summary>Details</summary>
Motivation: Address challenges in hardware heterogeneity and control complexity for LLM-IoT integration.

Method: Developed IoT-MCP framework using edge-deployed servers and benchmarked it with IoT-MCP Bench, encompassing Basic and Complex Tasks.

Result: The framework achieved 100% task success, 205ms average response time, and 74KB peak memory use across diverse IoT devices.

Conclusion: IoT-MCP provides a robust solution for LLM-IoT connectivity and establishes a standardized methodology alongside an open-source integration framework.

Abstract: The integration of Large Language Models (LLMs) with Internet-of-Things (IoT)
systems faces significant challenges in hardware heterogeneity and control
complexity. The Model Context Protocol (MCP) emerges as a critical enabler,
providing standardized communication between LLMs and physical devices. We
propose IoT-MCP, a novel framework that implements MCP through edge-deployed
servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we
introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g.,
``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel
so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation
across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100%
task success rate to generate tool calls that fully meet expectations and
obtain completely accurate results, 205ms average response time, and 74KB peak
memory footprint. This work delivers both an open-source integration framework
(https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized
evaluation methodology for LLM-IoT systems.

</details>
