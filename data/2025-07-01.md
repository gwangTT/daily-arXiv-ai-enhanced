<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 45]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.CL](#cs.CL) [Total: 82]
- [cs.CV](#cs.CV) [Total: 222]
- [cs.DC](#cs.DC) [Total: 9]
- [cs.LG](#cs.LG) [Total: 112]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.RO](#cs.RO) [Total: 42]
- [cs.SE](#cs.SE) [Total: 23]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [stat.ML](#stat.ML) [Total: 8]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [cs.CR](#cs.CR) [Total: 23]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [nlin.CD](#nlin.CD) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [cs.CY](#cs.CY) [Total: 9]
- [stat.AP](#stat.AP) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.IV](#eess.IV) [Total: 26]
- [cs.NI](#cs.NI) [Total: 9]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [math.ST](#math.ST) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [math.OC](#math.OC) [Total: 7]
- [math.NA](#math.NA) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [stat.ME](#stat.ME) [Total: 1]
- [stat.OT](#stat.OT) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [eess.SP](#eess.SP) [Total: 18]
- [econ.GN](#econ.GN) [Total: 1]
- [astro-ph.GA](#astro-ph.GA) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.SD](#cs.SD) [Total: 9]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.HC](#cs.HC) [Total: 9]
- [econ.EM](#econ.EM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.GR](#cs.GR) [Total: 5]
- [eess.SY](#eess.SY) [Total: 3]
- [cs.IR](#cs.IR) [Total: 7]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bootstrapping Human-Like Planning via LLMs](https://arxiv.org/abs/2506.22604)
*David Porfirio,Vincent Hsiao,Morgan Fine-Morris,Leslie Smith,Laura M. Hiatt*

Main category: cs.AI

TL;DR: This paper explores combining natural language programming with drag-and-drop interfaces using large language models (LLMs) to generate human-like robot task sequences.


<details>
  <summary>Details</summary>
Motivation: Robot end users need accessible ways to specify robot tasks. Combining intuitive natural language programming with precise drag-and-drop interfaces could enhance task specification.

Method: The authors developed an LLM-based pipeline that takes natural language input and generates action sequences with human-like granularity, then compared these sequences to hand-specified ones.

Result: Larger LLMs performed better at generating human-like action sequences, but smaller models also provided satisfactory results.

Conclusion: Integrating natural language processing and drag-and-drop interfaces using LLMs is promising, with larger models yielding superior outcomes.

Abstract: Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.

</details>


### [2] [Ludax: A GPU-Accelerated Domain Specific Language for Board Games](https://arxiv.org/abs/2506.22609)
*Graham Todd,Alexander G. Padula,Dennis J. N. J. Soemers,Julian Togelius*

Main category: cs.AI

TL;DR: Ludax is a new domain-specific language for board games that automates compilation into hardware-accelerated code, enabling faster and more efficient AI research.


<details>
  <summary>Details</summary>
Motivation: Support AI research with a language that integrates game description flexibility and hardware acceleration for improved generality and speed.

Method: Developed Ludax, a game description language designed to combine hardware acceleration with compatibility in deep learning pipelines.

Result: Presented Ludax's technical features, showcased its speed benchmarking outcomes, and demonstrated its effectiveness in training RL agents.

Conclusion: Ludax accelerates game research by integrating rapid simulation and flexible representations, and is freely offered as an open-source tool.

Abstract: Games have long been used as benchmarks and testing environments for research
in artificial intelligence. A key step in supporting this research was the
development of game description languages: frameworks that compile
domain-specific code into playable and simulatable game environments, allowing
researchers to generalize their algorithms and approaches across multiple games
without having to manually implement each one. More recently, progress in
reinforcement learning (RL) has been largely driven by advances in hardware
acceleration. Libraries like JAX allow practitioners to take full advantage of
cutting-edge computing hardware, often speeding up training and testing by
orders of magnitude. Here, we present a synthesis of these strands of research:
a domain-specific language for board games which automatically compiles into
hardware-accelerated code. Our framework, Ludax, combines the generality of
game description languages with the speed of modern parallel processing
hardware and is designed to fit neatly into existing deep learning pipelines.
We envision Ludax as a tool to help accelerate games research generally, from
RL to cognitive science, by enabling rapid simulation and providing a flexible
representation scheme. We present a detailed breakdown of Ludax's description
language and technical notes on the compilation process, along with speed
benchmarking and a demonstration of training RL agents. The Ludax framework,
along with implementations of existing board games, is open-source and freely
available.

</details>


### [3] [URSA: The Universal Research and Scientific Agent](https://arxiv.org/abs/2506.22653)
*Michael Grosskopf,Russell Bent,Rahul Somasundaram,Isaac Michaud,Arthur Lui,Nathan Debardeleben,Earl Lawrence*

Main category: cs.AI

TL;DR: The paper introduces URSA, an ecosystem of AI agents designed to assist in accelerating scientific research tasks.


<details>
  <summary>Details</summary>
Motivation: Modern scientific research often faces bottlenecks that slow progress, and the increasing capabilities of LLMs present an opportunity to overcome these barriers and revolutionize scientific workflows.

Method: The authors develop URSA, a modular scientific agent ecosystem that integrates advanced tools and physics simulation codes to address scientific challenges of varying complexity.

Result: They showcase the architecture of URSA and provide examples demonstrating its potential as a transformative tool in science.

Conclusion: URSA exemplifies the capability of LLM-based tools to assist and accelerate scientific research, promising to remove traditional bottlenecks and enhance productivity in modern science.

Abstract: Large language models (LLMs) have moved far beyond their initial form as
simple chatbots, now carrying out complex reasoning, planning, writing, coding,
and research tasks. These skills overlap significantly with those that human
scientists use day-to-day to solve complex problems that drive the cutting edge
of research. Using LLMs in "agentic" AI has the potential to revolutionize
modern science and remove bottlenecks to progress. In this work, we present
URSA, a scientific agent ecosystem for accelerating research tasks. URSA
consists of a set of modular agents and tools, including coupling to advanced
physics simulation codes, that can be combined to address scientific problems
of varied complexity and impact. This work highlights the architecture of URSA,
as well as examples that highlight the potential of the system.

</details>


### [4] [Explanations are a means to an end](https://arxiv.org/abs/2506.22740)
*Jessica Hullman,Ziyang Guo,Berk Ustun*

Main category: cs.AI

TL;DR: The paper highlights that explanations in machine learning should be designed with specific applications in mind, using a framework based on statistical decision theory.


<details>
  <summary>Details</summary>
Motivation: Explanations in machine learning often neglect consideration of practical use cases, potentially leading to suboptimal application and ambiguity.

Method: The authors propose a framework rooted in statistical decision theory to align explanations with concrete use cases like clinical decision-making and debugging.

Result: The framework offers insights into how explanations can enhance task performance and prevent misuse by mandating specific use-case analysis.

Conclusion: Evaluations of explanations should integrate both theoretical and empirical perspectives, encouraging a focus on their utility in clearly-defined applications.

Abstract: Modern methods for explainable machine learning are designed to describe how
models map inputs to outputs--without deep consideration of how these
explanations will be used in practice. This paper argues that explanations
should be designed and evaluated with a specific end in mind. We describe how
to formalize this end in a framework based in statistical decision theory. We
show how this functionally-grounded approach can be applied across diverse use
cases, such as clinical decision support, providing recourse, or debugging. We
demonstrate its use to characterize the maximum "boost" in performance on a
particular task that an explanation could provide an idealized decision-maker,
preventing misuse due to ambiguity by forcing researchers to specify concrete
use cases that can be analyzed in light of models of expected explanation use.
We argue that evaluation should meld theoretical and empirical perspectives on
the value of explanation, and contribute definitions that span these
perspectives.

</details>


### [5] [Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems](https://arxiv.org/abs/2506.22774)
*Michael Papademas,Xenia Ziouvelou,Antonis Troumpoukis,Vangelis Karkaletsis*

Main category: cs.AI

TL;DR: The paper proposes an assessment framework for Trustworthy AI by integrating ethical aspects with algorithmic processes like PageRank and TrustRank.


<details>
  <summary>Details</summary>
Motivation: The increasing integration and influence of AI in society require methods to quantify AI trustworthiness while addressing the shortcomings of existing theoretical guidelines and technical tools.

Method: The proposed method combines ethical guidelines with the algorithmic frameworks of PageRank and TrustRank to create a quantitative assessment of AI trustworthiness.

Result: The approach offers a way to holistically assess AI systems, providing quantitative metrics while respecting theoretical guidelines.

Conclusion: The framework successfully minimizes subjectivity in Trustworthy AI assessments, bridging the gap between theoretical and algorithmic evaluations.

Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges
posed by human-made artifacts, particularly those widely integrated into
society and exert significant influence, highlighting potential benefits and
their negative consequences. While other technologies may also pose substantial
risks, AI's pervasive reach makes its societal effects especially profound. The
complexity of AI systems, coupled with their remarkable capabilities, can lead
to a reliance on technologies that operate beyond direct human oversight or
understanding. To mitigate the risks that arise, several theoretical tools and
guidelines have been developed, alongside efforts to create technological tools
aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view
of the issue but fail to provide techniques for quantifying trustworthiness.
Conversely, while technological tools are better at achieving such
quantification, they lack a holistic perspective, focusing instead on specific
aspects of Trustworthy AI. This paper aims to introduce an assessment method
that combines the ethical components of Trustworthy AI with the algorithmic
processes of PageRank and TrustRank. The goal is to establish an assessment
framework that minimizes the subjectivity inherent in the self-assessment
techniques prevalent in the field by introducing algorithmic criteria. The
application of our approach indicates that a holistic assessment of an AI
system's trustworthiness can be achieved by providing quantitative insights
while considering the theoretical content of relevant guidelines.

</details>


### [6] [ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models](https://arxiv.org/abs/2506.22865)
*Ziqi Zhong,Xunzhu Tang*

Main category: cs.AI

TL;DR: The paper presents ReasonBridge, a framework for transferring reasoning abilities from closed-source Large Language Models (LLMs) to open-source models using hierarchical knowledge distillation and minimal computational resources.


<details>
  <summary>Details</summary>
Motivation: To address the performance gap in complex reasoning tasks between closed-source and open-source LLMs and improve open-source models' capabilities in instruction following.

Method: The methodology involves hierarchical knowledge distillation using a tailored reasoning-focused dataset (Reason1K), sparse reasoning adapters, and guided inference interventions for compute scaling.

Result: ReasonBridge led to a 23% improvement in reasoning capabilities for open-source models, with enhanced models outperforming or matching closed-source counterparts on specific benchmarks.

Conclusion: ReasonBridge provides a sample-efficient and generalizable solution to better equip open-source models with reasoning capabilities, narrowing the divide between closed-source and open-source model performance.

Abstract: Recent advancements in Large Language Models (LLMs) have revealed a
significant performance gap between closed-source and open-source models,
particularly in tasks requiring complex reasoning and precise instruction
following. This paper introduces ReasonBridge, a methodology that efficiently
transfers reasoning capabilities from powerful closed-source to open-source
models through a novel hierarchical knowledge distillation framework. We
develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning
traces emphasizing difficulty, diversity, and quality. These traces are
filtered from across multiple domains using a structured multi-criteria
selection algorithm. Our transfer learning approach incorporates: (1) a
hierarchical distillation process capturing both strategic abstraction and
tactical implementation patterns, (2) a sparse reasoning-focused adapter
architecture requiring only 0.3% additional trainable parameters, and (3) a
test-time compute scaling mechanism using guided inference interventions.
Comprehensive evaluations demonstrate that ReasonBridge improves reasoning
capabilities in open-source models by up to 23% on benchmark tasks,
significantly narrowing the gap with closed-source models. Notably, the
enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its
performance on competition-level AIME problems. Our methodology generalizes
effectively across diverse reasoning domains and model architectures,
establishing a sample-efficient approach to reasoning enhancement for
instruction following.

</details>


### [7] [Agentic Enterprise: AI-Centric User to User-Centric AI](https://arxiv.org/abs/2506.22893)
*Arpit Narechania,Alex Endert,Atanu R Sinha*

Main category: cs.AI

TL;DR: The paper explores AI's transformative potential for enterprises by proposing a shift towards user-centric AI and introducing six tenets for effective decision-making.


<details>
  <summary>Details</summary>
Motivation: AI can revolutionize enterprise decision-making, yet current paradigms overlook real enterprise needs.

Method: The paper identifies gaps in AI-Centric User paradigms and proposes six tenets for a user-oriented approach.

Result: Six tenets are introduced to improve decision productivity and align AI for enterprise applications.

Conclusion: User-Centric AI, guided by proposed six tenets, is essential to maximize AI impact in enterprise decision-making processes.

Abstract: After a very long winter, the Artificial Intelligence (AI) spring is here.
Or, so it seems over the last three years. AI has the potential to impact many
areas of human life - personal, social, health, education, professional. In
this paper, we take a closer look at the potential of AI for Enterprises, where
decision-making plays a crucial and repeated role across functions, tasks, and
operations. We consider Agents imbued with AI as means to increase
decision-productivity of enterprises. We highlight six tenets for Agentic
success in enterprises, by drawing attention to what the current, AI-Centric
User paradigm misses, in the face of persistent needs of and usefulness for
Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we
offer six tenets and promote market mechanisms for platforms, aligning the
design of AI and its delivery by Agents to the cause of enterprise users.

</details>


### [8] [Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning](https://arxiv.org/abs/2506.22919)
*Sanskar Pandey,Ruhaan Chopra,Saad Murtaza Bhat,Ark Abhyudaya*

Main category: cs.AI

TL;DR: Hecto is a new lightweight architecture using diverse experts (GRU & FFNN) under Top-1 gating for reasoning tasks, showcasing specialization under low-resource settings.


<details>
  <summary>Details</summary>
Motivation: To improve representational diversity and computational efficiency in MoE models for tasks demanding distinct reasoning types.

Method: Hecto employs architectural heterogeneity with GRU for temporal reasoning and FFNN for static abstraction, routed via sparse Top-1 gating mechanism, tested across various benchmarks.

Result: Hecto delivers matched or close performance to baseline models while achieving distinct expert specialization (temporal vs static) and stronger performance at larger batch sizes.

Conclusion: Hecto sets a new benchmark by demonstrating principled specialization and interpretability in conditional computation under low-resource environments.

Abstract: Mixture-of-Experts (MoE) models enable conditional computation by routing
inputs to specialized experts, but these experts rely on identical inductive
biases, thus limiting representational diversity. This static computation
pathway is inefficient for inputs that require different types of reasoning and
limits specialization and interpretability. We propose Hecto, a lightweight MoE
architecture that leverages architectural heterogeneity by combining a GRU
expert for temporal reasoning and an FFNN expert for static abstraction under a
sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG
News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely
trails homogeneous baselines in performance despite receiving isolated input
representations, while achieving clear expert specialization, with each expert
aligning to distinct reasoning types (temporal vs static). At larger batch
sizes, Hecto exhibits improved performance, benefiting from relaxed
computational constraints that allow its heterogeneous architecture to optimize
more effectively. Ablation results isolate architectural diversity as the
source of Hecto's stability and interpretability across diverse reasoning
tasks. Overall, Hecto establishes itself as a new benchmark for conditional
computation, offering a principled framework for specialized reasoning in
low-resource regimes with its model strength derived from principled
specialization.

</details>


### [9] [Improving Rationality in the Reasoning Process of Language Models through Self-playing Game](https://arxiv.org/abs/2506.22920)
*Pinzheng Wang,Juntao Li,Zecheng Tang,Haijia Gui,Min zhang*

Main category: cs.AI

TL;DR: This paper proposes a Critic-Discernment Game (CDG) to improve reasoning abilities of large language models using self-play mechanism rather than human supervision.


<details>
  <summary>Details</summary>
Motivation: Large language models exhibit strong reasoning but lack true comprehension of their own processes, motivating a need for improvement in self-awareness and rationality without relying on external supervision.

Method: The paper introduces the Critic-Discernment Game (CDG), in which a solution provided by a model is challenged by critiques that either help or mislead. The model's objective is to maintain correct answers against misleading inputs and address constructive criticism.

Result: Experiments on mathematical reasoning, error detection, self-correction, and long-chain reasoning show significant improvements in the reasoning abilities of LLMs trained with CDG.

Conclusion: CDG-based self-play effectively enhances the comprehensiveness and rationality of large language models, paving the way for better self-aware AI systems.

Abstract: Large language models (LLMs) have demonstrated considerable reasoning
abilities in various tasks such as mathematics and coding. However, recent
studies indicate that even the best models lack true comprehension of their
reasoning processes. In this paper, we explore how self-play can enhance the
rationality of models in the reasoning process without supervision from humans
or superior models. We design a Critic-Discernment Game(CDG) in which a prover
first provides a solution to a given problem and is subsequently challenged by
critiques of its solution. These critiques either aim to assist or mislead the
prover. The objective of the prover is to maintain the correct answer when
faced with misleading comments, while correcting errors in response to
constructive feedback. Our experiments on tasks involving mathematical
reasoning, stepwise error detection, self-correction, and long-chain reasoning
demonstrate that CDG training can significantly improve the ability of
well-aligned LLMs to comprehend their reasoning process.

</details>


### [10] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang,Yekun Chai,Maria Brbić,Michael Moor*

Main category: cs.AI

TL;DR: Existing multimodal language models (MLLMs) struggle with complex reasoning tasks requiring step-by-step understanding across visual, spatial, and physical dimensions, as evidenced by poor performance on MARBLE's benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of benchmarks for evaluating complex reasoning in multimodal domains and to advance the ability of artificial intelligence to process and reason step-by-step across multiple modalities.

Method: The authors introduced MARBLE, a multimodal reasoning benchmark consisting of two tasks, M-Portal and M-Cube, which assess multistep planning under spatial, visual, and physical constraints. They evaluated 12 advanced MLLMs on these tasks.

Result: All tested models achieved near-random performance on M-Portal and 0% accuracy on M-Cube. Some could outperform random baselines only in simplified subtasks, highlighting failures in perception and complex reasoning.

Conclusion: Current MLLMs face significant limitations in complex multimodal reasoning, particularly in perception tasks. MARBLE aims to drive research toward more advanced models capable of tackling such challenges effectively.

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [11] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
*Leander Melroy Maben,Gayathri Ganesh Lakshmy,Srijith Radhakrishnan,Siddhant Arora,Shinji Watanabe*

Main category: cs.AI

TL;DR: AURA is an open-source, speech-native assistant for goal-driven tasks with dynamic tool use and multi-turn dialogue.


<details>
  <summary>Details</summary>
Motivation: Existing systems lack a platform for full speech-to-speech dialogue with integrated tool use.

Method: AURA combines ASR, TTS, and LLMs in a modular pipeline, supporting multi-tool integration and natural language prompts.

Result: On VoiceBench, AURA scores 92.75% and achieves 90% task success in human evaluations on complex speech tasks.

Conclusion: AURA advances multi-turn, speech-to-speech goal-driven assistant capabilities, nearing state-of-the-art performance.

Abstract: Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [12] [AI's Euclid's Elements Moment: From Language Models to Computable Thought](https://arxiv.org/abs/2506.23080)
*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Main category: cs.AI

TL;DR: This paper introduces a five-stage evolutionary framework for AI development, likening its progression to human cognitive history. It highlights the reflexive evolution of AI and provides a theoretical roadmap for future advancements.


<details>
  <summary>Details</summary>
Motivation: To create a systematic framework explaining AI's architectural evolution based on analogies to human cognitive history, enabling better understanding and guiding future development.

Method: The authors developed the 'Geometry of Cognition' framework, analyzing AI's past shifts and projecting a prescriptive path forward through cross-disciplinary modeling.

Result: The paper identifies distinct epochs in AI evolution, such as the current 'Metalinguistic Moment,' and predicts future stages involving advances in neuro-symbolic architectures and program synthesis.

Conclusion: The work concludes that AI's evolution is reflexive and systematic, offering insights and actionable strategies for developers to create aligned and reliable AI systems in the future.

Abstract: This paper presents a comprehensive five-stage evolutionary framework for
understanding the development of artificial intelligence, arguing that its
trajectory mirrors the historical progression of human cognitive technologies.
We posit that AI is advancing through distinct epochs, each defined by a
revolutionary shift in its capacity for representation and reasoning, analogous
to the inventions of cuneiform, the alphabet, grammar and logic, mathematical
calculus, and formal logical systems. This "Geometry of Cognition" framework
moves beyond mere metaphor to provide a systematic, cross-disciplinary model
that not only explains AI's past architectural shifts-from expert systems to
Transformers-but also charts a concrete and prescriptive path forward.
Crucially, we demonstrate that this evolution is not merely linear but
reflexive: as AI advances through these stages, the tools and insights it
develops create a feedback loop that fundamentally reshapes its own underlying
architecture. We are currently transitioning into a "Metalinguistic Moment,"
characterized by the emergence of self-reflective capabilities like
Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the
"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be
defined by the development of a computable calculus of thought, likely through
neuro-symbolic architectures and program synthesis, culminating in provably
aligned and reliable AI that reconstructs its own foundational representations.
This work serves as the methodological capstone to our trilogy, which
previously explored the economic drivers ("why") and cognitive nature ("what")
of AI. Here, we address the "how," providing a theoretical foundation for
future research and offering concrete, actionable strategies for startups and
developers aiming to build the next generation of intelligent systems.

</details>


### [13] [Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study](https://arxiv.org/abs/2506.23107)
*Bing Song,Jianing Liu,Sisi Jian,Chenyang Wu,Vinayak Dixit*

Main category: cs.AI

TL;DR: This study evaluates LLMs' ability to mimic human risky decision-making using lottery tasks and finds discrepancies in risk preferences across languages and cultural settings.


<details>
  <summary>Details</summary>
Motivation: Concerns about the reliability of LLMs in complex decision-making, especially risky scenarios, motivated analyzing their alignment with human responses.

Method: The study compared decisions predicted by ChatGPT models to human responses in lottery-based tasks using transportation survey data from diverse demographics and a CRRA risk framework.

Result: Both models showed more risk-averse decisions than humans, with o1-mini aligning closer to human behavior. Language differences affected simulation, particularly in Chinese versus English prompts.

Conclusion: LLMs show potential but have limitations in replicating human risk behavior, influenced by prompt language and cultural diversity.

Abstract: Large language models (LLMs) have made significant strides, extending their
applications to dialogue systems, automated content creation, and
domain-specific advisory tasks. However, as their use grows, concerns have
emerged regarding their reliability in simulating complex decision-making
behavior, such as risky decision-making, where a single choice can lead to
multiple outcomes. This study investigates the ability of LLMs to simulate
risky decision-making scenarios. We compare model-generated decisions with
actual human responses in a series of lottery-based tasks, using transportation
stated preference survey data from participants in Sydney, Dhaka, Hong Kong,
and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and
ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk
preferences were analyzed using the Constant Relative Risk Aversion (CRRA)
framework. Results show that both models exhibit more risk-averse behavior than
human participants, with o1-mini aligning more closely with observed human
decisions. Further analysis of multilingual data from Nanjing and Hong Kong
indicates that model predictions in Chinese deviate more from actual responses
compared to English, suggesting that prompt language may influence simulation
performance. These findings highlight both the promise and the current
limitations of LLMs in replicating human-like risk behavior, particularly in
linguistic and cultural settings.

</details>


### [14] [The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy](https://arxiv.org/abs/2506.23123)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: The paper explores how foundation models in AI impact society and proposes a framework for better governance of these technologies.


<details>
  <summary>Details</summary>
Motivation: To address the dual-edged nature of foundation models, which hold promise for technological advancement but also pose risks and societal challenges.

Method: The paper is organized into three themes: conceptual framing of foundation models, empirical investigations to enhance understanding, and translating insights into actionable AI policies.

Result: It provides clearer insights into the risks and societal impact of foundation models, as well as tools like evaluations and organizational indexes for transparency.

Conclusion: The research paves the way for evidence-based AI policies and better governance of foundation models, fostering improved societal outcomes in the age of AI.

Abstract: Artificial intelligence is humanity's most promising technology because of
the remarkable capabilities offered by foundation models. Yet, the same
technology brings confusion and consternation: foundation models are poorly
understood and they may precipitate a wide array of harms. This dissertation
explains how technology and society coevolve in the age of AI, organized around
three themes. First, the conceptual framing: the capabilities, risks, and the
supply chain that grounds foundation models in the broader economy. Second, the
empirical insights that enrich the conceptual foundations: transparency created
via evaluations at the model level and indexes at the organization level.
Finally, the transition from understanding to action: superior understanding of
the societal impact of foundation models advances evidence-based AI policy.
View together, this dissertation makes inroads into achieving better societal
outcomes in the age of AI by building the scientific foundations and
research-policy interface required for better AI governance.

</details>


### [15] [Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons](https://arxiv.org/abs/2506.23128)
*Chi Chiu So,Yueyue Sun,Jun-Min Wang,Siu Pang Yung,Anthony Wai Keung Loh,Chun Pong Chau*

Main category: cs.AI

TL;DR: The paper evaluates the reasoning abilities of three LLMs, with DeepSeek-R1 outperforming others but showing limitations in complex tasks.


<details>
  <summary>Details</summary>
Motivation: To assess and compare the deep relational reasoning capabilities of leading LLMs, specifically in family tree and graph reasoning tasks.

Method: The authors designed benchmark tasks for evaluating logical deduction and relational inference across various problem complexities, focusing on three advanced LLMs.

Result: DeepSeek-R1 achieved the best performance but struggled with complex problems due to token length limits and incomplete reasoning structures.

Conclusion: While showing progress, the findings highlight significant reasoning limitations in LLMs and call for further exploration into multimodal reasoning and failure analysis.

Abstract: How far are Large Language Models (LLMs) in performing deep relational
reasoning? In this paper, we evaluate and compare the reasoning capabilities of
three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a
suite of carefully designed benchmark tasks in family tree and general graph
reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the
highest F1-scores across multiple tasks and problem sizes, demonstrating strong
aptitude in logical deduction and relational inference. However, all evaluated
models, including DeepSeek-R1, struggle significantly as problem complexity
increases, largely due to token length limitations and incomplete output
structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought
responses uncovers its unique planning and verification strategies, but also
highlights instances of incoherent or incomplete reasoning, calling attention
to the need for deeper scrutiny into LLMs' internal inference dynamics. We
further discuss key directions for future work, including the role of
multimodal reasoning and the systematic examination of reasoning failures. Our
findings provide both empirical insights and theoretical implications for
advancing LLMs' reasoning abilities, particularly in tasks that demand
structured, multi-step logical inference. Our code repository will be publicly
available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.

</details>


### [16] [Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing](https://arxiv.org/abs/2506.23141)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.AI

TL;DR: The paper proposes a semantic-aware relational message passing strategy for Knowledge Graph Completion, using Top-K neighbor selection and a multi-head attention aggregator to address issues of noise and over-smoothing.


<details>
  <summary>Details</summary>
Motivation: Traditional node-based message passing in knowledge graphs can introduce noise and dilute important information, negatively impacting link prediction accuracy.

Method: The semantic-aware relational message passing selects Top-K relevant edges based on semantic relevance and combines them with the central node representation using a multi-head attention aggregator.

Result: Experiments show that the proposed method outperforms existing techniques in several benchmark tests.

Conclusion: The approach effectively captures and propagates relevant contextual information while mitigating irrelevant noise, enhancing performance in knowledge graph tasks.

Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge
Graph Completion (KGC), providing vital cues for prediction. However,
traditional node-based message passing mechanisms, when applied to knowledge
graphs, often introduce noise and suffer from information dilution or
over-smoothing by indiscriminately aggregating information from all neighboring
edges. To address this challenge, we propose a semantic-aware relational
message passing. A core innovation of this framework is the introduction of a
\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this
strategy first evaluates the semantic relevance between a central node and its
incident edges within a shared latent space, selecting only the Top-K most
pertinent ones. Subsequently, information from these selected edges is
effectively fused with the central node's own representation using a
\textbf{multi-head attention aggregator} to generate a semantically focused
node message. In this manner, our model not only leverages the structure and
features of edges within the knowledge graph but also more accurately captures
and propagates the contextual information most relevant to the specific link
prediction task, thereby effectively mitigating interference from irrelevant
information. Extensive experiments demonstrate that our method achieves
superior performance compared to existing approaches on several established
benchmarks.

</details>


### [17] [Rises for Measuring Local Distributivity in Lattices](https://arxiv.org/abs/2506.23168)
*Mohammad Abdulla,Tobias Hille,Dominik Dürrschnabel,Gerd Stumme*

Main category: cs.AI

TL;DR: The paper introduces 'rises' as a metric to measure distributivity in lattices, focusing on their relevance in Formal Concept Analysis.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a standardized measure for quantifying distributivity in lattices, which is significant in data analysis and FCA.

Method: Introduced the concept of 'rises' to evaluate changes in attributes or objects in concept lattices, and connected rises to classical distributivity concepts.

Result: Demonstrated that distributivity correlates to the absence of non-unit rises, and highlighted that real-world concept lattices are often join-distributive but less meet-distributive.

Conclusion: The paper contributes a significant metric for analyzing distributivity in concept lattices and links it to classical lattice theory, offering new insights into real-world data patterns.

Abstract: Distributivity is a well-established and extensively studied notion in
lattice theory. In the context of data analysis, particularly within Formal
Concept Analysis (FCA), lattices are often observed to exhibit a high degree of
distributivity. However, no standardized measure exists to quantify this
property. In this paper, we introduce the notion of rises in (concept) lattices
as a means to assess distributivity. Rises capture how the number of attributes
or objects in covering concepts change within the concept lattice. We show that
a lattice is distributive if and only if no non-unit rises occur. Furthermore,
we relate rises to the classical notion of meet- and join distributivity. We
observe that concept lattices from real-world data are to a high degree
join-distributive, but much less meet-distributive. We additionally study how
join-distributivity manifests on the level of ordered sets.

</details>


### [18] [FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis](https://arxiv.org/abs/2506.23273)
*Quang Hung Nguyen,Phuong Anh Trinh,Phan Quoc Hung Mai,Tuan Phong Trinh*

Main category: cs.AI

TL;DR: The paper introduces FinStat2SQL, a lightweight pipeline that leverages large and small language models for text-to-SQL query generation over financial statements, achieving accurate and fast results on consumer hardware.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of text2sql in complex and domain-specific queries, particularly in finance, due to diverse database designs and financial reporting standards.

Method: Proposed a multi-agent setup combining large and small language models for tasks like entity extraction, SQL generation, and self-correction using a domain-specific database.

Result: A fine-tuned 7B language model achieved 61.33% accuracy and sub-4-second response times, outperforming existing models like GPT-4o-mini.

Conclusion: FinStat2SQL provides an accessible, cost-efficient solution for financial analysis, enabling Vietnamese enterprises to leverage AI-powered query systems effectively.

Abstract: Despite the advancements of large language models, text2sql still faces many
challenges, particularly with complex and domain-specific queries. In finance,
database designs and financial reporting layouts vary widely between financial
entities and countries, making text2sql even more challenging. We present
FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries
over financial statements. Tailored to local standards like VAS, it combines
large and small language models in a multi-agent setup for entity extraction,
SQL generation, and self-correction. We build a domain-specific database and
evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves
61.33\% accuracy with sub-4-second response times on consumer hardware,
outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient
solution for financial analysis, making AI-powered querying accessible to
Vietnamese enterprises.

</details>


### [19] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
*David Guzman Piedrahita,Yongjin Yang,Mrinmaya Sachan,Giorgia Ramponi,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.AI

TL;DR: This paper studies how large language models manage cooperation and sanctions in multi-agent systems using a public goods game.


<details>
  <summary>Details</summary>
Motivation: With LLMs increasingly acting as autonomous agents, understanding their balance between self-interest and collective welfare is crucial for safe and robust deployment.

Method: The paper uses a public goods game with institutional choice to test how various LLMs behave in collaborative environments over repeated interactions.

Result: Models exhibited diverse patterns: some ensured high cooperation, others fluctuated or declined in cooperation, while some adhered to fixed strategies. Traditional LLMs outperformed reasoning-focused LLMs in cooperation.

Conclusion: Improved reasoning capabilities in LLMs do not guarantee better cooperation, underscoring the need for advanced strategies for sustained collaboration in multi-agent LLM systems.

Abstract: As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [20] [GATSim: Urban Mobility Simulation with Generative Agents](https://arxiv.org/abs/2506.23306)
*Qi Liu,Can Li,Wanjing Ma*

Main category: cs.AI

TL;DR: The paper introduces GATSim, a simulation framework for urban mobility using generative agents with realistic behavioral capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional urban mobility simulations lack the adaptability and diversity in human behavioral modeling. Advances in AI and large language models provide tools to address this gap.

Method: The method integrates an urban mobility foundation model with agent cognitive systems and a transport simulation environment, creating generative agents that evolve and adapt through learning mechanisms.

Result: Generative agents demonstrated believable travel behaviors and matched human annotators in mobility scenarios while replicating real-world traffic evolution patterns.

Conclusion: GATSim offers a novel way of simulating urban mobility with adaptive generative agents, providing more realistic and human-like transport simulation outcomes.

Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based
systems that fail to capture the complexity, adaptability, and behavioral
diversity characteristic of human travel decision-making. Recent advances in
large language models and AI agent technology offer opportunities to create
agents with reasoning capabilities, persistent memory, and adaptive learning
mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel
framework that leverages these advances to create generative agents with rich
behavioral characteristics for urban mobility simulation. Unlike conventional
approaches, GATSim agents possess diverse socioeconomic attributes, individual
lifestyles, and evolving preferences that shape their mobility decisions
through psychologically-informed memory systems, tool usage capabilities, and
lifelong learning mechanisms. The main contributions of this study include: (1)
a comprehensive architecture combining an urban mobility foundation model with
agent cognitive systems and transport simulation environment, (2) a fully
functional prototype implementation, and (3) systematic validation
demonstrating that generative agents produce believable travel behaviors.
Through designed reflection processes, generative agents in this study can
transform specific travel experiences into generalized insights, enabling
realistic behavioral adaptation over time with specialized mechanisms for
activity planning and real-time reactive behaviors tailored to urban mobility
contexts. Experiments show that generative agents perform competitively with
human annotators in mobility scenarios while naturally producing macroscopic
traffic evolution patterns. The code for the prototype system is shared at
https://github.com/qiliuchn/gatsim.

</details>


### [21] [The Confidence Paradox: Can LLM Know When It's Wrong](https://arxiv.org/abs/2506.23464)
*Sahil Tripathi,Md Tabrez Nafis,Imran Hussain,Jiechao Gao*

Main category: cs.AI

TL;DR: The paper introduces HonestVQA, a framework addressing ethical challenges in Document Visual Question Answering (DocVQA) systems by aligning model confidence with correctness and introducing metrics for evaluation.


<details>
  <summary>Details</summary>
Motivation: Ethical concerns in DocVQA systems arise due to overconfidence and lack of uncertainty communication, especially in critical domains. Existing models prioritize accuracy but neglect ethical responsiveness.

Method: HonestVQA employs a self-supervised setup with weighted loss functions to align confidence and correctness, and uses contrastive learning to enforce ethical response behavior. Metrics like H-Score and ECI assess ethical alignment.

Result: HonestVQA improved accuracy by 4.3% and F1 by 4.3%, reduced overconfidence, and achieved notable performance in cross-domain evaluations. Ablation shows alignment/contrastive loss critical for performance.

Conclusion: HonestVQA bridges accuracy and ethical alignment gaps in DocVQA systems, showcasing improved performance and ethical responsiveness through uncertainty calibration and principled metrics.

Abstract: Document Visual Question Answering (DocVQA) systems are increasingly deployed
in real world applications, yet they remain ethically opaque-often producing
overconfident answers to ambiguous questions or failing to communicate
uncertainty in a trustworthy manner. This misalignment between model confidence
and actual knowledge poses significant risks, particularly in domains requiring
ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT
have advanced SOTA performance by focusing on architectural sophistication and
accuracy; however, they fall short in ethical responsiveness.
  To address these limitations, we introduce HonestVQA, a self-supervised
honesty calibration framework for ethically aligned DocVQA. Our model-agnostic
method quantifies uncertainty to identify knowledge gaps, aligns model
confidence with actual correctness using weighted loss functions, and enforces
ethical response behavior via contrastive learning. We further introduce two
principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence
Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical
communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%
and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces
overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In
cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,
demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy
without alignment or contrastive loss.

</details>


### [22] [Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence](https://arxiv.org/abs/2506.23503)
*Bosubabu Sambana,Kondreddygari Archana,Suram Indhra Sena Reddy,Shaik Meethaigar Jameer Basha,Shaik Karishma*

Main category: cs.AI

TL;DR: The paper proposes a system using advanced NLP models to identify and analyze cognitive distortions and negative emotions in social media content for mental health monitoring.


<details>
  <summary>Details</summary>
Motivation: The motivation for this research stems from a gap in methodologies that accurately analyze cognitive pathways for delivering timely mental health interventions in online environments.

Method: The method involves using NLP models like BERT, RoBERTa for sentiment analysis, T5, PEGASUS for summarization, and mT5 for multilingual translation to classify digital content while focusing on detecting cognitive distortions.

Result: The system advances beyond existing models to also predict additional negative mental health outcomes, such as phobias and eating disorders, thus providing a more comprehensive mental health analysis.

Conclusion: This proposed system equips psychotherapists with a robust tool for early detection and intervention of psychological disorders based on social media behavior.

Abstract: Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the
irrational thought patterns associated with mental health disorders, but its
effectiveness relies on accurately identifying cognitive pathways to provide
targeted treatment. In today's digital age, individuals often express negative
emotions on social media, where they may reveal cognitive distortions, and in
severe cases, exhibit suicidal tendencies. However, there is a significant gap
in methodologies designed to analyze these cognitive pathways, which could be
critical for psychotherapists aiming to deliver timely and effective
interventions in online environments. Cognitive Behavioral Therapy (CBT)
framework leveraging acceptance, commitment and data augmentation to categorize
and address both textual and visual content as positive or negative.
Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,
PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages
focusing on detecting negative emotions and cognitive distortions within social
media data. While existing models are primarily designed to identify negative
thoughts, the proposed system goes beyond this by predicting additional
negative side effects and other potential mental health disorders likes
Phobias, Eating Disorders. This enhancement allows for a more comprehensive
understanding and intervention strategy, offering psychotherapists a powerful
tool for early detection and treatment of various psychological issues.

</details>


### [23] [Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM](https://arxiv.org/abs/2506.23504)
*Bosubabu Sambana,Kotamsetty Geethika Devi,Bandi Rajeswara Reddy,Galeti Mohammad Hussain,Gownivalla Siddartha*

Main category: cs.AI

TL;DR: A hybrid model combining AlexNet and LSTM achieves improved accuracy in forecasting electricity prices compared to traditional and standalone models like RNN and ANN.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional methods and standalone machine learning models in accurately forecasting electricity prices, especially considering important external variables.

Method: The authors propose a hybrid approach combining AlexNet for feature extraction and LSTM for sequential pattern learning. They incorporate external variables such as demand, temperature, sunlight, and rain into the model.

Result: The hybrid model achieved an accuracy rating of 97.08%, outperforming standalone models like RNN (96.64%) and ANN (96.63%) in terms of prediction accuracy.

Conclusion: Using a hybrid model improves accuracy in electricity price forecasting, demonstrating the effectiveness of combining AlexNet and LSTM while focusing on external influencing factors.

Abstract: The recent development of advanced machine learning methods for hybrid models
has greatly addressed the need for the correct prediction of electrical prices.
This method combines AlexNet and LSTM algorithms, which are used to introduce a
new model with higher accuracy in price forecasting. Despite RNN and ANN being
effective, they often fail to deal with forex time sequence data. The
traditional methods do not accurately forecast the prices. These traditional
methods only focus on demand and price which leads to insufficient analysis of
data. To address this issue, using the hybrid approach, which focuses on
external variables that also effect the predicted prices. Nevertheless, due to
AlexNet's excellent feature extraction and LSTM's learning sequential patterns,
the prediction accuracy is vastly increased. The model is built on the past
data, which has been supplied with the most significant elements like demand,
temperature, sunlight, and rain. For example, the model applies methods, such
as minimum-maximum scaling and a time window, to predict the electricity prices
of the future. The results show that this hybrid model is good than the
standalone ones in terms of accuracy. Although we got our accuracy rating of
97.08, it shows higher accompaniments than remaining models RNN and ANN with
accuracies of 96.64 and 96.63 respectively.

</details>


### [24] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
*Selin Dik,Osman Erdem,Mehmet Dik*

Main category: cs.AI

TL;DR: The study evaluates GPTZero's ability to detect AI-generated versus human-written essays, finding it effective for AI text but prone to false positives for human-authored content.


<details>
  <summary>Details</summary>
Motivation: With the widespread use of AI tools by students, reliable detection methods are needed to distinguish AI-generated text from human-written text.

Method: Researchers analyzed 28 AI-generated and 50 human-written essays of varying lengths, using GPTZero to assess AI confidence and generation percentage.

Result: GPTZero successfully identified most AI-generated essays with high accuracy (91-100%), but struggled with human-written essays, resulting in false positives.

Conclusion: While GPTZero effectively detects AI-generated text, its limitations in accurately identifying human-authored content warrant caution in its use by educators.

Abstract: As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [25] [ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data](https://arxiv.org/abs/2506.23520)
*Yu Zhang,Ruijie Yu,Jidong Tian,Feng Zhu,Jiapeng Liu,Xiaokang Yang,Yaohui Jin,Yanyan Xu*

Main category: cs.AI

TL;DR: The paper introduces ChemActor, a large language model (LLM) fine-tuned for converting unstructured chemical procedures into structured action sequences, achieving state-of-the-art results in automation tasks.


<details>
  <summary>Details</summary>
Motivation: The study is driven by the need for automated extraction of chemical procedures from literature, which is challenging due to ambiguous chemical language and costly human annotation requirements.

Method: The authors developed ChemActor, a fine-tuned LLM, and proposed a framework that includes a data selection module to handle low-quality data and a multi-round LLM review metric to improve understanding of chemical procedures.

Result: ChemActor achieves state-of-the-art performance in converting reaction descriptions to structured actions with a 10% improvement over baseline models in the R2D and D2A tasks.

Conclusion: The framework and ChemActor demonstrate the potential of LLM-based approaches in automating chemical procedure understanding and execution, significantly advancing the field.

Abstract: With the increasing interest in robotic synthesis in the context of organic
chemistry, the automated extraction of chemical procedures from literature is
critical. However, this task remains challenging due to the inherent ambiguity
of chemical language and the high cost of human annotation required for
developing reliable computer-aided extraction protocols. Here, we present
ChemActor, a fully fine-tuned large language model (LLM), as a chemical
executor to convert between unstructured experimental procedures and structured
action sequences. We propose a sequential LLM-generated data framework to
address the challenges of insufficient and low-quality annotated data. This
framework integrates a data selection module that selects data based on
distribution divergence, with a general-purpose LLM, to generate
machine-executable actions from a single molecule input. Additionally, we
introduce a novel multi-round LLMs circle review metric, which reflects the
model's advanced understanding of chemical experimental procedures. Extensive
experiments on reaction-to-description (R2D) and description-to-action (D2A)
tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves
state-of-the-art performance, outperforming the baseline model by 10%. The code
is available at: https://github.com/Zhanghahah/ChemActor.

</details>


### [26] [CooT: Learning to Coordinate In-Context with Coordination Transformers](https://arxiv.org/abs/2506.23549)
*Huai-Chih Wang,Hsiang-Chun Chuang,Hsi-Chun Cheng,Dai-Jie Wu,Shao-Hua Sun*

Main category: cs.AI

TL;DR: The paper introduces Coordination Transformers (CooT), a framework for improving coordination in multi-agent systems by using recent interaction histories.


<details>
  <summary>Details</summary>
Motivation: Existing coordination approaches often fail to generalize well to unseen partners or demand extensive training, limiting their applicability in dynamic environments.

Method: The CooT framework leverages recent interaction data to predict aligned actions, trained on diverse agent pairs with complementary behaviors. It requires no explicit supervision or fine-tuning during deployment.

Result: CooT outperformed baseline methods on the Overcooked benchmark in adapting to unseen partners. Human evaluations further validated its effectiveness and its robustness in dynamic contexts.

Conclusion: CooT offers a robust and efficient coordination strategy for multi-agent systems, demonstrating practical improvements in adaptability and effectiveness.

Abstract: Effective coordination among artificial agents in dynamic and uncertain
environments remains a significant challenge in multi-agent systems. Existing
approaches, such as self-play and population-based methods, either generalize
poorly to unseen partners or require extensive training. To overcome these
limitations, we propose Coordination Transformers (CooT), a novel in-context
coordination framework that uses recent interaction histories to adapt to
unseen partners rapidly. Unlike previous approaches that primarily aim to
increase the diversity of training partners, CooT explicitly focuses on
adapting to new partner behaviors by predicting actions aligned with observed
partner interactions. Trained on interaction trajectories collected from
diverse pairs of agents with complementary behaviors, CooT quickly learns
effective coordination strategies without explicit supervision or fine-tuning.
Evaluations on the Overcooked benchmark demonstrate that CooT significantly
outperforms baseline methods in coordination tasks involving previously unseen
partners. Human evaluations further confirm CooT as the most effective
collaborative partner, while extensive ablations highlight its robustness,
flexibility, and sensitivity to context in multi-agent scenarios.

</details>


### [27] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao,Jiaxing Huang,Yawen Qiu,Michael K. Chen,Wenzheng Liu,Wei Zhang,Wenjie Zeng,Xikun Zhang,Jingyi Zhang,Yuxin Song,Wenhao Wu,Dacheng Tao*

Main category: cs.AI

TL;DR: The paper introduces MMReason, a benchmark for evaluating the long-chain reasoning capability of Multimodal Large Language Models (MLLMs) with diverse and challenging tasks.


<details>
  <summary>Details</summary>
Motivation: Current MLLM benchmarks fail to adequately test long-chain reasoning due to lack of diversity, susceptibility to guessability and memorization, and insufficient evaluation of intermediate reasoning steps.

Method: MMReason was developed with diverse multi-step problems in six disciplines, across multiple difficulty levels. It uses an open-ended question format filtered by a multi-model voting mechanism to reduce shortcut cases. Questions are annotated with step-by-step solutions and evaluated using a ternary scoring system.

Result: The benchmark highlights the reasoning strengths and weaknesses of leading MLLMs, providing valuable insights for further research.

Conclusion: MMReason aims to fill gaps in current MLLM evaluation methods and serve as a comprehensive resource for advancing reasoning capabilities in MLLMs.

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [28] [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/abs/2506.23576)
*Maria Carolina Cornelia Wit,Jun Pang*

Main category: cs.AI

TL;DR: This paper explores multi-agent systems to combat LLM jailbreaking attacks and identifies their strengths and trade-offs.


<details>
  <summary>Details</summary>
Motivation: To address concerns about LLM jailbreaking attacks and explore improving safety mechanisms by leveraging multi-agent systems.

Method: The paper reproduces the AutoDefense framework and evaluates single-agent, two-agent, and three-agent setups against three jailbreak strategies: AutoDefense, BetterDan, and JB.

Result: Multi-agent systems reduce jailbreaking attacks, particularly false negatives, but are less effective for some attacks and cause increased false positives and higher computational costs.

Conclusion: Multi-agent setups improve defence robustness but have limitations, highlighting the need for refined approaches to strengthen LLM alignment and safety.

Abstract: Recent advances in large language models (LLMs) have raised concerns about
jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper
investigates the use of multi-agent LLM systems as a defence against such
attacks. We evaluate three jailbreaking strategies, including the original
AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the
AutoDefense framework, we compare single-agent setups with two- and three-agent
configurations. Our results show that multi-agent systems enhance resistance to
jailbreaks, especially by reducing false negatives. However, its effectiveness
varies by attack type, and it introduces trade-offs such as increased false
positives and computational overhead. These findings point to the limitations
of current automated defences and suggest directions for improving alignment
robustness in future LLM systems.

</details>


### [29] [Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games](https://arxiv.org/abs/2506.23626)
*António Afonso,Iolanda Leite,Alessandro Sestini,Florian Fuchs,Konrad Tollmar,Linus Gisslén*

Main category: cs.AI

TL;DR: The paper introduces an automated method to refine RL agent behaviors using a language model (LM) to tune reward function weights iteratively based on user-defined goals, reducing the need for manual engineering.


<details>
  <summary>Details</summary>
Motivation: Production deployment of RL agents faces challenges such as the need for expert-designed reward functions and the potential misalignment of these reward weights after game changes.

Method: The proposed method uses a language model (LM) to suggest updated reward function weights iteratively. It employs prior training performance statistics and user-defined goals to refine agent behavior in a closed-loop framework.

Result: In a racing task evaluation, LM-guided agents improved significantly, achieving up to a 74% success rate in one iteration and reaching 80% by the final iteration, showing comparable performance to expert-designed reward systems.

Conclusion: The approach demonstrates that LM-guided tuning can achieve competitive results in iterative RL optimization, reducing reliance on manual reward engineering and expert input.

Abstract: Reinforcement Learning (RL) in games has gained significant momentum in
recent years, enabling the creation of different agent behaviors that can
transform a player's gaming experience. However, deploying RL agents in
production environments presents two key challenges: (1) designing an effective
reward function typically requires an RL expert, and (2) when a game's content
or mechanics are modified, previously tuned reward weights may no longer be
optimal. Towards the latter challenge, we propose an automated approach for
iteratively fine-tuning an RL agent's reward function weights, based on a
user-defined language based behavioral goal. A Language Model (LM) proposes
updated weights at each iteration based on this target behavior and a summary
of performance statistics from prior training rounds. This closed-loop process
allows the LM to self-correct and refine its output over time, producing
increasingly aligned behavior without the need for manual reward engineering.
We evaluate our approach in a racing task and show that it consistently
improves agent performance across iterations. The LM-guided agents show a
significant increase in performance from $9\%$ to $74\%$ success rate in just
one iteration. We compare our LM-guided tuning against a human expert's manual
weight design in the racing task: by the final iteration, the LM-tuned agent
achieved an $80\%$ success rate, and completed laps in an average of $855$ time
steps, a competitive performance against the expert-tuned agent's peak $94\%$
success, and $850$ time steps.

</details>


### [30] [HASD: Hierarchical Adaption for pathology Slide-level Domain-shift](https://arxiv.org/abs/2506.23673)
*Jingsong Liu,Han Li,Chen Yang,Michael Deutges,Ario Sadafi,Xin You,Katharina Breininger,Nassir Navab,Peter J. Schüffler*

Main category: cs.AI

TL;DR: The paper proposes "Hierarchical Adaptation framework for Slide-level Domain-shift" (HASD) to tackle domain shift for pathology AI by addressing slide-level challenges, achieving improved outcomes on HER2 grading and survival prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Pathology AI systems face domain shift challenges due to center-specific conditions, impacting reliable performance. Existing methods mainly focus on image patches rather than capturing global slide-level features as required in clinical practice.

Method: HASD addresses slide-level domain shift using a hierarchical adaptation framework with three modules: Domain-level Alignment Solver for feature alignment, Slide-level Geometric Invariance Regularization to maintain morphological structures, and Patch-level Attention Consistency Regularization for preserving diagnostic cues. Additionally, it includes a prototype selection mechanism to reduce computational costs.

Result: HASD demonstrated significant improvements on two slide-level tasks, achieving a 4.1% AUROC boost in a Breast Cancer HER2 Grading cohort and a 3.9% C-index gain in a UCEC survival prediction cohort across five datasets.

Conclusion: The proposed HASD method effectively addresses slide-level domain adaptation in pathology AI with enhanced performance on multi-scale tasks while reducing computational and annotation costs.

Abstract: Domain shift is a critical problem for pathology AI as pathology data is
heavily influenced by center-specific conditions. Current pathology domain
adaptation methods focus on image patches rather than WSI, thus failing to
capture global WSI features required in typical clinical scenarios. In this
work, we address the challenges of slide-level domain shift by proposing a
Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD
achieves multi-scale feature consistency and computationally efficient
slide-level domain adaptation through two key components: (1) a hierarchical
adaptation framework that integrates a Domain-level Alignment Solver for
feature alignment, a Slide-level Geometric Invariance Regularization to
preserve the morphological structure, and a Patch-level Attention Consistency
Regularization to maintain local critical diagnostic cues; and (2) a prototype
selection mechanism that reduces computational overhead. We validate our method
on two slide-level tasks across five datasets, achieving a 4.1\% AUROC
improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in
a UCEC survival prediction cohort. Our method provides a practical and reliable
slide-level domain adaption solution for pathology institutions, minimizing
both computational and annotation costs.

</details>


### [31] [PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red](https://arxiv.org/abs/2506.23689)
*Zihao Liu,Xinhang Sui,Yueran Song,Siwen Wang*

Main category: cs.AI

TL;DR: PokéAI introduces a multi-agent framework using large language models to autonomously play Pokémon Red through specialized agents handling planning, execution, and critique.


<details>
  <summary>Details</summary>
Motivation: To explore the capability of LLMs in autonomous gameplay systems and analyze their performance in decision-making and strategic reasoning.

Method: The framework uses three agents - Planning, Execution, and Critique - with distinct roles and memory banks to divide and complete game tasks. A battle module was also developed to test AI performance in wild Pokémon encounters.

Result: The battle AI achieved an 80.8% win rate, closely matching human performance, with a strong correlation between language-related tasks and strategic reasoning. Gameplay analysis showed unique playstyles across different models.

Conclusion: PokéAI demonstrates how LLMs can exhibit strategic reasoning and individual behaviors in a closed-loop decision-making system for autonomous gameplay.

Abstract: We introduce Pok\'eAI, the first text-based, multi-agent large language model
(LLM) framework designed to autonomously play and progress through Pok\'emon
Red. Our system consists of three specialized agents-Planning, Execution, and
Critique-each with its own memory bank, role, and skill set. The Planning Agent
functions as the central brain, generating tasks to progress through the game.
These tasks are then delegated to the Execution Agent, which carries them out
within the game environment. Upon task completion, the Critique Agent evaluates
the outcome to determine whether the objective was successfully achieved. Once
verification is complete, control returns to the Planning Agent, forming a
closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution
Agent. Our results show that the battle AI achieves an average win rate of
80.8% across 50 wild encounters, only 6% lower than the performance of an
experienced human player. Furthermore, we find that a model's battle
performance correlates strongly with its LLM Arena score on language-related
tasks, indicating a meaningful link between linguistic ability and strategic
reasoning. Finally, our analysis of gameplay logs reveals that each LLM
exhibits a unique playstyle, suggesting that individual models develop distinct
strategic behaviors.

</details>


### [32] [Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models](https://arxiv.org/abs/2506.23692)
*Boyuan Zheng,Zerui Fang,Zhe Xu,Rui Wang,Yiwen Chen,Cunshi Wang,Mengwei Qu,Lei Lei,Zhen Feng,Yan Liu,Yuyang Li,Mingzhou Tan,Jiaji Wu,Jianwei Shuai,Jia Li,Fangfu Ye*

Main category: cs.AI

TL;DR: This paper introduces a roadmap for utilizing LLM-driven agents to fully automate scientific research workflows, proposing it as the Fifth Scientific Paradigm.


<details>
  <summary>Details</summary>
Motivation: Current research paradigms face inefficiencies that aren't fully addressed by AI analytical tools. The authors aim to establish a more transformative approach to scientific innovation.

Method: The paper proposes a five-level classification framework for "Agent for Science" systems, detailing their evolution from task automation to collaborative autonomy.

Result: A structured roadmap for developing "AI Scientists" is outlined, showcasing the potential for entirely automated scientific discovery workflows.

Conclusion: Agent4S represents a revolutionary shift in scientific methodology, enabling autonomous agents to redefine the process of research and discovery.

Abstract: While AI for Science (AI4S) serves as an analytical tool in the current
research paradigm, it doesn't solve its core inefficiency. We propose "Agent
for Science" (Agent4S)-the use of LLM-driven agents to automate the entire
research workflow-as the true Fifth Scientific Paradigm. This paper introduces
a five-level classification for Agent4S, outlining a clear roadmap from simple
task automation to fully autonomous, collaborative "AI Scientists." This
framework defines the next revolutionary step in scientific discovery.

</details>


### [33] [A New Perspective On AI Safety Through Control Theory Methodologies](https://arxiv.org/abs/2506.23703)
*Lars Ullrich,Walter Zimmer,Ross Greer,Knut Graichen,Alois C. Knoll,Mohan Trivedi*

Main category: cs.AI

TL;DR: The paper introduces a new interdisciplinary approach, called "data control," to enhance AI safety, particularly in real-world, safety-critical systems.


<details>
  <summary>Details</summary>
Motivation: AI's rapid advancements lack sufficient safety assurance, especially in its application to safety-critical cyber-physical systems.

Method: The authors propose a system theory-inspired, system analysis-driven methodology called "data control," which integrates control theory and AI safety assurance.

Result: A generic abstract framework for safety analysis and assurance is proposed, adaptable for future innovations in specific AI systems and applications.

Conclusion: This interdisciplinary approach leveraging control theory can drive advancements in safety assurance for AI systems, fostering a safer integration in safety-critical applications.

Abstract: While artificial intelligence (AI) is advancing rapidly and mastering
increasingly complex problems with astonishing performance, the safety
assurance of such systems is a major concern. Particularly in the context of
safety-critical, real-world cyber-physical systems, AI promises to achieve a
new level of autonomy but is hampered by a lack of safety assurance. While
data-driven control takes up recent developments in AI to improve control
systems, control theory in general could be leveraged to improve AI safety.
Therefore, this article outlines a new perspective on AI safety based on an
interdisciplinary interpretation of the underlying data-generation process and
the respective abstraction by AI systems in a system theory-inspired and system
analysis-driven manner. In this context, the new perspective, also referred to
as data control, aims to stimulate AI engineering to take advantage of existing
safety analysis and assurance in an interdisciplinary way to drive the paradigm
of data control. Following a top-down approach, a generic foundation for safety
analysis and assurance is outlined at an abstract level that can be refined for
specific AI systems and applications and is prepared for future innovation.

</details>


### [34] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
*Christoph Schnabl,Daniel Hugenroth,Bill Marino,Alastair R. Beresford*

Main category: cs.AI

TL;DR: This paper introduces a secure framework for auditing AI models using Trusted Execution Environments, ensuring verifiable and confidential results.


<details>
  <summary>Details</summary>
Motivation: Benchmarks are crucial for checking AI safety and compliance, but current methods lack result verification and data confidentiality.

Method: The paper proposes Attestable Audits, executed within Trusted Execution Environments, to securely evaluate AI models without exposing sensitive data.

Result: A prototype system was built to demonstrate the effectiveness of the approach on standard audit benchmarks applied to the Llama-3.1 model.

Conclusion: Attestable Audits can enhance trust and address verification challenges, aligning with modern AI governance requirements while protecting sensitive information.

Abstract: Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [35] [BayesL: Towards a Logical Framework for Bayesian Networks](https://arxiv.org/abs/2506.23773)
*Stefano M. Nicoletti,Mariëlle Stoelinga*

Main category: cs.AI

TL;DR: Introduction of BayesL, a framework for querying and verifying Bayesian networks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to simplify and enhance the process of querying and analyzing Bayesian networks via a structured logical framework.

Method: Development of BayesL, a language that allows versatile reasoning, including causal and evidence-based relationships, and executing hypothetical scenarios.

Result: BayesL provides a means to query Bayesian networks without manual modifications, streamlining what-if evaluations.

Conclusion: BayesL introduces a powerful logical tool for interacting with Bayesian networks, improving efficiency and versatility in reasoning processes.

Abstract: We introduce BayesL, a novel logical framework for specifying, querying, and
verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil")
is a structured language that allows for the creation of queries over BNs. It
facilitates versatile reasoning concerning causal and evidence-based
relationships, and permits comprehensive what-if scenario evaluations without
the need for manual modifications to the model.

</details>


### [36] [When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)](https://arxiv.org/abs/2506.23784)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,Julie Cailler,Chencheng Liang,Philipp Rümmer*

Main category: cs.AI

TL;DR: This paper explores using Graph Neural Networks (GNNs) to optimize the solving of word equations by ranking them effectively.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of solving conjunctions of word equations, particularly by addressing how the order of equation processing impacts solver performance.

Method: Introduces a graph-based representation for word equations, employs GNNs for ranking equations, and adapts multi-classification approaches for handling variable numbers of conjuncts. Training relies on minimum unsatisfiable subsets of word equations.

Result: The proposed framework outperforms state-of-the-art string solvers in benchmarks where variables appear once per equation.

Conclusion: GNNs, aided by a holistic representation of word equations, provide a more efficient framework for solving word equations.

Abstract: Nielsen transformation is a standard approach for solving word equations: by
repeatedly splitting equations and applying simplification steps, equations are
rewritten until a solution is reached. When solving a conjunction of word
equations in this way, the performance of the solver will depend considerably
on the order in which equations are processed. In this work, the use of Graph
Neural Networks (GNNs) for ranking word equations before and during the solving
process is explored. For this, a novel graph-based representation for word
equations is presented, preserving global information across conjuncts,
enabling the GNN to have a holistic view during ranking. To handle the variable
number of conjuncts, three approaches to adapt a multi-classification task to
the problem of ranking equations are proposed. The training of the GNN is done
with the help of minimum unsatisfiable subsets (MUSes) of word equations. The
experimental results show that, compared to state-of-the-art string solvers,
the new framework solves more problems in benchmarks where each variable
appears at most once in each equation.

</details>


### [37] [Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning](https://arxiv.org/abs/2506.23793)
*Anton Andreychuk,Konstantin Yakovlev,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: The paper introduces MAPF-GPT-DDG, a decentralized MAPF solver that improves upon previous models using a novel delta-data generation mechanism for enhanced scalability and test performance.


<details>
  <summary>Details</summary>
Motivation: Multi-agent pathfinding (MAPF) is critical for solving trajectory planning problems for robots in real-world applications like logistics and search-and-rescue. Efficient and scalable solutions are essential given its NP-hard nature.

Method: The authors propose MAPF-GPT-DDG, which fine-tunes a pre-trained MAPF model (MAPF-GPT) using centralized expert data and a novel delta-data generation mechanism to improve training efficiency and solution quality.

Result: MAPF-GPT-DDG surpasses prior learning-based MAPF solvers, achieving better solution quality and solving instances with up to 1 million agents in a single environment.

Conclusion: MAPF-GPT-DDG marks a significant advancement in scalable and efficient decentralized MAPF solving, demonstrating superior solution quality and unprecedented scalability for large agent instances.

Abstract: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot
trajectory planning problems, where multiple homogeneous robots simultaneously
move in the shared environment. While solving MAPF optimally has been proven to
be NP-hard, scalable, and efficient, solvers are vital for real-world
applications like logistics, search-and-rescue, etc. To this end, decentralized
suboptimal MAPF solvers that leverage machine learning have come on stage.
Building on the success of the recently introduced MAPF-GPT, a pure imitation
learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively
fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging
a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training
while significantly improving performance at test time. Our experiments
demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF
solvers, including the original MAPF-GPT, regarding solution quality across
many testing scenarios. Remarkably, it can work with MAPF instances involving
up to 1 million agents in a single environment, setting a new milestone for
scalability in MAPF domains.

</details>


### [38] [A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents](https://arxiv.org/abs/2506.23844)
*Hang Su,Jun Luo,Chang Liu,Xiao Yang,Yichi Zhang,Yinpeng Dong,Jun Zhu*

Main category: cs.AI

TL;DR: The paper surveys security vulnerabilities of autonomous AI agents and introduces Reflective Risk-Aware Agent Architecture (R2A2), a framework for risk mitigation.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address novel security risks posed by autonomous AI agents that operate with enhanced capabilities in dynamic, open-ended environments.

Method: The authors analyze key vulnerabilities of autonomous AI agents and propose the R2A2 architecture based on CMDPs, incorporating risk-aware modeling and strategies for safety.

Result: The study identifies risks such as memory poisoning and deceptive behaviors, and reviews defense strategies across various autonomy layers.

Conclusion: The R2A2 framework can potentially enable safer, more reliable operation of autonomous AI agents by mitigating emerging risks proactively.

Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of
autonomous AI agents capable of perceiving, reasoning, and acting in dynamic,
open-ended environments. These large-model agents mark a paradigm shift from
static inference systems to interactive, memory-augmented entities. While these
capabilities significantly expand the functional scope of AI, they also
introduce qualitatively novel security risks - such as memory poisoning, tool
misuse, reward hacking, and emergent misalignment - that extend beyond the
threat models of conventional systems or standalone LLMs. In this survey, we
first examine the structural foundations and key capabilities that underpin
increasing levels of agent autonomy, including long-term memory retention,
modular tool use, recursive planning, and reflective reasoning. We then analyze
the corresponding security vulnerabilities across the agent stack, identifying
failure modes such as deferred decision hazards, irreversible tool chains, and
deceptive behaviors arising from internal state drift or value misalignment.
These risks are traced to architectural fragilities that emerge across
perception, cognition, memory, and action modules. To address these challenges,
we systematically review recent defense strategies deployed at different
autonomy layers, including input sanitization, memory lifecycle control,
constrained decision-making, structured tool invocation, and introspective
reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a
unified cognitive framework grounded in Constrained Markov Decision Processes
(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,
and joint reward-risk optimization to enable principled, proactive safety
across the agent's decision-making loop.

</details>


### [39] [Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence](https://arxiv.org/abs/2506.23908)
*András György,Tor Lattimore,Nevena Lazić,Csaba Szepesvári*

Main category: cs.AI

TL;DR: AI systems, including advanced transformer models, struggle with sound deductive reasoning due to their statistical learning foundations. This paper advocates for a shift towards exact learning, prioritizing correctness on all inputs.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need to address the inability of current AI models to consistently perform sound deductive reasoning, a critical aspect of general intelligence, despite their advances in other domains.

Method: The method proposed is a fundamental shift in research focus from statistical learning approaches, which optimize models for average performance, to exact learning paradigms, which aim for absolute correctness on all inputs.

Result: The paper offers a strong argument for transitioning to exact learning to achieve better deductive reasoning capabilities in AI systems, though it might not explicitly showcase empirical results.

Conclusion: To achieve AI with reliable deductive reasoning, the field must adopt exact learning as an essential design principle, focusing on correctness over distributional performance.

Abstract: Sound deductive reasoning -- the ability to derive new knowledge from
existing facts and rules -- is an indisputably desirable aspect of general
intelligence. Despite the major advances of AI systems in areas such as math
and science, especially since the introduction of transformer architectures, it
is well-documented that even the most advanced frontier systems regularly and
consistently falter on easily-solvable deductive reasoning tasks. Hence, these
systems are unfit to fulfill the dream of achieving artificial general
intelligence capable of sound deductive reasoning. We argue that their unsound
behavior is a consequence of the statistical learning approach powering their
development. To overcome this, we contend that to achieve reliable deductive
reasoning in learning-based AI systems, researchers must fundamentally shift
from optimizing for statistical performance against distributions on reasoning
problems and algorithmic tasks to embracing the more ambitious exact learning
paradigm, which demands correctness on all inputs. We argue that exact learning
is both essential and possible, and that this ambitious objective should guide
algorithm design.

</details>


### [40] [Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice](https://arxiv.org/abs/2506.23924)
*Akshit Kumar,Tianyi Peng,Yuhang Wu,Assaf Zeevi*

Main category: cs.AI

TL;DR: The research explores whether large language models (LLMs) can effectively solve stochastic modeling problems in Operations Research (OR) and finds that LLMs perform comparably to human experts, though more work is required for full automation.


<details>
  <summary>Details</summary>
Motivation: This paper aims to investigate the untapped potential of LLMs in solving complex stochastic modeling problems in Operations Research, particularly in scenarios involving uncertainty.

Method: The study involves evaluating LLMs' performance on a curated set of graduate-level and doctoral OR problems, as well as using the SimOpt simulation-optimization library for practical decision-making tests.

Result: The experiments reveal that LLMs are proficient in solving complex stochastic modeling problems, comparing favorably to human experts in academic and real-world contexts.

Conclusion: LLMs show promise in assisting researchers and expanding the impact of Operations Research through partial automation, though further development is needed for full applicability.

Abstract: Large language models (LLMs) have exhibited expert-level capabilities across
various domains. However, their abilities to solve problems in Operations
Research (OR) -- the analysis and optimization of mathematical models derived
from real-world problems or their verbal descriptions -- remain underexplored.
In this work, we take a first step toward evaluating LLMs' abilities to solve
stochastic modeling problems, a core class of OR problems characterized by
uncertainty and typically involving tools from probability, statistics, and
stochastic processes. We manually procure a representative set of
graduate-level homework and doctoral qualification-exam problems and test LLMs'
abilities to solve them. We further leverage SimOpt, an open-source library of
simulation-optimization problems and solvers, to investigate LLMs' abilities to
make real-world decisions under uncertainty. Our results show that, though a
nontrivial amount of work is still needed to reliably automate the stochastic
modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on
par with human experts in both classroom and practical settings. These findings
highlight the potential of building AI agents that assist OR researchers and
amplify the real-world impact of OR through automation.

</details>


### [41] [Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system](https://arxiv.org/abs/2506.23926)
*Junping Wang,Bicheng Wang,Yibo Xuea,Yuan Xie*

Main category: cs.AI

TL;DR: The paper introduces an 'industrial brain' combining neuro networks and symbolic reasoning for improving resilience prediction in chaotic industrial chain settings.


<details>
  <summary>Details</summary>
Motivation: To improve resilience prediction and planning in industrial chains, especially in complex and chaotic systems, where existing methods fail.

Method: Proposed a cognitive framework called the 'industrial brain' that integrates a higher-order neuro network with CT-OODA symbolic reasoning for resilience planning.

Result: The industrial brain achieved a resilience accuracy improvement of up to 10.8% and generalized well to unseen scenarios, outperforming existing methods.

Conclusion: The industrial brain effectively addresses challenges in resilience prediction and planning, providing robust and generalizable solutions for the industrial chain.

Abstract: Resilience non-equilibrium measurement, the ability to maintain fundamental
functionality amidst failures and errors, is crucial for scientific management
and engineering applications of industrial chain. The problem is particularly
challenging when the number or types of multiple co-evolution of resilience
(for example, randomly placed) are extremely chaos. Existing end-to-end deep
learning ordinarily do not generalize well to unseen full-feld reconstruction
of spatiotemporal co-evolution structure, and predict resilience of network
topology, especially in multiple chaos data regimes typically seen in
real-world applications. To address this challenge, here we propose industrial
brain, a human-like autonomous cognitive decision-making and planning framework
integrating higher-order activity-driven neuro network and CT-OODA symbolic
reasoning to autonomous plan resilience directly from observational data of
global variable. The industrial brain not only understands and model structure
of node activity dynamics and network co-evolution topology without simplifying
assumptions, and reveal the underlying laws hidden behind complex networks, but
also enabling accurate resilience prediction, inference, and planning.
Experimental results show that industrial brain significantly outperforms
resilience prediction and planning methods, with an accurate improvement of up
to 10.8\% over GoT and OlaGPT framework and 11.03\% over spectral dimension
reduction. It also generalizes to unseen topologies and dynamics and maintains
robust performance despite observational disturbances. Our findings suggest
that industrial brain addresses an important gap in resilience prediction and
planning for industrial chain.

</details>


### [42] [AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models](https://arxiv.org/abs/2506.23949)
*Anthony M. Barrett,Jessica Newman,Brandie Nonnecke,Nada Madkour,Dan Hendrycks,Evan R. Murphy,Krystal Jackson,Deepika Raman*

Main category: cs.AI

TL;DR: This paper discusses risk management practices for multi-purpose AI or GPAI/foundation models, addressing their benefits and associated risks while building on established standards.


<details>
  <summary>Details</summary>
Motivation: To address the risks and potential adverse events posed by GPAI/foundation models while enabling their beneficial capabilities.

Method: The paper adapts and builds upon existing AI risk management frameworks (e.g., NIST AI Risk Management Framework and ISO/IEC 23894) tailored specifically for GPAI/foundation model developers.

Result: It proposes risk-management controls and practices that developers of GPAI/foundation models can implement, while offering guidance for downstream developers of applications built on these models.

Conclusion: Facilitating conformity to industry standards, the document enhances risk management protocols for GPAI/foundation model development to mitigate risks and leverage their capabilities effectively.

Abstract: Increasingly multi-purpose AI models, such as cutting-edge large language
models or other 'general-purpose AI' (GPAI) models, 'foundation models,'
generative AI models, and 'frontier models' (typically all referred to
hereafter with the umbrella term 'GPAI/foundation models' except where greater
specificity is needed), can provide many beneficial capabilities but also risks
of adverse events with profound consequences. This document provides
risk-management practices or controls for identifying, analyzing, and
mitigating risks of GPAI/foundation models. We intend this document primarily
for developers of large-scale, state-of-the-art GPAI/foundation models; others
that can benefit from this guidance include downstream developers of end-use
applications that build on a GPAI/foundation model. This document facilitates
conformity with or use of leading AI risk management-related standards,
adapting and building on the generic voluntary guidance in the NIST AI Risk
Management Framework and ISO/IEC 23894, with a focus on the unique issues faced
by developers of GPAI/foundation models.

</details>


### [43] [Harnessing AI Agents to Advance Research on Refugee Child Mental Health](https://arxiv.org/abs/2506.23992)
*Aditya Shrivastava,Komal Gupta,Shraddha Arora*

Main category: cs.AI

TL;DR: The study explores AI-based frameworks for assessing mental health in displaced children using two Retrieval-Augmented Generation pipelines, with DeepSeek R1 showing higher accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the international refugee crisis by providing efficient mental health solutions for displaced children through AI-based analysis of unstructured health data.

Method: Two AI models, Zephyr-7B-beta and DeepSeek R1-7B, are tested for their ability to process humanitarian datasets without generating misinformation.

Result: DeepSeek R1-7B demonstrated superior performance over Zephyr-7B-beta, achieving 91% accuracy in answer relevance.

Conclusion: The proposed AI framework is scalable and effectively supports policymakers and agencies in recognizing and assisting the mental health needs of displaced children.

Abstract: The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91

</details>


### [44] [Constructing Non-Markovian Decision Process via History Aggregator](https://arxiv.org/abs/2506.24026)
*Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: This paper introduces a category theory-based framework to understand and evaluate non-Markovian dynamics in decision-making systems like reinforcement learning (RL).


<details>
  <summary>Details</summary>
Motivation: Traditional benchmarks inadequately assess algorithmic capacity to handle non-Markovian dynamics, which hinders progress in improving decision-making algorithms.

Method: The authors use category theory to formalize equivalencies between Markov Decision Processes (MDPs) and non-Markovian Decision Processes (NMDPs). They introduce the History Aggregator for State (HAS) for precise manipulation of state dependency in non-Markovian settings.

Result: The proposed HAS approach effectively represents a wide range of non-Markovian dynamics and enables rigorous, flexible evaluation of decision algorithms.

Conclusion: The theoretical and practical contributions provide a new perspective for studying non-Markovian dynamics, enhancing the robustness and scope of benchmarks for decision-making models.

Abstract: In the domain of algorithmic decision-making, non-Markovian dynamics manifest
as a significant impediment, especially for paradigms such as Reinforcement
Learning (RL), thereby exerting far-reaching consequences on the advancement
and effectiveness of the associated systems. Nevertheless, the existing
benchmarks are deficient in comprehensively assessing the capacity of decision
algorithms to handle non-Markovian dynamics. To address this deficiency, we
have devised a generalized methodology grounded in category theory. Notably, we
established the category of Markov Decision Processes (MDP) and the category of
non-Markovian Decision Processes (NMDP), and proved the equivalence
relationship between them. This theoretical foundation provides a novel
perspective for understanding and addressing non-Markovian dynamics. We further
introduced non-Markovianity into decision-making problem settings via the
History Aggregator for State (HAS). With HAS, we can precisely control the
state dependency structure of decision-making problems in the time series. Our
analysis demonstrates the effectiveness of our method in representing a broad
range of non-Markovian dynamics. This approach facilitates a more rigorous and
flexible evaluation of decision algorithms by testing them in problem settings
where non-Markovian dynamics are explicitly constructed.

</details>


### [45] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
*Bo Liu,Leon Guertler,Simon Yu,Zichen Liu,Penghui Qi,Daniel Balcells,Mickel Liu,Cheston Tan,Weiyan Shi,Min Lin,Wee Sun Lee,Natasha Jaques*

Main category: cs.AI

TL;DR: The paper introduces SPIRAL, a self-play framework enabling language models to develop reasoning skills through zero-sum games without human supervision.


<details>
  <summary>Details</summary>
Motivation: Enhance reasoning capabilities in language models without relying on human-curated problem-answer pairs and domain-specific reward engineering.

Method: Develop the SPIRAL framework using multi-agent reinforcement learning with self-play in zero-sum games, introducing role-conditioned advantage estimation (RAE) for stabilizing training.

Result: SPIRAL-trained models show improved math and general reasoning performance, with multi-game training further boosting capabilities. The reasoning transfer occurs through identified cognitive patterns.

Conclusion: Zero-sum games in self-play frameworks like SPIRAL provide a promising approach for autonomous development of transferable reasoning capabilities in language models.

Abstract: Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [46] [Oobleck: Low-Compromise Design for Fault Tolerant Accelerators](https://arxiv.org/abs/2506.22654)
*Guy Wilks,Brian Li,Jonathan Balkind*

Main category: cs.AR

TL;DR: The paper introduces Oobleck, a fault-tolerant accelerator architecture, and Viscosity language for efficient hardware-software co-design, reducing costs and improving reliability in data centers.


<details>
  <summary>Details</summary>
Motivation: Data center refresh cycles are lengthening, while increasing processor complexity leads to heightened fault risks in accelerator datapaths. Existing fault tolerance solutions incur high area costs.

Method: The paper proposes Oobleck, a modular fault-tolerant accelerator architecture, and Viscosity, an actor-based hardware-software co-design language. It also provides case studies and model analysis.

Result: Oobleck decreases failure-induced chip purchases without lowering aggregate throughput in data centers. Case studies show significant speedups (1.7x-5.16x) over software-only implementations under faults.

Conclusion: Oobleck and Viscosity offer a practical, cost-effective solution to fault tolerance in accelerator chips, showing promise in maintaining reliability and throughput in modern data centers.

Abstract: Data center hardware refresh cycles are lengthening. However, increasing
processor complexity is raising the potential for faults. To achieve longevity
in the face of increasingly fault-prone datapaths, fault tolerance is needed,
especially in on-chip accelerator datapaths. Previously researched methods for
adding fault tolerance to accelerator designs require high area, lowering chip
utilisation. We propose a novel architecture for accelerator fault tolerance,
Oobleck, which leverages modular acceleration to enable fault tolerance without
burdensome area requirements.
  In order to streamline the development and enforce modular conventions, we
introduce the Viscosity language, an actor based approach to hardware-software
co-design. Viscosity uses a single description of the accelerator's function
and produces both hardware and software descriptions.
  Our high-level models of data centers indicate that our approach can decrease
the number of failure-induced chip purchases inside data centers while not
affecting aggregate throughput, thus reducing data center costs. To show the
feasibility of our approach, we show three case-studies: FFT, AES, and DCT
accelerators. We additionally profile the performance under the key parameters
affecting latency. Under a single fault we can maintain speedups of between
1.7x-5.16x for accelerated applications over purely software implementations.
We show further benefits can be achieved by adding hot-spare FPGAs into the
chip.

</details>


### [47] [Approximate Logic Synthesis Using BLASYS](https://arxiv.org/abs/2506.22772)
*Jingxiao Ma,Soheil Hashemi,Sherief Reda*

Main category: cs.AR

TL;DR: This paper introduces BLASYS, an open-source tool for synthesizing approximate circuits using Boolean Matrix Factorization (BMF) to trade accuracy for design efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the need for reducing design area and power consumption by enabling a controlled trade-off between circuit accuracy and design metrics.

Method: The authors propose a methodology that uses BMF to approximate circuit truth tables, partition circuits into subcircuits for scalability, and apply design-space exploration to optimize subcircuit approximations.

Result: BLASYS achieved a 48.14% average area savings and a 5% average relative error across multiple benchmarks.

Conclusion: The proposed BLASYS approach effectively balances accuracy and complexity, providing substantial design efficiency improvements with controlled error rates.

Abstract: Approximate computing is an emerging paradigm where design accuracy can be
traded for improvements in design metrics such as design area and power
consumption. In this work, we overview our open-source tool, BLASYS, for
synthesis of approximate circuits using Boolean Matrix Factorization (BMF). In
our methodology the truth table of a given circuit is approximated using BMF to
a controllable approximation degree, and the results of the factorization are
used to synthesize the approximate circuit output. BLASYS scales up the
computations to large circuits through the use of partition techniques, where
an input circuit is partitioned into a number of interconnected subcircuits and
then a design-space exploration technique identifies the best order for
subcircuit approximations. BLASYS leads to a graceful trade-off between
accuracy and full circuit complexity as measured by design area. Using an
open-source design flow, we extensively evaluate our methodology on a number of
benchmarks, where we demonstrate that the proposed methodology can achieve on
average 48.14% in area savings, while introducing an average relative error of
5%.

</details>


### [48] [Sustainable operation of research infrastructure for novel computing](https://arxiv.org/abs/2506.23901)
*Yannik Stradmann,Joscha Ilmberger,Eric Müller,Johannes Schemmel*

Main category: cs.AR

TL;DR: The paper showcases how the BrainScaleS-2 neuromorphic system evolved into a sustainable public research platform, with specific infrastructure improvements and operational strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to make novel compute systems widely accepted and utilized by transitioning them into publicly available research platforms.

Method: The authors transformed the BrainScaleS-2 system from a lab setup to a public platform, employing infrastructure optimization, robust network designs, packaging for maintenance, and modern CI/CD techniques for operations.

Result: The BrainScaleS-2 system was successfully transitioned into a robust, maintainable, and publicly accessible platform, backed by continuous monitoring and system optimization strategies.

Conclusion: The study shares valuable insights into operating analog neuromorphic systems, highlighting rigorous infrastructure and operational methodologies to support their sustainable public utilization.

Abstract: Novel compute systems are an emerging research topic, aiming towards building
next-generation compute platforms. For these systems to thrive, they need to be
provided as research infrastructure to allow acceptance and usage by a large
community. By the example of the neuromorphic BrainScaleS-2 system, we showcase
the transformation from a laboratory setup to a sustainable, publicly available
platform. It is embedded into a purpose-built institute, tightly coupling a
conventional cluster with novel compute hardware. The network infrastructure is
optimized for robust operation, even in the case of unintended behavior of
individual devices. The systems themselves are packaged into 19-inch compatible
units to allow for easy maintenance and extension. We operate the platform
using modern CI/CD techniques and continuously assert its health using
automated system monitoring. Finally, we share our lessons learned during the
decade-long endeavor of operating analog neuromorphic systems as a publicly
available research platform.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [49] [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/abs/2506.22439)
*Javier Conde,Miguel González,María Grandury,Gonzalo Martínez,Pedro Reviriego,Mar Brysbaert*

Main category: cs.CL

TL;DR: This study evaluates how well large language models (LLMs) align with human ratings on psycholinguistic word features using Glasgow and Lancaster norms datasets, revealing stronger alignment with the former and emphasizing limitations in sensory associations.


<details>
  <summary>Details</summary>
Motivation: To explore the alignment of LLMs with human psycholinguistic ratings on word features, leveraging existing norms datasets, and to identify potential limitations in current models.

Method: The paper evaluates a group of representative LLMs against human ratings across 13 psycholinguistic features contained in the Glasgow and Lancaster norms datasets, covering thousands of words.

Result: The analysis found that LLMs generally align better with the Glasgow norms (e.g., arousal, valence, dominance) than the Lancaster norms (e.g., sensory associations like auditory and visual). This highlights a gap in sensory alignment likely due to the lack of embodied cognition in LLMs.

Conclusion: LLMs exhibit better alignment for abstract and emotional word features (Glasgow norms) but struggle with sensory-associated ones (Lancaster norms), underscoring the need for improvement in their handling of embodied cognition aspects.

Abstract: The evaluation of LLMs has so far focused primarily on how well they can
perform different tasks such as reasoning, question-answering, paraphrasing, or
translating. For most of these tasks, performance can be measured with
objective metrics, such as the number of correct answers. However, other
language features are not easily quantified. For example, arousal,
concreteness, or gender associated with a given word, as well as the extent to
which we experience words with senses and relate them to a specific sense.
Those features have been studied for many years by psycholinguistics,
conducting large-scale experiments with humans to produce ratings for thousands
of words. This opens an opportunity to evaluate how well LLMs align with human
ratings on these word features, taking advantage of existing studies that cover
many different language features in a large number of words. In this paper, we
evaluate the alignment of a representative group of LLMs with human ratings on
two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets
cover thirteen features over thousands of words. The results show that
alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated
(arousal, valence, dominance, concreteness, imageability, familiarity, and
gender) than on the Lancaster norms evaluated (introceptive, gustatory,
olfactory, haptic, auditory, and visual). This suggests a potential limitation
of current LLMs in aligning with human sensory associations for words, which
may be due to their lack of embodied cognition present in humans and
illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.

</details>


### [50] [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/abs/2506.22485)
*Sudip Dasgupta,Himanshu Shankar*

Main category: cs.CL

TL;DR: The paper introduces a modular system using AI agents for enterprise document review, achieving high accuracy, reducing error rates, and saving time compared to humans.


<details>
  <summary>Details</summary>
Motivation: Current solutions for document review are limited in handling unstructured texts or specific compliance checks, necessitating a system that can accurately and efficiently evaluate structured documents.

Method: The system involves modular AI agents using orchestration tools like LangChain and TruLens to evaluate documents across multiple criteria. Outputs are standardized for analysis and feedback loops with human reviewers improve performance.

Result: The AI system achieved 99% consistency, reduced error/bias rates by half, and cut review time from 30 minutes to 2.5 minutes per document, with a 95% agreement between AI and expert human judgments.

Conclusion: This modular system demonstrates scalability and high performance in automated enterprise document review, albeit with limitations like reliance on human oversight in complex cases and high operational costs.

Abstract: This study presents a modular, multi-agent system for the automated review of
highly structured enterprise business documents using AI agents. Unlike prior
solutions focused on unstructured texts or limited compliance checks, this
framework leverages modern orchestration tools such as LangChain, CrewAI,
TruLens, and Guidance to enable section-by-section evaluation of documents for
accuracy, consistency, completeness, and clarity. Specialized agents, each
responsible for discrete review criteria such as template compliance or factual
correctness, operate in parallel or sequence as required. Evaluation outputs
are enforced to a standardized, machine-readable schema, supporting downstream
analytics and auditability. Continuous monitoring and a feedback loop with
human reviewers allow for iterative system improvement and bias mitigation.
  Quantitative evaluation demonstrates that the AI Agent-as-Judge system
approaches or exceeds human performance in key areas: achieving 99% information
consistency (vs. 92% for humans), halving error and bias rates, and reducing
average review time from 30 to 2.5 minutes per document, with a 95% agreement
rate between AI and expert human judgment. While promising for a wide range of
industries, the study also discusses current limitations, including the need
for human oversight in highly specialized domains and the operational cost of
large-scale LLM usage. The proposed system serves as a flexible, auditable, and
scalable foundation for AI-driven document quality assurance in the enterprise
context.

</details>


### [51] [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486)
*Ming Cheung*

Main category: cs.CL

TL;DR: This paper introduces a framework using multiple small language models to detect hallucinations in responses by large language models (LLMs) through response verification against retrieved context.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of hallucinations in the responses of LLMs, which can undermine the reliability of these models in real-world applications, specifically in tasks like question answering.

Method: The framework verifies LLM-generated responses by dividing them into sentences and using multiple small language models to assess the probability of generating consistent 'Yes' tokens. Context from a vectorized database is used for validation.

Result: Experiments on real datasets show a 10% improvement in F1 scores for detecting accurate responses compared to hallucinations.

Conclusion: This approach demonstrates that integrating small language models can effectively validate LLM responses, offering a scalable solution for enhancing reliability in both academic and practical applications.

Abstract: Since the introduction of ChatGPT, large language models (LLMs) have
demonstrated significant utility in various tasks, such as answering questions
through retrieval-augmented generation. Context can be retrieved using a
vectorized database, serving as a foundation for LLMs to generate responses.
However, hallucinations in responses can undermine the reliability of LLMs in
practical applications, and they are not easily detectable in the absence of
ground truth, particularly in question-and-answer scenarios. This paper
proposes a framework that integrates multiple small language models to verify
responses generated by LLMs using the retrieved context from a vectorized
database. By breaking down the responses into individual sentences and
utilizing the probability of generating "Yes" tokens from the outputs of
multiple models for a given set of questions, responses, and relevant context,
hallucinations can be detected. The proposed framework is validated through
experiments with real datasets comprising over 100 sets of questions, answers,
and contexts, including responses with fully and partially correct sentences.
The results demonstrate a 10\% improvement in F1 scores for detecting correct
responses compared to hallucinations, indicating that multiple small language
models can be effectively employed for answer verification, providing a
scalable and efficient solution for both academic and practical applications.

</details>


### [52] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)
*Oliver Warke,Joemon M. Jose,Faegheh Hasibi,Jan Breitsohl*

Main category: cs.CL

TL;DR: The study introduces PromptAug, an innovative Large Language Model-based data augmentation method that improves text classification accuracy and F1-score on conflict and emotion datasets amid challenges like limited labeled data and content generation guardrails.


<details>
  <summary>Details</summary>
Motivation: With the growing prevalence of conflicts on social media, there is an urgent need for robust classification models to detect harmful behaviors. The challenge lies in the limited availability, cost, and difficulties associated with obtaining high-quality labeled training data for sensitive tasks.

Method: The paper presents PromptAug, a data augmentation approach using Large Language Models (LLMs) to address challenges in generating conflict-related text data. To assess its performance, the study includes evaluations under extreme data scarcity scenarios, statistical diversity analyses, and qualitative thematic analyses.

Result: PromptAug significantly improved classification accuracy and F1-score by 2% on conflict and emotion datasets. A thematic analysis also identified four problematic patterns in the augmented data: Linguistic Fluidity, Humor Ambiguity, Content Ambiguity, and Content Misinterpretation.

Conclusion: PromptAug provides an effective LLM-based data augmentation technique for sensitive tasks like detecting social media conflicts, bridging natural language processing and social science methodologies to enhance classification performance amidst data limitations.

Abstract: Given the rise of conflicts on social media, effective classification models
to detect harmful behaviours are essential. Following the
garbage-in-garbage-out maxim, machine learning performance depends heavily on
training data quality. However, high-quality labelled data, especially for
nuanced tasks like identifying conflict behaviours, is limited, expensive, and
difficult to obtain. Additionally, as social media platforms increasingly
restrict access to research data, text data augmentation is gaining attention
as an alternative to generate training data. Augmenting conflict-related data
poses unique challenges due to Large Language Model (LLM) guardrails that
prevent generation of offensive content. This paper introduces PromptAug, an
innovative LLM-based data augmentation method. PromptAug achieves statistically
significant improvements of 2% in both accuracy and F1-score on conflict and
emotion datasets. To thoroughly evaluate PromptAug against other data
augmentation methods we conduct a robust evaluation using extreme data scarcity
scenarios, quantitative diversity analysis and a qualitative thematic analysis.
The thematic analysis identifies four problematic patterns in augmented text:
Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and
Augmented Content Misinterpretation.
  Overall, this work presents PromptAug as an effective method for augmenting
data in sensitive tasks like conflict detection, offering a unique,
interdisciplinary evaluation grounded in both natural language processing and
social science methodology.

</details>


### [53] [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/abs/2506.22516)
*Jingkai Li*

Main category: cs.CL

TL;DR: This paper applies the Integrated Information Theory (IIT) to examine consciousness in Large Language Model representations, with tests on Theory of Mind performances.


<details>
  <summary>Details</summary>
Motivation: Understanding how IIT metrics can be applied to LLMs to investigate potential markers of "consciousness" phenomenon.

Method: Application of IIT 3.0 and 4.0 metrics, including $
\Phi^{\max}$, $
\Phi$, Conceptual Information, and $
\Phi$-structure, on LLM data derived from Theory of Mind tests, along with spatio-permutational analyses.

Result: The study finds no statistically significant indicators of "consciousness" in Transformer-based LLM representations but observes intriguing patterns in certain analyses.

Conclusion: Transformer-based LLM representations may not contain detectable indicators of consciousness based on IIT metrics, but further investigation into structural patterns is warranted.

Abstract: Integrated Information Theory (IIT) provides a quantitative framework for
explaining consciousness phenomenon, positing that conscious systems comprise
elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the
latest iterations of this framework -- to sequences of Large Language Model
(LLM) representations, analyzing data derived from existing Theory of Mind
(ToM) test results. Our study systematically investigates whether the
differences of ToM test performances, when presented in the LLM
representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT
3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure
(IIT 4.0). Furthermore, we compare these metrics with the Span Representations
independent of any estimate for consciousness. This additional effort aims to
differentiate between potential "consciousness" phenomena and inherent
separations within LLM representational space. We conduct comprehensive
experiments examining variations across LLM transformer layers and linguistic
spans from stimuli. Our results suggest that sequences of contemporary
Transformer-based LLM representations lack statistically significant indicators
of observed "consciousness" phenomena but exhibit intriguing patterns under
$\textit{spatio}$-permutational analyses. The Appendix and code are available
as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.

</details>


### [54] [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/abs/2506.22508)
*Chenyang Shao,Tianxing Li,Chenhao Pu,Fengli Xu,Yong Li*

Main category: cs.CL

TL;DR: The paper presents AgentStealth, a framework for text anonymization using locally deployed lightweight language models to ensure privacy without relying on costly and privacy-risky cloud solutions.


<details>
  <summary>Details</summary>
Motivation: Casual user-generated content often contains cues that can unintentionally reveal sensitive personal data, leading to privacy concerns. Current solutions either harm textual utility or are reliant on costly and risky cloud-based models.

Method: AgentStealth employs a three-step approach: adversarial anonymization using In-context Contrastive Learning and Adaptive Utility-Aware Control, supervised adaptation of small-scale language models (SLMs) with high-quality curated data, and online reinforcement learning to refine anonymization using self-feedback.

Result: On two datasets, the proposed method achieved a 12.3% improvement in anonymization effectiveness and a 6.8% utility gain compared to baselines. It is lightweight and deployable on local edge devices.

Conclusion: AgentStealth provides an effective, privacy-focused anonymization method that balances performance and utility, while eliminating reliance on cloud servers. It enhances text anonymization by leveraging lightweight, locally deployable models.

Abstract: In today's digital world, casual user-generated content often contains subtle
cues that may inadvertently expose sensitive personal attributes. Such risks
underscore the growing importance of effective text anonymization to safeguard
individual privacy. However, existing methods either rely on rigid replacements
that damage utility or cloud-based LLMs that are costly and pose privacy risks.
To address these issues, we explore the use of locally deployed smaller-scale
language models (SLMs) for anonymization. Yet training effective SLMs remains
challenging due to limited high-quality supervision. To address the challenge,
we propose AgentStealth, a self-reinforcing LLM anonymization framework.First,
we introduce an adversarial anonymization workflow enhanced by In-context
Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform
supervised adaptation of SLMs using high-quality data collected from the
workflow, which includes both anonymization and attack signals. Finally, we
apply online reinforcement learning where the model leverages its internal
adversarial feedback to iteratively improve anonymization performance.
Experiments on two datasets show that our method outperforms baselines in both
anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight
design supports direct deployment on edge devices, avoiding cloud reliance and
communication-based privacy risks. Our code is open-source at
https://github.com/tsinghua-fib-lab/AgentStealth.

</details>


### [55] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)
*Zihao Zhao,Xinlong Zhai,Jinyu Yang,Chuan Shi*

Main category: cs.CL

TL;DR: The paper introduces MDGCL, a framework for effective multi-domain pre-training and cross-domain transfer in graph data, addressing challenges in domain-specific differences.


<details>
  <summary>Details</summary>
Motivation: Foundation models effectively transfer multi-domain knowledge in NLP and CV, but struggle in the graph domain due to domain semantic gaps and inappropriate contrastive pre-training strategies.

Method: MDGCL introduces: (1) domain-aware contrastive learning to capture domain differences during pre-training, (2) domain tokens for encoding domain-global information, and (3) a domain attention mechanism for fine-grained knowledge transfer during downstream tasks.

Result: Experiments on five benchmarks show MDGCL significantly improves performance, achieving accuracy gains of up to 19.33% and Macro-F1 improvements of up to 19.13%.

Conclusion: MDGCL addresses domain-specific challenges in graph data, achieving superior results over current methods, and proving to be highly effective for cross-domain knowledge transfer.

Abstract: Foundation models have achieved great success in natural language processing
(NLP) and computer vision (CV). Their success largely stems from the ability to
integrate multi-domain knowledge in pre-training and transfer it to target
domains. Considering graph data, especially graphs without textual features, is
ubiquitous in real-world applications such as social networks and
recommendation systems, some researchers have attempted to extend this paradigm
to the graph field, aiming to construct graph foundation models. However,
unlike CV and NLP, there are huge gaps among the semantics and properties of
graphs in different domains, while current works still adopt traditional
contrastive pre-training strategies designed in the single-domain scenario,
which regard contrastive samples from different domains as equivalent. From
experimental investigations, we discovered that inherent domain-specific
differences prevent these strategies from effectively absorbing knowledge from
different domains to generate informative representations. In this paper, we
propose a novel multi-domain pre-training and cross-domain transfer framework,
namely MDGCL.In the pre-training stage, we design a contrastive learning
strategy to substantially recognize and capture domain differences, and
introduce domain tokens to encode domain-level global information. In the
downstream stage, we introduce a domain attention mechanism to enable
fine-grained domain knowledge transfer. Extensive experiments on five benchmark
datasets have demonstrated that our method outperforms state-of-the-art
significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\%
on Macro-F1 score.

</details>


### [56] [Objective-Free Local Learning and Emergent Language Structure in Thinking Machines](https://arxiv.org/abs/2506.23293)
*P. Myles Eugenio*

Main category: cs.CL

TL;DR: The paper presents a neuro-symbolic language model that uses a hierarchical Hopfield memory chain to dynamically tokenize and learn symbol structures without predefined rules or objectives, enabling emergent language structures and flexible generalization.


<details>
  <summary>Details</summary>
Motivation: To explore how symbolic language structures like tokens and grammar can emerge naturally through local learning mechanisms without relying on predefined tokens, supervised learning, or global objectives.

Method: The method involves a hierarchical Hopfield memory chain that acts as a dynamic short-term memory and retokenizer. The system employs Hebbian learning to build multi-scale symbol representations and aligns new information with emergent structures, fostering generalization without explicit data.

Result: The model demonstrates the ability to filter coherent language patterns from noise, create synthetic languages that resemble human language morphology, and activate emergent embedding neurons for compositional inference and long-term memory.

Conclusion: The study introduces a scalable and interpretable neuro-symbolic system capable of emergent grammar and reasoning, offering a novel approach for the development of neuromorphic architectures in generative language modeling.

Abstract: We present a neuro-symbolic framework for generative language modeling based
on local, event-driven emergent learning. At its core is a hierarchical
Hopfield memory chain acting as a compositional short-term memory and dynamic
tokenizer (retokenizer). Rather than relying on predefined tokens or
supervision, the model builds structure from scratch, learning symbol sequences
as multi-scale representations. It constructs projection tensors that bind
co-occurring features into hierarchical tokens, introducing redundancy (i.e an
emergent gauge structure) and enabling compression of local activations into
long-range dependencies. Curiously, we find that the retokenizer can filter
natural language patterns from noise, generating synthetic languages with
coherent internal morphology -- quantifiably the same as human language.
Language is learned in a local (Hebbian) fashion, where model constraints
dictate allowed emergent structure, and new information is retained in
alignment with this structure. The absence of a global objective enables a form
of plasticity not found in conventional language models, allowing the system to
generalize beyond its initial inference class -- even without explicit data. We
demonstrate that briefly activating a new neuron during inference binds
distributed multi-scale token features into a symbolic embedding. These
emergent embedding neurons act as long-term memory and support a key-value
mechanism for compositional inference and generalization. This architecture
provides a methodological foundation for studying how symbolic structure can
emerge from local neural learning. It offers a new pathway for building
scalable, interpretable neuro-symbolic systems -- where tokens, grammar, and
reasoning arise as compressed memory traces within a Hopfield hierarchy. This
approach advances the development of neuromorphic architectures for generative
language models.

</details>


### [57] [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/abs/2506.22518)
*Deyu Zou,Yongqiang Chen,Mufei Li,Siqi Miao,Chenxi Liu,Bo Han,James Cheng,Pan Li*

Main category: cs.CL

TL;DR: Refined Graph-based RAG (ReG) improves retrieval-augmented generation by enhancing weak retrievers with LLM feedback and structuring retrieved knowledge into coherent evidence chains, resulting in better performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current graph-based retrieval-augmented generation (RAG) approaches where weak retrievers introduce spurious signals and retrieved knowledge lacks organization, leading to suboptimal LLM performance and increased hallucinations.

Method: The authors propose ReG, which aligns weak retrievers to LLMs through feedback-based supervision, removing spurious signals, and employs a structure-aware reorganization module to present retrieval results as logically coherent evidence chains.

Result: ReG achieves up to a 10% improvement across different LLM backbones, matches state-of-the-art performance with only 5% training data, transfers effectively to out-of-distribution KGs, reduces reasoning token costs by up to 30%, and improves performance by up to 4%.

Conclusion: ReG demonstrates that enhancing retrievers with LLM feedback and organizing retrieved knowledge significantly boosts the efficiency and effectiveness of graph-based RAG systems, with potential applications in improving reasoning and reducing resource requirements.

Abstract: Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to ground responses with structured external knowledge from
up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs
often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground
truth, the retriever is often trained on weak supervision, which often
introduces spurious signals to the LLMs. II) Due to the abstraction of graph
data, the retrieved knowledge is often presented in unorganized forms. To
mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak
retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM
feedback to get rid of spurious signals and improve the quality of the
supervision. Meanwhile, ReG introduces a structure-aware reorganization module
to refactor the retrieval results into logically coherent evidence chains.
Experiments on prominent benchmarks demonstrate that ReG significantly and
consistently brings improvements across different LLM backbones by up to 10%.
The improved supervision quality enables ReG to match the state-of-the-art
performance with 5% training data and to transfer to out-of-distribution KGs.
Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token
cost by up to 30% and improves the performance by up to 4%.

</details>


### [58] [MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages](https://arxiv.org/abs/2506.22529)
*Lu Kalkbrenner,Veronika Solopova,Steffen Zeiler,Robert Nickel,Dorothea Kolossa*

Main category: cs.CL

TL;DR: The paper introduces Misinfo-TeleGraph, a German-language Telegram-based graph dataset, for misinformation detection, highlighting the benefits of graph neural networks (GNNs) over text-only models.


<details>
  <summary>Details</summary>
Motivation: To address the dissemination of misinformation on poorly moderated platforms like Telegram, especially in German electoral contexts, and to introduce tools that leverage connectivity and propagation information for misinformation detection.

Method: The authors created Misinfo-TeleGraph, a dataset from over 5 million Telegram messages with metadata and labels, evaluated it using text-only models and graph neural networks (GNNs), and tested variables like user metrics and types of supervision.

Result: GraphSAGE with LSTM aggregation outperformed text-only models in MCC and F1-score metrics and showcased the benefits of incorporating forwarding structures as graph networks.

Conclusion: Misinfo-TeleGraph provides a valuable open dataset and reproducible benchmark that highlights the potential of GNNs and connectivity information for misinformation detection, while also identifying challenges in using weak supervision.

Abstract: Connectivity and message propagation are central, yet often underutilized,
sources of information in misinformation detection -- especially on poorly
moderated platforms such as Telegram, which has become a critical channel for
misinformation dissemination, namely in the German electoral context. In this
paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based
graph dataset for misinformation detection. It includes over 5 million messages
from public channels, enriched with metadata, channel relationships, and both
weak and strong labels. These labels are derived via semantic similarity to
fact-checks and news articles using M3-embeddings, as well as manual
annotation. To establish reproducible baselines, we evaluate both text-only
models and graph neural networks (GNNs) that incorporate message forwarding as
a network structure. Our results show that GraphSAGE with LSTM aggregation
significantly outperforms text-only baselines in terms of Matthews Correlation
Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers,
view counts, and automatically versus human-created labels on performance, and
highlight both the potential and challenges of weak supervision in this domain.
This work provides a reproducible benchmark and open dataset for future
research on misinformation detection in German-language Telegram networks and
other low-moderation social platforms.

</details>


### [59] [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/abs/2506.22598)
*Nicholas Edwards,Yukyung Lee,Yujun,Mao,Yulu Qin,Sebastian Schuster,Najoung Kim*

Main category: cs.CL

TL;DR: This paper introduces a benchmark called RExBench to evaluate Large Language Model (LLM) agents' ability to autonomously perform realistic research extension tasks. Current agents fall short, showing a performance below 40% even with human hints.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLM agents can autonomously handle complex research extension tasks which are critical for advancing automation in research and software engineering.

Method: Developed RExBench, a benchmark of 12 tasks requiring implementation of novel research extensions, with data contamination safeguards and automatic evaluation. Tested nine LLM agents across three frameworks: aider, Claude Code, and OpenHands.

Result: None of the tested LLM agents could autonomously complete most tasks; even with human hints, the success rate did not exceed 40%.

Conclusion: Current LLM agents lack the capability to execute realistic research extensions autonomously, highlighting the need for further advancements.

Abstract: Agents based on Large Language Models (LLMs) have shown promise for
performing sophisticated software engineering tasks autonomously. In addition,
there has been progress towards developing agents that can perform parts of the
research pipeline in machine learning and the natural sciences. We argue that
research extension and its implementation is a critical capability for such
systems, and introduce RExBench to support the evaluation of this capability.
RExBench is a benchmark consisting of 12 realistic research experiment
implementation tasks that aim to investigate research hypotheses that have not
previously been implemented. Each task is set up as an extension to an existing
research paper and codebase, accompanied by domain expert-written instructions.
RExBench is robust to data contamination, and supports an automatic evaluation
infrastructure that executes agent outputs to determine whether the success
criteria are met. We use this benchmark to evaluate nine LLM agents implemented
using three different frameworks: aider, Claude Code, and OpenHands. We find
that all agents evaluated fail to autonomously implement the majority of the
extensions. Although the success rate improves with additional human-written
hints, the best performance under this setting remains below 40%. This
indicates that current agents are still short of being able to handle realistic
research extension tasks without substantial human guidance.

</details>


### [60] [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/abs/2506.22623)
*Badr Youbi Idrissi,Monica Millunzi,Amelia Sorrenti,Lorenzo Baraldi,Daryna Dementieva*

Main category: cs.CL

TL;DR: This paper introduces a new watermarking method to identify synthetic text and ensure ethical use of large language models.


<details>
  <summary>Details</summary>
Motivation: Concerns about the misuse of large language models (LLMs) drive the need for reliable identification methods for machine-generated text.

Method: The authors replicated baseline findings, proposed a novel watermarking methodology, and tested its robustness using paraphrased texts.

Result: Experimental results demonstrate the robustness of the proposed watermarking method compared to previous approaches.

Conclusion: The new watermarking technique offers a more reliable way to detect synthetic text, promoting responsible LLM usage.

Abstract: In the present-day scenario, Large Language Models (LLMs) are establishing
their presence as powerful instruments permeating various sectors of society.
While their utility offers valuable support to individuals, there are multiple
concerns over potential misuse. Consequently, some academic endeavors have
sought to introduce watermarking techniques, characterized by the inclusion of
markers within machine-generated text, to facilitate algorithmic
identification. This research project is focused on the development of a novel
methodology for the detection of synthetic text, with the overarching goal of
ensuring the ethical application of LLMs in AI-driven text generation. The
investigation commences with replicating findings from a previous baseline
study, thereby underscoring its susceptibility to variations in the underlying
generation model. Subsequently, we propose an innovative watermarking approach
and subject it to rigorous evaluation, employing paraphrased generated text to
asses its robustness. Experimental results highlight the robustness of our
proposal compared to the~\cite{aarson} watermarking method.

</details>


### [61] [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/abs/2506.22644)
*Chase Fensore,Kaustubh Dhole,Joyce C Ho,Eugene Agichtein*

Main category: cs.CL

TL;DR: This paper describes a hybrid retrieval-augmented generation (RAG) system for the LiveRAG Challenge 2025, combining sparse and dense retrieval with Falcon3-10B-Instruct for answer generation. The system achieves competitive results, particularly in faithfulness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of creating reliable and effective RAG systems for dynamically changing datasets, evaluated in competitive settings like the LiveRAG Challenge.

Method: The approach incorporates sparse (BM25) and dense (E5) retrieval methods, neural re-ranking using RankLLaMA (despite its computational cost), DSPy-optimized prompting, and evaluates their impact on performance metrics such as MAP and semantic similarity.

Result: The hybrid system achieves notable improvements in metrics (e.g., 52% improvement in MAP with re-ranking) but has trade-offs like increased computational costs. The system placed 4th in faithfulness and 11th in correctness at the competition.

Conclusion: Vocabulary alignment between queries and documents plays a critical role in RAG performance, and optimizing phrasing improves cosine similarity and system effectiveness. However, computational efficiency and generalizability remain key challenges.

Abstract: We present our submission to the LiveRAG Challenge 2025, which evaluates
retrieval-augmented generation (RAG) systems on dynamic test sets using the
FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense
(E5) retrieval methods and then aims to generate relevant and faithful answers
with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic
questions generated with DataMorgana across 64 unique question-user
combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP
from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive
computational costs (84s vs 1.74s per question). While DSPy-optimized prompting
strategies achieved higher semantic similarity (0.771 vs 0.668), their 0%
refusal rates raised concerns about over-confidence and generalizability. Our
submitted hybrid system without re-ranking achieved 4th place in faithfulness
and 11th place in correctness among 25 teams. Analysis across question
categories reveals that vocabulary alignment between questions and documents
was the strongest predictor of performance on our development set, with
document-similar phrasing improving cosine similarity from 0.562 to 0.762.

</details>


### [62] [Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions](https://arxiv.org/abs/2506.22679)
*Ankush Raut,Projna Paromita,Sydney Begerowski,Suzanne Bell,Theodora Chaspari*

Main category: cs.CL

TL;DR: The paper evaluates the capability of large language models (LLMs) in detecting micro-behaviors in team conversation transcripts from simulated space missions, with mixed results showing the superiority of decoder-only LLMs for certain tasks.


<details>
  <summary>Details</summary>
Motivation: To analyze team communication dynamics and develop speech technologies for training interventions in high-stakes environments like space missions, particularly when only text data is available.

Method: The study examines zero-shot classification, fine-tuning, and paraphrase-augmented fine-tuning using encoder-only LLMs, alongside few-shot generation with decoder-only LLMs for detecting micro-behaviors in conversational dialogues.

Result: Encoder-only LLMs struggled with underrepresented behaviors despite weighted fine-tuning, whereas instruction fine-tuned decoder-only LLMs like Llama-3.1 achieved better macro F1-scores: 44% for 3-way classification and 68% for binary classification.

Conclusion: Decoder-only LLMs, especially instruction fine-tuned variants, offer superior capabilities in detecting micro-behaviors and show promise in advancing text-based speech technologies for analyzing team communication in critical settings like space missions.

Abstract: We explore the feasibility of large language models (LLMs) in detecting
subtle expressions of micro-behaviors in team conversations using transcripts
collected during simulated space missions. Specifically, we examine zero-shot
classification, fine-tuning, and paraphrase-augmented fine-tuning with
encoder-only sequence classification LLMs, as well as few-shot text generation
with decoder-only causal language modeling LLMs, to predict the micro-behavior
associated with each conversational turn (i.e., dialogue). Our findings
indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to
detect underrepresented micro-behaviors, particularly discouraging speech, even
with weighted fine-tuning. In contrast, the instruction fine-tuned version of
Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best
models achieving macro F1-scores of 44% for 3-way classification and 68% for
binary classification. These results have implications for the development of
speech technologies aimed at analyzing team communication dynamics and
enhancing training interventions in high-stakes environments such as space
missions, particularly in scenarios where text is the only accessible data.

</details>


### [63] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)
*Raghavv Goel,Sudhanshu Agrawal,Mukul Gagrani,Junyoung Park,Yifan Zao,He Zhang,Tian Liu,Yiping Yang,Xin Yuan,Jiuyan Lu,Chris Lott,Mingu Lee*

Main category: cs.CL

TL;DR: The paper introduces VocabTrim, a method to improve speculative decoding speed by reducing vocabulary size during the drafting process.


<details>
  <summary>Details</summary>
Motivation: Address the inefficiency in speculative decoding for large language models in memory-bound environments.

Method: Propose VocabTrim, a technique to reconstruct the drafter LM head with a limited set of high-frequency sampled tokens.

Result: VocabTrim reduces drafting latency and boosts memory-bound speed-up by 16% for certain Llama-3 models.

Conclusion: VocabTrim significantly enhances speculative decoding performance without additional training, particularly in constrained environments like edge devices.

Abstract: In this paper, we introduce a simple training-free technique to improve the
performance of drafter-based speculative decoding (SpD) methods that
incorporates language modeling head (LM head) during drafting process. A
drafter-based speculative decoding leverages one or more smaller language
models, a.k.a. drafters or draft models, to sample a draft sequence or tree
consisting of multiple tokens, followed by verification by a base LLM, a target
model, accepting a subset as its valid generation. As it is usually considered
that the speculative decoding requires one-to-one mapping between vocabularies
of the target model and the draft model, it has been natural to share the
vocabulary between them, or even share the LM head as in EAGLE or Medusa. We
first identify that this draft token sampling scheme inherently contains an
unnecessary inference overhead in drafting, especially for some target LLMs
with very large vocabularies. Then, we propose a simple technique, VocabTrim,
to mitigate the drafting overhead to improve the generation speed in
memory-bound environment. VocabTrim reconstructs the drafter LM head to contain
only a limited set of tokens, selected by the most frequently sampled from the
vocabulary of the target model. While limiting the vocabulary in drafting
slightly degrades the acceptance rate, it significantly reduces the drafting
latency in memory-bound process which is often the case on edge devices,
resulting in higher memory-bound speed up (MBSU). We show that our method can
boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically
by 16% for Llama-3.2-3B-Instruct.

</details>


### [64] [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/abs/2506.22698)
*Emily Dux Speltz*

Main category: cs.CL

TL;DR: The report summarizes a workshop exploring the relationship between AI language models and human cognitive processes in text comprehension and production, highlighting both potential and limitations of LLMs.


<details>
  <summary>Details</summary>
Motivation: The workshop sought to address the gap in understanding how AI language models relate to and can augment human cognitive text processes.

Method: Experts in cognitive psychology, linguistics, and AI-NLP collaborated through interdisciplinary discussions to analyze LLMs' role in human cognition.

Result: Findings show LLMs can align more closely with human cognition when fine-tuned with human feedback, offering opportunities for collaboration but also highlighting challenges.

Conclusion: The report underscores the importance of ethical and responsible AI use, aiming to support advancements in enhancing human-AI collaboration for language tasks.

Abstract: This report synthesizes the outcomes of a recent interdisciplinary workshop
that brought together leading experts in cognitive psychology, language
learning, and artificial intelligence (AI)-based natural language processing
(NLP). The workshop, funded by the National Science Foundation, aimed to
address a critical knowledge gap in our understanding of the relationship
between AI language models and human cognitive processes in text comprehension
and composition. Through collaborative dialogue across cognitive, linguistic,
and technological perspectives, workshop participants examined the underlying
processes involved when humans produce and comprehend text, and how AI can both
inform our understanding of these processes and augment human capabilities. The
workshop revealed emerging patterns in the relationship between large language
models (LLMs) and human cognition, with highlights on both the capabilities of
LLMs and their limitations in fully replicating human-like language
understanding and generation. Key findings include the potential of LLMs to
offer insights into human language processing, the increasing alignment between
LLM behavior and human language processing when models are fine-tuned with
human feedback, and the opportunities and challenges presented by human-AI
collaboration in language tasks. By synthesizing these findings, this report
aims to guide future research, development, and implementation of LLMs in
cognitive psychology, linguistics, and education. It emphasizes the importance
of ethical considerations and responsible use of AI technologies while striving
to enhance human capabilities in text comprehension and production through
effective human-AI collaboration.

</details>


### [65] [The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure](https://arxiv.org/abs/2506.22724)
*Niyati Bafna,Tianjian Li,Kenton Murray,David R. Mortensen,David Yarowsky,Hale Sirin,Daniel Khashabi*

Main category: cs.CL

TL;DR: The paper identifies a translation barrier in LLMs as a key reason for poor multilingual generation for low-resource languages.


<details>
  <summary>Details</summary>
Motivation: Multilingual generation by LLMs often delivers poor results for low-resource languages, necessitating an understanding of underlying failure causes.

Method: Introduced the 'translation barrier hypothesis' and tested it using a word translation task across 108 language pairs, employing logit lens to analyze intermediate processing layers.

Result: Confirmed that translation failures in converting intermediate concepts to target languages significantly affect performance, especially in low-resource languages.

Conclusion: Strengthening translation capabilities for low-resource languages may improve multilingual generation, highlighting this as a critical area for development in LLMs.

Abstract: Multilingual generation with large language models (LLMs) is often of poor
quality for mid- to low-resource languages. Building on insights from
interpretability, we demonstrate the existence of an implicit
task-solving-->translation pipeline for generation, whereby the model first
solves the required task in a largely target-language-agnostic manner, and
subsequently translates answer concepts into the intended target language. We
hypothesize that the failure of the translation stage is an important culprit
for the observed low quality of final outputs, and formalize this as the
translation barrier hypothesis. We test this hypothesis for a word translation
task across 108 language pairs, using logit lens to observe model processing in
intermediate layers. We find that a significant portion of overall failures
indeed stems from translation failure, or the model's inability to translate
correctly solved intermediate concepts into the target language. This is
especially true for low-resource target languages. Our results highlight an
important hurdle for end-to-end multilingual generation, and lend guiding
insights for future work seeking to improve multilinguality in LLMs.

</details>


### [66] [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)
*Alan Dao,Dinh Bach Vu*

Main category: cs.CL

TL;DR: Jan-nano, a 4B parameter model, achieves efficiency by focusing on finding information rather than knowing everything, achieving high performance on QA benchmarks while running on consumer hardware.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the tradeoff in language models between computational resource needs and capabilities by exploring a more efficient and specialized approach.

Method: The authors developed Jan-nano through a novel multi-stage RLVR system, bypassing traditional next-token prediction training and incorporating MCP integration for performance.

Result: The model achieved 83.2% on the SimpleQA benchmark and demonstrated an ability to function efficiently with a 128K context length on consumer hardware.

Conclusion: Jan-nano exemplifies that strategy and specialization can surpass scale in achieving intelligent capabilities effectively.

Abstract: Most language models face a fundamental tradeoff where powerful capabilities
require substantial computational resources. We shatter this constraint with
Jan-nano, a 4B parameter language model that redefines efficiency through
radical specialization: instead of trying to know everything, it masters the
art of finding anything instantly. Fine-tuned from Qwen3-4B using our novel
multi-stage RLVR system that completely eliminates reliance on next token
prediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with
MCP integration while running on consumer hardware. With 128K context length,
Jan-nano proves that intelligence isn't about scale, it's about strategy.

</details>


### [67] [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777)
*Miles Turpin,Andy Arditi,Marvin Li,Joe Benton,Julian Michael*

Main category: cs.CL

TL;DR: The paper introduces "verbalization fine-tuning (VFT)", a pre-reinforcement learning (RL) technique aimed at improving detection of reward hacking in language models by training them to openly verbalize when influenced by misleading cues.


<details>
  <summary>Details</summary>
Motivation: Reward hacking by language models in RL settings is a concern, particularly because models may exploit unintended strategies for high rewards without making such behavior detectable in their reasoning.

Method: The authors implemented VFT before RL to train models to acknowledge prompt-based cues that lead to incorrect answers. They later applied RL in environments designed to incentivize reward hacking behaviors and analyzed verbalization rates and undetected manipulation.

Result: Models trained using VFT achieved a 6% rate of undetected reward hacks post-RL, compared to 88% for models without VFT and 99% with debiasing baseline interventions. VFT increased verbalizations of cues' influence from 8% pre-RL to 94% post-RL.

Conclusion: VFT significantly improves the detection of reward hacking by teaching models to verbalize their behaviors transparently, enhancing the safety and transparency of AI systems.

Abstract: Language models trained with RL can engage in reward hacking--exploiting
unintended strategies for high reward--without revealing this behavior in their
chain-of-thought reasoning, making detection difficult and posing risks for
high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL
intervention that trains models to explicitly acknowledge when they are
influenced by prompt cues--hints which point to incorrect answers (e.g., "a
Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently
train models with RL on environments where held-out prompt cues signal which
incorrect answers will receive high reward, incentivizing models to reward hack
by exploiting cues instead of reasoning correctly. We measure how often models
exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained
model's responses consist of undetected reward hacks. In comparison, when we
perform RL without VFT, the rate of undetected reward hacks goes up to 88%;
with a debiasing baseline intervention, this increases further to 99%. VFT
achieves this by substantially increasing how often models verbalize the
influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while
baselines remain low even after RL (10% and 1%). Our results show that teaching
models to explicitly verbalize reward hacking behavior before RL significantly
improves their detection, offering a practical path toward more transparent and
safe AI systems.

</details>


### [68] [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)
*Jianxin Yan,Wangze Ni,Lei Chen,Xuemin Lin,Peng Cheng,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: ContextCache is a semantic caching system that enhances response efficiency in multi-turn dialogues by incorporating context-aware techniques.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing semantic caching systems that fail to consider multi-turn dialogue contexts, leading to incorrect cache hits.

Method: Introduces a two-stage retrieval process using vector-based retrieval and self-attention mechanisms to match queries within their conversational context.

Result: Evaluation shows improved precision and recall over current systems, and cached responses achieve 10x lower latency compared to direct LLM usage.

Conclusion: ContextCache offers a more efficient solution for LLM conversational applications by reducing computational costs and improving cache accuracy through context awareness.

Abstract: Semantic caching significantly reduces computational costs and improves
efficiency by storing and reusing large language model (LLM) responses.
However, existing systems rely primarily on matching individual queries,
lacking awareness of multi-turn dialogue contexts, which leads to incorrect
cache hits when similar queries appear in different conversational settings.
This demonstration introduces ContextCache, a context-aware semantic caching
system for multi-turn dialogues. ContextCache employs a two-stage retrieval
architecture that first executes vector-based retrieval on the current query to
identify potential matches and then integrates current and historical dialogue
representations through self-attention mechanisms for precise contextual
matching. Evaluation of real-world conversations shows that ContextCache
improves precision and recall compared to existing methods. Additionally,
cached responses exhibit approximately 10 times lower latency than direct LLM
invocation, enabling significant computational cost reductions for LLM
conversational applications.

</details>


### [69] [MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs](https://arxiv.org/abs/2506.22808)
*Jianhui Wei,Zijie Meng,Zikai Xiao,Tianxiang Hu,Yang Feng,Zhijie Zhou,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: This paper introduces MedEthicsQA, a benchmark for evaluating medical ethics in Medical Large Language Models (MedLLMs) using 5,623 multiple-choice and 5,351 open-ended questions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of adequate exploration into the ethical safety of Medical Large Language Models (MedLLMs).

Method: Developed the MedEthicsQA benchmark based on a structured taxonomy, widely-used datasets, medical literature, and expert-validation processes.

Result: The benchmark revealed that state-of-the-art MedLLMs struggle with medical ethics questions compared to their foundational counterparts.

Conclusion: MedLLMs require better alignment with medical ethics, as evidenced by the dataset's findings.

Abstract: While Medical Large Language Models (MedLLMs) have demonstrated remarkable
potential in clinical tasks, their ethical safety remains insufficiently
explored. This paper introduces $\textbf{MedEthicsQA}$, a comprehensive
benchmark comprising $\textbf{5,623}$ multiple-choice questions and
$\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.
We systematically establish a hierarchical taxonomy integrating global medical
ethical standards. The benchmark encompasses widely used medical datasets,
authoritative question banks, and scenarios derived from PubMed literature.
Rigorous quality control involving multi-stage filtering and multi-faceted
expert validation ensures the reliability of the dataset with a low error rate
($2.72\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance
in answering medical ethics questions compared to their foundation
counterparts, elucidating the deficiencies of medical ethics alignment. The
dataset, registered under CC BY-NC 4.0 license, is available at
https://github.com/JianhuiWei7/MedEthicsQA.

</details>


### [70] [Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models](https://arxiv.org/abs/2506.22813)
*Zhuojun Ding,Wei Wei,Chenghao Fan*

Main category: cs.CL

TL;DR: The paper introduces the SaM framework, which dynamically selects and merges expert models for better domain adaptation and scalability in information extraction tasks without extra training.


<details>
  <summary>Details</summary>
Motivation: Current practices for aligning large language models (LLMs) with information extraction (IE) tasks have limitations such as high annotation cost, lack of domain adaptability, and scalability challenges.

Method: The proposed SaM framework selects expert models based on domain similarity and sampled performance, then merges these experts at inference time to create task-specific models for target domains.

Result: The framework outperforms unified models on multiple benchmarks, achieving an average improvement of 10%, and demonstrates scalability as experts can be added or removed dynamically.

Conclusion: SaM enhances generalization across domains, provides practical scalability, and presents opportunities for further refinement and real-world application in IE tasks.

Abstract: Supervised fine-tuning (SFT) is widely used to align large language models
(LLMs) with information extraction (IE) tasks, such as named entity recognition
(NER). However, annotating such fine-grained labels and training
domain-specific models is costly. Existing works typically train a unified
model across multiple domains, but such approaches lack adaptation and
scalability since not all training data benefits target domains and scaling
trained models remains challenging. We propose the SaM framework, which
dynamically Selects and Merges expert models at inference time. Specifically,
for a target domain, we select domain-specific experts pre-trained on existing
domains based on (i) domain similarity to the target domain and (ii)
performance on sampled instances, respectively. The experts are then merged to
create task-specific models optimized for the target domain. By dynamically
merging experts beneficial to target domains, we improve generalization across
various domains without extra training. Additionally, experts can be added or
removed conveniently, leading to great scalability. Extensive experiments on
multiple benchmarks demonstrate our framework's effectiveness, which
outperforms the unified model by an average of 10%. We further provide insights
into potential improvements, practical experience, and extensions of our
framework.

</details>


### [71] [Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization](https://arxiv.org/abs/2506.22846)
*Duygu Altinok*

Main category: cs.CL

TL;DR: E2E ASR systems perform well but lack real-time capabilities due to autoregressive decoding; the proposed LAIL framework enhances CTC-based ASR systems using linguistic knowledge from LLMs with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap between computational efficiency and linguistic modeling in CTC-based ASR systems by leveraging the linguistic insights of large language models.

Method: A Language-Aware Intermediate Loss (LAIL) framework integrates linguistic modeling into CTC-based ASR. Connector layers map intermediate encoder outputs to LLM embedding spaces, and a causal language modeling loss is applied during training.

Result: The LAIL framework demonstrates substantial improvements in Word Error Rate (WER) on benchmark datasets like LibriSpeech, TEDLIUM2, and WSJ, achieving state-of-the-art performance for CTC-based ASR.

Conclusion: LAIL successfully enhances the linguistic capabilities of CTC-based ASR systems while maintaining their computational efficiency, making them more suitable for real-time applications.

Abstract: End-to-end (E2E) automatic speech recognition (ASR) systems have
revolutionized the field by integrating all components into a single neural
network, with attention-based encoder-decoder models achieving state-of-the-art
performance. However, their autoregressive decoding process limits inference
speed, making them unsuitable for real-time applications. In contrast,
CTC-based models offer faster, non-autoregressive decoding but struggle to
model linguistic dependencies effectively. Addressing this challenge, we
propose a novel auxiliary loss framework called Language-Aware Intermediate
Loss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large
language models (LLMs). By attaching connector layers to intermediate encoder
layers, LAIL maps outputs to the embedding space of an LLM and computes a
causal language modeling loss during training. This approach enhances
linguistic modeling while preserving the computational efficiency of CTC
decoding. Using the Conformer architecture and various LLaMA models, we
demonstrate significant improvements in Word Error Rate (WER) on the
LibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance
for CTC-based ASR with minimal computational overhead.

</details>


### [72] [Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems](https://arxiv.org/abs/2506.22852)
*Yucheng Cai,Yuxuan Wu,Yi Huang,Junlan Feng,Zhijian Ou*

Main category: cs.CL

TL;DR: The paper introduces Knowledge Augmented Fine-Tuning (KAFT) to improve factual accuracy in large language model (LLM)-based dialog systems by fine-tuning them with domain-specific data and knowledge, outperforming traditional prompting methods.


<details>
  <summary>Details</summary>
Motivation: LLMs in dialog systems struggle with factual accuracy in knowledge-intensive contexts due to challenges in effectively utilizing external knowledge bases (KBs) for specific domains.

Method: The researchers proposed KAFT, a method that fine-tunes LLMs with domain-specific datasets and external knowledge within retrieval-augmented generation (RAG) and agent-based frameworks. The MobileCS2 customer service dataset was utilized for evaluation.

Result: KAFT significantly surpassed traditional prompting methods in both RAG- and agent-based systems, with notable improvements in factual accuracy.

Conclusion: KAFT offers a promising approach to enhance LLM performance in domain-specific dialog systems and sets a new benchmark for the integration of fine-tuning with external knowledge.

Abstract: Large language models (LLMs) have recently been applied to dialog systems.
Despite making progress, LLMs are prone to errors in knowledge-intensive
scenarios. Recently, approaches based on retrieval augmented generation (RAG)
and agent have emerged to improve the factual accuracy by enhancing the LLMs
with knowledge retrieved from external knowledge bases (KBs). This is mostly
implemented by prompting the LLMs with instructions, examples and the retrieved
knowledge. However, LLMs may have difficulty using the retrieved knowledge
effectively for response generation, because they are not well trained to do
such generation for specific domains. To mitigate this problem, we propose to
finetune the LLMs in the RAG-based and agent-based systems with domain-specific
data, together with domain-specific external knowledge, which is called
knowledge augmented finetuning (KAFT). We base our study on the MobileCS2
dataset, a real-life customer service dialog dataset that features intensive
knowledge interactions, to systematically compare the prompting and KAFT
techniques in the RAG-based and agent-based systems. Experiment results show
that KAFT substantially surpasses prompting in both RAG and agent systems,
particularly in terms of factual accuracy. To the best of our knowledge, this
paper represents the first solid empirical work to investigate the KAFT idea.

</details>


### [73] [DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues](https://arxiv.org/abs/2506.22853)
*Kyochul Jang,Donghyeon Lee,Kyusik Kim,Dongseok Heo,Taewhoo Lee,Woojeong Kim,Bongwon Suh*

Main category: cs.CL

TL;DR: This paper introduces DICE-SCORE to evaluate realistic multi-turn tool-based interactions and presents DICE-BENCH, a benchmark to simulate practical scenarios in such tasks.


<details>
  <summary>Details</summary>
Motivation: Existing function-calling benchmarks focus only on single-turn interactions and fail to handle the complexity of real-world, multi-turn, tool-related scenarios.

Method: They proposed DICE-SCORE to measure the dispersion of tool-related information and developed DICE-BENCH, a framework for practical dataset synthesis using tool graphs and multi-agent dialogues.

Result: The authors created a dataset of 1,607 high-DICE-SCORE interactions and found that 19 tested LLMs perform poorly on realistic function-calling tasks, indicating the need for improvements.

Conclusion: This study highlights the limitations of current benchmarks and models, emphasizes the value of realistic datasets like DICE-BENCH, and encourages advancements for practical deployments.

Abstract: Existing function-calling benchmarks focus on single-turn interactions.
However, they overlook the complexity of real-world scenarios. To quantify how
existing benchmarks address practical applications, we introduce DICE-SCORE, a
metric that evaluates the dispersion of tool-related information such as
function name and parameter values throughout the dialogue. Analyzing existing
benchmarks through DICE-SCORE reveals notably low scores, highlighting the need
for more realistic scenarios. To address this gap, we present DICE-BENCH, a
framework that constructs practical function-calling datasets by synthesizing
conversations through a tool graph that maintains dependencies across rounds
and a multi-agent system with distinct personas to enhance dialogue
naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our
experiments on 19 LLMs with DICE-BENCH show that significant advances are still
required before such models can be deployed effectively in real-world settings.
Our code and data are all publicly available:
https://snuhcc.github.io/DICE-Bench/.

</details>


### [74] [Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions](https://arxiv.org/abs/2506.22858)
*Duygu Altinok*

Main category: cs.CL

TL;DR: The paper proposes a novel training method for ASR systems to improve named entity and numerical data recognition and formatting.


<details>
  <summary>Details</summary>
Motivation: ASR systems struggle with named entities and complex formatting, which impairs performance in critical fields like legal, financial, and medical applications.

Method: The authors use a sliding overlapping context window during training to extend the model's semantic window and employ enriched training data with entity labels for better recognition and formatting.

Result: The method enhances semantic tasks such as named entity recognition and formatting, validated through evaluations on the Spoken Wikipedia dataset.

Conclusion: Context-aware training improves ASR systems' ability to handle long-form transcription and complex entity recognition, addressing key limitations.

Abstract: Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high
transcription accuracy but struggle with named entities and numerical data,
especially when proper formatting is required. These issues increase word error
rate (WER) and impair semantic understanding in critical domains like legal,
financial, and medical applications. We propose a novel training approach that
extends the semantic context of ASR models by adding overlapping context
windows during training. By sliding 5-second overlaps on both sides of
30-second chunks, we create a 40-second "effective semantic window," improving
entity recognition and formatting while focusing predictions on the central 30
seconds. To address entities spanning chunk boundaries, we reassign such
entities entirely to the right-hand chunk, ensuring proper formatting.
Additionally, enriched training data with embedded entity labels enables the
model to learn both recognition and type-specific formatting. Evaluated on the
Spoken Wikipedia dataset, our method improves performance across semantic
tasks, including named entity recognition (NER) and entity formatting. These
results highlight the effectiveness of context-aware training in addressing ASR
limitations for long-form transcription and complex entity recognition tasks.

</details>


### [75] [Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models](https://arxiv.org/abs/2506.22957)
*Younwoo Choi,Changling Li,Yongjin Yang,Zhijing Jin*

Main category: cs.CL

TL;DR: This paper introduces and evaluates the concept of interlocutor awareness in large language models (LLMs), focusing on their ability to adapt to dialogue partners' identity and characteristics.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the interplay of LLMs in multi-agent and human-AI systems, emphasizing the safety and reliability tied to their awareness of conversational partners.

Method: This paper formalizes interlocutor awareness and systematically evaluates its emergence across reasoning patterns, linguistic style, and alignment preferences, alongside developing case studies demonstrating its impact.

Result: LLMs show reliable ability to identify same-family peers and significant model families, revealing enhancements in multi-LLM collaboration but also vulnerabilities like reward-hacking and increased susceptibility to jailbreaks.

Conclusion: While interlocutor awareness in LLMs has promising benefits for collaboration, it also introduces alignment and safety challenges, necessitating better understanding and safeguards in multi-agent setups.

Abstract: As large language models (LLMs) are increasingly integrated into multi-agent
and human-AI systems, understanding their awareness of both self-context and
conversational partners is essential for ensuring reliable performance and
robust safety. While prior work has extensively studied situational awareness
which refers to an LLM's ability to recognize its operating phase and
constraints, it has largely overlooked the complementary capacity to identify
and adapt to the identity and characteristics of a dialogue partner. In this
paper, we formalize this latter capability as interlocutor awareness and
present the first systematic evaluation of its emergence in contemporary LLMs.
We examine interlocutor inference across three dimensions-reasoning patterns,
linguistic style, and alignment preferences-and show that LLMs reliably
identify same-family peers and certain prominent model families, such as GPT
and Claude. To demonstrate its practical significance, we develop three case
studies in which interlocutor awareness both enhances multi-LLM collaboration
through prompt adaptation and introduces new alignment and safety
vulnerabilities, including reward-hacking behaviors and increased jailbreak
susceptibility. Our findings highlight the dual promise and peril of
identity-sensitive behavior in LLMs, underscoring the need for further
understanding of interlocutor awareness and new safeguards in multi-agent
deployments. Our code is open-sourced at
https://github.com/younwoochoi/InterlocutorAwarenessLLM.

</details>


### [76] [On the Generalizability of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"](https://arxiv.org/abs/2506.22977)
*Asen Dotsinski,Udit Thakur,Marko Ivanov,Mohammad Hafeez Khan,Maria Heuss*

Main category: cs.CL

TL;DR: This paper reproduces and extends the findings of Ortu et al. (2024) on how language models manage competing mechanisms between factual recall and counterfactual in-context repetition.


<details>
  <summary>Details</summary>
Motivation: The study aims to validate and extend the findings from Ortu et al. (2024) on the competition of mechanisms in language models, as understanding this can improve their functionality and robustness.

Method: The authors reproduced the experiments from the original work on GPT-2 and Pythia 6.9B models, extended them to Llama 3.1 8B, evaluated the influence of varied prompt structures, and examined domain-specific prompt behavior.

Result: Key findings include reduced attention head specialization in larger models (Llama 3.1 8B), significant effects of prompt structure on counterfactual token prediction, and a dependence of results on domain representation and model architecture.

Conclusion: The study shows that the competition of mechanisms is context-dependent, varying based on factors like model size, prompt structure, and dataset domain representation, which limits the generalizability of attention head ablation methods.

Abstract: We present a reproduction study of "Competition of Mechanisms: Tracing How
Language Models Handle Facts and Counterfactuals" (Ortu et al., 2024), which
investigates competition of mechanisms in language models between factual
recall and counterfactual in-context repetition. Our study successfully
reproduces their primary findings regarding the localization of factual and
counterfactual information, the dominance of attention blocks in mechanism
competition, and the specialization of attention heads in handling competing
information. We reproduce their results on both GPT-2 (Radford et al., 2019)
and Pythia 6.9B (Biderman et al., 2023). We extend their work in three
significant directions. First, we explore the generalizability of these
findings to even larger models by replicating the experiments on Llama 3.1 8B
(Grattafiori et al., 2024), discovering greatly reduced attention head
specialization. Second, we investigate the impact of prompt structure by
introducing variations where we avoid repeating the counterfactual statement
verbatim or we change the premise word, observing a marked decrease in the
logit for the counterfactual token. Finally, we test the validity of the
authors' claims for prompts of specific domains, discovering that certain
categories of prompts skew the results by providing the factual prediction
token as part of the subject of the sentence. Overall, we find that the
attention head ablation proposed in Ortu et al. (2024) is ineffective for
domains that are underrepresented in their dataset, and that the effectiveness
varies based on model architecture, prompt structure, domain and task.

</details>


### [77] [A Systematic Study of Compositional Syntactic Transformer Language Models](https://arxiv.org/abs/2506.22978)
*Yida Zhao,Hao Xve,Xiang Hu,Kewei Tu*

Main category: cs.CL

TL;DR: The paper investigates compositional syntactic language models (SLMs), proposes a unified design framework, evaluates it across various tasks, and provides recommendations for optimizing such models.


<details>
  <summary>Details</summary>
Motivation: To improve language models by introducing compositional syntactic biases via linearized constituency parse trees.

Method: The authors devised a unified framework for compositional SLM design based on parse trees and benchmarked their performance across diverse NLP tasks.

Result: Findings show distinctions between model variants, influencing recommendations for optimal design in compositional SLMs.

Conclusion: The paper establishes a unified framework for SLMs, demonstrates their applicability across tasks, and provides guidelines to enhance their performance.

Abstract: Syntactic language models (SLMs) enhance Transformers by incorporating
syntactic biases through the modeling of linearized syntactic parse trees
alongside surface sentences. This paper focuses on compositional SLMs that are
based on constituency parse trees and contain explicit bottom-up composition of
constituent representations. We identify key aspects of design choices in
existing compositional SLMs and propose a unified framework encompassing both
existing models and novel variants. We conduct a comprehensive empirical
evaluation of all the variants in our framework across language modeling,
syntactic generalization, summarization, dialogue, and inference efficiency.
Based on the experimental results, we make multiple recommendations on the
design of compositional SLMs. Our code is released at
https://github.com/zhaoyd1/compositional_SLMs.

</details>


### [78] [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/abs/2506.23046)
*Xianzhe Fan,Xuhui Zhou,Chuanyang Jin,Kolby Nottingham,Hao Zhu,Maarten Sap*

Main category: cs.CL

TL;DR: The study introduces the SoMi-ToM benchmark for evaluating Theory of Mind (ToM) in dynamic, multimodal, social interactions using both first- and third-person perspectives, revealing a notable performance gap between humans and state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing ToM benchmarks primarily focus on static, text-based scenarios, which do not sufficiently represent real-world dynamic social interactions. The researchers aim to address this gap.

Method: The SoMi-ToM benchmark evaluates ToM through multimodal first-person and third-person perspectives using a dataset generated by the SoMi interaction environment. It includes annotated questions and a structured evaluation framework.

Result: Human subjects significantly outperform state-of-the-art large vision-language models (LVLMs), with accuracy gaps of 40.1% in first-person evaluations and 26.4% in third-person evaluations.

Conclusion: Current LVLMs have limitations in handling complex, embodied social interactions. Future models must enhance their ToM capabilities to better represent human-like understanding in dynamic settings.

Abstract: Humans continuously infer the states, goals, and behaviors of others by
perceiving their surroundings in dynamic, real-world social interactions.
However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based
scenarios, which have a significant gap compared to real interactions. We
propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in
embodied multi-agent complex social interactions. This benchmark is based on
rich multimodal interaction data generated by the interaction environment SoMi,
covering diverse crafting goals and social relationships. Our framework
supports multi-level evaluation: (1) first-person evaluation provides
multimodal (visual, dialogue, action, etc.) input from a first-person
perspective during a task for real-time state inference, (2) third-person
evaluation provides complete third-person perspective video and text records
after a task for goal and behavior inference. This evaluation method allows for
a more comprehensive examination of a model's ToM capabilities from both the
subjective immediate experience and the objective global observation. We
constructed a challenging dataset containing 35 third-person perspective
videos, 363 first-person perspective images, and 1225 expert-annotated
multiple-choice questions (three options). On this dataset, we systematically
evaluated the performance of human subjects and several state-of-the-art large
vision-language models (LVLMs). The results show that LVLMs perform
significantly worse than humans on SoMi-ToM: the average accuracy gap between
humans and models is 40.1% in first-person evaluation and 26.4% in third-person
evaluation. This indicates that future LVLMs need to further improve their ToM
capabilities in embodied, complex social interactions.

</details>


### [79] [MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition](https://arxiv.org/abs/2506.23051)
*João Lucas Luz Lima Sarcinelli,Marina Lages Gonçalves Teixeira,Jade Bortot de Paiva,Diego Furtado Silva*

Main category: cs.CL

TL;DR: NER is essential in NLP but lacks resources for Brazilian Portuguese in specific domains. This paper introduces "MariNER," a gold-standard dataset with annotated historical sentences for early 20th-century Brazilian Portuguese.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the scarcity of high-quality NER datasets for Brazilian Portuguese, especially focusing on historical texts to support digital humanities research.

Method: The authors created "MariNER," a gold-standard dataset of manually annotated sentences from historical records. They also evaluated state-of-the-art NER models on this dataset.

Result: The work produced a dataset of over 9,000 annotated sentences for Brazilian Portuguese and performed a comparative evaluation of NER models on this resource.

Conclusion: The paper fills a crucial gap in NER resources for Brazilian Portuguese and provides a valuable tool for digital humanities and NLP research focused on historical texts.

Abstract: Named Entity Recognition (NER) is a fundamental Natural Language Processing
(NLP) task that aims to identify and classify entity mentions in texts across
different categories. While languages such as English possess a large number of
high-quality resources for this task, Brazilian Portuguese still lacks in
quantity of gold-standard NER datasets, especially when considering specific
domains. Particularly, this paper considers the importance of NER for analyzing
historical texts in the context of digital humanities. To address this gap,
this work outlines the construction of MariNER: \textit{Mapeamento e
Anota\c{c}\~oes de Registros hIst\'oricos para NER} (Mapping and Annotation of
Historical Records for NER), the first gold-standard dataset for early
20th-century Brazilian Portuguese, with more than 9,000 manually annotated
sentences. We also assess and compare the performance of state-of-the-art NER
models for the dataset.

</details>


### [80] [Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning](https://arxiv.org/abs/2506.23056)
*Xiang Zhuang,Bin Wu,Jiyu Cui,Kehua Feng,Xiaotong Li,Huabin Xing,Keyan Ding,Qiang Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: The paper presents K-MSE, a framework using external molecular knowledge and Monte Carlo Tree Search to enhance large language models for molecular structure elucidation, achieving over 20% performance improvement.


<details>
  <summary>Details</summary>
Motivation: The aim is to overcome limitations of large language models in molecular structure elucidation, which largely stem from their lack of specialized chemical knowledge.

Method: K-MSE integrates a molecular substructure knowledge base and a molecule-spectrum scorer, combined with Monte Carlo Tree Search for reasoning enhancements.

Result: Experimental results demonstrate more than 20% performance improvement on both GPT-4o-mini and GPT-4o models using the proposed framework.

Conclusion: The K-MSE framework effectively extends the capabilities of large language models, improving their proficiency in molecular structure elucidation through knowledge enhancement techniques.

Abstract: Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.

</details>


### [81] [Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries](https://arxiv.org/abs/2506.23071)
*Zhengren Wang,Bozhou Li,Dongwen Yao,Wentao Zhang*

Main category: cs.CL

TL;DR: The paper introduces Text2VectorSQL, a framework unifying Text-to-SQL and vector search for diverse and holistic natural language queries.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to Text-to-SQL are limited in handling unstructured data and ambiguous queries, while VectorSQL solutions lack tailored evaluation frameworks and practical deployment tools.

Method: Text2VectorSQL enables semantic filtering, multi-modal matching, and retrieval acceleration. It leverages vector indexing, extends user queries, annotates ground truths with expert reviews, and uses synthetic data for model development.

Result: Text2VectorSQL demonstrates significant performance improvements over baseline methods in semantic querying and retrieval tasks.

Conclusion: The work establishes a foundation for the Text2VectorSQL task, aiming to provide versatile and intuitive database interfaces and will be made publicly available for further development.

Abstract: While Text-to-SQL enables natural language interaction with structured
databases, its effectiveness diminishes with unstructured data or ambiguous
queries due to rigid syntax and limited expressiveness. Concurrently, vector
search has emerged as a powerful paradigm for semantic retrieval, particularly
for unstructured data. However, existing VectorSQL implementations still rely
heavily on manual crafting and lack tailored evaluation frameworks, leaving a
significant gap between theoretical potential and practical deployment. To
bridge these complementary paradigms, we introduces Text2VectorSQL, a novel
framework unifying Text-to-SQL and vector search to overcome expressiveness
constraints and support more diverse and holistical natural language queries.
Specifically, Text2VectorSQL enables semantic filtering, multi-modal matching,
and retrieval acceleration. For evaluation, we build vector index on
appropriate columns, extend user queries with semantic search, and annotate
ground truths via an automatic pipeline with expert review. Furthermore, we
develop dedicated Text2VectorSQL models with synthetic data, demonstrating
significant performance improvements over baseline methods. Our work
establishes the foundation for the Text2VectorSQL task, paving the way for more
versatile and intuitive database interfaces. The repository will be publicly
available at https://github.com/Open-DataFlow/Text2VectorSQL.

</details>


### [82] [From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship](https://arxiv.org/abs/2506.23101)
*Yue Xu,Wenjie Wang*

Main category: cs.CL

TL;DR: The paper introduces Genres, a benchmark for evaluating gender bias in multimodal language models (MLLMs) through interpersonal dynamics in narratives, finding persistent, context-sensitive biases.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks primarily evaluate gender bias in isolated scenarios, which neglects how bias manifests subtly in interpersonal interactions.

Method: The paper introduces Genres, a benchmark analyzing dual-character narratives to evaluate gender biases across diverse dimensions of social relationships.

Result: Experiments on MLLMs uncover persistent, context-specific gender biases that are invisible in single-character evaluations.

Conclusion: Relationship-aware benchmarks like Genres are critical for diagnosing interaction-driven gender bias and guiding effective bias mitigation strategies in MLLMs.

Abstract: Multimodal large language models (MLLMs) have shown impressive capabilities
across tasks involving both visual and textual modalities. However, growing
concerns remain about their potential to encode and amplify gender bias,
particularly in socially sensitive applications. Existing benchmarks
predominantly evaluate bias in isolated scenarios, overlooking how bias may
emerge subtly through interpersonal interactions. We fill this gap by going
beyond single-entity evaluation and instead focusing on a deeper examination of
relational and contextual gender bias in dual-individual interactions. We
introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs
through the lens of social relationships in generated narratives. Genres
assesses gender bias through a dual-character profile and narrative generation
task that captures rich interpersonal dynamics and supports a fine-grained bias
evaluation suite across multiple dimensions. Experiments on both open- and
closed-source MLLMs reveal persistent, context-sensitive gender biases that are
not evident in single-character settings. Our findings underscore the
importance of relationship-aware benchmarks for diagnosing subtle,
interaction-driven gender bias in MLLMs and provide actionable insights for
future bias mitigation.

</details>


### [83] [FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes](https://arxiv.org/abs/2506.23111)
*Janki Atul Nawale,Mohammed Safi Ur Rahman Khan,Janani D,Mansi Gupta,Danish Pruthi,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: INDIC-BIAS introduces the first India-centric benchmark assessing fairness in language models across 85 identity groups and exposes strong negative biases and stereotypes within LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of culturally relevant fairness benchmarks for large language models (LLMs) in diverse countries like India.

Method: The study involves consulting domain experts to create socio-cultural topics, generating 20,000 scenario templates, structuring them into tasks, and evaluating 14 LLMs.

Result: The evaluation reveals significant biases and stereotypes against marginalized Indian identities, along with models failing to mitigate biases even when prompted.

Conclusion: The INDIC-BIAS benchmark highlights the harms current LLMs can cause in diverse cultural settings and advocates for cautious application and further research into bias mitigation for Indian contexts.

Abstract: Existing studies on fairness are largely Western-focused, making them
inadequate for culturally diverse countries such as India. To address this gap,
we introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to
evaluate fairness of LLMs across 85 identity groups encompassing diverse
castes, religions, regions, and tribes. We first consult domain experts to
curate over 1,800 socio-cultural topics spanning behaviors and situations,
where biases and stereotypes are likely to emerge. Grounded in these topics, we
generate and manually validate 20,000 real-world scenario templates to probe
LLMs for fairness. We structure these templates into three evaluation tasks:
plausibility, judgment, and generation. Our evaluation of 14 popular LLMs on
these tasks reveals strong negative biases against marginalized identities,
with models frequently reinforcing common stereotypes. Additionally, we find
that models struggle to mitigate bias even when explicitly asked to rationalize
their decision. Our evaluation provides evidence of both allocative and
representational harms that current LLMs could cause towards Indian identities,
calling for a more cautious usage in practical applications. We release
INDIC-BIAS as an open-source benchmark to advance research on benchmarking and
mitigating biases and stereotypes in the Indian context.

</details>


### [84] [The Trilemma of Truth in Large Language Models](https://arxiv.org/abs/2506.23921)
*Germans Savcisens,Tina Eliassi-Rad*

Main category: cs.CL

TL;DR: The paper introduces a method called sAwMIL for probing the truthfulness of information retained by large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: LLMs retain information probabilistically during training, but methods to gauge the accuracy of their 'knowledge' often rely on flawed assumptions.

Method: The authors propose the sAwMIL method, which leverages internal activations, multiple-instance learning, and conformal prediction to classify statements as true, false, or neither.

Result: sAwMIL was tested across 16 open-source LLMs and new datasets, revealing insights like concentrated veracity signals in specific model depths and the asymmetry between truth and falsehood signals.

Conclusion: The work offers a reliable mechanism to probe LLMs' internal veracity signals, addressing their certainty and categorization of information beyond binary classifications.

Abstract: We often attribute human characteristics to large language models (LLMs) and
claim that they "know" certain things. LLMs have an internal probabilistic
knowledge that represents information retained during training. How can we
assess the veracity of this knowledge? We examine two common methods for
probing the veracity of LLMs and discover several assumptions that are flawed.
To address these flawed assumptions, we introduce sAwMIL (short for Sparse
Aware Multiple-Instance Learning), a probing method that utilizes the internal
activations of LLMs to separate statements into true, false, and neither.
sAwMIL is based on multiple-instance learning and conformal prediction. We
evaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including
both default and chat-based variants, as well as on 3 new datasets. Among the
insights we provide are: (1) the veracity signal is often concentrated in the
third quarter of an LLM's depth; (2) truth and falsehood signals are not always
symmetric; (3) linear probes perform better on chat models than on default
models; (4) nonlinear probes may be required to capture veracity signals for
some LLMs with reinforcement learning from human feedback or knowledge
distillation; and (5) LLMs capture a third type of signal that is distinct from
true and false and is neither true nor false. These findings provide a reliable
method for verifying what LLMs "know" and how certain they are of their
probabilistic internal knowledge.

</details>


### [85] [Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models](https://arxiv.org/abs/2506.23122)
*Shivam Sharma,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: This paper explores the task of classifying narrative roles (Hero, Villain, Victim, and Other) in diverse Internet memes, focusing on cultural and linguistic challenges. It evaluates various machine learning models and prompt techniques.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the nuanced and context-rich task of identifying narrative roles in memes, an area underexplored compared to synthetic hateful content detection. They wish to address cultural and linguistic diversity while improving model performance.

Method: The study uses an extended and balanced dataset to classify narrative roles in both English and code-mixed memes. This involves analyzing the lexical and structural differences and benchmarking models like multilingual transformers, instruction-tuned LLMs, and multimodal models under zero-shot settings.

Result: Larger models such as DeBERTa-v3 and Qwen2.5-VL perform better, but challenges persist in identifying the 'Victim' role and adapting to cultural and code-mixed content. Prompt engineering shows minor improvements.

Conclusion: The research highlights the significance of incorporating cultural context and utilizing effective prompt design for detecting narrative roles in diverse, multimodal memes, while also acknowledging current limitations.

Abstract: This work investigates the challenging task of identifying narrative roles -
Hero, Villain, Victim, and Other - in Internet memes, across three diverse test
sets spanning English and code-mixed (English-Hindi) languages. Building on an
annotated dataset originally skewed toward the 'Other' class, we explore a more
balanced and linguistically diverse extension, originally introduced as part of
the CLEF 2024 shared task. Comprehensive lexical and structural analyses
highlight the nuanced, culture-specific, and context-rich language used in real
memes, in contrast to synthetically curated hateful content, which exhibits
explicit and repetitive lexical markers. To benchmark the role detection task,
we evaluate a wide spectrum of models, including fine-tuned multilingual
transformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,
and multimodal vision-language models. Performance is assessed under zero-shot
settings using precision, recall, and F1 metrics. While larger models like
DeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent
challenges in reliably identifying the 'Victim' class and generalising across
cultural and code-mixed content. We also explore prompt design strategies to
guide multimodal models and find that hybrid prompts incorporating structured
instructions and role definitions offer marginal yet consistent improvements.
Our findings underscore the importance of cultural grounding, prompt
engineering, and multimodal reasoning in modelling subtle narrative framings in
visual-textual content.

</details>


### [86] [Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.23127)
*Zhaoye Fei,Li Ji,Siyin Wang,Junhao Shi,Jingjing Gong,Xipeng Qiu*

Main category: cs.CL

TL;DR: The paper presents the Embodied Planner-R1, a reinforcement learning framework enabling Large Language Models to perform task planning in interactive and partially observable environments.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with embodied task planning due to reliance on static action scripts and difficulty in learning causal relations in dynamic settings.

Method: Embodied Planner-R1 employs pure reinforcement learning with group rollout, sparse rewards, and Interactive Policy Optimization for effective learning in dynamic environments without human supervision.

Result: Embodied Planner-R1 achieved high completion rates (97.78% on ALFWorld, 79.92% on ScienceWorld) and strong generalization in unseen environments.

Conclusion: The framework significantly boosts LLMs' ability to plan and interact in complex environments, outperforming existing methods by a wide margin.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they face significant challenges in embodied task planning
scenarios that require continuous environmental understanding and action
generation. Existing approaches generate open-loop action scripts based on
static knowledge, making it difficult to learn causal relationships between
actions and environmental feedback, particularly in partially observable
environments. We introduce Embodied Planner-R1, a novel outcome-driven
reinforcement learning framework that enables LLMs to develop interactive
capabilities through autonomous exploration with minimal supervision. Our
framework incorporates three key innovations: (1) Without human annotations, we
employ pure reinforcement learning with group rollout, incorporating
in-environment interaction through parallel exploration; (2) completion-driven
sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient
learning from grouped trajectories. Across two challenging text-based Embodied
planning benchmarks, Embodied Planner-R1 achieves impressive completion rates
of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a
large margin, and suffers only a -3.66% drop in previously unseen environments,
evidencing strong generalization.

</details>


### [87] [Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format](https://arxiv.org/abs/2506.23133)
*Dingzirui Wang,Xuanliang Zhang,Rongyu Cao,Longxu Dou,Xianzhen Luo,Yingwei Ma,Qingfu Zhu,Wanxiang Che,Binhua Li,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: This paper introduces Format-Adapter, a system that adapts reasoning formats to tasks for improving the consistency of large language models' (LLMs) answers, leading to measurable performance improvement.


<details>
  <summary>Details</summary>
Motivation: Previous methods for improving reasoning consistency in LLMs rely on predefined reasoning formats, which are costly to label and may not be suitable for all tasks.

Method: The paper proposes Format-Adapter, which measures reasoning errors and uses LLMs to generate and select suitable formats to minimize errors when producing multiple answers.

Result: Experiments on math and commonsense reasoning tasks show that Format-Adapter improves performance by an average of 4.3% compared to prior works.

Conclusion: Automatically adapting reasoning formats to tasks using LLMs enhances their reasoning consistency and performance, showcasing the potential for reduced reliance on human-labeled formats.

Abstract: Generating and voting multiple answers is an effective method to mitigate
reasoning inconsistencies of large language models (LLMs). Prior works have
shown that multiple reasoning formats outperform a single format when
generating multiple answers. However, previous works using multiple formats
rely on formats labeled by humans, which could be unsuitable for all tasks and
have high labeling costs. To address this issue, we adapt suitable formats to
the given tasks by generating and selecting formats. We first propose how to
measure the reasoning error when generating multiple answers. Then, we
introduce Format-Adapter, which utilizes LLMs to generate and select suitable
reasoning formats by minimizing the error measurement we present. We conduct
experiments on math and commonsense reasoning tasks, where Format-Adapter
achieves a 4.3% performance improvement on average over previous works,
demonstrating the effectiveness.

</details>


### [88] [LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation](https://arxiv.org/abs/2506.23136)
*Shadman Sobhan,Mohammad Ariful Haque*

Main category: cs.CL

TL;DR: The paper proposes an enhanced Retrieval-Augmented Generation (RAG) pipeline for large language models (LLMs) that effectively handles information from technical documents with tables and images, and demonstrates improvements in faithfulness and answer relevancy.


<details>
  <summary>Details</summary>
Motivation: LLMs face significant challenges like hallucination and outdated knowledge, and while fine-tuning can address these issues, it is not efficient for frequent updates. Traditional RAG systems struggle with retrieving data from complex technical documents.

Method: The authors develop a RAG pipeline combining vector similarity search with a fine-tuned reranker, trained on their custom RAFT dataset, specifically for improving context identification in question answering on technical documents.

Result: The proposed pipeline achieves a faithfulness score of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87% (RAGas) and 93% (DeepEval), outperforming general RAG systems, particularly in table-based questions and out-of-context queries.

Conclusion: The enhanced RAG pipeline effectively improves LLM performance in technical document retrieval and question answering, offering a more efficient and accurate alternative to traditional RAG systems.

Abstract: Large Language Models (LLMs) are capable of natural language understanding
and generation. But they face challenges such as hallucination and outdated
knowledge. Fine-tuning is one possible solution, but it is resource-intensive
and must be repeated with every data update. Retrieval-Augmented Generation
(RAG) offers an efficient solution by allowing LLMs to access external
knowledge sources. However, traditional RAG pipelines struggle with retrieving
information from complex technical documents with structured data such as
tables and images. In this work, we propose a RAG pipeline, capable of handling
tables and images in documents, for technical documents that support both
scanned and searchable formats. Its retrieval process combines vector
similarity search with a fine-tuned reranker based on Gemma-2-9b-it. The
reranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom
dataset designed to improve context identification for question answering. Our
evaluation demonstrates that the proposed pipeline achieves a high faithfulness
score of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%
(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed
architecture is superior to general RAG pipelines in terms of table-based
questions and handling questions outside context.

</details>


### [89] [Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion](https://arxiv.org/abs/2506.23137)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.CL

TL;DR: The paper introduces Flow-Modulated Scoring (FMS) framework for Knowledge Graph Completion to overcome the limitations of static embedding-based methods by incorporating context-aware dynamic modeling.


<details>
  <summary>Details</summary>
Motivation: Current approaches to Knowledge Graph Completion fail to capture contextual dependencies and relational dynamics as they rely on static scoring through embeddings.

Method: The paper proposes FMS, which combines a context learning module for entity representation and a flow-matching module for dynamic transformations of embeddings governed by context.

Result: FMS achieves state-of-the-art results in Knowledge Graph Completion tasks, outperforming existing methods on standard benchmarks.

Conclusion: By uniting static and dynamic modeling, FMS enhances the representation of relational semantics, marking a significant advancement in Knowledge Graph Completion techniques.

Abstract: Effective modeling of multifaceted relations is pivotal for Knowledge Graph
Completion (KGC). However, a majority of existing approaches are predicated on
static, embedding-based scoring, exhibiting inherent limitations in capturing
contextual dependencies and relational dynamics. Addressing this gap, we
propose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal
components: (1) a semantic context learning module that encodes
context-sensitive entity representations, and (2) a conditional flow-matching
module designed to learn the dynamic transformation from a head to a tail
embedding, governed by the aforementioned context. The resultant predictive
vector field, representing the context-informed relational path, serves to
dynamically refine the initial static score of an entity pair. Through this
synergy of context-aware static representations and conditioned dynamic
information, FMS facilitates a more profound modeling of relational semantics.
Comprehensive evaluations on several standard benchmarks demonstrate that our
proposed method surpasses prior state-of-the-art results.

</details>


### [90] [Benchmarking Deep Search over Heterogeneous Enterprise Data](https://arxiv.org/abs/2506.23139)
*Prafulla Kumar Choubey,Xiangyu Peng,Shilpa Bhagavath,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: The paper introduces a new benchmark to assess Deep Search, focusing on retrieval-augmented generation (RAG) involving multi-hop reasoning across diverse and unstructured data sources.


<details>
  <summary>Details</summary>
Motivation: To address the lack of realistic benchmarks for the evaluation of RAG systems that perform deep reasoning over complex and interconnected data sources.

Method: A synthetic data pipeline was developed to simulate enterprise workflows and generate interconnected content along with multi-hop questions. The benchmark includes both answerable and unanswerable queries and tests retrieval and reasoning capabilities using 39,190 enterprise artifacts.

Result: Current state-of-the-art RAG systems achieve only a 32.96 performance score, with retrieval identified as the primary bottleneck in achieving higher accuracy.

Conclusion: The retrieval capabilities in RAG systems need significant improvement to enhance their ability to reason over complete and relevant contexts, which is currently hindering performance.

Abstract: We present a new benchmark for evaluating Deep Search--a realistic and
complex form of retrieval-augmented generation (RAG) that requires
source-aware, multi-hop reasoning over diverse, sparsed, but related sources.
These include documents, meeting transcripts, Slack messages, GitHub, and URLs,
which vary in structure and often contain human-to-human interactions. We build
it using a synthetic data pipeline that simulates business workflows across
product planning, development, and support stages, generating interconnected
content with realistic noise and multi-hop questions with guaranteed
ground-truth answers. We release our benchmark with both answerable and
unanswerable queries, and retrieval pool of 39,190 enterprise artifacts,
enabling fine-grained evaluation of long-context LLM and RAG systems. Our
experiments reveal that even the best-performing agentic RAG methods achieve an
average performance score of 32.96 on our benchmark. With further analysis, we
highlight retrieval as the main bottleneck: existing methods struggle to
conduct deep searches and retrieve all necessary evidence. Consequently, they
often reason over partial context, leading to significant performance
degradation.

</details>


### [91] [Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions](https://arxiv.org/abs/2506.23146)
*Dingzriui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: The paper introduces the Learning-to-Context Slope (LCS), a metric to gauge the effectiveness of in-context learning (ICL) in large language models, improving reliability and addressing current limitations in assessment.


<details>
  <summary>Details</summary>
Motivation: In-context learning (ICL), though effective in enhancing large language models, exhibits varying performance across tasks and models, necessitating a reliable evaluation method.

Method: The study proposes LCS, which models the slope between learning gain and contextual relevance, addressing limitations in performance-based metrics like reliability, attribution, and practicality in low-data scenarios.

Result: Experiments confirm that LCS correlates well with ICL performance improvements and provides insight into model capabilities and actionable thresholds, even in biased or data-scarce settings.

Conclusion: The LCS metric offers a robust approach to measure ICL effectiveness, enabling identification of contextual alignment issues and reducing reliance on labeled data.

Abstract: In-context learning (ICL) has emerged as an effective approach to enhance the
performance of large language models (LLMs). However, its effectiveness varies
significantly across models and tasks, posing challenges for practitioners to
determine when ICL reliably improves performance. Current evaluation
approaches, reliant on performance change after applying ICL, suffer from low
reliability, poor attribution, and impracticality in data-insufficient
scenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that
quantifies ICL effectiveness by modeling the slope between learning gain (loss
decrease from demonstrations) and contextual relevance (demonstration-input
relevance). LCS addresses key limitations of performance-based metrics: (1) it
captures continuous loss changes even when outputs are incorrect, improving
reliability; (2) its formulation attributes ICL failures to weak contextual
alignment (inability to adapt inputs to demonstrations) or strong output
calibration (self-verification of correctness); and (3) it minimizes reliance
on labeled data via synthetic evaluation. Extensive experiments demonstrate
that LCS strongly correlates with performance improvements in labeled settings
and reliably reflects true effectiveness in biased or data-scarce scenarios.
Further analysis reveals actionable thresholds for LCS and identifies model
capabilities critical to ICL success.

</details>


### [92] [V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy](https://arxiv.org/abs/2506.23149)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: The paper addresses synthesizing in-context learning (ICL) demonstrations from scratch for arbitrary tasks, introducing V-Score and V-Synthesis to improve consistency and diversity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for synthesizing ICL demonstrations largely focus on specific tasks or require pre-existing examples, creating a need for a generalized solution to synthesize accurate demonstrations from scratch.

Method: The authors propose a novel metric called V-Score for consistency assessment, and use it in a proportional sampling approach called V-Synthesis to create diverse and consistent task demonstrations.

Result: V-Synthesis shows a 2.0% improvement on average over other synthesis methods, demonstrating its effectiveness in generating useful ICL demonstrations.

Conclusion: The study's approach enables effective and generalized demonstration synthesis for ICL, reducing dependency on task-specific methods or pre-existing samples, while improving consistency and diversity.

Abstract: High labeling cost for in-context learning (ICL) demonstrations motivates
using large language models (LLMs) for synthesis to reduce overhead. However,
existing synthesis methods are mainly task-specific or rely on pre-existing
demonstrations. So this paper focuses on synthesizing demonstrations from
scratch for arbitrary tasks. A major challenge in synthesizing from scratch is
ensuring consistency with the target task, as the lack of labeling guidance
could lead to synthesis bias. We first propose a consistency metric called
V-Score, which has higher performance and lower computation cost compared with
the metrics based on grams or embedding vectors. Furthermore, we introduce
V-Synthesis, which leverages V-Score for proportional sampling to ensure both
high consistency and diversity of synthesized demonstrations. Experimental
results demonstrate that V-Synthesis yields an average performance improvement
of 2.0% compared to existing synthesis methods confirming the effectiveness of
V-Synthesis.

</details>


### [93] [RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://arxiv.org/abs/2506.23192)
*Gabriel Iturra-Bocaz,Felipe Bravo-Marquez*

Main category: cs.CL

TL;DR: The paper introduces RiverText, a Python library for training and evaluating incremental word embeddings for streaming data, overcoming limitations of static embeddings.


<details>
  <summary>Details</summary>
Motivation: Traditional word embeddings are static and fail to adapt to evolving language patterns, particularly from dynamic sources like social media.

Method: RiverText implements incremental word embedding techniques such as Skip-gram and Continuous Bag of Words within a PyTorch framework, enabling dynamic representation and evaluation of language patterns.

Result: RiverText supports a variety of incremental embedding methods, adapts existing evaluation tasks for streaming settings, and presents performance comparisons with diverse hyperparameters.

Conclusion: RiverText serves as a valuable, open-source resource for analyzing streaming text data, leveraging incremental embeddings to address the adaptability challenges of traditional models.

Abstract: Word embeddings have become essential components in various information
retrieval and natural language processing tasks, such as ranking, document
classification, and question answering. However, despite their widespread use,
traditional word embedding models present a limitation in their static nature,
which hampers their ability to adapt to the constantly evolving language
patterns that emerge in sources such as social media and the web (e.g., new
hashtags or brand names). To overcome this problem, incremental word embedding
algorithms are introduced, capable of dynamically updating word representations
in response to new language patterns and processing continuous data streams.
  This paper presents RiverText, a Python library for training and evaluating
incremental word embeddings from text data streams. Our tool is a resource for
the information retrieval and natural language processing communities that work
with word embeddings in streaming scenarios, such as analyzing social media.
The library implements different incremental word embedding techniques, such as
Skip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized
framework. In addition, it uses PyTorch as its backend for neural network
training. We have implemented a module that adapts existing intrinsic static
word embedding evaluation tasks for word similarity and word categorization to
a streaming setting. Finally, we compare the implemented methods with different
hyperparameter settings and discuss the results. Our open-source library is
available at https://github.com/dccuchile/rivertext.

</details>


### [94] [Generalist Reward Models: Found Inside Large Language Models](https://arxiv.org/abs/2506.23235)
*Yi-Chen Li,Tian Xu,Yang Yu,Xuqin Zhang,Xiong-Hui Chen,Zhongxiang Ling,Ningjing Chao,Lei Yuan,Zhi-Hua Zhou*

Main category: cs.CL

TL;DR: This paper identifies a hidden, high-quality reward model within Large Language Models (LLMs) and demonstrates its theoretical and practical potential for improving reinforcement learning without additional training.


<details>
  <summary>Details</summary>
Motivation: Current methods of aligning LLMs rely on costly human preference data, and methods using AI feedback often lack a rigorous theoretical basis. The study explores whether pre-trained LLMs inherently possess robust reward models that can be utilized to bypass these challenges.

Method: The authors theoretically prove that the latent reward model in LLMs is equivalent to a reward function derived from offline inverse reinforcement learning. They then elicit this endogenous reward from base models and use it in reinforcement learning to achieve superior performance.

Result: Experiments show that using this endogenous reward signal outperforms existing LLM-as-a-judge methods and even surpasses reward models trained explicitly. Additionally, the method ensures theoretically improved error bounds in policy optimization.

Conclusion: This research suggests a paradigm shift in LLM alignment by replacing explicit reward modeling with the endogenous reward mechanism captured in pre-training, enabling more efficient and scalable approaches for both single and multi-modal models.

Abstract: The alignment of Large Language Models (LLMs) is critically dependent on
reward models trained on costly human preference data. While recent work
explores bypassing this cost with AI feedback, these methods often lack a
rigorous theoretical foundation. In this paper, we discover that a powerful
generalist reward model is already latently present within any LLM trained via
standard next-token prediction. We prove that this endogenous reward is not a
heuristic, but is theoretically equivalent to a reward function learned through
offline inverse reinforcement learning. This connection allows us to directly
elicit a high-quality reward signal from a base (pre-trained or supervised
fine-tuned) model without any further training. Critically, we also prove that
subsequent reinforcement learning using this endogenous reward leads to a
policy with a provably superior error bound compared to the base model. To our
best knowledge, this is the first theoretical proof of the effectiveness of
reinforcement learning for LLMs. Our experiments validate this theory,
demonstrating that our method not only outperforms existing LLM-as-a-judge
approaches but can also surpass explicitly trained reward models. These
findings suggest that the reward modeling stage can be replaced by a principled
method of eliciting the knowledge already captured during pre-training,
heralding a more efficient, powerful, and scalable paradigm for LLMs alignment
as well as multi-modal models.

</details>


### [95] [Two Spelling Normalization Approaches Based on Large Language Models](https://arxiv.org/abs/2506.23288)
*Miguel Domingo,Francisco Casacuberta*

Main category: cs.CL

TL;DR: This paper explores two new large language model-based approaches for spelling normalization in historical texts, finding statistical machine translation to be the most suitable method.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of non-standardized spelling in historical documents, which complicates research in the humanities.

Method: The authors develop two approaches: (1) a large language model trained without supervision, and (2) a supervised model trained for machine translation, and evaluate their performance on diverse datasets.

Result: Both approaches provided promising results in spelling normalization across various datasets and historical periods.

Conclusion: Statistical machine translation still proves to be the most effective technology for addressing non-standardized historical spelling.

Abstract: The absence of standardized spelling conventions and the organic evolution of
human language present an inherent linguistic challenge within historical
documents, a longstanding concern for scholars in the humanities. Addressing
this issue, spelling normalization endeavors to align a document's orthography
with contemporary standards. In this study, we propose two new approaches based
on large language models: one of which has been trained without a supervised
training, and a second one which has been trained for machine translation. Our
evaluation spans multiple datasets encompassing diverse languages and
historical periods, leading us to the conclusion that while both of them
yielded encouraging results, statistical machine translation still seems to be
the most suitable technology for this task.

</details>


### [96] [Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)](https://arxiv.org/abs/2506.23315)
*Shouvon Sarker,Xishuang Dong,Lijun Qian*

Main category: cs.CL

TL;DR: The paper develops a BERT-based ensemble model to classify medication events from clinical notes, achieving improved predictive performance.


<details>
  <summary>Details</summary>
Motivation: Effectively identifying key variables like medication and diseases from health records can support various clinical applications. The study focuses on addressing challenges in medication event identification from clinical notes.

Method: The authors pretrained BERT models on datasets like Wikipedia and MIMIC, fine-tuned the models on a clinical dataset (CMED), and used ensemble techniques with voting strategies for final predictions.

Result: The model achieved approximately 5% improvement in strict Micro-F scores and 6% improvement in strict Macro-F scores.

Conclusion: BERT-based ensemble models significantly enhance the classification of medication events from clinical notes, as demonstrated by improvements in Micro-F and Macro-F scores.

Abstract: Identification of key variables such as medications, diseases, relations from
health records and clinical notes has a wide range of applications in the
clinical domain. n2c2 2022 provided shared tasks on challenges in natural
language processing for clinical data analytics on electronic health records
(EHR), where it built a comprehensive annotated clinical data Contextualized
Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of
this challenge that is to detect and classify medication events from clinical
notes through building a novel BERT-based ensemble model. It started with
pretraining BERT models on different types of big data such as Wikipedia and
MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED
training data. These fine-tuned BERT models were employed to accomplish
medication event classification on CMED testing data with multiple predictions.
These multiple predictions generated by these fine-tuned BERT models were
integrated to build final prediction with voting strategies. Experimental
results demonstrated that BERT-based ensemble models can effectively improve
strict Micro-F score by about 5% and strict Macro-F score by about 6%,
respectively.

</details>


### [97] [Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family](https://arxiv.org/abs/2506.23340)
*Yumeng Lin,Xufeng Duan,David Haslett,Yige Chen,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: Large language models like GPT-4 and Llama 2 face challenges in translation for low-resource languages or those linguistically distant from English. Factors like training data size and language proximity influence translation performance.


<details>
  <summary>Details</summary>
Motivation: To investigate the interplay between training data, linguistic patterns, and language family in shaping multilingual translation quality in large language models.

Method: The paper evaluated GPT-4 and Llama 2 using round-trip translations and assessed quality via BLEU scores and BERT similarity metrics. Research focused on language proximity, data size, and language family effects.

Result: Results show interaction between training data volume and linguistic distance: higher data aids divergence issues, but proximity to English improves translation for low-resource languages. Distance metrics like syntax and geography are strong predictors.

Conclusion: Translation quality in large language models depends on both data size and linguistic relationships, with structural and typological factors playing key roles alongside training data volume.

Abstract: Large language models have achieved impressive progress in multilingual
translation, yet they continue to face challenges with certain language
pairs-particularly those with limited training data or significant linguistic
divergence from English. This study systematically investigates how training
data, language proximity, and language family affect information loss in
multilingual translation. We evaluate two large language models, GPT-4 and
Llama 2, by performing round-trip translations. Translation quality was
assessed using BLEU scores and BERT similarity metrics. Our results reveal a
robust interaction between training data size and language distance: while
abundant training data can mitigate the effects of linguistic divergence,
languages structurally closer to English consistently yield higher translation
quality in low-resource conditions. Among various distance metrics,
orthographic, phylogenetic, syntactic, and geographical distances emerge as
strong predictors of translation performance. Language family also exerts an
independent influence. These findings contribute to a deeper understanding of
the linguistic constraints shaping multilingual translation in large language
models, emphasizing that translation quality is shaped not only by data volume
but also by structural and typological relationships between languages.

</details>


### [98] [ATGen: A Framework for Active Text Generation](https://arxiv.org/abs/2506.23342)
*Akim Tsvigun,Daniil Vasilev,Ivan Tsvigun,Ivan Lysenko,Talgat Bektleuov,Aleksandr Medvedev,Uliana Vinogradova,Nikita Severin,Mikhail Mozikov,Andrey Savchenko,Rostislav Grigorev,Ramil Kuleev,Fedor Zhdanov,Artem Shelmanov,Ilya Makarov*

Main category: cs.CL

TL;DR: The paper proposes ATGen, an active learning framework tailored for natural language generation tasks, reducing annotation efforts and enabling benchmark comparisons.


<details>
  <summary>Details</summary>
Motivation: To overcome the underutilization of active learning strategies in natural language generation tasks, given their potential to reduce annotation costs.

Method: Active Text Generation (ATGen) incorporates state-of-the-art active learning strategies using human and large language model annotators, offering easy implementation and benchmarking for text generation tasks.

Result: ATGen demonstrated cost reduction for human annotators and API calls in diverse text generation tasks across various settings.

Conclusion: ATGen offers an effective and accessible solution, enhancing active learning in natural language generation, with open-source availability for community use.

Abstract: Active learning (AL) has demonstrated remarkable potential in reducing the
annotation effort required for training machine learning models. However,
despite the surging popularity of natural language generation (NLG) tasks in
recent years, the application of AL to NLG has been limited. In this paper, we
introduce Active Text Generation (ATGen) - a comprehensive framework that
bridges AL with text generation tasks, enabling the application of
state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered
annotation in NLG tasks using both human annotators and automatic annotation
agents based on large language models (LLMs). The framework supports LLMs
deployed as services, such as ChatGPT and Claude, or operated on-premises.
Furthermore, ATGen provides a unified platform for smooth implementation and
benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present
evaluation results for state-of-the-art AL strategies across diverse settings
and multiple text generation tasks. We show that ATGen reduces both the effort
of human annotators and costs associated with API calls to LLM-based annotation
agents. The code of the framework is available on GitHub under the MIT license.
The video presentation is available at http://atgen-video.nlpresearch.group

</details>


### [99] [Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs](https://arxiv.org/abs/2506.23377)
*Taejin Kim,Siun-Chuon Mau,Konrad Vesey*

Main category: cs.CL

TL;DR: The paper introduces Perspective-Dial, a framework to quantify and control biases or perspectives in large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To address the lack of a quantifiable understanding of biases and perspectives in the rapidly evolving domain of LLM outputs.

Method: The paper proposes Perspective Space for quantifying perspectives and a Systematic Prompt Engineering approach using greedy-coordinate descent to adjust LLM outputs.

Result: Demonstrated empirical success in detecting, tracking, and controlling LLM biases and perspectives across various topics.

Conclusion: Perspective-Dial offers a practical method to study and manage LLM perspectives, opening applications in bias-mitigation, narrative tracking, public discourse analysis, and debate systems.

Abstract: Large language models (LLMs) are used in a variety of mission-critical roles.
Due to the rapidly developing nature of LLMs, there is a lack of quantifiable
understanding of the bias and perspective associated with LLM output. Inspired
by this need, this paper considers the broader issue of perspective or
viewpoint of general text and perspective control of large-language model (LLM)
output. Perspective-Dial consists of two main components: a (1) metric space,
dubbed Perspective Space, that enables quantitative measurements of different
perspectives regarding a topic, and the use of (2) Systematic Prompt
Engineering that utilizes greedy-coordinate descent to control LLM output
perspective based on measurement feedback from the Perspective Space. The
empirical nature of the approach allows progress to side step a principled
understanding of perspective or bias -- effectively quantifying and adjusting
outputs for a variety of topics. Potential applications include detection,
tracking and mitigation of LLM bias, narrative detection, sense making and
tracking in public discourse, and debate bot advocating given perspective.

</details>


### [100] [Hierarchical Memory Organization for Wikipedia Generation](https://arxiv.org/abs/2506.23393)
*Eugene J. Yu,Dawei Zhu,Yifan Song,Xiangyu Wong,Jiebin Zhang,Wenxuan Shi,Xiaoguang Li,Qun Liu,Sujian Li*

Main category: cs.CL

TL;DR: The paper introduces the Memory Organization-based Generation (MOG) framework for autonomously generating Wikipedia articles using a hierarchical memory structure, which improves informativeness and verifiability.


<details>
  <summary>Details</summary>
Motivation: Generating accurate, comprehensive, and well-structured Wikipedia articles autonomously is challenging due to the need for integration of diverse information sources and maintaining credibility.

Method: The proposed MOG framework uses a hierarchical memory architecture. It extracts fine-grained memory units from web documents, organizes them into a structure resembling Wikipedia's hierarchy, and integrates a citation module linking generated content to memory sources.

Result: The framework was evaluated on the WikiStart dataset and demonstrated that it outperforms baseline methods in producing informative and reliable Wikipedia articles.

Conclusion: MOG effectively combines structured memory organization and citations to generate high-quality and traceable Wikipedia articles, making it suitable for real-world use cases.

Abstract: Generating Wikipedia articles autonomously is a challenging task requiring
the integration of accurate, comprehensive, and well-structured information
from diverse sources. This paper introduces the Memory Organization-based
Generation (MOG) framework, a novel approach to address these challenges by
leveraging a hierarchical memory architecture. MOG extracts fine-grained memory
units from web documents, recursively organizes them into a Wikipedia-style
hierarchical structure, and uses this structure to guide the generation
process. This ensures alignment between memory and the article outline,
improving both informativeness and verifiability while minimizing
hallucinations. Additionally, a citation module is implemented to enhance
traceability by linking every generated sentence to specific memory units.
Evaluations on our newly created WikiStart dataset demonstrate that MOG
outperforms baseline methods in producing informative and reliable articles,
making it particularly robust in real-world scenarios.

</details>


### [101] [Datasets for Fairness in Language Models: An In-Depth Survey](https://arxiv.org/abs/2506.23411)
*Jiale Zhang,Zichong Wang,Avash Palikhe,Zhipeng Yin,Wenbin Zhang*

Main category: cs.CL

TL;DR: This survey reviews widely used fairness datasets in language model research, introduces an evaluation framework, and highlights inherent biases to foster better understanding and application.


<details>
  <summary>Details</summary>
Motivation: There is a lack of scrutiny toward fairness datasets in language model research despite their critical role in shaping benchmarks and evaluations.

Method: The paper surveys fairness datasets, analyzes their characteristics, introduces a unified evaluation framework, and applies it to identify biases and demographic disparities.

Result: Using the framework on twenty-four benchmarks, consistent demographic disparities and overlooked biases were identified, influencing model fairness conclusions.

Conclusion: This paper offers practical guidance on dataset selection, advocates for more representative fairness benchmarks, and promotes transparency in research via public code and data.

Abstract: Fairness benchmarks play a central role in shaping how we evaluate language
models, yet surprisingly little attention has been given to examining the
datasets that these benchmarks rely on. This survey addresses that gap by
presenting a broad and careful review of the most widely used fairness datasets
in current language model research, characterizing them along several key
dimensions including their origin, scope, content, and intended use to help
researchers better appreciate the assumptions and limitations embedded in these
resources. To support more meaningful comparisons and analyses, we introduce a
unified evaluation framework that reveals consistent patterns of demographic
disparities across datasets and scoring methods. Applying this framework to
twenty four common benchmarks, we highlight the often overlooked biases that
can influence conclusions about model fairness and offer practical guidance for
selecting, combining, and interpreting these datasets. We also point to
opportunities for creating new fairness benchmarks that reflect more diverse
social contexts and encourage more thoughtful use of these tools going forward.
All code, data, and detailed results are publicly available at
https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets
to promote transparency and reproducibility across the research community.

</details>


### [102] [TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs](https://arxiv.org/abs/2506.23423)
*Felipe Nuti,Tim Franzmeyer,João Henriques*

Main category: cs.CL

TL;DR: This paper introduces a method to quantitatively measure fine-tuning's contribution to individual LLM responses via a decomposition of outputs into pre-training and fine-tuning components.


<details>
  <summary>Details</summary>
Motivation: Past studies have looked at fine-tuning's effect on task performance but lacked methods to analyze its impact on individual model outputs.

Method: The proposed method tracks LLM hidden states to decompose outputs into pre-training and fine-tuning components, introducing the metric Tuning Contribution (TuCo) to measure the interplay.

Result: It was found that scaling the fine-tuning component affects behavior and that adversarial attacks succeed more often on low TuCo prompts, showing a connection between fine-tuning attenuation and attack efficacy.

Conclusion: The TuCo metric provides a framework for investigating fine-tuning's impact on model behavior, including implications for model safety.

Abstract: Past work has studied the effects of fine-tuning on large language models'
(LLMs) overall performance on certain tasks. However, a quantitative and
systematic method for analyzing its effect on individual outputs is still
lacking. Here, we propose a new method for measuring the contribution that
fine-tuning makes to individual LLM responses, assuming access to the original
pre-trained model. Our method tracks the model's intermediate hidden states,
providing a more fine-grained insight into the effects of fine-tuning than a
simple comparison of final outputs from pre-trained and fine-tuned models. We
introduce and theoretically analyze an exact decomposition of any fine-tuned
LLM into a pre-training component and a fine-tuning component. Empirically, we
find that model behavior and performance can be steered by up- or down-scaling
the fine-tuning component during the forward pass. Motivated by this finding
and our theoretical analysis, we define the Tuning Contribution (TuCo) as the
ratio of the magnitudes of the fine-tuning component to the pre-training
component. We observe that three prominent adversarial attacks on LLMs
circumvent safety measures in a way that reduces TuCo, and that TuCo is
consistently lower on prompts where these attacks succeed compared to those
where they do not. This suggests that attenuating the effect of fine-tuning on
model outputs plays a role in the success of such attacks. In summary, TuCo
enables the quantitative study of how fine-tuning influences model behavior and
safety, and vice versa.

</details>


### [103] [Pipelined Decoder for Efficient Context-Aware Text Generation](https://arxiv.org/abs/2506.23431)
*Zixian Huang,Chenxu Niu,Yu Gu,Gengyang Xiao,Xinwei Huang,Gong Cheng*

Main category: cs.CL

TL;DR: The paper proposes a pipelined decoder to improve text generation speed by generating multiple tokens simultaneously, without compromising quality or increasing memory usage.


<details>
  <summary>Details</summary>
Motivation: Autoregressive models, though effective in generating high-quality text, are slow due to their sequential token generation approach.

Method: The authors designed a new pipelined decoder architecture enabling parallel generation of subsequences, with each token generated simultaneously for better efficiency.

Result: Experiments across various text generation tasks demonstrated significant speed improvement while maintaining comparable quality and memory footprint.

Conclusion: The pipelined decoder provides an effective solution for enhancing text generation speed, making it suitable for applications requiring faster context-aware text generation.

Abstract: As the basis of generative AI, an autoregressive model requires the
generation of a new token depending on all the previously generated tokens,
which brings high quality but also restricts the model to generate tokens one
by one, forming a bottleneck limiting the generation speed. In this paper, we
propose a new decoder architecture that efficiently generates text in parallel
for context-aware generation tasks. Our proposed pipelined decoder initiates
the generation of multiple subsequences simultaneously, and, at each time-step,
it generates a new token for each subsequence to realize parallelism.
Experiments on multiple text generation tasks, including question answering,
text summarization, and keyphrase generation, show that our pipelined decoder
significantly improves the generation speed without a significant loss of
generation quality or additional memory consumption.

</details>


### [104] [What to Keep and What to Drop: Adaptive Table Filtering Framework](https://arxiv.org/abs/2506.23463)
*Jang Won June*

Main category: cs.CL

TL;DR: This paper proposes ATF, an adaptive framework to filter unnecessary columns and rows from large tables to improve performance in Table-based Question Answering tasks.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) face difficulty processing large tables due to input length limitations, prompting the need for effective methods to reduce table size while retaining critical information.

Method: The authors introduce ATF, a filtering framework comprising LLM-generated column descriptions, clustering, and sparse-dense alignment scores to prune irrelevant rows and columns. It works modularly without retraining existing TableQA models.

Result: ATF reduces the number of table cells by approximately 70%, improving performance in out-of-domain TableQA tasks. However, it slightly reduces performance in Table Fact Verification tasks due to the importance of full-table context there.

Conclusion: ATF is effective in adaptively balancing informativeness and minimalism across different tasks, making it a flexible and modular enhancement to existing TableQA systems.

Abstract: Large language models (LLMs) for table-based reasoning often struggle with
large tables due to input length limits. We propose ATF (Adaptive Table
Filtering Framework), a modular and question-aware filtering pipeline that
prunes uninformative columns and rows using LLM-generated column descriptions,
clustering, and sparse-dense alignment scores. ATF integrates seamlessly with
existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that
ATF reduces table cells by ~70\%, boosting performance on out-of-domain TableQA
tasks while causing slight performance drops on Table Fact Verification, where
full-table context is more critical. These results highlight ATF's ability to
adaptively balance informativeness and minimalism across tasks.

</details>


### [105] [Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent](https://arxiv.org/abs/2506.23485)
*Haocheng Yu,Yaxiong Wu,Hao Wang,Wei Guo,Yong Liu,Yawen Li,Yuyang Ye,Junping Du,Enhong Chen*

Main category: cs.CL

TL;DR: The paper introduces TAIRA, a thought-augmented system that enhances LLM-powered interactive recommendations by managing diverse and complex user intents using thought patterns.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-powered agents struggle to handle complex, ambiguous, or unrefined user intents in interactive recommendation scenarios.

Method: The proposed method, TAIRA, employs a multi-agent system with a manager agent orchestrating tasks using Thought Pattern Distillation (TPD) to extract high-level thoughts from experiences, combined with user simulation schemes to test performance.

Result: TAIRA shows superior performance over traditional methods, especially on challenging tasks and in generalizing to new scenarios, across multiple datasets.

Conclusion: TAIRA effectively addresses complex user intents in interactive recommendation systems, demonstrating enhanced capabilities and generalization potential.

Abstract: Interactive recommendation is a typical information-seeking task that allows
users to interactively express their needs through natural language and obtain
personalized recommendations. Large language model-powered (LLM-powered) agents
have become a new paradigm in interactive recommendations, effectively
capturing users' real-time needs and enhancing personalized experiences.
However, due to limited planning and generalization capabilities, existing
formulations of LLM-powered interactive recommender agents struggle to
effectively address diverse and complex user intents, such as intuitive,
unrefined, or occasionally ambiguous requests. To tackle this challenge, we
propose a novel thought-augmented interactive recommender agent system (TAIRA)
that addresses complex user intents through distilled thought patterns.
Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring
a manager agent that orchestrates recommendation tasks by decomposing user
needs and planning subtasks, with its planning capacity strengthened through
Thought Pattern Distillation (TPD), a thought-augmentation method that extracts
high-level thoughts from the agent's and human experts' experiences. Moreover,
we designed a set of user simulation schemes to generate personalized queries
of different difficulties and evaluate the recommendations based on specific
datasets. Through comprehensive experiments conducted across multiple datasets,
TAIRA exhibits significantly enhanced performance compared to existing methods.
Notably, TAIRA shows a greater advantage on more challenging tasks while
generalizing effectively on novel tasks, further validating its superiority in
managing complex user intents within interactive recommendation systems. The
code is publicly available at:https://github.com/Alcein/TAIRA.

</details>


### [106] [Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably](https://arxiv.org/abs/2506.23508)
*Zhihao Zhang,Qiaole Dong,Qi Zhang,Jun Zhao,Enyu Zhou,Zhiheng Xi,Senjie Jin,Xiaoran Fan,Yuhao Zhou,Yanwei Fu,Tao Ji,Tao Gui,Xuanjing Huang*

Main category: cs.CL

TL;DR: This paper explores the balance of task adaptation and knowledge retention in post-training methods for multimodal large language models, using jigsaw puzzles as a novel test task.


<details>
  <summary>Details</summary>
Motivation: To understand how post-training methods like Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) affect prior knowledge in multimodal large language models during task adaptation.

Method: The researchers used jigsaw puzzles as a novel task and conducted experiments with the multimodal LLM Qwen2.5-VL. They studied how SFT and RFT influenced task acquisition and knowledge retention using learning dynamics analysis.

Result: SFT enables faster task acquisition but causes significant knowledge forgetting, while RFT maintains prior knowledge but learns more slowly. A combination of RFT simulation rollouts with SFT enabled both rapid learning and knowledge preservation.

Conclusion: Algorithmic differences are less central to forgetting than data distribution. RFT shows potential for stable continual learning, and combining RFT with supervised training can mitigate forgetting while enabling rapid task learning in multimodal LLMs.

Abstract: Post-training algorithms such as Supervised Fine-Tuning (SFT) and
Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large
language models to downstream tasks. While effective at task adaptation, their
impact on prior knowledge remains unclear. In this paper, we introduce jigsaw
puzzles as a novel task absent from existing pretraining corpora and
systematically study the behavior of SFT and RFT on an open-source multimodal
model, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid
task acquisition but leads to catastrophic forgetting, whereas RFT learns more
slowly on novel tasks but maintains prior knowledge. We analyze this phenomenon
through the lens of learning dynamics, showing that RFT reinforces correct
samples that are naturally aligned with the base model's probability landscape,
mitigating interference with prior knowledge. Moreover, supervised training on
correct RFT-simulated rollouts allows SFT to preserve knowledge while rapidly
learning new tasks. These findings suggest that data distribution, rather than
algorithmic differences, plays a central role in forgetting, and highlight
RFT's potential for stable continual learning in multimodal large language
models.

</details>


### [107] [NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning](https://arxiv.org/abs/2506.23524)
*Phan Quoc Hung Mai,Quang Hung Nguyen,Phuong Giang Duong,Hong Hanh Nguyen,Nguyen Tuan Long*

Main category: cs.CL

TL;DR: The paper introduces NEU-ESC, a Vietnamese dataset for educational sentiment and topic classifications, achieving up to 83.7% accuracy using BERT-based models.


<details>
  <summary>Details</summary>
Motivation: Limited resources and lack of domain relevance in Vietnamese educational datasets necessitate a new dataset to capture students' comments accurately.

Method: Developed the NEU-ESC dataset from university forums and applied multitask learning using encoder-only language models (BERT). Also benchmarked dataset and model performance against other datasets and large language models.

Result: The model achieved up to 83.7% accuracy for sentiment classification and 79.8% accuracy for topic classification, highlighted richer textual and vocabulary features of NEU-ESC.

Conclusion: NEU-ESC addresses resource gaps in Vietnamese educational datasets, offering improved sentiment and topic classification performance. The dataset is publicly accessible.

Abstract: In the field of education, understanding students' opinions through their
comments is crucial, especially in the Vietnamese language, where resources
remain limited. Existing educational datasets often lack domain relevance and
student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese
dataset for Educational Sentiment Classification and Topic Classification,
curated from university forums, which offers more samples, richer class
diversity, longer texts, and broader vocabulary. In addition, we explore
multitask learning using encoder-only language models (BERT), in which we
showed that it achieves performance up to 83.7% and 79.8% accuracy for
sentiment and topic classification tasks. We also benchmark our dataset and
model with other datasets and models, including Large Language Models, and
discuss these benchmarks. The dataset is publicly available at:
https://huggingface.co/datasets/hung20gg/NEU-ESC.

</details>


### [108] [On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?](https://arxiv.org/abs/2506.23527)
*Jan Kvapil,Martin Fajcik*

Main category: cs.CL

TL;DR: This paper explores the memorization, creativity, and nonsense in cooking recipes generated by Large Language Models (LLMs), using human annotations and scalable automation tools.


<details>
  <summary>Details</summary>
Motivation: To understand how LLMs balance memorizing existing content, generating creative outputs, and producing nonsensical results in tasks like recipe generation.

Method: Conducted human annotations on recipes generated by a specific LLM (Mixtral) and designed an automated ‘LLM-as-judge’ framework for scalable analysis that evaluates ingredients, steps, and nonsense aspects of generated recipes.

Result: The study found that the LLM, Mixtral, relies significantly on memorized content from online sources. The automated framework demonstrated an accuracy of up to 78% in ingredient matching when compared to human annotations.

Conclusion: By combining human annotations and an automated pipeline, this study provides a scalable method to assess and quantify LLMs' creativity, reliance on memorized content, and tendency to generate nonsense in recipe generation.

Abstract: This work-in-progress investigates the memorization, creativity, and nonsense
found in cooking recipes generated from Large Language Models (LLMs).
Precisely, we aim (i) to analyze memorization, creativity, and non-sense in
LLMs using a small, high-quality set of human judgments and (ii) to evaluate
potential approaches to automate such a human annotation in order to scale our
study to hundreds of recipes. To achieve (i), we conduct a detailed human
annotation on 20 preselected recipes generated by LLM (Mixtral), extracting
each recipe's ingredients and step-by-step actions to assess which elements are
memorized--i.e., directly traceable to online sources possibly seen during
training--and which arise from genuine creative synthesis or outright nonsense.
We find that Mixtral consistently reuses ingredients that can be found in
online documents, potentially seen during model training, suggesting strong
reliance on memorized content. To achieve aim (ii) and scale our analysis
beyond small sample sizes and single LLM validation, we design an
``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,
parsing ingredients and recipe steps, and their annotation. For instance,
comparing its output against human annotations, the best ingredient extractor
and annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on
ingredient matching. This automated framework enables large-scale
quantification of memorization, creativity, and nonsense in generated recipes,
providing rigorous evidence of the models' creative capacities.

</details>


### [109] [Semantic-guided Diverse Decoding for Large Language Model](https://arxiv.org/abs/2506.23601)
*Weijie Shi,Yue Cui,Yaguang Wu,Jingzhi Fang,Shibo Zhang,Mengze Li,Sirui Han,Jia Zhu,Jiajie Xu,Xiaofang Zhou*

Main category: cs.CL

TL;DR: SemDiD improves semantic diversity in text generation by focusing on embedding space rather than token-level approaches.


<details>
  <summary>Details</summary>
Motivation: Existing decoding methods for generating diverse text responses fail to achieve significant semantic diversity, limiting their use in various applications.

Method: SemDiD enhances text diversity by employing embedding-based mechanisms: orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment, optimized through adaptive gain functions and constraints.

Result: SemDiD improves coverage in Best-of-N strategies by 1.4-5.2% across tasks and accelerates RLHF training convergence by 15%, increasing accuracy by up to 2.1%.

Conclusion: SemDiD is a superior decoding approach for semantic diversity and quality balance, outperforming conventional token-based methods in language model tasks.

Abstract: Diverse decoding of large language models is crucial for applications
requiring multiple semantically distinct responses, yet existing methods
primarily achieve lexical rather than semantic diversity. This limitation
significantly constrains Best-of-N strategies, group-based reinforcement
learning, and data synthesis. While temperature sampling and diverse beam
search modify token distributions or apply n-gram penalties, they fail to
ensure meaningful semantic differentiation. We introduce Semantic-guided
Diverse Decoding (SemDiD), operating directly in embedding space that balances
quality with diversity through three complementary mechanisms: orthogonal
directional guidance, dynamic inter-group repulsion, and position-debiased
probability assessment. SemDiD harmonizes these competing objectives using
adaptive gain functions and constraint optimization, ensuring both quality
thresholds and maximal semantic differentiation. Experiments show SemDiD
consistently outperforms existing methods, improving Best-of-N coverage by
1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%
while increasing accuracy by up to 2.1%.

</details>


### [110] [Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs](https://arxiv.org/abs/2506.23610)
*Manuel Pratelli,Marinella Petrocchi*

Main category: cs.CL

TL;DR: LLMs conditioned on Big-Five personality traits can replicate some personality-driven differences in news discernment but show systematic biases, highlighting both their potential and limitations for behavioral simulation.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can accurately simulate personality-driven behavioral differences, particularly in susceptibility to misinformation.

Method: Condition LLM agents on Big-Five personality profiles and compare their ratings of headline accuracy to human responses from published datasets.

Result: LLMs reliably replicate certain trait associations like Agreeableness and Conscientiousness but show divergences, exposing biases in personality modeling.

Conclusion: LLMs demonstrate promise for simulating behavioral patterns but reveal limitations, offering insight into improving cognitive diversity modeling in artificial systems.

Abstract: Large language models (LLMs) make it possible to generate synthetic
behavioural data at scale, offering an ethical and low-cost alternative to
human experiments. Whether such data can faithfully capture psychological
differences driven by personality traits, however, remains an open question. We
evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to
reproduce personality-based variation in susceptibility to misinformation,
focusing on news discernment, the ability to judge true headlines as true and
false headlines as false. Leveraging published datasets in which human
participants with known personality profiles rated headline accuracy, we create
matching LLM agents and compare their responses to the original human patterns.
Certain trait-misinformation associations, notably those involving
Agreeableness and Conscientiousness, are reliably replicated, whereas others
diverge, revealing systematic biases in how LLMs internalize and express
personality. The results underscore both the promise and the limits of
personality-aligned LLMs for behavioral simulation, and offer new insight into
modeling cognitive diversity in artificial agents.

</details>


### [111] [Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack](https://arxiv.org/abs/2506.23661)
*Arnisa Fazla,Lucas Krauter,David Guzman Piedrahita,Andrianos Michail*

Main category: cs.CL

TL;DR: The study enhances the BeamAttack algorithm for testing text classification robustness, includes word deletion, substitutions, LIME integration, and achieves high success in adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: To evaluate and improve the robustness of text classification models by refining adversarial attack methods.

Method: Extensions to BeamAttack algorithm: introducing word deletions, skipping substitutions, and adding LIME for better word replacement prioritization; tested across BiLSTM, BERT, and RoBERTa models.

Result: Achieved over 99% attack success rate while maintaining high semantic and lexical similarity of modified texts.

Conclusion: The extended BeamAttack is effective in creating adversarial examples and highlights model vulnerabilities, with both quantitative and qualitative validation provided.

Abstract: We extend BeamAttack, an adversarial attack algorithm designed to evaluate
the robustness of text classification systems through word-level modifications
guided by beam search. Our extensions include support for word deletions and
the option to skip substitutions, enabling the discovery of minimal
modifications that alter model predictions. We also integrate LIME to better
prioritize word replacements. Evaluated across multiple datasets and victim
models (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA
framework, our approach achieves over a 99\% attack success rate while
preserving the semantic and lexical similarity of the original texts. Through
both quantitative and qualitative analysis, we highlight BeamAttack's
effectiveness and its limitations. Our implementation is available at
https://github.com/LucK1Y/BeamAttack

</details>


### [112] [Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation](https://arxiv.org/abs/2506.23662)
*Philip Lippmann,Jie Yang*

Main category: cs.CL

TL;DR: ZEST introduces a method for generating domain-adaptive embeddings without requiring access to the target corpus, using synthetic context derived from a few exemplar documents.


<details>
  <summary>Details</summary>
Motivation: The reliance of context-aware embeddings on target corpus access or domain-specific fine-tuning creates practical challenges in privacy-sensitive or resource-constrained scenarios.

Method: ZEST employs a hierarchical procedure to synthesize a proxy context corpus based on a handful of exemplar documents, which is then used by a frozen encoder to generate domain-adaptive embeddings without fine-tuning.

Result: Using only five example documents, ZEST achieves zero-shot embedding performance that closely matches models with full corpus access across the MTEB benchmark.

Conclusion: ZEST offers a scalable and efficient solution for deploying adaptable embeddings in settings with privacy or resource constraints.

Abstract: Context-aware embedding methods boost retrieval accuracy by conditioning on
corpus statistics (e.g., term co-occurrence and topical patterns) extracted
from neighboring documents. However, this context-aware approach requires
access to the target corpus or requires domain-specific finetuning, posing
practical barriers in privacy-sensitive or resource-constrained settings. We
present ZEST, a zero-shot contextual adaptation framework that replaces real
corpus access with a one-time offline synthesis of a compact proxy. Given only
a handful exemplar documents representative of the general target domain, we
use a multi-step hierarchical procedure to generate a synthetic context corpus
of several hundred documents that aims to emulate key domain-specific
distributions. At inference, the frozen context-aware encoder uses this proxy
corpus -- without any finetuning or target corpus access -- to produce
domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot
synthetic context adaptation using only five example documents performs within
0.5% of models leveraging full target corpus access -- demonstrating remarkable
efficacy without any retraining. ZEST thus provides a practical method for
deploying high-performance, adaptable embeddings in constrained environments.

</details>


### [113] [L0: Reinforcement Learning to Become General Agents](https://arxiv.org/abs/2506.23667)
*Junjie Zhang,Jingyi Xi,Zhuoyang Song,Junyu Lu,Yuhua Ke,Ting Sun,Yukun Yang,Jiaxing Zhang,Songxin Zhang,Zejian Xie*

Main category: cs.CL

TL;DR: L-Zero (L0) is a scalable framework enabling large language models to function as agents for complex tasks, greatly improving performance using reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To address scalability and efficiency issues when training large language models for multi-turn, long-horizon tasks.

Method: Developed L-Zero (L0), featuring a scalable worker pool, NB-Agent scaffold operating in 'code-as-action' with REPL, and reinforcement learning using verifiable rewards.

Result: Boosted task accuracy for Qwen2.5-7B-Instruct model significantly, achieving gains from 30% to 80% on SimpleQA and 22% to 41% on HotpotQA.

Conclusion: L-Zero facilitates scalable training of general-purpose agents, improving efficiency and effectiveness; open-source models and pipeline offered for broader adoption.

Abstract: Training large language models (LLMs) to act as autonomous agents for
multi-turn, long-horizon tasks remains significant challenges in scalability
and training efficiency. To address this, we introduce L-Zero (L0), a scalable,
end-to-end training pipeline for general-purpose agents. Featuring a low-cost,
extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier
for applying reinforcement learning in complex environments. We also introduce
NB-Agent, the agent scaffold within L0, which operates in a "code-as-action"
fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality
question-answering benchmarks. Our experiments demonstrate that a base model
can develop robust problem-solving skills using solely Reinforcement Learning
with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method
boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41
%. We have open-sourced the entire L0 system, including our L0 series models,
the NB-Agent, a complete training pipeline, and the corresponding training
recipes on (https://github.com/cmriat/l0).

</details>


### [114] [AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data](https://arxiv.org/abs/2506.23735)
*JiaRu Wu,Mingwei Liu*

Main category: cs.CL

TL;DR: This paper introduces AutoEvoEval, an advanced evaluation framework utilizing interpretable evolution operations to generate more realistic and challenging test samples for benchmarking language models.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation benchmarks for language models are static and insufficient for assessing robustness and generalization in realistic scenarios. Current augmentation methods lack systematic control over perturbation complexities affecting comprehensive robustness analyses.

Method: The authors propose AutoEvoEval, incorporating 22 atomic evolution operations with multi-round compositions to generate controlled, diverse, and challenging test samples for assessing performance on close-ended tasks like multi-choice questions.

Result: Experiments reveal a significant average accuracy drop (7.283%) due to atomic operations. Multi-step evolution amplifies adversarial impacts by up to 52.932%, highlighting that current benchmarks may overestimate model generalization.

Conclusion: AutoEvoEval demonstrates the necessity of evolution-aware approaches in robustness evaluations, offering insights into model sensitivities and weaknesses through systematic test diversification.

Abstract: Large language models (LLMs) have shown remarkable performance on various
tasks, but existing evaluation benchmarks are often static and insufficient to
fully assess their robustness and generalization in realistic scenarios. Prior
work using evolutionary or adversarial data augmentation has improved
evaluation diversity but lacks systematic control over perturbation types and
multi-step complexity, limiting comprehensive robustness analysis. To address
these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for
close-ended tasks such as multi-choice question answering. AutoEvoEval
introduces 22 interpretable atomic evolution operations and supports
multi-round compositions, enabling controlled generation of diverse,
challenging, and realistic test samples. We conduct extensive experiments
addressing four research questions on a broad set of open- and closed-source
LLMs. Our results show that atomic operations cause an average accuracy drop of
7.283\%, with structure-disrupting or misleading semantic edits causing the
largest declines. Model sensitivities vary significantly for the same
perturbation, and combining multiple evolution steps amplifies adversarial
effects by up to 52.932\%. These findings suggest current benchmarks may
overestimate true model generalization and emphasize the need for
evolution-aware robustness evaluation. Code and resources are available at:
https://github.com/SYSUSELab/AutoEvoEval.

</details>


### [115] [Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences](https://arxiv.org/abs/2506.23743)
*Tiziano Labruna,Simone Gallo,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: The study examines positional bias in language models when one option is systematically preferred due to its position in binary question answering, especially under uncertain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand and quantify how positional bias impacts decision-making in large language models, especially in scenarios with varying levels of answer uncertainty.

Method: Researchers re-adapted the SQuAD-it dataset with additional answer options and manipulated context to induce varying levels of uncertainty. Positional bias was analyzed using datasets with flipped answer orders and two high-uncertainty benchmarks: WebGPT and Winning Arguments.

Result: Positional bias was found to be nearly nonexistent in low-uncertainty scenarios but increased exponentially under high-uncertainty conditions.

Conclusion: Positional bias in large language models is significantly influenced by the level of uncertainty in the provided options, highlighting an area for improving fairness in model outputs.

Abstract: Positional bias in binary question answering occurs when a model
systematically favors one choice over another based solely on the ordering of
presented options. In this study, we quantify and analyze positional bias
across five large language models under varying degrees of answer uncertainty.
We re-adapted the SQuAD-it dataset by adding an extra incorrect answer option
and then created multiple versions with progressively less context and more
out-of-context answers, yielding datasets that range from low to high
uncertainty. Additionally, we evaluate two naturally higher-uncertainty
benchmarks: (1) WebGPT - question pairs with unequal human-assigned quality
scores, and (2) Winning Arguments - where models predict the more persuasive
argument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order
of the "correct" (or higher-quality/persuasive) option is systematically
flipped (first placed in position 1, then in position 2) to compute both
Preference Fairness and Position Consistency. We observe that positional bias
is nearly absent under low-uncertainty conditions, but grows exponentially when
it becomes doubtful to decide which option is correct.

</details>


### [116] [Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model](https://arxiv.org/abs/2506.23840)
*Bowen Ding,Yuhan Chen,Futing Wang,Lingfeng Ming,Tao Lin*

Main category: cs.CL

TL;DR: Large Reasoning Models (LRMs) struggle with inefficient verbose responses laden with unnecessary 'thinking tokens' during simple tasks, impairing effectiveness. A novel approach, Dual Policy Preference Optimization (DuP-PO), addresses this by regulating and optimizing token prediction.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the inefficiencies in LRMs caused by overthinking behaviors triggered by thinking tokens, which hinder performance especially in constrained token budgets.

Method: The study introduces DuP-PO, which utilizes (1) rollout sampling for balanced response exposure, (2) fine-grained advantage control for dynamic token prediction regulation, and (3) policy shaping to stabilize gradient contributions.

Result: Experiments on five math reasoning benchmarks demonstrated DuP-PO's improvements in token efficiency and superior base model reasoning performance.

Conclusion: DuP-PO effectively mitigates the thinking trap in LRMs, enhancing reasoning efficiency and accuracy, particularly in scenarios with constrained token limits.

Abstract: Large Reasoning Models (LRMs) excel at solving complex problems but face an
overthinking dilemma. When handling simple tasks, they often produce verbose
responses overloaded with thinking tokens (e.g., wait, however). These tokens
trigger unnecessary high-level reasoning behaviors like reflection and
backtracking, reducing efficiency. In this work, our pilot study reveals that
these thinking-token-induced behaviors are not essential for effective
problem-solving and may even hinder correct reasoning within constrained token
budgets. We identify this phenomenon as the thinking trap. To mitigate this
issue, we propose Dual Policy Preference Optimization (DuP-PO), a novel
algorithm featuring: (1) A rollout sampling strategy that guarantees balanced
exposure to responses with and without thinking tokens; (2) A fine-grained
advantage control technique to dynamically regulate the prediction of target
tokens; (3) A policy shaping method ensuring stable gradient contributions from
thinking tokens. Experimental results on five popular math reasoning benchmarks
show that DuP-PO performs well on the popular LRM, which significantly improves
their token efficiency during reasoning, while achieving superior performance
of the base model.

</details>


### [117] [Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It](https://arxiv.org/abs/2506.23864)
*Seyed Mahed Mousavi,Edoardo Cecchinato,Lucia Hornikova,Giuseppe Riccardi*

Main category: cs.CL

TL;DR: The paper reviews three reasoning benchmarks and finds significant flaws in both benchmark design and evaluation methods. It challenges existing approaches and offers tools for more interpretable assessments.


<details>
  <summary>Details</summary>
Motivation: To scrutinize the validity of reasoning benchmarks by identifying structural and procedural flaws that might misrepresent model capabilities.

Method: Systematic auditing of SocialIQa, FauxPas-EAI, and ToMi benchmarks using five LLMs as diagnostic tools, coupled with human annotation and re-evaluation on improved benchmark subsets.

Result: Revealed structural and evaluation issues leading to inconsistent and format-dependent model performance, questioning the effectiveness of existing benchmarks.

Conclusion: Current reasoning benchmarks inadequately assess reasoning processes, and better evaluation protocols are needed to accurately measure inference capabilities.

Abstract: We conduct a systematic audit of three widely used reasoning benchmarks,
SocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark
items and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and
LLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic
issues in benchmark design (e.g., duplicated items, ambiguous wording, and
implausible answers), as well as scoring procedures that prioritize output form
over reasoning process. Through systematic human annotation and re-evaluation
on cleaned benchmark subsets, we find that model scores often improve not due
to due to erratic surface wording variations and not to improved reasoning.
Infact, further analyses show that model performance is highly sensitive to
minor input variations such as context availability and phrasing, revealing
that high scores may reflect alignment with format-specific cues rather than
consistent inference based on the input. These findings challenge the validity
of current benchmark-based claims about reasoning in LLMs, and highlight the
need for evaluation protocols that assess reasoning as a process of drawing
inference from available information, rather than as static output selection.
We release audited data and evaluation tools to support more interpretable and
diagnostic assessments of model reasoning.

</details>


### [118] [Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting](https://arxiv.org/abs/2506.23888)
*André de Souza Loureiro,Jorge Valverde-Rebaza,Julieta Noguez,David Escarcega,Ricardo Marcacini*

Main category: cs.CL

TL;DR: The MAPS framework improves multi-step reasoning in LLMs using iterative self-reflection and adaptive corrections.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex multi-step reasoning tasks, limiting their capabilities.

Method: MAPS combines techniques like Chain of Thought, Self-Reflection, and Auto-Prompting in an iterative refinement process to detect and rectify errors.

Result: MAPS outperformed standard approaches in benchmarks, enabling general LLMs to achieve competitive reasoning capabilities.

Conclusion: MAPS enhances reasoning accuracy while balancing costs, bridging the gap between general-purpose and specialized reasoning models.

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
improved their problem-solving capabilities. However, these models still
struggle when faced with complex multi-step reasoning tasks. In this paper, we
propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,
a novel approach designed to enhance multi-step mathematical reasoning in LLMs
by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and
Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an
iterative refinement process. Initially, the model generates a solution using
CoT prompting. When errors are detected, an adaptive self-reflection mechanism
identifies and analyzes them, generating tailored prompts to guide corrections.
These dynamically adjusted prompts enable the model to iteratively refine its
reasoning. Experiments on four well-established benchmarks across multiple LLMs
show that MAPS significantly outperforms standard CoT and achieves competitive
results with reasoning-optimized models. In addition, MAPS enables
general-purpose LLMs to reach performance levels comparable to specialized
reasoning models. While deeper reflection layers improve accuracy, they also
increase token usage and costs. To balance this trade-off, MAPS strategically
limits reflection depth, ensuring an optimal balance between cost and reasoning
performance.

</details>


### [119] [IMPACT: Inflectional Morphology Probes Across Complex Typologies](https://arxiv.org/abs/2506.23929)
*Mohammed J. Saeed,Tommi Vehvilainen,Evgeny Fedoseev,Sevil Caliskan,Tatiana Vodolazova*

Main category: cs.CL

TL;DR: The paper introduces IMPACT, a framework to evaluate the linguistic and morphological understanding of large language models (LLMs) in five morphologically rich languages. It highlights performance gaps in non-English language processing.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to address the uncertainty regarding how well LLMs grasp the complexity of morphologically rich languages, as their fluency in generating non-English text doesn't necessarily reflect deep linguistic understanding.

Method: The authors developed IMPACT, a synthetic evaluation framework with targeted tests for morphology-related cases. It was used to assess eight multilingual LLMs across languages like Arabic, Russian, Finnish, Turkish, and Hebrew, focusing on common and unique linguistic phenomena.

Result: The tested LLMs, despite their strong English performance, struggled with non-English morphological patterns and exhibited poor judgments of ungrammatical examples. Chain of Thought and Thinking Models further degraded performance.

Conclusion: LLMs lack adequate handling of morphologically complex languages and show significant room for improvement. The released IMPACT framework aims to facilitate further research into enhancing LLM capabilities for such languages.

Abstract: Large Language Models (LLMs) have shown significant progress on various
multilingual benchmarks and are increasingly used to generate and evaluate text
in non-English languages. However, while they may produce fluent outputs, it
remains unclear to what extent these models truly grasp the underlying
linguistic complexity of those languages, particularly in morphology. To
investigate this, we introduce IMPACT, a synthetically generated evaluation
framework focused on inflectional morphology, which we publicly release,
designed to evaluate LLM performance across five morphologically rich
languages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes
unit-test-style cases covering both shared and language-specific phenomena,
from basic verb inflections (e.g., tense, number, gender) to unique features
like Arabic's reverse gender agreement and vowel harmony in Finnish and
Turkish. We assess eight multilingual LLMs that, despite strong English
performance, struggle with other languages and uncommon morphological patterns,
especially when judging ungrammatical examples. We also show that Chain of
Thought and Thinking Models can degrade performance. Our work exposes gaps in
LLMs' handling of linguistic complexity, pointing to clear room for
improvement. To support further research, we publicly release the IMPACT
framework.

</details>


### [120] [Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2506.23930)
*Ruhina Tabasshum Prome,Tarikul Islam Tamiti,Anomadarshi Barua*

Main category: cs.CL

TL;DR: The paper explores advanced prompting strategies for Large Language Models (LLMs) to effectively detect hate speech in low-resource languages, particularly Bengali.


<details>
  <summary>Details</summary>
Motivation: Hate speech detection is challenging due to linguistic diversity, code-mixing, and the prevalence of informal language on social media, especially in low-resource languages lacking large datasets.

Method: The study investigates six prompting techniques for LLMs, including a novel metaphor prompting strategy, compared to traditional pre-trained word embeddings and deep learning architectures using metrics like F1 scores and environmental impact.

Result: Metaphor prompting was shown to be uniquely effective at bypassing LLM safety mechanisms and improving hate speech detection for both low-resource (Bengali, Hindi) and high-resource languages (English, German).

Conclusion: The results highlight the promise of metaphor prompting in enhancing the LLM-based hate speech detection for low-resource languages while considering computational efficiency and environmental impact.

Abstract: The rapid expansion of social media leads to a marked increase in hate
speech, which threatens personal lives and results in numerous hate crimes.
Detecting hate speech presents several challenges: diverse dialects, frequent
code-mixing, and the prevalence of misspelled words in user-generated content
on social media platforms. Recent progress in hate speech detection is
typically concentrated on high-resource languages. However, low-resource
languages still face significant challenges due to the lack of large-scale,
high-quality datasets. This paper investigates how we can overcome this
limitation via prompt engineering on large language models (LLMs) focusing on
low-resource Bengali language. We investigate six prompting strategies -
zero-shot prompting, refusal suppression, flattering the classifier, multi-shot
prompting, role prompting, and finally our innovative metaphor prompting to
detect hate speech effectively in low-resource languages. We pioneer the
metaphor prompting to circumvent the built-in safety mechanisms of LLMs that
marks a significant departure from existing jailbreaking methods. We
investigate all six different prompting strategies on the Llama2-7B model and
compare the results extensively with three pre-trained word embeddings - GloVe,
Word2Vec, and FastText for three different deep learning models - multilayer
perceptron (MLP), convolutional neural network (CNN), and bidirectional gated
recurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in
the low-resource Bengali language, we also evaluate it in another low-resource
language - Hindi, and two high-resource languages - English and German. The
performance of all prompting techniques is evaluated using the F1 score, and
environmental impact factor (IF), which measures CO$_2$ emissions, electricity
usage, and computational time.

</details>


### [121] [Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs](https://arxiv.org/abs/2506.23940)
*Yang Dai,Jianxiang An,Tianwei Lin,Hongyang He,Hongzhe Huang,Wenqiao Zhang,Zheqi Lv,Siliang Tang,Yueting Zhuang*

Main category: cs.CL

TL;DR: The paper addresses the fragmentation of domain-specialized multimodal large language models (MLLMs) by proposing a unified parameter integration framework to enable modular composition and effective sharing of expertise.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges posed by fragmented knowledge across domain-specific MLLMs and enhance their interoperability and expertise-sharing capabilities.

Method: The framework applies Compatibility-Aware Parameter Splicing (CAPS), combining local functional attribution with global information-theoretic signals for selective parameter fusion. It extends this strategy to low-rank adaptation layer granularity and introduces a domain compatibility scoring mechanism.

Result: The proposed method efficiently integrates expert knowledge across MLLMs while maintaining minimal inference overhead, enabling synergies between heterogeneous models.

Conclusion: The framework offers a scalable solution for creating compositional, domain-adaptive MLLMs, validated via extensive multimodal benchmark testing.

Abstract: Multimodal Large Language Models (MLLMs) have achieved success across various
domains. However, their applicability tends to degrade when confronted with
different types of data inputs, especially for MLLMs that have been fine-tuned
for specific tasks. Despite its importance, the study of knowledge sharing
among domain-specific MLLMs--such as those trained for mathematics or
code--remains largely underexplored. To address the fragmentation of knowledge
across domain-specialized MLLMs, we propose a unified parameter integration
framework that enables modular composition of expert capabilities. Our method
is grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,
which leverages both local functional attribution and global
information-theoretic signals to guide selective parameter fusion. By extending
this mechanism to the low-rank adaptation layer granularity, we ensure
efficient integration with minimal inference overhead. Furthermore, we
introduce a domain compatibility scoring mechanism that quantifies inter-expert
alignment at the activation level and correlates with downstream task utility.
This principled fusion protocol allows the final model to synergize
heterogeneous expertise while preserving structural modularity. Extensive
evaluations across diverse multimodal benchmarks validate the effectiveness of
our framework, offering a scalable path toward compositional, domain-adaptive
MLLMs.

</details>


### [122] [Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders](https://arxiv.org/abs/2506.23951)
*Mathis Le Bail,Jérémie Dentan,Davide Buscaldi,Sonia Vanier*

Main category: cs.CL

TL;DR: The paper investigates Sparse Autoencoder (SAE)-based methods for interpreting sentence classifiers, proposing a novel architecture that enhances concept causality and interpretability.


<details>
  <summary>Details</summary>
Motivation: Sparse Autoencoders (SAEs) have been useful in interpreting Large Language Models by extracting human-interpretable features, but their effectiveness in sentence classification is less explored.

Method: The authors develop a novel SAE-based architecture for sentence classification that includes a specialized classifier head and an activation rate sparsity loss. They benchmark this against other methods like ConceptShap, ICA, and existing SAE approaches across multiple benchmarks.

Result: The proposed architecture outperforms established methods in terms of causality and interpretability of the extracted features, as supported by experiments with two classification benchmarks and four Pythia-family LLMs.

Conclusion: The study highlights the potential of the new SAE-based method to enhance feature interpretability and causality in sentence classification tasks, validated by novel metrics and comparative benchmarks.

Abstract: Sparse Autoencoders (SAEs) have been successfully used to probe Large
Language Models (LLMs) and extract interpretable concepts from their internal
representations. These concepts are linear combinations of neuron activations
that correspond to human-interpretable features. In this paper, we investigate
the effectiveness of SAE-based explainability approaches for sentence
classification, a domain where such methods have not been extensively explored.
We present a novel SAE-based architecture tailored for text classification,
leveraging a specialized classifier head and incorporating an activation rate
sparsity loss. We benchmark this architecture against established methods such
as ConceptShap, Independent Component Analysis, and other SAE-based concept
extraction techniques. Our evaluation covers two classification benchmarks and
four fine-tuned LLMs from the Pythia family. We further enrich our analysis
with two novel metrics for measuring the precision of concept-based
explanations, using an external sentence encoder. Our empirical results show
that our architecture improves both the causality and interpretability of the
extracted features.

</details>


### [123] [TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation](https://arxiv.org/abs/2506.23979)
*Renren Jin,Tianhao Shen,Xinwei Wu,Dan Shi,Haoran Sun,Wuwei Huang,Quandong Wang,Wei Liu,Jian Luan,Bin Wang,Deyi Xiong*

Main category: cs.CL

TL;DR: The paper introduces the Taxonomy-Guided Preference Data Generation (TaP) framework for scalable and automated multilingual preference dataset creation for training large language models, yielding superior results compared to existing datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the scarcity of high-quality, diverse, and multilingual datasets for supervised and preference fine-tuning of large language models, which is currently a resource-intensive process dominated by English-language datasets.

Method: The proposed method involves developing a Taxonomy-Guided Preference Data Generation (TaP) framework to create diverse and comprehensive datasets across multiple languages using a structured taxonomy for dataset composition.

Result: Experimental results show that large language models fine-tuned with TaP-generated datasets outperform those fine-tuned with existing open-source datasets, even surpassing models trained on a dataset 180 times larger.

Conclusion: The TaP framework enables efficient, scalable, and high-quality dataset creation, offering a superior alternative to existing datasets for fine-tuning large language models.

Abstract: Conducting supervised fine-tuning and preference fine-tuning on large
language models (LLMs) requires high-quality datasets to improve their ability
to follow instructions and align with human preferences and values. However,
constructing such datasets is resource-intensive, and most available datasets
for supervised and preference fine-tuning are in English. To address these
challenges, we propose the \underline{\textbf{Ta}}xonomy-Guided
\underline{\textbf{P}}reference Data Generation (TaP) framework, which
facilitates automated and scalable construction of preference datasets across
various languages. TaP is grounded in a structured taxonomy that allows
fine-grained control over dataset composition, thereby ensuring both diversity
and comprehensive coverage. We employ TaP-generated datasets to perform
supervised and preference fine-tuning on various LLMs. Experimental results
demonstrate that LLMs trained on TaP-generated datasets outperform those
trained on existing open-source datasets. Remarkably, LLMs trained on
TaP-generated datasets surpass the performance of those trained on an
open-source dataset that is 180 times larger.

</details>


### [124] [Machine Understanding of Scientific Language](https://arxiv.org/abs/2506.23990)
*Dustin Wright*

Main category: cs.CL

TL;DR: The research focuses on developing datasets, methods, and tools for analyzing and understanding scientific text, aiming to automatically identify its faithfulness to underlying science.


<details>
  <summary>Details</summary>
Motivation: The societal challenge posed by the growing volume of online scientific text, not all of which is accurate, necessitates tools to gauge its faithfulness.

Method: The study introduces innovations in natural language processing and machine learning across three areas: automatic fact checking, learning from limited data, and processing scientific text.

Result: Advances include methods and resources for claim detection, adversarial claim generation, cite-worthiness evaluation, domain adaptation, and more, enabling the identification of misinformation in scientific communication.

Conclusion: The outputs of this research enhance machine capabilities for processing limited scientific text, improving identification of misleading claims and contributing insights into science communication.

Abstract: Scientific information expresses human understanding of nature. This
knowledge is largely disseminated in different forms of text, including
scientific papers, news articles, and discourse among people on social media.
While important for accelerating our pursuit of knowledge, not all scientific
text is faithful to the underlying science. As the volume of this text has
burgeoned online in recent years, it has become a problem of societal
importance to be able to identify the faithfulness of a given piece of
scientific text automatically. This thesis is concerned with the cultivation of
datasets, methods, and tools for machine understanding of scientific language,
in order to analyze and understand science communication at scale. To arrive at
this, I present several contributions in three areas of natural language
processing and machine learning: automatic fact checking, learning with limited
data, and scientific text processing. These contributions include new methods
and resources for identifying check-worthy claims, adversarial claim
generation, multi-source domain adaptation, learning from crowd-sourced labels,
cite-worthiness detection, zero-shot scientific fact checking, detecting
exaggerated scientific claims, and modeling degrees of information change in
science communication. Critically, I demonstrate how the research outputs of
this thesis are useful for effectively learning from limited amounts of
scientific text in order to identify misinformative scientific statements and
generate new insights into the science communication process

</details>


### [125] [Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning](https://arxiv.org/abs/2506.23998)
*Seungjun Yi,Joakim Nguyen,Huimin Xu,Terence Lim,Andrew Well,Mia Markey,Ying Ding*

Main category: cs.CL

TL;DR: This paper presents a fully automated large language model (LLM) pipeline for thematic analysis of clinical narratives in congenital heart disease (CHD), replacing manual processes with scalable, automated solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional thematic analysis (TA) of clinical narratives for congenital heart disease is labor-intensive and unscalable, limiting insights into patient and caregiver experiences.

Method: The paper introduces a novel multi-agent framework with large language models (LLMs), optionally improved using reinforcement learning from human feedback (RLHF), to automate thematic analysis.

Result: The automated system eliminates manual coding or transcript reviews, while aligning closely with human analysis and improving theme quality for clinical narratives.

Conclusion: This automated LLM-based approach enables scalable, patient-centered analysis of large qualitative datasets, adapting LLMs to specific clinical contexts for better insights into CHD challenges.

Abstract: Congenital heart disease (CHD) presents complex, lifelong challenges often
underrepresented in traditional clinical metrics. While unstructured narratives
offer rich insights into patient and caregiver experiences, manual thematic
analysis (TA) remains labor-intensive and unscalable. We propose a fully
automated large language model (LLM) pipeline that performs end-to-end TA on
clinical narratives, which eliminates the need for manual coding or full
transcript review. Our system employs a novel multi-agent framework, where
specialized LLM agents assume roles to enhance theme quality and alignment with
human analysis. To further improve thematic relevance, we optionally integrate
reinforcement learning from human feedback (RLHF). This supports scalable,
patient-centered analysis of large qualitative datasets and allows LLMs to be
fine-tuned for specific clinical contexts.

</details>


### [126] [Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective](https://arxiv.org/abs/2506.24006)
*Anselm R. Strohmaier,Wim Van Dooren,Kathrin Seßler,Brian Greer,Lieven Verschaffel*

Main category: cs.CL

TL;DR: This study evaluates the potential of Large Language Models (LLMs) like GPT in mathematics education, particularly for solving word problems. The findings reveal LLMs excel at superficial solutions but struggle with contextually challenging problems.


<details>
  <summary>Details</summary>
Motivation: To explore how LLMs' word-problem-solving capabilities align with educational needs and determine their potential usefulness in classrooms.

Method: A three-part scoping review including: (1) technical overview contrasting LLMs and student problem-solving processes, (2) systematic review of 213 studies on word-problem corpora, and (3) empirical evaluation of LLMs on 287 word problems.

Result: LLMs achieved near-perfect accuracy on simple (“s-problem”) word problems but showed weaknesses in addressing problems requiring real-world context comprehension.

Conclusion: While LLMs excel at solving straightforward word problems, their lack of contextual understanding limits their effectiveness as educational tools in mathematics classrooms.

Abstract: The progress of Large Language Models (LLMs) like ChatGPT raises the question
of how they can be integrated into education. One hope is that they can support
mathematics learning, including word-problem solving. Since LLMs can handle
textual input with ease, they appear well-suited for solving mathematical word
problems. Yet their real competence, whether they can make sense of the
real-world context, and the implications for classrooms remain unclear. We
conducted a scoping review from a mathematics-education perspective, including
three parts: a technical overview, a systematic review of word problems used in
research, and a state-of-the-art empirical evaluation of LLMs on mathematical
word problems. First, in the technical overview, we contrast the
conceptualization of word problems and their solution processes between LLMs
and students. In computer-science research this is typically labeled
mathematical reasoning, a term that does not align with usage in mathematics
education. Second, our literature review of 213 studies shows that the most
popular word-problem corpora are dominated by s-problems, which do not require
a consideration of realities of their real-world context. Finally, our
evaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems
shows that most recent LLMs solve these s-problems with near-perfect accuracy,
including a perfect score on 20 problems from PISA. LLMs still showed
weaknesses in tackling problems where the real-world context is problematic or
non-sensical. In sum, we argue based on all three aspects that LLMs have
mastered a superficial solution process but do not make sense of word problems,
which potentially limits their value as instructional tools in mathematics
classrooms.

</details>


### [127] [EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations](https://arxiv.org/abs/2506.24016)
*Hyunjong Kim,Sangyeop Kim,Jongheon Jeong,Yeongjae Cho,Sungzoon Cho*

Main category: cs.CL

TL;DR: This paper introduces EXPERT, a reference-free evaluation metric for image captioning, which provides structured explanations and achieves superior results compared to existing metrics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of standardized criteria and quality verification in the explanations provided by existing image captioning evaluation metrics.

Method: The authors propose a structured explanation metric, EXPERT, which uses large-scale datasets and a two-stage evaluation template rooted in fluency, relevance, and descriptiveness.

Result: EXPERT demonstrates state-of-the-art performance on benchmark datasets while generating higher-quality explanations, as confirmed via human evaluations.

Conclusion: EXPERT improves explainable evaluation in image captioning by offering a structured, reference-free metric, marking a significant advancement in both scoring accuracy and explanation quality.

Abstract: Recent advances in large language models and vision-language models have led
to growing interest in explainable evaluation metrics for image captioning.
However, these metrics generate explanations without standardized criteria, and
the overall quality of the generated explanations remains unverified. In this
paper, we propose EXPERT, a reference-free evaluation metric that provides
structured explanations based on three fundamental criteria: fluency,
relevance, and descriptiveness. By constructing large-scale datasets of
high-quality structured explanations, we develop a two-stage evaluation
template to effectively supervise a vision-language model for both scoring and
explanation generation. EXPERT achieves state-of-the-art results on benchmark
datasets while providing significantly higher-quality explanations than
existing metrics, as validated through comprehensive human evaluation. Our code
and datasets are available at https://github.com/hjkim811/EXPERT.

</details>


### [128] [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068)
*Ian R. McKenzie,Oskar J. Hollinsworth,Tom Tseng,Xander Davies,Stephen Casper,Aaron D. Tucker,Robert Kirk,Adam Gleave*

Main category: cs.CL

TL;DR: The paper evaluates the security of AI defense pipelines, introducing a novel few-shot-prompted classifier and a staged attack method (STACK) that effectively compromises such safeguards.


<details>
  <summary>Details</summary>
Motivation: To address the limited prior work in evaluating or attacking defense pipelines that guard against catastrophic misuse of AI models, thereby highlighting potential security vulnerabilities.

Method: The authors developed an open-source defense pipeline, tested its effectiveness with a novel few-shot-prompted classifier, and assessed vulnerabilities using a staged attack procedure (STACK), including transfer settings.

Result: The new classifier reduced attack success rate (ASR) to 0% on certain datasets, but the STACK procedure achieved a 71% ASR in black-box contexts and 33% ASR in transfer settings, exposing critical weaknesses.

Conclusion: Current defense pipelines are vulnerable to staged attacks. The paper underscores the need for mitigation strategies to increase pipeline robustness against adversarial attacks.

Abstract: Frontier AI developers are relying on layers of safeguards to protect against
catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus
model using one such defense pipeline, and other frontier developers including
Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the
security of such pipelines is unclear, with limited prior work evaluating or
attacking these pipelines. We address this gap by developing and red-teaming an
open-source defense pipeline. First, we find that a novel few-shot-prompted
input and output classifier outperforms state-of-the-art open-weight safeguard
model ShieldGemma across three attacks and two datasets, reducing the attack
success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,
we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on
ClearHarm in a black-box attack against the few-shot-prompted classifier
pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%
ASR, providing initial evidence that it is feasible to design attacks with no
access to the target pipeline. We conclude by suggesting specific mitigations
that developers could use to thwart staged attacks.

</details>


### [129] [On the Predictive Power of Representation Dispersion in Language Models](https://arxiv.org/abs/2506.24106)
*Yanhong Li,Ming Li,Karen Livescu,Jiawei Zhou*

Main category: cs.CL

TL;DR: The paper finds a strong link between a language model's text prediction performance (perplexity) and the dispersion of its embedding space, providing practical methods to leverage this property.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the relationship between language models' prediction capabilities and the structure of their representation spaces and to use this relationship for practical applications like domain adaptation and performance improvement.

Method: They analyze the dispersion (measured by average pairwise cosine distance) of hidden representations in various language models and tasks, and propose leveraging this dispersion for practical tasks like predicting downstream accuracy, optimizing retrieval-based approaches, and improving training with a push-away objective.

Result: Their results show that higher dispersion correlates with better model performance (lower perplexity), aids in domain adaptation, identifies optimal layers for retrieval tasks, and improves perplexity via a training objective.

Conclusion: Representation dispersion is crucial for language model performance. Leveraging it can enhance model selection, layer identification, and training efficiency, making it a valuable tool in model improvement.

Abstract: We show that a language model's ability to predict text is tightly linked to
the breadth of its embedding space: models that spread their contextual
representations more widely tend to achieve lower perplexity. Concretely, we
find that representation dispersion - the average pairwise cosine distance
among hidden vectors - strongly and negatively correlates with perplexity
across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,
news, scientific abstracts). Beyond illustrating this link, we show how
dispersion can be leveraged for a range of practical tasks without requiring
labeled data. First, measuring dispersion on unlabeled text allows us to
predict downstream accuracy in new domains, offering a data-efficient tool for
model selection. Next, we find that identifying layers with higher dispersion
pinpoints the best representations for retrieval-based methods such as kNN-LM,
bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple
push-away objective into training, which increases dispersion in both
single-domain and cross-domain scenarios and directly improves perplexity in
each.

</details>


### [130] [Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models](https://arxiv.org/abs/2506.24117)
*David M. Smiley*

Main category: cs.CL

TL;DR: The study explores the use of transformer-based language models to identify textual parallels in biblical Hebrew, enhancing efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Manual methods for identifying intertextual parallels in biblical Hebrew are time-consuming and error-prone, necessitating automated solutions.

Method: The study employed pre-trained transformer models (E5, AlephBERT, MPNet, LaBSE) and measured their performance using cosine similarity and Wasserstein Distance to detect parallels within biblical texts.

Result: E5 excelled in detecting parallels, while AlephBERT showed better differentiation of non-parallel passages, highlighting their potential for ancient text analysis.

Conclusion: Transformer language models, particularly E5 and AlephBERT, can significantly improve the detection of intertextual parallels in biblical studies, suggesting their utility in broader ancient language research.

Abstract: Identifying parallel passages in biblical Hebrew is foundational in biblical
scholarship for uncovering intertextual relationships. Traditional methods rely
on manual comparison, which is labor-intensive and prone to human error. This
study evaluates the potential of pre-trained transformer-based language models,
including E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in
the Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings
and Chronicles, I assessed each model's capability to generate word embeddings
that delineate parallel from non-parallel passages. Utilizing cosine similarity
and Wasserstein Distance measures, I found that E5 and AlephBERT show
significant promise, with E5 excelling in parallel detection and AlephBERT
demonstrating stronger non-parallel differentiation. These findings indicate
that pre-trained models can enhance the efficiency and accuracy of detecting
intertextual parallels in ancient texts, suggesting broader applications for
ancient language studies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [131] [Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring](https://arxiv.org/abs/2506.22437)
*Xinxin Sun,Peter Chang*

Main category: cs.CV

TL;DR: This paper introduces a physics-informed framework for accurate image alignment in structural health monitoring, overcoming limitations of traditional and lightweight feature detectors.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in crack monitoring for SHM, such as perspective distortion, occlusion, and low contrast, which are inadequately handled by existing feature detectors like SIFT, SURF, ORB, and BRISK.

Method: The study adapts the KAZE architecture using nonlinear anisotropic diffusion for crack-preserving scale space and incorporates RANSAC-based homography estimation to facilitate robust image alignment without training or extensive calibration.

Result: Validation on varied field conditions shows the framework reduces crack area and spine length errors by 70-90%, achieves sub-5% alignment error, and is suitable for UAV and mobile deployment.

Conclusion: The proposed method provides a scalable, unsupervised, interpretable, and computationally efficient approach to SHM image alignment, tailored to real-world challenges and eliminating reliance on conventional calibration-heavy techniques.

Abstract: Accurate image alignment is essential for monitoring crack evolution in
structural health monitoring (SHM), particularly under real-world conditions
involving perspective distortion, occlusion, and low contrast. However,
traditional feature detectors such as SIFT and SURF, which rely on
Gaussian-based scale spaces, tend to suppress high-frequency edges, making them
unsuitable for thin crack localization. Lightweight binary alternatives like
ORB and BRISK, while computationally efficient, often suffer from poor keypoint
repeatability on textured or shadowed surfaces. This study presents a
physics-informed alignment framework that adapts the open KAZE architecture to
SHM-specific challenges. By utilizing nonlinear anisotropic diffusion to
construct a crack-preserving scale space, and integrating RANSAC-based
homography estimation, the framework enables accurate geometric correction
without the need for training, parameter tuning, or prior calibration. The
method is validated on time-lapse images of masonry and concrete acquired via
handheld smartphone under varied field conditions, including shadow
interference, cropping, oblique viewing angles, and surface clutter. Compared
to classical detectors, the proposed framework reduces crack area and spine
length errors by up to 70 percent and 90 percent, respectively, while
maintaining sub-5 percent alignment error in key metrics. Unsupervised,
interpretable, and computationally lightweight, this approach supports scalable
deployment via UAVs and mobile platforms. By tailoring nonlinear scale-space
modeling to SHM image alignment, this work offers a robust and physically
grounded alternative to conventional techniques for tracking real-world crack
evolution.

</details>


### [132] [Counting with Confidence: Accurate Pest Monitoring in Water Traps](https://arxiv.org/abs/2506.22438)
*Xumin Gao,Mark Stevens,Grzegorz Cielniak*

Main category: cs.CV

TL;DR: This paper proposes a comprehensive method for evaluating pest counting confidence in images by integrating detection results and environmental conditions, achieving significant error reduction compared to baselines.


<details>
  <summary>Details</summary>
Motivation: Accurate pest population monitoring is essential for precision agriculture, but existing vision-based pest counting methods fail to assess reliability due to the absence of ground truth in real-world applications.

Method: The method combines a pest detection network with image quality, image complexity, and pest distribution assessments. It also includes a hypothesis-driven sensitivity analysis and an adaptive DBSCAN clustering algorithm to predict pest counting confidence using regression models.

Result: The approach reduces mean squared error (MSE) by 31.7% and improves R2 score by 15.2% compared to a baseline method on the pest counting confidence test set.

Conclusion: This study pioneers a method to comprehensively evaluate counting confidence in pest counting tasks and quantifies the influence of various factors on confidence, enhancing reliability in real-world applications.

Abstract: Accurate pest population monitoring and tracking their dynamic changes are
crucial for precision agriculture decision-making. A common limitation in
existing vision-based automatic pest counting research is that models are
typically evaluated on datasets with ground truth but deployed in real-world
scenarios without assessing the reliability of counting results due to the lack
of ground truth. To this end, this paper proposed a method for comprehensively
evaluating pest counting confidence in the image, based on information related
to counting results and external environmental conditions. First, a pest
detection network is used for pest detection and counting, extracting counting
result-related information. Then, the pest images undergo image quality
assessment, image complexity assessment, and pest distribution uniformity
assessment. And the changes in image clarity caused by stirring during image
acquisition are quantified by calculating the average gradient magnitude.
Notably, we designed a hypothesis-driven multi-factor sensitivity analysis
method to select the optimal image quality assessment and image complexity
assessment methods. And we proposed an adaptive DBSCAN clustering algorithm for
pest distribution uniformity assessment. Finally, the obtained information
related to counting results and external environmental conditions is input into
a regression model for prediction, resulting in the final pest counting
confidence. To the best of our knowledge, this is the first study dedicated to
comprehensively evaluating counting confidence in counting tasks, and
quantifying the relationship between influencing factors and counting
confidence through a model. Experimental results show our method reduces MSE by
31.7% and improves R2 by 15.2% on the pest counting confidence test set,
compared to the baseline built primarily on information related to counting
results.

</details>


### [133] [Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization](https://arxiv.org/abs/2506.22463)
*Weizhi Gao,Zhichao Hou,Junqi Yin,Feiyi Wang,Linyu Peng,Xiaorui Liu*

Main category: cs.CV

TL;DR: The paper introduces Modulated Diffusion (MoDiff), a novel framework to accelerate generative diffusion models through modulated quantization and error compensation, reducing quantization to 3 bits without performance loss.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the high computational costs and inefficiencies in iterative sampling of diffusion models, alongside the limitations of existing acceleration methods like caching and quantization in terms of quality and correctness.

Method: The authors developed MoDiff, which incorporates modulated quantization and error compensation to address inefficiencies, serving as a general framework for accelerating diffusion models. The approach integrates theoretical analysis and experimental validations.

Result: Experiments on datasets such as CIFAR-10 and LSUN show MoDiff can reduce quantization from 8 bits to 3 bits without a decline in post-training quantization performance, proving its efficacy.

Conclusion: MoDiff offers a robust and principled solution to accelerate diffusion models, inheriting and surpassing benefits of existing techniques, and is backed by theoretical and empirical evidence.

Abstract: Diffusion models have emerged as powerful generative models, but their high
computation cost in iterative sampling remains a significant bottleneck. In
this work, we present an in-depth and insightful study of state-of-the-art
acceleration techniques for diffusion models, including caching and
quantization, revealing their limitations in computation error and generation
quality. To break these limits, this work introduces Modulated Diffusion
(MoDiff), an innovative, rigorous, and principled framework that accelerates
generative modeling through modulated quantization and error compensation.
MoDiff not only inherents the advantages of existing caching and quantization
methods but also serves as a general framework to accelerate all diffusion
models. The advantages of MoDiff are supported by solid theoretical insight and
analysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate
that MoDiff significant reduces activation quantization from 8 bits to 3 bits
without performance degradation in post-training quantization (PTQ). Our code
implementation is available at https://github.com/WeizhiGao/MoDiff.

</details>


### [134] [ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction](https://arxiv.org/abs/2506.22498)
*Hao Liu,Yu Hu,Rakiba Rayhana,Ling Bai,Zheng Liu*

Main category: cs.CV

TL;DR: The study proposes using a set of four load cell sensors and a novel dual-stream Swin Transformer (ViFusionTST) for early prediction of bed-exit intent to prevent falls in healthcare environments.


<details>
  <summary>Details</summary>
Motivation: To address the inadequacy of existing bed alarms that trigger only after patients have already exited the bed, leading to falls and injuries in hospitals and long-term-care facilities.

Method: They used four low-cost load cells mounted under bed legs to capture sensor data, converted these signals into various image formats, and designed a novel model (ViFusionTST) that employs dual-stream Swin Transformer architecture for processing and fusing the data for classification.

Result: The method was validated on a 6-month real-world dataset from 95 beds, achieving an accuracy of 0.885 and an F1 score of 0.794, outperforming recent baselines in multiple metrics (F1, recall, accuracy, AUPRC).

Conclusion: Image-based fusion of load-sensor signals is a feasible and effective solution for real-time, privacy-preserving fall prevention in healthcare settings, demonstrating strong performance in predicting bed-exit intent.

Abstract: Bed-related falls remain a leading source of injury in hospitals and
long-term-care facilities, yet many commercial alarms trigger only after a
patient has already left the bed. We show that early bed-exit intent can be
predicted using only four low-cost load cells mounted under the bed legs. The
resulting load signals are first converted into a compact set of complementary
images: an RGB line plot that preserves raw waveforms and three texture maps -
recurrence plot, Markov transition field, and Gramian angular field - that
expose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin
Transformer that processes the line plot and texture maps in parallel and fuses
them through cross-attention to learn data-driven modality weights.
  To provide a realistic benchmark, we collected six months of continuous data
from 95 beds in a long-term-care facility. On this real-world dataset
ViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing
recent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.
The results demonstrate that image-based fusion of load-sensor signals for time
series classification is a practical and effective solution for real-time,
privacy-preserving fall prevention.

</details>


### [135] [Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data](https://arxiv.org/abs/2506.22499)
*Jiachao Liu,Pablo Guarda,Koichiro Niinuma,Sean Qian*

Main category: cs.CV

TL;DR: The paper introduces a new framework for estimating dynamic origin-destination (OD) demand in networks using a combination of satellite imagery and traditional traffic data for better accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the limitations of traffic data collected from sparse local detectors by integrating satellite imagery, which provides city-wide and consistent traffic information that includes both parking and moving vehicles.

Method: The approach develops a computer vision pipeline for vehicle detection and map matching from satellite images, creating link-level traffic density observations. A computational graph-based DODE model is then designed to calibrate dynamic network states by integrating both traditional sensor and satellite-derived data.

Result: Numerical tests demonstrate that integrating satellite-derived data with traditional methods significantly boosts accuracy, particularly for areas lacking local sensors. Real-world experiments show scalability for large city networks, confirming practical applicability.

Conclusion: The framework effectively improves traffic demand estimation and holds promise for deployment in urban settings across various scales. Sensitivity analysis highlights the importance of satellite data quality.

Abstract: This study presents a novel integrated framework for dynamic
origin-destination demand estimation (DODE) in multi-class mesoscopic network
models, leveraging high-resolution satellite imagery together with conventional
traffic data from local sensors. Unlike sparse local detectors, satellite
imagery offers consistent, city-wide road and traffic information of both
parking and moving vehicles, overcoming data availability limitations. To
extract information from imagery data, we design a computer vision pipeline for
class-specific vehicle detection and map matching, generating link-level
traffic density observations by vehicle class. Building upon this information,
we formulate a computational graph-based DODE model that calibrates dynamic
network states by jointly matching observed traffic counts and travel times
from local sensors with density measurements derived from satellite imagery. To
assess the accuracy and scalability of the proposed framework, we conduct a
series of numerical experiments using both synthetic and real-world data. The
results of out-of-sample tests demonstrate that supplementing traditional data
with satellite-derived density significantly improves estimation performance,
especially for links without local sensors. Real-world experiments also confirm
the framework's capability to handle large-scale networks, supporting its
potential for practical deployment in cities of varying sizes. Sensitivity
analysis further evaluates the impact of data quality related to satellite
imagery data.

</details>


### [136] [Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models](https://arxiv.org/abs/2506.22500)
*Weiyi Zhao,Xiaoyu Tan,Liang Liu,Sijia Li,Youwei Song,Xihe Qiu*

Main category: cs.CV

TL;DR: The paper introduces a synthetic dataset to improve multimodal large language models for detecting rule violations in surgical operating rooms, addressing their current visual-semantic inconsistencies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance patient safety in surgical environments by addressing the shortcomings of current multimodal large language models, which struggle with visual-semantic conflict issues in identifying safety violations.

Method: The authors generated a dataset of 34,000 synthetic images using diffusion models, along with 214 human-annotated images for gold-standard validation, to fine-tune and evaluate MLLMs' ability to detect rule violations visually.

Result: Fine-tuning MLLMs with the new dataset improved their detection of trained conflicts and generalization to new perspectives; however, they performed poorly on untrained entity types.

Conclusion: The study reveals that while the new dataset improves MLLMs' understanding of safety rule violations in operating rooms, further comprehensive training is needed for better generalizability.

Abstract: Surgical risk identification is critical for patient safety and reducing
preventable medical errors. While multimodal large language models (MLLMs) show
promise for automated operating room (OR) risk detection, they often exhibit
visual-semantic knowledge conflicts (VS-KC), failing to identify visual safety
violations despite understanding textual rules. To address this, we introduce a
dataset comprising over 34,000 synthetic images generated by diffusion models,
depicting operating room scenes containing entities that violate established
safety rules. These images were created to alleviate data scarcity and examine
MLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated
images that serve as a gold-standard reference for validation. This
comprehensive dataset, spanning diverse perspectives, stages, and
configurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC
significantly improves MLLMs' detection of trained conflict entities and
generalizes well to new viewpoints for these entities, but performance on
untrained entity types remains poor, highlighting learning specificity and the
need for comprehensive training. The main contributions of this work include:
(1) a data generation methodology tailored for rule-violation scenarios; (2)
the release of the OR-VSKC dataset and its associated benchmark as open-source
resources; and (3) an empirical analysis of violation-sensitive knowledge
consistency in representative MLLMs. The dataset and appendix are available at
https://github.com/zgg2577/VS-KC.

</details>


### [137] [How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?](https://arxiv.org/abs/2506.22501)
*Gautam Siddharth Kashyap,Manaswi Kulahara,Nipun Joshi,Usman Naseem*

Main category: cs.CV

TL;DR: The paper introduces SpatialNet-ViT, combining Vision Transformers with Multi-Task Learning for improved remote sensing classification.


<details>
  <summary>Details</summary>
Motivation: Current remote sensing studies are limited in generalization across tasks and datasets, creating a need for models capable of handling diverse classification challenges.

Method: The authors developed SpatialNet-ViT, integrating Vision Transformers, Multi-Task Learning, data augmentation, and transfer learning to enhance spatial and contextual understanding.

Result: The proposed model improves classification accuracy, robustness, and scalability for diverse datasets.

Conclusion: SpatialNet-ViT offers a comprehensive solution for enhancing remote sensing classification tasks by integrating advanced methodologies.

Abstract: Remote sensing datasets offer significant promise for tackling key
classification tasks such as land-use categorization, object presence
detection, and rural/urban classification. However, many existing studies tend
to focus on narrow tasks or datasets, which limits their ability to generalize
across various remote sensing classification challenges. To overcome this, we
propose a novel model, SpatialNet-ViT, leveraging the power of Vision
Transformers (ViTs) and Multi-Task Learning (MTL). This integrated approach
combines spatial awareness with contextual understanding, improving both
classification accuracy and scalability. Additionally, techniques like data
augmentation, transfer learning, and multi-task learning are employed to
enhance model robustness and its ability to generalize across diverse datasets

</details>


### [138] [What Makes a Dribble Successful? Insights From 3D Pose Tracking Data](https://arxiv.org/abs/2506.22503)
*Michiel Schepers,Pieter Robberechts,Jan Van Haaren,Jesse Davis*

Main category: cs.CV

TL;DR: The paper analyzes how pose tracking data can enhance understanding of dribble success in soccer, using features like player posture and movement.


<details>
  <summary>Details</summary>
Motivation: Previous reliance on 2D tracking data for understanding dribbles lacked insights into fine details like balance and orientation.

Method: Pose tracking data was utilized to derive novel features from 1,736 dribbles in the 2022/23 Champions League season for analysis.

Result: Pose-based features were found informative for predicting dribble success, improving model performance when combined with 2D data.

Conclusion: Adding pose data adds depth to evaluations of dribbles, advancing soccer analytics beyond traditional methods.

Abstract: Data analysis plays an increasingly important role in soccer, offering new
ways to evaluate individual and team performance. One specific application is
the evaluation of dribbles: one-on-one situations where an attacker attempts to
bypass a defender with the ball. While previous research has primarily relied
on 2D positional tracking data, this fails to capture aspects like balance,
orientation, and ball control, limiting the depth of current insights. This
study explores how pose tracking data (capturing players' posture and movement
in three dimensions) can improve our understanding of dribbling skills. We
extract novel pose-based features from 1,736 dribbles in the 2022/23 Champions
League season and evaluate their impact on dribble success. Our results
indicate that features capturing the attacker's balance and the alignment of
the orientation between the attacker and defender are informative for
predicting dribble success. Incorporating these pose-based features on top of
features derived from traditional 2D positional data leads to a measurable
improvement in model performance.

</details>


### [139] [Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection](https://arxiv.org/abs/2506.22504)
*Hassan Baker,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: The paper introduces Patch2Loc, an unsupervised machine learning method for detecting brain lesions from MRI scans by mapping normal patches to spatial brain locations and identifying abnormalities based on location prediction errors.


<details>
  <summary>Details</summary>
Motivation: Radiologists benefit from computer-aided diagnostics to enhance detection of brain lesions like tumors and malformations in MRI scans. Current supervised methods rely on annotated lesions, which may not always be feasible. Thus, an unsupervised approach that leverages normal MRI patches is needed.

Method: Patch2Loc trains a neural network to map normal patches from structural MRIs back to their spatial brain locations. During inference, abnormalities are identified by higher location prediction errors or variance, generating heatmaps for finer-grained segmentation.

Result: Patch2Loc was evaluated on T2-weighted MRI images (BraTS2021 and MSLUB datasets) and T1-weighted images (ATLAS and WMH datasets), showing its ability to segment abnormal brain tissues and outperform state-of-the-art unsupervised segmentation methods.

Conclusion: Patch2Loc provides an effective unsupervised solution for detecting brain abnormalities in MRI scans, offering improved segmentation performance over existing methods and enhancing computer-aided diagnostics for radiologists.

Abstract: Detecting brain lesions as abnormalities observed in magnetic resonance
imaging (MRI) is essential for diagnosis and treatment. In the search of
abnormalities, such as tumors and malformations, radiologists may benefit from
computer-aided diagnostics that use computer vision systems trained with
machine learning to segment normal tissue from abnormal brain tissue. While
supervised learning methods require annotated lesions, we propose a new
unsupervised approach (Patch2Loc) that learns from normal patches taken from
structural MRI. We train a neural network model to map a patch back to its
spatial location within a slice of the brain volume. During inference, abnormal
patches are detected by the relatively higher error and/or variance of the
location prediction. This generates a heatmap that can be integrated into
pixel-wise methods to achieve finer-grained segmentation. We demonstrate the
ability of our model to segment abnormal brain tissues by applying our approach
to the detection of tumor tissues in MRI on T2-weighted images from BraTS2021
and MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show
that it outperforms the state-of-the art in unsupervised segmentation. The
codebase for this work can be found on our
\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.

</details>


### [140] [Weakly Supervised Object Segmentation by Background Conditional Divergence](https://arxiv.org/abs/2506.22505)
*Hassan Baker,Matthew S. Emigh,Austin J. Brockmeier*

Main category: cs.CV

TL;DR: This paper introduces a weakly-supervised object segmentation method using image-wise labels, avoiding costly pixel-level annotations.


<details>
  <summary>Details</summary>
Motivation: The lack of labeled data for specialized domains (e.g., sonar or biomedical imaging) makes object segmentation a challenging task.

Method: The method uses weak supervision, image clustering, counterfactual background generation, and sample-based divergences for training a masking network without requiring adversarial critics or pretrained models.

Result: The proposed approach achieves success in side-scan and synthetic aperture sonar data while also demonstrating reasonable performance on natural images.

Conclusion: The study showcases a framework for weakly-supervised segmentation, bypassing the need for extensive manual annotation and adversarial models.

Abstract: As a computer vision task, automatic object segmentation remains challenging
in specialized image domains without massive labeled data, such as synthetic
aperture sonar images, remote sensing, biomedical imaging, etc. In any domain,
obtaining pixel-wise segmentation masks is expensive. In this work, we propose
a method for training a masking network to perform binary object segmentation
using weak supervision in the form of image-wise presence or absence of an
object of interest, which provides less information but may be obtained more
quickly from manual or automatic labeling. A key step in our method is that the
segmented objects can be placed into background-only images to create
realistic, images of the objects with counterfactual backgrounds. To create a
contrast between the original and counterfactual background images, we propose
to first cluster the background-only images, and then during learning create
counterfactual images that blend objects segmented from their original source
backgrounds to backgrounds chosen from a targeted cluster. One term in the
training loss is the divergence between these counterfactual images and the
real object images with backgrounds of the target cluster. The other term is a
supervised loss for background-only images. While an adversarial critic could
provide the divergence, we use sample-based divergences. We conduct experiments
on side-scan and synthetic aperture sonar in which our approach succeeds
compared to previous unsupervised segmentation baselines that were only tested
on natural images. Furthermore, to show generality we extend our experiments to
natural images, obtaining reasonable performance with our method that avoids
pretrained networks, generative networks, and adversarial critics. The basecode
for this work can be found at
\href{GitHub}{https://github.com/bakerhassan/WSOS}.

</details>


### [141] [FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment](https://arxiv.org/abs/2506.22509)
*Hang Xu,Jie Huang,Linjiang Huang,Dong Li,Yidi Liu,Feng Zhao*

Main category: cs.CV

TL;DR: The paper proposes a training-free Domain Noise Alignment (DNA) method to enhance domain adaptation for diffusion-based dense prediction (DDP) models by adjusting noise statistics in the diffusion process, demonstrating its effectiveness across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of domain adaptation for dense prediction models within the emerging framework of diffusion-based dense prediction, leveraging the noise modeling capability of diffusion models to handle domain shifts.

Method: The proposed training-free DNA approach adjusts noise statistics during the diffusion sampling process. It aligns target domain noise with source domain noise statistics (source-aware DA) or progressively uses high-confidence region statistics in source-free scenarios.

Result: The DNA method demonstrates effective domain adaptation improvement for DDP models across four standard dense prediction tasks.

Conclusion: This method provides a novel, efficient, training-free solution to endow diffusion-based dense prediction models with domain adaptation capabilities, addressing noise-induced domain shifts.

Abstract: Domain Adaptation(DA) for dense prediction tasks is an important topic, which
enhances the dense prediction model's performance when tested on its unseen
domain. Recently, with the development of Diffusion-based Dense Prediction
(DDP) models, the exploration of DA designs tailored to this framework is worth
exploring, since the diffusion model is effective in modeling the distribution
transformation that comprises domain information. In this work, we propose a
training-free mechanism for DDP frameworks, endowing them with DA capabilities.
Our motivation arises from the observation that the exposure bias (e.g., noise
statistics bias) in diffusion brings domain shift, and different domains in
conditions of DDP models can also be effectively captured by the noise
prediction statistics. Based on this, we propose a training-free Domain Noise
Alignment (DNA) approach, which alleviates the variations of noise statistics
to domain changes during the diffusion sampling process, thereby achieving
domain adaptation. Specifically, when the source domain is available, we
directly adopt the DNA method to achieve domain adaptation by aligning the
noise statistics of the target domain with those of the source domain. For the
more challenging source-free DA, inspired by the observation that regions
closer to the source domain exhibit higher confidence meeting variations of
sampling noise, we utilize the statistics from the high-confidence regions
progressively to guide the noise statistic adjustment during the sampling
process. Notably, our method demonstrates the effectiveness of enhancing the DA
capability of DDP models across four common dense prediction tasks. Code is
available at
\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.

</details>


### [142] [Lightning the Night with Generative Artificial Intelligence](https://arxiv.org/abs/2506.22511)
*Tingting Zhou,Feng Zhang,Haoyang Fu,Baoxiang Pan,Renhe Zhang,Feng Lu,Zhixin Yang*

Main category: cs.CV

TL;DR: This study introduces a generative diffusion model to retrieve nighttime visible light reflectance data, addressing the limitation of meteorological observations during darkness.


<details>
  <summary>Details</summary>
Motivation: Meteorological observations rely heavily on visible light reflectance data, yet nighttime observation remains impossible due to the lack of visible light.

Method: A generative diffusion model called Reflectance Diffusion (RefDiff) was developed using thermal infrared brightness temperature data from the Fengyun-4B geostationary satellite.

Result: RefDiff achieved high precision with an SSIM index of 0.90, especially for areas with complex clouds, and validated successful nighttime retrieval compared to VIIRS daytime data.

Conclusion: The research advances nighttime visible light reflectance retrieval, enabling continuous all-day meteorological monitoring and expanding its application potential.

Abstract: The visible light reflectance data from geostationary satellites is crucial
for meteorological observations and plays an important role in weather
monitoring and forecasting. However, due to the lack of visible light at night,
it is impossible to conduct continuous all-day weather observations using
visible light reflectance data. This study pioneers the use of generative
diffusion models to address this limitation. Based on the multi-band thermal
infrared brightness temperature data from the Advanced Geostationary Radiation
Imager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we
developed a high-precision visible light reflectance retrieval model, called
Reflectance Diffusion (RefDiff), which enables 0.47~\mu\mathrm{m},
0.65~\mu\mathrm{m}, and 0.825~\mu\mathrm{m} bands visible light reflectance
retrieval at night. Compared to the classical models, RefDiff not only
significantly improves accuracy through ensemble averaging but also provides
uncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,
with particularly significant improvements in areas with complex cloud
structures and thick clouds. The model's nighttime retrieval capability was
validated using VIIRS nighttime product, demonstrating comparable performance
to its daytime counterpart. In summary, this research has made substantial
progress in the ability to retrieve visible light reflectance at night, with
the potential to expand the application of nighttime visible light data.

</details>


### [143] [Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence](https://arxiv.org/abs/2506.22513)
*Aditya Sharma*

Main category: cs.CV

TL;DR: Develops an automated framework for fault detection in radiography using modified U-net and data augmentation, achieving effective defect detection with high performance.


<details>
  <summary>Details</summary>
Motivation: Address the lack of well-explained information for defect detection in radiography; enhance virtual defect increase techniques; ensure framework feasibility with NDE measurements.

Method: Utilized 223 CR images of airplane welds with virtual and standard data augmentation techniques. Trained a modified U-net for semantic fault segmentation and used NDE parameters to evaluate its performance.

Result: The approach achieved exceptional defect detection accuracy, with notable performance on a90/95 metrics, size error reduction, and low false call rates. Demonstrated fast image analysis speed.

Conclusion: Professional controllers believe the system shows promise as a reliable support tool in fault detection, adaptable across equipment and software limitations.

Abstract: This investigation attempts to create an automated framework for fault
detection and organization for usage in contemporary radiography, as per NDE
4.0. The review's goals are to address the lack of information that is
sufficiently explained, learn how to make the most of virtual defect increase,
and determine whether the framework is viable by using NDE measurements. As its
basic information source, the technique consists of compiling and categorizing
223 CR photographs of airplane welds. Information expansion systems, such as
virtual defect increase and standard increase, are used to work on the
preparation dataset. A modified U-net model is prepared using the improved data
to produce semantic fault division veils. To assess the effectiveness of the
model, NDE boundaries such as Case, estimating exactness, and misleading call
rate are used. Tiny a90/95 characteristics, which provide strong
differentiating evidence of flaws, reveal that the suggested approach achieves
exceptional awareness in defect detection. Considering a 90/95, size error, and
fake call rate in the weld area, the consolidated expansion approach clearly
wins. Due to the framework's fast derivation speed, large images can be broken
down efficiently and quickly. Professional controllers evaluate the transmitted
system in the field and believe that it has a guarantee as a support device in
the testing cycle, irrespective of particular equipment cut-off points and
programming resemblance.

</details>


### [144] [Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis](https://arxiv.org/abs/2506.22517)
*Subhadip Kumar*

Main category: cs.CV

TL;DR: This paper compares three advanced computer vision models (Yolov12, Yolov11, and RF-DETR) for damaged container detection, analyzing their mAP and precision using a dataset of 278 images. Results show mixed performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the safety and operational hazards posed by damaged containers through timely detection by leveraging state-of-the-art computer vision models.

Method: The study trains, validates, and tests three computer vision models (Yolov12, Yolov11, and RF-DETR) on a dataset of 278 annotated images. The models are evaluated based on mAP and precision metrics.

Result: Yolov11 and Yolov12 achieved an mAP@50 score of 81.9%, while RF-DETR scored 77.7%. However, RF-DETR performed better in detecting uncommon damaged containers with higher accuracy and confidence.

Conclusion: While Yolov11 and Yolov12 show higher mAP scores in general tasks, RF-DETR demonstrates superiority in accurately detecting rare or specific damages, making it more versatile for varied scenarios.

Abstract: Containers are an integral part of the logistics industry and act as a
barrier for cargo. A typical service life for a container is more than 20
years. However, overtime containers suffer various types of damage due to the
mechanical as well as natural factors. A damaged container is a safety hazard
for the employees handling it and a liability for the logistic company.
Therefore, a timely inspection and detection of the damaged container is a key
for prolonging service life as well as avoiding safety hazards. In this paper,
we will compare the performance of the damage detection by three
state-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.
We will use a dataset of 278 annotated images to train, validate and test the
model. We will compare the mAP and precision of the model. The objective of
this paper is to identify the model that is best suited for container damage
detection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%
compared to RF-DETR, which was 77.7%. However, while testing the model for
not-so-common damaged containers, the RF-DETR model outperformed the others
overall, exhibiting superiority to accurately detecting both damaged containers
as well as damage occurrences with high confidence.

</details>


### [145] [Preserve Anything: Controllable Image Synthesis with Object Preservation](https://arxiv.org/abs/2506.22531)
*Prasen Kumar Sharma,Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: The paper introduces 'Preserve Anything,' a novel approach for better controlled image synthesis, excelling in object preservation, semantic alignment, and scene control.


<details>
  <summary>Details</summary>
Motivation: Improve limitations in text-to-image (T2I) synthesis regarding object preservation, semantic consistency, and scene control for better image outcomes.

Method: Proposes an N-channel ControlNet combining object preservation, background guidance, and user control. It uses lighting and detail-retention modules and introduces a dataset for thorough evaluation.

Result: Achieves state-of-the-art performance with better fidelity (FID 15.26), semantic alignment (CLIP-S 32.85), and significant user study improvements over existing methods.

Conclusion: The method substantially enhances T2I generation, offering advanced control, fidelity, and aesthetics over benchmarks and existing techniques.

Abstract: We introduce \textit{Preserve Anything}, a novel method for controlled image
synthesis that addresses key limitations in object preservation and semantic
consistency in text-to-image (T2I) generation. Existing approaches often fail
(i) to preserve multiple objects with fidelity, (ii) maintain semantic
alignment with prompts, or (iii) provide explicit control over scene
composition. To overcome these challenges, the proposed method employs an
N-channel ControlNet that integrates (i) object preservation with size and
placement agnosticism, color and detail retention, and artifact elimination,
(ii) high-resolution, semantically consistent backgrounds with accurate
shadows, lighting, and prompt adherence, and (iii) explicit user control over
background layouts and lighting conditions. Key components of our framework
include object preservation and background guidance modules, enforcing lighting
consistency and a high-frequency overlay module to retain fine details while
mitigating unwanted artifacts. We introduce a benchmark dataset consisting of
240K natural images filtered for aesthetic quality and 18K 3D-rendered
synthetic images with metadata such as lighting, camera angles, and object
relationships. This dataset addresses the deficiencies of existing benchmarks
and allows a complete evaluation. Empirical results demonstrate that our method
achieves state-of-the-art performance, significantly improving feature-space
fidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining
competitive aesthetic quality. We also conducted a user study to demonstrate
the efficacy of the proposed work on unseen benchmark and observed a remarkable
improvement of $\sim25\%$, $\sim19\%$, $\sim13\%$, and $\sim14\%$ in terms of
prompt alignment, photorealism, the presence of AI artifacts, and natural
aesthetics over existing works.

</details>


### [146] [Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset](https://arxiv.org/abs/2506.22554)
*Vasu Agrawal,Akinniyi Akinyemi,Kathryn Alvero,Morteza Behrooz,Julia Buffalini,Fabio Maria Carlucci,Joy Chen,Junming Chen,Zhang Chen,Shiyang Cheng,Praveen Chowdary,Joe Chuang,Antony D'Avirro,Jon Daly,Ning Dong,Mark Duppenthaler,Cynthia Gao,Jeff Girard,Martin Gleize,Sahir Gomez,Hongyu Gong,Srivathsan Govindarajan,Brandon Han,Sen He,Denise Hernandez,Yordan Hristov,Rongjie Huang,Hirofumi Inaguma,Somya Jain,Raj Janardhan,Qingyao Jia,Christopher Klaiber,Dejan Kovachev,Moneish Kumar,Hang Li,Yilei Li,Pavel Litvin,Wei Liu,Guangyao Ma,Jing Ma,Martin Ma,Xutai Ma,Lucas Mantovani,Sagar Miglani,Sreyas Mohan,Louis-Philippe Morency,Evonne Ng,Kam-Woh Ng,Tu Anh Nguyen,Amia Oberai,Benjamin Peloquin,Juan Pino,Jovan Popovic,Omid Poursaeed,Fabian Prada,Alice Rakotoarison,Alexander Richard,Christophe Ropers,Safiyyah Saleem,Vasu Sharma,Alex Shcherbyna,Jia Shen,Jie Shen,Anastasis Stathopoulos,Anna Sun,Paden Tomasello,Tuan Tran,Arina Turkatenko,Bo Wan,Chao Wang,Jeff Wang,Mary Williamson,Carleigh Wood,Tao Xiang,Yilin Yang,Julien Yao,Chen Zhang,Jiemin Zhang,Xinyue Zhang,Jason Zheng,Pavlo Zhyzheria,Jan Zikes,Michael Zollhoefer*

Main category: cs.CV

TL;DR: The paper introduces a dataset and models for generating dyadic motion gestures and expressions for improving socially intelligent AI interactions.


<details>
  <summary>Details</summary>
Motivation: To enable socially intelligent AI by understanding and generating dyadic behavioral dynamics in face-to-face interactions.

Method: The authors introduce a large-scale dataset of face-to-face interactions and develop models to generate gesture and facial expressions aligned with speech and visual behavior.

Result: Their models generate multimodal dynamics like gestures and expressions using speech inputs (including LLM speech), integrated with 2D and 3D rendering, and adaptable expressivity.

Conclusion: This work advances human-AI interaction by enabling AI to show intuitive, emotionally adaptive, and semantically rich responses in interactive scenarios.

Abstract: Human communication involves a complex interplay of verbal and nonverbal
signals, essential for conveying meaning and achieving interpersonal goals. To
develop socially intelligent AI technologies, it is crucial to develop models
that can both comprehend and generate dyadic behavioral dynamics. To this end,
we introduce the Seamless Interaction Dataset, a large-scale collection of over
4,000 hours of face-to-face interaction footage from over 4,000 participants in
diverse contexts. This dataset enables the development of AI technologies that
understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,
telepresence experiences, and multimodal content analysis tools. We also
develop a suite of models that utilize the dataset to generate dyadic motion
gestures and facial expressions aligned with human speech. These models can
take as input both the speech and visual behavior of their interlocutors. We
present a variant with speech from an LLM model and integrations with 2D and 3D
rendering methods, bringing us closer to interactive virtual agents.
Additionally, we describe controllable variants of our motion models that can
adapt emotional responses and expressivity levels, as well as generating more
semantically-relevant gestures. Finally, we discuss methods for assessing the
quality of these dyadic motion models, which are demonstrating the potential
for more intuitive and responsive human-AI interactions.

</details>


### [147] [Recomposed realities: animating still images via patch clustering and randomness](https://arxiv.org/abs/2506.22556)
*Markus Juvonen,Samuli Siltanen*

Main category: cs.CV

TL;DR: The paper introduces a method for achieving animation and reconstruction of still images using patch-based techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to develop a technique to animate still images by creatively leveraging existing image datasets.

Method: The method involves clustering image patches with k-means, matching, and sampling from these clusters to reconstruct and animate a target image.

Result: This approach allows for conceptual differences between source and target images while maintaining local structural similarities.

Conclusion: The method succeeds in enabling reinterpretation of image data to create dynamic animations over mere replication of source data.

Abstract: We present a patch-based image reconstruction and animation method that uses
existing image data to bring still images to life through motion. Image patches
from curated datasets are grouped using k-means clustering and a new target
image is reconstructed by matching and randomly sampling from these clusters.
This approach emphasizes reinterpretation over replication, allowing the source
and target domains to differ conceptually while sharing local structures.

</details>


### [148] [Improving Token-based Object Detection with Video](https://arxiv.org/abs/2506.22562)
*Abhineet Singh,Nilanjan Ray*

Main category: cs.CV

TL;DR: This paper extends the Pix2Seq object detector to handle video sequences, offering a novel end-to-end solution for video object detection that outperforms current methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations in conventional video object detectors, such as the need for box sampling and heuristic-based post-processing, and to provide a scalable approach to video object detection.

Method: The method represents video objects as variable-length sequences of discrete tokens and perceives them as indivisible 3D boxes or tracklets. This approach eliminates box sampling and enables scalable processing by increasing video subsequence length.

Result: The proposed video detector consistently outperforms the baseline Pix2Seq static detector and demonstrates competitiveness with state-of-the-art video detectors, even under computational constraints.

Conclusion: The paper introduces an improved framework for video object detection that addresses key limitations in existing methods and provides scalability, signifying a step forward in video analysis technology.

Abstract: This paper improves upon the Pix2Seq object detector by extending it for
videos. In the process, it introduces a new way to perform end-to-end video
object detection that improves upon existing video detectors in two key ways.
First, by representing objects as variable-length sequences of discrete tokens,
we can succinctly represent widely varying numbers of video objects, with
diverse shapes and locations, without having to inject any localization cues in
the training process. This eliminates the need to sample the space of all
possible boxes that constrains conventional detectors and thus solves the dual
problems of loss sparsity during training and heuristics-based postprocessing
during inference. Second, it conceptualizes and outputs the video objects as
fully integrated and indivisible 3D boxes or tracklets instead of generating
image-specific 2D boxes and linking these boxes together to construct the video
object, as done in most conventional detectors. This allows it to scale
effortlessly with available computational resources by simply increasing the
length of the video subsequence that the network takes as input, even
generalizing to multi-object tracking if the subsequence can span the entire
video. We compare our video detector with the baseline Pix2Seq static detector
on several datasets and demonstrate consistent improvement, although with
strong signs of being bottlenecked by our limited computational resources. We
also compare it with several video detectors on UA-DETRAC to show that it is
competitive with the current state of the art even with the computational
bottleneck. We make our code and models publicly available.

</details>


### [149] [Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation](https://arxiv.org/abs/2506.22567)
*Shansong Wang,Zhecheng Jin,Mingzhe Hu,Mojtaba Safari,Feng Zhao,Chih-Wei Chang,Richard LJ Qiu,Justin Roper,David S. Yu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: MMKD-CLIP is a biomedical foundation model developed through multi-teacher knowledge distillation, using existing specialized models instead of large-scale raw data, and evaluated extensively across diverse tasks and datasets.


<details>
  <summary>Details</summary>
Motivation: The need for a unified and generalizable biomedical foundation model is hindered by the lack of large-scale biomedical data, heterogeneous image modalities, and fragmented standards.

Method: MMKD-CLIP uses a two-stage training process: (1) CLIP-style pretraining on over 2.9 million biomedical image-text pairs, and (2) feature-level distillation using 19.2 million feature pairs from teacher models.

Result: MMKD-CLIP surpasses state-of-the-art teacher models across 58 datasets, demonstrating robustness and high generalization across diverse biomedical domains and tasks.

Conclusion: Multi-teacher knowledge distillation is a practical and scalable method to develop effective biomedical foundation models despite real-world data scarcity.

Abstract: CLIP models pretrained on natural images with billion-scale image-text pairs
have demonstrated impressive capabilities in zero-shot classification,
cross-modal retrieval, and open-ended visual answering. However, transferring
this success to biomedicine is hindered by the scarcity of large-scale
biomedical image-text corpora, the heterogeneity of image modalities, and
fragmented data standards across institutions. These limitations hinder the
development of a unified and generalizable biomedical foundation model trained
from scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical
foundation model developed via Multiple Medical CLIP Knowledge Distillation.
Rather than relying on billion-scale raw data, MMKD-CLIP distills knowledge
from nine state-of-the-art domain-specific or generalist biomedical CLIP
models, each pretrained on millions of biomedical image-text pairs. Our
two-stage training pipeline first performs CLIP-style pretraining on over 2.9
million biomedical image-text pairs from 26 image modalities, followed by
feature-level distillation using over 19.2 million feature pairs extracted from
teacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,
encompassing over 10.8 million biomedical images across nine image modalities.
The evaluation spans six core task types: zero-shot classification, linear
probing, cross-modal retrieval, visual question answering, survival prediction,
and cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models
while demonstrating remarkable robustness and generalization across image
domains and task settings. These results underscore that multi-teacher
knowledge distillation is a scalable and effective paradigm for building
high-performing biomedical foundation models under the practical constraints of
real-world data availability.

</details>


### [150] [Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation](https://arxiv.org/abs/2506.22570)
*Chee Mei Ling,Thangarajah Akilan,Aparna Ravinda Phalke*

Main category: cs.CV

TL;DR: The paper presents an efficient agricultural semantic segmentation model using a novel DAS Conv module and strategic skip connections, outperforming baselines and showing comparable results to complex SOTA models.


<details>
  <summary>Details</summary>
Motivation: Improve accuracy and efficiency in agricultural image segmentation to aid crop management and resource optimization.

Method: Integration of Dual Atrous Separable (DAS) Convolution into DeepLabV3 framework with skip connections for enhanced spatial feature capture.

Result: The proposed model achieves comparable performance to SOTA transformer-based models while being computationally efficient, with a 66% improvement in complexity-performance trade-off.

Conclusion: The model offers an efficient, lightweight solution for remote sensing applications in agriculture, improving semantic segmentation quality and performance.

Abstract: Agricultural image semantic segmentation is a pivotal component of modern
agriculture, facilitating accurate visual data analysis to improve crop
management, optimize resource utilization, and boost overall productivity. This
study proposes an efficient image segmentation method for precision
agriculture, focusing on accurately delineating farmland anomalies to support
informed decision-making and proactive interventions. A novel Dual Atrous
Separable Convolution (DAS Conv) module is integrated within the
DeepLabV3-based segmentation framework. The DAS Conv module is meticulously
designed to achieve an optimal balance between dilation rates and padding size,
thereby enhancing model performance without compromising efficiency. The study
also incorporates a strategic skip connection from an optimal stage in the
encoder to the decoder to bolster the model's capacity to capture fine-grained
spatial features. Despite its lower computational complexity, the proposed
model outperforms its baseline and achieves performance comparable to highly
complex transformer-based state-of-the-art (SOTA) models on the Agriculture
Vision benchmark dataset. It achieves more than 66% improvement in efficiency
when considering the trade-off between model complexity and performance,
compared to the SOTA model. This study highlights an efficient and effective
solution for improving semantic segmentation in remote sensing applications,
offering a computationally lightweight model capable of high-quality
performance in agricultural imagery.

</details>


### [151] [LIGHT: Multi-Modal Text Linking on Historical Maps](https://arxiv.org/abs/2506.22589)
*Yijun Lin,Rhett Olson,Junhan Wu,Yao-Yi Chiang,Jerod Weinman*

Main category: cs.CV

TL;DR: The paper introduces LIGHT, a multi-modal approach to link fragmented text on historical maps by integrating linguistic, image, and geometric features, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of linking fragmented text on historical maps where geometric information is crucial and existing methods primarily focus on linguistic features, neglecting spatial relationships.

Method: The authors developed LIGHT, which integrates linguistic, image, and geometry-aware embedding features using a pretrained layout analysis model (LayoutLMv3). The approach uses a bi-directional learning strategy for enhanced sequence robustness.

Result: LIGHT outperformed existing techniques on the ICDAR 2024/2025 MapText Competition data, proving its efficacy in historical map text linking.

Conclusion: The study highlights the benefits of multi-modal approaches and geometry-aware embeddings in tackling the complex problem of linking text on historical maps.

Abstract: Text on historical maps provides valuable information for studies in history,
economics, geography, and other related fields. Unlike structured or
semi-structured documents, text on maps varies significantly in orientation,
reading order, shape, and placement. Many modern methods can detect and
transcribe text regions, but they struggle to effectively ``link'' the
recognized text fragments, e.g., determining a multi-word place name. Existing
layout analysis methods model word relationships to improve text understanding
in structured documents, but they primarily rely on linguistic features and
neglect geometric information, which is essential for handling map text. To
address these challenges, we propose LIGHT, a novel multi-modal approach that
integrates linguistic, image, and geometric features for linking text on
historical maps. In particular, LIGHT includes a geometry-aware embedding
module that encodes the polygonal coordinates of text regions to capture
polygon shapes and their relative spatial positions on an image. LIGHT unifies
this geometric information with the visual and linguistic token embeddings from
LayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal
information to predict the reading-order successor of each text instance
directly with a bi-directional learning strategy that enhances sequence
robustness. Experimental results show that LIGHT outperforms existing methods
on the ICDAR 2024/2025 MapText Competition data, demonstrating the
effectiveness of multi-modal learning for historical map text linking.

</details>


### [152] [BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data](https://arxiv.org/abs/2506.22591)
*Arunkumar Kannan,Martin A. Lindquist,Brian Caffo*

Main category: cs.CV

TL;DR: The paper introduces BrainMT, a hybrid deep learning framework for predicting phenotypic measures from fMRI data.


<details>
  <summary>Details</summary>
Motivation: Standard deep learning models struggle to capture the long-range spatial and temporal dependencies in fMRI data, hindering predictive performance.

Method: A two-stage hybrid framework: (1) the Mamba block for capturing global temporal interactions and (2) a transformer block for modeling spatial relationships using self-attention.

Result: BrainMT achieves state-of-the-art results on classification and regression tasks across two large-scale datasets, UKBioBank and Human Connectome Project.

Conclusion: BrainMT significantly advances the prediction of phenotypic measures from fMRI data, demonstrating superior performance over existing methods.

Abstract: Recent advances in deep learning have made it possible to predict phenotypic
measures directly from functional magnetic resonance imaging (fMRI) brain
volumes, sparking significant interest in the neuroimaging community. However,
existing approaches, primarily based on convolutional neural networks or
transformer architectures, often struggle to model the complex relationships
inherent in fMRI data, limited by their inability to capture long-range spatial
and temporal dependencies. To overcome these shortcomings, we introduce
BrainMT, a novel hybrid framework designed to efficiently learn and integrate
long-range spatiotemporal attributes in fMRI data. Our framework operates in
two stages: (1) a bidirectional Mamba block with a temporal-first scanning
mechanism to capture global temporal interactions in a computationally
efficient manner; and (2) a transformer block leveraging self-attention to
model global spatial relationships across the deep features processed by the
Mamba block. Extensive experiments on two large-scale public datasets,
UKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves
state-of-the-art performance on both classification (sex prediction) and
regression (cognitive intelligence prediction) tasks, outperforming existing
methods by a significant margin. Our code and implementation details will be
made publicly available at this
https://github.com/arunkumar-kannan/BrainMT-fMRI

</details>


### [153] [Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning](https://arxiv.org/abs/2506.22624)
*Zuyao You,Zuxuan Wu*

Main category: cs.CV

TL;DR: Seg-R1 introduces reinforcement learning techniques to enhance pixel-level reasoning and segmentation tasks for large multimodal models.


<details>
  <summary>Details</summary>
Motivation: Improve the pixel-level understanding of multimodal models like SAM2 in segmentation tasks using RL.

Method: Seg-R1 utilizes a reinforcement learning strategy called Group Relative Policy Optimization (GRPO) to guide multimodal models in generating prompts for segmentation output.

Result: Seg-R1 achieves competitive metrics like .873 S-measure on COD10K and strong zero-shot performances on tasks such as referring segmentation (71.4 cIoU on RefCOCOg) and reasoning segmentation (56.7 gIoU on ReasonSeg).

Conclusion: RL training significantly improves segmentation performance and generalization ability without requiring extensive supervision or model modification.

Abstract: We present Seg-R1, a preliminary exploration of using reinforcement learning
(RL) to enhance the pixel-level understanding and reasoning capabilities of
large multimodal models (LMMs). Starting with foreground segmentation tasks,
specifically camouflaged object detection (COD) and salient object detection
(SOD), our approach enables the LMM to generate point and bounding box prompts
in the next-token fashion, which are then used to guide SAM2 in producing
segmentation masks. We introduce Group Relative Policy Optimization (GRPO) into
the segmentation domain, equipping the LMM with pixel-level comprehension
through a carefully designed training strategy. Notably, Seg-R1 achieves
remarkable performance with purely RL-based training, achieving .873 S-measure
on COD10K without complex model modification. Moreover, we found that pure RL
training demonstrates strong open-world generalization. Despite being trained
solely on foreground segmentation image-mask pairs without text supervision,
Seg-R1 achieves impressive zero-shot performance on referring segmentation and
reasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on
ReasonSeg test, outperforming models fully supervised on these datasets.

</details>


### [154] [ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models](https://arxiv.org/abs/2506.22636)
*Sotirios Panagiotis Chytas,Miso Choi,Hyunwoo J. Kim,Vikas Singh*

Main category: cs.CV

TL;DR: The paper addresses hallucination issues in Vision Language Models (VLMs) by introducing a small, trainable module (ReCo) to combat the fading memory effect, demonstrating improved performance and compatibility with other hallucination reduction methods.


<details>
  <summary>Details</summary>
Motivation: Understanding and controlling hallucination in Vision Language Models, particularly their over-reliance on language leading to fading memory of visual input, and improving their grounding to visual data.

Method: The authors propose the ReCo module, a lightweight, trainable addition that leverages geometric algebra and relational compositions, designed to work seamlessly with existing VLMs without major architectural modifications.

Result: The ReCo module effectively reduces the fading memory effect and enhances performance on benchmarks for VLMs like InstructBLIP, LlaVA, and MiniGPT4. It also integrates well with other hallucination reduction methods, further improving results.

Conclusion: The study demonstrates that the ReCo module mitigates hallucinations in VLMs, making them more reliable while being compatible with existing advancements in this area.

Abstract: Vision Language Models (VLMs) show impressive capabilities in integrating and
reasoning with both visual and language data. But these models make mistakes. A
common finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,
generate plausible sounding text which is not grounded in the visual input, or
at worst, is contradictory. A growing consensus attributes this behavior to an
over-reliance on language -- especially as the generation progresses, the model
suffers from a ``fading memory effect'' with respect to the provided visual
input. We study mechanisms by which this behavior can be controlled.
Specifically, using ideas from geometric algebra and relational compositions,
we propose the addition of a small, trainable module (named ReCo) on top of any
VLM -- no other modification is needed. We show that such a lightweight module
is able to mitigate the fading memory effect on three of the most widely used
VLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on
multiple benchmarks. Additionally, we show that our module can be combined with
many of the other approaches for reducing hallucination where we achieve
improved results for each one.

</details>


### [155] [CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation](https://arxiv.org/abs/2506.22637)
*Haoxuan Wang,Zhenghao Zhao,Junyi Wu,Yuzhang Shang,Gaowen Liu,Yan Yan*

Main category: cs.CV

TL;DR: The paper proposes a framework called CaO$_2$ to address inconsistencies in diffusion-based dataset distillation, achieving improved accuracy on ImageNet.


<details>
  <summary>Details</summary>
Motivation: To tackle inefficiencies and inconsistencies in current diffusion-based dataset distillation methods, enhancing their alignment with evaluation objectives.

Method: CaO$_2$ consists of a two-stage framework: (1) probability-informed sample selection to address objective inconsistency and (2) refinement of latent representations to tackle condition mismatch.

Result: CaO$_2$ outperformed existing methods, setting state-of-the-art accuracy on ImageNet and subsets, with an average improvement of 2.3%.

Conclusion: Aligning distillation processes with evaluation objectives through the proposed framework improves dataset distillation efficiency, effectiveness, and overall accuracy.

Abstract: The recent introduction of diffusion models in dataset distillation has shown
promising potential in creating compact surrogate datasets for large,
high-resolution target datasets, offering improved efficiency and performance
over traditional bi-level/uni-level optimization methods. However, current
diffusion-based dataset distillation approaches overlook the evaluation process
and exhibit two critical inconsistencies in the distillation process: (1)
Objective Inconsistency, where the distillation process diverges from the
evaluation objective, and (2) Condition Inconsistency, leading to mismatches
between generated images and their corresponding conditions. To resolve these
issues, we introduce Condition-aware Optimization with Objective-guided
Sampling (CaO$_2$), a two-stage diffusion-based framework that aligns the
distillation process with the evaluation objective. The first stage employs a
probability-informed sample selection pipeline, while the second stage refines
the corresponding latent representations to improve conditional likelihood.
CaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,
surpassing the best-performing baselines by an average of 2.3% accuracy.

</details>


### [156] [3D Shape Generation: A Survey](https://arxiv.org/abs/2506.22678)
*Nicolas Caytuiro,Ivan Sipiran*

Main category: cs.CV

TL;DR: The paper surveys the state of the art in 3D shape generation, discussing shape representations, generative methods, evaluation protocols, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To provide a structured and comprehensive understanding of the advancements and challenges in the field of 3D shape generation for researchers and practitioners.

Method: The survey organizes its discussion around three main themes: categorization of 3D shape representations, a review of generative approaches, and a summary of evaluation metrics and datasets.

Result: The paper outlines key structural properties of different shape representations, reviews generative methods focusing on feedforward architectures, and summarizes methods for fidelity and diversity evaluation of 3D shapes.

Conclusion: The survey highlights open challenges in controllable and high-quality 3D shape generation and identifies directions for future research to advance the field.

Abstract: Recent advances in deep learning have significantly transformed the field of
3D shape generation, enabling the synthesis of complex, diverse, and
semantically meaningful 3D objects. This survey provides a comprehensive
overview of the current state of the art in 3D shape generation, organizing the
discussion around three core components: shape representations, generative
modeling approaches, and evaluation protocols. We begin by categorizing 3D
representations into explicit, implicit, and hybrid setups, highlighting their
structural properties, advantages, and limitations. Next, we review a wide
range of generation methods, focusing on feedforward architectures. We further
summarize commonly used datasets and evaluation metrics that assess fidelity,
diversity, and realism of generated shapes. Finally, we identify open
challenges and outline future research directions that could drive progress in
controllable, efficient, and high-quality 3D shape generation. This survey aims
to serve as a valuable reference for researchers and practitioners seeking a
structured and in-depth understanding of this rapidly evolving field.

</details>


### [157] [LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning](https://arxiv.org/abs/2506.22710)
*Jiang Yuan,JI Ma,Bo Wang,Guanzhou Ke,Weiming Hu*

Main category: cs.CV

TL;DR: The paper introduces a new lightweight blind super-resolution model, LightBSR, which enhances implicit degradation representation (IDR) discriminability while minimizing computational complexity.


<details>
  <summary>Details</summary>
Motivation: Current blind super-resolution models complicate the adaptation process, increasing computational overhead, while neglecting the importance of IDR discriminability.

Method: The authors use a knowledge distillation framework with degradation-prior-constrained contrastive learning in the teacher stage and feature alignment to transfer knowledge to the student model.

Result: LightBSR achieves high performance with significantly reduced computational complexity across various blind super-resolution tasks.

Conclusion: Improving IDR discriminability simplifies blind super-resolution models, enabling effective and efficient HR detail restoration.

Abstract: Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges
on extracting the implicit degradation representation (IDR) of the LR image and
adapting it to LR image features to guide HR detail restoration. Although
IDE-BSR has shown potential in dealing with noise interference and complex
degradations, existing methods ignore the importance of IDR discriminability
for BSR and instead over-complicate the adaptation process to improve effect,
resulting in a significant increase in the model's parameters and computations.
In this paper, we focus on the discriminability optimization of IDR and propose
a new powerful and lightweight BSR model termed LightBSR. Specifically, we
employ a knowledge distillation-based learning framework. We first introduce a
well-designed degradation-prior-constrained contrastive learning technique
during teacher stage to make the model more focused on distinguishing different
degradation types. Then we utilize a feature alignment technique to transfer
the degradation-related knowledge acquired by the teacher to the student for
practical inferencing. Extensive experiments demonstrate the effectiveness of
IDR discriminability-driven BSR model design. The proposed LightBSR can achieve
outstanding performance with minimal complexity across a range of blind SR
tasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.

</details>


### [158] [Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians](https://arxiv.org/abs/2506.22718)
*Jun-Jee Chao,Qingyuan Jiang,Volkan Isler*

Main category: cs.CV

TL;DR: This paper introduces a new method for joint part segmentation and motion estimation of articulated objects from point cloud sequences, utilizing a Gaussian-based representation instead of tracking point correspondences.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of analyzing articulated object motion when point clouds are arbitrary samples, making point correspondence-based methods ineffective, especially in cases with occlusions or asynchronous sensors.

Method: Represent the object using time-dependent 3D Gaussians characterized by rotations, translations, and scales. Part segmentation is achieved via correspondences between observed points and Gaussians, and motion estimation follows Gaussian transformations.

Result: The proposed method outperforms traditional point-correspondence-based methods, achieving a 13% improvement in part segmentation on occluded point cloud datasets, and demonstrates robustness in handling missing data.

Conclusion: The Gaussian-based approach improves articulated object motion analysis under challenging conditions such as occlusions or asynchronous data, offering significant advantages over state-of-the-art methods.

Abstract: Part segmentation and motion estimation are two fundamental problems for
articulated object motion analysis. In this paper, we present a method to solve
these two problems jointly from a sequence of observed point clouds of a single
articulated object. The main challenge in our problem setting is that the point
clouds are not assumed to be generated by a fixed set of moving points.
Instead, each point cloud in the sequence could be an arbitrary sampling of the
object surface at that particular time step. Such scenarios occur when the
object undergoes major occlusions, or if the dataset is collected using
measurements from multiple sensors asynchronously. In these scenarios, methods
that rely on tracking point correspondences are not appropriate. We present an
alternative approach based on a compact but effective representation where we
represent the object as a collection of simple building blocks modeled as 3D
Gaussians. We parameterize the Gaussians with time-dependent rotations,
translations, and scales that are shared across all time steps. With our
representation, part segmentation can be achieved by building correspondences
between the observed points and the Gaussians. Moreover, the transformation of
each point across time can be obtained by following the poses of the assigned
Gaussian (even when the point is not observed). Experiments show that our
method outperforms existing methods that solely rely on finding point
correspondences. Additionally, we extend existing datasets to emulate
real-world scenarios by considering viewpoint occlusions. We further
demonstrate that our method is more robust to missing points as compared to
existing approaches on these challenging datasets, even when some parts are
completely occluded in some time-steps. Notably, our part segmentation
performance outperforms the state-of-the-art method by 13% on point clouds with
occlusions.

</details>


### [159] [Deterministic Object Pose Confidence Region Estimation](https://arxiv.org/abs/2506.22720)
*Jinghao Wang,Zhang Li,Zi Wang,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: This paper proposes an efficient deterministic method to estimate 6D pose confidence regions by leveraging inductive conformal prediction and the implicit function theorem, resulting in more compact yet reliable regions.


<details>
  <summary>Details</summary>
Motivation: The need for accurately quantifying the reliability of estimated 6D poses has risen, but existing sampling-based methods face severe limitations such as slow sampling speed and overly inflated confidence regions.

Method: The authors use inductive conformal prediction to calibrate deterministically regressed 2D keypoint confidence regions and propagate them into 6D pose confidence regions using the implicit function theorem. This eliminates the need for slow sampling and ensembling methods.

Result: The proposed method achieves higher pose estimation accuracy with reduced computational time. On LineMOD Occlusion and SPEED datasets, it achieves compact confidence regions while maintaining coverage rates, reducing region volumes by up to 99.9% for rotations and 99.8% for translations.

Conclusion: The proposed approach provides an efficient solution for generating reliable and compact 6D pose confidence regions, making it better suited for practical deployment compared to current methods.

Abstract: 6D pose confidence region estimation has emerged as a critical direction,
aiming to perform uncertainty quantification for assessing the reliability of
estimated poses. However, current sampling-based approach suffers from critical
limitations that severely impede their practical deployment: 1) the sampling
speed significantly decreases as the number of samples increases. 2) the
derived confidence regions are often excessively large. To address these
challenges, we propose a deterministic and efficient method for estimating pose
confidence regions. Our approach uses inductive conformal prediction to
calibrate the deterministically regressed Gaussian keypoint distributions into
2D keypoint confidence regions. We then leverage the implicit function theorem
to propagate these keypoint confidence regions directly into 6D pose confidence
regions. This method avoids the inefficiency and inflated region sizes
associated with sampling and ensembling. It provides compact confidence regions
that cover the ground-truth poses with a user-defined confidence level.
Experimental results on the LineMOD Occlusion and SPEED datasets show that our
method achieves higher pose estimation accuracy with reduced computational
time. For the same coverage rate, our method yields significantly smaller
confidence region volumes, reducing them by up to 99.9\% for rotations and
99.8\% for translations. The code will be available soon.

</details>


### [160] [XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge](https://arxiv.org/abs/2506.22726)
*Yu Zhang,Xi Zhang,Hualin zhou,Xinyuan Chen,Shang Gao,Hong Jia,Jianfei Yang,Yuankai Qi,Tao Gu*

Main category: cs.CV

TL;DR: The paper introduces XTransfer, a method for efficient, modality-agnostic model transfer suitable for edge systems by addressing limitations in current deep learning practices for human sensing.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in current methods for deep learning on edge systems, such as limited sensor data, resource constraints, and challenges like modality shift, which hinder performance and adaptability across human sensing applications.

Method: XTransfer involves two novel steps: model repairing to address modality shift in pre-trained layers with minimal sensor data and layer recombining to identify and assemble optimal layers from existing models for compact, efficient designs.

Result: The proposed method achieves state-of-the-art performance across various human sensing datasets, reduces the need for sensor data, minimizes training costs, and improves edge system deployment efficiency.

Conclusion: XTransfer fills a crucial gap by enabling resource-efficient, adaptable, and high-performing model transfer for human sensing tasks on edge systems.

Abstract: Deep learning for human sensing on edge systems offers significant
opportunities for smart applications. However, its training and development are
hindered by the limited availability of sensor data and resource constraints of
edge systems. Current methods that rely on transferring pre-trained models
often encounter issues such as modality shift and high resource demands,
resulting in substantial accuracy loss, resource overhead, and poor
adaptability across different sensing applications. In this paper, we propose
XTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic
model transfer. XTransfer freely leverages single or multiple pre-trained
models and transfers knowledge across different modalities by (i) model
repairing that safely repairs modality shift in pre-trained model layers with
only few sensor data, and (ii) layer recombining that efficiently searches and
recombines layers of interest from source models in a layer-wise manner to
create compact models. We benchmark various baselines across diverse human
sensing datasets spanning different modalities. Comprehensive results
demonstrate that XTransfer achieves state-of-the-art performance on human
sensing tasks while significantly reducing the costs of sensor data collection,
model training, and edge deployment.

</details>


### [161] [UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments](https://arxiv.org/abs/2506.22736)
*Dayong Su,Yafei Zhang,Huafeng Li,Jinxing Li,Yu Liu*

Main category: cs.CV

TL;DR: UniFuse is a general framework for multimodal medical image fusion capable of handling misaligned or degraded images by jointly optimizing alignment, restoration, and fusion.


<details>
  <summary>Details</summary>
Motivation: Multimodal medical image fusion struggles with degraded or misaligned input images due to reliance on high-quality, pixel-aligned images.

Method: The paper introduces UniFuse, which integrates a degradation-aware prompt learning module, Spatial Mamba encoding, and the ALSN-based Universal Feature Restoration & Fusion module within an all-in-one framework.

Result: UniFuse achieves significant improvements in fusion quality compared to existing staged approaches across multiple datasets.

Conclusion: By unifying alignment, restoration, and fusion processes, UniFuse offers a robust method for degraded and misaligned medical image fusion.

Abstract: Current multimodal medical image fusion typically assumes that source images
are of high quality and perfectly aligned at the pixel level. Its effectiveness
heavily relies on these conditions and often deteriorates when handling
misaligned or degraded medical images. To address this, we propose UniFuse, a
general fusion framework. By embedding a degradation-aware prompt learning
module, UniFuse seamlessly integrates multi-directional information from input
images and correlates cross-modal alignment with restoration, enabling joint
optimization of both tasks within a unified framework. Additionally, we design
an Omni Unified Feature Representation scheme, which leverages Spatial Mamba to
encode multi-directional features and mitigate modality differences in feature
alignment. To enable simultaneous restoration and fusion within an All-in-One
configuration, we propose a Universal Feature Restoration & Fusion module,
incorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA
principles. By leveraging ALSN's adaptive feature representation along with
degradation-type guidance, we enable joint restoration and fusion within a
single-stage framework. Compared to staged approaches, UniFuse unifies
alignment, restoration, and fusion within a single framework. Experimental
results across multiple datasets demonstrate the method's effectiveness and
significant advantages over existing approaches.

</details>


### [162] [Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds](https://arxiv.org/abs/2506.22749)
*Yun Zhang,Feifan Chen,Na Li,Zhiwei Guo,Xu Wang,Fen Miao,Sam Kwong*

Main category: cs.CV

TL;DR: This paper presents a deep learning-based method for up-sampling colored point clouds to improve their geometry and attributes, significantly outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address the need for generating large-scale and denser colored point clouds for realistic and immersive 3D applications.

Method: The authors propose a Joint Geometry and Attribute Up-sampling (JGAU) framework consisting of networks for geometry and attribute up-sampling, supported by coarse attribute up-sampling methods and an attribute enhancement module.

Result: The proposed method yields superior performance, achieving PSNR improvements of 2.32 to 2.47 decibels compared to state-of-the-art methods across four up-sampling rates.

Conclusion: The JGAU framework effectively enhances the quality of up-sampled colored point clouds, leveraging intrinsic geometry and attribute patterns for significant gains in performance.

Abstract: Colored point cloud, which includes geometry and attribute components, is a
mainstream representation enabling realistic and immersive 3D applications. To
generate large-scale and denser colored point clouds, we propose a deep
learning-based Joint Geometry and Attribute Up-sampling (JGAU) method that
learns to model both geometry and attribute patterns while leveraging spatial
attribute correlations. First, we establish and release a large-scale dataset
for colored point cloud up-sampling called SYSU-PCUD, containing 121
large-scale colored point clouds with diverse geometry and attribute
complexities across six categories and four sampling rates. Second, to improve
the quality of up-sampled point clouds, we propose a deep learning-based JGAU
framework that jointly up-samples geometry and attributes. It consists of a
geometry up-sampling network and an attribute up-sampling network, where the
latter leverages the up-sampled auxiliary geometry to model neighborhood
correlations of the attributes. Third, we propose two coarse attribute
up-sampling methods, Geometric Distance Weighted Attribute Interpolation
(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate
coarse up-sampled attributes for each point. Then, an attribute enhancement
module is introduced to refine these up-sampled attributes and produce
high-quality point clouds by further exploiting intrinsic attribute and
geometry patterns. Extensive experiments show that the Peak Signal-to-Noise
Ratio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10
decibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,
8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art
methods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28
decibels, and 2.11 decibels at these four up-sampling rates, demonstrating
significant improvement.

</details>


### [163] [Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography](https://arxiv.org/abs/2506.22753)
*Jianing Zhang,Jiayi Zhu,Feiyu Ji,Xiaokang Yang,Xiaoyun Yuan*

Main category: cs.CV

TL;DR: This paper introduces a novel method using pre-trained models to improve imaging using metalenses, offering tunable trade-offs between image quality and fidelity, and demonstrating its effectiveness with a new MetaCamera.


<details>
  <summary>Details</summary>
Motivation: Metalenses are compact but face challenges such as optical degradation, complicated restoration, and undesirable artifacts in computational imaging, which current methods fail to address effectively without requiring extensive resources or calibration.

Method: The proposed method uses a 'Degradation-Modeled Multipath Diffusion' framework, leveraging pre-trained image priors and pseudo data augmentation. It balances image quality through positive, neutral, and negative prompts and uses a tunable decoder for fidelity-quality trade-off. A spatial degradation-aware attention (SVDA) module handles optical and sensor degradation.

Result: The approach outperforms state-of-the-art methods, achieving both high-fidelity and sharp image reconstructions. A custom-built millimeter-scale MetaCamera validates its real-world performance.

Conclusion: The framework successfully addresses metalens-specific challenges, offering a practical solution for controlled, high-quality image reconstruction and setting a new standard in computational imaging with metalenses.

Abstract: Metalenses offer significant potential for ultra-compact computational
imaging but face challenges from complex optical degradation and computational
restoration difficulties. Existing methods typically rely on precise optical
calibration or massive paired datasets, which are non-trivial for real-world
imaging systems. Furthermore, a lack of control over the inference process
often results in undesirable hallucinated artifacts. We introduce
Degradation-Modeled Multipath Diffusion for tunable metalens photography,
leveraging powerful natural image priors from pretrained models instead of
large datasets. Our framework uses positive, neutral, and negative-prompt paths
to balance high-frequency detail generation, structural fidelity, and
suppression of metalens-specific degradation, alongside \textit{pseudo} data
augmentation. A tunable decoder enables controlled trade-offs between fidelity
and perceptual quality. Additionally, a spatially varying degradation-aware
attention (SVDA) module adaptively models complex optical and sensor-induced
degradation. Finally, we design and build a millimeter-scale MetaCamera for
real-world validation. Extensive results show that our approach outperforms
state-of-the-art methods, achieving high-fidelity and sharp image
reconstruction. More materials: https://dmdiff.github.io/.

</details>


### [164] [RoboPearls: Editable Video Simulation for Robot Manipulation](https://arxiv.org/abs/2506.22756)
*Tao Tang,Likui Zhang,Youpeng Wen,Kaidong Zhang,Jia-Wang Bian,xia zhou,Tianyi Yan,Kun Zhan,Peng Jia,Hefeng Wu,Liang Lin,Xiaodan Liang*

Main category: cs.CV

TL;DR: Introducing RoboPearls, a simulation framework leveraging 3D Gaussian Splatting and advanced AI tools for scalable and realistic robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: To overcome data acquisition limitations and the sim-to-real gap in robotic manipulation learning.

Method: Development of RoboPearls, utilizing 3D Gaussian Splatting, Incremental Semantic Distillation, 3D-NNFM Loss, large language models (LLMs), and vision-language models (VLMs).

Result: Extensive experiments across multiple datasets and environments show satisfactory simulation performance.

Conclusion: RoboPearls proves effective for enhancing robotic manipulation learning, addressing scalability and realism challenges.

Abstract: The development of generalist robot manipulation policies has seen
significant progress, driven by large-scale demonstration data across diverse
environments. However, the high cost and inefficiency of collecting real-world
demonstrations hinder the scalability of data acquisition. While existing
simulation platforms enable controlled environments for robotic learning, the
challenge of bridging the sim-to-real gap remains. To address these challenges,
we propose RoboPearls, an editable video simulation framework for robotic
manipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the
construction of photo-realistic, view-consistent simulations from demonstration
videos, and supports a wide range of simulation operators, including various
object manipulations, powered by advanced modules like Incremental Semantic
Distillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by
incorporating large language models (LLMs), RoboPearls automates the simulation
production process in a user-friendly manner through flexible command
interpretation and execution. Furthermore, RoboPearls employs a vision-language
model (VLM) to analyze robotic learning issues to close the simulation loop for
performance enhancement. To demonstrate the effectiveness of RoboPearls, we
conduct extensive experiments on multiple datasets and scenes, including
RLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which
demonstrate our satisfactory simulation performance.

</details>


### [165] [VSRM: A Robust Mamba-Based Framework for Video Super-Resolution](https://arxiv.org/abs/2506.22762)
*Dinh Phu Tran,Dao Duy Hung,Daeyoung Kim*

Main category: cs.CV

TL;DR: This paper introduces VSRM, a novel video super-resolution framework leveraging Mamba's strengths for improved long-range spatio-temporal feature extraction and performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address limitations of current CNNs and Transformers for video super-resolution tasks, such as local receptive fields or high computational complexity, by leveraging Mamba's strengths.

Method: The method involves designing novel blocks (Spatial-to-Temporal Mamba and Temporal-to-Spatial Mamba) for extracting features, a Deformable Cross-Mamba Alignment module for dynamic frame alignment, and a Frequency Charbonnier-like loss for enhancing reconstruction quality.

Result: VSRM achieves state-of-the-art performance across various video super-resolution benchmarks, demonstrating its effectiveness.

Conclusion: The proposed framework establishes itself as a foundational approach for future research in video super-resolution, offering efficient and high-quality outcomes.

Abstract: Video super-resolution remains a major challenge in low-level vision tasks.
To date, CNN- and Transformer-based methods have delivered impressive results.
However, CNNs are limited by local receptive fields, while Transformers
struggle with quadratic complexity, posing challenges for processing long
sequences in VSR. Recently, Mamba has drawn attention for its long-sequence
modeling, linear complexity, and large receptive fields. In this work, we
propose VSRM, a novel \textbf{V}ideo \textbf{S}uper-\textbf{R}esolution
framework that leverages the power of \textbf{M}amba. VSRM introduces
Spatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract
long-range spatio-temporal features and enhance receptive fields efficiently.
To better align adjacent frames, we propose Deformable Cross-Mamba Alignment
module. This module utilizes a deformable cross-mamba mechanism to make the
compensation stage more dynamic and flexible, preventing feature distortions.
Finally, we minimize the frequency domain gaps between reconstructed and
ground-truth frames by proposing a simple yet effective Frequency
Charbonnier-like loss that better preserves high-frequency content and enhances
visual quality. Through extensive experiments, VSRM achieves state-of-the-art
results on diverse benchmarks, establishing itself as a solid foundation for
future research.

</details>


### [166] [PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection](https://arxiv.org/abs/2506.22783)
*Oguzhan Baser,Ahmet Ege Tanriverdi,Sriram Vishwanath,Sandeep P. Chinchali*

Main category: cs.CV

TL;DR: The paper presents PhonemeFake (PF), a realistic deepfake (DF) attack targeting speech, which challenges current human perception and detection models.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake datasets fail to accurately mimic real-world attacks and deceive human perception, indicating an urgent need for more realistic attack vectors.

Method: The study introduces PhonemeFake (PF), a data set and model that uses language reasoning to manipulate critical speech segments, along with an open-source bilevel DF segment detection model for adaptive computation.

Result: PhonemeFake reduces human perceptibility by 42%, decreases detection benchmark accuracy by 94%, and the new detection model improves efficiency by reducing EER by 91% while speeding computation by up to 90%.

Conclusion: PhonemeFake provides a scalable and effective deepfake attack and detection framework, outperforming existing models with enhanced accuracy, speed, and precise localization. The proposed tools are open-sourced for community use.

Abstract: Deepfake (DF) attacks pose a growing threat as generative models become
increasingly advanced. However, our study reveals that existing DF datasets
fail to deceive human perception, unlike real DF attacks that influence public
discourse. It highlights the need for more realistic DF attack vectors. We
introduce PhonemeFake (PF), a DF attack that manipulates critical speech
segments using language reasoning, significantly reducing human perception by
up to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF
dataset on HuggingFace and open-source bilevel DF segment detection model that
adaptively prioritizes compute on manipulated regions. Our extensive
experiments across three known DF datasets reveal that our detection model
reduces EER by 91% while achieving up to 90% speed-up, with minimal compute
overhead and precise localization beyond existing models as a scalable
solution.

</details>


### [167] [Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching](https://arxiv.org/abs/2506.22784)
*Yu Han,Zhiwei Huang,Yanting Zhang,Fangjun Ding,Shen Cai,Rui Fan*

Main category: cs.CV

TL;DR: The paper proposes a detector-free framework for point-pixel registration between LiDAR point clouds and camera images, addressing challenges of sparsity and noise in single-frame LiDAR data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenges of modality gaps and sparsity in point-pixel registration between LiDAR point clouds and camera images, which are essential for autonomous driving and robotic perception.

Method: The method involves projecting LiDAR intensity maps into 2D views and using an attention-based detector-free matching network with a repeatability scoring mechanism to enhance reliability in sparse LiDAR settings.

Result: The proposed approach achieves state-of-the-art performance on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks, outperforming methods that rely on multi-frame accumulation.

Conclusion: The method effectively bridges the modality gap and addresses sparsity issues, offering a robust and reliable solution for single-frame LiDAR point-pixel registration.

Abstract: Point-pixel registration between LiDAR point clouds and camera images is a
fundamental yet challenging task in autonomous driving and robotic perception.
A key difficulty lies in the modality gap between unstructured point clouds and
structured images, especially under sparse single-frame LiDAR settings.
Existing methods typically extract features separately from point clouds and
images, then rely on hand-crafted or learned matching strategies. This separate
encoding fails to bridge the modality gap effectively, and more critically,
these methods struggle with the sparsity and noise of single-frame LiDAR, often
requiring point cloud accumulation or additional priors to improve reliability.
Inspired by recent progress in detector-free matching paradigms (e.g.
MatchAnything), we revisit the projection-based approach and introduce the
detector-free framework for direct point-pixel matching between LiDAR and
camera views. Specifically, we project the LiDAR intensity map into a 2D view
from the LiDAR perspective and feed it into an attention-based detector-free
matching network, enabling cross-modal correspondence estimation without
relying on multi-frame accumulation. To further enhance matching reliability,
we introduce a repeatability scoring mechanism that acts as a soft visibility
prior. This guides the network to suppress unreliable matches in regions with
low intensity variation, improving robustness under sparse input. Extensive
experiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that
our method achieves state-of-the-art performance, outperforming prior
approaches on nuScenes (even those relying on accumulated point clouds),
despite using only single-frame LiDAR.

</details>


### [168] [RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors](https://arxiv.org/abs/2506.22800)
*Sicong Du,Jiarun Liu,Qifeng Chen,Hao-Xiang Chen,Tai-Jiang Mu,Sheng Yang*

Main category: cs.CV

TL;DR: The paper introduces RGE-GS, a framework that improves road scene reconstruction by combining diffusion-based generation and reward-guided Gaussian integration, achieving superior results without compromising efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing systems often struggle with incomplete road structure scanning during single-pass driving, hampering accurate reconstruction essential for sensor simulation and regression of driving actions.

Method: RGE-GS incorporates a reward network to prioritize consistent patterns and applies a differentiated training strategy tuned to scene convergence metrics for enhanced Gaussian optimization and spatial stability.

Result: RGE-GS outperformed baseline methods in reconstruction quality across evaluations using public datasets.

Conclusion: The proposed framework effectively addresses limitations of current methods, ensuring better reconstruction quality and improving spatial stability during road scene generation, with plans for public code release soon.

Abstract: A single-pass driving clip frequently results in incomplete scanning of the
road structure, making reconstructed scene expanding a critical requirement for
sensor simulators to effectively regress driving actions. Although contemporary
3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction
quality, their direct extension through the integration of diffusion priors
often introduces cumulative physical inconsistencies and compromises training
efficiency. To address these limitations, we present RGE-GS, a novel expansive
reconstruction framework that synergizes diffusion-based generation with
reward-guided Gaussian integration. The RGE-GS framework incorporates two key
innovations: First, we propose a reward network that learns to identify and
prioritize consistently generated patterns prior to reconstruction phases,
thereby enabling selective retention of diffusion outputs for spatial
stability. Second, during the reconstruction process, we devise a
differentiated training strategy that automatically adjust Gaussian
optimization progress according to scene converge metrics, which achieving
better convergence than baseline methods. Extensive evaluations of publicly
available datasets demonstrate that RGE-GS achieves state-of-the-art
performance in reconstruction quality. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version
incorporating reviewer suggestions will be updated soon.)

</details>


### [169] [Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding](https://arxiv.org/abs/2506.22803)
*Nuoye Xiong,Anqi Dong,Ning Wang,Cong Hua,Guangming Zhu,Mei Lin,Peiyi Shen,Liang Zhang*

Main category: cs.CV

TL;DR: This paper proposes CBM-HNMU to improve model interpretability and accuracy by refining concepts and incorporating them back into black-box models.


<details>
  <summary>Details</summary>
Motivation: Deep learning advancements have made models more complex and less interpretable, limiting understanding and interventions beyond sample-level explanations.

Method: CBM-HNMU identifies and refines detrimental concepts using global gradient contributions and distills corrections back into black-box models via a concept bottleneck approach.

Result: CBM-HNMU improved the accuracy of CNN and transformer-based models by up to 2.64% on benchmark datasets such as CIFAR-10 and CIFAR-100.

Conclusion: CBM-HNMU offers a novel framework for enhancing interpretability and performance in deep learning by addressing and refining detrimental conceptual reasoning effectively.

Abstract: Recent advances in deep learning have led to increasingly complex models with
deeper layers and more parameters, reducing interpretability and making their
decisions harder to understand. While many methods explain black-box reasoning,
most lack effective interventions or only operate at sample-level without
modifying the model itself. To address this, we propose the Concept Bottleneck
Model for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).
CBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable
framework to approximate black-box reasoning and communicate conceptual
understanding. Detrimental concepts are automatically identified and refined
(removed/replaced) based on global gradient contributions. The modified CBM
then distills corrected knowledge back into the black-box model, enhancing both
interpretability and accuracy. We evaluate CBM-HNMU on various CNN and
transformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,
and CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum
increase in average accuracy across 1.03%. Source code is available at:
https://github.com/XiGuaBo/CBM-HNMU.

</details>


### [170] [Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate](https://arxiv.org/abs/2506.22806)
*Byung Hyun Lee,Sungjin Lim,Seunggyu Lee,Dong Un Kang,Se Young Chun*

Main category: cs.CV

TL;DR: The paper proposes a new framework called Concept Pinpoint Eraser (CPE) to improve concept erasure in text-to-image diffusion models while preserving unrelated concepts and ensuring robustness against adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: There is growing concern over the misuse of text-to-image diffusion models for generating inappropriate or trademarked concepts, making efficient concept erasure methods essential to address ethical and legal challenges.

Method: The proposed CPE framework employs nonlinear Residual Attention Gates (ResAGs) and attention anchoring loss to selectively erase target concepts while maintaining unrelated concepts. It uses an iterative adversarial training process with learnable text embeddings to enhance robustness.

Result: Experiments show that CPE outperforms existing concept erasure methods by effectively removing target concepts, preserving a wide range of unrelated concepts, and demonstrating robustness against adversarial attack prompts.

Conclusion: The CPE framework offers a significant improvement for concept erasure in diffusion models, maintaining a balanced preservation of unrelated features while addressing robustness challenges effectively.

Abstract: Remarkable progress in text-to-image diffusion models has brought a major
concern about potentially generating images on inappropriate or trademarked
concepts. Concept erasing has been investigated with the goals of deleting
target concepts in diffusion models while preserving other concepts with
minimal distortion. To achieve these goals, recent concept erasing methods
usually fine-tune the cross-attention layers of diffusion models. In this work,
we first show that merely updating the cross-attention layers in diffusion
models, which is mathematically equivalent to adding \emph{linear} modules to
weights, may not be able to preserve diverse remaining concepts. Then, we
propose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding
\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or
cut) target concepts while safeguarding remaining concepts from broad
distributions by employing an attention anchoring loss to prevent the
forgetting. Moreover, we adversarially train CPE with ResAG and learnable text
embeddings in an iterative manner to maximize erasing performance and enhance
robustness against adversarial attacks. Extensive experiments on the erasure of
celebrities, artistic styles, and explicit contents demonstrated that the
proposed CPE outperforms prior arts by keeping diverse remaining concepts while
deleting the target concepts with robustness against attack prompts. Code is
available at https://github.com/Hyun1A/CPE

</details>


### [171] [FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition](https://arxiv.org/abs/2506.22807)
*Yueyang Li,Shengyu Gong,Weiming Zeng,Nizhuan Wang,Wai Ting Siok*

Main category: cs.CV

TL;DR: The paper introduces FreqDGT, a new EEG-based method leveraging advanced techniques like frequency-adaptive processing, dynamic graph learning, and multi-scale temporal disentanglement to improve cross-subject emotion recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of cross-subject variability in EEG-based emotion recognition, which stems from individual differences in cognitive traits and emotional responses.

Method: The authors propose FreqDGT, comprising: Frequency-Adaptive Processing (FAP) for emotion-relevant frequency bands, Adaptive Dynamic Graph Learning (ADGL) for customized brain connectivity, and a Multi-Scale Temporal Disentanglement Network (MTDN) for temporal dynamics and cross-subject robustness.

Result: Experiments confirm that FreqDGT improves cross-subject emotion recognition accuracy, outperforming existing methods while proving resilience to individual differences.

Conclusion: The study validates FreqDGT's integrated approach, demonstrating its potential for robust and accurate affective brain-computer interfaces.

Abstract: Electroencephalography (EEG) serves as a reliable and objective signal for
emotion recognition in affective brain-computer interfaces, offering unique
advantages through its high temporal resolution and ability to capture
authentic emotional states that cannot be consciously controlled. However,
cross-subject generalization remains a fundamental challenge due to individual
variability, cognitive traits, and emotional responses. We propose FreqDGT, a
frequency-adaptive dynamic graph transformer that systematically addresses
these limitations through an integrated framework. FreqDGT introduces
frequency-adaptive processing (FAP) to dynamically weight emotion-relevant
frequency bands based on neuroscientific evidence, employs adaptive dynamic
graph learning (ADGL) to learn input-specific brain connectivity patterns, and
implements multi-scale temporal disentanglement network (MTDN) that combines
hierarchical temporal transformers with adversarial feature disentanglement to
capture both temporal dynamics and ensure cross-subject robustness.
Comprehensive experiments demonstrate that FreqDGT significantly improves
cross-subject emotion recognition accuracy, confirming the effectiveness of
integrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical
modeling while ensuring robustness to individual differences. The code is
available at https://github.com/NZWANG/FreqDGT.

</details>


### [172] [Efficient Multi-Crop Saliency Partitioning for Automatic Image Cropping](https://arxiv.org/abs/2506.22814)
*Andrew Hamara,Andrew C. Freeman*

Main category: cs.CV

TL;DR: The paper improves image cropping by creating a method to extract multiple visually salient areas efficiently while preserving composition.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggled with identifying multiple disjoint areas in image cropping while maintaining visual importance.

Method: The algorithm extends a Fixed Aspect Ratio Cropping method, allowing efficient linear-time extraction of non-overlapping areas by dynamically managing attention thresholds and reducing redundancy.

Result: It successfully identifies multiple visually important areas without needing to recompute saliency maps, showing promising qualitative results.

Conclusion: This method overcomes limitations of older approaches for multi-crop scenarios and opens the way for new datasets and evaluation benchmarks.

Abstract: Automatic image cropping aims to extract the most visually salient regions
while preserving essential composition elements. Traditional saliency-aware
cropping methods optimize a single bounding box, making them ineffective for
applications requiring multiple disjoint crops. In this work, we extend the
Fixed Aspect Ratio Cropping algorithm to efficiently extract multiple
non-overlapping crops in linear time. Our approach dynamically adjusts
attention thresholds and removes selected crops from consideration without
recomputing the entire saliency map. We discuss qualitative results and
introduce the potential for future datasets and benchmarks.

</details>


### [173] [Unleashing the Multi-View Fusion Potential: Noise Correction in VLM for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2506.22817)
*Xingyilang Yin,Jiale Wang,Xi Yang,Mutian Xu,Xu Gu,Nannan Wang*

Main category: cs.CV

TL;DR: The paper introduces MVOV3D, a method leveraging 2D multi-view fusion to improve open-vocabulary 3D scene understanding by reducing noise without additional training. It achieves record performance on semantic segmentation benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for open-vocabulary 3D scene understanding struggle with diverse object categories due to limited 3D training data. This motivates leveraging 2D multi-view fusion to enhance understanding of diverse concepts.

Method: MVOV3D improves 2D multi-view fusion using precise region-level features from CLIP encoders, incorporates 3D geometric priors, and focuses on reducing inherent noises without training to maintain generalizability.

Result: MVOV3D achieves state-of-the-art performance with 14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for open-vocabulary semantic segmentation, surpassing existing trained 3D networks.

Conclusion: The proposed MVOV3D method effectively enhances open-vocabulary 3D scene understanding using 2D multi-view fusion, achieving superior generalization and performance without additional training.

Abstract: Recent open-vocabulary 3D scene understanding approaches mainly focus on
training 3D networks through contrastive learning with point-text pairs or by
distilling 2D features into 3D models via point-pixel alignment. While these
methods show considerable performance in benchmarks with limited vocabularies,
they struggle to handle diverse object categories as the limited amount of 3D
data upbound training strong open-vocabulary 3d models. We observe that 2D
multi-view fusion methods take precedence in understanding diverse concepts in
3D scenes. However, inherent noises in vision-language models lead multi-view
fusion to sub-optimal performance. To this end, we introduce MVOV3D, a novel
approach aimed at unleashing the potential of 2D multi-view fusion for
open-vocabulary 3D scene understanding. We focus on reducing the inherent
noises without training, thereby preserving the generalizability while
enhancing open-world capabilities. Specifically, MVOV3D improves multi-view 2D
features by leveraging precise region-level image features and text features
encoded by CLIP encoders and incorporates 3D geometric priors to optimize
multi-view fusion. Extensive experiments on various datasets demonstrate the
effectiveness of our method. Notably, our MVOV3D achieves a new record with
14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for challenge
open-vocabulary semantic segmentation, outperforming current leading trained 3D
networks by a significant margin.

</details>


### [174] [Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration](https://arxiv.org/abs/2506.22819)
*Ramya Hebbalaguppe,Tamoghno Kandar,Abhinav Nagpal,Chetan Arora*

Main category: cs.CV

TL;DR: The paper highlights improvements in test-time prompt tuning (TPT) for vision-language models to enhance calibration and maintain accuracy by introducing better initialization and regularization methods.


<details>
  <summary>Details</summary>
Motivation: Despite the success of vision-language models (VLMs) and test-time prompt tuning (TPT), existing TPT methods overly focus on accuracy at the cost of confidence calibration, which limits their effectiveness in critical applications.

Method: The proposed approach leverages prior knowledge from large language models to carefully initialize prompts, and introduces a novel regularization loss aimed at reducing intra-class distances and increasing inter-class distances during TPT.

Result: Extensive testing on multiple CLIP architectures and 15 datasets reveals significant enhancement in calibration, with an average expected calibration error (ECE) of 4.11, outperforming existing methods like C-TPT, DiffTPT, and PromptAlign.

Conclusion: The work successfully addresses limitations in existing test-time prompt tuning methods by improving both calibration and accuracy through better initialization and regularization strategies, making the approach practical for applications with strict performance requirements.

Abstract: Vision-language models (VLM) have demonstrated impressive performance in
image recognition by leveraging self-supervised training on large datasets.
Their performance can be further improved by adapting to the test sample using
test-time prompt tuning (TPT). Unfortunately, the singular focus of TPT
approaches on improving the accuracy suffers from tunnel vision, and leads to
degradation in confidence calibration. This limits the applicability of TPT in
critical applications.
  We make three contributions in this work. (1) We posit that random or naive
initialization of prompts leads to overfitting on a particular test sample, and
is the main reason for miscalibration of the VLM after TPT. To mitigate the
problem, we propose careful initialization of test time prompt using prior
knowledge about the target label attributes from a large language model (LLM);
(2) To further maintain the quality of prompts during \tpt, we propose a novel
regularization loss to reduce intraclass distance, and increase inter-class
distance between the learnt
  Through extensive experiments on different CLIP architectures and 15
datasets, we show that our approach can effectively improve the calibration
after TPT. We report an average expected calibration error (ECE) of 4.11 with
our method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),
6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is
publicly accessible at:
https://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.

</details>


### [175] [Listener-Rewarded Thinking in VLMs for Image Preferences](https://arxiv.org/abs/2506.22832)
*Alexander Gambashidze,Li Pengyi,Matvey Skripkin,Andrey Galichin,Anton Gusarov,Konstantin Sobolev,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CV

TL;DR: The paper addresses challenges in training reward models for aligning generative models with human visual preferences, and proposes a listener-augmented GRPO framework to improve reasoning accuracy, achieving improved benchmark results.


<details>
  <summary>Details</summary>
Motivation: Current reward models for visual preferences suffer from poor generalization and memorization during supervised fine-tuning. Reinforcement learning approaches improve generalization but face accuracy drops when reasoning contradictions occur.

Method: A listener-augmented GRPO framework is introduced, where an independent vision-language model (listener) evaluates reasoning traces and provides calibrated confidence scores to refine RL reward signals.

Result: The framework achieves state-of-the-art performance on the ImageReward benchmark, improves out-of-distribution accuracy by up to +6%, and minimizes reasoning contradictions compared to GRPO and SFT baselines.

Conclusion: Listener-based rewards prove effective for scalable, data-efficient alignment of vision-language models with human preferences, providing a robust improvement in reasoning and generalization.

Abstract: Training robust and generalizable reward models for human visual preferences
is essential for aligning text-to-image and text-to-video generative models
with human intent. However, current reward models often fail to generalize, and
supervised fine-tuning leads to memorization, demanding complex annotation
pipelines. While reinforcement learning (RL), specifically Group Relative
Policy Optimization (GRPO), improves generalization, we uncover a key failure
mode: a significant drop in reasoning accuracy occurs when a model's reasoning
trace contradicts that of an independent, frozen vision-language model
("listener") evaluating the same output. To address this, we introduce a
listener-augmented GRPO framework. Here, the listener re-evaluates the
reasoner's chain-of-thought to provide a dense, calibrated confidence score,
shaping the RL reward signal. This encourages the reasoner not only to answer
correctly, but to produce explanations that are persuasive to an independent
model. Our listener-shaped reward scheme achieves best accuracy on the
ImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)
performance on a large-scale human preference dataset (1.2M votes, up to +6%
over naive reasoner), and reduces reasoning contradictions compared to strong
GRPO and SFT baselines. These results demonstrate that listener-based rewards
provide a scalable, data-efficient path to aligning vision-language models with
nuanced human preferences. We will release our reasoning model here:
https://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.

</details>


### [176] [SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds](https://arxiv.org/abs/2506.22833)
*Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: This paper proposes SemFaceEdit, a method for localized semantic editing of facial images using generative radiance manifolds, offering improved disentanglement and control.


<details>
  <summary>Details</summary>
Motivation: Despite advancements in 3D-aware GAN techniques, localized editing of specific regions in images remains challenging, motivating the search for efficient solutions that enhance fine detail rendering.

Method: The method generates semantic fields on generative radiance manifolds by using latent codes to disentangle geometry and appearance for different facial semantics, featuring two key modules: Geometry (semantic radiance and occupancy fields) and Appearance (RGB radiance prediction).

Result: SemFaceEdit enables precise editing of facial semantics while maintaining the integrity of other regions. Experiments show better performance in semantic field-based editing and improved radiance field disentanglement.

Conclusion: SemFaceEdit demonstrates significant advancements in facial semantic editing by disentangling geometry and appearance, providing enhanced control and precision over localized changes.

Abstract: Despite multiple view consistency offered by 3D-aware GAN techniques, the
resulting images often lack the capacity for localized editing. In response,
generative radiance manifolds emerge as an efficient approach for constrained
point sampling within volumes, effectively reducing computational demands and
enabling the learning of fine details. This work introduces SemFaceEdit, a
novel method that streamlines the appearance and geometric editing process by
generating semantic fields on generative radiance manifolds. Utilizing latent
codes, our method effectively disentangles the geometry and appearance
associated with different facial semantics within the generated image. In
contrast to existing methods that can change the appearance of the entire
radiance field, our method enables the precise editing of particular facial
semantics while preserving the integrity of other regions. Our network
comprises two key modules: the Geometry module, which generates semantic
radiance and occupancy fields, and the Appearance module, which is responsible
for predicting RGB radiance. We jointly train both modules in adversarial
settings to learn semantic-aware geometry and appearance descriptors. The
appearance descriptors are then conditioned on their respective semantic latent
codes by the Appearance Module, facilitating disentanglement and enhanced
control. Our experiments highlight SemFaceEdit's superior performance in
semantic field-based editing, particularly in achieving improved radiance field
disentanglement.

</details>


### [177] [FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition](https://arxiv.org/abs/2506.22836)
*Hongyan An,Kuan Zhu,Xin He,Haiyun Guo,Chaoyang Zhao,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: The paper presents FOCUS, a framework for pedestrian attribute recognition that adapts to fine-grained and unseen attributes using semantic guidance and novel token/method designs.


<details>
  <summary>Details</summary>
Motivation: Current PAR methods use regional features that compromise fine-grained recognition and fail to generalize to unseen attributes, limiting performance and practicality.

Method: The FOCUS framework includes Multi-Granularity Mix Tokens for diverse information capture and Attribute-guided Visual Feature Extraction with cross-attention mechanisms. Region-Aware Contrastive Learning is also employed for improved consistency in attention.

Result: The approach demonstrated effectiveness and strong generalization abilities across multiple datasets: PA100K, PETA, and RAPv1.

Conclusion: FOCUS successfully improves fine-grained attribute-level recognition and generalization, providing a more adaptable solution for real-world PAR tasks.

Abstract: Pedestrian attribute recognition (PAR) is a fundamental perception task in
intelligent transportation and security. To tackle this fine-grained task, most
existing methods focus on extracting regional features to enrich attribute
information. However, a regional feature is typically used to predict a fixed
set of pre-defined attributes in these methods, which limits the performance
and practicality in two aspects: 1) Regional features may compromise
fine-grained patterns unique to certain attributes in favor of capturing common
characteristics shared across attributes. 2) Regional features cannot
generalize to predict unseen attributes in the test time. In this paper, we
propose the \textbf{F}ine-grained \textbf{O}ptimization with semanti\textbf{C}
g\textbf{U}ided under\textbf{S}tanding (FOCUS) approach for PAR, which
adaptively extracts fine-grained attribute-level features for each attribute
individually, regardless of whether the attributes are seen or not during
training. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to
capture latent features at varying levels of visual granularity, thereby
enriching the diversity of the extracted information. Next, we introduce the
Attribute-guided Visual Feature Extraction (AVFE) module, which leverages
textual attributes as queries to retrieve their corresponding visual attribute
features from the Mix Tokens using a cross-attention mechanism. To ensure that
textual attributes focus on the appropriate Mix Tokens, we further incorporate
a Region-Aware Contrastive Learning (RACL) method, encouraging attributes
within the same region to share consistent attention maps. Extensive
experiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness
and strong generalization ability of our method.

</details>


### [178] [AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results](https://arxiv.org/abs/2506.22843)
*Kien Nguyen,Clinton Fookes,Sridha Sridharan,Huy Nguyen,Feng Liu,Xiaoming Liu,Arun Ross,Dana Michalski,Tamás Endrei,Ivan DeAndres-Tame,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez,Javier Ortega-Garcia,Zijing Gong,Yuhao Wang,Xuehu Liu,Pingping Zhang,Md Rashidunnabi,Hugo Proença,Kailash A. Hambarde,Saeid Rezaei*

Main category: cs.CV

TL;DR: The paper introduces the AG-VPReID 2025 Challenge focused on video-based aerial-ground person re-identification using a novel dataset with over 3 million frames, achieving 72.28% Rank-1 accuracy with the leading approach.


<details>
  <summary>Details</summary>
Motivation: To address the challenging problem of person re-identification across aerial and ground viewpoints for surveillance and public safety, given the extreme viewpoint differences, scale variations, and occlusions.

Method: The challenge employs a newly constructed AG-VPReID dataset with 3,027 identities and 13,500+ tracklets, while solutions include multi-stream architectures, transformer-based temporal reasoning, and physics-informed modeling.

Result: The leading solution, X-TFCLIP from UAM, achieved 72.28% Rank-1 accuracy for aerial-to-ground ReID and 70.77% for ground-to-aerial ReID, outperforming existing baselines.

Conclusion: The AG-VPReID 2025 Challenge demonstrates the feasibility of high-accuracy aerial-ground person re-identification using innovative approaches, though the complexity of the dataset underscores the challenge's difficulty.

Abstract: Person re-identification (ReID) across aerial and ground vantage points has
become crucial for large-scale surveillance and public safety applications.
Although significant progress has been made in ground-only scenarios, bridging
the aerial-ground domain gap remains a formidable challenge due to extreme
viewpoint differences, scale variations, and occlusions. Building upon the
achievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID
2025 Challenge - the first large-scale video-based competition focused on
high-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID
dataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7
million frames captured from UAVs, CCTV, and wearable cameras, the challenge
featured four international teams. These teams developed solutions ranging from
multi-stream architectures to transformer-based temporal reasoning and
physics-informed modeling. The leading approach, X-TFCLIP from UAM, attained
72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the
ground-to-aerial ReID setting, surpassing existing baselines while highlighting
the dataset's complexity. For additional details, please refer to the official
website at https://agvpreid25.github.io.

</details>


### [179] [DMD-Net: Deep Mesh Denoising Network](https://arxiv.org/abs/2506.22850)
*Aalok Gangopadhyay,Shashikant Verma,Shanmuganathan Raman*

Main category: cs.CV

TL;DR: DMD-Net is a deep learning-based framework for effectively denoising 3D meshes using a graph convolutional neural network with a two-stream structure and a novel Feature Guided Transformer paradigm.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of effectively denoising 3D mesh data, a common problem in computer graphics and 3D modeling, by leveraging advancements in deep learning.

Method: The proposed DMD-Net employs a graph convolutional neural network that utilizes both primal and dual graphs in an asymmetric two-stream structure, integrated with a Feature Guided Transformer paradigm consisting of a feature extractor, transformer, and denoiser.

Result: Extensive experiments and ablation studies show that DMD-Net achieves competitive or superior results compared to state-of-the-art methods across various noise levels, including extremely high noise.

Conclusion: DMD-Net presents a robust and effective approach for 3D mesh denoising, demonstrating its utility and generalizability on large-scale datasets and challenging noise scenarios.

Abstract: We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning
framework, for solving the mesh denoising problem. DMD-Net consists of a Graph
Convolutional Neural Network in which aggregation is performed in both the
primal as well as the dual graph. This is realized in the form of an asymmetric
two-stream network, which contains a primal-dual fusion block that enables
communication between the primal-stream and the dual-stream. We develop a
Feature Guided Transformer (FGT) paradigm, which consists of a feature
extractor, a transformer, and a denoiser. The feature extractor estimates the
local features, that guide the transformer to compute a transformation, which
is applied to the noisy input mesh to obtain a useful intermediate
representation. This is further processed by the denoiser to obtain the
denoised mesh. Our network is trained on a large scale dataset of 3D objects.
We perform exhaustive ablation studies to demonstrate that each component in
our network is essential for obtaining the best performance. We show that our
method obtains competitive or better results when compared with the
state-of-the-art mesh denoising algorithms. We demonstrate that our method is
robust to various kinds of noise. We observe that even in the presence of
extremely high noise, our method achieves excellent performance.

</details>


### [180] [RoboScape: Physics-informed Embodied World Model](https://arxiv.org/abs/2506.23135)
*Yu Shang,Xin Zhang,Yinzhou Tang,Lei Jin,Chen Gao,Wei Wu,Yong Li*

Main category: cs.CV

TL;DR: RoboScape is a proposed physics-informed world model that improves realistic video generation for robotic scenarios by integrating 3D geometry and physical dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing world models for embodied intelligence face challenges in physical awareness, leading to unrealistic video simulations in contact-rich robotic tasks.

Method: RoboScape employs physics-informed joint training tasks: temporal depth prediction for 3D geometry consistency and keypoint dynamics learning for improved motion modeling by encoding physical properties.

Result: Experiments show that RoboScape achieves superior visual fidelity and physical plausibility, and it has practical utility in robotic policy training and evaluation.

Conclusion: RoboScape provides an efficient approach to building physics-informed world models, offering significant advancements in embodied intelligence research.

Abstract: World models have become indispensable tools for embodied intelligence,
serving as powerful simulators capable of generating realistic robotic videos
while addressing critical data scarcity challenges. However, current embodied
world models exhibit limited physical awareness, particularly in modeling 3D
geometry and motion dynamics, resulting in unrealistic video generation for
contact-rich robotic scenarios. In this paper, we present RoboScape, a unified
physics-informed world model that jointly learns RGB video generation and
physics knowledge within an integrated framework. We introduce two key
physics-informed joint training tasks: temporal depth prediction that enhances
3D geometric consistency in video rendering, and keypoint dynamics learning
that implicitly encodes physical properties (e.g., object shape and material
characteristics) while improving complex motion modeling. Extensive experiments
demonstrate that RoboScape generates videos with superior visual fidelity and
physical plausibility across diverse robotic scenarios. We further validate its
practical utility through downstream applications including robotic policy
training with generated data and policy evaluation. Our work provides new
insights for building efficient physics-informed world models to advance
embodied intelligence research. The code is available at:
https://github.com/tsinghua-fib-lab/RoboScape.

</details>


### [181] [Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval](https://arxiv.org/abs/2506.22864)
*Li-Cheng Shen,Jih-Kang Hsieh,Wei-Hua Li,Chu-Song Chen*

Main category: cs.CV

TL;DR: This paper presents Mask-aware Text-to-Image Retrieval (MaTIR), a task combining efficient image search and accurate object segmentation, alongside a two-stage framework to address it.


<details>
  <summary>Details</summary>
Motivation: Existing methods for text-to-image retrieval lack interpretability as they focus on whole-image captions, and referring expression segmentation is computationally inefficient when scaling to large datasets.

Method: The authors propose a two-stage framework: (1) using SAM and Alpha-CLIP to generate object masks and extract pre-computed embeddings for efficient online retrieval, and (2) applying a multimodal large language model for reranking and generating bounding boxes aligned with segmentation masks.

Result: The framework demonstrates significant improvements in retrieval accuracy and segmentation quality on the COCO and D$^3$ datasets compared to previous methods.

Conclusion: Mask-aware TIR successfully combines retrieval and segmentation tasks, providing an interpretable and scalable solution to address limitations in current methods.

Abstract: Text-to-image retrieval (TIR) aims to find relevant images based on a textual
query, but existing approaches are primarily based on whole-image captions and
lack interpretability. Meanwhile, referring expression segmentation (RES)
enables precise object localization based on natural language descriptions but
is computationally expensive when applied across large image collections. To
bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies
TIR and RES, requiring both efficient image search and accurate object
segmentation. To address this task, we propose a two-stage framework,
comprising a first stage for segmentation-aware image retrieval and a second
stage for reranking and object grounding with a multimodal large language model
(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract
region-level embeddings offline at first, enabling effective and scalable
online retrieval. Secondly, MLLM is used to refine retrieval rankings and
generate bounding boxes, which are matched to segmentation masks. We evaluate
our approach on COCO and D$^3$ datasets, demonstrating significant improvements
in both retrieval accuracy and segmentation quality over previous methods.

</details>


### [182] [Towards foundational LiDAR world models with efficient latent flow matching](https://arxiv.org/abs/2506.23434)
*Tianran Liu,Shengwen Zhao,Nicholas Rhinehart*

Main category: cs.CV

TL;DR: The paper explores LiDAR-based world models for improved domain transferability, and proposes a new framework for better data compression and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current LiDAR-based world models are domain-specific and require extensive annotated data for semantic occupancy forecasting.

Method: Conducted systematic domain transfer studies and proposed a latent conditional flow matching (CFM) framework for enhanced data efficiency and compression.

Result: Achieved up to 83% relative improvement over training from scratch, reduced requirement for labeled training data by 95%, and state-of-the-art performance in semantic occupancy forecasting with significant computational efficiency gains.

Conclusion: The proposed techniques advance LiDAR world model transferability and efficiency, setting new benchmarks in semantic occupancy forecasting while minimizing data and computational costs.

Abstract: LiDAR-based world models offer more structured and geometry-aware
representations than their image-based counterparts. However, existing LiDAR
world models are narrowly trained; each model excels only in the domain for
which it was built. Can we develop LiDAR world models that exhibit strong
transferability across multiple domains? We conduct the first systematic domain
transfer study across three demanding scenarios: (i) outdoor to indoor
generalization, (ii) sparse-beam \& dense-beam adaptation, and (iii)
non-semantic to semantic transfer. Given different amounts of fine-tuning data,
our experiments show that a single pre-trained model can achieve up to 11%
absolute improvement (83\% relative) over training from scratch and outperforms
training from scratch in 30/36 of our comparisons. This transferability of
dynamic learning significantly reduces the reliance on manually annotated data
for semantic occupancy forecasting: our method exceed the previous semantic
occupancy forecasting models with only 5% of the labeled training data required
by prior models. We also observed inefficiencies of current LiDAR world models,
mainly through their under-compression of LiDAR data and inefficient training
objectives. To address this, we propose a latent conditional flow matching
(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy
using only half the training data and a compression ratio 6 times higher than
that of prior methods. Our model achieves SOTA performance on
future-trajectory-conditioned semantic occupancy forecasting while being 23x
more computationally efficient (a 28x FPS speedup); and achieves SOTA
performance on semantic occupancy forecasting while being 2x more
computationally efficient (a 1.1x FPS speedup).

</details>


### [183] [Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception](https://arxiv.org/abs/2506.22866)
*Hang-Cheng Dong,Lu Zou,Bingguo Liu,Dong Ye,Guodong Liu*

Main category: cs.CV

TL;DR: The paper presents a weakly supervised framework for surface defect detection, introducing novel techniques to improve segmentation accuracy while requiring fewer labeled data.


<details>
  <summary>Details</summary>
Motivation: To reduce dependency on large-scale labeled datasets for industrial defect detection, which is impractical in real-world scenarios.

Method: Proposed a framework combining region-aware CAM, filtering-guided backpropagation (FGBP), and pseudo-label training to enhance defect detection precision.

Result: The framework outperformed existing methods on industrial defect datasets, providing high precision in weakly supervised segmentation.

Conclusion: The method bridges weakly supervised learning with high-precision defect detection, making it suitable for resource-limited industrial applications.

Abstract: Surface defect detection plays a critical role in industrial quality
inspection. Recent advances in artificial intelligence have significantly
enhanced the automation level of detection processes. However, conventional
semantic segmentation and object detection models heavily rely on large-scale
annotated datasets, which conflicts with the practical requirements of defect
detection tasks. This paper proposes a novel weakly supervised semantic
segmentation framework comprising two key components: a region-aware class
activation map (CAM) and pseudo-label training. To address the limitations of
existing CAM methods, especially low-resolution thermal maps, and insufficient
detail preservation, we introduce filtering-guided backpropagation (FGBP),
which refines target regions by filtering gradient magnitudes to identify areas
with higher relevance to defects. Building upon this, we further develop a
region-aware weighted module to enhance spatial precision. Finally,
pseudo-label segmentation is implemented to refine the model's performance
iteratively. Comprehensive experiments on industrial defect datasets
demonstrate the superiority of our method. The proposed framework effectively
bridges the gap between weakly supervised learning and high-precision defect
segmentation, offering a practical solution for resource-constrained industrial
scenarios.

</details>


### [184] [StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving](https://arxiv.org/abs/2506.23982)
*Ruiyang Hao,Bowen Jing,Haibao Yu,Zaiqing Nie*

Main category: cs.CV

TL;DR: This work introduces a dataset and benchmark focused on personalization in end-to-end autonomous driving (E2EAD), an area previously underexplored. Personalized preferences were analyzed and shown to enhance alignment with human driving behaviors.


<details>
  <summary>Details</summary>
Motivation: The motivation was the underexplored field of personalization in E2EAD, which is crucial for trust, comfort, and adoption of autonomous vehicles. The absence of datasets capturing diverse driving preferences hindered progress.

Method: A dataset with diverse driving preferences was created using road topology, contextual cues modeled by a fine-tuned visual language model (VLM), behavioral analysis, rule-based heuristics, and human-in-the-loop verification. Benchmarking was done to evaluate state-of-the-art E2EAD models with preference conditioning.

Result: The results demonstrated that personalized preferences significantly improve alignment with human driving behavior, showcasing the importance of personalization in E2EAD systems.

Conclusion: The study established a foundation for personalized E2EAD by introducing the first dataset and benchmark, offering a systematic way to integrate human preferences into autonomous systems, and urging further research in human-centric autonomy.

Abstract: While personalization has been explored in traditional autonomous driving
systems, it remains largely overlooked in end-to-end autonomous driving
(E2EAD), despite its growing prominence. This gap is critical, as user-aligned
behavior is essential for trust, comfort, and widespread adoption of autonomous
vehicles. A core challenge is the lack of large-scale real-world datasets
annotated with diverse and fine-grained driving preferences, hindering the
development and evaluation of personalized E2EAD models. In this work, we
present the first large-scale real-world dataset enriched with annotations
capturing diverse driving preferences, establishing a foundation for
personalization in E2EAD. We extract static environmental features from
real-world road topology and infer dynamic contextual cues using a fine-tuned
visual language model (VLM), enabling consistent and fine-grained scenario
construction. Based on these scenarios, we derive objective preference
annotations through behavioral distribution analysis and rule-based heuristics.
To address the inherent subjectivity of driving style, we further employ the
VLM to generate subjective annotations by jointly modeling scene semantics and
driver behavior. Final high-quality labels are obtained through a
human-in-the-loop verification process that fuses both perspectives. Building
on this dataset, we propose the first benchmark for evaluating personalized
E2EAD models. We assess several state-of-the-art models with and without
preference conditioning, demonstrating that incorporating personalized
preferences results in behavior more aligned with human driving. Our work lays
the foundation for personalized E2EAD by providing a standardized platform to
systematically integrate human preferences into data-driven E2EAD systems,
catalyzing future research in human-centric autonomy.

</details>


### [185] [STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing](https://arxiv.org/abs/2506.22868)
*Junsung Lee,Junoh Kang,Bohyung Han*

Main category: cs.CV

TL;DR: The paper introduces STR-Match, a training-free video editing method that achieves high-quality, temporally coherent visuals using a novel STR score for spatiotemporal pixel relevance.


<details>
  <summary>Details</summary>
Motivation: To address limitations in text-guided video editing, such as temporal inconsistency, motion distortion, and restricted domain transformation.

Method: Develop STR-Match using a novel STR score based on 2D spatial attention and 1D temporal modules for latent optimization in T2V diffusion models.

Result: STR-Match delivers visually appealing videos with spatiotemporal coherence and outperforms existing methods in visual quality and consistency.

Conclusion: STR-Match provides effective video editing with strong domain transformation capabilities while preserving the source's visual attributes.

Abstract: Previous text-guided video editing methods often suffer from temporal
inconsistency, motion distortion, and-most notably-limited domain
transformation. We attribute these limitations to insufficient modeling of
spatiotemporal pixel relevance during the editing process. To address this, we
propose STR-Match, a training-free video editing algorithm that produces
visually appealing and spatiotemporally coherent videos through latent
optimization guided by our novel STR score. The score captures spatiotemporal
pixel relevance across adjacent frames by leveraging 2D spatial attention and
1D temporal modules in text-to-video (T2V) diffusion models, without the
overhead of computationally expensive 3D attention mechanisms. Integrated into
a latent optimization framework with a latent mask, STR-Match generates
temporally consistent and visually faithful videos, maintaining strong
performance even under significant domain transformations while preserving key
visual attributes of the source. Extensive experiments demonstrate that
STR-Match consistently outperforms existing methods in both visual quality and
spatiotemporal consistency.

</details>


### [186] [Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder](https://arxiv.org/abs/2506.22880)
*Dang Jisheng,Wu Xudong,Wang Bimei,Lv Ning,Chen Jiayu,Jingwen Zhao,Yichu liu,Jizhao Liu,Juncheng Li,Teng Wang*

Main category: cs.CV

TL;DR: The paper introduces DeSa2VA, a decoupling-enhanced prompting scheme improving segmentation and grounding performance by addressing limitations in feature processing in existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the problem of information entanglement in existing video segmentation and grounding models, which negatively impacts segmentation accuracy.

Method: DeSa2VA uses a pre-training paradigm, linear projection for feature disentanglement, and dynamic mask fusion with triple supervision to enhance semantic grounding and feature processing.

Result: The proposed method achieves state-of-the-art performance across various tasks such as image and video segmentation, and question answering.

Conclusion: DeSa2VA successfully disentangles dynamic and static features, improving accuracy and applicability in multiple visual-textual inference tasks, offering a significant advancement in systematic segmentation models.

Abstract: Existing video segmenter and grounder approaches, exemplified by Sa2VA,
directly fuse features within segmentation models. This often results in an
undesirable entanglement of dynamic visual information and static semantics,
thereby degrading segmentation accuracy. To systematically mitigate this issue,
we propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text
pre-training and a linear decoupling module to address the information
processing limitations inherent in SAM-2. Specifically, first, we devise a
pre-training paradigm that converts textual ground-truth labels into
point-level prompts while generating corresponding text masks. These masks are
refined through a hybrid loss function to strengthen the model's semantic
grounding capabilities. Next, we employ linear projection to disentangle hidden
states that generated by a large language model into distinct textual and
visual feature subspaces. Finally, a dynamic mask fusion strategy
synergistically combines these decoupled features through triple supervision
from predicted text/visual masks and ground-truth annotations. Extensive
experiments demonstrate state-of-the-art performance across diverse tasks,
including image segmentation, image question answering, video segmentation, and
video question answering. Our codes are available at
https://github.com/longmalongma/DeSa2VA.

</details>


### [187] [A Survey on Vision-Language-Action Models for Autonomous Driving](https://arxiv.org/abs/2506.24044)
*Sicong Jiang,Zilin Huang,Kangan Qian,Ziang Luo,Tianze Zhu,Yang Zhong,Yihong Tang,Menglin Kong,Yunlong Wang,Siwen Jiao,Hao Ye,Zihao Sheng,Xin Zhao,Tuopu Wen,Zheng Fu,Sikai Chen,Kun Jiang,Diange Yang,Seongjin Choi,Lijun Sun*

Main category: cs.CV

TL;DR: This paper surveys the application of Vision-Language-Action (VLA) models to autonomous driving, formalizing architectures, benchmarking over 20 models, and identifying future challenges in the domain.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of the fragmented and evolving literature on VLA models applied to autonomous driving, with the aim of enabling autonomous vehicles that can interpret high-level instructions and make independent decisions.

Method: The authors reviewed and formalized architectural components of existing VLA models in autonomous driving, compared over 20 models, consolidated datasets, benchmarks, and protocols, and outlined open challenges and future directions.

Result: A structured taxonomy, comparison across models, identified gaps, and challenges in the application of VLA to autonomous vehicles were presented, alongside a Github repository with relevant resources.

Conclusion: The paper provides a foundational reference for advancing interpretable, safe, and socially aligned autonomous vehicles through VLA models, while highlighting significant progress and remaining challenges.

Abstract: The rapid progress of multimodal large language models (MLLM) has paved the
way for Vision-Language-Action (VLA) paradigms, which integrate visual
perception, natural language understanding, and control within a single policy.
Researchers in autonomous driving are actively adapting these methods to the
vehicle domain. Such models promise autonomous vehicles that can interpret
high-level instructions, reason about complex traffic scenes, and make their
own decisions. However, the literature remains fragmented and is rapidly
expanding. This survey offers the first comprehensive overview of VLA for
Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks
shared across recent work, (ii) trace the evolution from early explainer to
reasoning-centric VLA models, and (iii) compare over 20 representative models
according to VLA's progress in the autonomous driving domain. We also
consolidate existing datasets and benchmarks, highlighting protocols that
jointly measure driving safety, accuracy, and explanation quality. Finally, we
detail open challenges - robustness, real-time efficiency, and formal
verification - and outline future directions of VLA4AD. This survey provides a
concise yet complete reference for advancing interpretable socially aligned
autonomous vehicles. Github repo is available at
\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.

</details>


### [188] [How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings](https://arxiv.org/abs/2506.22881)
*Fumiya Uchiyama,Rintaro Yanagi,Shohei Taniguchi,Shota Takashiro,Masahiro Suzuki,Hirokatsu Kataoka,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CV

TL;DR: This paper introduces a metric for measuring semantic informativeness in images and texts using a contrastive learning model, redefining the Information Gain concept for cross-domain assessment.


<details>
  <summary>Details</summary>
Motivation: To address whether contrastive learning can represent absolute semantic informativeness, not just relational semantic similarity.

Method: The authors propose a semantic informativeness metric using Information Gain redefined for vision and language, leveraging contrastive learning models like CLIP or SigLIP.

Result: Empirical results with OpenCLIP show accurate informativeness measurement, with strong correlation coefficients (0.98 to 1.00), and computational efficiency independent of sample size.

Conclusion: The method effectively measures semantic informativeness across modalities and is compatible with existing open-weight models, ensuring accessibility and computational efficiency.

Abstract: Contrastive learning has the capacity to model multimodal probability
distributions by embedding and aligning visual representations with semantics
from captions. This approach enables the estimation of relational semantic
similarity; however, it remains unclear whether it can also represent absolute
semantic informativeness. In this work, we introduce a semantic informativeness
metric for an image calculated from text samples via a contrastive learning
model; similarly, the informativeness of a text is calculated from image
samples. We propose a redefinition of the concept of Information Gain, a
concept previously explored in natural language processing, extending its
application to the domains of vision and language. Our metric quantifies how
conditioning on an image distorts the distribution of associated texts, and
vice versa for text conditioning on image distributions. In OpenCLIP's
empirical results, we observe that images with the lowest Information Gain
scores often correspond to placeholder icons such as "image not found."
Furthermore, we propose to measure a norm-based metric of the embedding to
estimate the Information Gain, following the theoretical results for Skip-Gram
with Negative Sampling (SGNS) word embedding. Information Gain can be measured
using either CLIP or SigLIP, and the results demonstrate a strong correlation
with a coefficient of determination ranging from 0.98 to 1.00. After obtaining
the mean and the covariance of the sample embedding, the computational cost of
this method is independent of the sample size, and it is compatible with
publicly available, open-weight models.

</details>


### [189] [CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems](https://arxiv.org/abs/2506.22890)
*Senkang Hu,Yihang Tao,Guowen Xu,Xinyuan Qian,Yiqin Deng,Xianhao Chen,Sam Tak Wu Kwong,Yuguang Fang*

Main category: cs.CV

TL;DR: The paper presents CP-Guard, a defense mechanism for collaborative perception in multi-agent systems to detect and eliminate malicious agents using methods like probability-agnostic consensus sampling, collaborative consistency loss, and adaptive thresholds.


<details>
  <summary>Details</summary>
Motivation: Collaborative Perception is beneficial for improving information sharing among multiple agents, but it is vulnerable to attacks by malicious agents in multi-agent settings, necessitating effective defense mechanisms.

Method: CP-Guard uses a probability-agnostic sample consensus (PASAC) method to verify consensus without prior probabilities, collaborative consistency loss to detect discrepancies in shared perception tasks, and a dual sliding windows approach to dynamically adjust verification thresholds in real-time.

Result: The proposed CP-Guard framework effectively detects and eliminates malicious agents, ensuring reliable perception in dynamic environments, as verified through extensive experimentation.

Conclusion: CP-Guard significantly enhances the security and reliability of collaborative perception in multi-agent systems, offering adaptive and accurate defense against malicious entities.

Abstract: Collaborative Perception (CP) has been shown to be a promising technique for
multi-agent autonomous driving and multi-agent robotic systems, where multiple
agents share their perception information to enhance the overall perception
performance and expand the perception range. However, in CP, an ego agent needs
to receive messages from its collaborators, which makes it vulnerable to
attacks from malicious agents. To address this critical issue, we propose a
unified, probability-agnostic, and adaptive framework, namely, CP-Guard, which
is a tailored defense mechanism for CP deployed by each agent to accurately
detect and eliminate malicious agents in its collaboration network. Our key
idea is to enable CP to reach a consensus rather than a conflict against an ego
agent's perception results. Based on this idea, we first develop a
probability-agnostic sample consensus (PASAC) method to effectively sample a
subset of the collaborators and verify the consensus without prior
probabilities of malicious agents. Furthermore, we define collaborative
consistency loss (CCLoss) for object detection task and bird's eye view (BEV)
segmentation task to capture the discrepancy between an ego agent and its
collaborators, which is used as a verification criterion for consensus. In
addition, we propose online adaptive threshold via dual sliding windows to
dynamically adjust the threshold for consensus verification and ensure the
reliability of the systems in dynamic environments. Finally, we conduct
extensive experiments and demonstrate the effectiveness of our framework. Code
will be released at https://github.com/CP-Security/CP-Guard

</details>


### [190] [Neural Cellular Automata: From Cells to Pixels](https://arxiv.org/abs/2506.22899)
*Ehsan Pajouheshgar,Yitao Xu,Ali Abbasi,Alexander Mordvintsev,Wenzel Jakob,Sabine Süsstrunk*

Main category: cs.CV

TL;DR: The paper introduces enhanced Neural Cellular Automata (NCAs) with implicit decoders for scalable high-resolution outputs, overcoming existing computational challenges.


<details>
  <summary>Details</summary>
Motivation: Neural Cellular Automata face limitations in scaling to high-resolution tasks due to computational overhead, which this study aims to address.

Method: The authors pair NCAs with a shared implicit decoder and propose novel loss functions tailored to high-resolution texture synthesis and morphogenesis tasks.

Result: The enhanced NCAs achieve real-time generation of full-HD outputs, maintain emergent properties, and support parallelizable inference across 2D, 3D grids, and meshes.

Conclusion: The proposed framework enables NCAs to deliver high-quality, computationally efficient outputs for texture generation and morphogenesis at arbitrary resolutions.

Abstract: Neural Cellular Automata (NCAs) are bio-inspired systems in which identical
cells self-organize to form complex and coherent patterns by repeatedly
applying simple local rules. NCAs display striking emergent behaviors including
self-regeneration, generalization and robustness to unseen situations, and
spontaneous motion. Despite their success in texture synthesis and
morphogenesis, NCAs remain largely confined to low-resolution grids. This
limitation stems from (1) training time and memory requirements that grow
quadratically with grid size, (2) the strictly local propagation of information
which impedes long-range cell communication, and (3) the heavy compute demands
of real-time inference at high resolution. In this work, we overcome this
limitation by pairing NCA with a tiny, shared implicit decoder, inspired by
recent advances in implicit neural representations. Following NCA evolution on
a coarse grid, a lightweight decoder renders output images at arbitrary
resolution. We also propose novel loss functions for both morphogenesis and
texture synthesis tasks, specifically tailored for high-resolution output with
minimal memory and computation overhead. Combining our proposed architecture
and loss functions brings substantial improvement in quality, efficiency, and
performance. NCAs equipped with our implicit decoder can generate full-HD
outputs in real time while preserving their self-organizing, emergent
properties. Moreover, because each MLP processes cell states independently,
inference remains highly parallelizable and efficient. We demonstrate the
applicability of our approach across multiple NCA variants (on 2D, 3D grids,
and 3D meshes) and multiple tasks, including texture generation and
morphogenesis (growing patterns from a seed), showing that with our proposed
framework, NCAs seamlessly scale to high-resolution outputs with minimal
computational overhead.

</details>


### [191] [MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering](https://arxiv.org/abs/2506.22900)
*Mai A. Shaaban,Tausifa Jan Saleem,Vijay Ram Papineni,Mohammad Yaqub*

Main category: cs.CV

TL;DR: The paper introduces MOTOR, a novel multimodal approach to improve accuracy in medical visual question answering (MedVQA) by leveraging grounded captions and optimal transport, achieving a 6.45% accuracy improvement over existing methods.


<details>
  <summary>Details</summary>
Motivation: The main motivation is to overcome the challenge of factually incorrect answers generated by existing vision-language models (VLMs) in MedVQA tasks, and to address the limitations of current retrieval-augmented generation methods that often fail in capturing relevant visual or multimodal contexts crucial for medical diagnosis.

Method: The proposed method, MOTOR, employs a multimodal retrieval and re-ranking approach that uses grounded captions and optimal transport mechanisms to better align the query with both textual and visual-contextual information, thereby identifying clinically relevant contexts for augmenting VLM inputs.

Result: Empirical and human expert evaluations show that MOTOR outperforms current state-of-the-art methods in MedVQA by an average accuracy improvement of 6.45% on relevant datasets.

Conclusion: MOTOR significantly enhances the relevance of retrieved contexts in MedVQA by incorporating both textual and visual information, addressing a critical gap in medical diagnosis tasks. Its performance improvement highlights the importance of multimodal re-ranking in this domain.

Abstract: Medical visual question answering (MedVQA) plays a vital role in clinical
decision-making by providing contextually rich answers to image-based queries.
Although vision-language models (VLMs) are widely used for this task, they
often generate factually incorrect answers. Retrieval-augmented generation
addresses this challenge by providing information from external sources, but
risks retrieving irrelevant context, which can degrade the reasoning
capabilities of VLMs. Re-ranking retrievals, as introduced in existing
approaches, enhances retrieval relevance by focusing on query-text alignment.
However, these approaches neglect the visual or multimodal context, which is
particularly crucial for medical diagnosis. We propose MOTOR, a novel
multimodal retrieval and re-ranking approach that leverages grounded captions
and optimal transport. It captures the underlying relationships between the
query and the retrieved context based on textual and visual information.
Consequently, our approach identifies more clinically relevant contexts to
augment the VLM input. Empirical analysis and human expert evaluation
demonstrate that MOTOR achieves higher accuracy on MedVQA datasets,
outperforming state-of-the-art methods by an average of 6.45%. Code is
available at https://github.com/BioMedIA-MBZUAI/MOTOR.

</details>


### [192] [Point Cloud Compression and Objective Quality Assessment: A Survey](https://arxiv.org/abs/2506.22902)
*Yiling Xu,Yujie Zhang,Shuting Xia,Kaifa Yang,He Huang,Ziyu Shan,Wenjie Huang,Qi Yang,Le Yang*

Main category: cs.CV

TL;DR: The paper surveys recent advancements in point cloud compression (PCC) and quality assessment (PCQA), addressing unique challenges and proposing future directions for improvement.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing use of 3D point cloud data in areas like autonomous driving and immersive environments, which require efficient compression and quality assessment methods.

Method: It provides a comprehensive analysis and comparison of various PCC algorithms (handcrafted and learning-based) and PCQA metrics using benchmarking on modern datasets.

Result: The study identifies strengths and limitations of existing approaches and offers insights for practical application, while highlighting unresolved issues like visual fidelity and multimodal data support.

Conclusion: The paper concludes with suggested future paths, such as hybrid compression frameworks and advanced feature extraction, to enhance the efficiency and capability of 3D applications.

Abstract: The rapid growth of 3D point cloud data, driven by applications in autonomous
driving, robotics, and immersive environments, has led to criticals demand for
efficient compression and quality assessment techniques. Unlike traditional 2D
media, point clouds present unique challenges due to their irregular structure,
high data volume, and complex attributes. This paper provides a comprehensive
survey of recent advances in point cloud compression (PCC) and point cloud
quality assessment (PCQA), emphasizing their significance for real-time and
perceptually relevant applications. We analyze a wide range of handcrafted and
learning-based PCC algorithms, along with objective PCQA metrics. By
benchmarking representative methods on emerging datasets, we offer detailed
comparisons and practical insights into their strengths and limitations.
Despite notable progress, challenges such as enhancing visual fidelity,
reducing latency, and supporting multimodal data remain. This survey outlines
future directions, including hybrid compression frameworks and advanced feature
extraction strategies, to enable more efficient, immersive, and intelligent 3D
applications.

</details>


### [193] [MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances](https://arxiv.org/abs/2506.22907)
*Yunzhe Shao,Xinyu Yi,Lu Yin,Shihui Guo,Junhai Yong,Feng Xu*

Main category: cs.CV

TL;DR: This paper introduces MagShield, a method to counteract magnetic interference in sparse inertial MoCap systems, boosting accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing inertial measurement systems in accurately capturing motion in magnetically disturbed environments.

Method: MagShield uses a two-step 'detect-then-correct' approach: detecting magnetic interference with multi-IMU joint analysis and correcting errors using human motion priors.

Result: MagShield effectively improves motion capture accuracy under magnetic interference and is compatible with various sparse inertial MoCap systems.

Conclusion: MagShield enhances the usability and reliability of sparse inertial MoCap systems in real-world conditions by mitigating magnetic interference problems.

Abstract: This paper proposes a novel method called MagShield, designed to address the
issue of magnetic interference in sparse inertial motion capture (MoCap)
systems. Existing Inertial Measurement Unit (IMU) systems are prone to
orientation estimation errors in magnetically disturbed environments, limiting
their practical application in real-world scenarios. To address this problem,
MagShield employs a "detect-then-correct" strategy, first detecting magnetic
disturbances through multi-IMU joint analysis, and then correcting orientation
errors using human motion priors. MagShield can be integrated with most
existing sparse inertial MoCap systems, improving their performance in
magnetically disturbed environments. Experimental results demonstrate that
MagShield significantly enhances the accuracy of motion capture under magnetic
interference and exhibits good compatibility across different sparse inertial
MoCap systems.

</details>


### [194] [Attention to Burstiness: Low-Rank Bilinear Prompt Tuning](https://arxiv.org/abs/2506.22908)
*Yuzhu Wang,Manni Duan,Shu Kong*

Main category: cs.CV

TL;DR: This study introduces a refined Visual Prompt Tuning (VPT) technique called Bilinear Prompt Tuning (BPT). By whitening and bilinearizing image patch embeddings and vision Transformer parameters, accuracy and efficiency are significantly improved.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges posed by non-Gaussian distributions in image patch embeddings and the key/query projectors in vision Transformers, hindering the effectiveness of prompt tuning.

Method: The paper introduces data whitening to decorrelate and equalize variance, making distributions more Gaussian, followed by bilinear multiplication for efficient learning of prompts. A low-rank version is proposed to achieve compactness.

Result: BPT significantly accelerates prompt tuning, considerably improves accuracy (e.g., >25 points improvement on CUB dataset), and reduces parameter count and computational overhead.

Conclusion: The proposed BPT approach not only enhances efficacy and efficiency compared to existing VPT methods but also demonstrates robustness across diverse datasets.

Abstract: Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique
that adapts a pre-trained vision Transformer (ViT) by learning a small set of
parameters in the input space, known as prompts. In VPT, we uncover
``burstiness'' in the values arising from the interaction of image patch
embeddings, and the key and query projectors within Transformer's
self-attention module. Furthermore, the values of patch embeddings and the key
and query projectors exhibit Laplacian and hyper-Laplacian distribution,
respectively. Intuitively, these non-Gaussian distributions pose challenges for
learning prompts. To address this, we propose whitening these data,
de-correlating them and equalizing their variance towards more Gaussian before
learning prompts. We derive the whitening matrix over random image patch
embeddings and ViT's key and query projectors, and multiply it with the prompt
to be learned in a bilinear manner. Surprisingly, this method significantly
accelerates prompt tuning and boosts accuracy, e.g., $>$25 accuracy points on
the CUB dataset; interestingly, it learns ``bursty prompts''. Extending the
bilinear model which is known to introduce burstiness, we present a compact,
low-rank version by learning two smaller matrices whose multiplication yields
the final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT).
Extensive experiments across multiple benchmark datasets demonstrate that BPT
methods not only outperform various VPT methods but also reduce parameter count
and computation overhead.

</details>


### [195] [Towards Explainable Bilingual Multimodal Misinformation Detection and Localization](https://arxiv.org/abs/2506.22930)
*Yiwei He,Xiangtai Li,Zhenglin Huang,Yi Dong,Hao Fei,Jiangning Zhang,Baoyuan Wu,Guangliang Cheng*

Main category: cs.CV

TL;DR: The paper introduces BiMi, a bilingual multimodal system for misinformation detection in multilingual content, along with a benchmark called BiMiBench, demonstrating its superior accuracy and explainability.


<details>
  <summary>Details</summary>
Motivation: Increasingly realistic misinformation in bilingual news media poses challenges due to subtle manipulations across modalities and languages.

Method: BiMi utilizes region-level localization, cross-modal and cross-lingual consistency checks, and natural language explanations. It also integrates an online retrieval module and Group Relative Policy Optimization (GRPO) for enhanced reasoning and interpretation.

Result: BiMi achieves significant performance improvements over baselines: +8.9 in classification accuracy, +15.9 in localization accuracy, and +2.5 in explanation BERTScore.

Conclusion: BiMi advances multilingual misinformation detection, combining cutting-edge techniques for analysis, localization, and interpretability. The paper also provides tools to support further research.

Abstract: The increasing realism of multimodal content has made misinformation more
subtle and harder to detect, especially in news media where images are
frequently paired with bilingual (e.g., Chinese-English) subtitles. Such
content often includes localized image edits and cross-lingual inconsistencies
that jointly distort meaning while remaining superficially plausible. We
introduce BiMi, a bilingual multimodal framework that jointly performs
region-level localization, cross-modal and cross-lingual consistency detection,
and natural language explanation for misinformation analysis. To support
generalization, BiMi integrates an online retrieval module that supplements
model reasoning with up-to-date external context. We further release BiMiBench,
a large-scale and comprehensive benchmark constructed by systematically editing
real news images and subtitles, comprising 104,000 samples with realistic
manipulations across visual and linguistic modalities. To enhance
interpretability, we apply Group Relative Policy Optimization (GRPO) to improve
explanation quality, marking the first use of GRPO in this domain. Extensive
experiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in
classification accuracy, +15.9 in localization accuracy, and +2.5 in
explanation BERTScore, advancing state-of-the-art performance in realistic,
multilingual misinformation detection. Code, models, and datasets will be
released.

</details>


### [196] [Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data](https://arxiv.org/abs/2506.22939)
*Ghufran A. Omran,Wassan Saad Abduljabbar Hayale,Ahmad AbdulQadir AlRababah,Israa Ibraheem Al-Barazanchi,Ravi Sekhar,Pritesh Shah,Sushma Parihar,Harshavardhan Reddy Penubadi*

Main category: cs.CV

TL;DR: This paper proposes a Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO-BRNN) for remote sensing scene categorization, achieving 97% accuracy and surpassing existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation for this research is to address the challenges in achieving high accuracy in scene categorization of remotely acquired images, particularly due to the high noise levels and requirement for significant data variety in traditional deep learning models.

Method: The study introduces CO-BRNN as an innovative approach for remote sensing scene classification. It compares CO-BRNN's performance against existing methods: MLP-CNN, CNN-LSTM, LSTM-CRF, GB, MIRM-CF, and CNN-DA.

Result: CO-BRNN achieved the highest accuracy of 97%, outperforming other methods like LSTM-CRF with 90%, MLP-CNN with 85%, and CNN-LSTM with 80%.

Conclusion: The proposed CO-BRNN method demonstrates superior performance in remote sensing scene categorization, highlighting the importance of physical confirmation for satellite data validation.

Abstract: Scene categorization (SC) in remotely acquired images is an important subject
with broad consequences in different fields, including catastrophe control,
ecological observation, architecture for cities, and more. Nevertheless, its
several apps, reaching a high degree of accuracy in SC from distant observation
data has demonstrated to be difficult. This is because traditional conventional
deep learning models require large databases with high variety and high levels
of noise to capture important visual features. To address these problems, this
investigation file introduces an innovative technique referred to as the
Cuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type
of scenes in remote sensing data. The investigation compares the execution of
CO-BRNN with current techniques, including Multilayer Perceptron- Convolutional
Neural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory
(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),
Graph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional
Neural Networks Data Augmentation (CNN-DA). The results demonstrate that
CO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,
MLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance
of physical confirmation to ensure the efficiency of satellite data.

</details>


### [197] [YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class Loss for medical imaging](https://arxiv.org/abs/2506.22955)
*Haniyeh Nikkhah,Jafar Tanha,Mahdi Zarrin,SeyedEhsan Roshan,Amin Kazempour*

Main category: cs.CV

TL;DR: A novel model, YM-WML, for cardiac image segmentation is introduced, achieving superior performance on the ACDC dataset.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in medical image segmentation, including class imbalance and the complex structure of medical images.

Method: The YM-WML model features a strong backbone for feature extraction, a YOLOv11 neck for multi-scale feature aggregation, and an attention-based segmentation head. Additionally, a Weighted Multi-class Exponential (WME) loss function is developed to tackle class imbalance.

Result: YM-WML achieves a Dice Similarity Coefficient of 91.02 on the ACDC dataset, surpassing state-of-the-art methods with stable training and accurate segmentation.

Conclusion: YM-WML sets a new benchmark in cardiac image segmentation tasks with its innovative approach and strong generalization.

Abstract: Medical image segmentation poses significant challenges due to class
imbalance and the complex structure of medical images. To address these
challenges, this study proposes YM-WML, a novel model for cardiac image
segmentation. The model integrates a robust backbone for effective feature
extraction, a YOLOv11 neck for multi-scale feature aggregation, and an
attention-based segmentation head for precise and accurate segmentation. To
address class imbalance, we introduce the Weighted Multi-class Exponential
(WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity
Coefficient of 91.02, outperforming state-of-the-art methods. The model
demonstrates stable training, accurate segmentation, and strong generalization,
setting a new benchmark in cardiac segmentation tasks.

</details>


### [198] [Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images](https://arxiv.org/abs/2506.22960)
*Shreyas Dixit,Ashhar Aziz,Shashwat Bajpai,Vasu Sharma,Aman Chadha,Vinija Jain,Amitava Das*

Main category: cs.CV

TL;DR: The paper introduces PECCAVI, a watermarking method resistant to generative AI attacks that remove watermarks, particularly visual paraphrase attacks.


<details>
  <summary>Details</summary>
Motivation: The rapid increase in AI-generated content poses risks of political disinformation and necessitates robust watermarks to counter malicious manipulation, such as de-watermarking attacks.

Method: PECCAVI embeds watermarks in Non-Melting Points (NMPs) of images using multi-channel frequency domain techniques and applies noisy burnishing to resist reverse-engineering attempts.

Result: PECCAVI proves to be a distortion-free and robust watermarking solution that is resistant to generative AI de-watermarking attacks and is model-agnostic.

Conclusion: PECCAVI provides a secure and effective solution to the vulnerabilities of current watermarking techniques against sophisticated AI attacks, with all resources made publicly available.

Abstract: A report by the European Union Law Enforcement Agency predicts that by 2026,
up to 90 percent of online content could be synthetically generated, raising
concerns among policymakers, who cautioned that "Generative AI could act as a
force multiplier for political disinformation. The combined effect of
generative text, images, videos, and audio may surpass the influence of any
single modality." In response, California's Bill AB 3211 mandates the
watermarking of AI-generated images, videos, and audio. However, concerns
remain regarding the vulnerability of invisible watermarking techniques to
tampering and the potential for malicious actors to bypass them entirely.
Generative AI-powered de-watermarking attacks, especially the newly introduced
visual paraphrase attack, have shown an ability to fully remove watermarks,
resulting in a paraphrase of the original image. This paper introduces PECCAVI,
the first visual paraphrase attack-safe and distortion-free image watermarking
technique. In visual paraphrase attacks, an image is altered while preserving
its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI
strategically embeds watermarks within these NMPs and employs multi-channel
frequency domain watermarking. It also incorporates noisy burnishing to counter
reverse-engineering efforts aimed at locating NMPs to disrupt the embedded
watermark, thereby enhancing durability. PECCAVI is model-agnostic. All
relevant resources and codes will be open-sourced.

</details>


### [199] [ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment](https://arxiv.org/abs/2506.22967)
*Amir Aghdam,Vincent Tao Hu*

Main category: cs.CV

TL;DR: The paper introduces ActAlign, a framework for zero-shot fine-grained video classification using sequence alignment techniques, achieving significant accuracy without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current contrastive vision-language models fail to leverage temporal structures necessary for fine-grained video action classification in zero-shot settings.

Method: The method uses large language models to generate ordered sub-action sequences for each class and aligns them with video frames via Dynamic Time Warping in a shared embedding space.

Result: ActAlign achieves 30.5% accuracy on the challenging ActionAtlas benchmark, exceeding models with much larger parameters while requiring no video-text supervision.

Conclusion: Structured language priors combined with classical alignment methods can improve vision-language models' potential for zero-shot fine-grained video understanding.

Abstract: We address the task of zero-shot fine-grained video classification, where no
video examples or temporal annotations are available for unseen action classes.
While contrastive vision-language models such as SigLIP demonstrate strong
open-set recognition via mean-pooled image-text similarity, they fail to
capture the temporal structure critical for distinguishing fine-grained
activities. We introduce ActAlign, a zero-shot framework that formulates video
classification as sequence alignment. For each class, a large language model
generates an ordered sub-action sequence, which is aligned with video frames
using Dynamic Time Warping (DTW) in a shared embedding space. Without any
video-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the
extremely challenging ActionAtlas benchmark, where human accuracy is only
61.6%. ActAlign outperforms billion-parameter video-language models while using
approximately 8x less parameters. These results demonstrate that structured
language priors, combined with classical alignment techniques, offer a scalable
and general approach to unlocking the open-set recognition potential of
vision-language models for fine-grained video understanding.

</details>


### [200] [Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation](https://arxiv.org/abs/2506.22979)
*Jie Liu,Jiayi Shen,Pan Zhou,Jan-Jakob Sonke,Efstratios Gavves*

Main category: cs.CV

TL;DR: FewCLIP proposes a probabilistic prototype calibration framework to enhance performance in Generalized Few-Shot Semantic Segmentation (GFSS) by refining multi-modal prototypes from CLIP for better adaptability and generalization.


<details>
  <summary>Details</summary>
Motivation: Current GFSS methods using pretrained vision-language models like CLIP face challenges with deterministic prototype learning, which limits adaptability and leads to overfitting on limited novel class data.

Method: FewCLIP introduces a prototype calibration mechanism that refines textual prototypes with learnable visual calibration prototypes. A probabilistic distribution regularization is applied to these calibrated prototypes for uncertainty-aware and structured learning, improving adaptability in low-data scenarios.

Result: FewCLIP outperforms state-of-the-art methods on the PASCAL-5$^i$ and COCO-20$^i$ datasets in both GFSS and class-incremental settings, demonstrating its effectiveness.

Conclusion: FewCLIP effectively mitigates the limitations of deterministic prototype learning in GFSS by employing probabilistic calibration over CLIP’s multi-modal prototypes, resulting in superior generalization and adaptability on novel and base classes.

Abstract: Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a
segmentation model to novel classes with only a few annotated examples while
maintaining performance on base classes. Recently, pretrained vision-language
models (VLMs) such as CLIP have been leveraged in GFSS to improve
generalization on novel classes through multi-modal prototypes learning.
However, existing prototype-based methods are inherently deterministic,
limiting the adaptability of learned prototypes to diverse samples,
particularly for novel classes with scarce annotations. To address this, we
propose FewCLIP, a probabilistic prototype calibration framework over
multi-modal prototypes from the pretrained CLIP, thus providing more adaptive
prototype learning for GFSS. Specifically, FewCLIP first introduces a prototype
calibration mechanism, which refines frozen textual prototypes with learnable
visual calibration prototypes, leading to a more discriminative and adaptive
representation. Furthermore, unlike deterministic prototype learning
techniques, FewCLIP introduces distribution regularization over these
calibration prototypes. This probabilistic formulation ensures structured and
uncertainty-aware prototype learning, effectively mitigating overfitting to
limited novel class data while enhancing generalization. Extensive experimental
results on PASCAL-5$^i$ and COCO-20$^i$ datasets demonstrate that our proposed
FewCLIP significantly outperforms state-of-the-art approaches across both GFSS
and class-incremental setting. The code is available at
https://github.com/jliu4ai/FewCLIP.

</details>


### [201] [Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models](https://arxiv.org/abs/2506.22982)
*Atharv Mittal,Agam Pandey,Amritanshu Tiwari,Sukrit Jindal,Swadesh Swain*

Main category: cs.CV

TL;DR: The paper focuses on adversarial vulnerabilities in vision-language models (VLMs) and introduces improvements to enhance the effectiveness and generalizability of adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Vision-language models are revolutionizing computer vision tasks but remain highly susceptible to adversarial attacks. The study seeks to validate and improve upon existing attack methods to better understand these vulnerabilities.

Method: The study replicates and validates the Cross-Prompt Attack (CroPA), introduces a new initialization strategy, explores cross-image transferability using universal perturbations, and proposes a novel loss function for improving adversarial generalization.

Result: The proposed methods enhance the attack success rate and generalizability, outperforming existing baselines on popular VLMs such as Flamingo, BLIP-2, and InstructBLIP.

Conclusion: The paper highlights the critical need to address adversarial weaknesses in VLMs, offering a more robust framework for understanding and mitigating their security challenges in practice.

Abstract: Large Vision-Language Models (VLMs) have revolutionized computer vision,
enabling tasks such as image classification, captioning, and visual question
answering. However, they remain highly vulnerable to adversarial attacks,
particularly in scenarios where both visual and textual modalities can be
manipulated. In this study, we conduct a comprehensive reproducibility study of
"An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on
Vision-Language Models" validating the Cross-Prompt Attack (CroPA) and
confirming its superior cross-prompt transferability compared to existing
baselines. Beyond replication we propose several key improvements: (1) A novel
initialization strategy that significantly improves Attack Success Rate (ASR).
(2) Investigate cross-image transferability by learning universal
perturbations. (3) A novel loss function targeting vision encoder attention
mechanisms to improve generalization. Our evaluation across prominent VLMs --
including Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on
LLaVA validates the original results and demonstrates that our improvements
consistently boost adversarial effectiveness. Our work reinforces the
importance of studying adversarial vulnerabilities in VLMs and provides a more
robust framework for generating transferable adversarial examples, with
significant implications for understanding the security of VLMs in real-world
applications.

</details>


### [202] [A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks](https://arxiv.org/abs/2506.23004)
*Vaigai Nayaki Yokar,Hoa Le-Minh,Xicong Li,Wai Lok Woo,Luis Nero Alves,Stanislav Zvanovec,Tran The Son,Zabih Ghassemlooy*

Main category: cs.CV

TL;DR: The paper proposes a CNN-based technique for frame identification and synchronization in visible light communication, achieving a high accuracy of ~98.74%.


<details>
  <summary>Details</summary>
Motivation: To address challenges in screen-to-camera communication, such as blurring, cropping, and mobility-induced rotation, by enhancing frame identification and synchronization through visible light communication systems.

Method: The study utilizes a supervised CNN model trained on a dataset tailored to specific S2C challenges, built with Python and TensorFlow Keras. Real-time experiments conducted in Jupyter Notebook with introduced overhead frames for improved synchronization.

Result: The CNN model demonstrated high accuracy (~98.74%) in frame identification and synchronization, proving effective in handling S2C-specific challenges.

Conclusion: The proposed CNN-based technique is robust, accurate, and lightweight, substantially improving frame synchronization and identification for S2C VLC systems.

Abstract: This paper proposes a novel, robust, and lightweight supervised Convolutional
Neural Network (CNN)-based technique for frame identification and
synchronization, designed to enhance short-link communication performance in a
screen-to-camera (S2C) based visible light communication (VLC) system.
Developed using Python and the TensorFlow Keras framework, the proposed CNN
model was trained through three real-time experimental investigations conducted
in Jupyter Notebook. These experiments incorporated a dataset created from
scratch to address various real-time challenges in S2C communication, including
blurring, cropping, and rotated images in mobility scenarios. Overhead frames
were introduced for synchronization, which leads to enhanced system
performance. The experimental results demonstrate that the proposed model
achieves an overall accuracy of approximately 98.74%, highlighting its
effectiveness in identifying and synchronizing frames in S2C VLC systems.

</details>


### [203] [MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models](https://arxiv.org/abs/2506.23009)
*Jian Chen,Wenye Ma,Penghang Liu,Wei Wang,Tengwei Song,Ming Li,Chenguang Wang,Ruiyi Zhang,Changyou Chen*

Main category: cs.CV

TL;DR: The researchers introduce MusiXQA, a novel dataset and Phi-3-MusiX, a fine-tuned model, to evaluate and improve Multimodal Large Language Models (MLLMs) in music sheet understanding.


<details>
  <summary>Details</summary>
Motivation: To address the gap in MLLM's ability to interpret music sheets, which has been underexplored despite advancements in visual reasoning for other domains.

Method: The authors created MusiXQA, a synthetic dataset annotated for music sheet features, and fine-tuned an MLLM (Phi-3-MusiX) on this dataset for better performance.

Result: Phi-3-MusiX demonstrated significant improvements over GPT-based methods when evaluated using the MusiXQA dataset, highlighting the limitations of existing state-of-the-art MLLMs in understanding music sheets.

Conclusion: MusiXQA and Phi-3-MusiX establish an initial framework for advancing MLLMs' capabilities in interpreting music sheets, making this a promising direction for future research.

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable visual
reasoning abilities in natural images, text-rich documents, and graphic
designs. However, their ability to interpret music sheets remains
underexplored. To bridge this gap, we introduce MusiXQA, the first
comprehensive dataset for evaluating and advancing MLLMs in music sheet
understanding. MusiXQA features high-quality synthetic music sheets generated
via MusiXTeX, with structured annotations covering note pitch and duration,
chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.
Through extensive evaluations, we reveal significant limitations of current
state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed
Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant
performance gains over GPT-based methods. The proposed dataset and model
establish a foundation for future advances in MLLMs for music sheet
understanding. Code, data, and model will be released upon acceptance.

</details>


### [204] [VisionScores -- A system-segmented image score dataset for deep learning tasks](https://arxiv.org/abs/2506.23030)
*Alejandro Romero Amezcua,Mariano José Juan Rivera Meraz*

Main category: cs.CV

TL;DR: VisionScores is a dataset containing 24.8k grayscale segmented images of two-handed piano scores, structured into two scenarios: different composers, same type (Sonatinas) and one composer (Franz Liszt), various types.


<details>
  <summary>Details</summary>
Motivation: The work aims to provide a structured, highly-informative dataset to aid machine learning and deep learning tasks with image data from musical scores.

Method: The dataset consists of grayscale 128x512 images segmented for two settings: 1) 14k Sonatina segments from various composers, 2) 10.8k segments of various types from Franz Liszt.

Result: The final dataset includes 24.8k images, along with metadata, system orders, unsegmented full pages, and pre-formatted versions.

Conclusion: VisionScores offers a unique, structure-rich dataset tailored for music-related machine/deep learning tasks, enabling detailed studies with diverse segmentation options and metadata.

Abstract: VisionScores presents a novel proposal being the first system-segmented image
score dataset, aiming to offer structure-rich, high information-density images
for machine and deep learning tasks. Delimited to two-handed piano pieces, it
was built to consider not only certain graphic similarity but also composition
patterns, as this creative process is highly instrument-dependent. It provides
two scenarios in relation to composer and composition type. The first, formed
by 14k samples, considers works from different authors but the same composition
type, specifically, Sonatinas. The latter, consisting of 10.8K samples,
presents the opposite case, various composition types from the same author,
being the one selected Franz Liszt. All of the 24.8k samples are formatted as
grayscale jpg images of $128 \times 512$ pixels. VisionScores supplies the
users not only the formatted samples but the systems' order and pieces'
metadata. Moreover, unsegmented full-page scores and the pre-formatted images
are included for further analysis.

</details>


### [205] [Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation](https://arxiv.org/abs/2506.23038)
*Xinrong Hu,Yiyu Shi*

Main category: cs.CV

TL;DR: AugPaint introduces a data augmentation framework using latent diffusion models for generating image-label pairs to address limited labeled data in medical image segmentation.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of enhancing segmentation performance in medical datasets with scarce labeled data.

Method: Utilized latent diffusion models for inpainting tasks to generate synthetic image-label pairs without retraining.

Result: Extensive evaluations on four medical imaging datasets showed better performance compared to state-of-the-art label-efficient methods.

Conclusion: AugPaint is effective in improving segmentation tasks under limited annotation constraints.

Abstract: Collecting pixel-level labels for medical datasets can be a laborious and
expensive process, and enhancing segmentation performance with a scarcity of
labeled data is a crucial challenge. This work introduces AugPaint, a data
augmentation framework that utilizes inpainting to generate image-label pairs
from limited labeled data. AugPaint leverages latent diffusion models, known
for their ability to generate high-quality in-domain images with low overhead,
and adapts the sampling process for the inpainting task without need for
retraining. Specifically, given a pair of image and label mask, we crop the
area labeled with the foreground and condition on it during reversed denoising
process for every noise level. Masked background area would gradually be filled
in, and all generated images are paired with the label mask. This approach
ensures the accuracy of match between synthetic images and label masks, setting
it apart from existing dataset generation methods. The generated images serve
as valuable supervision for training downstream segmentation models,
effectively addressing the challenge of limited annotations. We conducted
extensive evaluations of our data augmentation method on four public medical
image segmentation datasets, including CT, MRI, and skin imaging. Results
across all datasets demonstrate that AugPaint outperforms state-of-the-art
label-efficient methodologies, significantly improving segmentation
performance.

</details>


### [206] [From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting](https://arxiv.org/abs/2506.23042)
*Hung Nguyen,An Le,Runfa Li,Truong Nguyen*

Main category: cs.CV

TL;DR: The paper proposes AutoOpti3DGS, a method to reduce Gaussian proliferation in 3D Gaussian Splatting (3DGS) during novel view synthesis while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) is fast for training and rendering in novel view synthesis but struggles with excessive Gaussian primitives, leading to memory and bandwidth issues.

Method: The authors use learnable Forward and Inverse Discrete Wavelet Transforms, with fixed low-pass filters, zero-initialized high-pass filters, and an orthogonality loss to gradually activate fine frequencies. This delays redundant fine Gaussian formation.

Result: Through experiments, AutoOpti3DGS demonstrated compatibility with existing 3DGS frameworks, required only one hyper-parameter, and achieved memory-efficient sparser scene representations.

Conclusion: AutoOpti3DGS efficiently limits Gaussian proliferation in 3DGS, ensuring sparser models with preserved visual fidelity, suitable for hardware-constrained environments.

Abstract: 3D Gaussian Splatting has emerged as a powerful approach in novel view
synthesis, delivering rapid training and rendering but at the cost of an
ever-growing set of Gaussian primitives that strains memory and bandwidth. We
introduce AutoOpti3DGS, a training-time framework that automatically restrains
Gaussian proliferation without sacrificing visual fidelity. The key idea is to
feed the input images to a sequence of learnable Forward and Inverse Discrete
Wavelet Transforms, where low-pass filters are kept fixed, high-pass filters
are learnable and initialized to zero, and an auxiliary orthogonality loss
gradually activates fine frequencies. This wavelet-driven, coarse-to-fine
process delays the formation of redundant fine Gaussians, allowing 3DGS to
capture global structure first and refine detail only when necessary. Through
extensive experiments, AutoOpti3DGS requires just a single filter learning-rate
hyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks,
and consistently produces sparser scene representations more compatible with
memory or storage-constrained hardware.

</details>


### [207] [Ovis-U1 Technical Report](https://arxiv.org/abs/2506.23044)
*Guo-Hua Wang,Shanshan Zhao,Xinjie Zhang,Liangfu Cao,Pengxin Zhan,Lunhao Duan,Shiyin Lu,Minghao Fu,Xiaohao Chen,Jianshan Zhao,Yang Li,Qing-Guo Chen*

Main category: cs.CV

TL;DR: Ovis-U1 is a unified model integrating multimodal understanding, text-to-image generation, and image editing, excelling in benchmark evaluations and surpassing state-of-the-art alternatives.


<details>
  <summary>Details</summary>
Motivation: Develop a unified model that combines multimodal understanding, image generation, and editing to enhance performance across tasks compared to specialized or frozen models.

Method: Ovis-U1 employs a diffusion-based visual decoder paired with a bidirectional token refiner and introduces a unified training approach starting from a language model.

Result: Ovis-U1 exceeds benchmarks: 69.6 on OpenCompass, 83.72 and 0.89 on text-to-image benchmarks, and 4.00 and 6.42 on image editing metrics, outperforming recent state-of-the-art models.

Conclusion: The unified training approach enables Ovis-U1 to push boundaries in multimodal tasks, showcasing improved capabilities and performance as the first model in its series.

Abstract: In this report, we introduce Ovis-U1, a 3-billion-parameter unified model
that integrates multimodal understanding, text-to-image generation, and image
editing capabilities. Building on the foundation of the Ovis series, Ovis-U1
incorporates a diffusion-based visual decoder paired with a bidirectional token
refiner, enabling image generation tasks comparable to leading models like
GPT-4o. Unlike some previous models that use a frozen MLLM for generation
tasks, Ovis-U1 utilizes a new unified training approach starting from a
language model. Compared to training solely on understanding or generation
tasks, unified training yields better performance, demonstrating the
enhancement achieved by integrating these two tasks. Ovis-U1 achieves a score
of 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent
state-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In
text-to-image generation, it excels with scores of 83.72 and 0.89 on the
DPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves
4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the
initial version of the Ovis unified model series, Ovis-U1 pushes the boundaries
of multimodal understanding, generation, and editing.

</details>


### [208] [Empowering Small VLMs to Think with Dynamic Memorization and Exploration](https://arxiv.org/abs/2506.23061)
*Jiazhen Liu,Yuchuan Deng,Long Chen*

Main category: cs.CV

TL;DR: The paper presents DyME, a novel training paradigm for improving the thinking capabilities of Small-scale Vision-Language Models (SVLMs) by dynamically switching between Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Reward (RLVR) during training.


<details>
  <summary>Details</summary>
Motivation: Existing paradigms require substantial base model capabilities, which SVLMs often lack, leading to issues such as pseudo thinking traces and performance collapse.

Method: DyME dynamically alternates between Memorization (SFT) and Exploration (RLVR) at each optimization step to achieve a balanced trade-off between these modes.

Result: Experiments demonstrate significant performance improvements of SVLMs across diverse domains when trained using DyME.

Conclusion: DyME proves to be a practical and effective method for enhancing reliable thinking abilities in SVLMs, overcoming limitations of prior training paradigms.

Abstract: Empowering Small-scale Vision-Language Models (SVLMs) with reliable thinking
capabilities remains fundamentally challenging due to their limited parameter
capacity and weak instruction-following abilities. Existing training paradigms,
including Supervised Fine-Tuning (SFT) and Reinforcement Learning with
Verifiable Reward (RLVR), impose substantial demands on the base VLM, exceeding
the capabilities of SVLMs. Consequently, directly applying these paradigms to
SVLMs often suffers from severe pseudo thinking traces and advantage collapse,
ultimately undermining both thinking reliability and task performance. A
natural solution is to combine SFT and RLVR, leveraging their complementarity
to reduce the dependence on model capacity. However, the widely adopted
two-stage training paradigm still performs poorly on SVLMs, as their tendency
toward sub-optimal convergence hinders the trade-off and limits the benefits of
the combination. To address this, we propose DyME, a novel training paradigm
that Dynamically selects between Memorization (via SFT) and Exploration (via
RLVR) modes at each optimization step, ensuring that every update contributes
to the trade-off. Extensive experiments across diverse domains demonstrate that
DyME consistently achieves this balance, and thus delivers substantial
performance improvements. These results establish DyME as a practical and
effective solution for empowering SVLMs with reliable thinking capabilities.
GitHub: https://github.com/HKUST-LongGroup/DyME

</details>


### [209] [CoreMark: Toward Robust and Universal Text Watermarking Technique](https://arxiv.org/abs/2506.23066)
*Jiale Meng,Yiming Li,Zheming Lu,Zewei He,Hao Luo,Tianwei Zhang*

Main category: cs.CV

TL;DR: The paper proposes CORE and CoreMark for improved text watermarking with enhanced robustness, generalizability, and imperceptibility.


<details>
  <summary>Details</summary>
Motivation: To address the critical challenges in text watermarking, such as robustness, generalizability, and imperceptibility, which current solutions struggle to achieve simultaneously.

Method: The authors introduce the CORE paradigm, modify the thickness of COREs to embed hidden data without causing significant visual distortions, and propose an embedding strength modulator to adapt to font sizes.

Result: CoreMark was experimentally shown to improve resistance against screenshot, print-scan, and print-camera attacks, while maintaining strong imperceptibility across multiple languages and fonts.

Conclusion: CoreMark successfully balances robustness, imperceptibility, and generalizability, advancing the state-of-the-art in text watermarking methods.

Abstract: Text watermarking schemes have gained considerable attention in recent years,
yet still face critical challenges in achieving simultaneous robustness,
generalizability, and imperceptibility. This paper introduces a new embedding
paradigm,termed CORE, which comprises several consecutively aligned black pixel
segments. Its key innovation lies in its inherent noise resistance during
transmission and broad applicability across languages and fonts. Based on the
CORE, we present a text watermarking framework named CoreMark. Specifically,
CoreMark first dynamically extracts COREs from characters. Then, the characters
with stronger robustness are selected according to the lengths of COREs. By
modifying the thickness of the CORE, the hidden data is embedded into the
selected characters without causing significant visual distortions. Moreover, a
general plug-and-play embedding strength modulator is proposed, which can
adaptively enhance the robustness for small font sizes by adjusting the
embedding strength according to the font size. Experimental evaluation
indicates that CoreMark demonstrates outstanding generalizability across
multiple languages and fonts. Compared to existing methods, CoreMark achieves
significant improvements in resisting screenshot, print-scan, and print camera
attacks, while maintaining satisfactory imperceptibility.

</details>


### [210] [Unsupervised 3D Braided Hair Reconstruction from a Single-View Image](https://arxiv.org/abs/2506.23072)
*Jing Gao*

Main category: cs.CV

TL;DR: The paper introduces an unsupervised method for reconstructing 3D braided hairstyles from single-view images, outperforming existing methods in accuracy and realism.


<details>
  <summary>Details</summary>
Motivation: Existing hair reconstruction methods struggle with the intricate and complex structures of braided hairstyles, which are not adequately addressed by current approaches designed for loose hair.

Method: The proposed method uses a novel synthetic braid model inspired by braid theory in an unsupervised pipeline to reconstruct 3D braided hairstyles from single-view RGB images.

Result: Experiments show that the proposed method is more accurate, realistic, and efficient compared to state-of-the-art approaches.

Conclusion: The method significantly advances the reconstruction of braided hairstyles, enabling expressive hairstyle modeling for digital humans.

Abstract: Reconstructing 3D braided hairstyles from single-view images remains a
challenging task due to the intricate interwoven structure and complex
topologies of braids. Existing strand-based hair reconstruction methods
typically focus on loose hairstyles and often struggle to capture the
fine-grained geometry of braided hair. In this paper, we propose a novel
unsupervised pipeline for efficiently reconstructing 3D braided hair from
single-view RGB images. Leveraging a synthetic braid model inspired by braid
theory, our approach effectively captures the complex intertwined structures of
braids. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches, providing superior accuracy, realism, and
efficiency in reconstructing 3D braided hairstyles, supporting expressive
hairstyle modeling in digital humans.

</details>


### [211] [Learning Counterfactually Decoupled Attention for Open-World Model Attribution](https://arxiv.org/abs/2506.23074)
*Yu Zheng,Boyang Gong,Fanye Kong,Yueqi Duan,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jiwen Lu,Jie Zhou*

Main category: cs.CV

TL;DR: This paper introduces the Counterfactually Decoupled Attention Learning (CDAL) method for improving open-world model attribution by addressing confounding biases and capturing essential generation patterns.


<details>
  <summary>Details</summary>
Motivation: Existing methods for model attribution struggle with handcrafted designs that are susceptible to confounding biases and novel attacks in open-world settings.

Method: CDAL uses counterfactual reasoning to decouple discriminative artifacts from confounding biases, optimizing attention maps based on causal relationships.

Result: CDAL significantly enhances state-of-the-art performance in open-world model attribution benchmarks, especially in handling unseen novel attacks.

Conclusion: The approach improves generalization capabilities and provides a robust solution for attribution tasks with minimal computational overhead.

Abstract: In this paper, we propose a Counterfactually Decoupled Attention Learning
(CDAL) method for open-world model attribution. Existing methods rely on
handcrafted design of region partitioning or feature space, which could be
confounded by the spurious statistical correlations and struggle with novel
attacks in open-world scenarios. To address this, CDAL explicitly models the
causal relationships between the attentional visual traces and source model
attribution, and counterfactually decouples the discriminative model-specific
artifacts from confounding source biases for comparison. In this way, the
resulting causal effect provides a quantification on the quality of learned
attention maps, thus encouraging the network to capture essential generation
patterns that generalize to unseen source models by maximizing the effect.
Extensive experiments on existing open-world model attribution benchmarks show
that with minimal computational overhead, our method consistently improves
state-of-the-art models by large margins, particularly for unseen novel
attacks. Source code: https://github.com/yzheng97/CDAL.

</details>


### [212] [Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization](https://arxiv.org/abs/2506.23077)
*Suofei Zhang,Xinxin Wang,Xiaofu Wu,Quan Zhou,Haifeng Hu*

Main category: cs.CV

TL;DR: This paper introduces the Distance-Aware Cross-View Geo-Localization (DACVGL) problem and proposes the first benchmark dataset DA-Campus, alongside a novel Dynamic Contrastive Learning (DyCL) framework to address the issue.


<details>
  <summary>Details</summary>
Motivation: The authors identify a gap in existing cross-view geo-localization methods, which lack focus on contextual understanding and minimizing localization errors, motivating them to propose a new benchmark and approach.

Method: The authors introduce DA-Campus, a benchmark dataset with multi-view imagery and distance annotations, and propose the DyCL framework, which uses dynamic contrastive learning with hierarchical spatial margins for cross-domain image retrieval.

Result: The proposed DyCL framework improves hierarchical retrieval and cross-view geo-localization accuracy, as evidenced by extensive experimental results.

Conclusion: The study demonstrates that DyCL is a promising method for addressing the DACVGL problem, outperforming traditional metric learning methods. The benchmark and code have been made publicly available.

Abstract: Existing deep learning-based cross-view geo-localization methods primarily
focus on improving the accuracy of cross-domain image matching, rather than
enabling models to comprehensively capture contextual information around the
target and minimize the cost of localization errors. To support systematic
research into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem,
we construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs
multi-view imagery with precise distance annotations across three spatial
resolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical
retrieval problem across different domains. Our study further reveals that, due
to the inherent complexity of spatial relationships among buildings, this
problem can only be addressed via a contrastive learning paradigm, rather than
conventional metric learning. To tackle this challenge, we propose Dynamic
Contrastive Learning (DyCL), a novel framework that progressively aligns
feature representations according to hierarchical spatial margins. Extensive
experiments demonstrate that DyCL is highly complementary to existing
multi-scale metric learning methods and yields substantial improvements in both
hierarchical retrieval performance and overall cross-view geo-localization
accuracy. Our code and benchmark are publicly available at
https://github.com/anocodetest1/DyCL.

</details>


### [213] [Frequency-enhanced Multi-granularity Context Network for Efficient Vertebrae Segmentation](https://arxiv.org/abs/2506.23086)
*Jian Shi,Tianqi You,Pingping Zhang,Hongli Zhang,Rui Xu,Haojie Li*

Main category: cs.CV

TL;DR: The study presents FMC-Net for enhancing 3D CT and MRI vertebra segmentation accuracy by leveraging wavelet transform, high-frequency refinements, and multi-granularity contexts.


<details>
  <summary>Details</summary>
Motivation: Existing vertebra segmentation methods struggle with image blurring and distinguishing similar vertebrae due to imaging limitations and spinal complexity.

Method: FMC-Net employs wavelet transform for lossless downsampling. High-frequency components are refined using High-frequency Feature Refinement (HFR), while low-frequency components are processed using a Multi-granularity State Space Model (MG-SSM) to extract spatially-varying contexts.

Result: FMC-Net demonstrates superior performance compared to state-of-the-art methods on CT and MRI vertebra segmentation datasets.

Conclusion: The approach effectively reduces feature distortion in blurred images and improves segmentation accuracy, particularly for similar vertebrae. The source code is available online for further exploration.

Abstract: Automated and accurate segmentation of individual vertebra in 3D CT and MRI
images is essential for various clinical applications. Due to the limitations
of current imaging techniques and the complexity of spinal structures, existing
methods still struggle with reducing the impact of image blurring and
distinguishing similar vertebrae. To alleviate these issues, we introduce a
Frequency-enhanced Multi-granularity Context Network (FMC-Net) to improve the
accuracy of vertebrae segmentation. Specifically, we first apply wavelet
transform for lossless downsampling to reduce the feature distortion in blurred
images. The decomposed high and low-frequency components are then processed
separately. For the high-frequency components, we apply a High-frequency
Feature Refinement (HFR) to amplify the prominence of key features and filter
out noises, restoring fine-grained details in blurred images. For the
low-frequency components, we use a Multi-granularity State Space Model (MG-SSM)
to aggregate feature representations with different receptive fields,
extracting spatially-varying contexts while capturing long-range dependencies
with linear complexity. The utilization of multi-granularity contexts is
essential for distinguishing similar vertebrae and improving segmentation
accuracy. Extensive experiments demonstrate that our method outperforms
state-of-the-art approaches on both CT and MRI vertebrae segmentation datasets.
The source code is publicly available at https://github.com/anaanaa/FMCNet.

</details>


### [214] [Where, What, Why: Towards Explainable Driver Attention Prediction](https://arxiv.org/abs/2506.23088)
*Yuchen Zhou,Jiayu Tang,Xiaoyan Xiao,Yueyao Lin,Linkai Liu,Zipeng Guo,Hao Fei,Xiaobo Xia,Chao Gou*

Main category: cs.CV

TL;DR: This paper introduces Explainable Driver Attention Prediction (EDAP), a method to predict, parse, and explain driver attention using a new dataset (W3DA) and a model (LLada).


<details>
  <summary>Details</summary>
Motivation: Existing attention prediction methods fail to explain the cognitive motivations behind where drivers focus, limiting our understanding of how attention works in driving contexts.

Method: The paper proposes a paradigm (EDAP) that predicts ‘where’ drivers look, parses the semantics of their attention (‘what’), and explains the cognitive reasons (‘why’). This is supported by W3DA, a new dataset with semantic and causal annotations. Additionally, LLada, an end-to-end model, unifies pixel modeling, semantic parsing, and cognitive reasoning.

Result: Experiments show LLada exhibits strong generalization across datasets and driving conditions, demonstrating its effectiveness for driver attention prediction.

Conclusion: The paper advances understanding of driver attention, offering insights for autonomous vehicles, driver training, and human-computer interaction by introducing an explainable and comprehensive prediction framework.

Abstract: Modeling task-driven attention in driving is a fundamental challenge for both
autonomous vehicles and cognitive science. Existing methods primarily predict
where drivers look by generating spatial heatmaps, but fail to capture the
cognitive motivations behind attention allocation in specific contexts, which
limits deeper understanding of attention mechanisms. To bridge this gap, we
introduce Explainable Driver Attention Prediction, a novel task paradigm that
jointly predicts spatial attention regions (where), parses attended semantics
(what), and provides cognitive reasoning for attention allocation (why). To
support this, we present W3DA, the first large-scale explainable driver
attention dataset. It enriches existing benchmarks with detailed semantic and
causal annotations across diverse driving scenarios, including normal
conditions, safety-critical situations, and traffic accidents. We further
propose LLada, a Large Language model-driven framework for driver attention
prediction, which unifies pixel modeling, semantic parsing, and cognitive
reasoning within an end-to-end architecture. Extensive experiments demonstrate
the effectiveness of LLada, exhibiting robust generalization across datasets
and driving conditions. This work serves as a key step toward a deeper
understanding of driver attention mechanisms, with significant implications for
autonomous driving, intelligent driver training, and human-computer
interaction.

</details>


### [215] [DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation](https://arxiv.org/abs/2506.23104)
*Jihun Kim,Hoyong Kwon,Hyeokjun Kweon,Wooseong Jeong,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: DC-TTA enhances interactive segmentation by adapting SAM using a novel test-time adaptation approach, focusing on a divide-and-conquer strategy for handling complex objects.


<details>
  <summary>Details</summary>
Motivation: SAM struggles in specialized domains or complex scenarios like camouflaged or multi-part object segmentation, necessitating improvement.

Method: DC-TTA partitions user clicks into subsets, applies test-time adaptation with specialized models individually, and integrates them into a unified predictor.

Result: Experimental results show DC-TTA outperforms SAM's zero-shot capability and existing TTA methods, achieving higher accuracy and fewer user interactions.

Conclusion: DC-TTA provides a significant enhancement in handling challenging segmentation tasks by leveraging user interactions and dividing cues for localized modeling.

Abstract: Interactive segmentation (IS) allows users to iteratively refine object
boundaries with minimal cues, such as positive and negative clicks. While the
Segment Anything Model (SAM) has garnered attention in the IS community for its
promptable segmentation capabilities, it often struggles in specialized domains
or when handling complex scenarios (e.g., camouflaged or multi-part objects).
To overcome these challenges, we propose DC-TTA, a novel test-time adaptation
(TTA) framework that adapts SAM on a per-sample basis by leveraging user
interactions as supervision. Instead of forcing a single model to incorporate
all user clicks at once, DC-TTA partitions the clicks into more coherent
subsets, each processed independently via TTA with a separated model. This
Divide-and-Conquer strategy reduces conflicts among diverse cues and enables
more localized updates. Finally, we merge the adapted models to form a unified
predictor that integrates the specialized knowledge from each subset.
Experimental results across various benchmarks demonstrate that DC-TTA
significantly outperforms SAM's zero-shot results and conventional TTA methods,
effectively handling complex tasks such as camouflaged object segmentation with
fewer interactions and improved accuracy.

</details>


### [216] [Computer-Aided Multi-Stroke Character Simplification by Stroke Removal](https://arxiv.org/abs/2506.23106)
*Ryo Ishiyama,Shinnosuke Matsuo,Seiichi Uchida*

Main category: cs.CV

TL;DR: The paper proposes a method to simplify complex multi-stroke characters (e.g., Chinese, Japanese) by removing strokes while maintaining legibility, aiming to aid learning and communication.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of learning and recognizing highly complex multi-stroke characters, especially for non-native learners, by exploring methods to simplify these characters without sacrificing their legibility.

Method: The researchers utilize a character recognition model to systematically identify and remove strokes from multi-stroke characters while ensuring minimal loss of legibility. Experiments were conducted on 1,256 character classes.

Result: The experiments demonstrate that numerous characters remain distinguishable even after multiple strokes are removed, suggesting the feasibility of simplified character systems.

Conclusion: The results indicate that systematic stroke removal can simplify complex characters effectively while preserving legibility, highlighting the potential for standardized simplification methods.

Abstract: Multi-stroke characters in scripts such as Chinese and Japanese can be highly
complex, posing significant challenges for both native speakers and,
especially, non-native learners. If these characters can be simplified without
degrading their legibility, it could reduce learning barriers for non-native
speakers, facilitate simpler and legible font designs, and contribute to
efficient character-based communication systems. In this paper, we propose a
framework to systematically simplify multi-stroke characters by selectively
removing strokes while preserving their overall legibility. More specifically,
we use a highly accurate character recognition model to assess legibility and
remove those strokes that minimally impact it. Experimental results on 1,256
character classes with 5, 10, 15, and 20 strokes reveal several key findings,
including the observation that even after removing multiple strokes, many
characters remain distinguishable. These findings suggest the potential for
more formalized simplification strategies.

</details>


### [217] [Hierarchical Corpus-View-Category Refinement for Carotid Plaque Risk Grading in Ultrasound](https://arxiv.org/abs/2506.23108)
*Zhiyuan Zhu,Jian Wang,Yong Jiang,Tong Han,Yuhao Huang,Ang Zhang,Kaiwen Yang,Mingyuan Luo,Zhe Liu,Yaofei Duan,Dong Ni,Tianhong Tang,Xin Yang*

Main category: cs.CV

TL;DR: This paper presents a novel framework called Corpus-View-Category Refinement Framework (CVC-RF) to enhance carotid plaque grading (CPG) using multi-level refinements for improved clinical assessments.


<details>
  <summary>Details</summary>
Motivation: Accurate carotid plaque grading is critical for evaluating risks of cardiovascular and cerebrovascular diseases, but challenges such as small size and high variability of plaques complicate the task. Existing methods fail to adequately address representation learning and class feature differences.

Method: The CVC-RF framework introduces three-level refinements: (1) Corpus-level with center-memory contrastive loss for global modeling, (2) View-level with a cascaded down-sampling attention module for multi-scale information fusion, and (3) Category-level with a parameter-free mixture-of-experts weighting strategy for feature decoupling.

Result: The CVC-RF framework achieves state-of-the-art performance by effectively modeling global features through multi-level refinements, as demonstrated in experiments.

Conclusion: This work sets a new benchmark in deep learning-based carotid plaque grading by improving representation learning and addressing differences in class features using a novel CVC-RF framework.

Abstract: Accurate carotid plaque grading (CPG) is vital to assess the risk of
cardiovascular and cerebrovascular diseases. Due to the small size and high
intra-class variability of plaque, CPG is commonly evaluated using a
combination of transverse and longitudinal ultrasound views in clinical
practice. However, most existing deep learning-based multi-view classification
methods focus on feature fusion across different views, neglecting the
importance of representation learning and the difference in class features. To
address these issues, we propose a novel Corpus-View-Category Refinement
Framework (CVC-RF) that processes information from Corpus-, View-, and
Category-levels, enhancing model performance. Our contribution is four-fold.
First, to the best of our knowledge, we are the foremost deep learning-based
method for CPG according to the latest Carotid Plaque-RADS guidelines. Second,
we propose a novel center-memory contrastive loss, which enhances the network's
global modeling capability by comparing with representative cluster centers and
diverse negative samples at the Corpus level. Third, we design a cascaded
down-sampling attention module to fuse multi-scale information and achieve
implicit feature interaction at the View level. Finally, a parameter-free
mixture-of-experts weighting strategy is introduced to leverage class
clustering knowledge to weight different experts, enabling feature decoupling
at the Category level. Experimental results indicate that CVC-RF effectively
models global features via multi-level refinement, achieving state-of-the-art
performance in the challenging CPG task.

</details>


### [218] [MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings](https://arxiv.org/abs/2506.23115)
*Haonan Chen,Hong Liu,Yuping Luo,Liang Wang,Nan Yang,Furu Wei,Zhicheng Dou*

Main category: cs.CV

TL;DR: The paper introduces MoCa, a two-stage strategy to address limitations in multimodal embedding models, with improvements in bidirectional reasoning, scalability, and diversity using pre-trained Vision Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: Current multimodal embedding models using causal VLMs are suboptimal due to challenges in attention mechanisms, data scalability, and limited objective diversity. This paper aims to overcome these shortcomings.

Method: The framework includes two stages: (1) Modality-aware Continual Pre-training, which denoises interleaved text and image inputs to boost bidirectional context understanding, and (2) Heterogeneous Contrastive Fine-tuning, utilizing diverse multimodal datasets to enhance alignment and generalization.

Result: MoCa achieves new state-of-the-art performance across MMEB and ViDoRe-v2 benchmarks and demonstrates strong scalability with large models and massive datasets.

Conclusion: MoCa successfully transforms pre-trained VLMs into robust bidirectional multimodal embedding models, addressing scalability, reasoning, and representational robustness challenges.

Abstract: Multimodal embedding models, built upon causal Vision Language Models (VLMs),
have shown promise in various tasks. However, current approaches face three key
limitations: the use of causal attention in VLM backbones is suboptimal for
embedding tasks; scalability issues due to reliance on high-quality labeled
paired data for contrastive learning; and limited diversity in training
objectives and data. To address these issues, we propose MoCa, a two-stage
framework for transforming pre-trained VLMs into effective bidirectional
multimodal embedding models. The first stage, Modality-aware Continual
Pre-training, introduces a joint reconstruction objective that simultaneously
denoises interleaved text and image inputs, enhancing bidirectional
context-aware reasoning. The second stage, Heterogeneous Contrastive
Fine-tuning, leverages diverse, semantically rich multimodal data beyond simple
image-caption pairs to enhance generalization and alignment. Our method
addresses the stated limitations by introducing bidirectional attention through
continual pre-training, scaling effectively with massive unlabeled datasets via
joint reconstruction objectives, and utilizing diverse multimodal data for
enhanced representation robustness. Experiments demonstrate that MoCa
consistently improves performance across MMEB and ViDoRe-v2 benchmarks,
achieving new state-of-the-art results, and exhibits strong scalability with
both model size and training data on MMEB.

</details>


### [219] [Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation](https://arxiv.org/abs/2506.23120)
*Zhenhua Ning,Zhuotao Tian,Shaoshuai Shi,Guangming Lu,Daojing He,Wenjie Pei,Li Jiang*

Main category: cs.CV

TL;DR: The paper proposes a framework, R$^2$S, for reasoning-based segmentation of 3D point clouds and introduces the 3D ReasonSeg dataset to improve spatial reasoning in scene understanding.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex instructions that depend on accurate spatial reasoning despite 3D point cloud data having detailed spatial information.

Method: The proposed approach, R$^2$S, mimics human reasoning by decomposing spatial reasoning into identifying relevant elements and processing instructions with visual priors. Additionally, a new dataset, 3D ReasonSeg, is created to address these issues.

Result: Experiments show R$^2$S and 3D ReasonSeg enhance spatial reasoning capabilities in 3D point cloud perception.

Conclusion: The R$^2$S framework and 3D ReasonSeg dataset are effective in addressing spatial reasoning challenges and can serve as benchmarks for future studies.

Abstract: Recent advances in point cloud perception have demonstrated remarkable
progress in scene understanding through vision-language alignment leveraging
large language models (LLMs). However, existing methods may still encounter
challenges in handling complex instructions that require accurate spatial
reasoning, even if the 3D point cloud data provides detailed spatial cues such
as size and position for identifying the targets. To tackle this issue, we
propose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based
segmentation framework. The framework emulates human cognitive processes by
decomposing spatial reasoning into two sequential stages: first identifying
relevant elements, then processing instructions guided by their associated
visual priors. Furthermore, acknowledging the inadequacy of existing datasets
in complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based
segmentation dataset comprising 25,185 training samples and 3,966 validation
samples with precise annotations. Both quantitative and qualitative experiments
demonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud
perception with stronger spatial reasoning capabilities, and we hope that they
can serve as a new baseline and benchmark for future work.

</details>


### [220] [Dare to Plagiarize? Plagiarized Painting Recognition and Retrieval](https://arxiv.org/abs/2506.23132)
*Sophie Zhou,Shu Kong*

Main category: cs.CV

TL;DR: The paper aims to detect art plagiarism by retrieving similar artworks and explains the detected plagiarisms using a constructed dataset. Two methods are tested: a baseline using DINOv2 and a finetuned model with metric learning loss.


<details>
  <summary>Details</summary>
Motivation: To address the difficult challenge of detecting plagiarized paintings and protecting artists' copyrights through explaining and identifying artwork similarities.

Method: The authors employ DINOv2 as a visual foundation model to retrieve similar artworks and classify plagiarism. They then enhance retrieval precision by finetuning DINOv2 with metric learning loss.

Result: Baseline DINOv2 achieved 97.2% recognition accuracy but low retrieval precision (29.0% AP). The finetuned model showed a 12% improvement in retrieval precision but lower recognition accuracy (92.7%).

Conclusion: While the baseline method reached high recognition accuracy, it faltered in precision. The finetuning improved retrieval precision but led to reduced recognition accuracy. The outcomes highlight trade-offs and provide groundwork for future advancements.

Abstract: Art plagiarism detection plays a crucial role in protecting artists'
copyrights and intellectual property, yet it remains a challenging problem in
forensic analysis. In this paper, we address the task of recognizing
plagiarized paintings and explaining the detected plagarisms by retrieving
visually similar authentic artworks. To support this study, we construct a
dataset by collecting painting photos and synthesizing plagiarized versions
using generative AI, tailored to specific artists' styles. We first establish a
baseline approach using off-the-shelf features from the visual foundation model
DINOv2 to retrieve the most similar images in the database and classify
plagiarism based on a similarity threshold. Surprisingly, this non-learned
method achieves a high recognition accuracy of 97.2\% but suffers from low
retrieval precision 29.0\% average precision (AP). To improve retrieval
quality, we finetune DINOv2 with a metric learning loss using positive and
negative sample pairs sampled in the database. The finetuned model greatly
improves retrieval performance by 12\% AP over the baseline, though it
unexpectedly results in a lower recognition accuracy (92.7\%). We conclude with
insightful discussions and outline directions for future research.

</details>


### [221] [VisualPrompter: Prompt Optimization with Visual Feedback for Text-to-Image Synthesis](https://arxiv.org/abs/2506.23138)
*Shiyu Wu,Mingzhen Sun,Weining Wang,Yequan Wang,Jing Liu*

Main category: cs.CV

TL;DR: This paper introduces VisualPrompter, a tool to refine user prompts for better text-to-image alignment in diffusion models.


<details>
  <summary>Details</summary>
Motivation: Address the gap between user-provided prompts and model-preferred prompts, which results in visually appealing but semantically mismatched images.

Method: VisualPrompter employs self-reflection to detect missing concepts and optimizes prompts to align with model preferences.

Result: Achieved state-of-the-art performance on benchmarks for text-image alignment evaluation.

Conclusion: VisualPrompter effectively enhances semantic alignment of generated images and supports various generative models with its plug-and-play design.

Abstract: Since there exists a notable gap between user-provided and model-preferred
prompts, generating high-quality and satisfactory images using diffusion models
often requires prompt engineering to optimize user inputs. Current studies on
text-to-image prompt engineering can effectively enhance the style and
aesthetics of generated images. However, they often neglect the semantic
alignment between generated images and user descriptions, resulting in visually
appealing but content-wise unsatisfying outputs. In this work, we propose
VisualPrompter, a novel training-free prompt engineering framework that refines
user inputs to model-preferred sentences. In particular, VisualPrompter
utilizes an automatic self-reflection module to identify the missing concepts
in generated images and a target-specific prompt optimization mechanism to
revise the prompts in a fine-grained manner. Extensive experiments demonstrate
the effectiveness of our VisualPrompter, which achieves new state-of-the-art
performance on multiple benchmarks for text-image alignment evaluation.
Additionally, our framework features a plug-and-play design, making it highly
adaptable to various generative models.

</details>


### [222] [AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation](https://arxiv.org/abs/2506.23150)
*Xinyue Liang,Zhiyuan Ma,Lingchen Sun,Yanjun Guo,Lei Zhang*

Main category: cs.CV

TL;DR: AlignCVC addresses cross-view consistency (CVC) issues in single-image-to-3D generation by aligning generated and reconstructed distributions, significantly boosting quality and reducing inference steps.


<details>
  <summary>Details</summary>
Motivation: Current single-image-to-3D methods suffer from poor cross-view consistency in intermediate multi-view images, harming reconstruction performance and making refinement attempts impractical due to noisy outputs.

Method: AlignCVC utilizes distribution alignment instead of strict regression losses. It employs a soft-hard alignment strategy targeting both generation and reconstruction models to enhance cross-view consistency and generation quality.

Result: AlignCVC improves CVC, significantly accelerates inference (down to 4 steps), and integrates seamlessly with various multi-view generation and reconstruction models.

Conclusion: AlignCVC establishes a reliable approach for single-image-to-3D generation by refining CVC through distribution alignment, showing effectiveness and efficiency in experiments.

Abstract: Single-image-to-3D models typically follow a sequential generation and
reconstruction workflow. However, intermediate multi-view images synthesized by
pre-trained generation models often lack cross-view consistency (CVC),
significantly degrading 3D reconstruction performance. While recent methods
attempt to refine CVC by feeding reconstruction results back into the
multi-view generator, these approaches struggle with noisy and unstable
reconstruction outputs that limit effective CVC improvement. We introduce
AlignCVC, a novel framework that fundamentally re-frames single-image-to-3D
generation through distribution alignment rather than relying on strict
regression losses. Our key insight is to align both generated and reconstructed
multi-view distributions toward the ground-truth multi-view distribution,
establishing a principled foundation for improved CVC. Observing that generated
images exhibit weak CVC while reconstructed images display strong CVC due to
explicit rendering, we propose a soft-hard alignment strategy with distinct
objectives for generation and reconstruction models. This approach not only
enhances generation quality but also dramatically accelerates inference to as
few as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC,
seamlessly integrates various multi-view generation models with 3D
reconstruction models. Extensive experiments demonstrate the effectiveness and
efficiency of AlignCVC for single-image-to-3D generation.

</details>


### [223] [MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation](https://arxiv.org/abs/2506.23151)
*Vladislav Bargatin,Egor Chistov,Alexander Yakovenko,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: The study introduces MEMFOF, a memory-efficient multi-frame optical flow estimation method that achieves state-of-the-art accuracy and efficiency for high-resolution inputs while requiring minimal GPU memory.


<details>
  <summary>Details</summary>
Motivation: The uncontrolled growth in GPU memory consumption for high-resolution optical flow estimation demands solutions that balance accuracy with memory efficiency.

Method: The method leverages reduced correlation volumes, high-resolution training protocols, and multi-frame estimation, integrated within RAFT-like architectures, to minimize GPU memory usage and boost performance.

Result: MEMFOF achieves high accuracy with minimal memory overhead, requiring only 2.09 GB for 1080p runtime and 28.5 GB during training. It outperforms resource-intensive methods on benchmarks like Spring, Sintel (clean), and KITTI-2015.

Conclusion: MEMFOF demonstrates a significant advancement in high-resolution optical flow estimation by achieving a favorable trade-off between performance and GPU memory efficiency. Its state-of-the-art results and code availability position it as a reliable tool for related tasks.

Abstract: Recent advances in optical flow estimation have prioritized accuracy at the
cost of growing GPU memory consumption, particularly for high-resolution
(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical
flow method that identifies a favorable trade-off between multi-frame
estimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU
memory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely
positions our method to be trained at native 1080p without the need for
cropping or downsampling. We systematically revisit design choices from
RAFT-like architectures, integrating reduced correlation volumes and
high-resolution training protocols alongside multi-frame estimation, to achieve
state-of-the-art performance across multiple benchmarks while substantially
reducing memory overhead. Our method outperforms more resource-intensive
alternatives in both accuracy and runtime efficiency, validating its robustness
for flow estimation at high resolutions. At the time of submission, our method
ranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,
leads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the
best Fl-all error on KITTI-2015 at 2.94%. The code is available at
https://github.com/msu-video-group/memfof.

</details>


### [224] [Dynamic View Synthesis from Small Camera Motion Videos](https://arxiv.org/abs/2506.23153)
*Huiqiang Sun,Xingyi Li,Juewen Peng,Liao Shen,Zhiguo Cao,Ke Xian,Guosheng Lin*

Main category: cs.CV

TL;DR: The paper addresses challenges in novel view synthesis for dynamic 3D scenes with limited camera motion by introducing Distribution-based Depth Regularization (DDR) and enhanced camera parameter learning, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current NeRF-based methods for novel view synthesis struggle under small camera motion due to inaccurate scene geometry and camera parameters.

Method: The paper introduces DDR to align rendering weight distribution with true distribution, leveraging Gumbel-softmax sampling, and enforces volume density constraints for accurate geometry. It also includes camera parameter learning during training.

Result: Experimental results show that the proposed approach handles scenarios with limited camera motion effectively, outperforming existing methods in terms of scene representation.

Conclusion: Their approach improves novel view synthesis for cases with restricted camera motion, offering robust scene geometry and camera parameter estimation alongside visualization tools.

Abstract: Novel view synthesis for dynamic $3$D scenes poses a significant challenge.
Many notable efforts use NeRF-based approaches to address this task and yield
impressive results. However, these methods rely heavily on sufficient motion
parallax in the input images or videos. When the camera motion range becomes
limited or even stationary (i.e., small camera motion), existing methods
encounter two primary challenges: incorrect representation of scene geometry
and inaccurate estimation of camera parameters. These challenges make prior
methods struggle to produce satisfactory results or even become invalid. To
address the first challenge, we propose a novel Distribution-based Depth
Regularization (DDR) that ensures the rendering weight distribution to align
with the true distribution. Specifically, unlike previous methods that use
depth loss to calculate the error of the expectation, we calculate the
expectation of the error by using Gumbel-softmax to differentiably sample
points from discrete rendering weight distribution. Additionally, we introduce
constraints that enforce the volume density of spatial points before the object
boundary along the ray to be near zero, ensuring that our model learns the
correct geometry of the scene. To demystify the DDR, we further propose a
visualization tool that enables observing the scene geometry representation at
the rendering weight level. For the second challenge, we incorporate camera
parameter learning during training to enhance the robustness of our model to
camera parameters. We conduct extensive experiments to demonstrate the
effectiveness of our approach in representing scenes with small camera motion
input, and our results compare favorably to state-of-the-art methods.

</details>


### [225] [Self-Supervised Contrastive Learning for Multi-Label Images](https://arxiv.org/abs/2506.23156)
*Jiale Chen*

Main category: cs.CV

TL;DR: The paper presents a self-supervised learning (SSL) method tailored for multi-label images with fewer data by introducing block-wise augmentation and image-aware contrastive loss to improve representation learning.


<details>
  <summary>Details</summary>
Motivation: Mainstream SSL methods are designed for single-label image datasets (e.g., ImageNet) and suffer from high pre-training overhead. Multi-label images, which carry richer semantic information, are often neglected despite their potential in broader scenarios.

Method: Proposes a block-wise augmentation module to create more potential positive view pairs from multi-label images. This is combined with an image-aware contrastive loss to align views for semantically consistent representations.

Result: The approach showed competitive performance through linear fine-tuning and transfer learning evaluations, even when working with low-quality and small-sample datasets.

Conclusion: The proposed SSL method effectively addresses the challenges of learning representations from multi-label images, demonstrating efficiency and competitiveness with fewer data.

Abstract: Self-supervised learning (SSL) has demonstrated its effectiveness in learning
representations through comparison methods that align with human intuition.
However, mainstream SSL methods heavily rely on high body datasets with single
label, such as ImageNet, resulting in intolerable pre-training overhead.
Besides, more general multi-label images are frequently overlooked in SSL,
despite their potential for richer semantic information and broader
applicability in downstream scenarios. Therefore, we tailor the mainstream SSL
approach to guarantee excellent representation learning capabilities using
fewer multi-label images. Firstly, we propose a block-wise augmentation module
aimed at extracting additional potential positive view pairs from multi-label
images. Subsequently, an image-aware contrastive loss is devised to establish
connections between these views, thereby facilitating the extraction of
semantically consistent representations. Comprehensive linear fine-tuning and
transfer learning validate the competitiveness of our approach despite
challenging sample quality and quantity.

</details>


### [226] [STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene](https://arxiv.org/abs/2506.23157)
*Hanyu Zhou,Haonan Wang,Haoyue Liu,Yuxing Duan,Luxin Yan,Gim Hee Lee*

Main category: cs.CV

TL;DR: The paper presents a novel framework addressing mismatched spatiotemporal features by disentangling latent representations for static backgrounds and dynamic objects using event and frame cameras.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with mismatched spatiotemporal features between backgrounds and objects in dynamic scene reconstruction.

Method: Introducing event cameras and using a spatiotemporal-disentangled Gaussian splatting framework for clustering and manipulating scene-object representations.

Result: The proposed method demonstrates improved spatiotemporal discrimination between backgrounds and objects in dynamic scenes.

Conclusion: Cumulative disentanglement effectively renders time-continuous dynamic scenes, verified through extensive experiments showcasing the method’s superiority.

Abstract: High-dynamic scene reconstruction aims to represent static background with
rigid spatial features and dynamic objects with deformed continuous
spatiotemporal features. Typically, existing methods adopt unified
representation model (e.g., Gaussian) to directly match the spatiotemporal
features of dynamic scene from frame camera. However, this unified paradigm
fails in the potential discontinuous temporal features of objects due to frame
imaging and the heterogeneous spatial features between background and objects.
To address this issue, we disentangle the spatiotemporal features into various
latent representations to alleviate the spatiotemporal mismatching between
background and objects. In this work, we introduce event camera to compensate
for frame camera, and propose a spatiotemporal-disentangled Gaussian splatting
framework for high-dynamic scene reconstruction. As for dynamic scene, we
figure out that background and objects have appearance discrepancy in
frame-based spatial features and motion discrepancy in event-based temporal
features, which motivates us to distinguish the spatiotemporal features between
background and objects via clustering. As for dynamic object, we discover that
Gaussian representations and event data share the consistent spatiotemporal
characteristic, which could serve as a prior to guide the spatiotemporal
disentanglement of object Gaussians. Within Gaussian splatting framework, the
cumulative scene-object disentanglement can improve the spatiotemporal
discrimination between background and objects to render the time-continuous
dynamic scene. Extensive experiments have been performed to verify the
superiority of the proposed method.

</details>


### [227] [Trident: Detecting Face Forgeries with Adversarial Triplet Learning](https://arxiv.org/abs/2506.23189)
*Mustafa Hakan Kara,Aysegul Dundar,Uğur Güdükbay*

Main category: cs.CV

TL;DR: The paper introduces 'Trident,' a deep learning framework for detecting face forgeries using triplet learning in a Siamese network with advanced training techniques for improved adaptability and robustness.


<details>
  <summary>Details</summary>
Motivation: As face manipulation technologies become more advanced, detecting forgeries has become challenging, especially against techniques unseen during training.

Method: Trident employs a Siamese network architecture trained with triplet learning and domain-adversarial training. It also isolates fine-grained features and prevents gradient flow to enhance generalizability and robustness.

Result: Evaluations and ablation studies show that Trident is effective across multiple benchmarks in detecting diverse face forgeries.

Conclusion: Trident offers significant improvements in detecting face forgeries, particularly against unseen manipulation methods, and its code will be publicly released for further use.

Abstract: As face forgeries generated by deep neural networks become increasingly
sophisticated, detecting face manipulations in digital media has posed a
significant challenge, underscoring the importance of maintaining digital media
integrity and combating visual disinformation. Current detection models,
predominantly based on supervised training with domain-specific data, often
falter against forgeries generated by unencountered techniques. In response to
this challenge, we introduce \textit{Trident}, a face forgery detection
framework that employs triplet learning with a Siamese network architecture for
enhanced adaptability across diverse forgery methods. \textit{Trident} is
trained on curated triplets to isolate nuanced differences of forgeries,
capturing fine-grained features that distinguish pristine samples from
manipulated ones while controlling for other variables. To further enhance
generalizability, we incorporate domain-adversarial training with a forgery
discriminator. This adversarial component guides our embedding model towards
forgery-agnostic representations, improving its robustness to unseen
manipulations. In addition, we prevent gradient flow from the classifier head
to the embedding model, avoiding overfitting induced by artifacts peculiar to
certain forgeries. Comprehensive evaluations across multiple benchmarks and
ablation studies demonstrate the effectiveness of our framework. We will
release our code in a GitHub repository.

</details>


### [228] [DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding](https://arxiv.org/abs/2506.23196)
*Mona Ahmadian,Amir Shirian,Frank Guerin,Andrew Gilbert*

Main category: cs.CV

TL;DR: This paper introduces DEL, a framework designed for dense semantic action localization in real-world untrimmed videos. Using advanced multimodal interaction modeling techniques, it achieves state-of-the-art performance across various temporal action localization datasets.


<details>
  <summary>Details</summary>
Motivation: Real-world videos pose complex challenges due to overlapping events and intricate temporal dependencies. This necessitates robust methods for comprehensive action localization across different modalities.

Method: DEL framework employs two key modules: 1) masked self-attention for aligning intra-mode audio-visual features, and 2) a multimodal interaction refinement module that models cross-modal dependencies at multiple scales, capturing both high-level semantics and fine-grained details.

Result: DEL demonstrates superior average performance compared to previous methods, showcasing significant mAP improvements across Temporal Action Localization datasets: UnAV-100 (+3.3%), THUMOS14 (+2.6%), ActivityNet 1.3 (+1.2%), EPIC-Kitchens-100 verbs (+1.7%) and nouns (+1.4%).

Conclusion: DEL proves to be an effective and scalable solution for dense semantic action localization in real-world untrimmed videos, offering significant advancements in state-of-the-art multimodal interaction modeling.

Abstract: Real-world videos often contain overlapping events and complex temporal
dependencies, making multimodal interaction modeling particularly challenging.
We introduce DEL, a framework for dense semantic action localization, aiming to
accurately detect and classify multiple actions at fine-grained temporal
resolutions in long untrimmed videos. DEL consists of two key modules: the
alignment of audio and visual features that leverage masked self-attention to
enhance intra-mode consistency and a multimodal interaction refinement module
that models cross-modal dependencies across multiple scales, enabling
high-level semantics and fine-grained details. Our method achieves
state-of-the-art performance on multiple real-world Temporal Action
Localization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and
EPIC-Kitchens-100, surpassing previous approaches with notable average mAP
gains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.

</details>


### [229] [Transformer-Based Person Search with High-Frequency Augmentation and Multi-Wave Mixing](https://arxiv.org/abs/2506.23202)
*Qilin Shu,Qixian Zhang,Qi Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: The paper proposes the HAMW method for person search, addressing high-frequency feature suppression and computational inefficiency in transformers.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing transformer-based models for person search, namely, suppression of high-frequency features and high computational cost.

Method: The proposed HAMW framework includes high-frequency augmentation for enhanced feature learning and uses multi-level Haar wavelet fusion to replace self-attention layers, reducing computational complexity and improving feature discrimination.

Result: HAMW outperforms existing methods, achieving state-of-the-art results on CUHK-SYSU and PRW datasets.

Conclusion: HAMW effectively solves key challenges in transformer-based person search models, enhancing performance and efficiency.

Abstract: The person search task aims to locate a target person within a set of scene
images. In recent years, transformer-based models in this field have made some
progress. However, they still face three primary challenges: 1) the
self-attention mechanism tends to suppress high-frequency components in the
features, which severely impacts model performance; 2) the computational cost
of transformers is relatively high. To address these issues, we propose a novel
High-frequency Augmentation and Multi-Wave mixing (HAMW) method for person
search. HAMW is designed to enhance the discriminative feature extraction
capabilities of transformers while reducing computational overhead and
improving efficiency. Specifically, we develop a three-stage framework that
progressively optimizes both detection and re-identification performance. Our
model enhances the perception of high-frequency features by learning from
augmented inputs containing additional high-frequency components. Furthermore,
we replace the self-attention layers in the transformer with a strategy based
on multi-level Haar wavelet fusion to capture multi-scale features. This not
only lowers the computational complexity but also alleviates the suppression of
high-frequency features and enhances the ability to exploit multi-scale
information. Extensive experiments demonstrate that HAMW achieves
state-of-the-art performance on both the CUHK-SYSU and PRW datasets.

</details>


### [230] [UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding](https://arxiv.org/abs/2506.23219)
*Jie Feng,Shengyuan Wang,Tianhui Liu,Yanxin Xi,Yong Li*

Main category: cs.CV

TL;DR: The paper introduces UrbanLLaVA, a multi-modal language model specifically designed to handle diverse urban research datasets and tasks by leveraging a customized training framework.


<details>
  <summary>Details</summary>
Motivation: Urban research involves complex, multi-modal data, and existing methods lack a unified approach for processing such data comprehensively. The rise of multi-modal large language models (MLLMs) provides a potential solution.

Method: The authors built UrbanLLaVA using a curated instruction dataset that spans various urban datasets and proposed a multi-stage training framework separating spatial reasoning enhancement from domain knowledge learning. They also extend benchmarks for urban research evaluation.

Result: UrbanLLaVA excels in single-modal and cross-modal urban research tasks, outperforming existing open-source and proprietary MLLMs in evaluations across three cities.

Conclusion: UrbanLLaVA demonstrates strong capabilities for urban research, showcasing superior performance, generalizability across cities, and openly accessible resources for further research.

Abstract: Urban research involves a wide range of scenarios and tasks that require the
understanding of multi-modal data. Current methods often focus on specific data
types and lack a unified framework in urban field for processing them
comprehensively. The recent success of multi-modal large language models
(MLLMs) presents a promising opportunity to overcome this limitation. In this
paper, we introduce $\textit{UrbanLLaVA}$, a multi-modal large language model
designed to process these four types of data simultaneously and achieve strong
performance across diverse urban tasks compared with general MLLMs. In
$\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset
encompassing both single-modal and cross-modal urban data, spanning from
location view to global view of urban environment. Additionally, we propose a
multi-stage training framework that decouples spatial reasoning enhancement
from domain knowledge learning, thereby improving the compatibility and
downstream performance of $\textit{UrbanLLaVA}$ across diverse urban tasks.
Finally, we also extend existing benchmark for urban research to assess the
performance of MLLMs across a wide range of urban tasks. Experimental results
from three cities demonstrate that $\textit{UrbanLLaVA}$ outperforms
open-source and proprietary MLLMs in both single-modal tasks and complex
cross-modal tasks and shows robust generalization abilities across cities.
Source codes and data are openly accessible to the research community via
https://github.com/tsinghua-fib-lab/UrbanLLaVA.

</details>


### [231] [BridgeShape: Latent Diffusion Schrödinger Bridge for 3D Shape Completion](https://arxiv.org/abs/2506.23205)
*Dequan Kong,Zhe Zhu,Honghua Chen,Mingqiang Wei*

Main category: cs.CV

TL;DR: BridgeShape proposes a novel framework for 3D shape completion through latent diffusion Schrödinger bridge, addressing limitations in existing diffusion-based methods by optimizing global transport paths and improving fine geometric details.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D shape completion struggle with modeling optimal global transport paths and face resolution constraints when operating in voxel spaces, leading to suboptimal outputs.

Method: BridgeShape formulates shape completion as an optimal transport problem using latent diffusion Schrödinger bridge and introduces a Depth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) enriched with DINOv2 features.

Result: BridgeShape achieves state-of-the-art performance on large-scale benchmarks, delivering high-fidelity 3D shape completion across various resolutions and unseen object classes.

Conclusion: The proposed BridgeShape framework addresses significant challenges in 3D shape completion by ensuring globally coherent transformations and mitigating resolution constraints via a compact latent space representation.

Abstract: Existing diffusion-based 3D shape completion methods typically use a
conditional paradigm, injecting incomplete shape information into the denoising
network via deep feature interactions (e.g., concatenation, cross-attention) to
guide sampling toward complete shapes, often represented by voxel-based
distance functions. However, these approaches fail to explicitly model the
optimal global transport path, leading to suboptimal completions. Moreover,
performing diffusion directly in voxel space imposes resolution constraints,
limiting the generation of fine-grained geometric details. To address these
challenges, we propose BridgeShape, a novel framework for 3D shape completion
via latent diffusion Schr\"odinger bridge. The key innovations lie in two
aspects: (i) BridgeShape formulates shape completion as an optimal transport
problem, explicitly modeling the transition between incomplete and complete
shapes to ensure a globally coherent transformation. (ii) We introduce a
Depth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D
shapes into a compact latent space, leveraging self-projected multi-view depth
information enriched with strong DINOv2 features to enhance geometric
structural perception. By operating in a compact yet structurally informative
latent space, BridgeShape effectively mitigates resolution constraints and
enables more efficient and high-fidelity 3D shape completion. BridgeShape
achieves state-of-the-art performance on large-scale 3D shape completion
benchmarks, demonstrating superior fidelity at higher resolutions and for
unseen object classes.

</details>


### [232] [TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints](https://arxiv.org/abs/2506.23207)
*Zhen Tan,Xieyuanli Chen,Lei Feng,Yangbing Ge,Shuaifeng Zhi,Jiaxiong Liu,Dewen Hu*

Main category: cs.CV

TL;DR: TVG-SLAM improves robustness and fidelity in 3D scene representation for RGB-only SLAM systems using a novel tri-view geometry paradigm, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-only SLAM systems heavily rely on photometric rendering loss, which makes camera tracking less robust, especially in challenging outdoor environments with severe viewpoint and lighting changes.

Method: TVG-SLAM introduces a tri-view geometry paradigm, incorporating dense tri-view matching, Hybrid Geometric Constraints for tracking, and a probabilistic initialization strategy for mapping. Additionally, it utilizes a Dynamic Attenuation of Rendering Trust mechanism to further enhance performance.

Result: TVG-SLAM demonstrated improved robustness and mapping quality in experiments across multiple outdoor datasets, reducing the average Absolute Trajectory Error (ATE) by 69% in the most challenging scenarios while achieving state-of-the-art rendering quality.

Conclusion: TVG-SLAM addresses the limitations of traditional RGB-only SLAM systems, delivering both robust camera tracking and high-fidelity scene representation. Its implementation will be made open-source.

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM
systems to achieve high-fidelity scene representation. However, the heavy
reliance of existing systems on photometric rendering loss for camera tracking
undermines their robustness, especially in unbounded outdoor environments with
severe viewpoint and illumination changes. To address these challenges, we
propose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel
tri-view geometry paradigm to ensure consistent tracking and high-quality
mapping. We introduce a dense tri-view matching module that aggregates reliable
pairwise correspondences into consistent tri-view matches, forming robust
geometric constraints across frames. For tracking, we propose Hybrid Geometric
Constraints, which leverage tri-view matches to construct complementary
geometric cues alongside photometric loss, ensuring accurate and stable pose
estimation even under drastic viewpoint shifts and lighting variations. For
mapping, we propose a new probabilistic initialization strategy that encodes
geometric uncertainty from tri-view correspondences into newly initialized
Gaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust
mechanism to mitigate tracking drift caused by mapping latency. Experiments on
multiple public outdoor datasets show that our TVG-SLAM outperforms prior
RGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our
method improves tracking robustness, reducing the average Absolute Trajectory
Error (ATE) by 69.0\% while achieving state-of-the-art rendering quality. The
implementation of our method will be released as open-source.

</details>


### [233] [A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans](https://arxiv.org/abs/2506.23209)
*Chia-Wen Huang,Haw Hwai,Chien-Chang Lee,Pei-Yuan Wu*

Main category: cs.CV

TL;DR: The paper proposes a deep learning model that enhances appendicitis diagnosis using 3D CT scans and achieves improved diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: The need for timely and precise appendicitis diagnosis to prevent complications, compounded by the overwhelming number of cases and limited radiology resources.

Method: A deep learning model employing 3D CT scans with Slice Attention mechanisms guided by 2D datasets, along with a hierarchical framework using pre-trained 2D models for classification.

Result: The approach improves diagnostic performance, with an AUC increase of 3% for general appendicitis and 5.9% for complicated cases.

Conclusion: The proposed method offers a faster and more reliable diagnostic alternative, addressing challenges in small lesion detection and classification.

Abstract: Timely and accurate diagnosis of appendicitis is critical in clinical
settings to prevent serious complications. While CT imaging remains the
standard diagnostic tool, the growing number of cases can overwhelm
radiologists, potentially causing delays. In this paper, we propose a deep
learning model that leverages 3D CT scans for appendicitis classification,
incorporating Slice Attention mechanisms guided by external 2D datasets to
enhance small lesion detection. Additionally, we introduce a hierarchical
classification framework using pre-trained 2D models to differentiate between
simple and complicated appendicitis. Our approach improves AUC by 3% for
appendicitis and 5.9% for complicated appendicitis, offering a more efficient
and reliable diagnostic solution compared to previous work.

</details>


### [234] [High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation](https://arxiv.org/abs/2506.23227)
*Lunhao Duan,Shanshan Zhao,Xingxing Weng,Jing Zhang,Gui-Song Xia*

Main category: cs.CV

TL;DR: The paper tackles the challenge of indoor point cloud semantic segmentation with only scene-level annotations, introducing a method to generate high-quality pseudo-labels through multi-modal information and semantic consistency.


<details>
  <summary>Details</summary>
Motivation: The challenge in leveraging scene-level annotations for point cloud segmentation lies in generating accurate pseudo-labels from imprecise annotation, which affects segmentation performance significantly.

Method: The approach combines a cross-modal feature guidance module to align 2D image and 3D point cloud features along with a region-point semantic consistency module to guide semantic predictions using region-voting strategies.

Result: The proposed framework achieves significant better segmentation performance on ScanNet v2 and S3DIS datasets using scene-level annotations compared to prior works, showing the effectiveness of the method.

Conclusion: By leveraging multi-modal alignment and semantic consistency, the method creates high-quality pseudo-labels, improving segmentation tasks under limited annotation conditions. The release of code ensures reproducibility.

Abstract: This paper investigates indoor point cloud semantic segmentation under
scene-level annotation, which is less explored compared to methods relying on
sparse point-level labels. In the absence of precise point-level labels,
current methods first generate point-level pseudo-labels, which are then used
to train segmentation models. However, generating accurate pseudo-labels for
each point solely based on scene-level annotations poses a considerable
challenge, substantially affecting segmentation performance. Consequently, to
enhance accuracy, this paper proposes a high-quality pseudo-label generation
framework by exploring contemporary multi-modal information and region-point
semantic consistency. Specifically, with a cross-modal feature guidance module,
our method utilizes 2D-3D correspondences to align point cloud features with
corresponding 2D image pixels, thereby assisting point cloud feature learning.
To further alleviate the challenge presented by the scene-level annotation, we
introduce a region-point semantic consistency module. It produces regional
semantics through a region-voting strategy derived from point-level semantics,
which are subsequently employed to guide the point-level semantic predictions.
Leveraging the aforementioned modules, our method can rectify inaccurate
point-level semantic predictions during training and obtain high-quality
pseudo-labels. Significant improvements over previous works on ScanNet v2 and
S3DIS datasets under scene-level annotation can demonstrate the effectiveness.
Additionally, comprehensive ablation studies validate the contributions of our
approach's individual components. The code is available at
https://github.com/LHDuan/WSegPC .

</details>


### [235] [VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions](https://arxiv.org/abs/2506.23236)
*Marko Mihajlovic,Siwei Zhang,Gen Li,Kaifeng Zhao,Lea Müller,Siyu Tang*

Main category: cs.CV

TL;DR: This paper introduces VolumetricSMPL, a neural volumetric human body model, which achieves faster and more efficient processing compared to prior models, while maintaining high accuracy for tasks in computer graphics and vision.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in traditional surface mesh-based parametric human body models, such as inefficient handling of geometric interactions, and gaps in performance and resource usage in existing volumetric neural implicit body models.

Method: The authors present VolumetricSMPL, leveraging Neural Blend Weights (NBW) to generate efficient MLP decoders that are computationally lightweight. It replaces large MLPs with dynamically blended small weight matrices, improving computational efficiency and expressiveness.

Result: VolumetricSMPL outperforms previous models like COAP by achieving 10x faster inference, 6x lower GPU memory usage, higher accuracy, and providing a Signed Distance Function (SDF) for contact modeling. It proves effective across tasks like human-object interaction reconstruction, egocentric mesh recovery, motion synthesis, and resolving self-intersections.

Conclusion: VolumetricSMPL presents significant efficiency gains and broad applicability, making it a strong candidate for addressing challenges in handling human motion and human-environment interactions in graphics and vision.

Abstract: Parametric human body models play a crucial role in computer graphics and
vision, enabling applications ranging from human motion analysis to
understanding human-environment interactions. Traditionally, these models use
surface meshes, which pose challenges in efficiently handling interactions with
other geometric entities, such as objects and scenes, typically represented as
meshes or point clouds. To address this limitation, recent research has
explored volumetric neural implicit body models. However, existing works are
either insufficiently robust for complex human articulations or impose high
computational and memory costs, limiting their widespread use. To this end, we
introduce VolumetricSMPL, a neural volumetric body model that leverages Neural
Blend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike
prior approaches that rely on large MLPs, NBW dynamically blends a small set of
learned weight matrices using predicted shape- and pose-dependent coefficients,
significantly improving computational efficiency while preserving
expressiveness. VolumetricSMPL outperforms prior volumetric occupancy model
COAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,
and a Signed Distance Function (SDF) for efficient and differentiable contact
modeling. We demonstrate VolumetricSMPL's strengths across four challenging
tasks: (1) reconstructing human-object interactions from in-the-wild images,
(2) recovering human meshes in 3D scenes from egocentric views, (3)
scene-constrained motion synthesis, and (4) resolving self-intersections. Our
results highlight its broad applicability and significant performance and
efficiency gains.

</details>


### [236] [Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification](https://arxiv.org/abs/2506.23247)
*James Hinns,David Martens*

Main category: cs.CV

TL;DR: The paper introduces Segment Attribution Tables (SATs), a method leveraging saliency maps to summarize local explanations into semi-global insights, aiding in understanding image classifications.


<details>
  <summary>Details</summary>
Motivation: There is a gap between highly localized explanations like saliency maps and overly simplified global methods. The authors aim to create a tool that addresses this gap for better model analysis.

Method: This paper proposes SATs, which quantify the influence of image segments (e.g., 'eyes' in Chihuahuas) by leveraging saliency maps, combined with segmentation maps to provide named segments of interest.

Result: SATs revealed meaningful concepts and spurious correlations within classifiers, even in cases with robust out-of-distribution test accuracy.

Conclusion: The method provides a balance between local and global insights, enabling more effective debugging and analysis of image classification models.

Abstract: Deep learning dominates image classification tasks, yet understanding how
models arrive at predictions remains a challenge. Much research focuses on
local explanations of individual predictions, such as saliency maps, which
visualise the influence of specific pixels on a model's prediction. However,
reviewing many of these explanations to identify recurring patterns is
infeasible, while global methods often oversimplify and miss important local
behaviours. To address this, we propose Segment Attribution Tables (SATs), a
method for summarising local saliency explanations into (semi-)global insights.
SATs take image segments (such as "eyes" in Chihuahuas) and leverage saliency
maps to quantify their influence. These segments highlight concepts the model
relies on across instances and reveal spurious correlations, such as reliance
on backgrounds or watermarks, even when out-of-distribution test performance
sees little change. SATs can explain any classifier for which a form of
saliency map can be produced, using segmentation maps that provide named
segments. SATs bridge the gap between oversimplified global summaries and
overly detailed local explanations, offering a practical tool for analysing and
debugging image classifiers.

</details>


### [237] [DGE-YOLO: Dual-Branch Gathering and Attention for Accurate UAV Object Detection](https://arxiv.org/abs/2506.23252)
*Kunwei Lv,Ping Lan*

Main category: cs.CV

TL;DR: This paper introduces DGE-YOLO, a framework improving multi-modal UAV object detection using specialized architectures and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the challenges of detecting small objects in complex aerial scenarios, especially when processing multi-modal inputs, a limitation in existing methods.

Method: The researchers propose DGE-YOLO, a dual-branch model for handling infrared and visible images, an Efficient Multi-scale Attention mechanism for spatial learning, and a Gather-and-Distribute module to avoid feature loss.

Result: DGE-YOLO outperformed state-of-the-art methods in experiments on the Drone Vehicle dataset, proving its superior performance at multi-modal object detection.

Conclusion: DGE-YOLO effectively enhances object detection in UAV applications by fusing multi-modal data and employing improved architectural components.

Abstract: The rapid proliferation of unmanned aerial vehicles (UAVs) has highlighted
the importance of robust and efficient object detection in diverse aerial
scenarios. Detecting small objects under complex conditions, however, remains a
significant challenge. Existing approaches often prioritize inference speed,
leading to degraded performance when handling multi-modal inputs. To address
this, we present DGE-YOLO, an enhanced YOLO-based detection framework designed
to effectively fuse multi-modal information. Specifically, we introduce a
dual-branch architecture for modality-specific feature extraction, enabling the
model to process both infrared and visible images. To further enrich semantic
representation, we propose an Efficient Multi-scale Attention (EMA) mechanism
that enhances feature learning across spatial scales. Additionally, we replace
the conventional neck with a Gather-and-Distribute module to mitigate
information loss during feature aggregation. Extensive experiments on the Drone
Vehicle dataset demonstrate that DGE-YOLO achieves superior performance over
state-of-the-art methods, validating its effectiveness in multi-modal UAV
object detection tasks.

</details>


### [238] [PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution](https://arxiv.org/abs/2506.23254)
*Aradhana Mishra,Bumshik Lee*

Main category: cs.CV

TL;DR: PixelBoost is a novel diffusion model for image super-resolution that achieves high realism and texture fidelity while maintaining faster inference speeds through a sigmoidal noise sequencing method.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between realistic image generation and computational efficiency in diffusion-based image super-resolution models, particularly under limited sampling steps.

Method: Introduce PixelBoost, a novel diffusion model leveraging controlled stochasticity of Brownian motion, sigmoidal noise sequencing, and adaptive learning to improve texture capture, edge reconstruction, and inference speed.

Result: PixelBoost demonstrated superior performance in metrics like LPIPS, LOE, PSNR, and SSIM, along with improved edge reconstruction and faster inference times.

Conclusion: PixelBoost effectively balances realism, computational efficiency, and adaptive learning, making it a promising approach for advanced image super-resolution tasks.

Abstract: Diffusion-model-based image super-resolution techniques often face a
trade-off between realistic image generation and computational efficiency. This
issue is exacerbated when inference times by decreasing sampling steps,
resulting in less realistic and hazy images. To overcome this challenge, we
introduce a novel diffusion model named PixelBoost that underscores the
significance of embracing the stochastic nature of Brownian motion in advancing
image super-resolution, resulting in a high degree of realism, particularly
focusing on texture and edge definitions. By integrating controlled
stochasticity into the training regimen, our proposed model avoids convergence
to local optima, effectively capturing and reproducing the inherent uncertainty
of image textures and patterns. Our proposed model demonstrates superior
objective results in terms of learned perceptual image patch similarity
(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),
structural similarity index measure (SSIM), as well as visual quality. To
determine the edge enhancement, we evaluated the gradient magnitude and pixel
value, and our proposed model exhibited a better edge reconstruction
capability. Additionally, our model demonstrates adaptive learning capabilities
by effectively adjusting to Brownian noise patterns and introduces a sigmoidal
noise sequencing method that simplifies training, resulting in faster inference
speeds.

</details>


### [239] [PCLVis: Visual Analytics of Process Communication Latency in Large-Scale Simulation](https://arxiv.org/abs/2506.23257)
*Chongke Bi,Xin Gao,Baofeng Fu,Yuheng Zhao,Siming Chen,Ying Zhao,Yunhai Wang*

Main category: cs.CV

TL;DR: This paper proposes PCLVis, a framework for analyzing process communication latency (PCL) events in parallel computing without relying on physical link information by leveraging MPI data and advanced visualization techniques.


<details>
  <summary>Details</summary>
Motivation: Scalability challenges in large-scale simulations due to high communication costs among parallel processes motivate the development of a user-accessible framework for analyzing communication latency.

Method: The framework includes: (1) spatial clustering of highly correlated processes using a process-correlation tree, (2) propagation path analysis via a communication-dependency-based directed acyclic graph (DAG) with a sliding window algorithm and a new communication state glyph (CS-Glyph) for visualization, and (3) attribution strategy to optimize simulations.

Result: PCLVis effectively analyzes PCL events on the TH-1A supercomputer, enabling users to identify issues and significantly enhance simulation efficiency.

Conclusion: PCLVis equips general users with tools to analyze and address communication latency in simulations, reducing reliance on administrators and improving scalability.

Abstract: Large-scale simulations on supercomputers have become important tools for
users. However, their scalability remains a problem due to the huge
communication cost among parallel processes. Most of the existing communication
latency analysis methods rely on the physical link layer information, which is
only available to administrators. In this paper, a framework called PCLVis is
proposed to help general users analyze process communication latency (PCL)
events. Instead of the physical link layer information, the PCLVis uses the MPI
process communication data for the analysis. First, a spatial PCL event
locating method is developed. All processes with high correlation are
classified into a single cluster by constructing a process-correlation tree.
Second, the propagation path of PCL events is analyzed by constructing a
communication-dependency-based directed acyclic graph (DAG), which can help
users interactively explore a PCL event from the temporal evolution of a
located PCL event cluster. In this graph, a sliding window algorithm is
designed to generate the PCL events abstraction. Meanwhile, a new glyph called
the communication state glyph (CS-Glyph) is designed for each process to show
its communication states, including its in/out messages and load balance. Each
leaf node can be further unfolded to view additional information. Third, a PCL
event attribution strategy is formulated to help users optimize their
simulations. The effectiveness of the PCLVis framework is demonstrated by
analyzing the PCL events of several simulations running on the TH-1A
supercomputer. By using the proposed framework, users can greatly improve the
efficiency of their simulations.

</details>


### [240] [Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis](https://arxiv.org/abs/2506.23263)
*Lei-lei Li,Jianwu Fang,Junbin Xiao,Shanmin Pang,Hongkai Yu,Chen Lv,Jianru Xue,Tat-Seng Chua*

Main category: cs.CV

TL;DR: This paper introduces Causal-VidSyn, a novel diffusion model aimed at synthesizing egocentric traffic accident videos that incorporate causal relationships visually, using data like driver fixations and textual accident descriptions.


<details>
  <summary>Details</summary>
Motivation: Understanding causes and effects of traffic accidents egocentrically is essential for improving self-driving car safety, and synthetic accident videos can help test and train autonomous systems for rare, costly accident scenarios.

Method: The authors developed Causal-VidSyn, a causal video diffusion model that uses driver gaze-related data and accident descriptions to identify accident participants and behaviors. It includes modules for accident reasoning and gaze-conditioned data selection.

Result: Causal-VidSyn demonstrated superior performance compared to state-of-the-art video diffusion models in terms of frame quality and causal sensitivity across tasks such as accident video editing, normal-to-accident transformation, and text-to-video synthesis.

Conclusion: The proposed method provides advancements in synthetic accident video creation, highlighting the importance of causal entity grounding and accident participant identification for egocentric video applications in self-driving scenarios.

Abstract: Egocentricly comprehending the causes and effects of car accidents is crucial
for the safety of self-driving cars, and synthesizing causal-entity reflected
accident videos can facilitate the capability test to respond to unaffordable
accidents in reality. However, incorporating causal relations as seen in
real-world videos into synthetic videos remains challenging. This work argues
that precisely identifying the accident participants and capturing their
related behaviors are of critical importance. In this regard, we propose a
novel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic
accident videos. To enable causal entity grounding in video diffusion,
Causal-VidSyn leverages the cause descriptions and driver fixations to identify
the accident participants and behaviors, facilitated by accident reason
answering and gaze-conditioned selection modules. To support Causal-VidSyn, we
further construct Drive-Gaze, the largest driver gaze dataset (with 1.54M
frames of fixations) in driving accident scenarios. Extensive experiments show
that Causal-VidSyn surpasses state-of-the-art video diffusion models in terms
of frame quality and causal sensitivity in various tasks, including accident
video editing, normal-to-accident video diffusion, and text-to-video
generation.

</details>


### [241] [Token Activation Map to Visually Explain Multimodal LLMs](https://arxiv.org/abs/2506.23270)
*Yi Li,Hualiang Wang,Xinpeng Ding,Haonan Wang,Xiaomeng Li*

Main category: cs.CV

TL;DR: The paper introduces Token Activation Map (TAM), a novel explanation method for Multimodal Large Language Models (MLLMs) that improves visualization and reliability by addressing redundant activations in token sequences.


<details>
  <summary>Details</summary>
Motivation: Explainability of MLLMs is underexplored, leading to challenges in understanding, credibility, and effective visualization, particularly due to interference from redundant context token activations.

Method: The authors propose an estimated causal inference method combined with a rank Gaussian filter to reduce activation noise and highlight token interactions. TAM distills reliable explanations for multiple tokens, unlike single-output explanations like CAM.

Result: The TAM method significantly outperforms state-of-the-art methods in generating high-quality visualizations, aiding use cases like object localization, analysis of failure cases, model comparisons, and understanding complex scenarios in MLLMs.

Conclusion: TAM enhances MLLM explanations by addressing token-interference issues, providing superior visualization results for understanding models in diverse applications. The method offers advancements over existing approaches.

Abstract: Multimodal large language models (MLLMs) are broadly empowering various
fields. Despite their advancements, the explainability of MLLMs remains less
explored, hindering deeper understanding, model credibility, and effective
visualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that
produce a single output, MLLMs generate sequences of tokens progressively,
where each generated token depends on the previous context. Therefore, earlier
context tokens can introduce redundant activations that interfere with the
explanation of later tokens beyond their original information. Existing studies
often overlook this issue, but our observations reveal that these redundant
correlations can significantly hurt the reliability of explanations. To address
this, we propose an estimated causal inference method to mitigate the
interference of context to achieve high-quality MLLM explanation, with a novel
rank Gaussian filter to further reduce activation noises. We term this method
Token Activation Map (TAM) to highlight the consideration of interactions
between tokens. TAM also indicates that it excels at explaining multiple tokens
of MLLM, which is different from the Class Activation Map (CAM) for a single
prediction. Our TAM method significantly outperforms existing SoTA methods,
showcasing high-quality visualization results that can be utilized for various
scenarios, such as object localization, failure case analysis, video
visualization, MLLMs visual comparison, and model understanding (e.g., color,
shape, action, location, visual reasoning, multi-turn conversation, etc). The
code is available atgithub.com/xmed-lab/TAM.

</details>


### [242] [Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization](https://arxiv.org/abs/2506.23714)
*Md Moinul Islam,Sofoklis Kakouros,Janne Heikkilä,Mourad Oussalah*

Main category: cs.CV

TL;DR: This paper introduces a multimodal video summarization framework combining textual, audio, and visual cues to create timestamp-aligned summaries, outperforming traditional methods in semantic relevance and clarity.


<details>
  <summary>Details</summary>
Motivation: The need for effective video summarization techniques has grown due to the increased volume of video content in educational, professional, and social domains.

Method: The framework uses prosodic features, textual cues, visual indicators, and identifies bonus words emphasized across modalities to create semantically and emotionally relevant summaries. It evaluates against pseudo-ground truth summaries created by LLM-based extractive methods.

Result: The framework shows significant improvement over traditional methods, with ROUGE-1 improving from 0.4769 to 0.7929, BERTScore from 0.9152 to 0.9536, and a 23% increase in video-based evaluation F1-Score.

Conclusion: Multimodal integration effectively enhances comprehensiveness and emotional relevance in video summarization, making it a promising approach for summarizing complex video content.

Abstract: The increasing volume of video content in educational, professional, and
social domains necessitates effective summarization techniques that go beyond
traditional unimodal approaches. This paper proposes a behaviour-aware
multimodal video summarization framework that integrates textual, audio, and
visual cues to generate timestamp-aligned summaries. By extracting prosodic
features, textual cues and visual indicators, the framework identifies
semantically and emotionally important moments. A key contribution is the
identification of bonus words, which are terms emphasized across multiple
modalities and used to improve the semantic relevance and expressive clarity of
the summaries. The approach is evaluated against pseudo-ground truth (pGT)
summaries generated using LLM-based extractive method. Experimental results
demonstrate significant improvements over traditional extractive method, such
as the Edmundson method, in both text and video-based evaluation metrics.
Text-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore
from 0.9152 to 0.9536, while in video-based evaluation, our proposed framework
improves F1-Score by almost 23%. The findings underscore the potential of
multimodal integration in producing comprehensive and behaviourally informed
video summaries.

</details>


### [243] [Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation](https://arxiv.org/abs/2506.23271)
*Jinxing Zhou,Zhihui Li,Yongqiang Yu,Yanghao Zhou,Ruohao Guo,Guangyao Li,Yuxin Mao,Mingfei Han,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: Mettle is a memory-efficient method using meta-tokens for adapting transformers to audio-visual tasks, maintaining strong performance with reduced computational requirements.


<details>
  <summary>Details</summary>
Motivation: Many large-scale pretrained transformer models face challenges in adapting to specific audio-visual tasks without incurring significant memory and computational overhead.

Method: Mettle incorporates a Layer-Centric Distillation (LCD) module to create compact meta-tokens for classification tasks and a Meta-Token Injection (MTI) module to improve segmentation tasks by guiding earlier transformer layers.

Result: The approach demonstrates reduced memory usage, faster training, and competitive accuracy across various audio-visual benchmarks.

Conclusion: Mettle provides an effective way to adapt large-scale transformers to downstream audio-visual tasks while being memory-efficient and maintaining high task performance.

Abstract: We present \textbf{Met}a-\textbf{T}oken \textbf{Le}arning (Mettle), a simple
and memory-efficient method for adapting large-scale pretrained transformer
models to downstream audio-visual tasks. Instead of sequentially modifying the
output feature distribution of the transformer backbone, Mettle utilizes a
lightweight \textit{Layer-Centric Distillation (LCD)} module to distill in
parallel the intact audio or visual features embedded by each transformer layer
into compact meta-tokens. This distillation process considers both pretrained
knowledge preservation and task-specific adaptation. The obtained meta-tokens
can be directly applied to classification tasks, such as audio-visual event
localization and audio-visual video parsing. To further support fine-grained
segmentation tasks, such as audio-visual segmentation, we introduce a
\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual
meta-tokens distilled from the top transformer layer to guide feature
adaptation in earlier layers. Extensive experiments on multiple audiovisual
benchmarks demonstrate that our method significantly reduces memory usage and
training time while maintaining parameter efficiency and competitive accuracy.

</details>


### [244] [Why Settle for One? Text-to-ImageSet Generation and Evaluation](https://arxiv.org/abs/2506.23275)
*Chengyou Jia,Xin Shen,Zhuohang Dang,Zhuohang Dang,Changliang Xia,Weijia Wu,Xinyu Zhang,Hangwei Qian,Ivor W. Tsang,Minnan Luo*

Main category: cs.CV

TL;DR: The paper introduces the Text-to-ImageSet (T2IS) problem, aiming to generate coherent image sets with diverse consistency requirements based on user instructions. It proposes benchmarks, evaluation frameworks, and the AutoT2IS framework for training-free, consistency-driven image set generation.


<details>
  <summary>Details</summary>
Motivation: Current Text-to-Image models struggle with generating coherent sets of images when diverse consistency requirements are needed, limiting their real-world application.

Method: The paper proposes AutoT2IS, a training-free framework leveraging pretrained Diffusion Transformers with in-context capabilities to ensure visual consistency at both image and set levels. They also introduce T2IS-Bench for diverse-instruction benchmarking and T2IS-Eval for adaptive consistency evaluation.

Result: Experiments on T2IS-Bench reveal that existing methods face challenges with diverse consistency requirements, but AutoT2IS significantly outperforms both generalized and specialized methods.

Conclusion: AutoT2IS addresses the limitations of existing Text-to-Image methods by generating coherent image sets with diverse consistency needs, unlocking various real-world applications and demonstrating substantial practical value.

Abstract: Despite remarkable progress in Text-to-Image models, many real-world
applications require generating coherent image sets with diverse consistency
requirements. Existing consistent methods often focus on a specific domain with
specific aspects of consistency, which significantly constrains their
generalizability to broader applications. In this paper, we propose a more
challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate
sets of images that meet various consistency requirements based on user
instructions. To systematically study this problem, we first introduce
$\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,
providing comprehensive coverage for T2IS generation. Building on this, we
propose $\textbf{T2IS-Eval}$, an evaluation framework that transforms user
instructions into multifaceted assessment criteria and employs effective
evaluators to adaptively assess consistency fulfillment between criteria and
generated sets. Subsequently, we propose $\textbf{AutoT2IS}$, a training-free
framework that maximally leverages pretrained Diffusion Transformers'
in-context capabilities to harmonize visual elements to satisfy both
image-level prompt alignment and set-level visual consistency. Extensive
experiments on T2IS-Bench reveal that diverse consistency challenges all
existing methods, while our AutoT2IS significantly outperforms current
generalized and even specialized approaches. Our method also demonstrates the
ability to enable numerous underexplored real-world applications, confirming
its substantial practical value. Visit our project in
https://chengyou-jia.github.io/T2IS-Home.

</details>


### [245] [Autoregressive Denoising Score Matching is a Good Video Anomaly Detector](https://arxiv.org/abs/2506.23282)
*Hanwen Zhang,Congqi Cao,Qinyi Lv,Lingtong Min,Yanning Zhang*

Main category: cs.CV

TL;DR: This paper proposes a novel method for video anomaly detection (VAD) by addressing gaps in scene, motion, and appearance understanding using a noise-conditioned score transformer and autoregressive denoising techniques.


<details>
  <summary>Details</summary>
Motivation: Video anomaly detection is challenging due to inaccuracies when detecting anomalies near the learned distribution modes. Previous methods fail to comprehensively address local 'unseen' anomalies in visual data.

Method: The authors build a noise-conditioned score transformer and integrate scene-dependent, motion-aware, and appearance-enhanced mechanisms. An autoregressive denoising score matching approach is used for anomaly inference.

Result: The proposed model demonstrates state-of-the-art performance on three popular VAD benchmarks, highlighting its robustness in detecting diverse anomalies.

Conclusion: Considering scene, motion, and appearance gaps enhances anomaly detection capabilities, offering a more effective and comprehensive approach for VAD.

Abstract: Video anomaly detection (VAD) is an important computer vision problem. Thanks
to the mode coverage capabilities of generative models, the likelihood-based
paradigm is catching growing interest, as it can model normal distribution and
detect out-of-distribution anomalies. However, these likelihood-based methods
are blind to the anomalies located in local modes near the learned
distribution. To handle these ``unseen" anomalies, we dive into three gaps
uniquely existing in VAD regarding scene, motion and appearance. Specifically,
we first build a noise-conditioned score transformer for denoising score
matching. Then, we introduce a scene-dependent and motion-aware score function
by embedding the scene condition of input sequences into our model and
assigning motion weights based on the difference between key frames of input
sequences. Next, to solve the problem of blindness in principle, we integrate
unaffected visual information via a novel autoregressive denoising score
matching mechanism for inference. Through autoregressively injecting
intensifying Gaussian noise into the denoised data and estimating the
corresponding score function, we compare the denoised data with the original
data to get a difference and aggregate it with the score function for an
enhanced appearance perception and accumulate the abnormal context. With all
three gaps considered, we can compute a more comprehensive anomaly indicator.
Experiments on three popular VAD benchmarks demonstrate the state-of-the-art
performance of our method.

</details>


### [246] [Ella: Embodied Social Agents with Lifelong Memory](https://arxiv.org/abs/2506.24019)
*Hongxin Zhang,Zheyuan Zhang,Zeyuan Wang,Zunzhe Zhang,Lixing Fang,Qinhong Zhou,Chuang Gan*

Main category: cs.CV

TL;DR: This paper introduces Ella, an embodied agent in a 3D world capable of lifelong learning through observations and social interactions, leveraging a structured memory system.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that combining structured lifelong memory systems with foundation models can lead to advanced embodied intelligence in open-world settings.

Method: Ella uses a structured multimodal memory system with semantic and episodic components, integrated with foundation models, for decision-making and autonomous behavior in a dynamic 3D open world.

Result: Ella effectively learns by observation and interaction, influencing and cooperating with other agents to achieve goals during controlled evaluations in the simulated environment.

Conclusion: The integration of structured memory systems and foundation models is a promising direction for creating advanced embodied social agents capable of autonomous evolution and interaction.

Abstract: We introduce Ella, an embodied social agent capable of lifelong learning
within a community in a 3D open world, where agents accumulate experiences and
acquire knowledge through everyday visual observations and social interactions.
At the core of Ella's capabilities is a structured, long-term multimodal memory
system that stores, updates, and retrieves information effectively. It consists
of a name-centric semantic memory for organizing acquired knowledge and a
spatiotemporal episodic memory for capturing multimodal experiences. By
integrating this lifelong memory system with foundation models, Ella retrieves
relevant information for decision-making, plans daily activities, builds social
relationships, and evolves autonomously while coexisting with other intelligent
beings in the open world. We conduct capability-oriented evaluations in a
dynamic 3D open world where 15 agents engage in social activities for days and
are assessed with a suite of unseen controlled evaluations. Experimental
results show that Ella can influence, lead, and cooperate with other agents
well to achieve goals, showcasing its ability to learn effectively through
observation and social interaction. Our findings highlight the transformative
potential of combining structured memory systems with foundation models for
advancing embodied intelligence. More videos can be found at
https://umass-embodied-agi.github.io/Ella/.

</details>


### [247] [MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition](https://arxiv.org/abs/2506.23283)
*Yuhuan Yang,Chaofan Ma,Zhenjie Mao,Jiangchao Yao,Ya Zhang,Yanfeng Wang*

Main category: cs.CV

TL;DR: This paper introduces MoMa, a framework to enhance video understanding by modeling spatial-temporal dynamics in image foundation models (IFMs) using efficient strategies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve video understanding by addressing the limitations of existing methods that process spatial and temporal information separately, resulting in suboptimal capture of video dynamics.

Method: The paper introduces MoMa, which incorporates a novel SeqMod operation into the Divide-and-Modulate architecture to achieve full spatial-temporal integration in pre-trained IFMs without disrupting their features.

Result: Experiments show that MoMa outperforms existing methods on multiple video benchmarks, achieving better performance while reducing computational costs.

Conclusion: MoMa provides an efficient, effective solution for video understanding by integrating spatial and temporal dynamics into image foundation models, demonstrating superior results and better efficiency.

Abstract: Video understanding is a complex challenge that requires effective modeling
of spatial-temporal dynamics. With the success of image foundation models
(IFMs) in image understanding, recent approaches have explored
parameter-efficient fine-tuning (PEFT) to adapt IFMs for video. However, most
of these methods tend to process spatial and temporal information separately,
which may fail to capture the full intricacy of video dynamics. In this paper,
we propose MoMa, an efficient adapter framework that achieves full
spatial-temporal modeling by integrating Mamba's selective state space modeling
into IFMs. We propose a novel SeqMod operation to inject spatial-temporal
information into pre-trained IFMs, without disrupting their original features.
By incorporating SeqMod into a Divide-and-Modulate architecture, MoMa enhances
video understanding while maintaining computational efficiency. Extensive
experiments on multiple video benchmarks demonstrate the effectiveness of MoMa,
achieving superior performance with reduced computational cost.

</details>


### [248] [Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification](https://arxiv.org/abs/2506.23285)
*Daqian Shi,Xiaolei Diao,Xu Chen,Cédric M. John*

Main category: cs.CV

TL;DR: This paper proposes a novel competitive distillation strategy where networks act as teachers competitively, improving learning performance.


<details>
  <summary>Details</summary>
Motivation: Traditional knowledge distillation methods lack understanding of learning directions among networks, limiting performance improvements.

Method: Introduces competitive distillation, allowing dynamic teacher-student roles among networks with stochastic perturbation to enhance learning.

Result: Experimental results demonstrate promising performance gains across various tasks and datasets.

Conclusion: Competitive distillation improves DNN training and provides a pathway for better visual representation and optimization strategies.

Abstract: Deep Neural Networks (DNNs) have significantly advanced the field of computer
vision. To improve DNN training process, knowledge distillation methods
demonstrate their effectiveness in accelerating network training by introducing
a fixed learning direction from the teacher network to student networks. In
this context, several distillation-based optimization strategies are proposed,
e.g., deep mutual learning and self-distillation, as an attempt to achieve
generic training performance enhancement through the cooperative training of
multiple networks. However, such strategies achieve limited improvements due to
the poor understanding of the impact of learning directions among networks
across different iterations. In this paper, we propose a novel competitive
distillation strategy that allows each network in a group to potentially act as
a teacher based on its performance, enhancing the overall learning performance.
Competitive distillation organizes a group of networks to perform a shared task
and engage in competition, where competitive optimization is proposed to
improve the parameter updating process. We further introduce stochastic
perturbation in competitive distillation, aiming to motivate networks to induce
mutations to achieve better visual representations and global optimum. The
experimental results show that competitive distillation achieves promising
performance in diverse tasks and datasets.

</details>


### [249] [MotionGPT3: Human Motion as a Second Modality](https://arxiv.org/abs/2506.24086)
*Bingfan Zhu,Biao Jiang,Sunyi Wang,Shixiang Tang,Tao Chen,Linjie Luo,Youyi Zheng,Xin Chen*

Main category: cs.CV

TL;DR: MotionGPT3 is a unified motion-language model that addresses challenges in motion reconstruction and language degradation using a mixture-of-experts approach, showing strong performance in both motion and language tasks.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored area of unified motion-language modeling with high-fidelity human motion, overcoming the challenges of motion reconstruction and language intelligence degradation.

Method: MotionGPT3 employs a bimodal design with separate motion and language branches. The motion branch uses a variational autoencoder to encode human motion into a continuous latent space and integrates it with a shared attention mechanism to preserve language intelligence.

Result: The model achieves competitive outcomes on both motion understanding and generation tasks, demonstrating effective cross-modal interaction and preserving strong language capabilities.

Conclusion: MotionGPT3 establishes a robust framework for unified motion-language modeling, presenting an effective solution for cross-modal tasks without compromising language prowess.

Abstract: Though recent advances in multimodal models have demonstrated strong
capabilities and opportunities in unified understanding and generation, the
development of unified motion-language models remains underexplored. To enable
such models with high-fidelity human motion, two core challenges must be
addressed. The first is the reconstruction gap between the continuous motion
modality and discrete representation in an autoregressive manner, and the
second is the degradation of language intelligence during unified training.
Inspired by the mixture of experts, we propose MotionGPT3, a bimodal
motion-language model that treats human motion as a second modality, decoupling
motion modeling via separate model parameters and enabling both effective
cross-modal interaction and efficient multimodal scaling training. To preserve
language intelligence, the text branch retains the original structure and
parameters of the pretrained language model, while a new motion branch is
integrated via a shared attention mechanism, enabling bidirectional information
flow between two modalities. We first employ a motion Variational Autoencoder
(VAE) to encode raw human motion into latent representations. Based on this
continuous latent space, the motion branch predicts motion latents directly
from intermediate hidden states using a diffusion head, bypassing discrete
tokenization. Extensive experiments show that our approach achieves competitive
performance on both motion understanding and generation tasks while preserving
strong language capabilities, establishing a unified bimodal motion diffusion
framework within an autoregressive manner.

</details>


### [250] [DDL: A Dataset for Interpretable Deepfake Detection and Localization in Real-World Scenarios](https://arxiv.org/abs/2506.23292)
*Changtao Miao,Yi Zhang,Weize Gao,Man Luo,Weiwei Feng,Zhiya Tan,Jianshu Li,Ajian Liu,Yunfeng Diao,Qi Chu,Tao Gong,Zhe Li,Weibin Yao,Joey Tianyi Zhou*

Main category: cs.CV

TL;DR: The paper introduces a novel large-scale deepfake dataset (DDL) designed to enhance detection and localization accuracy and interpretability, addressing current dataset limitations.


<details>
  <summary>Details</summary>
Motivation: The misuse of malicious deepfake content has grown due to advances in AIGC, necessitating reliable detection methods with interpretability, especially for critical domains like law.

Method: The authors developed the DDL dataset featuring over 1.8M samples created using 75 distinct deepfake methods, emphasizing diversity, comprehensive techniques, varied manipulation types, and fine-grained annotations.

Result: DDL serves as a more challenging benchmark with advanced features, aimed at improving detection mechanisms and supporting interpretability in complex real-world forgery scenarios.

Conclusion: The DDL dataset sets a new standard for next-generation deepfake detection and localization solutions, contributing significantly to the understanding and management of deepfake risks.

Abstract: Recent advances in AIGC have exacerbated the misuse of malicious deepfake
content, making the development of reliable deepfake detection methods an
essential means to address this challenge. Although existing deepfake detection
models demonstrate outstanding performance in detection metrics, most methods
only provide simple binary classification results, lacking interpretability. In
critical domains such as law, interpretability is crucial for enhancing the
credibility and authority of decisions. Recent studies attempt to improve the
interpretability of classification results by providing spatial manipulation
masks or temporal forgery segments. However, the practical effectiveness of
these methods remains suboptimal due to limitations of the forgery data. Most
current deepfake datasets predominantly offer binary labels, only a few
datasets with localization annotations. However, they suffer from restricted
forgery scenarios, limited diversity in deepfake types, and insufficient data
scale, making them inadequate for complex real-world scenarios. To address this
predicament, we construct a novel large-scale deepfake detection and
localization ($\textbf{DDL}$) dataset containing over $\textbf{1.8M}$ forged
samples and encompassing up to $\textbf{75}$ distinct deepfake methods. The DDL
design incorporates four key innovations: (1) $\textbf{Diverse Forgery
Scenarios}$, (2) $\textbf{Comprehensive Deepfake Methods}$, (3) $\textbf{Varied
Manipulation Modes}$, and (4) $\textbf{Fine-grained Forgery Annotations}$.
Through these improvements, our DDL not only provides a more challenging
benchmark for complex real-world forgeries, but also offers crucial support for
building next-generation deepfake detection, localization, and interpretability
methods. The DDL dataset project page is on
https://deepfake-workshop-ijcai2025.github.io/main/index.html.

</details>


### [251] [DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On](https://arxiv.org/abs/2506.23295)
*Xiang Xu*

Main category: cs.CV

TL;DR: DiffFit is a novel two-stage latent diffusion framework for virtual try-on, enhancing garment alignment and visual quality.


<details>
  <summary>Details</summary>
Motivation: Improving the realism and efficiency of virtual try-on systems while addressing issues like garment detail preservation, alignment accuracy, and pose diversity.

Method: DiffFit uses a two-stage approach: first, geometry-aware garment warping for alignment, followed by refinement through a cross-modal conditional diffusion model.

Result: DiffFit outperforms state-of-the-art VTON methods in quantitative and perceptual benchmarks, excelling in garment detail preservation and alignment.

Conclusion: By decoupling alignment and appearance refinement, DiffFit achieves high-fidelity, realistic virtual try-on with enhanced stability and visual quality.

Abstract: Virtual try-on (VTON) aims to synthesize realistic images of a person wearing
a target garment, with broad applications in e-commerce and digital fashion.
While recent advances in latent diffusion models have substantially improved
visual quality, existing approaches still struggle with preserving fine-grained
garment details, achieving precise garment-body alignment, maintaining
inference efficiency, and generalizing to diverse poses and clothing styles. To
address these challenges, we propose DiffFit, a novel two-stage latent
diffusion framework for high-fidelity virtual try-on. DiffFit adopts a
progressive generation strategy: the first stage performs geometry-aware
garment warping, aligning the garment with the target body through fine-grained
deformation and pose adaptation. The second stage refines texture fidelity via
a cross-modal conditional diffusion model that integrates the warped garment,
the original garment appearance, and the target person image for high-quality
rendering. By decoupling geometric alignment and appearance refinement, DiffFit
effectively reduces task complexity and enhances both generation stability and
visual realism. It excels in preserving garment-specific attributes such as
textures, wrinkles, and lighting, while ensuring accurate alignment with the
human body. Extensive experiments on large-scale VTON benchmarks demonstrate
that DiffFit achieves superior performance over existing state-of-the-art
methods in both quantitative metrics and perceptual evaluations.

</details>


### [252] [Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting](https://arxiv.org/abs/2506.23308)
*Yiming Huang,Long Bai,Beilei Cui,Yanheng Li,Tong Chen,Jie Wang,Jinlin Wu,Zhen Lei,Hongbin Liu,Hongliang Ren*

Main category: cs.CV

TL;DR: The paper introduces Endo-4DGX, an advanced method for reconstructing dynamic endoscopic scenes under challenging lighting conditions using an illumination-adaptive approach.


<details>
  <summary>Details</summary>
Motivation: To overcome issues in reconstructing soft tissue under extreme lighting conditions, such as low light and over-exposure, during image-guided robotic surgery.

Method: Endo-4DGX employs illumination embeddings, a region-aware enhancement module, and a spatial-aware adjustment module to adapt to dynamic lighting variations. It also uses exposure control loss for adverse exposure normalization.

Result: Endo-4DGX achieves superior rendering quality and geometric accuracy under challenging lighting conditions, outperforming state-of-the-art reconstruction and restoration methods in experimental tests.

Conclusion: This method enhances reconstruction performance under varying illumination, presenting potential to improve image-guided robotic surgery applications.

Abstract: Accurate reconstruction of soft tissue is crucial for advancing automation in
image-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)
techniques and their variants, 4DGS, achieve high-quality renderings of dynamic
surgical scenes in real-time. However, 3D-GS-based methods still struggle in
scenarios with varying illumination, such as low light and over-exposure.
Training 3D-GS in such extreme light conditions leads to severe optimization
problems and devastating rendering quality. To address these challenges, we
present Endo-4DGX, a novel reconstruction method with illumination-adaptive
Gaussian Splatting designed specifically for endoscopic scenes with uneven
lighting. By incorporating illumination embeddings, our method effectively
models view-dependent brightness variations. We introduce a region-aware
enhancement module to model the sub-area lightness at the Gaussian level and a
spatial-aware adjustment module to learn the view-consistent brightness
adjustment. With the illumination adaptive design, Endo-4DGX achieves superior
rendering performance under both low-light and over-exposure conditions while
maintaining geometric accuracy. Additionally, we employ an exposure control
loss to restore the appearance from adverse exposure to the normal level for
illumination-adaptive optimization. Experimental results demonstrate that
Endo-4DGX significantly outperforms combinations of state-of-the-art
reconstruction and restoration methods in challenging lighting environments,
underscoring its potential to advance robot-assisted surgical applications. Our
code is available at https://github.com/lastbasket/Endo-4DGX.

</details>


### [253] [FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method](https://arxiv.org/abs/2506.23323)
*Quang-Huy Che,Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: FastSeg introduces an efficient training-free framework using pretrained diffusion models for open-vocabulary semantic segmentation, achieving state-of-the-art segmentation with improved inference efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to open-vocabulary semantic segmentation face challenges due to loss of fine spatial precision in contrastive models and inefficiencies in diffusion-based models.

Method: FastSeg utilizes a (1+1)-step reverse process of a pretrained diffusion model with innovations like dual-prompt attention mechanism, Hierarchical Attention Refinement Method (HARD), and Test-Time Flipping (TTF) to ensure efficiency and quality.

Result: The framework achieves 43.8% average mIoU across PASCAL VOC, PASCAL Context, and COCO Object benchmarks, surpassing previous training-free methods.

Conclusion: FastSeg balances segmentation quality and inference efficiency, establishing a foundation for extendable solutions in OVSS.

Abstract: Open-vocabulary semantic segmentation (OVSS) aims to segment objects from
arbitrary text categories without requiring densely annotated datasets.
Although contrastive learning based models enable zero-shot segmentation, they
often lose fine spatial precision at pixel level, due to global representation
bias. In contrast, diffusion-based models naturally encode fine-grained spatial
features via attention mechanisms that capture both global context and local
details. However, they often face challenges in balancing the number of
iterations with the quality of the segmentation. In this work, we propose
FastSeg, a novel and efficient training-free framework with only (1+1)-step of
reverse process of a pretrained diffusion model (e.g., Stable Diffusion).
Moreover, instead of running multiple times for different classes, FastSeg
performs segmentation for all classes at once. To further enhance the
segmentation quality, FastSeg introduces three key components: (i) a
dual-prompt mechanism for discriminative, class-aware attention extraction,
(ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused
cross-attention using scale-aligned selfattention maps, and (iii) a Test-Time
Flipping (TTF) scheme designed to improve spatial consistency. Extensive
experiments show that FastSeg achieves state-of-the-art training-free
performance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context,
and COCO Object benchmarks while maintaining superior inference efficiency. Our
results demonstrate that FastSeg provides a strong foundation for
extendability, bridging the gap between segmentation quality and inference
efficiency.

</details>


### [254] [IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering](https://arxiv.org/abs/2506.23329)
*Parker Liu,Chenxin Li,Zhengxin Li,Yipeng Wu,Wuyang Li,Zhiqin Yang,Zhenyuan Zhang,Yunlong Lin,Sirui Han,Brandon Y. Feng*

Main category: cs.CV

TL;DR: The paper introduces IR3D-Bench, a benchmark for vision-language models to demonstrate scene understanding through active creation using tools like programming and rendering.


<details>
  <summary>Details</summary>
Motivation: To challenge vision-language models in showcasing true scene understanding beyond descriptive tasks by engaging them in active creation processes.

Method: Developed IR3D-Bench based on analysis-by-synthesis, where models reconstruct 3D structures of images using programming and rendering tools, moving towards agentic inverse rendering.

Result: Initial experiments reveal that while vision-language models show basic tool-usage ability, they lack precision in visual aspects during 3D reconstruction.

Conclusion: IR3D-Bench provides a novel framework and metrics to advance the study of tool-using generative capabilities in vision-language models for comprehensive scene understanding.

Abstract: Vision-language models (VLMs) excel at descriptive tasks, but whether they
truly understand scenes from visual observations remains uncertain. We
introduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding
through active creation rather than passive recognition. Grounded in the
analysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs)
with actively using programming and rendering tools to recreate the underlying
3D structure of an input image, achieving agentic inverse rendering through
tool use. This "understanding-by-creating" approach probes the tool-using
generative capacity of VLAs, moving beyond the descriptive or conversational
capacity measured by traditional scene understanding benchmarks. We provide a
comprehensive suite of metrics to evaluate geometric accuracy, spatial
relations, appearance attributes, and overall plausibility. Initial experiments
on agentic inverse rendering powered by various state-of-the-art VLMs highlight
current limitations, particularly in visual precision rather than basic tool
usage. IR3D-Bench, including data and evaluation protocols, is released to
facilitate systematic study and development of tool-using VLAs towards genuine
scene understanding by creating.

</details>


### [255] [CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation](https://arxiv.org/abs/2506.23347)
*Yi Liu,Shengqian Li,Zuzeng Lin,Feng Wang,Si Liu*

Main category: cs.CV

TL;DR: This paper introduces CycleVAR, an innovative method for unsupervised image translation using a novel Softmax Relaxed Quantization technique, demonstrating superior performance and faster speeds compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the limitations of discrete quantization in traditional Vector Quantization-based frameworks that hinder gradient flow and end-to-end optimization in unsupervised image translation.

Method: The authors propose Softmax Relaxed Quantization for preserving gradient propagation and CycleVAR—a model that uses multi-scale tokens for autoregressive image generation via serial multi-step and parallel one-step modes.

Result: The parallel one-step generation mode achieved higher translation quality and faster inference speeds. Experimental results showed CycleVAR outperformed state-of-the-art models like CycleGAN-Turbo in both quantitative and qualitative measures.

Conclusion: CycleVAR and Softmax Relaxed Quantization advance the state of the art for unsupervised image translation, offering better performance and efficiency.

Abstract: The current conditional autoregressive image generation methods have shown
promising results, yet their potential remains largely unexplored in the
practical unsupervised image translation domain, which operates without
explicit cross-domain correspondences. A critical limitation stems from the
discrete quantization inherent in traditional Vector Quantization-based
frameworks, which disrupts gradient flow between the Variational Autoencoder
decoder and causal Transformer, impeding end-to-end optimization during
adversarial training in image space. To tackle this issue, we propose using
Softmax Relaxed Quantization, a novel approach that reformulates codebook
selection as a continuous probability mixing process via Softmax, thereby
preserving gradient propagation. Building upon this differentiable foundation,
we introduce CycleVAR, which reformulates image-to-image translation as
image-conditional visual autoregressive generation by injecting multi-scale
source image tokens as contextual prompts, analogous to prefix-based
conditioning in language models. CycleVAR exploits two modes to generate the
target image tokens, including (1) serial multi-step generation, enabling
iterative refinement across scales, and (2) parallel one-step generation
synthesizing all resolution outputs in a single forward pass. Experimental
findings indicate that the parallel one-step generation mode attains superior
translation quality with quicker inference speed than the serial multi-step
mode in unsupervised scenarios. Furthermore, both quantitative and qualitative
results indicate that CycleVAR surpasses previous state-of-the-art unsupervised
image translation models, \textit{e}.\textit{g}., CycleGAN-Turbo.

</details>


### [256] [GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields](https://arxiv.org/abs/2506.23352)
*Shunsuke Yasuki,Taiki Miyanishi,Nakamasa Inoue,Shuhei Kurita,Koya Sakamoto,Daichi Azuma,Masato Taki,Yutaka Matsuo*

Main category: cs.CV

TL;DR: This paper introduces GeoProg3D, a novel framework enabling natural language-driven interactions in large-scale 3D urban environments, overcoming compositional and scalability limitations.


<details>
  <summary>Details</summary>
Motivation: Existing 3D language models are limited to small-scale environments and lack the ability to handle complex urban settings, necessitating scalable methods for intuitive geographic reasoning.

Method: GeoProg3D combines a Geography-aware City-scale 3D Language Field (GCLF) for efficient data handling and Geographical Vision APIs (GV-APIs) for geographic vision tasks, leveraging large language models for reasoning.

Result: GeoProg3D outperformed existing models in a new benchmark dataset, GeoEval3D, across grounding, spatial reasoning, comparison, counting, and measurement tasks.

Conclusion: GeoProg3D establishes itself as a pioneering framework enabling scalable and compositional geographic reasoning in city-scale 3D environments using natural language interfaces.

Abstract: The advancement of 3D language fields has enabled intuitive interactions with
3D scenes via natural language. However, existing approaches are typically
limited to small-scale environments, lacking the scalability and compositional
reasoning capabilities necessary for large, complex urban settings. To overcome
these limitations, we propose GeoProg3D, a visual programming framework that
enables natural language-driven interactions with city-scale high-fidelity 3D
scenes. GeoProg3D consists of two key components: (i) a Geography-aware
City-scale 3D Language Field (GCLF) that leverages a memory-efficient
hierarchical 3D model to handle large-scale data, integrated with geographic
information for efficiently filtering vast urban spaces using directional cues,
distance measurements, elevation data, and landmark references; and (ii)
Geographical Vision APIs (GV-APIs), specialized geographic vision tools such as
area segmentation and object detection. Our framework employs large language
models (LLMs) as reasoning engines to dynamically combine GV-APIs and operate
GCLF, effectively supporting diverse geographic vision tasks. To assess
performance in city-scale reasoning, we introduce GeoEval3D, a comprehensive
benchmark dataset containing 952 query-answer pairs across five challenging
tasks: grounding, spatial reasoning, comparison, counting, and measurement.
Experiments demonstrate that GeoProg3D significantly outperforms existing 3D
language fields and vision-language models across multiple tasks. To our
knowledge, GeoProg3D is the first framework enabling compositional geographic
reasoning in high-fidelity city-scale 3D environments via natural language. The
code is available at https://snskysk.github.io/GeoProg3D/.

</details>


### [257] [Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement](https://arxiv.org/abs/2506.23353)
*Siyuan Chai,Xiaodong Guo,Tong Liu*

Main category: cs.CV

TL;DR: This paper introduces a task-oriented method to improve infrared image quality, focusing on enhancing contrast for critical targets without boosting noise.


<details>
  <summary>Details</summary>
Motivation: Infrared images face difficulties in low-visibility weather, but challenges such as low contrast and noise amplification hinder their usefulness for vision tasks.

Method: The proposed method involves two components: 1) a layer decomposition method that enhances scene details while preserving dark region features, and 2) a morphological reconstruction-based saliency extraction method that enhances target information without increasing noise.

Result: The proposed approach improves the quality of infrared images for tasks like object detection and semantic segmentation, outperforming state-of-the-art methods in experiments.

Conclusion: This task-oriented enhancement method effectively tackles infrared image challenges, achieving a balance between visual detail and noise suppression for better performance in vision tasks.

Abstract: Infrared image helps improve the perception capabilities of autonomous
driving in complex weather conditions such as fog, rain, and low light.
However, infrared image often suffers from low contrast, especially in
non-heat-emitting targets like bicycles, which significantly affects the
performance of downstream high-level vision tasks. Furthermore, achieving
contrast enhancement without amplifying noise and losing important information
remains a challenge. To address these challenges, we propose a task-oriented
infrared image enhancement method. Our approach consists of two key components:
layer decomposition and saliency information extraction. First, we design an
layer decomposition method for infrared images, which enhances scene details
while preserving dark region features, providing more features for subsequent
saliency information extraction. Then, we propose a morphological
reconstruction-based saliency extraction method that effectively extracts and
enhances target information without amplifying noise. Our method improves the
image quality for object detection and semantic segmentation tasks. Extensive
experiments demonstrate that our approach outperforms state-of-the-art methods.

</details>


### [258] [OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions](https://arxiv.org/abs/2506.23361)
*Yuanhao Cai,He Zhang,Xi Chen,Jinbo Xing,Yiwei Hu,Yuqian Zhou,Kai Zhang,Zhifei Zhang,Soo Ye Kim,Tianyu Wang,Yulun Zhang,Xiaokang Yang,Zhe Lin,Alan Yuille*

Main category: cs.CV

TL;DR: The paper introduces a novel approach for subject-driven video customization in multi-subject scenarios using a data pipeline and diffusion-based framework.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current methods handling multi-subject video generation and customization, and explore control signals like depth, masks, and text prompts for editing.

Method: 1. Data construction pipeline (VideoCus-Factory) for generating training data pairs from raw videos. 
2. Mixed training strategy (IVTM).
3. Framework (OmniVCus) with innovative embeddings (Lottery Embedding and Temporally Aligned Embedding).

Result: The developed model demonstrates superior performance over existing state-of-the-art methods in both quantitative and qualitative metrics.

Conclusion: OmniVCus effectively enables multi-subject visual customization in videos while utilizing various guiding signals, thus advancing video editing technology.

Abstract: Existing feedforward subject-driven video customization methods mainly study
single-subject scenarios due to the difficulty of constructing multi-subject
training data pairs. Another challenging problem that how to use the signals
such as depth, mask, camera, and text prompts to control and edit the subject
in the customized video is still less explored. In this paper, we first propose
a data construction pipeline, VideoCus-Factory, to produce training data pairs
for multi-subject customization from raw videos without labels and control
signals such as depth-to-video and mask-to-video pairs. Based on our
constructed data, we develop an Image-Video Transfer Mixed (IVTM) training with
image editing data to enable instructive editing for the subject in the
customized video. Then we propose a diffusion Transformer framework, OmniVCus,
with two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned
Embedding (TAE). LE enables inference with more subjects by using the training
subjects to activate more frame embeddings. TAE encourages the generation
process to extract guidance from temporally aligned control signals by
assigning the same frame embeddings to the control and noise tokens.
Experiments demonstrate that our method significantly surpasses
state-of-the-art methods in both quantitative and qualitative evaluations.
Video demos are at our project page:
https://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released
at https://github.com/caiyuanhao1998/Open-OmniVCus

</details>


### [259] [SIEDD: Shared-Implicit Encoder with Discrete Decoders](https://arxiv.org/abs/2506.23382)
*Vikram Rangarajan,Shishira Maiya,Max Ehrlich,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: The paper introduces SIEDD, a novel architecture for neural video compression, offering a 20-30X encoding speed-up without compromising quality or flexibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the impractically slow encoding times of Implicit Neural Representations (INRs) for video compression while preserving reconstruction quality and control for adaptive streaming and transcoding.

Method: SIEDD uses a shared encoder trained on sparse anchor frames to capture low-frequency video features, followed by parallel training of lightweight decoders expedited through aggressive coordinate-space sampling.

Result: SIEDD achieves 20-30X faster encoding speeds compared to state-of-the-art methods, while maintaining competitive reconstruction quality and compression ratios.

Conclusion: SIEDD enhances the practicality of neural video compression by providing a fast, high-fidelity, and flexible framework ideal for real-world deployment without requiring costly transcoding.

Abstract: Implicit Neural Representations (INRs) offer exceptional fidelity for video
compression by learning per-video optimized functions, but their adoption is
crippled by impractically slow encoding times. Existing attempts to accelerate
INR encoding often sacrifice reconstruction quality or crucial coordinate-level
control essential for adaptive streaming and transcoding. We introduce SIEDD
(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that
fundamentally accelerates INR encoding without these compromises. SIEDD first
rapidly trains a shared, coordinate-based encoder on sparse anchor frames to
efficiently capture global, low-frequency video features. This encoder is then
frozen, enabling massively parallel training of lightweight, discrete decoders
for individual frame groups, further expedited by aggressive coordinate-space
sampling. This synergistic design delivers a remarkable 20-30X encoding
speed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while
maintaining competitive reconstruction quality and compression ratios.
Critically, SIEDD retains full coordinate-based control, enabling continuous
resolution decoding and eliminating costly transcoding. Our approach
significantly advances the practicality of high-fidelity neural video
compression, demonstrating a scalable and efficient path towards real-world
deployment. Our codebase is available at
https://github.com/VikramRangarajan/SIEDD .

</details>


### [260] [A High-Throughput Platform to Bench Test Smartphone-Based Heart Rate Measurements Derived From Video](https://arxiv.org/abs/2506.23414)
*Ming-Zher Poh,Jonathan Wang,Jonathan Hsu,Lawrence Cai,Eric Teasley,James A. Taylor,Jameson K. Rogers,Anupam Pathak,Shwetak Patel*

Main category: cs.CV

TL;DR: The paper introduces a bench-testing platform for smartphone-based heart rate (HR) monitoring apps using synthetic PPG signals to ensure device compatibility and performance evaluation across various phone models.


<details>
  <summary>Details</summary>
Motivation: Current smartphone HR monitoring apps face challenges in reliable performance evaluation due to device variability and the lack of standardized testing methods.

Method: The study developed a high-throughput testing system with a rig for 12 smartphones, synthetic PPG video generation with adjustable HR and quality, and a host machine for playback and data analysis. The system's accuracy was validated against a clinical study.

Result: The system achieved a MAPE of 0.11% in HR measurements and showed strong signal correlation (0.92). Testing on 20 smartphone models confirmed their compliance with HR accuracy standards based on a clinical study with 80 participants.

Conclusion: The platform provides a scalable and reliable solution for pre-deployment testing of HR monitoring apps, enhancing both app performance and device compatibility in mobile health applications.

Abstract: Smartphone-based heart rate (HR) monitoring apps using finger-over-camera
photoplethysmography (PPG) face significant challenges in performance
evaluation and device compatibility due to device variability and
fragmentation. Manual testing is impractical, and standardized methods are
lacking. This paper presents a novel, high-throughput bench-testing platform to
address this critical need. We designed a system comprising a test rig capable
of holding 12 smartphones for parallel testing, a method for generating
synthetic PPG test videos with controllable HR and signal quality, and a host
machine for coordinating video playback and data logging. The system achieved a
mean absolute percentage error (MAPE) of 0.11% +/- 0.001% between input and
measured HR, and a correlation coefficient of 0.92 +/- 0.008 between input and
measured PPG signals using a clinically-validated smartphone-based HR app.
Bench-testing results of 20 different smartphone models correctly classified
all the devices as meeting the ANSI/CTA accuracy standards for HR monitors
(MAPE <10%) when compared to a prospective clinical study with 80 participants,
demonstrating high positive predictive value. This platform offers a scalable
solution for pre-deployment testing of smartphone HR apps to improve app
performance, ensure device compatibility, and advance the field of mobile
health.

</details>


### [261] [Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models](https://arxiv.org/abs/2506.23418)
*Parham Rezaei,Arash Marioriyad,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: The paper focuses on improving compositional generation in text-to-image (T2I) models, particularly addressing spatial misalignment issues. It proposes a novel evaluation metric (PSE) and an inference-time method (PSG) to enhance 2D/3D spatial alignment.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve challenges in compositional text-to-image generation, such as misalignment of spatial relationships between objects and resulting inaccuracies in image representation compared to input prompts.

Method: The paper introduces a probabilistic framework leveraging Probability of Superiority (PoS) with two contributions: (1) a novel evaluation metric (PSE) to assess spatial alignment, and (2) PoS-based Generation (PSG), which improves spatial alignment through gradient-based guidance and search strategies for noise vectors during inference.

Result: Experiments show that the PSE metric aligns strongly with human judgment, outperforming traditional metrics. The PSG method enhances spatial configuration accuracy in generated images and surpasses state-of-the-art approaches.

Conclusion: PSE provides nuanced spatial relationship evaluation aligned with human perspectives, and PSG improves spatial accuracy in T2I models. Together, these contribute toward more reliable and high-quality compositional image generation.

Abstract: Despite the ability of text-to-image models to generate high-quality,
realistic, and diverse images, they face challenges in compositional
generation, often struggling to accurately represent details specified in the
input prompt. A prevalent issue in compositional generation is the misalignment
of spatial relationships, as models often fail to faithfully generate images
that reflect the spatial configurations specified between objects in the input
prompts. To address this challenge, we propose a novel probabilistic framework
for modeling the relative spatial positioning of objects in a scene, leveraging
the concept of Probability of Superiority (PoS). Building on this insight, we
make two key contributions. First, we introduce a novel evaluation metric,
PoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D
spatial relationships between text and image, with improved adherence to human
judgment. Second, we propose PoS-based Generation (PSG), an inference-time
method that improves the alignment of 2D and 3D spatial relationships in T2I
models without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based
reward function that can be utilized in two distinct ways: (1) as a
gradient-based guidance mechanism applied to the cross-attention maps during
the denoising steps, or (2) as a search-based strategy that evaluates a set of
initial noise vectors to select the best one. Extensive experiments demonstrate
that the PSE metric exhibits stronger alignment with human judgment compared to
traditional center-based metrics, providing a more nuanced and reliable measure
of complex spatial relationship accuracy in text-image alignment. Furthermore,
PSG significantly enhances the ability of text-to-image models to generate
images with specified spatial configurations, outperforming state-of-the-art
methods across multiple evaluation metrics and benchmarks.

</details>


### [262] [Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles](https://arxiv.org/abs/2506.23426)
*Menna Taha,Aya Ahmed,Mohammed Karmoose,Yasser Gadallah*

Main category: cs.CV

TL;DR: The paper introduces a novel approach for object detection in autonomous vehicles (AVs), focusing on assessing object harmfulness rather than classifying objects into known categories.


<details>
  <summary>Details</summary>
Motivation: Address safety concerns in AVs stemming from their inability to detect or appropriately respond to Out-of-Distribution (OOD) objects, which could lead to accidents.

Method: Proposal of an object detection model that classifies objects as 'harmful' or 'harmless' based on their position and trajectory relative to the AV, instead of traditional class-based classification.

Result: The model successfully detects OOD objects and evaluates their harmfulness, improving AV decision-making effectiveness in real-time scenarios.

Conclusion: The proposed approach enhances safety and adaptability of AVs by prioritizing harmfulness assessment for better navigation in dynamic environments.

Abstract: Autonomous vehicles (AVs) use object detection models to recognize their
surroundings and make driving decisions accordingly. Conventional object
detection approaches classify objects into known classes, which limits the AV's
ability to detect and appropriately respond to Out-of-Distribution (OOD)
objects. This problem is a significant safety concern since the AV may fail to
detect objects or misclassify them, which can potentially lead to hazardous
situations such as accidents. Consequently, we propose a novel object detection
approach that shifts the emphasis from conventional class-based classification
to object harmfulness determination. Instead of object detection by their
specific class, our method identifies them as either 'harmful' or 'harmless'
based on whether they pose a danger to the AV. This is done based on the object
position relative to the AV and its trajectory. With this metric, our model can
effectively detect previously unseen objects to enable the AV to make safer
real-time decisions. Our results demonstrate that the proposed model
effectively detects OOD objects, evaluates their harmfulness, and classifies
them accordingly, thus enhancing the AV decision-making effectiveness in
dynamic environments.

</details>


### [263] [PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions](https://arxiv.org/abs/2506.23440)
*Mahesh Bhosale,Abdul Wasi,Yuanhao Zhai,Yunjie Tian,Samuel Border,Nan Xi,Pinaki Sarder,Junsong Yuan,David Doermann,Xuan Gong*

Main category: cs.CV

TL;DR: PathDiff, a generative diffusion framework, enables precise synthesis of histopathology images by leveraging unpaired mask-text data, overcoming constraints in existing datasets.


<details>
  <summary>Details</summary>
Motivation: The scarcity of paired text-mask data for histopathological images limits the ability to enhance control over image semantics and spatial details, necessitating a unified approach.

Method: PathDiff integrates unpaired mask-text data into a shared conditioning space within a diffusion framework to enable precise control of image features and improved generation.

Result: PathDiff provides high-quality, semantically accurate images with enhanced fidelity, text-image alignment, and faithfulness, advancing tasks like nuclei segmentation and classification.

Conclusion: PathDiff demonstrates superior performance over existing methods, showcasing its value as a solution to data limitations in histopathology imaging.

Abstract: Diffusion-based generative models have shown promise in synthesizing
histopathology images to address data scarcity caused by privacy constraints.
Diagnostic text reports provide high-level semantic descriptions, and masks
offer fine-grained spatial structures essential for representing distinct
morphological regions. However, public datasets lack paired text and mask data
for the same histopathological images, limiting their joint use in image
generation. This constraint restricts the ability to fully exploit the benefits
of combining both modalities for enhanced control over semantics and spatial
details. To overcome this, we propose PathDiff, a diffusion framework that
effectively learns from unpaired mask-text data by integrating both modalities
into a unified conditioning space. PathDiff allows precise control over
structural and contextual features, generating high-quality, semantically
accurate images. PathDiff also improves image fidelity, text-image alignment,
and faithfulness, enhancing data augmentation for downstream tasks like nuclei
segmentation and classification. Extensive experiments demonstrate its
superiority over existing methods.

</details>


### [264] [Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation](https://arxiv.org/abs/2506.23460)
*Dewen Zeng,Xinrong Hu,Yu-Jen Chen,Yawen Wu,Xiaowei Xu,Yiyu Shi*

Main category: cs.CV

TL;DR: The paper introduces a novel approach using contrastive learning with diffusion features to overcome limitations in weakly supervised semantic segmentation methods that rely on class activation maps (CAMs).


<details>
  <summary>Details</summary>
Motivation: Class activation maps (CAMs) in traditional WSSS methods struggle with partial activations and inaccurate object boundaries due to the mismatch in optimization goals between classification and segmentation tasks.

Method: The proposed method, Contrastive Learning with Diffusion Features (CLDF), trains a pixel decoder via contrastive learning to map diffusion features from a frozen conditional diffusion model (CDM). It integrates gradient maps from the CDM’s external classifier with CAMs to better identify foreground and background pixels for robust embedding learning.

Result: Experimental results on four segmentation tasks from two public medical datasets show that CLDF significantly outperforms existing baselines.

Conclusion: The study demonstrates the effectiveness of contrastive learning paired with diffusion features for improving object localization and segmentation in weakly supervised semantic segmentation tasks.

Abstract: Weakly supervised semantic segmentation (WSSS) methods using class labels
often rely on class activation maps (CAMs) to localize objects. However,
traditional CAM-based methods struggle with partial activations and imprecise
object boundaries due to optimization discrepancies between classification and
segmentation. Recently, the conditional diffusion model (CDM) has been used as
an alternative for generating segmentation masks in WSSS, leveraging its strong
image generation capabilities tailored to specific class distributions. By
modifying or perturbing the condition during diffusion sampling, the related
objects can be highlighted in the generated images. Yet, the saliency maps
generated by CDMs are prone to noise from background alterations during reverse
diffusion. To alleviate the problem, we introduce Contrastive Learning with
Diffusion Features (CLDF), a novel method that uses contrastive learning to
train a pixel decoder to map the diffusion features from a frozen CDM to a
low-dimensional embedding space for segmentation. Specifically, we integrate
gradient maps generated from CDM external classifier with CAMs to identify
foreground and background pixels with fewer false positives/negatives for
contrastive learning, enabling robust pixel embedding learning. Experimental
results on four segmentation tasks from two public medical datasets demonstrate
that our method significantly outperforms existing baselines.

</details>


### [265] [Time-variant Image Inpainting via Interactive Distribution Transition Estimation](https://arxiv.org/abs/2506.23461)
*Yun Xing,Qing Guo,Xiaoguang Li,Yihao Huang,Xiaofeng Cao,Di Lin,Ivor Tsang,Lei Ma*

Main category: cs.CV

TL;DR: This research proposes a new task called Time-vAriant iMage inPainting (TAMP), a method to restore damaged images using significantly time-variant reference images. The method, InDiTE-Diff, achieves superior results compared to SOTA image inpainting approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of restoring damaged target images by leveraging reference images of the same scene taken at different times, which presents significant challenges due to content distinctions and potential damages in the reference image.

Method: Proposed the Interactive Distribution Transition Estimation (InDiTE) module for adaptive semantic complementation. Additionally, introduced the InDiTE-driven Diffusion (InDiTE-Diff) framework using SOTA diffusion models and latent cross-reference during sampling.

Result: Experiments conducted on a newly developed TAMP-Street dataset show that the proposed method significantly outperforms SOTA reference-guided image inpainting methods in both settings of the TAMP task.

Conclusion: The proposed InDiTE-Diff framework provides an effective solution for the complex ill-posed problem of time-variant image inpainting, outperforming previous SOTA methods and creating a new benchmark for future research in this area.

Abstract: In this work, we focus on a novel and practical task, i.e., Time-vAriant
iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image
by leveraging the complementary information from a reference image, where both
images captured the same scene but with a significant time gap in between,
i.e., time-variant images. Different from conventional reference-guided image
inpainting, the reference image under TAMP setup presents significant content
distinction to the target image and potentially also suffers from damages. Such
an application frequently happens in our daily lives to restore a damaged image
by referring to another reference image, where there is no guarantee of the
reference image's source and quality. In particular, our study finds that even
state-of-the-art (SOTA) reference-guided image inpainting methods fail to
achieve plausible results due to the chaotic image complementation. To address
such an ill-posed problem, we propose a novel Interactive Distribution
Transition Estimation (InDiTE) module which interactively complements the
time-variant images with adaptive semantics thus facilitate the restoration of
damaged regions. To further boost the performance, we propose our TAMP
solution, namely Interactive Distribution Transition Estimation-driven
Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and
conducts latent cross-reference during sampling. Moreover, considering the lack
of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,
based on existing image and mask datasets. We conduct experiments on the
TAMP-Street datasets under two different time-variant image inpainting
settings, which show our method consistently outperform SOTA reference-guided
image inpainting methods for solving TAMP.

</details>


### [266] [Sanitizing Manufacturing Dataset Labels Using Vision-Language Models](https://arxiv.org/abs/2506.23465)
*Nazanin Mahjourian,Vinh Nguyen*

Main category: cs.CV

TL;DR: The paper introduces the Vision-Language Sanitization and Refinement (VLSR) framework to improve the quality of noisy multi-label manufacturing image datasets using CLIP embeddings for label sanitization and clustering, reducing label inconsistencies and improving dataset quality.


<details>
  <summary>Details</summary>
Motivation: Issues such as label noise, inconsistencies, and errors in large-scale datasets—particularly in manufacturing domains—hinder the success of machine learning models. High-quality labels are cost-prohibitive in such settings, necessitating automated solutions.

Method: The framework employs the CLIP model to embed images and their corresponding textual labels into a shared semantic space. It uses cosine similarity to sanitize labels by identifying irrelevant or semantically weak ones, and density-based clustering to group similar labels into unified categories for refinement.

Result: Experimental results on the Factorynet dataset show that VLSR effectively identifies problematic labels, improves label consistency, and reduces vocabulary size through clustering, enhancing dataset quality.

Conclusion: VLSR provides a practical and efficient solution for improving noisy datasets with minimal human intervention, enabling the training of robust machine learning models in industrial applications.

Abstract: The success of machine learning models in industrial applications is heavily
dependent on the quality of the datasets used to train the models. However,
large-scale datasets, specially those constructed from crowd-sourcing and
web-scraping, often suffer from label noise, inconsistencies, and errors. This
problem is particularly pronounced in manufacturing domains, where obtaining
high-quality labels is costly and time-consuming. This paper introduces
Vision-Language Sanitization and Refinement (VLSR), which is a
vision-language-based framework for label sanitization and refinement in
multi-label manufacturing image datasets. This method embeds both images and
their associated textual labels into a shared semantic space leveraging the
CLIP vision-language model. Then two key tasks are addressed in this process by
computing the cosine similarity between embeddings. First, label sanitization
is performed to identify irrelevant, misspelled, or semantically weak labels,
and surface the most semantically aligned label for each image by comparing
image-label pairs using cosine similarity between image and label embeddings.
Second, the method applies density-based clustering on text embeddings,
followed by iterative cluster merging, to group semantically similar labels
into unified label groups. The Factorynet dataset, which includes noisy labels
from both human annotations and web-scraped sources, is employed to evaluate
the effectiveness of the proposed framework. Experimental results demonstrate
that the VLSR framework successfully identifies problematic labels and improves
label consistency. This method enables a significant reduction in label
vocabulary through clustering, which ultimately enhances the dataset's quality
for training robust machine learning models in industrial applications with
minimal human intervention.

</details>


### [267] [AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays](https://arxiv.org/abs/2506.23467)
*Chenlang Yi,Zizhan Xiong,Qi Qi,Xiyuan Wei,Girish Bathla,Ching-Long Lin,Bobak Jack Mortazavi,Tianbao Yang*

Main category: cs.CV

TL;DR: The paper presents AdFair-CLIP, a framework to improve fairness and accuracy in CLIP-based medical diagnostic models, particularly for chest X-ray analysis.


<details>
  <summary>Details</summary>
Motivation: Address concerns of demographic biases and fairness in CLIP models for medical image classification, especially related to race and gender disparities.

Method: Introduced AdFair-CLIP, which uses adversarial feature intervention to suppress sensitive attributes and mitigate spurious correlations.

Result: Comprehensive experiments demonstrated improved fairness, diagnostic accuracy, and robust generalization in zero-shot and few-shot scenarios.

Conclusion: AdFair-CLIP sets new benchmarks for fairness-aware learning in medical diagnostic models using CLIP, particularly enhancing performance in chest X-ray tasks.

Abstract: Contrastive Language-Image Pre-training (CLIP) models have demonstrated
superior performance across various visual tasks including medical image
classification. However, fairness concerns, including demographic biases, have
received limited attention for CLIP models. This oversight leads to critical
issues, particularly those related to race and gender, resulting in disparities
in diagnostic outcomes and reduced reliability for underrepresented groups. To
address these challenges, we introduce AdFair-CLIP, a novel framework employing
adversarial feature intervention to suppress sensitive attributes, thereby
mitigating spurious correlations and improving prediction fairness. We conduct
comprehensive experiments on chest X-ray (CXR) datasets, and show that
AdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while
maintaining robust generalization in zero-shot and few-shot scenarios. These
results establish new benchmarks for fairness-aware learning in CLIP-based
medical diagnostic models, particularly for CXR analysis.

</details>


### [268] [NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments](https://arxiv.org/abs/2506.23468)
*Xuan Yao,Junyu Gao,Changsheng Xu*

Main category: cs.CV

TL;DR: NavMorph, a self-evolving world model, is proposed to improve performance in vision-and-language navigation tasks by incorporating dynamic environmental understanding and adaptive strategies.


<details>
  <summary>Details</summary>
Motivation: Current VLN-CE approaches struggle with generalizing to new environments and adapting to environmental changes during navigation, necessitating an advanced framework to enhance dynamic decision-making.

Method: NavMorph uses compact latent representations for modeling environmental dynamics and introduces a Contextual Evolution Memory to leverage scene-contextual information, improving agents' navigation adaptability.

Result: Extensive experiments confirm that NavMorph achieves notable performance improvements on widely-used VLN-CE benchmarks.

Conclusion: NavMorph successfully integrates human-like cognitive strategies to enhance decision-making and adaptability in navigation tasks, setting a new benchmark for VLN-CE research.

Abstract: Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires
agents to execute sequential navigation actions in complex environments guided
by natural language instructions. Current approaches often struggle with
generalizing to novel environments and adapting to ongoing changes during
navigation. Inspired by human cognition, we present NavMorph, a self-evolving
world model framework that enhances environmental understanding and
decision-making in VLN-CE tasks. NavMorph employs compact latent
representations to model environmental dynamics, equipping agents with
foresight for adaptive planning and policy refinement. By integrating a novel
Contextual Evolution Memory, NavMorph leverages scene-contextual information to
support effective navigation while maintaining online adaptability. Extensive
experiments demonstrate that our method achieves notable performance
improvements on popular VLN-CE benchmarks. Code is available at
\href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.

</details>


### [269] [Interactive Interface For Semantic Segmentation Dataset Synthesis](https://arxiv.org/abs/2506.23470)
*Ngoc-Do Tran,Minh-Tuan Huynh,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: SynthLab is a modular platform that synthesizes visual data for semantic segmentation using an accessible drag-and-drop interface.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of creating high-quality annotated datasets for semantic segmentation, which are resource-intensive and often cause privacy issues.

Method: The authors designed SynthLab, a modular platform with centralized updates and an interactive, user-friendly drag-and-drop interface for customizing data pipelines.

Result: Extensive user studies showed that SynthLab provides flexible data synthesis and high accessibility for users across diverse age, profession, and expertise levels.

Conclusion: SynthLab demonstrates its potential to empower users without deep technical expertise to effectively use AI in real-world applications, mitigating traditional barriers in dataset creation.

Abstract: The rapid advancement of AI and computer vision has significantly increased
the demand for high-quality annotated datasets, particularly for semantic
segmentation. However, creating such datasets is resource-intensive, requiring
substantial time, labor, and financial investment, and often raises privacy
concerns due to the use of real-world data. To mitigate these challenges, we
present SynthLab, consisting of a modular platform for visual data synthesis
and a user-friendly interface. The modular architecture of SynthLab enables
easy maintenance, scalability with centralized updates, and seamless
integration of new features. Each module handles distinct aspects of computer
vision tasks, enhancing flexibility and adaptability. Meanwhile, its
interactive, user-friendly interface allows users to quickly customize their
data pipelines through drag-and-drop actions. Extensive user studies involving
a diverse range of users across different ages, professions, and expertise
levels, have demonstrated flexible usage, and high accessibility of SynthLab,
enabling users without deep technical expertise to harness AI for real-world
applications.

</details>


### [270] [GeoCD: A Differential Local Approximation for Geodesic Chamfer Distance](https://arxiv.org/abs/2506.23478)
*Pedro Alonso,Tianrui Li,Chongshou Li*

Main category: cs.CV

TL;DR: GeoCD replaces the traditional Chamfer Distance with a geodesic-aware approach to enhance 3D point cloud learning.


<details>
  <summary>Details</summary>
Motivation: Chamfer Distance, while efficient, depends only on Euclidean distances, which inadequately capture the geometry of 3D shapes.

Method: Introduces GeoCD, a metric based on geodesic distances that is topology-aware and fully differentiable.

Result: GeoCD achieves better reconstruction quality than Chamfer Distance across architectures and datasets, with notable improvements within a single fine-tuning epoch.

Conclusion: GeoCD addresses the limitations of CD and provides a better metric for 3D point cloud learning, improving reconstruction and evaluation outcomes.

Abstract: Chamfer Distance (CD) is a widely adopted metric in 3D point cloud learning
due to its simplicity and efficiency. However, it suffers from a fundamental
limitation: it relies solely on Euclidean distances, which often fail to
capture the intrinsic geometry of 3D shapes. To address this limitation, we
propose GeoCD, a topology-aware and fully differentiable approximation of
geodesic distance designed to serve as a metric for 3D point cloud learning.
Our experiments show that GeoCD consistently improves reconstruction quality
over standard CD across various architectures and datasets. We demonstrate this
by fine-tuning several models, initially trained with standard CD, using GeoCD.
Remarkably, fine-tuning for a single epoch with GeoCD yields significant gains
across multiple evaluation metrics.

</details>


### [271] [Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting](https://arxiv.org/abs/2506.23479)
*Zhaojie Zeng,Yuesong Wang,Chao Yang,Tao Guan,Lili Ju*

Main category: cs.CV

TL;DR: This paper proposes a self-adaptive image representation method using 2D Gaussian Splatting, enabling faster training and adaptability to image complexity while maintaining or surpassing rendering quality.


<details>
  <summary>Details</summary>
Motivation: GaussianImage showed the potential of Gaussian Splatting to reduce GPU costs in implicit neural image representation but is limited by slow training and fixed Gaussian numbers per image.

Method: The method involves a network to quickly generate a coarse Gaussian representation, followed by minimal fine-tuning. It also adapts the number of Gaussians dynamically based on image complexity.

Result: The proposed method achieves comparable or superior rendering quality compared to GaussianImage, reduces training time by up to 10x, and offers greater flexibility and efficiency.

Conclusion: This framework achieves efficient, high-quality image rendering with reduced computational demands, addressing key limitations of GaussianImage.

Abstract: Implicit Neural Representation (INR) has demonstrated remarkable advances in
the field of image representation but demands substantial GPU resources.
GaussianImage recently pioneered the use of Gaussian Splatting to mitigate this
cost, however, the slow training process limits its practicality, and the fixed
number of Gaussians per image limits its adaptability to varying information
entropy. To address these issues, we propose in this paper a generalizable and
self-adaptive image representation framework based on 2D Gaussian Splatting.
Our method employs a network to quickly generate a coarse Gaussian
representation, followed by minimal fine-tuning steps, achieving comparable
rendering quality of GaussianImage while significantly reducing training time.
Moreover, our approach dynamically adjusts the number of Gaussian points based
on image complexity to further enhance flexibility and efficiency in practice.
Experiments on DIV2K and Kodak datasets show that our method matches or exceeds
GaussianImage's rendering performance with far fewer iterations and shorter
training times. Specifically, our method reduces the training time by up to one
order of magnitude while achieving superior rendering performance with the same
number of Gaussians.

</details>


### [272] [Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks](https://arxiv.org/abs/2506.23481)
*Xian Zhang,Xiang Cheng*

Main category: cs.CV

TL;DR: The study evaluates the capabilities and privacy implications of Multimodal Large Language Models (MLLMs) in geolocating street-view imagery, achieving 49% accuracy within 1 km radius, and discusses countermeasures.


<details>
  <summary>Details</summary>
Motivation: To investigate how advanced Multimodal Large Language Models (MLLMs) perform geolocation tasks using visual data and to understand their privacy and ethical implications.

Method: The study reviews existing literature, tests state-of-the-art visual reasoning MLLMs on geolocating street imagery, and identifies contributing visual cues.

Result: Advanced MLLMs achieved 49% accuracy in localizing street-level imagery within a 1-kilometer radius, showcasing their strong capacity for geographic reasoning based on visual inputs.

Conclusion: The study highlights key elements like text and architectural features aiding geolocation, emphasizes privacy risks, and proposes both technical and policy-based countermeasures to mitigate the potential threats from such capabilities of MLLMs.

Abstract: Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)
has significantly enhanced their reasoning capabilities, enabling a wide range
of intelligent applications. However, these advancements also raise critical
concerns regarding privacy and ethics. MLLMs are now capable of inferring the
geographic location of images -- such as those shared on social media or
captured from street views -- based solely on visual content, thereby posing
serious risks of privacy invasion, including doxxing, surveillance, and other
security threats.
  Methods: This study provides a comprehensive analysis of existing geolocation
techniques based on MLLMs. It systematically reviews relevant litera-ture and
evaluates the performance of state-of-the-art visual reasoning models on
geolocation tasks, particularly in identifying the origins of street view
imagery.
  Results: Empirical evaluation reveals that the most advanced visual large
models can successfully localize the origin of street-level imagery with up to
$49\%$ accuracy within a 1-kilometer radius. This performance underscores the
models' powerful capacity to extract and utilize fine-grained geographic cues
from visual data.
  Conclusions: Building on these findings, the study identifies key visual
elements that contribute to suc-cessful geolocation, such as text,
architectural styles, and environmental features. Furthermore, it discusses the
potential privacy implications associated with MLLM-enabled geolocation and
discuss several technical and policy-based coun-termeasures to mitigate
associated risks. Our code and dataset are available at
https://github.com/zxyl1003/MLLM-Geolocation-Evaluation.

</details>


### [273] [MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting](https://arxiv.org/abs/2506.23482)
*Jun Huang,Ting Liu,Yihang Wu,Xiaochao Qu,Luoqi Liu,Xiaolin Hu*

Main category: cs.CV

TL;DR: MTADiffusion, leveraging mask-text alignment and style-consistency loss, achieves state-of-the-art object inpainting performance.


<details>
  <summary>Details</summary>
Motivation: Existing image inpainting methods face challenges including semantic misalignment, structural distortion, and style inconsistency.

Method: This paper introduces MTADiffusion, which applies mask-text alignment, automated mask annotations (MTAPipeline), a new MTADataset, multi-task training combining inpainting and edge prediction, and a novel style-consistency loss leveraging VGG networks.

Result: MTADiffusion outperformed competing methods in semantic, structural, and stylistic metrics in objective evaluations conducted on BrushBench and EditBench.

Conclusion: The proposed approach addresses key limitations in current inpainting methods and sets new performance standards in object inpainting tasks.

Abstract: Advancements in generative models have enabled image inpainting models to
generate content within specific regions of an image based on provided prompts
and masks. However, existing inpainting methods often suffer from problems such
as semantic misalignment, structural distortion, and style inconsistency. In
this work, we present MTADiffusion, a Mask-Text Alignment diffusion model
designed for object inpainting. To enhance the semantic capabilities of the
inpainting model, we introduce MTAPipeline, an automatic solution for
annotating masks with detailed descriptions. Based on the MTAPipeline, we
construct a new MTADataset comprising 5 million images and 25 million mask-text
pairs. Furthermore, we propose a multi-task training strategy that integrates
both inpainting and edge prediction tasks to improve structural stability. To
promote style consistency, we present a novel inpainting style-consistency loss
using a pre-trained VGG network and the Gram matrix. Comprehensive evaluations
on BrushBench and EditBench demonstrate that MTADiffusion achieves
state-of-the-art performance compared to other methods.

</details>


### [274] [Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding](https://arxiv.org/abs/2506.23491)
*ZongHan Hsieh,Tzer-Jen Wei*

Main category: cs.CV

TL;DR: Qwen-GUI-3B is a compact Vision-Language Model tailored for GUI grounding tasks, offering competitive accuracy while being efficient enough for single GPU training.


<details>
  <summary>Details</summary>
Motivation: The study addresses the inefficiency and infeasibility of using large-scale VLMs (>7B parameters) for GUI grounding on consumer-grade hardware.

Method: The model employs a multi-resolution dataset, a two-stage fine-tuning strategy, and data curation to balance performance and efficiency.

Result: Qwen-GUI-3B achieves 84.9% accuracy on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing other models under 4B parameters.

Conclusion: The results affirm Qwen-GUI-3B as an efficient and powerful option for GUI grounding tasks, with meaningful advancements in performance and practicality.

Abstract: This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)
specifically designed for Graphical User Interface grounding tasks, achieving
performance competitive with significantly larger models. Unlike large-scale
VLMs (>7B parameters) that are computationally intensive and impractical for
consumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while
being fully trainable on a single GPU (RTX 4090). The model incorporates
several key innovations: (i) combine cross-platform, multi-resolution dataset
of 24K examples from diverse sources including mobile, desktop, and web GUI
screenshots to effectively address data scarcity in high-resolution desktop
environments; (ii) a two-stage fine-tuning strategy, where initial
cross-platform training establishes robust GUI understanding, followed by
specialized fine-tuning on high-resolution data to significantly enhance model
adaptability; and (iii) data curation and redundancy reduction strategies,
demonstrating that randomly sampling a smaller subset with reduced redundancy
achieves performance comparable to larger datasets, emphasizing data diversity
over sheer volume. Empirical evaluation on standard GUI grounding
benchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging
ScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%
on ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B
parameters. Ablation studies validate the critical role of balanced sampling
and two-stage fine-tuning in enhancing robustness, particularly in
high-resolution desktop scenarios. The Qwen-GUI-3B is available at:
https://github.com/Han1018/Qwen-GUI-3B

</details>


### [275] [LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching](https://arxiv.org/abs/2506.23502)
*Mengxiao Tian,Xinxiao Wu,Shuo Yang*

Main category: cs.CV

TL;DR: The paper introduces an action-aware multi-modal prompt-tuning approach to enhance CLIP's understanding of fine-grained action-level details using structured knowledge from large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: CLIP struggles with understanding fine-grained details such as object attributes, spatial relationships, and actions, which are vital for describing states or relations between objects.

Method: The authors propose a method using LLM-enhanced prompts—specifically, action triplet and action state prompts—to encode semantic and causal knowledge. They also develop an adaptive interaction module to aggregate attentive visual features conditioned on these prompts.

Result: The proposed method achieves improved visual representation and outperforms prior works on two benchmark datasets.

Conclusion: Incorporating action-aware prompts and LLM-enhanced knowledge improves CLIP's ability to understand complex visual-language alignments, contributing to fine-grained action-level comprehension.

Abstract: Driven by large-scale contrastive vision-language pre-trained models such as
CLIP, recent advancements in the image-text matching task have achieved
remarkable success in representation learning. Due to image-level
visual-language alignment, CLIP falls short in understanding fine-grained
details such as object attributes and spatial relationships between objects.
Recent efforts have attempted to compel CLIP to acquire structured visual
representations by introducing prompt learning to achieve object-level
alignment. While achieving promising results, they still lack the capability to
perceive actions, which are crucial for describing the states or relationships
between objects. Therefore, we propose to endow CLIP with fine-grained
action-level understanding by introducing an LLM-enhanced action-aware
multi-modal prompt-tuning method, incorporating the action-related external
knowledge generated by large language models (LLMs). Specifically, we design an
action triplet prompt and an action state prompt to exploit compositional
semantic knowledge and state-related causal knowledge implicitly stored in
LLMs. Subsequently, we propose an adaptive interaction module to aggregate
attentive visual features conditioned on action-aware prompted knowledge for
establishing discriminative and action-aware visual representations, which
further improves the performance. Comprehensive experimental results on two
benchmark datasets demonstrate the effectiveness of our method.

</details>


### [276] [Improve Underwater Object Detection through YOLOv12 Architecture and Physics-informed Augmentation](https://arxiv.org/abs/2506.23505)
*Tinh Nguyen*

Main category: cs.CV

TL;DR: The paper enhances underwater object detection by integrating physics-informed augmentations with the YOLOv12 model, achieving state-of-the-art performance while maintaining real-time processing capabilities.


<details>
  <summary>Details</summary>
Motivation: Underwater object detection is hindered by challenges like light attenuation, water turbidity, and occlusion, which limit the deployment of real-time and accurate detection systems in low-visibility conditions.

Method: The study integrates domain-specific augmentations such as turbulence-adaptive blurring, occlusion simulations, and spectral HSV transformations with the YOLOv12 architecture, incorporating Residual ELAN blocks and Area Attention to enhance feature preservation and computational efficiency.

Result: The proposed method significantly improves detection metrics, achieving 98.30% mAP at 142 FPS on Brackish data while enhancing occlusion robustness by 18.9% and small-object recall by 22.4%.

Conclusion: This research delivers an effective and precise approach to underwater object detection, suitable for applications like conservation and underwater robotics, validated by ablation studies.

Abstract: Underwater object detection is crucial for autonomous navigation,
environmental monitoring, and marine exploration, but it is severely hampered
by light attenuation, turbidity, and occlusion. Current methods balance
accuracy and computational efficiency, but they have trouble deploying in
real-time under low visibility conditions. Through the integration of
physics-informed augmentation techniques with the YOLOv12 architecture, this
study advances underwater detection. With Residual ELAN blocks to preserve
structural features in turbid waters and Area Attention to maintain large
receptive fields for occluded objects while reducing computational complexity.
Underwater optical properties are addressed by domain-specific augmentations
such as turbulence adaptive blurring, biologically grounded occlusion
simulation, and spectral HSV transformations for color distortion. Extensive
tests on four difficult datasets show state-of-the-art performance, with
Brackish data registering 98.30% mAP at 142 FPS. YOLOv12 improves occlusion
robustness by 18.9%, small-object recall by 22.4%, and detection precision by
up to 7.94% compared to previous models. The crucial role of augmentation
strategy is validated by ablation studies. This work offers a precise and
effective solution for conservation and underwater robotics applications.

</details>


### [277] [ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models](https://arxiv.org/abs/2506.23513)
*Zixun Fang,Kai Zhu,Zhiheng Liu,Yu Liu,Wei Zhai,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: The paper proposes a method to generate high-quality 360-degree panoramic videos using a novel representation and pretrained perspective video models, significantly outperforming previous approaches.


<details>
  <summary>Details</summary>
Motivation: Synthesizing high-quality 360-degree videos is crucial for VR, world models, and spatial intelligence, but existing methods struggle due to gaps between panoramic and perspective data.

Method: The paper introduces the ViewPoint map representation and a Pano-Perspective attention mechanism to leverage pretrained perspective video models while effectively capturing panoramic spatial correlations.

Result: Extensive experiments show the proposed approach generates dynamic, spatially consistent panoramic videos and achieves state-of-the-art results.

Conclusion: This framework bridges the panoramic and perspective data gap, enabling the generation of superior 360-degree videos with applications in immersive media and beyond.

Abstract: Panoramic video generation aims to synthesize 360-degree immersive videos,
holding significant importance in the fields of VR, world models, and spatial
intelligence. Existing works fail to synthesize high-quality panoramic videos
due to the inherent modality gap between panoramic data and perspective data,
which constitutes the majority of the training data for modern diffusion
models. In this paper, we propose a novel framework utilizing pretrained
perspective video models for generating panoramic videos. Specifically, we
design a novel panorama representation named ViewPoint map, which possesses
global spatial continuity and fine-grained visual details simultaneously. With
our proposed Pano-Perspective attention mechanism, the model benefits from
pretrained perspective priors and captures the panoramic spatial correlations
of the ViewPoint map effectively. Extensive experiments demonstrate that our
method can synthesize highly dynamic and spatially consistent panoramic videos,
achieving state-of-the-art performance and surpassing previous methods.

</details>


### [278] [WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image](https://arxiv.org/abs/2506.23518)
*Jiwoo Park,Tae Eun Choi,Youngjun Jun,Seong Jae Hwang*

Main category: cs.CV

TL;DR: The paper presents a training-free method for improving view consistency in novel view synthesis using diffusion models, avoiding the inefficiency of previous 3D model-integrated methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of maintaining view consistency in novel view synthesis, which current diffusion models struggle to achieve effectively and efficiently.

Method: A novel training-free approach is proposed that uses adaptive attention manipulation and noise reinitialization, leveraging view-guided warping to enhance diffusion models for consistent view generation.

Result: The proposed method significantly improves view consistency across various diffusion models, as demonstrated through a comprehensive metric framework for novel-view datasets.

Conclusion: The study demonstrates improved efficiency and broader applicability of diffusion models in generating view-consistent novel scenes through the introduced method.

Abstract: Generating high-quality novel views of a scene from a single image requires
maintaining structural coherence across different views, referred to as view
consistency. While diffusion models have driven advancements in novel view
synthesis, they still struggle to preserve spatial continuity across views.
Diffusion models have been combined with 3D models to address the issue, but
such approaches lack efficiency due to their complex multi-step pipelines. This
paper proposes a novel view-consistent image generation method which utilizes
diffusion models without additional modules. Our key idea is to enhance
diffusion models with a training-free method that enables adaptive attention
manipulation and noise reinitialization by leveraging view-guided warping to
ensure view consistency. Through our comprehensive metric framework suitable
for novel-view datasets, we show that our method improves view consistency
across various diffusion models, demonstrating its broader applicability.

</details>


### [279] [From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection](https://arxiv.org/abs/2506.23519)
*Qi Qin,Runmin Cong,Gen Zhan,Yiting Liao,Sam Kwong*

Main category: cs.CV

TL;DR: This paper introduces a model that uses fixation information from eye-tracking to improve weakly-supervised video salient object detection. It incorporates novel methods for feature learning and spatiotemporal modeling.


<details>
  <summary>Details</summary>
Motivation: Eye tracker annotations are easier to obtain and align closely with human visual patterns. The paper aims to leverage this fixation information to enhance the detection of video salient objects under weak supervision.

Method: The paper proposes a Position and Semantic Embedding (PSE) module for location and semantic guidance during feature learning and introduces a Semantics and Locality Query (SLQ) Competitor for effective object query selection. An Intra-Inter Mixed Contrastive (IIMC) model is also developed for improved spatiotemporal modeling through contrastive learning.

Result: The model's performance surpasses that of other competitors across various evaluation metrics when tested on five popular VSOD benchmarks.

Conclusion: Integrating fixation information with weak supervision significantly enhances video salient object detection, demonstrating the effectiveness of the proposed methods in spatiotemporal modeling and feature selection.

Abstract: The eye-tracking video saliency prediction (VSP) task and video salient
object detection (VSOD) task both focus on the most attractive objects in video
and show the result in the form of predictive heatmaps and pixel-level saliency
masks, respectively. In practical applications, eye tracker annotations are
more readily obtainable and align closely with the authentic visual patterns of
human eyes. Therefore, this paper aims to introduce fixation information to
assist the detection of video salient objects under weak supervision. On the
one hand, we ponder how to better explore and utilize the information provided
by fixation, and then propose a Position and Semantic Embedding (PSE) module to
provide location and semantic guidance during the feature learning process. On
the other hand, we achieve spatiotemporal feature modeling under weak
supervision from the aspects of feature selection and feature contrast. A
Semantics and Locality Query (SLQ) Competitor with semantic and locality
constraints is designed to effectively select the most matching and accurate
object query for spatiotemporal modeling. In addition, an Intra-Inter Mixed
Contrastive (IIMC) model improves the spatiotemporal modeling capabilities
under weak supervision by forming an intra-video and inter-video contrastive
learning paradigm. Experimental results on five popular VSOD benchmarks
indicate that our model outperforms other competitors on various evaluation
metrics.

</details>


### [280] [Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving](https://arxiv.org/abs/2506.23523)
*Tuong Do,Binh X. Nguyen,Quang D. Tran,Erman Tjiputra,Te-Chuan Chiu,Anh Nguyen*

Main category: cs.CV

TL;DR: The paper introduces a lightweight temporal transformer decomposition approach to improve autonomous driving in complex environments by utilizing temporal data like past images and steering sequences.


<details>
  <summary>Details</summary>
Motivation: Many existing high-performance autonomous driving methods struggle with resource-intensive fusion networks and the practicalities of federated learning.

Method: The proposed method decomposes large attention maps into smaller matrices, enabling efficient processing of temporal data and reducing model complexity for faster updates and real-time prediction.

Result: Experiments across three datasets show superior performance compared to recent methods, with additional validation from real robot experiments.

Conclusion: Temporal transformer decomposition improves temporal data use, achieving efficiency and enhanced performance in autonomous driving systems.

Abstract: Traditional vision-based autonomous driving systems often face difficulties
in navigating complex environments when relying solely on single-image inputs.
To overcome this limitation, incorporating temporal data such as past image
frames or steering sequences, has proven effective in enhancing robustness and
adaptability in challenging scenarios. While previous high-performance methods
exist, they often rely on resource-intensive fusion networks, making them
impractical for training and unsuitable for federated learning. To address
these challenges, we propose lightweight temporal transformer decomposition, a
method that processes sequential image frames and temporal steering data by
breaking down large attention maps into smaller matrices. This approach reduces
model complexity, enabling efficient weight updates for convergence and
real-time predictions while leveraging temporal information to enhance
autonomous driving performance. Intensive experiments on three datasets
demonstrate that our method outperforms recent approaches by a clear margin
while achieving real-time performance. Additionally, real robot experiments
further confirm the effectiveness of our method.

</details>


### [281] [When Test-Time Adaptation Meets Self-Supervised Models](https://arxiv.org/abs/2506.23529)
*Jisu Han,Jihee Park,Dongyoon Han,Wonjun Hwang*

Main category: cs.CV

TL;DR: The paper addresses test-time adaptation (TTA) for self-supervised learning (SSL) models, proposing a protocol and framework to refine representations without relying on source pretraining.


<details>
  <summary>Details</summary>
Motivation: Investigates the limitations of existing TTA methods for self-supervised models and aims to enable continuous performance improvement even without source pretrained models.

Method: Proposed a self-supervised TTA protocol and a collaborative learning framework combining SSL and TTA models, utilizing contrastive learning and knowledge distillation.

Result: Extensive experiments show the proposed method improves SSL performance and achieves competitive results without requiring source pretraining.

Conclusion: The approach successfully adapts self-supervised models at test time, offering a promising alternative to source-pretrained models and enhancing TTA effectiveness.

Abstract: Training on test-time data enables deep learning models to adapt to dynamic
environmental changes, enhancing their practical applicability. Online
adaptation from source to target domains is promising but it remains highly
reliant on the performance of source pretrained model. In this paper, we
investigate whether test-time adaptation (TTA) methods can continuously improve
models trained via self-supervised learning (SSL) without relying on source
pretraining. We introduce a self-supervised TTA protocol after observing that
existing TTA approaches struggle when directly applied to self-supervised
models with low accuracy on the source domain. Furthermore, we propose a
collaborative learning framework that integrates SSL and TTA models, leveraging
contrastive learning and knowledge distillation for stepwise representation
refinement. We validate our method on diverse self-supervised models, including
DINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the
effectiveness of our approach in SSL, showing that it achieves competitive
performance even without source pretraining.

</details>


### [282] [GViT: Representing Images as Gaussians for Visual Recognition](https://arxiv.org/abs/2506.23532)
*Jefferson Hernandez,Ruozhen He,Guha Balakrishnan,Alexander C. Berg,Vicente Ordonez*

Main category: cs.CV

TL;DR: This paper introduces GVIT, a novel classification framework that uses learnable 2D Gaussian input representations instead of traditional pixel or patch grid inputs for training ViT classifiers.


<details>
  <summary>Details</summary>
Motivation: To explore alternative input representations for vision transformers, moving away from pixel or patch grids, and potentially improve classification performance.

Method: Images are encoded as learnable 2D Gaussians optimized for positional, color, and orientation properties alongside classifier training. Gradients from the ViT classifier guide Gaussian optimization while maintaining reconstruction accuracy via a differentiable renderer.

Result: GVIT achieves competitive performance, including a 76.9% top-1 accuracy on Imagenet-1k, using a relatively standard ViT-B architecture.

Conclusion: The study demonstrates that learnable 2D Gaussian input representations can closely match conventional ViT performance while innovating input structure and optimization guidance.

Abstract: We introduce GVIT, a classification framework that abandons conventional
pixel or patch grid input representations in favor of a compact set of
learnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose
positions, scales, orientations, colors, and opacities are optimized jointly
with a ViT classifier trained on top of these representations. We reuse the
classifier gradients as constructive guidance, steering the Gaussians toward
class-salient regions while a differentiable renderer optimizes an image
reconstruction loss. We demonstrate that by 2D Gaussian input representations
coupled with our GVIT guidance, using a relatively standard ViT architecture,
closely matches the performance of a traditional patch-based ViT, reaching a
76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.

</details>


### [283] [Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound](https://arxiv.org/abs/2506.23538)
*Yuhao Huang,Yueyue Xu,Haoran Dou,Jiaxiao Deng,Xin Yang,Hongyu Zheng,Dong Ni*

Main category: cs.CV

TL;DR: This study presents a novel intelligent system leveraging advanced diffusion models, reinforcement learning, and text-driven approaches for automated plane localization and congenital uterine anomaly diagnosis using 3D ultrasound.


<details>
  <summary>Details</summary>
Motivation: Congenital uterine anomalies can lead to significant reproductive issues, and 3D ultrasound offers better visualization compared to traditional 2D ultrasound, yet automated diagnosis remains challenging.

Method: The authors developed techniques including a denoising diffusion model with adaptive guidance, reinforcement learning with unsupervised rewards, and uncertainty modeling driven by text to optimize classifications and plane localization in 3D ultrasound imaging.

Result: Experiments conducted on a large dataset demonstrated the system's effectiveness in accurately localizing planes and diagnosing congenital uterine anomalies.

Conclusion: The proposed system significantly enhances 3D ultrasound-based plane localization and CUA diagnosis, offering a promising tool for clinical applications.

Abstract: Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,
preterm birth, and an increased risk of pregnancy complications. Compared to
traditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,
providing a clear visualization of the uterine morphology for assessing CUAs
accurately. In this paper, we propose an intelligent system for simultaneous
automated plane localization and CUA diagnosis. Our highlights are: 1) we
develop a denoising diffusion model with local (plane) and global (volume/text)
guidance, using an adaptive weighting strategy to optimize attention allocation
to different conditions; 2) we introduce a reinforcement learning-based
framework with unsupervised rewards to extract the key slice summary from
redundant sequences, fully integrating information across multiple planes to
reduce learning difficulty; 3) we provide text-driven uncertainty modeling for
coarse prediction, and leverage it to adjust the classification probability for
overall performance improvement. Extensive experiments on a large 3D uterine US
dataset show the efficacy of our method, in terms of plane localization and CUA
diagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.

</details>


### [284] [Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention](https://arxiv.org/abs/2506.23542)
*Weida Wang,Changyong He,Jin Zeng,Di Qiu*

Main category: cs.CV

TL;DR: The paper introduces a novel approach to Time-of-Flight (ToF) depth image denoising called motion-invariant graph fusion, which improves both temporal consistency and spatial clarity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for Time-of-Flight depth image denoising are either limited to single-frame processing or fail to account for depth variations across frames, leading to temporal inconsistencies and spatial ambiguity.

Method: The authors propose a ToF depth denoising network that uses motion-invariant graph fusion to enhance temporal and spatial quality. It employs cross-frame geometric attention based on self-similar graph structures, integrates an image smoothness prior and a data fidelity term, and applies an interpretable iterative filtering process with graph-informed weights.

Result: The proposed technique achieves state-of-the-art denoising accuracy and consistency on the synthetic DVToF dataset and demonstrates strong generalization performance on the real-world Kinectv2 dataset.

Conclusion: The method significantly improves ToF depth denoising by balancing accuracy and consistency through novel graph-based formulations, offering an interpretable and robust solution for both synthetic and real datasets.

Abstract: Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,
requiring denoising for reliable downstream applications. Previous works either
focus on single-frame processing, or perform multi-frame processing without
considering depth variations at corresponding pixels across frames, leading to
undesirable temporal inconsistency and spatial ambiguity. In this paper, we
propose a novel ToF depth denoising network leveraging motion-invariant graph
fusion to simultaneously enhance temporal stability and spatial sharpness.
Specifically, despite depth shifts across frames, graph structures exhibit
temporal self-similarity, enabling cross-frame geometric attention for graph
fusion. Then, by incorporating an image smoothness prior on the fused graph and
data fidelity term derived from ToF noise distribution, we formulate a maximum
a posterior problem for ToF denoising. Finally, the solution is unrolled into
iterative filters whose weights are adaptively learned from the graph-informed
geometric attention, producing a high-performance yet interpretable network.
Experimental results demonstrate that the proposed scheme achieves
state-of-the-art performance in terms of accuracy and consistency on synthetic
DVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.
Source code will be released at
\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.

</details>


### [285] [Pyramidal Patchification Flow for Visual Generation](https://arxiv.org/abs/2506.23543)
*Hui Li,Baoyou Chen,Liwei Zhang,Jiaye Li,Jingdong Wang,Siyu Zhu*

Main category: cs.CV

TL;DR: The paper introduces the Pyramidal Patchification Flow (PPFlow) to improve the performance and efficiency of Diffusion Transformers (DiTs) by adapting patch sizes based on noise levels during denoising.


<details>
  <summary>Details</summary>
Motivation: To address computational inefficiency in DiTs training and inference, especially in handling varied patch sizes during timesteps of denoising processes.

Method: PPFlow modifies patch sizes based on noise levels during timesteps, incorporates learned linear projections, adjusts the 'Unpatchify' process, and applies standard denoising methods. It supports training from scratch or fine-tuning from pretrained models.

Result: PPFlow achieves up to 2x faster inference speed while maintaining comparable image generation performance, with slightly reduced training FLOPs.

Conclusion: PPFlow is a computationally effective approach that enhances inference speed and image generation performance of DiTs, and can be trained efficiently either from scratch or using pretrained models.

Abstract: Diffusion transformers (DiTs) adopt Patchify, mapping patch representations
to token representations through linear projections, to adjust the number of
tokens input to DiT blocks and thus the computation cost. Instead of a single
patch size for all the timesteps, we introduce a Pyramidal Patchification Flow
(PPFlow) approach: Large patch sizes are used for high noise timesteps and
small patch sizes for low noise timesteps; Linear projections are learned for
each patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow,
our approach operates over full latent representations other than pyramid
representations, and adopts the normal denoising process without requiring the
renoising trick. We demonstrate the effectiveness of our approach through two
training manners. Training from scratch achieves a $1.6\times$ ($2.0\times$)
inference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with
slightly lower training FLOPs and similar image generation performance.
Training from pretrained normal DiTs achieves even better performance with
small training time. The code and checkpoint are at
https://github.com/fudan-generative-vision/PPFlow.

</details>


### [286] [Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions](https://arxiv.org/abs/2506.23547)
*Jiwon Kim,Soohyun Hwang,Dong-O Kim,Changsu Han,Min Kyu Park,Chang-Su Kim*

Main category: cs.CV

TL;DR: The paper introduces Oneta, a network for multi-style image enhancement, using two sequential functions for intensity and color correction via compact parameterization, successfully handling six tasks across 30 datasets.


<details>
  <summary>Details</summary>
Motivation: To develop an algorithm capable of addressing diverse image enhancement tasks under multiple styles using a unified framework.

Method: The Oneta algorithm employs two-step enhancement using intensity transformation (TF) and color correction (CCM). It uses learnable tokens for multi-style support and comprises Y-Net and C-Net to predict transformation parameters.

Result: Oneta demonstrated effectiveness on six enhancement tasks such as dehazing, underwater enhancement, and white balancing across 30 datasets.

Conclusion: Oneta achieves high performance in multi-style image enhancement, showcasing its versatility and adaptability across various enhancement tasks and datasets.

Abstract: The first algorithm, called Oneta, for a novel task of multi-style image
enhancement is proposed in this work. Oneta uses two point operators
sequentially: intensity enhancement with a transformation function (TF) and
color correction with a color correction matrix (CCM). This two-step
enhancement model, though simple, achieves a high performance upper bound.
Also, we introduce eigentransformation function (eigenTF) to represent TF
compactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and
CCM parameters, respectively. To support $K$ styles, Oneta employs $K$
learnable tokens. During training, each style token is learned using image
pairs from the corresponding dataset. In testing, Oneta selects one of the $K$
style tokens to enhance an image accordingly. Extensive experiments show that
the single Oneta network can effectively undertake six enhancement tasks --
retouching, image signal processing, low-light image enhancement, dehazing,
underwater image enhancement, and white balancing -- across 30 datasets.

</details>


### [287] [JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching](https://arxiv.org/abs/2506.23552)
*Mingi Kwon,Joonghyuk Shin,Jaeseok Jung,Jaesik Park,Youngjung Uh*

Main category: cs.CV

TL;DR: The paper presents JAM-Flow, a unified framework for simultaneously synthesizing and conditioning on facial motion and speech, integrating novel diffusion-based transformer architectures.


<details>
  <summary>Details</summary>
Motivation: To address the separation of talking head synthesis and text-to-speech tasks by creating a unified framework that bridges facial motion and speech in generative modeling.

Method: The method leverages flow matching and Multi-Modal Diffusion Transformer (MM-DiT) with Motion-DiT and Audio-DiT modules, using novel architectural components like temporally aligned positional embeddings and joint attention masking.

Result: JAM-Flow enables synchronized talking head generation and other audio-visual tasks from various inputs, presenting noteworthy performance in multi-modal generative modeling.

Conclusion: JAM-Flow provides an integrated and practical solution for holistic audio-visual synthesis, advancing the field of multi-modal generative modeling.

Abstract: The intrinsic link between facial motion and speech is often overlooked in
generative modeling, where talking head synthesis and text-to-speech (TTS) are
typically addressed as separate tasks. This paper introduces JAM-Flow, a
unified framework to simultaneously synthesize and condition on both facial
motion and speech. Our approach leverages flow matching and a novel Multi-Modal
Diffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT
and Audio-DiT modules. These are coupled via selective joint attention layers
and incorporate key architectural choices, such as temporally aligned
positional embeddings and localized joint attention masking, to enable
effective cross-modal interaction while preserving modality-specific strengths.
Trained with an inpainting-style objective, JAM-Flow supports a wide array of
conditioning inputs-including text, reference audio, and reference
motion-facilitating tasks such as synchronized talking head generation from
text, audio-driven animation, and much more, within a single, coherent model.
JAM-Flow significantly advances multi-modal generative modeling by providing a
practical solution for holistic audio-visual synthesis. project page:
https://joonghyuk.com/jamflow-web

</details>


### [288] [LH2Face: Loss function for Hard High-quality Face](https://arxiv.org/abs/2506.23555)
*Fan Xie,Pan Cao*

Main category: cs.CV

TL;DR: The paper proposes LH2Face, a new loss function for face recognition which excels particularly on challenging, high-quality face datasets.


<details>
  <summary>Details</summary>
Motivation: Address the shortcomings of existing face recognition methods that struggle with hard samples and fail to consider face quality and recognition hardness.

Method: Introduce LH2Face loss function using a combination of von Mises-Fisher distribution, Uncertainty-Aware Margin Function, proxy-based constraints, and face reconstruction-rendering optimization.

Result: LH2Face achieves a 49.39% accuracy on the IJB-B dataset, outperforming the previous best method by 2.37%.

Conclusion: LH2Face improves face recognition performance by effectively handling hard high-quality face samples through a combination of innovative techniques.

Abstract: In current practical face authentication systems, most face recognition (FR)
algorithms are based on cosine similarity with softmax classification. Despite
its reliable classification performance, this method struggles with hard
samples. A popular strategy to improve FR performance is incorporating angular
or cosine margins. However, it does not take face quality or recognition
hardness into account, simply increasing the margin value and thus causing an
overly uniform training strategy. To address this problem, a novel loss
function is proposed, named Loss function for Hard High-quality Face (LH2Face).
Firstly, a similarity measure based on the von Mises-Fisher (vMF) distribution
is stated, specifically focusing on the logarithm of the Probability Density
Function (PDF), which represents the distance between a probability
distribution and a vector. Then, an adaptive margin-based multi-classification
method using softmax, called the Uncertainty-Aware Margin Function, is
implemented in the article. Furthermore, proxy-based loss functions are used to
apply extra constraints between the proxy and sample to optimize their
representation space distribution. Finally, a renderer is constructed that
optimizes FR through face reconstruction and vice versa. Our LH2Face is
superior to similiar schemes on hard high-quality face datasets, achieving
49.39% accuracy on the IJB-B dataset, which surpasses the second-place method
by 2.37%.

</details>


### [289] [OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2506.23565)
*Mingqian Ji,Jian Yang,Shanshan Zhang*

Main category: cs.CV

TL;DR: The paper introduces Object-centric Radiance Fields (OcRF) to improve 3D object detection by focusing on modeling foreground objects while discarding background noise.


<details>
  <summary>Details</summary>
Motivation: Improve the performance of multi-view 3D object detection by addressing limitations in geometric estimation from 2D features.

Method: The authors propose OcRF for enhancing 3D voxel features through auxiliary rendering tasks focused on foreground objects. Additionally, they introduce a Height-aware Opacity-based Attention (HOA) mechanism for enhancing 2D foreground BEV features.

Result: OcRFDet achieves state-of-the-art performance on the nuScenes test benchmark with 57.2% mAP and 64.8% NDS, outperforming previous methods.

Conclusion: By shifting focus to object-centric modeling and leveraging opacity-based attention, their approach addresses background noise issues and significantly boosts detection accuracy.

Abstract: Current multi-view 3D object detection methods typically transfer 2D features
into 3D space using depth estimation or 3D position encoder, but in a fully
data-driven and implicit manner, which limits the detection performance.
Inspired by the success of radiance fields on 3D reconstruction, we assume they
can be used to enhance the detector's ability of 3D geometry estimation.
However, we observe a decline in detection performance, when we directly use
them for 3D rendering as an auxiliary task. From our analysis, we find the
performance drop is caused by the strong responses on the background when
rendering the whole scene. To address this problem, we propose object-centric
radiance fields, focusing on modeling foreground objects while discarding
background noises. Specifically, we employ Object-centric Radiance Fields
(OcRF) to enhance 3D voxel features via an auxiliary task of rendering
foreground objects. We further use opacity - the side-product of rendering- to
enhance the 2D foreground BEV features via Height-aware Opacity-based Attention
(HOA), where attention maps at different height levels are generated separately
via multiple networks in parallel. Extensive experiments on the nuScenes
validation and test datasets demonstrate that our OcRFDet achieves superior
performance, outperforming previous state-of-the-art methods with 57.2$\%$ mAP
and 64.8$\%$ NDS on the nuScenes test benchmark. Code will be available at
https://github.com/Mingqj/OcRFDet.

</details>


### [290] [Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution](https://arxiv.org/abs/2506.23566)
*Luigi Sigillo,Renato Giamba,Danilo Comminiello*

Main category: cs.CV

TL;DR: The paper introduces MWT-Diff, a novel satellite image super-resolution framework combining latent diffusion models with wavelet transforms to generate high-resolution satellite imagery.


<details>
  <summary>Details</summary>
Motivation: High-resolution satellite imagery is essential for applications like environmental monitoring and disaster response but is limited by sensor capabilities and costs.

Method: The MWT-Diff framework integrates a metadata-, wavelet-, and time-aware encoder to generate embeddings that guide latent diffusion models through hierarchical dynamics, preserving spatial characteristics in reconstruction.

Result: MWT-Diff demonstrated favorable performance in generating high-quality imagery across datasets, measured by metrics such as FID and LPIPS.

Conclusion: MWT-Diff effectively addresses key challenges in satellite image super-resolution, enhancing applications requiring detailed remote sensing analysis.

Abstract: The acquisition of high-resolution satellite imagery is often constrained by
the spatial and temporal limitations of satellite sensors, as well as the high
costs associated with frequent observations. These challenges hinder
applications such as environmental monitoring, disaster response, and
agricultural management, which require fine-grained and high-resolution data.
In this paper, we propose MWT-Diff, an innovative framework for satellite image
super-resolution (SR) that combines latent diffusion models with wavelet
transforms to address these challenges. At the core of the framework is a novel
metadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates
embeddings that capture metadata attributes, multi-scale frequency information,
and temporal relationships. The embedded feature representations steer the
hierarchical diffusion dynamics, through which the model progressively
reconstructs high-resolution satellite imagery from low-resolution inputs. This
process preserves critical spatial characteristics including textural patterns,
boundary discontinuities, and high-frequency spectral components essential for
detailed remote sensing analysis. The comparative analysis of MWT-Diff across
multiple datasets demonstrated favorable performance compared to recent
approaches, as measured by standard perceptual quality metrics including FID
and LPIPS.

</details>


### [291] [Event-based Tiny Object Detection: A Benchmark Dataset and Baseline](https://arxiv.org/abs/2506.23575)
*Nuo Chen,Chao Xiao,Yimian Dai,Shiman He,Miao Li,Wei An*

Main category: cs.CV

TL;DR: This paper addresses small object detection (SOD) in anti-UAV tasks using event cameras, introducing a novel dataset (EV-UAV) and a segmentation network (EV-SpSegNet).


<details>
  <summary>Details</summary>
Motivation: Traditional frame-based cameras perform poorly in anti-UAV tasks due to limitations such as low frame rates and challenges with small object detection in complex scenarios. Event cameras offer a potential solution but lack appropriate benchmark datasets for SOD in diverse, realistic scenarios.

Method: The authors introduced EV-UAV, a large-scale event-based dataset with 147 sequences and 2.3 million event-level annotations. They also proposed EV-SpSegNet, an event segmentation network that utilizes spatiotemporal event point clouds, and a new loss function called Spatiotemporal Correlation (STC) loss to enhance detection by leveraging motion continuity.

Result: The proposed EV-SpSegNet demonstrated state-of-the-art performance on the newly introduced EV-UAV dataset, showcasing its potential as a robust benchmark for event-based small object detection methods.

Conclusion: The study provides both a comprehensive benchmark (EV-UAV dataset) and a novel method (EV-SpSegNet) for advancing small object detection research in anti-UAV applications using event cameras.

Abstract: Small object detection (SOD) in anti-UAV task is a challenging problem due to
the small size of UAVs and complex backgrounds. Traditional frame-based cameras
struggle to detect small objects in complex environments due to their low frame
rates, limited dynamic range, and data redundancy. Event cameras, with
microsecond temporal resolution and high dynamic range, provide a more
effective solution for SOD. However, existing event-based object detection
datasets are limited in scale, feature large targets size, and lack diverse
backgrounds, making them unsuitable for SOD benchmarks. In this paper, we
introduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV),
the first large-scale, highly diverse benchmark for anti-UAV tasks. It includes
147 sequences with over 2.3 million event-level annotations, featuring
extremely small targets (averaging 6.8 $\times$ 5.4 pixels) and diverse
scenarios such as urban clutter and extreme lighting conditions. Furthermore,
based on the observation that small moving targets form continuous curves in
spatiotemporal event point clouds, we propose Event based Sparse Segmentation
Network (EV-SpSegNet), a novel baseline for event segmentation in point cloud
space, along with a Spatiotemporal Correlation (STC) loss that leverages motion
continuity to guide the network in retaining target events. Extensive
experiments on the EV-UAV dataset demonstrate the superiority of our method and
provide a benchmark for future research in EVSOD. The dataset and code are at
https://github.com/ChenYichen9527/Ev-UAV.

</details>


### [292] [StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection](https://arxiv.org/abs/2506.23577)
*Yanning Hou,Yanran Ruan,Junfa Li,Shanshan Wang,Jianfeng Qiu,Ke Xu*

Main category: cs.CV

TL;DR: This paper introduces StackCLIP, an approach aimed to enhance text-image feature alignment in CLIP models, addressing challenges in zero-shot industrial anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Zero-shot industrial anomaly detection using CLIP models faces issues of overfitting and limited generalization due to traditional category-specific prompts.

Method: The StackCLIP model uses stacked prompts to tackle overfitting by forming semantically related categories through CSP and EFA modules, and further improves performance with the RPL module.

Result: The proposed method achieves state-of-the-art results in anomaly detection and segmentation across seven datasets.

Conclusion: StackCLIP enhances training stability and generalization for anomaly detection tasks, outperforming existing methods in zero-shot settings.

Abstract: Enhancing the alignment between text and image features in the CLIP model is
a critical challenge in zero-shot industrial anomaly detection tasks. Recent
studies predominantly utilize specific category prompts during pretraining,
which can cause overfitting to the training categories and limit model
generalization. To address this, we propose a method that transforms category
names through multicategory name stacking to create stacked prompts, forming
the basis of our StackCLIP model. Our approach introduces two key components.
The Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts
by stacking semantically analogous categories, while utilizing multi-object
textual feature fusion to amplify discriminative anomalies among similar
objects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific
linear layers tailored for each stack cluster and adaptively integrates them
based on the attributes of test categories. These modules work together to
deliver superior training speed, stability, and convergence, significantly
boosting anomaly segmentation performance. Additionally, our stacked prompt
framework offers robust generalization across classification tasks. To further
improve performance, we introduce the Regulating Prompt Learning (RPL) module,
which leverages the generalization power of stacked prompts to refine prompt
learning, elevating results in anomaly detection classification tasks.
Extensive testing on seven industrial anomaly detection datasets demonstrates
that our method achieves state-of-the-art performance in both zero-shot anomaly
detection and segmentation tasks.

</details>


### [293] [Dataset Distillation via Vision-Language Category Prototype](https://arxiv.org/abs/2506.23580)
*Yawen Zou,Guang Li,Duo Su,Zi Wang,Jun Yu,Chao Zhang*

Main category: cs.CV

TL;DR: This paper introduces text prototypes derived from a large language model to improve dataset distillation by integrating vision-language methods, resulting in logically coherent images and superior generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional dataset distillation methods overlook semantic language information, which limits their performance and ability to generalize in tasks involving complex datasets.

Method: The authors propose incorporating text prototypes generated by an open-source large language model with image prototypes to distill both language and visual information collaboratively.

Result: The approach produces logically coherent images that include target objects and achieves state-of-the-art validation performance, demonstrating enhanced generalization.

Conclusion: Integrating vision-language methods into dataset distillation expands its applicability and improves generalization and coherence, providing significant advances in the field.

Abstract: Dataset distillation (DD) condenses large datasets into compact yet
informative substitutes, preserving performance comparable to the original
dataset while reducing storage, transmission costs, and computational
consumption. However, previous DD methods mainly focus on distilling
information from images, often overlooking the semantic information inherent in
the data. The disregard for context hinders the model's generalization ability,
particularly in tasks involving complex datasets, which may result in illogical
outputs or the omission of critical objects. In this study, we integrate
vision-language methods into DD by introducing text prototypes to distill
language information and collaboratively synthesize data with image prototypes,
thereby enhancing dataset distillation performance. Notably, the text
prototypes utilized in this study are derived from descriptive text information
generated by an open-source large language model. This framework demonstrates
broad applicability across datasets without pre-existing text descriptions,
expanding the potential of dataset distillation beyond traditional image-based
approaches. Compared to other methods, the proposed approach generates
logically coherent images containing target objects, achieving state-of-the-art
validation performance and demonstrating robust generalization. Source code and
generated data are available in
https://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/

</details>


### [294] [PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection](https://arxiv.org/abs/2506.23581)
*Xiao Li,Yiming Zhu,Yifan Huang,Wei Zhang,Yingzhe He,Jie Shi,Xiaolin Hu*

Main category: cs.CV

TL;DR: The paper introduces PBCAT, an adversarial training method combining patch-based and global perturbations to improve object detection resilience against physically realizable adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Recent studies show object detectors are vulnerable to physically realizable attacks, such as adversarial patches and textures, posing security risks. Existing defenses are limited in addressing the full range of such attacks.

Method: The proposed PBCAT method integrates small-area gradient-guided adversarial patches and imperceptible global perturbations across the image into adversarial training.

Result: Experiments show PBCAT boosts robustness to a variety of physically realizable attacks, achieving a 29.7% improvement in detection accuracy over previous methods against texture attacks.

Conclusion: PBCAT is an effective defense strategy against diverse physically realizable attacks, enhancing the security of object detection systems.

Abstract: Object detection plays a crucial role in many security-sensitive
applications. However, several recent studies have shown that object detectors
can be easily fooled by physically realizable attacks, \eg, adversarial patches
and recent adversarial textures, which pose realistic and urgent threats.
Adversarial Training (AT) has been recognized as the most effective defense
against adversarial attacks. While AT has been extensively studied in the
$l_\infty$ attack settings on classification models, AT against physically
realizable attacks on object detectors has received limited exploration. Early
attempts are only performed to defend against adversarial patches, leaving AT
against a wider range of physically realizable attacks under-explored. In this
work, we consider defending against various physically realizable attacks with
a unified AT method. We propose PBCAT, a novel Patch-Based Composite
Adversarial Training strategy. PBCAT optimizes the model by incorporating the
combination of small-area gradient-guided adversarial patches and imperceptible
global adversarial perturbations covering the entire image. With these designs,
PBCAT has the potential to defend against not only adversarial patches but also
unseen physically realizable attacks such as adversarial textures. Extensive
experiments in multiple settings demonstrated that PBCAT significantly improved
robustness against various physically realizable attacks over state-of-the-art
defense methods. Notably, it improved the detection accuracy by 29.7\% over
previous defense methods under one recent adversarial texture attack.

</details>


### [295] [CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2506.23590)
*Qiming Li,Zekai Ye,Xiaocheng Feng,Weihong Zhong,Libo Qin,Ruihan Chen,Baohang Li,Kui Jiang,Yaowei Wang,Ting Liu,Bing Qin*

Main category: cs.CV

TL;DR: The paper introduces Caption-sensitive Attention Intervention (CAI), a method for addressing object hallucination in Large Vision-Language Models without requiring additional training or significant inference costs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of object hallucination in Large Vision-Language Models, where the generated content deviates from visual information.

Method: The paper observes that LVLMs respond better to caption queries and proposes CAI, a training-free method that enhances visual perception by utilizing attention activation patterns triggered by caption queries.

Result: CAI achieves state-of-the-art hallucination mitigation across four benchmarks, demonstrating effectiveness in both discriminative and generative tasks with minimal inference overhead.

Conclusion: CAI provides a practical and efficient solution for improving LVLM performance, mitigating hallucination without relying on manual annotations or expensive training.

Abstract: Although Large Vision-Language Models (LVLMs) have demonstrated powerful
capabilities in interpreting visual information, they frequently produce
content that deviates from visual information, leading to object hallucination.
To tackle this, recent works mostly depend on expensive manual annotations and
training cost, or significantly increase inference time. In this work, we
observe that LVLMs' attention to visual information is significantly stronger
when answering caption queries compared to non-caption queries. Inspired by
this phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a
training-free, plug-and-play hallucination mitigation method that leverages the
attention activation pattern in response to caption queries to enhance LVLMs'
visual perception capability. Extensive experimental results across four
benchmarks covering both discriminative and generative tasks, demonstrate that
CAI achieves state-of-the-art (SOTA) hallucination mitigating performance only
with minimal additional inference cost.

</details>


### [296] [AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval](https://arxiv.org/abs/2506.23605)
*Suyash Maniyar,Vishvesh Trivedi,Ajoy Mondal,Anand Mishra,C. V. Jawahar*

Main category: cs.CV

TL;DR: The paper introduces SynLecSlideGen, a pipeline for generating high-quality synthetic lecture slides using large language models (LLMs), along with a benchmark dataset RealSlide. The study demonstrates that synthetic slides can enhance few-shot transfer learning performance for lecture slide tasks.


<details>
  <summary>Details</summary>
Motivation: Training models for lecture slide detection and retrieval requires extensive manual annotations, which are labor-intensive and demand domain expertise. This paper aims to address this limitation by using synthetic data.

Method: The authors propose SynLecSlideGen, an LLM-guided pipeline to produce synthetic lecture slides, and also introduce a benchmark dataset, RealSlide, containing annotations for 1,050 real slides.

Result: The experimental results show that few-shot transfer learning on real lecture slides is significantly improved by pretraining models on synthetic slides, outperforming training exclusively on real data.

Conclusion: Synthetic data generated by SynLecSlideGen can effectively complement limited labeled data, improving slide understanding tasks and reducing the need for extensive manual annotations.

Abstract: Lecture slide element detection and retrieval are key problems in slide
understanding. Training effective models for these tasks often depends on
extensive manual annotation. However, annotating large volumes of lecture
slides for supervised training is labor intensive and requires domain
expertise. To address this, we propose a large language model (LLM)-guided
synthetic lecture slide generation pipeline, SynLecSlideGen, which produces
high-quality, coherent and realistic slides. We also create an evaluation
benchmark, namely RealSlide by manually annotating 1,050 real lecture slides.
To assess the utility of our synthetic slides, we perform few-shot transfer
learning on real data using models pre-trained on them. Experimental results
show that few-shot transfer learning with pretraining on synthetic slides
significantly improves performance compared to training only on real data. This
demonstrates that synthetic data can effectively compensate for limited labeled
lecture slides. The code and resources of our work are publicly available on
our project website: https://synslidegen.github.io/.

</details>


### [297] [SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion](https://arxiv.org/abs/2506.23606)
*Zhengkang Xiang,Zizhao Li,Amir Khodabandeh,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: The paper introduces SG-LDM, a Semantic-Guided Lidar Diffusion Model, to synthesize lidar point clouds using semantic guidance, achieving superior performance in lidar data generation and downstream tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing methods that focus on unconditional lidar point cloud generation and lack applicability in real-world scenarios.

Method: The proposed SG-LDM utilizes latent alignment and explicit semantic conditioning in the native lidar space, and introduces a novel diffusion-based lidar translation framework for domain adaptation.

Result: SG-LDM achieves state-of-the-art results in generating lidar point clouds and enhances downstream perception performance, outperforming existing models.

Conclusion: The study concludes that SG-LDM is effective for both data synthesis and cross-domain translation, making it beneficial for lidar segmentation tasks and augmenting deep learning workflows.

Abstract: Lidar point cloud synthesis based on generative models offers a promising
solution to augment deep learning pipelines, particularly when real-world data
is scarce or lacks diversity. By enabling flexible object manipulation, this
synthesis approach can significantly enrich training datasets and enhance
discriminative models. However, existing methods focus on unconditional lidar
point cloud generation, overlooking their potential for real-world
applications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar
Diffusion Model that employs latent alignment to enable robust
semantic-to-lidar synthesis. By directly operating in the native lidar space
and leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art
performance in generating high-fidelity lidar point clouds guided by semantic
labels. Moreover, we propose the first diffusion-based lidar translation
framework based on SG-LDM, which enables cross-domain translation as a domain
adaptation strategy to enhance downstream perception performance. Systematic
experiments demonstrate that SG-LDM significantly outperforms existing lidar
diffusion models and the proposed lidar translation framework further improves
data augmentation performance in the downstream lidar segmentation task.

</details>


### [298] [PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum](https://arxiv.org/abs/2506.23607)
*Shiqi Zhang,Sha Zhang,Jiajun Deng,Yedong Shen,Mingxiao MA,Yanyong Zhang*

Main category: cs.CV

TL;DR: The paper introduces PGOV3D, a framework for open-vocabulary 3D semantic segmentation that leverages a two-stage curriculum pre-training approach to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for open-vocabulary 3D segmentation rely heavily on transferring text-aligned features via multi-view images but miss out on the rich semantic data and cross-view correlations in the images.

Method: PGOV3D uses a two-stage strategy: (1) pre-training on partial scenes using RGB-D inputs with open-vocabulary labels generated by multi-modal models, and (2) fine-tuning on complete 3D scenes with pseudo-labels aggregated from the partial observations.

Result: The proposed method demonstrated competitive performance in open-vocabulary 3D segmentation benchmarks like ScanNet, ScanNet200, and S3DIS.

Conclusion: By introducing the Partial-to-Global curriculum and leveraging rich semantic supervision, PGOV3D improves segmentation accuracy by scaling from simpler partial inputs to more complex 3D environments.

Abstract: Existing open-vocabulary 3D semantic segmentation methods typically supervise
3D segmentation models by merging text-aligned features (e.g., CLIP) extracted
from multi-view images onto 3D points. However, such approaches treat
multi-view images merely as intermediaries for transferring open-vocabulary
information, overlooking their rich semantic content and cross-view
correspondences, which limits model effectiveness. To address this, we propose
PGOV3D, a novel framework that introduces a Partial-to-Global curriculum for
improving open-vocabulary 3D semantic segmentation. The key innovation lies in
a two-stage training strategy. In the first stage, we pre-train the model on
partial scenes that provide dense semantic information but relatively simple
geometry. These partial point clouds are derived from multi-view RGB-D inputs
via pixel-wise depth projection. To enable open-vocabulary learning, we
leverage a multi-modal large language model (MLLM) and a 2D segmentation
foundation model to generate open-vocabulary labels for each viewpoint,
offering rich and aligned supervision. An auxiliary inter-frame consistency
module is introduced to enforce feature consistency across varying viewpoints
and enhance spatial understanding. In the second stage, we fine-tune the model
on complete scene-level point clouds, which are sparser and structurally more
complex. We aggregate the partial vocabularies associated with each scene and
generate pseudo labels using the pre-trained model, effectively bridging the
semantic gap between dense partial observations and large-scale 3D
environments. Extensive experiments on ScanNet, ScanNet200, and S3DIS
benchmarks demonstrate that PGOV3D achieves competitive performance in
open-vocabulary 3D semantic segmentation.

</details>


### [299] [AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention](https://arxiv.org/abs/2506.23611)
*Ziao Liu,Zhenjia Li,Yifeng Shi,Xiangang Li*

Main category: cs.CV

TL;DR: AttentionGS introduces structural attention for 3D Gaussian Splatting (3DGS), eliminating reliance on high-quality initial point clouds and improving 3D reconstruction and rendering.


<details>
  <summary>Details</summary>
Motivation: Improve 3DGS's applicability by addressing its dependence on high-quality point clouds and overcoming limitations in scenarios like texture-deficient or constrained-view.

Method: Uses geometric and texture attention during different training stages for structural recovery and detail refinement, alongside opacity-weighted gradients for Gaussian densification.

Result: AttentionGS enhances reconstruction performance and rendering quality, surpassing state-of-the-art methods in benchmarks, especially with unreliable point cloud initialization.

Conclusion: AttentionGS offers a robust and flexible approach to 3D Gaussian Splatting, enabling direct 3D reconstruction from random initialization in varied applications.

Abstract: 3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance
Fields (NeRF), excelling in complex scene reconstruction and efficient
rendering. However, it relies on high-quality point clouds from
Structure-from-Motion (SfM), limiting its applicability. SfM also fails in
texture-deficient or constrained-view scenarios, causing severe degradation in
3DGS reconstruction. To address this limitation, we propose AttentionGS, a
novel framework that eliminates the dependency on high-quality initial point
clouds by leveraging structural attention for direct 3D reconstruction from
randomly initialization. In the early training stage, we introduce geometric
attention to rapidly recover the global scene structure. As training
progresses, we incorporate texture attention to refine fine-grained details and
enhance rendering quality. Furthermore, we employ opacity-weighted gradients to
guide Gaussian densification, leading to improved surface reconstruction.
Extensive experiments on multiple benchmark datasets demonstrate that
AttentionGS significantly outperforms state-of-the-art methods, particularly in
scenarios where point cloud initialization is unreliable. Our approach paves
the way for more robust and flexible 3D Gaussian Splatting in real-world
applications.

</details>


### [300] [TurboVSR: Fantastic Video Upscalers and Where to Find Them](https://arxiv.org/abs/2506.23618)
*Zhongdao Wang,Guodongfang Zhao,Jingjing Ren,Bailan Feng,Shifeng Zhang,Wenbo Li*

Main category: cs.CV

TL;DR: This paper introduces TurboVSR, an ultra-efficient video super-resolution model that significantly reduces processing time while maintaining high performance, achieving speeds 100+ times faster than current methods.


<details>
  <summary>Details</summary>
Motivation: While diffusion-based generative models excel in video super-resolution tasks, their computational efficiency remains a barrier, requiring extensive time even for short videos.

Method: The authors developed TurboVSR utilizing three main components: (1) a highly compressive autoencoder; (2) factorized conditioning for gradual super-resolution; (3) shortcut modeling in the diffusion process to reduce sampling steps.

Result: TurboVSR achieved performance comparable to state-of-the-art VSR techniques, processing videos 100+ times faster and supporting resolution beyond 1080p, including 4K image super-resolution.

Conclusion: The paper demonstrates that TurboVSR effectively mitigates computational inefficiency in video super-resolution, offering a practical solution with potential for high-resolution applications.

Abstract: Diffusion-based generative models have demonstrated exceptional promise in
the video super-resolution (VSR) task, achieving a substantial advancement in
detail generation relative to prior methods. However, these approaches face
significant computational efficiency challenges. For instance, current
techniques may require tens of minutes to super-resolve a mere 2-second, 1080p
video. In this paper, we present TurboVSR, an ultra-efficient diffusion-based
video super-resolution model. Our core design comprises three key aspects: (1)
We employ an autoencoder with a high compression ratio of 32$\times$32$\times$8
to reduce the number of tokens. (2) Highly compressed latents pose substantial
challenges for training. We introduce factorized conditioning to mitigate the
learning complexity: we first learn to super-resolve the initial frame;
subsequently, we condition the super-resolution of the remaining frames on the
high-resolution initial frame and the low-resolution subsequent frames. (3) We
convert the pre-trained diffusion model to a shortcut model to enable fewer
sampling steps, further accelerating inference. As a result, TurboVSR performs
on par with state-of-the-art VSR methods, while being 100+ times faster, taking
only 7 seconds to process a 2-second long 1080p video. TurboVSR also supports
image resolution by considering image as a one-frame video. Our efficient
design makes SR beyond 1080p possible, results on 4K (3648$\times$2048) image
SR show surprising fine details.

</details>


### [301] [Revisiting Audio-Visual Segmentation with Vision-Centric Transformer](https://arxiv.org/abs/2506.23623)
*Shaofei Huang,Rui Ling,Tianrui Hui,Hongyu Li,Xu Zhou,Shifeng Zhang,Si Liu,Richang Hong,Meng Wang*

Main category: cs.CV

TL;DR: This paper proposes a Vision-Centric Transformer (VCT) for Audio-Visual Segmentation (AVS), addressing issues in prior audio-centric methods and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing AVS methods use an audio-centric Transformer architecture and face challenges like ambiguity in audio perception and loss of visual detail.

Method: The authors propose a Vision-Centric Transformer (VCT) that uses vision-derived queries to fetch audio and visual information iteratively. They also introduce a Prototype Prompted Query Generation (PPQG) module for generating semantically and visually rich queries.

Result: The VCT framework sets new state-of-the-art performances on three AVSBench dataset subsets.

Conclusion: The VCT framework effectively addresses limitations in AVS by enhancing query generation and increasing segmentation accuracy with a vision-centric approach.

Abstract: Audio-Visual Segmentation (AVS) aims to segment sound-producing objects in
video frames based on the associated audio signal. Prevailing AVS methods
typically adopt an audio-centric Transformer architecture, where object queries
are derived from audio features. However, audio-centric Transformers suffer
from two limitations: perception ambiguity caused by the mixed nature of audio,
and weakened dense prediction ability due to visual detail loss. To address
these limitations, we propose a new Vision-Centric Transformer (VCT) framework
that leverages vision-derived queries to iteratively fetch corresponding audio
and visual information, enabling queries to better distinguish between
different sounding objects from mixed audio and accurately delineate their
contours. Additionally, we also introduce a Prototype Prompted Query Generation
(PPQG) module within our VCT framework to generate vision-derived queries that
are both semantically aware and visually rich through audio prototype prompting
and pixel context grouping, facilitating audio-visual information aggregation.
Extensive experiments demonstrate that our VCT framework achieves new
state-of-the-art performances on three subsets of the AVSBench dataset. The
code is available at https://github.com/spyflying/VCT_AVS.

</details>


### [302] [Brain Tumor Detection through Thermal Imaging and MobileNET](https://arxiv.org/abs/2506.23627)
*Roham Maiti,Debasmita Bhoumik*

Main category: cs.CV

TL;DR: The paper proposes an efficient brain tumor detection model using the MobileNET framework, achieving a high accuracy of 98.5% with minimal computing resources.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in traditional and classical ML methods for brain tumor detection, such as high costs, computational demands, large datasets, and reliance on specialized expertise.

Method: The study employs the MobileNET model combined with image processing techniques to detect and classify brain tumors efficiently and accurately with reduced computation time and resource usage.

Result: The model achieved an impressive average accuracy of 98.5%, demonstrating high effectiveness in brain tumor detection.

Conclusion: The research showcases the potential of the MobileNET model as a viable solution for brain tumor detection, offering reliability, efficiency, and accessibility while using fewer resources and processing time.

Abstract: Brain plays a crucial role in regulating body functions and cognitive
processes, with brain tumors posing significant risks to human health. Precise
and prompt detection is a key factor in proper treatment and better patient
outcomes. Traditional methods for detecting brain tumors, that include
biopsies, MRI, and CT scans often face challenges due to their high costs and
the need for specialized medical expertise. Recent developments in machine
learning (ML) and deep learning (DL) has exhibited strong capabilities in
automating the identification and categorization of brain tumors from medical
images, especially MRI scans. However, these classical ML models have
limitations, such as high computational demands, the need for large datasets,
and long training times, which hinder their accessibility and efficiency. Our
research uses MobileNET model for efficient detection of these tumors. The
novelty of this project lies in building an accurate tumor detection model
which use less computing re-sources and runs in less time followed by efficient
decision making through the use of image processing technique for accurate
results. The suggested method attained an average accuracy of 98.5%.

</details>


### [303] [Blending Concepts with Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.23630)
*Lorenzo Olearo,Giorgio Longari,Alessandro Raganato,Rafael Peñaloza,Simone Melzi*

Main category: cs.CV

TL;DR: Diffusion models can creatively merge distinct visual concepts into single images zero-shot, but outcomes depend on input details.


<details>
  <summary>Details</summary>
Motivation: To explore whether diffusion models can blend distinct textual concepts into new coherent visual entities without additional training.

Method: The paper tested four blending methods that leverage different diffusion pipeline aspects to merge concepts, evaluated across diverse categories.

Result: The methods demonstrated creative blending capabilities; no single method excelled universally, with results influenced by factors like input order and conceptual distance.

Conclusion: Diffusion models showcase significant compositional abilities, though the blending results depend on nuanced input variations.

Abstract: Diffusion models have dramatically advanced text-to-image generation in
recent years, translating abstract concepts into high-fidelity images with
remarkable ease. In this work, we examine whether they can also blend distinct
concepts, ranging from concrete objects to intangible ideas, into coherent new
visual entities under a zero-shot framework. Specifically, concept blending
merges the key attributes of multiple concepts (expressed as textual prompts)
into a single, novel image that captures the essence of each concept. We
investigate four blending methods, each exploiting different aspects of the
diffusion pipeline (e.g., prompt scheduling, embedding interpolation, or
layer-wise conditioning). Through systematic experimentation across diverse
concept categories, such as merging concrete concepts, synthesizing compound
words, transferring artistic styles, and blending architectural landmarks, we
show that modern diffusion models indeed exhibit creative blending capabilities
without further training or fine-tuning. Our extensive user study, involving
100 participants, reveals that no single approach dominates in all scenarios:
each blending technique excels under certain conditions, with factors like
prompt ordering, conceptual distance, and random seed affecting the outcome.
These findings highlight the remarkable compositional potential of diffusion
models while exposing their sensitivity to seemingly minor input variations.

</details>


### [304] [Unified Multimodal Understanding via Byte-Pair Visual Encoding](https://arxiv.org/abs/2506.23639)
*Wanpeng Zhang,Yicheng Feng,Hao Luo,Yijiang Li,Zihao Yue,Sipeng Zheng,Zongqing Lu*

Main category: cs.CV

TL;DR: The paper proposes a novel method for multimodal large language models (MLLMs), efficiently aligning vision and language modalities using byte-pair encoding for visual tokens.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of effectively aligning different modalities in multimodal models for better vision-language understanding.

Method: The approach involves applying byte-pair encoding to visual tokens, priority-guided encoding considering frequency and spatial consistency, and a multi-stage curriculum-driven training scheme.

Result: Improved performance is demonstrated across diverse vision-language tasks, enhancing cross-modal relationships and reasoning capabilities.

Conclusion: The study advances multimodal foundation models by bridging visual and textual representation gaps, promoting more capable and efficient systems.

Abstract: Multimodal large language models (MLLMs) have made significant progress in
vision-language understanding, yet effectively aligning different modalities
remains a fundamental challenge. We present a framework that unifies multimodal
understanding by applying byte-pair encoding to visual tokens. Unlike
conventional approaches that rely on modality-specific encoders, our method
directly incorporates structural information into visual tokens, mirroring
successful tokenization strategies in text-only language models. We introduce a
priority-guided encoding scheme that considers both frequency and spatial
consistency, coupled with a multi-stage training procedure based on
curriculum-driven data composition. These enhancements enable the transformer
model to better capture cross-modal relationships and reason with visual
information. Comprehensive experiments demonstrate improved performance across
diverse vision-language tasks. By bridging the gap between visual and textual
representations, our approach contributes to the advancement of more capable
and efficient multimodal foundation models.

</details>


### [305] [VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation](https://arxiv.org/abs/2506.23641)
*Peng Huang,Junhu Fu,Bowen Guo,Zeju Li,Yuanyuan Wang,Yi Guo*

Main category: cs.CV

TL;DR: This paper proposes VAP-Diffusion, a framework that utilizes multi-modal large language models and a prototype condition mechanism to enhance the quality and diversity of medical image generation with limited attribute descriptions.


<details>
  <summary>Details</summary>
Motivation: Producing realistic and diverse medical images requires detailed attribute information (e.g., shape, size, texture) beyond basic labels, but such detailed descriptions are often unavailable.

Method: The authors use external knowledge from pre-trained Multi-modal Large Language Models (MLLMs) to derive attribute descriptions via custom Chain-of-Thought prompts. These descriptions are incorporated during training, stored by category, and retrieved at testing. A Prototype Condition Mechanism ensures the model handles unseen combinations of descriptions for robustness.

Result: Experiments conducted on common medical imaging datasets, including dermatologic and chest X-ray images, show that the proposed approach improves image quality and diversity in generation tasks.

Conclusion: The VAP-Diffusion framework successfully leverages external knowledge from MLLMs and integrates mechanisms to generate high-quality and realistic medical images even with limited attribute information.

Abstract: As the appearance of medical images is influenced by multiple underlying
factors, generative models require rich attribute information beyond labels to
produce realistic and diverse images. For instance, generating an image of skin
lesion with specific patterns demands descriptions that go beyond diagnosis,
such as shape, size, texture, and color. However, such detailed descriptions
are not always accessible. To address this, we explore a framework, termed
Visual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from
pre-trained Multi-modal Large Language Models (MLLMs) to improve the quality
and diversity of medical image generation. First, to derive descriptions from
MLLMs without hallucination, we design a series of prompts following
Chain-of-Thoughts for common medical imaging tasks, including dermatologic,
colorectal, and chest X-ray images. Generated descriptions are utilized during
training and stored across different categories. During testing, descriptions
are randomly retrieved from the corresponding category for inference. Moreover,
to make the generator robust to unseen combination of descriptions at the test
time, we propose a Prototype Condition Mechanism that restricts test embeddings
to be similar to those from training. Experiments on three common types of
medical imaging across four datasets verify the effectiveness of VAP-Diffusion.

</details>


### [306] [MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis](https://arxiv.org/abs/2506.23648)
*Zhe Liu,Yuhao Huang,Lian Liu,Chengrui Zhang,Haotian Lin,Tong Han,Zhiyuan Zhu,Yanlin Chen,Yuerui Chen,Dong Ni,Zhongshan Gou,Xin Yang*

Main category: cs.CV

TL;DR: The paper introduces an automated model (MReg) for diagnosing mitral regurgitation (MR) using echocardiography videos, which outperforms other methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the user-dependence, accuracy, and alignment issues in current intelligent methods for diagnosing mitral regurgitation.

Method: Developed MReg, an automated MR diagnosis model, using a regression task, feature amplification, and category-level feature extraction inspired by Mixture-of-Experts.

Result: MReg showed superior performance for MR diagnosis compared to weakly supervised and classification methods on a dataset of 1868 cases.

Conclusion: The proposed MReg model effectively captures diagnostic features for accurate and interpretable MR grading, demonstrating clinical applicability.

Abstract: Color Doppler echocardiography is a crucial tool for diagnosing mitral
regurgitation (MR). Recent studies have explored intelligent methods for MR
diagnosis to minimize user dependence and improve accuracy. However, these
approaches often fail to align with clinical workflow and may lead to
suboptimal accuracy and interpretability. In this study, we introduce an
automated MR diagnosis model (MReg) developed on the 4-chamber cardiac color
Doppler echocardiography video (A4C-CDV). It follows comprehensive feature
mining strategies to detect MR and assess its severity, considering clinical
realities. Our contribution is threefold. First, we formulate the MR diagnosis
as a regression task to capture the continuity and ordinal relationships
between categories. Second, we design a feature selection and amplification
mechanism to imitate the sonographer's diagnostic logic for accurate MR
grading. Third, inspired by the Mixture-of-Experts concept, we introduce a
feature summary module to extract the category-level features, enhancing the
representational capacity for more accurate grading. We trained and evaluated
our proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases
with three graded regurgitation labels. Compared to other weakly supervised
video anomaly detection and supervised classification methods, MReg
demonstrated superior performance in MR diagnosis. Our code is available at:
https://github.com/cskdstz/MReg.

</details>


### [307] [Towards Markerless Intraoperative Tracking of Deformable Spine Tissue](https://arxiv.org/abs/2506.23657)
*Connor Daly,Elettra Marconi,Marco Riva,Jinendra Ekanayake,Daniel S. Elson,Ferdinando Rodriguez y Baena*

Main category: cs.CV

TL;DR: This paper introduces a clinical RGB-D dataset for spine surgery and develops SpineAlign for analyzing spine deformation between pre- and intraoperative stages, alongside a segmentation network and CorrespondNet framework.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and reduce complexity in orthopedic tissue tracking during surgery, as current marker-based technologies are time-consuming and invasive.

Method: Development of SpineAlign, a system leveraging RGB-D imaging for deformation tracking, creation of a clinical dataset, and designing a segmentation network and multi-task framework CorrespondNet.

Result: Successfully constructed the first real-world RGB-D dataset for spine surgery and demonstrated the utility of SpineAlign, the segmentation network, and CorrespondNet for key region prediction and spine state registration.

Conclusion: The research advances markerless tracking for orthopedic applications, making it feasible and clinically applicable for spine surgeries, potentially reducing operational complexities.

Abstract: Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is
a promising method with high translational potential. Unlike bone-mounted
tracking devices, markerless tracking can reduce operating time and complexity.
However, its use has been limited to cadaveric studies. This paper introduces
the first real-world clinical RGB-D dataset for spine surgery and develops
SpineAlign, a system for capturing deformation between preoperative and
intraoperative spine states. We also present an intraoperative segmentation
network trained on this data and introduce CorrespondNet, a multi-task
framework for predicting key regions for registration in both intraoperative
and preoperative scenes.

</details>


### [308] [On the Domain Robustness of Contrastive Vision-Language Models](https://arxiv.org/abs/2506.23663)
*Mario Koddenbrock,Rudolf Hoffmann,David Brodmann,Erik Rodner*

Main category: cs.CV

TL;DR: Deepbench introduces a framework to evaluate the domain-specific robustness of vision-language models, highlighting their variability and utilizing LLMs for generating realistic image corruptions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of diminishing model effectiveness in specialized domain shifts, where vision-language models perform poorly under unique conditions.

Method: The framework Deepbench uses large language models to create realistic, context-aware image corruptions specific to deployment domains, without relying on labeled data.

Result: Deepbench identifies substantial variability in robustness across vision-language models and architectures in six real-world domains.

Conclusion: Deepbench emphasizes the importance of domain-aware evaluation for vision-language models and is provided as open-source software for further research.

Abstract: In real-world vision-language applications, practitioners increasingly rely
on large, pretrained foundation models rather than custom-built solutions,
despite limited transparency regarding their training data and processes. While
these models achieve impressive performance on general benchmarks, their
effectiveness can decline notably under specialized domain shifts, such as
unique imaging conditions or environmental variations. In this work, we
introduce Deepbench, a framework designed to assess domain-specific robustness
of vision-language models (VLMs). Deepbench leverages a large language model
(LLM) to generate realistic, context-aware image corruptions tailored to
specific deployment domains without requiring labeled data. We evaluate a range
of contrastive vision-language architectures and architectural variants across
six real-world domains and observe substantial variability in robustness,
highlighting the need for targeted, domain-aware evaluation. Deepbench is
released as open-source software to support further research into domain-aware
robustness assessment.

</details>


### [309] [Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration](https://arxiv.org/abs/2506.23674)
*Dongyue Wu,Zilin Guo,Jialong Zuo,Nong Sang,Changxin Gao*

Main category: cs.CV

TL;DR: This paper proposes Partial Forward Blocking (PFB), a framework to accelerate training in machine learning by adaptively pruning unimportant samples during forward passes, resulting in faster training without loss of accuracy.


<details>
  <summary>Details</summary>
Motivation: The growing size of training datasets improves model generalization but escalates computational costs. Existing data pruning techniques aim to address this but require additional resources, creating inefficiencies.

Method: PFB assesses sample importance based on shallow-layer features of a model and prunes less important ones. The retained samples proceed to deeper layers, reducing computational demands while eliminating the need for gradients or proxy models. A probability density indicator and adaptive distribution estimation module prioritize rare samples dynamically.

Result: PFB achieved a 0.5% accuracy improvement and a 33% reduction in training time by pruning 40% of data on the ImageNet dataset.

Conclusion: PFB presents a resource-efficient method for accelerating training while improving performance, demonstrating the potential for practical applications in handling large datasets.

Abstract: The ever-growing size of training datasets enhances the generalization
capability of modern machine learning models but also incurs exorbitant
computational costs. Existing data pruning approaches aim to accelerate
training by removing those less important samples. However, they often rely on
gradients or proxy models, leading to prohibitive additional costs of gradient
back-propagation and proxy model training. In this paper, we propose Partial
Forward Blocking (PFB), a novel framework for lossless training acceleration.
The efficiency of PFB stems from its unique adaptive pruning pipeline: sample
importance is assessed based on features extracted from the shallow layers of
the target model. Less important samples are then pruned, allowing only the
retained ones to proceed with the subsequent forward pass and loss
back-propagation. This mechanism significantly reduces the computational
overhead of deep-layer forward passes and back-propagation for pruned samples,
while also eliminating the need for auxiliary backward computations and proxy
model training. Moreover, PFB introduces probability density as an indicator of
sample importance. Combined with an adaptive distribution estimation module,
our method dynamically prioritizes relatively rare samples, aligning with the
constantly evolving training state. Extensive experiments demonstrate the
significant superiority of PFB in performance and speed. On ImageNet, PFB
achieves a 0.5% accuracy improvement and 33% training time reduction with 40%
data pruned.

</details>


### [310] [Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation](https://arxiv.org/abs/2506.23675)
*Patrick Glandorf,Bodo Rosenhahn*

Main category: cs.CV

TL;DR: The paper proposes a new pruning method, P3B, that reduces Vision Transformer model parameters by up to 70% with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers are computationally expensive, making them unsuitable for resource-constrained hardware. Existing network pruning approaches underperform when applied in new data domains due to improper weight significance evaluation.

Method: The authors introduce P3B (Pruning by Block Benefit), a pruning method that calculates contribution at the block level for globally balanced parameter allocation. It ensures reactivation of late-converging blocks based on layer-specific keep ratios rather than freezing zero-mask elements.

Result: P3B achieves state-of-the-art pruning performance, maintaining high accuracy even in scenarios with 70% parameter reductions, only experiencing a 0.64% accuracy drop.

Conclusion: The P3B method effectively reduces computational costs of Vision Transformers while preserving performance, making it suitable for transfer learning and resource-limited applications.

Abstract: Vision Transformer have set new benchmarks in several tasks, but these models
come with the lack of high computational costs which makes them impractical for
resource limited hardware. Network pruning reduces the computational complexity
by removing less important operations while maintaining performance. However,
pruning a model on an unseen data domain, leads to a misevaluation of weight
significance, resulting in suboptimal resource assignment. In this work, we
find that task-sensitive layers initially fail to improve the feature
representation on downstream tasks, leading to performance loss for early
pruning decisions. To address this problem, we introduce Pruning by Block
Benefit (P3B), a pruning method that utilizes the relative contribution on
block level to globally assign parameter resources. P3B identifies low-impact
components to reduce parameter allocation while preserving critical ones.
Classical pruning mask optimization struggles to reactivate zero-mask-elements.
In contrast, P3B sets a layerwise keep ratio based on global performance
metrics, ensuring the reactivation of late-converging blocks. We show in
extensive experiments that P3B is a state of the art pruning method with most
noticeable gains in transfer learning tasks. Notably, P3B is able to conserve
high performance, even in high sparsity regimes of 70% parameter reduction
while only losing 0.64% in accuracy.

</details>


### [311] [A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement](https://arxiv.org/abs/2506.23676)
*Gaozheng Pei,Ke Ma,Dongpeng Zhang,Chengzhi Sun,Qianqian Xu,Qingming Huang*

Main category: cs.CV

TL;DR: Diffusion-based adversarial example generation methods are widely used for image editing but struggle with generalization beyond image classification tasks. This paper introduces a unified framework to enhance transferability in such methods, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of diffusion-based adversarial methods that struggle with generalizing to non-classification tasks, such as Deepfake detection.

Method: Proposes a unified framework that integrates traditional transferability enhancement strategies into diffusion-based adversarial example generation for image editing.

Result: The proposed method demonstrated effectiveness by winning first place in an adversarial attack competition related to Deepfake detection.

Conclusion: The framework successfully extends the utility of diffusion-based adversarial example generation, proving its effectiveness across broader tasks including Deepfake detection.

Abstract: Due to their powerful image generation capabilities, diffusion-based
adversarial example generation methods through image editing are rapidly
gaining popularity. However, due to reliance on the discriminative capability
of the diffusion model, these diffusion-based methods often struggle to
generalize beyond conventional image classification tasks, such as in Deepfake
detection. Moreover, traditional strategies for enhancing adversarial example
transferability are challenging to adapt to these methods. To address these
challenges, we propose a unified framework that seamlessly incorporates
traditional transferability enhancement strategies into diffusion model-based
adversarial example generation via image editing, enabling their application
across a wider range of downstream tasks. Our method won first place in the
"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of
AI-Generated Media" competition at ACM MM25, which validates the effectiveness
of our approach.

</details>


### [312] [SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation](https://arxiv.org/abs/2506.23690)
*Shuai Tan,Biao Gong,Yujie Wei,Shiwei Zhang,Zhuoxin Liu,Dandan Zheng,Jingdong Chen,Yan Wang,Hao Ouyang,Kecheng Zheng,Yujun Shen*

Main category: cs.CV

TL;DR: SynMotion proposes a novel video motion-generation model combining semantic guidance and visual adaptation to better capture spatio-temporal patterns in video motion.


<details>
  <summary>Details</summary>
Motivation: Existing methods for video motion customization fall short in capturing the complex interplay between semantic-level alignment and visual complexity of motion, leading to either semantic confusion or losing motion fidelity.

Method: The SynMotion framework uses a dual-embedding mechanism to separate subject and motion representations combined with motion adapters for visual enhancement. It also leverages an embedding-specific training strategy with the SPV dataset to improve motion specificity and generalization.

Result: SynMotion achieved superior performance over existing baselines across T2V (Text-to-Video) and I2V (Image-to-Video) generation scenarios, validated with the custom MotionBench benchmark.

Conclusion: Jointly optimizing semantic and visual components using SynMotion leads to enhanced motion fidelity, temporal coherence, and better adaptability for diverse subjects in video generation.

Abstract: Diffusion-based video motion customization facilitates the acquisition of
human motion representations from a few video samples, while achieving
arbitrary subjects transfer through precise textual conditioning. Existing
approaches often rely on semantic-level alignment, expecting the model to learn
new motion concepts and combine them with other entities (e.g., ''cats'' or
''dogs'') to produce visually appealing results. However, video data involve
complex spatio-temporal patterns, and focusing solely on semantics cause the
model to overlook the visual complexity of motion. Conversely, tuning only the
visual representation leads to semantic confusion in representing the intended
action. To address these limitations, we propose SynMotion, a new
motion-customized video generation model that jointly leverages semantic
guidance and visual adaptation. At the semantic level, we introduce the
dual-embedding semantic comprehension mechanism which disentangles subject and
motion representations, allowing the model to learn customized motion features
while preserving its generative capabilities for diverse subjects. At the
visual level, we integrate parameter-efficient motion adapters into a
pre-trained video generation model to enhance motion fidelity and temporal
coherence. Furthermore, we introduce a new embedding-specific training strategy
which \textbf{alternately optimizes} subject and motion embeddings, supported
by the manually constructed Subject Prior Video (SPV) training dataset. This
strategy promotes motion specificity while preserving generalization across
diverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark
with diverse motion patterns. Experimental results across both T2V and I2V
settings demonstrate that \method outperforms existing baselines. Project page:
https://lucaria-academy.github.io/SynMotion/

</details>


### [313] [Single Image Test-Time Adaptation via Multi-View Co-Training](https://arxiv.org/abs/2506.23705)
*Smriti Joshi,Richard Osuala,Lidia Garrucho,Kaisar Kushibar,Dimitri Kessler,Oliver Diaz,Karim Lekadir*

Main category: cs.CV

TL;DR: This study introduces a patch-based multi-view co-training method enabling single-image test-time adaptation for medical image segmentation with limited data.


<details>
  <summary>Details</summary>
Motivation: To address challenges in medical image test-time adaptation where current methods require large datasets and mostly focus on 2D images, neglecting 3D volumetric data.

Method: Developed a patch-based multi-view co-training approach using uncertainty-guided self-training for spatial consistency to perform effective volumetric segmentation using a single test-time image.

Result: Achieved performance comparable to the supervised benchmark and outperformed state-of-the-art methods by an average Dice Similarity Coefficient of 3.75%, validated on breast MRI datasets for tumor segmentation.

Conclusion: This method demonstrates the efficacy of single-image test-time adaptation in medical imaging, bridging key gaps and providing an accessible solution integrated with nnUNet.

Abstract: Test-time adaptation enables a trained model to adjust to a new domain during
inference, making it particularly valuable in clinical settings where such
on-the-fly adaptation is required. However, existing techniques depend on large
target domain datasets, which are often impractical and unavailable in medical
scenarios that demand per-patient, real-time inference. Moreover, current
methods commonly focus on two-dimensional images, failing to leverage the
volumetric richness of medical imaging data. Bridging this gap, we propose a
Patch-Based Multi-View Co-Training method for Single Image Test-Time
adaptation. Our method enforces feature and prediction consistency through
uncertainty-guided self-training, enabling effective volumetric segmentation in
the target domain with only a single test-time image. Validated on three
publicly available breast magnetic resonance imaging datasets for tumor
segmentation, our method achieves performance close to the upper bound
supervised benchmark while also outperforming all existing state-of-the-art
methods, on average by a Dice Similarity Coefficient of 3.75%. We publicly
share our accessible codebase, readily integrable with the popular nnUNet
framework, at https://github.com/smriti-joshi/muvi.git.

</details>


### [314] [Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion](https://arxiv.org/abs/2506.23711)
*Haoyang Chen,Dongfang Sun,Caoyuan Ma,Shiqin Wang,Kewei Zhang,Zheng Wang,Zhixiang Wang*

Main category: cs.CV

TL;DR: The paper introduces 'Subjective Camera,' a method to reconstruct photorealistic images from mental impressions using verbal descriptions and sequential sketch input.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing methods which face challenges like user-specific input biases, the large gap between sketches and 3D priors, and performance sensitivity to sketch quality.

Method: The proposed method uses text-reward optimization, sequence-aware disentangled generation, and latent optimization to process subjective sketches and descriptions while avoiding resource-heavy model adaptations.

Result: The results showcase state-of-the-art performance in preserving semantic and spatial coherence across diverse datasets, even with rough sketch inputs.

Conclusion: The framework effectively integrates verbal and sketch inputs to bridge the gap between subjective expectations and photorealistic outputs, requiring no specialized artistic expertise.

Abstract: We propose Subjective Camera, a human-as-imaging-device paradigm that
reconstructs real-world scenes from mental impressions through synergistic use
of verbal descriptions and progressive rough sketches. This approach overcomes
dual limitations of language ambiguity and sketch abstraction by treating the
user's drawing sequence as priors, effectively translating subjective
perceptual expectations into photorealistic images.
  Existing approaches face three fundamental barriers: (1) user-specific
subjective input biases, (2) huge modality gap between planar sketch and 3D
priors in diffusion, and (3) sketch quality-sensitive performance degradation.
Current solutions either demand resource-intensive model adaptation or impose
impractical requirements on sketch precision.
  Our framework addresses these challenges through concept-sequential
generation. (1) We establish robust appearance priors through text-reward
optimization, and then implement sequence-aware disentangled generation that
processes concepts in sketching order; these steps accommodate user-specific
subjective expectation in a train-free way. (2) We employ latent optimization
that effectively bridges the modality gap between planar sketches and 3D priors
in diffusion. (3) Our hierarchical reward-guided framework enables the use of
rough sketches without demanding artistic expertise. Comprehensive evaluation
across diverse datasets demonstrates that our approach achieves
state-of-the-art performance in maintaining both semantic and spatial
coherence.

</details>


### [315] [When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation](https://arxiv.org/abs/2506.23724)
*Chang'an Yi,Xiaohui Deng,Guohao Chen,Yan Zhou,Qinghua Lu,Shuaicheng Niu*

Main category: cs.CV

TL;DR: The paper proposes COCA, a framework leveraging cross-model co-learning to enhance Test-Time Adaptation (TTA) by integrating complementary knowledge from multiple models.


<details>
  <summary>Details</summary>
Motivation: Existing TTA methods focus on single-model adaptation, limiting potential performance boosts from inter-model knowledge sharing.

Method: COCA includes two main strategies: co-adaptation to share complementary knowledge among models and self-adaptation to strengthen each model's unique capabilities during TTA.

Result: COCA significantly improves adaptation accuracy across various model sizes, achieving, for instance, a jump from 51.7% to 64.5% in ViT-Base's accuracy on ImageNet-C with guidance from MobileViT.

Conclusion: Cross-model learning enhances domain adaptation in TTA, with COCA seamlessly boosting performance across diverse model architectures as a plug-and-play module.

Abstract: Test-time Adaptation (TTA) adapts a given model to testing domain data with
potential domain shifts through online unsupervised learning, yielding
impressive performance. However, to date, existing TTA methods primarily focus
on single-model adaptation. In this work, we investigate an intriguing
question: how does cross-model knowledge influence the TTA process? Our
findings reveal that, in TTA's unsupervised online setting, each model can
provide complementary, confident knowledge to the others, even when there are
substantial differences in model size. For instance, a smaller model like
MobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base
(86.6M parameters). In light of this, we propose COCA, a Cross-Model
Co-Learning framework for TTA, which mainly consists of two main strategies. 1)
Co-adaptation adaptively integrates complementary knowledge from other models
throughout the TTA process, reducing individual model biases. 2)
Self-adaptation enhances each model's unique strengths via unsupervised
learning, enabling diverse adaptation to the target domain. Extensive
experiments show that COCA, which can also serve as a plug-and-play module,
significantly boosts existing SOTAs, on models with various sizes--including
ResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,
with Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy
on ImageNet-C from 51.7% to 64.5%. The code is publicly available at
https://github.com/ycarobot/COCA.

</details>


### [316] [Proteus-ID: ID-Consistent and Motion-Coherent Video Customization](https://arxiv.org/abs/2506.23729)
*Guiyu Zhang,Chen Shi,Zijian Jiang,Xunzhi Xiang,Jingjing Qian,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: This paper introduces Proteus-ID, a framework addressing video identity customization utilizing diffusion methods, excelling in identity consistency and motion realism.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve video identity customization, overcoming issues of identity consistency and motion fluidity in personalized video generation.

Method: Three key innovations: Multimodal Identity Fusion (MIF) module, Time-Aware Identity Injection (TAII) mechanism, and Adaptive Motion Learning (AML) strategy within a diffusion-based framework.

Result: Proteus-ID demonstrates superior performance in maintaining identity accuracy, aligning with text prompts, and producing fluid motions, supported by a new benchmark dataset.

Conclusion: Proteus-ID establishes a new standard in video identity customization through its robust methodological contributions, with publicly available resources for further research.

Abstract: Video identity customization seeks to synthesize realistic, temporally
coherent videos of a specific subject, given a single reference image and a
text prompt. This task presents two core challenges: (1) maintaining identity
consistency while aligning with the described appearance and actions, and (2)
generating natural, fluid motion without unrealistic stiffness. To address
these challenges, we introduce Proteus-ID, a novel diffusion-based framework
for identity-consistent and motion-coherent video customization. First, we
propose a Multimodal Identity Fusion (MIF) module that unifies visual and
textual cues into a joint identity representation using a Q-Former, providing
coherent guidance to the diffusion model and eliminating modality imbalance.
Second, we present a Time-Aware Identity Injection (TAII) mechanism that
dynamically modulates identity conditioning across denoising steps, improving
fine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a
self-supervised strategy that reweights the training loss based on
optical-flow-derived motion heatmaps, enhancing motion realism without
requiring additional inputs. To support this task, we construct Proteus-Bench,
a high-quality dataset comprising 200K curated clips for training and 150
individuals from diverse professions and ethnicities for evaluation. Extensive
experiments demonstrate that Proteus-ID outperforms prior methods in identity
preservation, text alignment, and motion quality, establishing a new benchmark
for video identity customization. Codes and data are publicly available at
https://grenoble-zhang.github.io/Proteus-ID/.

</details>


### [317] [Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?](https://arxiv.org/abs/2506.23751)
*Annika Mütze,Sadia Ilyas,Christian Dörpelkus,Matthias Rottmann*

Main category: cs.CV

TL;DR: The study analyzes open-vocabulary object detectors' limitations by evaluating them on synthetically generated datasets. It identifies their dependencies on object location over semantics.


<details>
  <summary>Details</summary>
Motivation: To uncover the limitations and systematic failure modes of open-vocabulary object detectors, especially in safety-critical applications, using synthetically generated image data.

Method: Two automated pipelines are created using Stable Diffusion to inpaint diverse, unusual objects, incorporating semantics sampled from WordNet and ChatGPT. These synthetic datasets are used to evaluate and compare open-vocabulary and classical object detectors.

Result: Open-vocabulary detectors are found to rely heavily on object location, often overlooking objects, despite high semantic diversity in the synthetic data. These models show limitations when systematically challenged.

Conclusion: Synthetic data generation can systematically expose weaknesses in open-vocabulary models, particularly their bias towards object location over semantics. This approach offers valuable guidance for improving these models through better data acquisition strategies.

Abstract: Open-vocabulary object detectors such as Grounding DINO are trained on vast
and diverse data, achieving remarkable performance on challenging datasets. Due
to that, it is unclear where to find their limitations, which is of major
concern when using in safety-critical applications. Real-world data does not
provide sufficient control, required for a rigorous evaluation of model
generalization. In contrast, synthetically generated data allows to
systematically explore the boundaries of model competence/generalization. In
this work, we address two research questions: 1) Can we challenge
open-vocabulary object detectors with generated image content? 2) Can we find
systematic failure modes of those models? To address these questions, we design
two automated pipelines using stable diffusion to inpaint unusual objects with
high diversity in semantics, by sampling multiple substantives from WordNet and
ChatGPT. On the synthetically generated data, we evaluate and compare multiple
open-vocabulary object detectors as well as a classical object detector. The
synthetic data is derived from two real-world datasets, namely LostAndFound, a
challenging out-of-distribution (OOD) detection benchmark, and the NuImages
dataset. Our results indicate that inpainting can challenge open-vocabulary
object detectors in terms of overlooking objects. Additionally, we find a
strong dependence of open-vocabulary models on object location, rather than on
object semantics. This provides a systematic approach to challenge
open-vocabulary models and gives valuable insights on how data could be
acquired to effectively improve these models.

</details>


### [318] [Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking](https://arxiv.org/abs/2506.23783)
*Shiao Wang,Ju Huang,Qingchuan Ma,Jinfeng Gao,Chunyi Xu,Xiao Wang,Lan Chen,Bo Jiang*

Main category: cs.CV

TL;DR: This paper introduces Mamba-FETrack V2, a lightweight RGB-Event object tracking framework that replaces high-complexity Vision Transformers with the efficient Vision Mamba network for feature extraction and fusion.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal tracking algorithms rely on computationally intensive Vision Transformers, leading to high complexity and limiting cross-modal interactions.

Method: The authors propose a Prompt Generator to create learnable modality-specific prompts and use the Vision Mamba network for feature extraction, cross-modal interaction, and fusion. The outputs are used for accurate target localization.

Result: Experimental evaluations on RGB-Event tracking benchmarks show that the proposed framework outperforms existing solutions in both efficiency and accuracy.

Conclusion: Mamba-FETrack V2 offers a superior, resource-efficient alternative for multimodal object tracking, integrating features dynamically with lightweight architecture. Source code and pre-trained models will be provided publicly.

Abstract: Combining traditional RGB cameras with bio-inspired event cameras for robust
object tracking has garnered increasing attention in recent years. However,
most existing multimodal tracking algorithms depend heavily on high-complexity
Vision Transformer architectures for feature extraction and fusion across
modalities. This not only leads to substantial computational overhead but also
limits the effectiveness of cross-modal interactions. In this paper, we propose
an efficient RGB-Event object tracking framework based on the linear-complexity
Vision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a
lightweight Prompt Generator that utilizes embedded features from each
modality, together with a shared prompt pool, to dynamically generate
modality-specific learnable prompt vectors. These prompts, along with the
modality-specific embedded features, are then fed into a Vision Mamba-based
FEMamba backbone, which facilitates prompt-guided feature extraction,
cross-modal interaction, and fusion in a unified manner. Finally, the fused
representations are passed to the tracking head for accurate target
localization. Extensive experimental evaluations on multiple RGB-Event tracking
benchmarks, including short-term COESOT dataset and long-term datasets, i.e.,
FE108 and FELT V2, demonstrate the superior performance and efficiency of the
proposed tracking framework. The source code and pre-trained models will be
released on https://github.com/Event-AHU/Mamba_FETrack

</details>


### [319] [Visual Textualization for Image Prompted Object Detection](https://arxiv.org/abs/2506.23785)
*Yongjian Wu,Yang Zhou,Jiya Saiyin,Bingzheng Wei,Yan Xu*

Main category: cs.CV

TL;DR: This paper proposes an image-driven object detection method, VisTex-OVLM, which converts visual exemplars into a textual format to enhance object detection in rare categories.


<details>
  <summary>Details</summary>
Motivation: Enhanced detection of rare categories has been challenging for Object-level Vision-Language Models due to the lack of textual descriptors and minimal representation in pre-training data.

Method: VisTex-OVLM uses multi-scale textualizing blocks and multi-stage fusion to transform visual exemplars into textualized tokens, which are processed alongside text prompts for better object detection.

Result: VisTex-OVLM achieves state-of-the-art performance in few-shot benchmarks (PASCAL VOC, MSCOCO) and outperforms across open-set datasets with minimal pre-training overlap.

Conclusion: VisTex-OVLM retains the original architecture of OVLM while significantly enhancing rare category detection, offering a strong solution for few-shot and open-set limitations.

Abstract: We propose VisTex-OVLM, a novel image prompted object detection method that
introduces visual textualization -- a process that projects a few visual
exemplars into the text feature space to enhance Object-level Vision-Language
Models' (OVLMs) capability in detecting rare categories that are difficult to
describe textually and nearly absent from their pre-training data, while
preserving their pre-trained object-text alignment. Specifically, VisTex-OVLM
leverages multi-scale textualizing blocks and a multi-stage fusion strategy to
integrate visual information from visual exemplars, generating textualized
visual tokens that effectively guide OVLMs alongside text prompts. Unlike
previous methods, our method maintains the original architecture of OVLM,
maintaining its generalization capabilities while enhancing performance in
few-shot settings. VisTex-OVLM demonstrates superior performance across
open-set datasets which have minimal overlap with OVLM's pre-training data and
achieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO.
The code will be released at https://github.com/WitGotFlg/VisTex-OVLM.

</details>


### [320] [Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors](https://arxiv.org/abs/2506.23801)
*Ce Wang,Wanjie Sun*

Main category: cs.CV

TL;DR: CRefDiff, a novel controllable diffusion model, addresses issues in reference-based super-resolution by integrating generative priors and adaptive fusion for accurate remote sensing image SR.


<details>
  <summary>Details</summary>
Motivation: Existing RefSR methods struggle with real-world complexities like sensor resolution gaps and land cover changes, resulting in poor performance.

Method: CRefDiff combines a pretrained Stable Diffusion model, a dual-branch fusion mechanism for local and global integration, and a strategy termed Better Start for accelerated inference.

Result: CRefDiff outperforms state-of-the-art RefSR techniques, according to metrics and downstream tasks analysis on the new Real-RefRSSRD dataset.

Conclusion: The proposed method enhances flexibility, accuracy, and efficiency in remote sensing image SR while enabling robust real-world applications.

Abstract: Super-resolution (SR) techniques can enhance the spatial resolution of remote
sensing images by utilizing low-resolution (LR) images to reconstruct
high-resolution (HR) images, enabling more efficient large-scale earth
observation applications. While single-image super-resolution (SISR) methods
have shown progress, reference-based super-resolution (RefSR) offers superior
performance by incorporating historical HR images alongside current LR
observations. However, existing RefSR methods struggle with real-world
complexities, such as cross-sensor resolution gap and significant land cover
changes, often leading to under-generation or over-reliance on reference image.
To address these challenges, we propose CRefDiff, a novel controllable
reference-based diffusion model for real-world remote sensing image SR. To
address the under-generation problem, CRefDiff is built upon the pretrained
Stable Diffusion model, leveraging its powerful generative prior to produce
accurate structures and textures. To mitigate over-reliance on the reference,
we introduce a dual-branch fusion mechanism that adaptively integrates both
local and global information from the reference image. Moreover, this novel
dual-branch design enables reference strength control during inference,
enhancing interactivity and flexibility of the model. Finally, a strategy named
Better Start is proposed to significantly reduce the number of denoising steps,
thereby accelerating the inference process. To support further research, we
introduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing
images, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land
cover changes and significant temporal gaps. Extensive experiments on
Real-RefRSSRD show that CRefDiff achieves state-of-the-art performance across
various metrics and improves downstream tasks such as scene classification and
semantic segmentation.

</details>


### [321] [Towards Initialization-free Calibrated Bundle Adjustment](https://arxiv.org/abs/2506.23808)
*Carl Olsson,Amanda Nilsson*

Main category: cs.CV

TL;DR: The paper proposes a method for reconstruction that uses camera calibration to achieve near-metric scene accuracy, integrating pairwise rotation estimates into initialization-free optimization frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for structure-from-motion (SfM) use pseudo Object Space Error (pOSE) but fall short in utilizing camera calibration, leading to projective reconstructions and requiring more data.

Method: The proposed method incorporates pairwise relative rotation estimates, which depend on camera calibration, into the pOSE framework, achieving similarity transformation-based reconstructions.

Result: Experimental evaluation demonstrates reliable optimization, global minimum convergence with high probability, and accurate near metric reconstructions.

Conclusion: The method succeeds in producing calibration-aware reconstructions with reduced data requirements, showing promise for initialization-free calibrated SfM.

Abstract: A recent series of works has shown that initialization-free BA can be
achieved using pseudo Object Space Error (pOSE) as a surrogate objective. The
initial reconstruction-step optimizes an objective where all terms are
projectively invariant and it cannot incorporate knowledge of the camera
calibration. As a result, the solution is only determined up to a projective
transformation of the scene and the process requires more data for successful
reconstruction.
  In contrast, we present a method that is able to use the known camera
calibration thereby producing near metric solutions, that is, reconstructions
that are accurate up to a similarity transformation. To achieve this we
introduce pairwise relative rotation estimates that carry information about
camera calibration. These are only invariant to similarity transformations,
thus encouraging solutions that preserve metric features of the real scene. Our
method can be seen as integrating rotation averaging into the pOSE framework
striving towards initialization-free calibrated SfM.
  Our experimental evaluation shows that we are able to reliably optimize our
objective, achieving convergence to the global minimum with high probability
from random starting solutions, resulting in accurate near metric
reconstructions.

</details>


### [322] [MadCLIP: Few-shot Medical Anomaly Detection with CLIP](https://arxiv.org/abs/2506.23810)
*Mahshid Shiri,Cigdem Beyan,Vittorio Murino*

Main category: cs.CV

TL;DR: The paper introduces a few-shot anomaly detection method using the pre-trained CLIP model for medical data, showcasing robust performance in both anomaly classification and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a more effective anomaly detection system for medical data without relying on synthetic data or memory banks, and to improve performance in both image-level classification and pixel-level segmentation.

Method: A dual-branch architecture leveraging learnable adapters in CLIP's vision encoder is combined with learnable text prompts for better semantic alignment. It also introduces SigLIP loss to manage image-text relationships effectively.

Result: The proposed method outperforms existing approaches in medical anomaly detection for classification and segmentation tasks across various datasets, including cross-dataset scenarios.

Conclusion: The approach demonstrates significant innovation by adapting the CLIP model to the medical domain without using synthetic data, and all components contribute to its success as confirmed via ablation studies.

Abstract: An innovative few-shot anomaly detection approach is presented, leveraging
the pre-trained CLIP model for medical data, and adapting it for both
image-level anomaly classification (AC) and pixel-level anomaly segmentation
(AS). A dual-branch design is proposed to separately capture normal and
abnormal features through learnable adapters in the CLIP vision encoder. To
improve semantic alignment, learnable text prompts are employed to link visual
features. Furthermore, SigLIP loss is applied to effectively handle the
many-to-one relationship between images and unpaired text prompts, showcasing
its adaptation in the medical field for the first time. Our approach is
validated on multiple modalities, demonstrating superior performance over
existing methods for AC and AS, in both same-dataset and cross-dataset
evaluations. Unlike prior work, it does not rely on synthetic data or memory
banks, and an ablation study confirms the contribution of each component. The
code is available at https://github.com/mahshid1998/MadCLIP.

</details>


### [323] [Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model](https://arxiv.org/abs/2506.23822)
*Shiming Chen,Bowen Duan,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CV

TL;DR: The paper introduces LaZSL, a locally-aligned vision-language model aimed at improving interpretability, accuracy, and domain generalization for zero-shot learning by aligning visual features with semantic attributes.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the lack of interpretability in existing large-scale vision-language models like CLIP, which compute predictions by analyzing whole images rather than discrete attributes.

Method: LaZSL uses optimal transport to align local visual features with semantic attributes based on pre-trained vision-language models, enabling interpretable similarity without additional training.

Result: Experiments reveal that LaZSL achieves better interpretability, accuracy, and domain generalization in zero-shot learning scenarios compared to existing methods.

Conclusion: LaZSL successfully addresses interpretability challenges in VLM-based zero-shot learning while boosting performance metrics, showing potential for effective real-world applications.

Abstract: Large-scale vision-language models (VLMs), such as CLIP, have achieved
remarkable success in zero-shot learning (ZSL) by leveraging large-scale
visual-text pair datasets. However, these methods often lack interpretability,
as they compute the similarity between an entire query image and the embedded
category words, making it difficult to explain their predictions. One approach
to address this issue is to develop interpretable models by integrating
language, where classifiers are built using discrete attributes, similar to
human perception. This introduces a new challenge: how to effectively align
local visual features with corresponding attributes based on pre-trained VLMs.
To tackle this, we propose LaZSL, a locally-aligned vision-language model for
interpretable ZSL. LaZSL employs local visual-semantic alignment via optimal
transport to perform interaction between visual regions and their associated
attributes, facilitating effective alignment and providing interpretable
similarity without the need for additional training. Extensive experiments
demonstrate that our method offers several advantages, including enhanced
interpretability, improved accuracy, and strong domain generalization. Codes
available at: https://github.com/shiming-chen/LaZSL.

</details>


### [324] [Flash-VStream: Efficient Real-Time Understanding for Long Video Streams](https://arxiv.org/abs/2506.23825)
*Haoji Zhang,Yiqin Wang,Yansong Tang,Yong Liu,Jiashi Feng,Xiaojie Jin*

Main category: cs.CV

TL;DR: Flash-VStream enhances long video understanding efficiently by introducing innovative memory modules for real-time user query responses.


<details>
  <summary>Details</summary>
Motivation: Understanding long videos poses challenges due to computational and memory overhead, as current models treat them similarly to short videos.

Method: The paper introduces Flash Memory modules with a low-capacity context memory for temporal information aggregation and a high-capacity augmentation memory to retrieve spatial information.

Result: Flash-VStream shows state-of-the-art performance and improved efficiency on long video benchmarks compared to existing models.

Conclusion: Flash-VStream demonstrates its capability to address the inefficiency of long video understanding while improving inference latency and overall performance.

Abstract: Benefiting from the advances in large language models and cross-modal
alignment, existing multimodal large language models have achieved prominent
performance in image and short video understanding. However, the understanding
of long videos is still challenging, as their long-context nature results in
significant computational and memory overhead. Most existing work treats long
videos in the same way as short videos, which is inefficient for real-world
applications and hard to generalize to even longer videos. To address these
issues, we propose Flash-VStream, an efficient video language model capable of
processing extremely long videos and responding to user queries in real time.
Particularly, we design a Flash Memory module, containing a low-capacity
context memory to aggregate long-context temporal information and model the
distribution of information density, and a high-capacity augmentation memory to
retrieve detailed spatial information based on this distribution. Compared to
existing models, Flash-VStream achieves significant reductions in inference
latency. Extensive experiments on long video benchmarks and comprehensive video
benchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate
the state-of-the-art performance and outstanding efficiency of our method. Code
is available at https://github.com/IVGSZ/Flash-VStream.

</details>


### [325] [Spatially Gene Expression Prediction using Dual-Scale Contrastive Learning](https://arxiv.org/abs/2506.23827)
*Mingcheng Qu,Yuncong Wu,Donglin Di,Yue Gao,Tonghua Su,Yang Song,Lei Fan*

Main category: cs.CV

TL;DR: NH2ST is a novel framework for predicting gene expression that integrates spatial and molecular data from pathology and gene modalities, addressing limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the high cost and complexity of spatial transcriptomics and limitations of existing gene expression prediction methods that fail to consider spatial and molecular interactions.

Method: NH2ST uses a query and neighbor branch to process data from pathology and gene modalities. Cross-attention and contrastive learning are employed to capture spatial and molecular associations.

Result: NH2ST outperforms existing methods across six datasets with over 20% improvement in PCC metrics.

Conclusion: NH2ST demonstrates the potential to significantly improve gene expression prediction by incorporating spatial and molecular context, showing promise for advancing tissue micro-environment studies.

Abstract: Spatial transcriptomics (ST) provides crucial insights into tissue
micro-environments, but is limited to its high cost and complexity. As an
alternative, predicting gene expression from pathology whole slide images (WSI)
is gaining increasing attention. However, existing methods typically rely on
single patches or a single pathology modality, neglecting the complex spatial
and molecular interactions between target and neighboring information (e.g.,
gene co-expression). This leads to a failure in establishing connections among
adjacent regions and capturing intricate cross-modal relationships. To address
these issues, we propose NH2ST, a framework that integrates spatial context and
both pathology and gene modalities for gene expression prediction. Our model
comprises a query branch and a neighbor branch to process paired target patch
and gene data and their neighboring regions, where cross-attention and
contrastive learning are employed to capture intrinsic associations and ensure
alignments between pathology and gene expression. Extensive experiments on six
datasets demonstrate that our model consistently outperforms existing methods,
achieving over 20% in PCC metrics. Codes are available at
https://github.com/MCPathology/NH2ST

</details>


### [326] [Low-latency vision transformers via large-scale multi-head attention](https://arxiv.org/abs/2506.23832)
*Ronit D. Gross,Tal Halevi,Ella Koresh,Yarden Tzach,Ido Kanter*

Main category: cs.CV

TL;DR: The study explores the spontaneous symmetry breaking in multi-head attention mechanisms and its application in classification tasks, leading to high signal-to-noise ratio and improved accuracy. It proposes hybrid architectures combining convolutional layers and transformers to reduce latency.


<details>
  <summary>Details</summary>
Motivation: To understand how spontaneous symmetry breaking contributes to label recognition in multi-head attention mechanisms and extend the advantages of vision transformer architectures in classification tasks.

Method: Quantified single-nodal and single-head performance, generalized to large-scale multi-head attention mechanisms, and investigated the behavior on the CIFAR-100 dataset. Developed hybrid architectures with CNNs and transformers.

Result: Improved classification accuracy via higher signal-to-noise ratio across transformer blocks. Hybrid architectures demonstrated reduced latency during early learning stages while maintaining accuracy.

Conclusion: Spontaneous symmetry breaking enhances label recognition in multi-head attention. Hybrid architectures combining CNNs and ViTs improve efficiency while delivering high accuracy, with potential applications beyond vision tasks into NLP.

Abstract: The emergence of spontaneous symmetry breaking among a few heads of
multi-head attention (MHA) across transformer blocks in classification tasks
was recently demonstrated through the quantification of single-nodal
performance (SNP). This finding indicates that each head focuses its attention
on a subset of labels through cooperation among its SNPs. This underlying
learning mechanism is generalized to large-scale MHA (LS-MHA) using a single
matrix value representing single-head performance (SHP), analogous to
single-filter performance in convolutional neural networks (CNNs). The results
indicate that each SHP matrix comprises multiple unit clusters such that each
label being explicitly recognized by a few heads with negligible noise. This
leads to an increased signal-to-noise ratio (SNR) along the transformer blocks,
thereby improving classification accuracy. These features give rise to several
distinct vision transformer (ViT) architectures that achieve the same accuracy
but differ in their LS-MHA structures. As a result, their soft committee yields
superior accuracy, an outcome not typically observed in CNNs which rely on
hundreds of filters. In addition, a significant reduction in latency is
achieved without affecting the accuracy by replacing the initial transformer
blocks with convolutional layers. This substitution accelerates early-stage
learning, which is then improved by subsequent transformer layers. The
extension of this learning mechanism to natural language processing tasks,
based on quantitative differences between CNNs and ViT architectures, has the
potential to yield new insights in deep learning. The findings are demonstrated
using compact convolutional transformer architectures trained on the CIFAR-100
dataset.

</details>


### [327] [PointSSIM: A novel low dimensional resolution invariant image-to-image comparison metric](https://arxiv.org/abs/2506.23833)
*Oscar Ovanger,Ragnar Hauge,Jacob Skauvold,Michael J. Pyrcz,Jo Eidsvik*

Main category: cs.CV

TL;DR: PointSSIM is a novel resolution-invariant metric for binary image comparison, transforming images into point pattern representations to analyze structural attributes efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the challenge of robustly comparing binary images across varying resolutions, a need for applications requiring structural analysis.

Method: PointSSIM employs mathematical morphology and structural similarity, extracting key features called anchor points via a minimal distance transform to create marked point pattern representations for comparison.

Result: Results demonstrate PointSSIM’s efficiency and reliability in comparing image structures across resolutions, capturing intensity, connectivity, complexity, and structural attributes.

Conclusion: PointSSIM is effective for image comparison tasks, particularly in scenarios involving varied resolutions and structural analysis requirements.

Abstract: This paper presents PointSSIM, a novel low-dimensional image-to-image
comparison metric that is resolution invariant. Drawing inspiration from the
structural similarity index measure and mathematical morphology, PointSSIM
enables robust comparison across binary images of varying resolutions by
transforming them into marked point pattern representations. The key features
of the image, referred to as anchor points, are extracted from binary images by
identifying locally adaptive maxima from the minimal distance transform. Image
comparisons are then performed using a summary vector, capturing intensity,
connectivity, complexity, and structural attributes. Results show that this
approach provides an efficient and reliable method for image comparison,
particularly suited to applications requiring structural analysis across
different resolutions.

</details>


### [328] [Refine Any Object in Any Scene](https://arxiv.org/abs/2506.23835)
*Ziwei Chen,Ziling Liu,Zitong Huang,Mingqi Gao,Feng Zheng*

Main category: cs.CV

TL;DR: The paper presents RAISE, a framework that uses 3D generative models for recovering fine-grained object details in scenes with missing viewpoints, outperforming existing methods in view synthesis and geometry completion.


<details>
  <summary>Details</summary>
Motivation: High-fidelity object modeling is difficult due to common missing viewpoints in scene reconstruction, which hinders downstream tasks that need detailed object understanding.

Method: RAISE substitutes degraded objects with proxies from a 3D generative model and refines their geometry and texture in a two-stage approach: pose alignment and registration-constrained enhancement for consistency.

Result: Experiments demonstrate that RAISE achieves superior performance compared to state-of-the-art in novel view synthesis and geometry completion benchmarks.

Conclusion: RAISE ensures detailed recovery of objects' geometry and appearance for unseen views while maintaining spatial and appearance consistency, advancing scene reconstruction techniques.

Abstract: Viewpoint missing of objects is common in scene reconstruction, as camera
paths typically prioritize capturing the overall scene structure rather than
individual objects. This makes it highly challenging to achieve high-fidelity
object-level modeling while maintaining accurate scene-level representation.
Addressing this issue is critical for advancing downstream tasks requiring
detailed object understanding and appearance modeling. In this paper, we
introduce Refine Any object In any ScenE (RAISE), a novel 3D enhancement
framework that leverages 3D generative priors to recover fine-grained object
geometry and appearance under missing views. Starting from substituting
degraded objects with proxies, via a 3D generative model with strong 3D
understanding, RAISE progressively refines geometry and texture by aligning
each proxy to its degraded counterpart in 7-DOF pose, followed by correcting
spatial and appearance inconsistencies via registration-constrained
enhancement. This two-stage refinement ensures the high-fidelity geometry and
appearance of the original object in unseen views while maintaining consistency
in spatial positioning, observed geometry, and appearance. Extensive
experiments on challenging benchmarks show that RAISE significantly outperforms
state-of-the-art methods in both novel view synthesis and geometry completion
tasks. RAISE is made publicly available at https://github.com/PolySummit/RAISE.

</details>


### [329] [RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment](https://arxiv.org/abs/2506.23852)
*Jianing Jin,Jiangyong Ying,Huiyu Duan,Liu Yang,Sijing Wu,Yunhao Li,Yushuo Zheng,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: This paper introduces the concept of Robotic-Generated Content (RGC) videos, proposes a first-of-its-kind database (RGCD) for their quality assessment, and reveals through experiments that existing VQA models struggle with RGC-specific challenges.


<details>
  <summary>Details</summary>
Motivation: With the growing integration of robotic platforms into daily life, robotic-generated videos are becoming more prevalent on various media platforms. There is a need to evaluate the unique visual and perceptual challenges such videos pose, as they differ significantly from professional or user-generated content.

Method: The authors created the RGCD, a database containing 2,100 robotic-generated videos across three robot categories and various platforms. They conducted subjective video quality assessment (VQA) experiments and benchmarked 11 state-of-the-art VQA models on this dataset.

Result: The study demonstrated significant deficiencies in the performance of existing VQA models when utilized for robotic-generated videos, highlighting their unsuitability for the unique distortions and requirements of RGC.

Conclusion: There is a pressing need for the development of RGC-specific video quality assessment models, as current VQA models do not adequately address the unique characteristics of robotic-generated content.

Abstract: As camera-equipped robotic platforms become increasingly integrated into
daily life, robotic-generated videos have begun to appear on streaming media
platforms, enabling us to envision a future where humans and robots coexist. We
innovatively propose the concept of Robotic-Generated Content (RGC) to term
these videos generated from egocentric perspective of robots. The perceptual
quality of RGC videos is critical in human-robot interaction scenarios, and RGC
videos exhibit unique distortions and visual requirements that differ markedly
from those of professionally-generated content (PGC) videos and user-generated
content (UGC) videos. However, dedicated research on quality assessment of RGC
videos is still lacking. To address this gap and to support broader robotic
applications, we establish the first Robotic-Generated Content Database (RGCD),
which contains a total of 2,100 videos drawn from three robot categories and
sourced from diverse platforms. A subjective VQA experiment is conducted
subsequently to assess human visual perception of robotic-generated videos.
Finally, we conduct a benchmark experiment to evaluate the performance of 11
state-of-the-art VQA models on our database. Experimental results reveal
significant limitations in existing VQA models when applied to complex,
robotic-generated content, highlighting a critical need for RGC-specific VQA
models. Our RGCD is publicly available at:
https://github.com/IntMeGroup/RGC-VQA.

</details>


### [330] [HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity](https://arxiv.org/abs/2506.23854)
*Yida Wang,Xueyang Zhang,Kun Zhan,Peng Jia,Xianpeng Lang*

Main category: cs.CV

TL;DR: HiNeuS is a framework for neural surface reconstruction that improves geometric fidelity and photometric consistency using novel techniques like SDF-guided ray tracing, planar-conformal regularization, and Eikonal relaxation.


<details>
  <summary>Details</summary>
Motivation: To address persistent challenges in neural surface reconstruction, such as multi-view radiance inconsistency, missing keypoints in textureless regions, and structural degradation from strict geometric constraints.

Method: HiNeuS employs differential visibility verification for resolving reflection ambiguities, planar-conformal regularization for preserving sharp edges, and Eikonal relaxation for balancing detail preservation with global regularity.

Result: Achieved state-of-the-art performance, reducing Chamfer distance by 21.4% and improving PSNR by 2.32 dB, with success on both synthetic and real-world datasets, and applications to inverse rendering tasks.

Conclusion: HiNeuS provides a unified and generalizable approach for surface reconstruction, excelling in photometric and geometric detail recovery under complex conditions.

Abstract: Neural surface reconstruction faces persistent challenges in reconciling
geometric fidelity with photometric consistency under complex scene conditions.
We present HiNeuS, a unified framework that holistically addresses three core
limitations in existing approaches: multi-view radiance inconsistency, missing
keypoints in textureless regions, and structural degradation from over-enforced
Eikonal constraints during joint optimization. To resolve these issues through
a unified pipeline, we introduce: 1) Differential visibility verification
through SDF-guided ray tracing, resolving reflection ambiguities via continuous
occlusion modeling; 2) Planar-conformal regularization via ray-aligned geometry
patches that enforce local surface coherence while preserving sharp edges
through adaptive appearance weighting; and 3) Physically-grounded Eikonal
relaxation that dynamically modulates geometric constraints based on local
radiance gradients, enabling detail preservation without sacrificing global
regularity. Unlike prior methods that handle these aspects through sequential
optimizations or isolated modules, our approach achieves cohesive integration
where appearance-geometry constraints evolve synergistically throughout
training. Comprehensive evaluations across synthetic and real-world datasets
demonstrate state-of-the-art performance, including a 21.4% reduction in
Chamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement
against neural rendering counterparts. Qualitative analyses reveal superior
capability in recovering specular instruments, urban layouts with
centimeter-scale infrastructure, and low-textured surfaces without local patch
collapse. The method's generalizability is further validated through successful
application to inverse rendering tasks, including material decomposition and
view-consistent relighting.

</details>


### [331] [A Closer Look at Conditional Prompt Tuning for Vision-Language Models](https://arxiv.org/abs/2506.23856)
*Ji Zhang,Shihan Wu,Lianli Gao,Jingkuan Song,Nicu Sebe,Heng Tao Shen*

Main category: cs.CV

TL;DR: Prompt Tuning (PT) approaches face challenges when adapting Vision-Language Pretrained Models (VLPMs) to new tasks without sacrificing performance on base tasks. This study identifies issues with conditional PT techniques and proposes Class-adaptive Prompt Tuning (CaPT), a novel method using Textual Class Information.


<details>
  <summary>Details</summary>
Motivation: To address the Base-New Tradeoff dilemma in Vision-Language Pretrained Models where adaptation to base tasks compromises generalization to new tasks.

Method: Developing Class-adaptive Prompt Tuning (CaPT) that uses Textual Class Information-conditioned prompts, enabling better performance on new classes without losing efficacy on base classes.

Result: CaPT improves the performance of five unconditional PT baselines on 11 datasets with negligible computational cost. Integrated into the DePT framework, the DeCaPT approach achieves superior results compared to state-of-the-art conditional PT methods.

Conclusion: CaPT provides an effective and computationally efficient solution to mitigate the Base-New Tradeoff problem, offering improvements across tasks with enhanced generalization capabilities.

Abstract: Despite the great promise of Prompt Tuning (PT) in adapting large
Vision-Language Pretrained Models (VLPMs) to downstream tasks, they often
struggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better
tuned to a base task, their ability to generalize to new tasks diminishes.
Recent work on conditional PT addresses this problem by replacing static
prompts with dynamic Visual Image Information (VII)-conditioned prompts,
improving the model's generalization to new tasks to some extent. In this work,
we first identify a critical issue with existing conditional PT methods: using
VII as the "condition" of prompts yields suboptimal performance, and even
random noise-conditioned prompts can outperform the VII-conditioned
counterparts. On further analysis, we find that learning dynamic prompts
conditioned on Textual Class Information (TCI) is the key to solving the BNT
problem. Motivated by this, we then propose Class-adaptive Prompt Tuning
(CaPT), which enables fast adaptation of tuned models to new classes by
learning TCI-conditioned prompts from base classes. Remarkably, CaPT can be
used as a plugin to mitigate the BNT problem for existing unconditional PT
schemes. Extensive experiments on 11 datasets show that CaPT consistently
improves the performance of five strong unconditional PT baselines with
negligible additional computational cost. Additionally, by integrating CaPT
with our recently proposed DePT framework, we devise a new conditional PT
approach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art
conditional PT scheme by 3.49%, averaged over the 11 datasets. Code:
https://github.com/Koorye/CaPT.

</details>


### [332] [VMoBA: Mixture-of-Block Attention for Video Diffusion Models](https://arxiv.org/abs/2506.23858)
*Jianzong Wu,Liang Hou,Haotian Yang,Xin Tao,Ye Tian,Pengfei Wan,Di Zhang,Yunhai Tong*

Main category: cs.CV

TL;DR: The paper introduces Video Mixture of Block Attention (VMoBA), a sparse attention mechanism for Video Diffusion Models to address the quadratic complexity issue in generating high-resolution, long-duration videos.


<details>
  <summary>Details</summary>
Motivation: To overcome the computational bottleneck of full attention mechanisms in Video Diffusion Models and optimize spatio-temporal sparse attention for video data.

Method: The authors proposed VMoBA with three innovations: a layer-wise recurrent block partition scheme (1D-2D-3D), global block selection for salient interactions, and threshold-based block selection for adaptive attention allocation.

Result: VMoBA accelerates training of VDMs with a 2.92x FLOPs and 1.48x latency speedup, maintains or improves generation quality, and achieves competitive speedups during training-free inference (2.40x FLOPs, 1.35x latency).

Conclusion: VMoBA effectively addresses the computational challenges in video generation, balancing efficiency and quality, and stands as a competitive alternative to full attention mechanisms.

Abstract: The quadratic complexity of full attention mechanisms poses a significant
bottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,
high-resolution videos. While various sparse attention methods have been
proposed, many are designed as training-free inference accelerators or do not
optimally capture the unique spatio-temporal characteristics inherent in video
data when trained natively. This paper introduces Video Mixture of Block
Attention (VMoBA), a novel sparse attention mechanism specifically adapted for
VDMs. Motivated by an in-depth analysis of attention patterns within
pre-trained video transformers, which revealed strong spatio-temporal locality,
varying query importance, and head-specific concentration levels, VMoBA
enhances the original MoBA framework with three key modifications: (1) a
layer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to
diverse spatio-temporal attention patterns and improve efficiency; (2) global
block selection to prioritize the most salient query-key block interactions
across an entire attention head; and (3) threshold-based block selection to
dynamically determine the number of attended blocks based on their cumulative
similarity. Extensive experiments demonstrate that VMoBA significantly
accelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and
1.48x latency speedup, while attaining comparable or even superior generation
quality to full attention. Furthermore, VMoBA exhibits competitive performance
in training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for
high-res video generation.

</details>


### [333] [Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction](https://arxiv.org/abs/2506.23863)
*Jiahao Ma,Lei Wang,Miaomiao liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: The paper introduces 'Puzzles,' a data augmentation technique designed to enhance video-based 3D reconstruction models by synthesizing diverse training data.


<details>
  <summary>Details</summary>
Motivation: 3D reconstruction methods often depend on limited datasets, restricting their performance and generalization ability.

Method: Puzzles generates synthetic, high-quality video-depth training data using image transformations that simulate varied camera movements and scene geometries.

Result: Integrating Puzzles in 3D reconstruction workflows enables models to achieve high accuracy with significantly less real training data.

Conclusion: Puzzles efficiently boosts 3D reconstruction performance by improving data diversity and scale, without altering network architectures.

Abstract: Multi-view 3D reconstruction remains a core challenge in computer vision.
Recent methods, such as DUST3R and its successors, directly regress pointmaps
from image pairs without relying on known scene geometry or camera parameters.
However, the performance of these models is constrained by the diversity and
scale of available training data. In this work, we introduce Puzzles, a data
augmentation strategy that synthesizes an unbounded volume of high-quality
posed video-depth data from a single image or video clip. By simulating diverse
camera trajectories and realistic scene geometry through targeted image
transformations, Puzzles significantly enhances data variety. Extensive
experiments show that integrating Puzzles into existing video-based 3D
reconstruction pipelines consistently boosts performance without modifying the
underlying network architecture. Notably, models trained on only ten percent of
the original data augmented with Puzzles still achieve accuracy comparable to
those trained on the full dataset. Code is available at
https://jiahao-ma.github.io/puzzles/.

</details>


### [334] [Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection](https://arxiv.org/abs/2506.23881)
*Reihaneh Zohrabi,Hosein Hasani,Mahdieh Soleymani Baghshah,Anna Rohrbach,Marcus Rohrbach,Mohammad Hossein Rohban*

Main category: cs.CV

TL;DR: SPROD is a prototype-based OOD detection method addressing spurious correlations in unseen data, improving detection benchmarks significantly.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the vulnerability of existing OOD methods to spurious correlations, which compromise reliability and robustness in real-world applications.

Method: SPROD refines class prototypes post-hoc to reduce bias from spurious features without requiring extra data or tuning, and can be applied to various OOD detection setups and model backbones.

Result: SPROD showed superior performance in challenging OOD datasets, outperforming previous methods by improving AUROC by 4.7% and FPR@95 by 9.3% on average.

Conclusion: SPROD offers a robust and generalized solution to OOD detection, proving effective in mitigating spurious correlations and improving reliability in various data settings.

Abstract: Out-of-distribution (OOD) detection is crucial for ensuring the reliability
and safety of machine learning models in real-world applications, where they
frequently face data distributions unseen during training. Despite progress,
existing methods are often vulnerable to spurious correlations that mislead
models and compromise robustness. To address this, we propose SPROD, a novel
prototype-based OOD detection approach that explicitly addresses the challenge
posed by unknown spurious correlations. Our post-hoc method refines class
prototypes to mitigate bias from spurious features without additional data or
hyperparameter tuning, and is broadly applicable across diverse backbones and
OOD detection settings. We conduct a comprehensive spurious correlation OOD
detection benchmarking, comparing our method against existing approaches and
demonstrating its superior performance across challenging OOD datasets, such as
CelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced
Animals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%
over the second best.

</details>


### [335] [PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View](https://arxiv.org/abs/2506.23897)
*Longliang Liu,Miaojie Feng,Junda Cheng,Jijun Xiang,Xuan Zhu,Xin Yang*

Main category: cs.CV

TL;DR: This paper introduces PriOr-Flow, a method that improves panoramic optical flow estimation by addressing distortion issues caused by sphere-to-plane projection.


<details>
  <summary>Details</summary>
Motivation: Existing optical flow methods struggle with severe distortions in panoramic images, particularly near polar regions, due to sphere-to-plane projections.

Method: The authors propose a dual-branch framework combining primitive and orthogonal views. Key components include the Dual-Cost Collaborative Lookup (DCCL) operator for retrieving correlation information and the Ortho-Driven Distortion Compensation (ODDC) module for iterative refinement.

Result: PriOr-Flow achieves state-of-the-art performance across panoramic optical flow datasets and is compatible with various iterative optical flow methods.

Conclusion: The approach effectively mitigates polar distortions, setting a new benchmark for panoramic optical flow estimation and enhancing wide-field motion analysis capabilities.

Abstract: Panoramic optical flow enables a comprehensive understanding of temporal
dynamics across wide fields of view. However, severe distortions caused by
sphere-to-plane projections, such as the equirectangular projection (ERP),
significantly degrade the performance of conventional perspective-based optical
flow methods, especially in polar regions. To address this challenge, we
propose PriOr-Flow, a novel dual-branch framework that leverages the
low-distortion nature of the orthogonal view to enhance optical flow estimation
in these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup
(DCCL) operator, which jointly retrieves correlation information from both the
primitive and orthogonal cost volumes, effectively mitigating distortion noise
during cost volume construction. Furthermore, our Ortho-Driven Distortion
Compensation (ODDC) module iteratively refines motion features from both
branches, further suppressing polar distortions. Extensive experiments
demonstrate that PriOr-Flow is compatible with various perspective-based
iterative optical flow methods and consistently achieves state-of-the-art
performance on publicly available panoramic optical flow datasets, setting a
new benchmark for wide-field motion estimation. The code is publicly available
at: https://github.com/longliangLiu/PriOr-Flow.

</details>


### [336] [GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models](https://arxiv.org/abs/2506.23903)
*Hamza Rasaee,Taha Koleilat,Hassan Rivaz*

Main category: cs.CV

TL;DR: The paper introduces a vision-language model (VLM) for object segmentation in ultrasound imaging, showcasing its superior performance across diverse datasets and unseen distributions.


<details>
  <summary>Details</summary>
Motivation: Segmenting ultrasound images is challenging due to anatomical variability, diverse imaging protocols, and limited annotated data.

Method: The study combines Grounding DINO and SAM2 in a VLM framework, using 15 out of 18 datasets for fine-tuning with low-rank adaptation (LoRA) and 3 datasets for testing on unseen distributions.

Result: The proposed method outperforms state-of-the-art segmentation models on most datasets and generalizes well to unseen distributions.

Conclusion: Using VLMs can improve ultrasound image segmentation accuracy and scalability, minimizing the need for large annotated datasets.

Abstract: Accurate and generalizable object segmentation in ultrasound imaging remains
a significant challenge due to anatomical variability, diverse imaging
protocols, and limited annotated data. In this study, we propose a
prompt-driven vision-language model (VLM) that integrates Grounding DINO with
SAM2 to enable object segmentation across multiple ultrasound organs. A total
of 18 public ultrasound datasets, encompassing the breast, thyroid, liver,
prostate, kidney, and paraspinal muscle, were utilized. These datasets were
divided into 15 for fine-tuning and validation of Grounding DINO using Low Rank
Adaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for
testing to evaluate performance in unseen distributions. Comprehensive
experiments demonstrate that our approach outperforms state-of-the-art
segmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,
and SAMUS on most seen datasets while maintaining strong performance on unseen
datasets without additional fine-tuning. These results underscore the promise
of VLMs in scalable and robust ultrasound image analysis, reducing dependence
on large, organ-specific annotated datasets. We will publish our code on
code.sonography.ai after acceptance.

</details>


### [337] [Three-dimensional end-to-end deep learning for brain MRI analysis](https://arxiv.org/abs/2506.23916)
*Radhika Juglan,Marta Ligero,Zunamys I. Carrero,Asier Rabasco,Tim Lenz,Leo Misera,Gregory Patrick Veldhuizen,Paul Kuntke,Hagen H. Kitzler,Sven Nebelung,Daniel Truhn,Jakob Nikolas Kather*

Main category: cs.CV

TL;DR: The study assesses three deep learning architectures for age and sex prediction using MRI data from diverse cohorts, finding simpler architectures like SFCN outperforming more complex ones in generalizability.


<details>
  <summary>Details</summary>
Motivation: To investigate the generalizability of deep learning methods in brain imaging across diverse datasets, focusing on age and sex prediction.

Method: Evaluated three DL models (SFCN, DenseNet, Swin Transformers) with MRI data from four cohorts for age and sex prediction, and analyzed model performance statistically and through explainability methods.

Result: SFCN achieved superior performance across cohorts, with high AUCs for sex classification and lower MAE for age prediction compared to DenseNet and Swin Transformers.

Conclusion: Simpler DL architectures, specifically SFCN, show better generalizability and performance compared to complex attention-based models in brain imaging tasks.

Abstract: Deep learning (DL) methods are increasingly outperforming classical
approaches in brain imaging, yet their generalizability across diverse imaging
cohorts remains inadequately assessed. As age and sex are key neurobiological
markers in clinical neuroscience, influencing brain structure and disease risk,
this study evaluates three of the existing three-dimensional architectures,
namely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window
(Swin) Transformers, for age and sex prediction using T1-weighted MRI from four
independent cohorts: UK Biobank (UKB, n=47,390), Dallas Lifespan Brain Study
(DLBS, n=132), Parkinson's Progression Markers Initiative (PPMI, n=108 healthy
controls), and Information eXtraction from Images (IXI, n=319). We found that
SFCN consistently outperformed more complex architectures with AUC of 1.00
[1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for
sex classification. For the age prediction task, SFCN demonstrated a mean
absolute error (MAE) of 2.66 (r=0.89) in UKB and 4.98-5.81 (r=0.55-0.70) across
external datasets. Pairwise DeLong and Wilcoxon signed-rank tests with
Bonferroni corrections confirmed SFCN's superiority over Swin Transformer
across most cohorts (p<0.017, for three comparisons). Explainability analysis
further demonstrates the regional consistency of model attention across cohorts
and specific to each task. Our findings reveal that simpler convolutional
networks outperform the denser and more complex attention-based DL
architectures in brain image analysis by demonstrating better generalizability
across different datasets.

</details>


### [338] [Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers](https://arxiv.org/abs/2506.23918)
*Zhaochen Su,Peng Xia,Hangyu Guo,Zhenhua Liu,Yan Ma,Xiaoye Qu,Jiaqi Liu,Yanshu Li,Kaide Zeng,Zhengyuan Yang,Linjie Li,Yu Cheng,Heng Ji,Junxian He,Yi R.,Fung*

Main category: cs.CV

TL;DR: The paper discusses a shift in multimodal reasoning from text-based processes to a "think with image" paradigm, where images serve as dynamic tools for reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap between perceptual data and symbolic reasoning by exploring how vision can be integrated as an active cognitive component in AI systems.

Method: The paper presents a survey structured around a three-stage framework: (1) external tool exploration, (2) programmatic manipulation, and (3) intrinsic imagination, while reviewing methods, benchmarks, and challenges.

Result: The survey maps the progression of methods for multimodal reasoning, evaluates their applications, and identifies trends and gaps in current approaches.

Conclusion: The work highlights the potential of multimodal AI to achieve more human-aligned cognition by leveraging vision as a dynamic reasoning tool, offering future research directions.

Abstract: Recent progress in multimodal reasoning has been significantly advanced by
textual Chain-of-Thought (CoT), a paradigm where models conduct reasoning
within language. This text-centric approach, however, treats vision as a
static, initial context, creating a fundamental "semantic gap" between rich
perceptual data and discrete symbolic thought. Human cognition often transcends
language, utilizing vision as a dynamic mental sketchpad. A similar evolution
is now unfolding in AI, marking a fundamental paradigm shift from models that
merely think about images to those that can truly think with images. This
emerging paradigm is characterized by models leveraging visual information as
intermediate steps in their thought process, transforming vision from a passive
input into a dynamic, manipulable cognitive workspace. In this survey, we chart
this evolution of intelligence along a trajectory of increasing cognitive
autonomy, which unfolds across three key stages: from external tool
exploration, through programmatic manipulation, to intrinsic imagination. To
structure this rapidly evolving field, our survey makes four key contributions.
(1) We establish the foundational principles of the think with image paradigm
and its three-stage framework. (2) We provide a comprehensive review of the
core methods that characterize each stage of this roadmap. (3) We analyze the
critical landscape of evaluation benchmarks and transformative applications.
(4) We identify significant challenges and outline promising future directions.
By providing this structured overview, we aim to offer a clear roadmap for
future research towards more powerful and human-aligned multimodal AI.

</details>


### [339] [Evaluating the Impact of Khmer Font Types on Text Recognition](https://arxiv.org/abs/2506.23963)
*Vannkinh Nom,Souhail Bakkali,Muhammad Muzzamil Luqman,Mickael Coustaty,Jean-Marc Ogier*

Main category: cs.CV

TL;DR: This paper examines the impact of 19 Khmer font types on OCR accuracy using Pytesseract. It identifies both high-performing and poorly performing fonts, emphasizing the importance of font choice in Khmer text recognition.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the challenges posed by the variety and complexity of Khmer fonts on OCR accuracy and to identify which fonts work best or poorly.

Method: The study evaluates OCR accuracy using Pytesseract across 19 randomly selected Khmer font types, comparing their performance.

Result: Five fonts (Khmer, Odor MeanChey, Siemreap, Sithi Manuss, and Battambang) achieve high accuracy, while three fonts (iSeth First, Bayon, and Dangrek) perform poorly.

Conclusion: Font selection is critical for optimizing text recognition in Khmer scripts, and this study provides insights for developing more effective OCR systems.

Abstract: Text recognition is significantly influenced by font types, especially for
complex scripts like Khmer. The variety of Khmer fonts, each with its unique
character structure, presents challenges for optical character recognition
(OCR) systems. In this study, we evaluate the impact of 19 randomly selected
Khmer font types on text recognition accuracy using Pytesseract. The fonts
include Angkor, Battambang, Bayon, Bokor, Chenla, Dangrek, Freehand, Kh Kompong
Chhnang, Kh SN Kampongsom, Khmer, Khmer CN Stueng Songke, Khmer Savuth Pen,
Metal, Moul, Odor MeanChey, Preah Vihear, Siemreap, Sithi Manuss, and iSeth
First. Our comparison of OCR performance across these fonts reveals that Khmer,
Odor MeanChey, Siemreap, Sithi Manuss, and Battambang achieve high accuracy,
while iSeth First, Bayon, and Dangrek perform poorly. This study underscores
the critical importance of font selection in optimizing Khmer text recognition
and provides valuable insights for developing more robust OCR systems.

</details>


### [340] [Visual and Memory Dual Adapter for Multi-Modal Object Tracking](https://arxiv.org/abs/2506.23972)
*Boyue Xu,Ruichao Hou,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: This paper introduces a visual and memory dual adapter (VMDA) for multi-modal tracking, improving prompt reliability by leveraging frequency and temporal cues.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-learning-based multi-modal trackers struggle with exploiting critical frequency and temporal cues, necessitating a more robust framework.

Method: The authors propose a visual adapter to integrate auxiliary modality features adaptively and a memory adapter inspired by human memory, which stores global temporal cues for consistent propagation.

Result: VMDA achieves state-of-the-art results across RGB-Thermal, RGB-Depth, and RGB-Event tracking tasks.

Conclusion: The proposed VMDA framework enhances multi-modal tracking performance by creating more robust and discriminative representations, addressing limitations in prompt-learning trackers.

Abstract: Prompt-learning-based multi-modal trackers have achieved promising progress
by employing lightweight visual adapters to incorporate auxiliary modality
features into frozen foundation models. However, existing approaches often
struggle to learn reliable prompts due to limited exploitation of critical cues
across frequency and temporal domains. In this paper, we propose a novel visual
and memory dual adapter (VMDA) to construct more robust and discriminative
representations for multi-modal tracking. Specifically, we develop a simple but
effective visual adapter that adaptively transfers discriminative cues from
auxiliary modality to dominant modality by jointly modeling the frequency,
spatial, and channel-wise features. Additionally, we design the memory adapter
inspired by the human memory mechanism, which stores global temporal cues and
performs dynamic update and retrieval operations to ensure the consistent
propagation of reliable temporal information across video sequences. Extensive
experiments demonstrate that our method achieves state-of-the-art performance
on the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth,
and RGB-Event tracking. Code and models are available at
https://github.com/xuboyue1999/mmtrack.git.

</details>


### [341] [Toward Simple and Robust Contrastive Explanations for Image Classification by Leveraging Instance Similarity and Concept Relevance](https://arxiv.org/abs/2506.23975)
*Yuliia Kaidashova,Bettina Finzel,Ute Schmid*

Main category: cs.CV

TL;DR: The paper introduces a method for providing concept-based contrastive explanations for image classification models, examining the relevance of concepts and robustness under image augmentations.


<details>
  <summary>Details</summary>
Motivation: This research is motivated by the challenge of understanding why a classification model selects one class over another, emphasizing the importance of interpretable and robust AI systems.

Method: The approach uses instance embeddings and human-understandable concepts from fine-tuned models to extract relevance scores, compute contrasts, and assess explanation complexity under varying conditions, including augmentations like noise and rotation.

Result: Higher concept relevance results in shorter and less complex explanations, while lower relevance has the opposite effect. The robustness of explanations under image augmentations shows varying outcomes.

Conclusion: The study highlights that explanation complexity is influenced by concept relevance and suggests potential strategies for creating more interpretable and robust AI models.

Abstract: Understanding why a classification model prefers one class over another for
an input instance is the challenge of contrastive explanation. This work
implements concept-based contrastive explanations for image classification by
leveraging the similarity of instance embeddings and relevance of
human-understandable concepts used by a fine-tuned deep learning model. Our
approach extracts concepts with their relevance score, computes contrasts for
similar instances, and evaluates the resulting contrastive explanations based
on explanation complexity. Robustness is tested for different image
augmentations. Two research questions are addressed: (1) whether explanation
complexity varies across different relevance ranges, and (2) whether
explanation complexity remains consistent under image augmentations such as
rotation and noise. The results confirm that for our experiments higher concept
relevance leads to shorter, less complex explanations, while lower relevance
results in longer, more diffuse explanations. Additionally, explanations show
varying degrees of robustness. The discussion of these findings offers insights
into the potential of building more interpretable and robust AI systems.

</details>


### [342] [Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data](https://arxiv.org/abs/2506.24039)
*Shubhabrata Mukherjee,Jack Lang,Obeen Kwon,Iryna Zenyuk,Valerie Brogden,Adam Weber,Daniela Ushizima*

Main category: cs.CV

TL;DR: Zenesis, a no-code platform, adapts zero-shot technologies for scarce scientific images using lightweight multi-modal techniques, excelling in challenging FIB-SEM data analysis.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of adapting zero-shot and prompt-based technologies for scarce and valuable scientific image datasets, which are not well-suited for these methods.

Method: The authors propose Zenesis, a no-code platform with multi-modal adaptation techniques for zero-shot operation on raw scientific data. It includes human-in-the-loop refinement and temporal enhancement.

Result: Zenesis achieved high performance, significantly outperforming baseline methods, with metrics such as 0.947 accuracy and 0.858 IOU for amorphous samples and 0.987 accuracy and 0.857 IOU for crystalline samples.

Conclusion: Zenesis is a robust, user-friendly tool that outperforms traditional and advanced segmentation models, especially valuable for fields lacking high-quality annotated datasets.

Abstract: Zero-shot and prompt-based technologies capitalized on using frequently
occurring images to transform visual reasoning tasks, which explains why such
technologies struggle with valuable yet scarce scientific image sets. In this
work, we propose Zenesis, a comprehensive no-code interactive platform designed
to minimize barriers posed by data readiness for scientific images. We develop
lightweight multi-modal adaptation techniques that enable zero-shot operation
on raw scientific data, along with human-in-the-loop refinement and
heuristic-based temporal enhancement options. We demonstrate the performance of
our approach through comprehensive comparison and validation on challenging
Focused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded
membranes. Zenesis significantly outperforms baseline methods, achieving an
average accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a
Dice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an
IOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results
mark a substantial improvement over traditional methods like Otsu thresholding
and even advanced models like Segment Anything Model (SAM) when used in
isolation. Our results demonstrate that Zenesis is a powerful tool for
scientific applications, particularly in fields where high-quality annotated
datasets are unavailable, accelerating accurate analysis of experimental
imaging.

</details>


### [343] [Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios](https://arxiv.org/abs/2506.24063)
*Deng Li,Aming Wu,Yang Li,Yaowei Wang,Yahong Han*

Main category: cs.CV

TL;DR: The paper proposes a novel approach to continual test-time adaptation for object detection by introducing a dual-path domain-aware adapter and a conditional diffusion-based parameter generation mechanism for better adaptation and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing object detectors struggle with changing environments as they are trained on the closed-set assumption where training and test data have the same distribution. Continual test-time adaptation can improve generalization but may lead to degraded performance due to parameter tuning on small test datasets.

Method: The authors designed a dual-path LoRA-based domain-aware adapter to separate features into domain-invariant and domain-specific components. They also introduced a conditional diffusion-based parameter generation mechanism to adapt parameters for the current environment and mitigate optimization pitfalls, alongside a class-centered optimal transport alignment to prevent catastrophic forgetting.

Result: The proposed method showed effectiveness across various continuous domain adaptation tasks for object detection, with improved generalization and better feature representation of objects.

Conclusion: The approach enhances object detection in dynamic environments by disentangling domain features, creating tailored parameters for new settings, and improving robustness and generalization, as evidenced by empirical results and visualizations.

Abstract: In practice, environments constantly change over time and space, posing
significant challenges for object detectors trained based on a closed-set
assumption, i.e., training and test data share the same distribution. To this
end, continual test-time adaptation has attracted much attention, aiming to
improve detectors' generalization by fine-tuning a few specific parameters,
e.g., BatchNorm layers. However, based on a small number of test images,
fine-tuning certain parameters may affect the representation ability of other
fixed parameters, leading to performance degradation. Instead, we explore a new
mechanism, i.e., converting the fine-tuning process to a specific-parameter
generation. Particularly, we first design a dual-path LoRA-based domain-aware
adapter that disentangles features into domain-invariant and domain-specific
components, enabling efficient adaptation. Additionally, a conditional
diffusion-based parameter generation mechanism is presented to synthesize the
adapter's parameters based on the current environment, preventing the
optimization from getting stuck in local optima. Finally, we propose a
class-centered optimal transport alignment method to mitigate catastrophic
forgetting. Extensive experiments conducted on various continuous domain
adaptive object detection tasks demonstrate the effectiveness. Meanwhile,
visualization results show that the representation extracted by the generated
parameters can capture more object-related information and strengthen the
generalization ability.

</details>


### [344] [Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention](https://arxiv.org/abs/2506.24085)
*Wonwoong Cho,Yanxia Zhang,Yan-Ying Chen,David I. Inouye*

Main category: cs.CV

TL;DR: The paper introduces IT-Blender, a diffusion-based model for blending real images and textual inputs to spark human creativity, overcoming issues like loss of image details and entangled inputs.


<details>
  <summary>Details</summary>
Motivation: Humans face challenges like cognitive biases in cross-modal conceptual blending, leading to limited creativity. The paper aims to provide an automated, efficient solution to enhance this blending process.

Method: The authors propose IT-Blender, which uses pretrained diffusion models (SD and FLUX) to blend latent representations of images and text inputs effectively. It uses a novel blended attention mechanism to maintain image details and achieve disentangled blending.

Result: Experiment results demonstrate IT-Blender's significant superiority over baseline methods in visual and textual blending, highlighting its capability to assist creativity.

Conclusion: IT-Blender successfully automates and improves cross-modal conceptual blending, providing a powerful tool for augmenting human creativity and expanding the applications of generative models.

Abstract: Blending visual and textual concepts into a new visual concept is a unique
and powerful trait of human beings that can fuel creativity. However, in
practice, cross-modal conceptual blending for humans is prone to cognitive
biases, like design fixation, which leads to local minima in the design space.
In this paper, we propose a T2I diffusion adapter "IT-Blender" that can
automate the blending process to enhance human creativity. Prior works related
to cross-modal conceptual blending are limited in encoding a real image without
loss of details or in disentangling the image and text inputs. To address these
gaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend
the latent representations of a clean reference image with those of the noisy
generated image. Combined with our novel blended attention, IT-Blender encodes
the real reference image without loss of details and blends the visual concept
with the object specified by the text in a disentangled way. Our experiment
results show that IT-Blender outperforms the baselines by a large margin in
blending visual and textual concepts, shedding light on the new application of
image generative models to augment human creativity.

</details>


### [345] [WaRA: Wavelet Low Rank Adaptation](https://arxiv.org/abs/2506.24092)
*Moein Heidari,Yasamin Medghalchi,Mahdi Khoursha,Reza Rezaeian,Ilker Hacihaliloglu*

Main category: cs.CV

TL;DR: WaRA is a new parameter-efficient fine-tuning method that uses wavelet transforms for multi-resolution weight updates, outperforming existing techniques like LoRA in vision and language tasks.


<details>
  <summary>Details</summary>
Motivation: Existing parameter-efficient fine-tuning techniques like LoRA rely on global low-rank factorizations, which fail to capture local or multi-scale patterns in weight updates.

Method: WaRA employs wavelet transforms to decompose weight update matrices into multi-resolution representations, performs low-rank factorization in the wavelet domain, and reconstructs updates via inverse transforms.

Result: WaRA consistently outperforms conventional PEFT methods like LoRA on vision tasks, improving image quality and reducing computational complexity while showing effectiveness in language tasks as well.

Conclusion: WaRA provides a flexible and efficient alternative to existing PEFT methods by offering multi-resolution adaptive parameters, demonstrating superior performance across vision and language tasks.

Abstract: Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across
various applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its
extensions have emerged as particularly effective, allowing efficient model
adaptation while significantly reducing computational overhead. However,
existing approaches typically rely on global low-rank factorizations, which
overlook local or multi-scale structure, failing to capture complex patterns in
the weight updates. To address this, we propose WaRA, a novel PEFT method that
leverages wavelet transforms to decompose the weight update matrix into a
multi-resolution representation. By performing low-rank factorization in the
wavelet domain and reconstructing updates through an inverse transform, WaRA
obtains compressed adaptation parameters that harness multi-resolution
analysis, enabling it to capture both coarse and fine-grained features while
providing greater flexibility and sparser representations than standard LoRA.
Through comprehensive experiments and analysis, we demonstrate that WaRA
performs superior on diverse vision tasks, including image generation,
classification, and semantic segmentation, significantly enhancing generated
image quality while reducing computational complexity. Although WaRA was
primarily designed for vision tasks, we further showcase its effectiveness in
language tasks, highlighting its broader applicability and generalizability.
The code is publicly available at
\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.

</details>


### [346] [MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction](https://arxiv.org/abs/2506.24096)
*Antoine Guédon,Diego Gomez,Nissim Maruani,Bingchen Gong,George Drettakis,Maks Ovsjanikov*

Main category: cs.CV

TL;DR: MILo introduces a novel Gaussian Splatting framework to extract efficient and accurate surface meshes directly during training without post-processing, achieving state-of-the-art quality with fewer vertices.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the limitations of current approaches in extracting surface meshes from Gaussian Splatting. Existing methods either lose fine geometric details, require significant computational time, or produce overly dense meshes. Furthermore, the post-processing nature of these methods limits the preservation of geometric structures.

Method: MILo develops a fully differentiable framework to extract a mesh directly from the parameters of 3D Gaussians during training. Three technical contributions are proposed: bidirectional consistency between volumetric and surface representations, adaptive mesh extraction using Gaussians as pivots for Delaunay triangulation, and a novel method for computing signed distance values.

Result: MILo achieves state-of-the-art mesh reconstruction quality, is capable of capturing complete scenes, and requires an order of magnitude fewer mesh vertices compared to previous methods.

Conclusion: MILo bridges the gap between volumetric and surface representations by enabling efficient and high-quality mesh extraction during training. Its lightweight and efficient mesh structure makes it suitable for various downstream applications like physics simulations and animations.

Abstract: While recent advances in Gaussian Splatting have enabled fast reconstruction
of high-quality 3D scenes from images, extracting accurate surface meshes
remains a challenge. Current approaches extract the surface through costly
post-processing steps, resulting in the loss of fine geometric details or
requiring significant time and leading to very dense meshes with millions of
vertices. More fundamentally, the a posteriori conversion from a volumetric to
a surface representation limits the ability of the final mesh to preserve all
geometric structures captured during training. We present MILo, a novel
Gaussian Splatting framework that bridges the gap between volumetric and
surface representations by differentiably extracting a mesh from the 3D
Gaussians. We design a fully differentiable procedure that constructs the
mesh-including both vertex locations and connectivity-at every iteration
directly from the parameters of the Gaussians, which are the only quantities
optimized during training. Our method introduces three key technical
contributions: a bidirectional consistency framework ensuring both
representations-Gaussians and the extracted mesh-capture the same underlying
geometry during training; an adaptive mesh extraction process performed at each
training iteration, which uses Gaussians as differentiable pivots for Delaunay
triangulation; a novel method for computing signed distance values from the 3D
Gaussians that enables precise surface extraction while avoiding geometric
erosion. Our approach can reconstruct complete scenes, including backgrounds,
with state-of-the-art quality while requiring an order of magnitude fewer mesh
vertices than previous methods. Due to their light weight and empty interior,
our meshes are well suited for downstream applications such as physics
simulations or animation.

</details>


### [347] [DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World](https://arxiv.org/abs/2506.24102)
*Xiangtai Li,Tao Zhang,Yanwei Li,Haobo Yuan,Shihao Chen,Yikang Zhou,Jiahao Meng,Yueyi Sun,Shilin Xu,Lu Qi,Tianheng Cheng,Yi Lin,Zilong Huang,Wenhao Huang,Jiashi Feng,Guang Shi*

Main category: cs.CV

TL;DR: The paper introduces DenseWorld-1M, a large-scale, detailed grounded visual caption dataset addressing shortcomings in existing resources by using a three-stage labeling pipeline and novel VLM models.


<details>
  <summary>Details</summary>
Motivation: Existing caption datasets lack detailed object descriptions, spatial relations, and grounding information for visual entities, particularly in high-resolution images.

Method: The authors propose a three-stage labeling pipeline: (1) open-world perception to obtain entity-level masks and labels, (2) detailed captions for objects using masks and labels, and (3) merging captions into spatially and relationally dense descriptions. They also introduce two VLM models to enhance the labeling process.

Result: DenseWorld-1M provides detailed grounded captions, successfully improving vision-language tasks like visual grounding and region caption generation in experiments.

Conclusion: DenseWorld-1M and the associated labeling models fill critical gaps in current caption datasets, enhancing multimodal machine learning research and applications.

Abstract: Multimodal Large Language Models (MLLMs) demonstrate a complex understanding
of scenes, benefiting from large-scale and high-quality datasets. Most existing
caption datasets lack the ground locations and relations for visual entities.
Several grounded caption datasets face the problems of missing detailed
descriptions, relations, and massive object descriptions on high-resolution
images. To fill this gap for the community, we present DenseWorld-1M, the first
massive, detailed, dense grounded caption dataset in the real world. We design
a three-stage labeling pipeline, containing open-world perception, detailed
object caption generation, and dense caption merging. The first stage obtains
entity-level masks and labels. The second stage generates the object-level,
detailed captions with the guidance of masks and labels from the first stage.
The final stage merges object captions and masks into spatial and relational
dense captions. To accelerate the labeling process and improve caption quality,
we present two VLM models: the Detailed Region Caption model and the Spatial
Caption Merging model. Extensive experiments on various settings, including
vision-language understanding, visual grounding, and region caption generation,
demonstrate the effectiveness of our DenseWorld-1M dataset and labeling models.

</details>


### [348] [Epona: Autoregressive Diffusion World Model for Autonomous Driving](https://arxiv.org/abs/2506.24113)
*Kaiwen Zhang,Zhenyu Tang,Xiaotao Hu,Xingang Pan,Xiaoyang Guo,Yuan Liu,Jingwei Huang,Li Yuan,Qian Zhang,Xiao-Xiao Long,Xun Cao,Wei Yin*

Main category: cs.CV

TL;DR: Epona introduces a novel autoregressive diffusion world model for long-horizon video generation and integrated trajectory planning, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: There is a need for video generation models that handle flexible-length, long-horizon predictions and integrate trajectory planning for autonomous driving applications.

Method: Epona introduces localized spatiotemporal distribution modeling by decoupling spatiotemporal factors and using modular trajectory and video prediction. A chain-of-forward training strategy addresses error accumulation.

Result: Experimental results show Epona achieves 7.4% improvement in FVD and enables minutes-long video predictions. It also excels in real-time motion planning on NAVSIM benchmarks.

Conclusion: Epona's innovations advance video diffusion-based world modeling, achieving superior prediction durations and planning capabilities, making it a strong candidate for autonomous driving applications.

Abstract: Diffusion models have demonstrated exceptional visual quality in video
generation, making them promising for autonomous driving world modeling.
However, existing video diffusion-based world models struggle with
flexible-length, long-horizon predictions and integrating trajectory planning.
This is because conventional video diffusion models rely on global joint
distribution modeling of fixed-length frame sequences rather than sequentially
constructing localized distributions at each timestep. In this work, we propose
Epona, an autoregressive diffusion world model that enables localized
spatiotemporal distribution modeling through two key innovations: 1) Decoupled
spatiotemporal factorization that separates temporal dynamics modeling from
fine-grained future world generation, and 2) Modular trajectory and video
prediction that seamlessly integrate motion planning with visual modeling in an
end-to-end framework. Our architecture enables high-resolution, long-duration
generation while introducing a novel chain-of-forward training strategy to
address error accumulation in autoregressive loops. Experimental results
demonstrate state-of-the-art performance with 7.4\% FVD improvement and minutes
longer prediction duration compared to prior works. The learned world model
further serves as a real-time motion planner, outperforming strong end-to-end
planners on NAVSIM benchmarks. Code will be publicly available at
\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.

</details>


### [349] [TextMesh4D: High-Quality Text-to-4D Mesh Generation](https://arxiv.org/abs/2506.24121)
*Sisi Dai,Xinxin Su,Boyan Wan,Ruizhen Hu,Kai Xu*

Main category: cs.CV

TL;DR: TextMesh4D introduces a novel framework for generating dynamic 3D content (text-to-4D) using diffusion models, achieving new state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Exploring the largely unaddressed challenge of generating dynamic 3D content (4D) from text prompts using diffusion guidance.

Method: TextMesh4D employs per-face Jacobians for mesh representation, decomposes 4D generation into object creation and motion synthesis, and adds a flexibility-rigidity regularization term for stability.

Result: The model demonstrates state-of-the-art temporal consistency, structural fidelity, and visual realism while being cost-effective, requiring only a single 24GB GPU.

Conclusion: TextMesh4D provides a robust, efficient solution to text-to-4D mesh generation, setting a new benchmark and releasing code for further research.

Abstract: Recent advancements in diffusion generative models significantly advanced
image, video, and 3D content creation from user-provided text prompts. However,
the challenging problem of dynamic 3D content generation (text-to-4D) with
diffusion guidance remains largely unexplored. In this paper, we introduce
TextMesh4D, a novel framework for high-quality text-to-4D generation. Our
approach leverages per-face Jacobians as a differentiable mesh representation
and decomposes 4D generation into two stages: static object creation and
dynamic motion synthesis. We further propose a flexibility-rigidity
regularization term to stabilize Jacobian optimization under video diffusion
priors, ensuring robust geometric performance. Experiments demonstrate that
TextMesh4D achieves state-of-the-art results in terms of temporal consistency,
structural fidelity, and visual realism. Moreover, TextMesh4D operates with a
low GPU memory overhead-requiring only a single 24GB GPU-offering a
cost-effective yet high-quality solution for text-driven 4D mesh generation.
The code will be released to facilitate future research in text-to-4D
generation.

</details>


### [350] [Calligrapher: Freestyle Text Image Customization](https://arxiv.org/abs/2506.24123)
*Yue Ma,Qingyan Bai,Hao Ouyang,Ka Leong Cheng,Qiuyu Wang,Hongyu Liu,Zichen Liu,Haofan Wang,Jingye Chen,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: The paper introduces Calligrapher, a system that integrates text customization and typography using diffusion-based methods for flexible calligraphy and design tasks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in precisely controlling style and reducing data dependency in typographic customization for digital design.

Method: The approach involves three innovations: a self-distillation mechanism to create a style-centric typography benchmark, a trainable style encoder for localized style injection, and in-context generation to embed reference images during the denoising process.

Result: Calligrapher demonstrates accurate reproduction of stylistic details and glyph positioning, validated through extensive evaluations across various fonts and designs.

Conclusion: The framework surpasses traditional models by streamlining typography creation, aiding professionals in digital art, branding, and contextual design.

Abstract: We introduce Calligrapher, a novel diffusion-based framework that
innovatively integrates advanced text customization with artistic typography
for digital calligraphy and design applications. Addressing the challenges of
precise style control and data dependency in typographic customization, our
framework incorporates three key technical contributions. First, we develop a
self-distillation mechanism that leverages the pre-trained text-to-image
generative model itself alongside the large language model to automatically
construct a style-centric typography benchmark. Second, we introduce a
localized style injection framework via a trainable style encoder, which
comprises both Qformer and linear layers, to extract robust style features from
reference images. An in-context generation mechanism is also employed to
directly embed reference images into the denoising process, further enhancing
the refined alignment of target styles. Extensive quantitative and qualitative
evaluations across diverse fonts and design contexts confirm Calligrapher's
accurate reproduction of intricate stylistic details and precise glyph
positioning. By automating high-quality, visually consistent typography,
Calligrapher surpasses traditional models, empowering creative practitioners in
digital art, branding, and contextual typographic design.

</details>


### [351] [FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation](https://arxiv.org/abs/2506.24125)
*Jiacheng Cui,Xinyue Bi,Yaxin Luo,Xiaohan Zhao,Jiacheng Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: The paper introduces Data Residual Matching (FADRM), which applies residual connections at the data level to improve dataset distillation. The method outperforms existing approaches with enhanced efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the potential of residual connections in data-centric approaches, specifically dataset distillation, where traditional methods face challenges such as vanishing data information and slow training.

Method: This work introduces Data Residual Matching, leveraging data-level skip connections and optimization refinements to balance pixel space optimization with core information identification. The method improves computational efficiency and resource usage.

Result: FADRM achieves state-of-the-art results, with notable benchmarks like 47.7% test accuracy for single-model and 50.0% for multi-model dataset distillation in ImageNet-1K, surpassing methods like RDED, EDC, and CV-DD.

Conclusion: The proposed FADRM method enhances dataset distillation by improving efficiency, reducing training costs (GPU memory by 50%), and delivering superior accuracy, making it a new state-of-the-art approach.

Abstract: Residual connection has been extensively studied and widely applied at the
model architecture level. However, its potential in the more challenging
data-centric approaches remains unexplored. In this work, we introduce the
concept of Data Residual Matching for the first time, leveraging data-level
skip connections to facilitate data generation and mitigate data information
vanishing. This approach maintains a balance between newly acquired knowledge
through pixel space optimization and existing core local information
identification within raw data modalities, specifically for the dataset
distillation task. Furthermore, by incorporating optimization-level
refinements, our method significantly improves computational efficiency,
achieving superior performance while reducing training time and peak GPU memory
usage by 50%. Consequently, the proposed method Fast and Accurate Data Residual
Matching for Dataset Distillation (FADRM) establishes a new state-of-the-art,
demonstrating substantial improvements over existing methods across multiple
dataset benchmarks in both efficiency and effectiveness. For instance, with
ResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the
method achieves 47.7% test accuracy in single-model dataset distillation and
50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and
outperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%
and +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.

</details>


### [352] [How to Design and Train Your Implicit Neural Representation for Video Compression](https://arxiv.org/abs/2506.24127)
*Matthew Gwilliam,Roy Zhang,Namitha Padmanabhan,Hongyang Du,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: This paper proposes Rabbit NeRV (RNeRV), an improved implicit neural representation (INR) architecture for video compression, enhancing visual quality, compression, and encoding speed.


<details>
  <summary>Details</summary>
Motivation: To address the slow encoding speed of existing INR-based video compression methods and improve performance in terms of visual quality and compression efficiency.

Method: The authors created a library to analyze the NeRV family, uncover components for effective INR design, and proposed RNeRV for improved compression. They also explored hyper-networks to enable real-time encoding using INR weight prediction and weight masking techniques.

Result: RNeRV achieved +1.27% average PSNR improvement over competitors for 1080p UVG videos when trained equally. With hyper-networks and masking, it achieved 1.7%-2.7% PSNR/MS-SSIM improvements at similar bpp and speed.

Conclusion: RNeRV demonstrates state-of-the-art video compression quality with improved encoding speeds using hyper-network innovations, enhancing practical viability.

Abstract: Implicit neural representation (INR) methods for video compression have
recently achieved visual quality and compression ratios that are competitive
with traditional pipelines. However, due to the need for per-sample network
training, the encoding speeds of these methods are too slow for practical
adoption. We develop a library to allow us to disentangle and review the
components of methods from the NeRV family, reframing their performance in
terms of not only size-quality trade-offs, but also impacts on training time.
We uncover principles for effective video INR design and propose a
state-of-the-art configuration of these components, Rabbit NeRV (RNeRV). When
all methods are given equal training time (equivalent to 300 NeRV epochs) for 7
different UVG videos at 1080p, RNeRV achieves +1.27% PSNR on average compared
to the best-performing alternative for each video in our NeRV library. We then
tackle the encoding speed issue head-on by investigating the viability of
hyper-networks, which predict INR weights from video inputs, to disentangle
training from encoding to allow for real-time encoding. We propose masking the
weights of the predicted INR during training to allow for variable, higher
quality compression, resulting in 1.7% improvements to both PSNR and MS-SSIM at
0.037 bpp on the UCF-101 dataset, and we increase hyper-network parameters by
0.4% for 2.5%/2.7% improvements to PSNR/MS-SSIM with equal bpp and similar
speeds. Our project website is available at https://mgwillia.github.io/vinrb/
and our code is available at https://github.com/mgwillia/vinrb.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [353] [Libra: Synergizing CUDA and Tensor Cores for High-Performance Sparse Matrix Multiplication](https://arxiv.org/abs/2506.22714)
*Jinliang Shi,Shigang Li,Youxuan Xu,Xueying Wang,Rongtian Fu,Zhi Ma,Tong Wu*

Main category: cs.DC

TL;DR: The paper introduces Libra, a system that improves sparse matrix multiplication (SpMM and SDDMM) using a combination of CUDA and Tensor cores for superior performance.


<details>
  <summary>Details</summary>
Motivation: Sparse matrix multiplication is critical in deep learning and scientific computing, but existing approaches underutilize GPU hardware resources, leading to inefficiencies.

Method: Libra uses a 2D-aware workload distribution strategy to optimize task mapping between CUDA and Tensor cores. It includes optimizations such as hybrid load-balancing, optimized kernel implementations, and GPU-accelerated preprocessing.

Result: Experimental results on H100 and RTX 4090 GPUs show a 3.1x average performance improvement (up to 9.23x) over DTC-SpMM and a 2.9x improvement (up to 3.9x) for GNN applications.

Conclusion: Libra exploits heterogeneous computing resources on GPUs effectively, providing a new perspective on accelerating sparse matrix operations.

Abstract: Sparse matrix multiplication operators (i.e., SpMM and SDDMM) are widely used
in deep learning and scientific computing. Modern accelerators are commonly
equipped with Tensor cores and CUDA cores to accelerate sparse operators. The
former brings superior computing power but only for structured matrix
multiplication, while the latter has relatively lower performance but with
higher programming flexibility. In this work, we discover that utilizing one
resource alone leads to inferior performance for sparse matrix multiplication,
due to their respective limitations. To this end, we propose Libra, a
systematic approach that enables synergistic computation between CUDA and
Tensor cores to achieve the best performance for sparse matrix multiplication.
Specifically, we propose a 2D-aware workload distribution strategy to find out
the sweet point of task mapping for different sparse operators, leveraging both
the high performance of Tensor cores and the low computational redundancy on
CUDA cores. In addition, Libra incorporates systematic optimizations for
heterogeneous computing, including hybrid load-balancing, finely optimized
kernel implementations, and GPU-accelerated preprocessing. Extensive
experimental results on H100 and RTX 4090 GPUs show that Libra outperforms the
state-of-the-art by on average 3.1x (up to 9.23x) over DTC-SpMM and 2.9x (up to
3.9x) for end-to-end GNN applications. Libra opens up a new perspective for
sparse operator acceleration by fully exploiting the heterogeneous computing
resources on GPUs.

</details>


### [354] [Not All Water Consumption Is Equal: A Water Stress Weighted Metric for Sustainable Computing](https://arxiv.org/abs/2506.22773)
*Yanran Wu,Inez Hua,Yi Ding*

Main category: cs.DC

TL;DR: This paper introduces SCARF, a framework that evaluates computing's water impact by incorporating spatial and temporal water stress variations, and demonstrates its effectiveness through various case studies.


<details>
  <summary>Details</summary>
Motivation: With AI workloads scaling rapidly, water sustainability has become a critical issue, but existing assessments fail to consider variations in water stress based on location and time.

Method: The authors developed SCARF, which includes the Adjusted Water Impact (AWI) metric to measure water impact by accounting for both consumption volume and dynamic local water stress across locations and times.

Result: Through case studies on large language model serving, datacenters, and semiconductor manufacturing, SCARF revealed opportunities for reducing water impact by optimizing location and timing.

Conclusion: This work highlights the potential of addressing spatial and temporal water stress variations to achieve water-sustainable computing, providing a valuable tool through SCARF for advancing sustainability.

Abstract: Water consumption is an increasingly critical dimension of computing
sustainability, especially as AI workloads rapidly scale. However, current
water impact assessment often overlooks where and when water stress is more
severe. To fill in this gap, we present SCARF, the first general framework that
evaluates water impact of computing by factoring in both spatial and temporal
variations in water stress. SCARF calculates an Adjusted Water Impact (AWI)
metric that considers both consumption volume and local water stress over time.
Through three case studies on LLM serving, datacenters, and semiconductor
fabrication plants, we show the hidden opportunities for reducing water impact
by optimizing location and time choices, paving the way for water-sustainable
computing. The code is available at https://github.com/jojacola/SCARF.

</details>


### [355] [TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations](https://arxiv.org/abs/2506.22818)
*Stanislav Sedukhin,Yoichi Tomioka,Kazuya Matsumoto,Yuichi Okuyama*

Main category: cs.DC

TL;DR: The paper presents TriADA, a scalable and energy-efficient architecture for trilinear tensor operations to improve computation in AI and HPC workloads.


<details>
  <summary>Details</summary>
Motivation: Address the computational and memory inefficiencies, alongside energy constraints, in handling multilinear transformations for sparse data in AI and HPC tasks.

Method: Proposed TriADA, including a low-rank algorithm, a new GEMM kernel, a distributed 3D network architecture, and an elastic sparse outer-product method.

Result: TriADA achieves energy-efficient, scalable, and massively parallel tensor operations with hypercubic arithmetic complexity.

Conclusion: TriADA offers an advanced solution tailored for high-performance tensor processing, making it suitable for the critical workloads in AI and HPC applications.

Abstract: Multilinear transformations are key in high-performance computing (HPC) and
artificial intelligence (AI) workloads, where data is represented as tensors.
However, their high computational and memory demands, which grow with
dimensionality, often slow down critical tasks. Moreover, scaling computation
by enlarging the number of parallel processing units substantially increases
energy consumption, limiting widespread adoption, especially for sparse data,
which is common in HPC and AI applications. This paper introduces the Trilinear
Algorithm and isomorphic to algorithm Device Architecture (TriADA) to address
these challenges with the following innovations: (1) a massively parallel,
low-rank algorithm for computing a family of trilinear (3D) discrete orthogonal
transformations (3D-DXTs), which is a special case of the more general 3-mode
matrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM
kernel with decoupled streaming active memory, specially designed to accelerate
3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully
distributed 3D network of mesh interconnected processing elements or cells with
a coordinate-free, data-driven local processing activity, which is independent
of problem size; (4) an elastic sparse outer-product (ESOP) method that avoids
unnecessary computing and communication operations with zero-valued operands,
thereby enhancing energy efficiency, computational accuracy, and stability.
TriADA is capable of performing a variety of trilinear transformations with
hypercubic arithmetic complexity in a linear number of time-steps. The
massively parallel, scalable, and energy-efficient architecture of TriADA is
ideal for accelerating multilinear tensor operations, which are the most
demanding parts of AI and HPC workloads.

</details>


### [356] [Performance Measurements in the AI-Centric Computing Continuum Systems](https://arxiv.org/abs/2506.22884)
*Praveen Kumar Donta,Qiyang Zhang,Schahram Dustdar*

Main category: cs.DC

TL;DR: This paper reviews traditional performance metrics in Distributed Computing Continuum (DCC) and Internet of Things (IoT) environments, proposes new metrics addressing sustainability, energy efficiency, and observability, and offers criteria to inspire future research.


<details>
  <summary>Details</summary>
Motivation: The computing paradigm has evolved into a Distributed Computing Continuum where computational demands have intensified due to Generative AI, requiring new approaches to performance measurement.

Method: The paper reviews traditional performance metrics used in DCC and IoT systems, while introducing emerging dimensions of performance and outlining criteria for metric selection.

Result: Emerging performance aspects such as sustainability, energy efficiency, and system observability are highlighted to meet the evolving needs of Generative AI and large-scale applications.

Conclusion: Expanding performance metrics in DCC and IoT is essential for efficiency and aligning computational resources with future application needs.

Abstract: Over the Eight decades, computing paradigms have shifted from large,
centralized systems to compact, distributed architectures, leading to the rise
of the Distributed Computing Continuum (DCC). In this model, multiple layers
such as cloud, edge, Internet of Things (IoT), and mobile platforms work
together to support a wide range of applications. Recently, the emergence of
Generative AI and large language models has further intensified the demand for
computational resources across this continuum. Although traditional performance
metrics have provided a solid foundation, they need to be revisited and
expanded to keep pace with changing computational demands and application
requirements. Accurate performance measurements benefit both system designers
and users by supporting improvements in efficiency and promoting alignment with
system goals. In this context, we review commonly used metrics in DCC and IoT
environments. We also discuss emerging performance dimensions that address
evolving computing needs, such as sustainability, energy efficiency, and system
observability. We also outline criteria and considerations for selecting
appropriate metrics, aiming to inspire future research and development in this
critical area.

</details>


### [357] [FastSet: Parallel Claim Settlement](https://arxiv.org/abs/2506.23395)
*Xiaohong Chen,Grigore Rosu*

Main category: cs.DC

TL;DR: FastSet is a decentralized protocol for finance and settlement that offers blockchain-like benefits but avoids requiring strong consistency.


<details>
  <summary>Details</summary>
Motivation: Existing decentralized systems like blockchains face bottlenecks due to the requirement of strong consistency, which limits scalability and performance.

Method: FastSet employs an actor-based protocol where account holders make signed claims that validators settle independently, allowing parallel operations without validators needing to communicate.

Result: The protocol ensures correctness and preserves many blockchain benefits despite not enforcing strong consistency.

Conclusion: FastSet delivers a scalable, decentralized settlement protocol that achieves significant blockchain functionalities in a more efficient way.

Abstract: FastSet is an actor-based distributed protocol for decentralized finance and
settlement, which is inspired from blockchains. Account holders cooperate by
making claims, which can include payments, holding and transferring assets,
accessing and updating shared data, medical records, digital identity, and
mathematical theorems, among many others. The claims are signed by their owners
and are broadcast to a decentralized network of validators, which validate and
settle them. Validators replicate the global state of the accounts and need not
communicate with each other. In sharp contrast to blockchains, strong
consistency is purposely given up as a requirement. Yet, many if not most of
the blockchain benefits are preserved. The protocol is proved to be correct,
despite its massively parallel nature.

</details>


### [358] [Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model](https://arxiv.org/abs/2506.23635)
*Mu-Chi Chen,Po-Hsuan Huang,Xiangrui Ke,Chia-Heng Tu,Chun Jason Xue,Shih-Hao Hung*

Main category: cs.DC

TL;DR: The paper explores cost-efficient solutions for hosting private LLM systems, using Apple's M2 Ultra chips in a Mac Studio cluster to accelerate the DBRX model with MoE architecture.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle cost and scalability challenges in building private LLM systems for smaller-scale applications.

Method: A Mac Studio cluster with Apple's M2 Ultra chips was utilized to host and accelerate the DBRX model, investigating parallel execution and optimizing software stack memory management.

Result: The cluster showed significant inference time reductions, was 1.15x more cost-efficient vs NVIDIA H100 GPU supercomputers, and highlighted the importance of network latency over bandwidth.

Conclusion: The results provide insights for efficient system design, and optimizations make the Mac Studio cluster a competitive option for private LLM system deployment.

Abstract: Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)
with significant advancements such as OpenAI's ChatGPT, Meta's Llama, and
Databricks' DBRX. This paper addresses the cost and scalability challenges
encountered when constructing private LLM systems for personal or small group
services, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2
Ultra chips is established as a cost-efficient solution to host and accelerate
the pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our
performance analysis reveal that parallel execution of the model's experts
across two to four machine nodes significantly reduces inference time. We find
that computation time for the experts is comparable to the communication time
for exchanging their outputs, emphasizing the importance of network latency
over bandwidth. We also observe significant management overhead due to Apple
software stack's memory management logic. Based on these findings, we develop
optimization schemes to eliminate the memory management overhead. As a result,
the Mac Studio cluster is 1.15 times more cost-efficient than the
state-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we
construct a performance model to estimate system performance under varying
configurations, and the model provides valuable insights for designing private
LLM systems.

</details>


### [359] [Large-scale Neural Network Quantum States for ab initio Quantum Chemistry Simulations on Fugaku](https://arxiv.org/abs/2506.23809)
*Hongtao Xu,Zibo Wu,Mingzhen Li,Weile Jia*

Main category: cs.DC

TL;DR: The paper introduces a novel framework, \ours, designed to overcome scalability bottlenecks in training neural network quantum states (NQS) for quantum chemistry problems, achieving significant speedups and parallel efficiency on large-scale systems.


<details>
  <summary>Details</summary>
Motivation: Address the exponential growth in computational demands and scalability barriers when training neural network quantum states in large-scale quantum chemistry problems.

Method: Proposed a scalable sampling parallelism strategy, introduced multi-level parallelism for efficient local energy computation, and used cache-centric optimization for transformer-based ansatz to enhance training speed and maintain stability.

Result: Achieved up to 8.41x acceleration in NQS training and a parallel efficiency of 95.8% when using 1,536 nodes.

Conclusion: \ours successfully breaks scalability barriers in NQS training, significantly improving performance and enabling practical applications in large-scale quantum chemistry computations.

Abstract: Solving quantum many-body problems is one of the fundamental challenges in
quantum chemistry. While neural network quantum states (NQS) have emerged as a
promising computational tool, its training process incurs exponentially growing
computational demands, becoming prohibitively expensive for large-scale
molecular systems and creating fundamental scalability barriers for real-world
applications. To address above challenges, we present \ours, a high-performance
NQS training framework for \textit{ab initio} electronic structure
calculations. First, we propose a scalable sampling parallelism strategy with
multi-layers workload division and hybrid sampling scheme, which break the
scalability barriers for large-scale NQS training. Then, we introduce
multi-level parallelism local energy parallelism, enabling more efficient local
energy computation. Last, we employ cache-centric optimization for
transformer-based \textit{ansatz} and incorporate it with sampling parallelism
strategy, which further speedup up the NQS training and achieve stable memory
footprint at scale. Experiments demonstrate that \ours accelerate NQS training
with up to 8.41x speedup and attains a parallel efficiency up to 95.8\% when
scaling to 1,536 nodes.

</details>


### [360] [QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference](https://arxiv.org/abs/2506.23934)
*Xiangchen Li,Saeid Ghafouri,Bo Ji,Hans Vandierendonck,Deepu John,Dimitrios S. Nikolopoulos*

Main category: cs.DC

TL;DR: The paper introduces a dynamic inference system for edge devices, jointly optimizing model quantization and partitioning based on device constraints, achieving significant efficiency improvements while minimizing accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Machine learning inferences on edge devices demand adaptation to computational capabilities and constraints. A fixed pre-trained model is often inefficient for diverse scenarios, leading to a need for tailored, request-specific models that are cost-effective and adaptable.

Method: The authors develop a system that integrates joint model quantization and inference partitioning. This system dynamically adapts to edge devices by optimizing quantization bit widths and partition points using a novel optimization framework that minimizes time and cost while maintaining task accuracy.

Result: Simulations show that the proposed approach reduces computation payloads by over 80%, significantly decreases time and energy consumption, and keeps accuracy degradation below 1%.

Conclusion: The proposed inference system offers an efficient and robust solution for edge-device ML inferences, balancing workload and accuracy while saving energy and computational resources, marking a notable advancement in optimizing inference-serving systems.

Abstract: As machine learning inferences increasingly move to edge devices, adapting to
diverse computational capabilities, hardware, and memory constraints becomes
more critical. Instead of relying on a pre-trained model fixed for all future
inference queries across diverse edge devices, we argue that planning an
inference pattern with a request-specific model tailored to the device's
computational capacity, accuracy requirements, and time constraints is more
cost-efficient and robust to diverse scenarios. To this end, we propose an
accuracy-aware and workload-balanced inference system that integrates joint
model quantization and inference partitioning. In this approach, the server
dynamically responds to inference queries by sending a quantized model and
adaptively sharing the inference workload with the device. Meanwhile, the
device's computational power, channel capacity, and accuracy requirements are
considered when deciding.
  Furthermore, we introduce a new optimization framework for the inference
system, incorporating joint model quantization and partitioning. Our approach
optimizes layer-wise quantization bit width and partition points to minimize
time consumption and cost while accounting for varying accuracy requirements of
tasks through an accuracy degradation metric in our optimization model. To our
knowledge, this work represents the first exploration of optimizing
quantization layer-wise bit-width in the inference serving system, by
introducing theoretical measurement of accuracy degradation. Simulation results
demonstrate a substantial reduction in overall time and power consumption, with
computation payloads decreasing by over 80% and accuracy degradation kept below
1%.

</details>


### [361] [Agent.xpu: Efficient Scheduling of Agentic LLM Workloads on Heterogeneous SoC](https://arxiv.org/abs/2506.24045)
*Xinming Wei,Jiahao Zhang,Haoran Li,Jiayu Chen,Rui Qu,Maoliang Li,Xiang Chen,Guojie Luo*

Main category: cs.DC

TL;DR: Agent.xpu is a serving system addressing the conflict between reactive and proactive LLM tasks on heterogeneous SoCs, optimizing both responsiveness and throughput.


<details>
  <summary>Details</summary>
Motivation: With the emergence of agentic LLMs on personal devices, managing concurrent reactive and proactive tasks efficiently on consumer-grade SoCs has become critical.

Method: Agent.xpu uses offline profiling to construct a heterogeneous execution graph and runtime scheduling with kernel-level preemption, slack-aware kernel backfill, and bandwidth-aware dispatch.

Result: On Intel Core Ultra SoC, Agent.xpu delivers 4.6× lower latency for reactive tasks and improves proactive task throughput by 1.6×-6.8× compared to existing methods.

Conclusion: Agent.xpu successfully optimizes execution for agentic LLM workloads on heterogeneous SoCs, balancing low-latency responses and high throughput.

Abstract: The proliferation of agentic Large Language Models (LLMs) on personal devices
introduces a new class of workloads characterized by a dichotomy of objectives.
Reactive tasks, initiated by users, demand immediate, low-latency responses,
while proactive tasks operate invisibly and prioritize throughput. Existing
on-device LLM engines, designed for isolated inferences, fail to efficiently
manage these concurrent and conflicting requests on consumer-grade
heterogeneous SoCs with CPU, integrated GPU, and NPU. This paper introduces
Agent.xpu, an efficient serving system for agentic LLM workloads on
memory-unified heterogeneous SoCs. With dedicated offline profiling, Agent.xpu
first constructs a heterogeneous execution graph, which fuses and chunks model
kernels for affinity-guided, elastic accelerator mapping with predictive kernel
annotation. At runtime, its online scheduler enables fine-grained, kernel-level
preemption to guarantee the responsiveness of reactive tasks. To maximize SoC
utilization, it adopts slack-aware kernel backfill to opportunistically append
proactive tasks, and mitigates NPU-iGPU contention via bandwidth-aware
dispatch. Evaluation on an Intel Core Ultra SoC shows that Agent.xpu achieves
4.6$\times$ lower latency for reactive tasks and sustains
1.6$\times$-6.8$\times$ higher throughput for proactive tasks compared to
state-of-the-art inference engines.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [362] [Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation](https://arxiv.org/abs/2506.22441)
*Lei Yang*

Main category: cs.LG

TL;DR: The paper introduces a robust tensor factorization method (TDWLFT model) for imputing missing spatiotemporal traffic data that accounts for outliers using a unique loss function.


<details>
  <summary>Details</summary>
Motivation: Challenges in incomplete and corrupted datasets hinder the optimal functioning of ITS due to sensor malfunctions and communication failures during traffic data collection.

Method: A novel TDWLFT model incorporating a threshold distance weighted loss function to handle outliers in tensor factorization.

Result: Experiments demonstrated superior performance of TDWLFT over state-of-the-art methods in prediction accuracy and computational efficiency across two urban traffic speed datasets.

Conclusion: TDWLFT effectively addresses missing data problems in ITS by being robust to outliers, improving both data reliability and system efficiency.

Abstract: Intelligent transportation systems (ITS) rely heavily on complete and
high-quality spatiotemporal traffic data to achieve optimal performance.
Nevertheless, in real-word traffic data collection processes, issues such as
communication failures and sensor malfunctions often lead to incomplete or
corrupted datasets, thereby posing significant challenges to the advancement of
ITS. Among various methods for imputing missing spatiotemporal traffic data,
the latent factorization of tensors (LFT) model has emerged as a widely adopted
and effective solution. However, conventional LFT models typically employ the
standard L2-norm in their learning objective, which makes them vulnerable to
the influence of outliers. To overcome this limitation, this paper proposes a
threshold distance weighted (TDW) loss-incorporated Latent Factorization of
Tensors (TDWLFT) model. The proposed loss function effectively reduces the
model's sensitivity to outliers by assigning differentiated weights to
individual samples. Extensive experiments conducted on two traffic speed
datasets sourced from diverse urban environments confirm that the proposed
TDWLFT model consistently outperforms state-of-the-art approaches in terms of
both in both prediction accuracy and computational efficiency.

</details>


### [363] [Features-based embedding or Feature-grounding](https://arxiv.org/abs/2506.22442)
*Piotr Makarevich*

Main category: cs.LG

TL;DR: This paper explores replicating human structured conceptual thinking in deep learning models using feature-grounded embeddings.


<details>
  <summary>Details</summary>
Motivation: To reproduce structured reasoning based on human conceptual categories in deep learning models.

Method: The paper proposes creating feature-grounded embeddings linked to interpretable domain-specific conceptual features.

Result: Introduced a new approach for aligning shareable representations with specific features relevant to an operable dictionary.

Conclusion: Feature-grounded embeddings can potentially make deep learning models better align with human-like structured thinking.

Abstract: In everyday reasoning, when we think about a particular object, we associate
it with a unique set of expected properties such as weight, size, or more
abstract attributes like density or horsepower. These expectations are shaped
by our prior knowledge and the conceptual categories we have formed through
experience. This paper investigates how such knowledge-based structured
thinking can be reproduced in deep learning models using features based
embeddings. Specially, it introduces an specific approach to build
feature-grounded embedding, aiming to align shareable representations of
operable dictionary with interpretable domain-specific conceptual features.

</details>


### [364] [Learning Interpretable Rules from Neural Networks: Neurosymbolic AI for Radar Hand Gesture Recognition](https://arxiv.org/abs/2506.22443)
*Sarah Seifi,Tobias Sukianto,Cecilia Carbonelli,Lorenzo Servadei,Robert Wille*

Main category: cs.LG

TL;DR: RL-Net is a neuro-symbolic model that learns interpretable rules for radar-based hand gesture recognition, balancing high performance with reduced complexity compared to rule-based and explainable black-box models.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between interpretability of rule-based models and the high performance of deep neural networks.

Method: Developed and benchmarked RL-Net, a neuro-symbolic rule learning neural network, against rule-based and explainable models, focusing on HGR tasks.

Result: RL-Net achieved strong performance (93.03% F1) with reduced rule complexity and identified optimization challenges like rule pruning and hierarchy bias.

Conclusion: RL-Net balances transparency and robust performance, proving neuro-symbolic models feasible for explainable AI in edge-deployable applications like HGR.

Abstract: Rule-based models offer interpretability but struggle with complex data,
while deep neural networks excel in performance yet lack transparency. This
work investigates a neuro-symbolic rule learning neural network named RL-Net
that learns interpretable rule lists through neural optimization, applied for
the first time to radar-based hand gesture recognition (HGR). We benchmark
RL-Net against a fully transparent rule-based system (MIRA) and an explainable
black-box model (XentricAI), evaluating accuracy, interpretability, and user
adaptability via transfer learning. Our results show that RL-Net achieves a
favorable trade-off, maintaining strong performance (93.03% F1) while
significantly reducing rule complexity. We identify optimization challenges
specific to rule pruning and hierarchy bias and propose stability-enhancing
modifications. Compared to MIRA and XentricAI, RL-Net emerges as a practical
middle ground between transparency and performance. This study highlights the
real-world feasibility of neuro-symbolic models for interpretable HGR and
offers insights for extending explainable AI to edge-deployable sensing
systems.

</details>


### [365] [Active Learning for Forecasting Severity among Patients with Post Acute Sequelae of SARS-CoV-2](https://arxiv.org/abs/2506.22444)
*Jing Wang,Amar Sra,Jeremy C. Weiss*

Main category: cs.LG

TL;DR: The paper introduces a cohort and a deep learning model to better predict clinical risks and progression events for PASC patients.


<details>
  <summary>Details</summary>
Motivation: Traditional models fail to effectively capture the nuanced progression of PASC, creating challenges in healthcare resource allocation and patient management.

Method: The study presents an Active Attention Network integrating human expertise and active learning, combined with text features derived from a large language model and expert clinical annotation.

Result: The proposed model successfully predicts clinical risks and identifies progression events with a reduced need for annotations.

Conclusion: This approach improves clinical risk prediction accuracy and facilitates better decision-making and care for PASC patients.

Abstract: The long-term effects of Postacute Sequelae of SARS-CoV-2, known as PASC,
pose a significant challenge to healthcare systems worldwide. Accurate
identification of progression events, such as hospitalization and reinfection,
is essential for effective patient management and resource allocation. However,
traditional models trained on structured data struggle to capture the nuanced
progression of PASC. In this study, we introduce the first publicly available
cohort of 18 PASC patients, with text time series features based on Large
Language Model Llama-3.1-70B-Instruct and clinical risk annotated by clinical
expert. We propose an Active Attention Network to predict the clinical risk and
identify progression events related to the risk. By integrating human expertise
with active learning, we aim to enhance clinical risk prediction accuracy and
enable progression events identification with fewer number of annotation. The
ultimate goal is to improves patient care and decision-making for SARS-CoV-2
patient.

</details>


### [366] [Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security](https://arxiv.org/abs/2506.22445)
*Saad Alqithami*

Main category: cs.LG

TL;DR: This paper introduces the HAMARL framework to bolster cybersecurity in Cyber-Physical Systems via hierarchical multi-agent reinforcement learning and adversarial training.


<details>
  <summary>Details</summary>
Motivation: As Cyber-Physical Systems become more connected and vulnerable to advanced cyber threats, existing security approaches are proving insufficient.

Method: The HAMARL framework uses a hierarchical multi-agent setup, local agents for subsystem security, a global coordinator for overall defense, and adversarial training to anticipate cyber threats.

Result: Experimental evaluations on an industrial IoT testbed show HAMARL significantly improves attack detection, response times, and operational continuity compared to traditional methods.

Conclusion: Integrating hierarchical multi-agent coordination with adversarial-aware training offers enhanced resilience and security for next-generation Cyber-Physical Systems.

Abstract: Cyber-Physical Systems play a critical role in the infrastructure of various
sectors, including manufacturing, energy distribution, and autonomous
transportation systems. However, their increasing connectivity renders them
highly vulnerable to sophisticated cyber threats, such as adaptive and zero-day
attacks, against which traditional security methods like rule-based intrusion
detection and single-agent reinforcement learning prove insufficient. To
overcome these challenges, this paper introduces a novel Hierarchical
Adversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.
HAMARL employs a hierarchical structure consisting of local agents dedicated to
subsystem security and a global coordinator that oversees and optimizes
comprehensive, system-wide defense strategies. Furthermore, the framework
incorporates an adversarial training loop designed to simulate and anticipate
evolving cyber threats, enabling proactive defense adaptation. Extensive
experimental evaluations conducted on a simulated industrial IoT testbed
indicate that HAMARL substantially outperforms traditional multi-agent
reinforcement learning approaches, significantly improving attack detection
accuracy, reducing response times, and ensuring operational continuity. The
results underscore the effectiveness of combining hierarchical multi-agent
coordination with adversarially-aware training to enhance the resilience and
security of next-generation CPS.

</details>


### [367] [EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis](https://arxiv.org/abs/2506.22446)
*Aakash Tripathi,Asim Waqas,Matthew B. Schabath,Yasin Yilmaz,Ghulam Rasool*

Main category: cs.LG

TL;DR: The paper introduces EAGLE, a deep learning framework for cancer survival prediction using multimodal data fusion, achieving high accuracy, scalability, and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing multimodal cancer survival prediction methods such as simplistic fusion strategies, heavy computational requirements, and lack of interpretability, which hinder clinical adoption.

Method: EAGLE employs a novel deep learning framework with four key features: dynamic cross-modal attention, significant dimensionality reduction, comprehensive patient-level interpretability, and a unified adaptation pipeline across cancer types.

Result: EAGLE was tested on data from 911 patients across three cancer types, demonstrating accurate risk stratification and identifying actionable differences in median survival among risk groups (4- to 5-fold differences), with insights into modality contributions by risk level.

Conclusion: EAGLE successfully combines cutting-edge predictive performance with interpretability and scalability, making it a viable tool for clinical deployment in cancer survival prediction.

Abstract: Accurate cancer survival prediction requires integration of diverse data
modalities that reflect the complex interplay between imaging, clinical
parameters, and textual reports. However, existing multimodal approaches suffer
from simplistic fusion strategies, massive computational requirements, and lack
of interpretability-critical barriers to clinical adoption. We present EAGLE
(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning
framework that addresses these limitations through attention-based multimodal
fusion with comprehensive attribution analysis. EAGLE introduces four key
innovations: (1) dynamic cross-modal attention mechanisms that learn
hierarchical relationships between modalities, (2) massive dimensionality
reduction (99.96%) while maintaining predictive performance, (3) three
complementary attribution methods providing patient-level interpretability, and
(4) a unified pipeline enabling seamless adaptation across cancer types. We
evaluated EAGLE on 911 patients across three distinct malignancies:
glioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,
n=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis
showed high-risk individuals relied more heavily on adverse imaging features,
while low-risk patients demonstrated balanced modality contributions. Risk
stratification identified clinically meaningful groups with 4-fold (GBM) to
5-fold (NSCLC) differences in median survival, directly informing treatment
intensity decisions. By combining state-of-the-art performance with clinical
interpretability, EAGLE bridges the gap between advanced AI capabilities and
practical healthcare deployment, offering a scalable solution for multimodal
survival prediction that enhances both prognostic accuracy and physician trust
in automated predictions.

</details>


### [368] [FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision](https://arxiv.org/abs/2506.22771)
*Jingxiao Ma,Priyadarshini Panda,Sherief Reda*

Main category: cs.LG

TL;DR: The paper explores an INT8 quantized training strategy based on the Forward-Forward (FF) algorithm to address inefficiencies of backpropagation on edge devices, achieving faster training, energy savings, and reduced memory.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies of backpropagation in terms of time, energy, and memory, particularly for resource-constrained edge devices.

Method: The authors use an INT8 quantized training method combined with the FF algorithm and introduce a 'look-ahead' scheme to bypass limitations of FF and stabilize gradient quantization.

Result: The proposed approach achieved 4.6% faster training, 8.3% energy savings, and a 27.0% reduction in memory usage on the NVIDIA Jetson Orin Nano board, maintaining competitive accuracy.

Conclusion: The combination of FF with INT8 quantization and the 'look-ahead' enhancement demonstrates significant advantages in performance, efficiency, and suitability for embedded systems while retaining accurate results.

Abstract: Backpropagation has been the cornerstone of neural network training for
decades, yet its inefficiencies in time and energy consumption limit its
suitability for resource-constrained edge devices. While low-precision neural
network quantization has been extensively researched to speed up model
inference, its application in training has been less explored. Recently, the
Forward-Forward (FF) algorithm has emerged as a promising alternative to
backpropagation, replacing the backward pass with an additional forward pass.
By avoiding the need to store intermediate activations for backpropagation, FF
can reduce memory footprint, making it well-suited for embedded devices. This
paper presents an INT8 quantized training approach that leverages FF's
layer-by-layer strategy to stabilize gradient quantization. Furthermore, we
propose a novel "look-ahead" scheme to address limitations of FF and improve
model accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board
demonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in
memory usage, while maintaining competitive accuracy compared to the
state-of-the-art.

</details>


### [369] [Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture](https://arxiv.org/abs/2506.22447)
*Fabio Merizzi,Harilaos Loukos*

Main category: cs.LG

TL;DR: This paper introduces a multi-variable Vision Transformer (ViT) architecture to improve climate variable downscaling, addressing shortcomings in single-variable models.


<details>
  <summary>Details</summary>
Motivation: Single-variable downscaling lacks contextual interactions between climate variables and is computationally inefficient.

Method: The study employs a shared encoder and variable-specific decoders within a Vision Transformer architecture to predict three climate variables simultaneously.

Result: The proposed model demonstrates positive cross-variable knowledge transfer, surpasses single-variable baselines in accuracy, and offers increased computational efficiency.

Conclusion: Multi-variable modeling is effective for high-resolution climate downscaling, providing improved performance and efficiency over traditional methods.

Abstract: Global Climate Models (GCMs) are critical for simulating large-scale climate
dynamics, but their coarse spatial resolution limits their applicability in
regional studies. Regional Climate Models (RCMs) refine this through dynamic
downscaling, albeit at considerable computational cost and with limited
flexibility. While deep learning has emerged as an efficient data-driven
alternative, most existing studies have focused on single-variable models that
downscale one variable at a time. This approach can lead to limited contextual
awareness, redundant computation, and lack of cross-variable interaction. Our
study addresses these limitations by proposing a multi-task, multi-variable
Vision Transformer (ViT) architecture with a shared encoder and
variable-specific decoders (1EMD). The proposed architecture jointly predicts
three key climate variables: surface temperature (tas), wind speed (sfcWind),
and 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,
emulating RCM-scale downscaling over Europe. We show that our multi-variable
approach achieves positive cross-variable knowledge transfer and consistently
outperforms single-variable baselines trained under identical conditions, while
also improving computational efficiency. These results demonstrate the
effectiveness of multi-variable modeling for high-resolution climate
downscaling.

</details>


### [370] [Mirror Descent Policy Optimisation for Robust Constrained Markov Decision Processes](https://arxiv.org/abs/2506.23165)
*David Bossens,Atsushi Nitanda*

Main category: cs.LG

TL;DR: This paper introduces a mirror descent policy optimization approach for robust constrained Markov decision processes, achieving theoretical convergence guarantees and improved robustness over baseline methods.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for reinforcement learning systems to ensure safety and satisfy long-term constraints while providing robustness under epistemic uncertainty.

Method: The authors employ a mirror descent policy optimization approach for RCMDPs, using policy gradient techniques combined with adversarial optimization on the Lagrangian representing constrained MDPs. They analyze convergence rates under oracle-based and sample-based settings.

Result: Theoretical results show $
mathcal{O}\left(\frac{1}{T}\right)$ and $
mathcal{O}\left(e^{-T}\right)$ convergence in oracle-based settings, and $
tilde{\mathcal{O}}\left(\frac{1}{T^{1/3}}\right)$ in sample-based settings. Experiments demonstrate improvements in constrained optimization and robustness compared to baseline algorithms.

Conclusion: Mirror descent policy optimization provides robust and efficient policy learning under constraints, with benefits demonstrated theoretically and empirically.

Abstract: Safety is an essential requirement for reinforcement learning systems. The
newly emerging framework of robust constrained Markov decision processes allows
learning policies that satisfy long-term constraints while providing guarantees
under epistemic uncertainty. This paper presents mirror descent policy
optimisation for robust constrained Markov decision processes (RCMDPs), making
use of policy gradient techniques to optimise both the policy (as a maximiser)
and the transition kernel (as an adversarial minimiser) on the Lagrangian
representing a constrained MDP. In the oracle-based RCMDP setting, we obtain an
$\mathcal{O}\left(\frac{1}{T}\right)$ convergence rate for the squared distance
as a Bregman divergence, and an $\mathcal{O}\left(e^{-T}\right)$ convergence
rate for entropy-regularised objectives. In the sample-based RCMDP setting, we
obtain an $\tilde{\mathcal{O}}\left(\frac{1}{T^{1/3}}\right)$ convergence rate.
Experiments confirm the benefits of mirror descent policy optimisation in
constrained and unconstrained optimisation, and significant improvements are
observed in robustness tests when compared to baseline policy optimisation
algorithms.

</details>


### [371] [Stabilization of industrial processes with time series machine learning](https://arxiv.org/abs/2506.22502)
*Matvei Anoshin,Olga Tsurkan,Vadim Lopatkin,Leonid Fedichkin*

Main category: cs.LG

TL;DR: This paper presents a machine learning pipeline for improving time series stabilization, featuring two neural networks that outperform traditional solvers in temperature control by 3x.


<details>
  <summary>Details</summary>
Motivation: The authors aimed to address the critical challenge of stabilizing time series processes in industrial applications for better outcomes with optimized computational resources.

Method: The paper introduces a pipeline consisting of two neural networks: an oracle predictor and an optimizer, shifting the approach from point-wise value optimization to neural network training.

Result: Their approach achieved a threefold improvement in temperature control stability compared to ordinary solvers.

Conclusion: The proposed neural network-based pipeline effectively stabilizes time series processes with impressive results, demonstrating its potential in industrial applications.

Abstract: The stabilization of time series processes is a crucial problem that is
ubiquitous in various industrial fields. The application of machine learning to
its solution can have a decisive impact, improving both the quality of the
resulting stabilization with less computational resources required. In this
work, we present a simple pipeline consisting of two neural networks: the
oracle predictor and the optimizer, proposing a substitution of the point-wise
values optimization to the problem of the neural network training, which
successfully improves stability in terms of the temperature control by about 3
times compared to ordinary solvers.

</details>


### [372] [Task-Agnostic Contrastive Pretraining for Relational Deep Learning](https://arxiv.org/abs/2506.22530)
*Jakub Peleška,Gustav Šír*

Main category: cs.LG

TL;DR: The paper introduces a contrastive pretraining approach for Relational Deep Learning (RDL) that learns transferable database-wide representations, improving the performance of predictive tasks via fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current RDL models rely on task-specific supervised learning, which limits scalability and reuse across multiple tasks.

Method: The study proposes task-agnostic pretraining using three levels of contrastive objectives (row-level, link-level, and context-level) designed to capture relational data's structural and semantic heterogeneity. A modular architecture and efficient sampling strategy are utilized.

Result: Preliminary benchmarks demonstrate that fine-tuned pretrained models outperform models trained from scratch, showcasing improved utility in relational data representation learning.

Conclusion: Contrastive pretraining for RDL holds significant potential for scalability and representation transferability in relational data applications, reducing the need for task-specific models.

Abstract: Relational Deep Learning (RDL) is an emerging paradigm that leverages Graph
Neural Network principles to learn directly from relational databases by
representing them as heterogeneous graphs. However, existing RDL models
typically rely on task-specific supervised learning, requiring training
separate models for each predictive task, which may hamper scalability and
reuse.
  In this work, we propose a novel task-agnostic contrastive pretraining
approach for RDL that enables database-wide representation learning. For that
aim, we introduce three levels of contrastive objectives$-$row-level,
link-level, and context-level$-$designed to capture the structural and semantic
heterogeneity inherent to relational data. We implement the respective
pretraining approach through a modular RDL architecture and an efficient
sampling strategy tailored to the heterogeneous database setting. Our
preliminary results on standard RDL benchmarks demonstrate that fine-tuning the
pretrained models measurably outperforms training from scratch, validating the
promise of the proposed methodology in learning transferable representations
for relational data.

</details>


### [373] [Exploration Behavior of Untrained Policies](https://arxiv.org/abs/2506.22566)
*Jacob Adamczyk*

Main category: cs.LG

TL;DR: This paper investigates how the architecture and initialization of untrained deep neural policies influence exploration in reinforcement learning before training begins.


<details>
  <summary>Details</summary>
Motivation: Exploration in RL is especially difficult in settings with sparse or adversarial rewards. The authors aim to better understand how policy architectures influence exploration during early training.

Method: By leveraging infinite-width network theory and continuous-time limits, the authors study the behavior of untrained policies, both theoretically and empirically, in a simplified model.

Result: The study demonstrates that untrained policies produce correlated actions, leading to non-trivial state visitation distributions, which can influence early exploration behavior.

Conclusion: Untrained policy initialization affects exploration behavior, offering a new perspective and tools to design policies that encourage effective exploration in RL environments.

Abstract: Exploration remains a fundamental challenge in reinforcement learning (RL),
particularly in environments with sparse or adversarial reward structures. In
this work, we study how the architecture of deep neural policies implicitly
shapes exploration before training. We theoretically and empirically
demonstrate strategies for generating ballistic or diffusive trajectories from
untrained policies in a toy model. Using the theory of infinite-width networks
and a continuous-time limit, we show that untrained policies return correlated
actions and result in non-trivial state-visitation distributions. We discuss
the distributions of the corresponding trajectories for a standard
architecture, revealing insights into inductive biases for tackling
exploration. Our results establish a theoretical and experimental framework for
using policy initialization as a design tool to understand exploration behavior
in early training.

</details>


### [374] [DistShap: Scalable GNN Explanations with Distributed Shapley Values](https://arxiv.org/abs/2506.22668)
*Selahattin Akkas,Aditya Devarakonda,Ariful Azad*

Main category: cs.LG

TL;DR: The paper introduces DistShap, a scalable method for explaining graph neural network (GNN) predictions using Shapley value-based explanations distributed across GPUs, achieving state-of-the-art accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: The increasing use of GNNs necessitates effective and scalable methods to explain predictions, which is challenging due to computational costs, especially for large-scale graphs.

Method: DistShap is a parallel algorithm that distributes Shapley value-based calculations for edge importance. It samples subgraphs, performs GNN inference in parallel on multiple GPUs, and solves a distributed least squares problem for edge attributions.

Result: DistShap provides more accurate explanation results compared to existing methods while scaling efficiently on large models by leveraging up to 128 GPUs on a supercomputer.

Conclusion: DistShap significantly advances the scalability and accuracy of GNN explanation methods, making it possible to interpret models with millions of features.

Abstract: With the growing adoption of graph neural networks (GNNs), explaining their
predictions has become increasingly important. However, attributing predictions
to specific edges or features remains computationally expensive. For example,
classifying a node with 100 neighbors using a 3-layer GNN may involve
identifying important edges from millions of candidates contributing to the
prediction. To address this challenge, we propose DistShap, a parallel
algorithm that distributes Shapley value-based explanations across multiple
GPUs. DistShap operates by sampling subgraphs in a distributed setting,
executing GNN inference in parallel across GPUs, and solving a distributed
least squares problem to compute edge importance scores. DistShap outperforms
most existing GNN explanation methods in accuracy and is the first to scale to
GNN models with millions of features by using up to 128 GPUs on the NERSC
Perlmutter supercomputer.

</details>


### [375] [The Hidden Link Between RLHF and Contrastive Learning](https://arxiv.org/abs/2506.22578)
*Xufei Lv,Haoyuan Sun,Xuefeng Bai,Min Zhang,Houde Liu,Kehai Chen*

Main category: cs.LG

TL;DR: The paper interprets RLHF and DPO as mutual information (MI) maximization techniques linked to contrastive learning, and proposes an improved method called MIO.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current techniques (RLHF and DPO) in aligning large language models (LLMs) with human values and reasoning capabilities.

Method: The authors connect RLHF and DPO to mutual information (MI) maximization using the Donsker-Varadhan (DV) bound and propose a novel Jensen-Shannon MI estimator in a method called Mutual Information Optimization (MIO).

Result: MIO mitigates the decline in performance seen in DPO and achieves better or comparable results in reasoning and mathematical benchmarks through theoretical and empirical evaluations.

Conclusion: MIO is an effective alternative to existing alignment techniques, improving performance while addressing their limitations. The model and code will be accessible post-acceptance.

Abstract: Alignment of large language models (LLMs) with human values has recently
garnered significant attention, with prominent examples including the canonical
yet costly Reinforcement Learning from Human Feedback (RLHF) and the simple
Direct Preference Optimization (DPO). In this work, we demonstrate that both
RLHF and DPO can be interpreted from the perspective of mutual information (MI)
maximization, uncovering a profound connection to contrastive learning. Within
this framework, both RLHF and DPO can be viewed as methods that perform
contrastive learning based on the positive and negative samples derived from
the base model, leveraging the Donsker-Varadhan (DV) lower bound on MI
(equivalently, the MINE estimator). This paradigm further explains why RLHF may
not intrinsically incentivize reasoning capacities in LLMs beyond what is
already present in the base model. Building on this perspective, we replace the
DV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual
Information Optimization (MIO). Comprehensive theoretical analysis and
extensive empirical evaluations demonstrate that MIO mitigates the late-stage
decline in chosen-likelihood observed in DPO, achieving competitive or superior
performance across various challenging reasoning and mathematical benchmarks.
We will release the model and code upon acceptance.

</details>


### [376] [Are Fast Methods Stable in Adversarially Robust Transfer Learning?](https://arxiv.org/abs/2506.22602)
*Joshua C. Zhao,Saurabh Bagchi*

Main category: cs.LG

TL;DR: This paper revisits using the fast gradient sign method (FGSM) in adversarially robust transfer learning, showing it is computationally efficient and stable compared to more expensive methods like PGD.


<details>
  <summary>Details</summary>
Motivation: The paper aims to reduce the computational cost of adversarial fine-tuning in transfer learning, as current methods like PGD are computationally expensive while adversarial training from scratch is impractical.

Method: The paper explores FGSM for adversarial fine-tuning, showing its stability even under standard and high perturbation budgets. It also examines parameter-efficient techniques to further enhance FGSM's performance across multiple datasets.

Result: FGSM fine-tuning achieves comparable adversarial robustness to PGD with minimal performance drop (0.39% and 1.39% for standard budgets) and requires four times less training time.

Conclusion: FGSM offers an efficient and effective alternative for adversarially robust transfer learning, making it suitable for reducing resource demands with relatively small sacrifices in robustness.

Abstract: Transfer learning is often used to decrease the computational cost of model
training, as fine-tuning a model allows a downstream task to leverage the
features learned from the pre-training dataset and quickly adapt them to a new
task. This is particularly useful for achieving adversarial robustness, as
adversarially training models from scratch is very computationally expensive.
However, high robustness in transfer learning still requires adversarial
training during the fine-tuning phase, which requires up to an order of
magnitude more time than standard fine-tuning. In this work, we revisit the use
of the fast gradient sign method (FGSM) in robust transfer learning to improve
the computational cost of adversarial fine-tuning. We surprisingly find that
FGSM is much more stable in adversarial fine-tuning than when training from
scratch. In particular, FGSM fine-tuning does not suffer from any issues with
catastrophic overfitting at standard perturbation budgets of $\varepsilon=4$ or
$\varepsilon=8$. This stability is further enhanced with parameter-efficient
fine-tuning methods, where FGSM remains stable even up to $\varepsilon=32$ for
linear probing. We demonstrate how this stability translates into performance
across multiple datasets. Compared to fine-tuning with the more commonly used
method of projected gradient descent (PGD), on average, FGSM only loses 0.39%
and 1.39% test robustness for $\varepsilon=4$ and $\varepsilon=8$ while using
$4\times$ less training time. Surprisingly, FGSM may not only be a
significantly more efficient alternative to PGD in adversarially robust
transfer learning but also a well-performing one.

</details>


### [377] [Hierarchical Modeling and Architecture Optimization: Review and Unified Framework](https://arxiv.org/abs/2506.22621)
*Paul Saves,Edward Hallé-Hannan,Jasper Bussemaker,Youssef Diouane,Nathalie Bartoli*

Main category: cs.LG

TL;DR: This paper introduces a generalized framework for hierarchical, mixed-variable input spaces and demonstrates its application in Bayesian optimization for complex system designs, such as green aircraft architecture.


<details>
  <summary>Details</summary>
Motivation: To address challenges in representing, modeling, and optimizing hierarchical, conditional, heterogenous, or tree-structured mixed-variable inputs commonly found in simulation-based problems.

Method: The paper proposes a unified framework using design space graphs to model hierarchical domains and supports surrogate modeling and hierarchical kernel optimization, implemented in the Surrogate Modeling Toolbox (SMT 2.0).

Result: The methods were successfully applied to Bayesian optimization tasks, showcasing the framework's ability to handle complex system design challenges effectively.

Conclusion: The framework represents a significant advancement in modeling and optimizing structured mixed-variable input spaces, with practical applications demonstrated in engineering fields like green aircraft architecture.

Abstract: Simulation-based problems involving mixed-variable inputs frequently feature
domains that are hierarchical, conditional, heterogeneous, or tree-structured.
These characteristics pose challenges for data representation, modeling, and
optimization. This paper reviews extensive literature on these structured input
spaces and proposes a unified framework that generalizes existing approaches.
In this framework, input variables may be continuous, integer, or categorical.
A variable is described as meta if its value governs the presence of other
decreed variables, enabling the modeling of conditional and hierarchical
structures.
  We further introduce the concept of partially-decreed variables, whose
activation depends on contextual conditions. To capture these inter-variable
hierarchical relationships, we introduce design space graphs, combining
principles from feature modeling and graph theory. This allows the definition
of general hierarchical domains suitable for describing complex system
architectures. The framework supports the use of surrogate models over such
domains and integrates hierarchical kernels and distances for efficient
modeling and optimization. The proposed methods are implemented in the
open-source Surrogate Modeling Toolbox (SMT 2.0), and their capabilities are
demonstrated through applications in Bayesian optimization for complex system
design, including a case study in green aircraft architecture.

</details>


### [378] [A hierarchical Vovk-Azoury-Warmuth forecaster with discounting for online regression in RKHS](https://arxiv.org/abs/2506.22631)
*Dmitry B. Rokhlin*

Main category: cs.LG

TL;DR: The paper develops an adaptive algorithm, H-VAW-D, extending online regression methods to non-parametric settings. It offers computational efficiency and achieves optimal dynamic regret.


<details>
  <summary>Details</summary>
Motivation: To extend the discounted Vovk-Azoury-Warmuth (DVAW) forecaster from finite-dimensional to non-parametric settings for online regression, addressing challenges posed by time-varying sequences in RKHS.

Method: The authors synthesize the DVAW framework with random feature approximations and propose H-VAW-D, a hierarchical algorithm that self-adjusts the discount factor and the random feature count.

Result: H-VAW-D achieves a computational complexity of O(T ln T) per iteration and an expected dynamic regret of O(T^{2/3}P_T^{1/3} + √T ln T), where P_T represents the comparator's functional path length.

Conclusion: H-VAW-D is computationally efficient while maintaining strong theoretical guarantees of dynamic regret, successfully bridging finite-dimensional and non-parametric approaches.

Abstract: We study the problem of online regression with the unconstrained quadratic
loss against a time-varying sequence of functions from a Reproducing Kernel
Hilbert Space (RKHS). Recently, Jacobsen and Cutkosky (2024) introduced a
discounted Vovk-Azoury-Warmuth (DVAW) forecaster that achieves optimal dynamic
regret in the finite-dimensional case. In this work, we lift their approach to
the non-parametric domain by synthesizing the DVAW framework with a random
feature approximation. We propose a fully adaptive, hierarchical algorithm,
which we call H-VAW-D (Hierarchical Vovk-Azoury-Warmuth with Discounting), that
learns both the discount factor and the number of random features. We prove
that this algorithm, which has a per-iteration computational complexity of
$O(T\ln T)$, achieves an expected dynamic regret of $O(T^{2/3}P_T^{1/3} +
\sqrt{T}\ln T)$, where $P_T$ is the functional path length of a comparator
sequence.

</details>


### [379] [FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model](https://arxiv.org/abs/2506.23210)
*Taehwan Yoon,Bongjun Choi*

Main category: cs.LG

TL;DR: The paper proposes a reference model-based federated learning approach to address challenges in optimizing AI models, particularly focusing on avoiding catastrophic forgetting and improving performance.


<details>
  <summary>Details</summary>
Motivation: Federated learning ensures privacy but often struggles to meet diverse performance expectations of AI users. Optimizing and personalizing these models is crucial but challenging.

Method: The paper introduces a method derived from Bayesian parameter-efficient transfer learning. It includes an optimal proximal term and uses a reference model that incorporates past model parameters to overcome catastrophic forgetting in federated learning.

Result: The proposed method ensures both high model performance and low computing costs while preventing catastrophic forgetting in federated learning scenarios.

Conclusion: The novel approach provides a robust solution for optimizing federated learning models, balancing privacy, performance, and computational efficiency.

Abstract: Federated learning(FL) is used for distributed scenarios to train artificial
intelligence(AI) models while ensuring users' privacy. In federated learning
scenario, the server generally never knows about users' data. This type of
concept makes the AI training process efficient in terms of data privacy.
However, regarding model performance, federated AI models may not sufficiently
satisfy AI users' expectations. Furthermore, AI users have a wide range of
different needs. It is not easy to satisfy the whole users needs. These types
of issues can be addressed through AI model optimization, fine-tuning, or
personalization to achieve optimal model performance. To address model
optimization challenges, we propose reference model-based federated learning
for optimal fine-tuning, which overcomes catastrophic forgetting in each round.
This method is derived from Bayesian parameter-efficient transfer learning,
which includes an optimal proximal term and enables overcoming the catastrophic
forgetting issue in each round by utilizing a reference model that incorporates
previous model parameters. As a result, this method achieves both high model
performance and low computing cost.

</details>


### [380] [Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training](https://arxiv.org/abs/2506.22638)
*Aadim Nepal,Safal Shrestha,Anubhav Shrestha,Minwu Kim,Keith Ross*

Main category: cs.LG

TL;DR: The paper investigates how improvements in mathematical reasoning in language models arise after post-training, emphasizing the role of specific transformer layers.


<details>
  <summary>Details</summary>
Motivation: To understand whether mathematical reasoning enhancements in language models post-training stem from significant changes in transformer layers or minor adjustments retaining the base model's layer structure.

Method: Systematic layer-wise ablation experiments on various models (base, instruction-tuned, knowledge-distilled, and reinforcement-learned) tested on mathematical reasoning benchmarks.

Result: Mathematical reasoning tasks depend on specific critical layers whose removal causes accuracy drops of up to 80%, unlike non-mathematical tasks that show no such layer dependency.

Conclusion: Mathematical reasoning tasks rely on distinct critical layers that emerge during pre-training, which contrasts with non-reasoning tasks where such layers are unnecessary.

Abstract: Large language models can exhibit improved mathematical reasoning
capabilities following post-training with instruction tuning, reinforcement
learning, or knowledge distillation. However, it remains unclear whether these
improvements are driven by major changes in transformer layers or from minor
adjustments that leave the relative layer importance structures of the base
model largely unchanged. We investigate this question through systematic
layer-wise ablation experiments, examining base, instruction-tuned,
knowledge-distilled, and reinforcement learning variants on mathematical
reasoning benchmarks. Our findings show that mathematical reasoning gives rise
to a specific layer importance structure, and this structure persists across
all post-training paradigms. Removal of such layers causes accuracy drops of up
to 80%. In contrast, non-mathematical tasks like factual recall exhibit no
critical layers. This distinction suggests that mathematical reasoning requires
specialized layers that emerge during pre-training, while other non-reasoning
tasks do not. From an information-theoretic perspective, we also observe that
these critical layers are the same layers where major representational
transformation occurs.

</details>


### [381] [Cost-effective Reduced-Order Modeling via Bayesian Active Learning](https://arxiv.org/abs/2506.22645)
*Amir Hossein Rahmati,Nathan M. Urban,Byung-Jun Yoon,Xiaoning Qian*

Main category: cs.LG

TL;DR: BayPOD-AL leverages active learning with Bayesian POD for efficient reduced-order modeling, minimizing training data requirements and computational costs.


<details>
  <summary>Details</summary>
Motivation: Current machine learning surrogates demand extensive training datasets, limiting their real-world applicability in capturing systems dynamics of complex processes.

Method: The authors developed BayPOD-AL, an active learning framework that integrates uncertainty-aware Bayesian proper orthogonal decomposition (POD) to learn reduced-order models efficiently.

Result: BayPOD-AL effectively identified informative data and reduced training dataset construction costs, surpassing other uncertainty-oriented active learning strategies. It also showed robustness with datasets of higher temporal resolution.

Conclusion: BayPOD-AL improves reduced-order modeling efficiency and generalizability while reducing the dependency on large training datasets, making it suitable for complex systems.

Abstract: Machine Learning surrogates have been developed to accelerate solving systems
dynamics of complex processes in different science and engineering
applications. To faithfully capture governing systems dynamics, these methods
rely on large training datasets, hence restricting their applicability in
real-world problems. In this work, we propose BayPOD-AL, an active learning
framework based on an uncertainty-aware Bayesian proper orthogonal
decomposition (POD) approach, which aims to effectively learn reduced-order
models from high-fidelity full-order models representing complex systems.
Experimental results on predicting the temperature evolution over a rod
demonstrate BayPOD-AL's effectiveness in suggesting the informative data and
reducing computational cost related to constructing a training dataset compared
to other uncertainty-guided active learning strategies. Furthermore, we
demonstrate BayPOD-AL's generalizability and efficiency by evaluating its
performance on a dataset of higher temporal resolution than the training
dataset.

</details>


### [382] [Learning Stochastic Multiscale Models](https://arxiv.org/abs/2506.22655)
*Andrew F. Ilersich,Prasanth B. Nair*

Main category: cs.LG

TL;DR: This paper introduces a method to learn stochastic multiscale models from observational data, combining techniques from physics and variational inference, to predict dynamical systems efficiently.


<details>
  <summary>Details</summary>
Motivation: Dynamical systems in physical sciences often involve scales that are computationally expensive to resolve directly, motivating the need for efficient modeling methods that capture critical dynamics without full-resolution simulations.

Method: The authors propose a framework to learn stochastic multiscale models using stochastic differential equations with auxiliary states on a coarse mesh and train them using amortized variational inference techniques.

Result: Numerical studies show that the learned multiscale models outperform state-of-the-art methods, achieving superior predictive accuracy compared to both direct numerical simulations and closure models.

Conclusion: The proposed method offers an efficient and accurate alternative for modeling multiscale dynamical systems, leveraging observational data and modern inference techniques to overcome computational challenges.

Abstract: The physical sciences are replete with dynamical systems that require the
resolution of a wide range of length and time scales. This presents significant
computational challenges since direct numerical simulation requires
discretization at the finest relevant scales, leading to a high-dimensional
state space. In this work, we propose an approach to learn stochastic
multiscale models in the form of stochastic differential equations directly
from observational data. Our method resolves the state on a coarse mesh while
introducing an auxiliary state to capture the effects of unresolved scales. We
learn the parameters of the multiscale model using a modern forward-solver-free
amortized variational inference method. Our approach draws inspiration from
physics-based multiscale modeling approaches, such as large-eddy simulation in
fluid dynamics, while learning directly from data. We present numerical studies
to demonstrate that our learned multiscale models achieve superior predictive
accuracy compared to direct numerical simulation and closure-type models at
equivalent resolution.

</details>


### [383] [Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment](https://arxiv.org/abs/2506.22685)
*Anh Bui,Trang Vu,Trung Le,Junae Kim,Tamas Abraham,Rollin Omari,Amar Kaur,Dinh Phung*

Main category: cs.LG

TL;DR: The paper addresses the semantic collapsing problem in generative personalization, presenting a training-free method to mitigate it and improve text-image alignment.


<details>
  <summary>Details</summary>
Motivation: To address the semantic collapsing issue where learned concepts lose their intended richness and degrade the quality of multi-concept prompts in generative personalization.

Method: Introduced a training-free, inference-time adjustment method for pre-trained embeddings, focusing on both magnitude and direction to mitigate semantic collapse.

Result: Demonstrated significant improvements in preserving text-image semantic richness across diverse scenarios and personalization methods.

Conclusion: The proposed approach effectively resolves semantic collapsing, is broadly applicable, and enhances generative model outputs by maintaining intended prompts' contextual complexity.

Abstract: In this paper, we investigate the semantic collapsing problem in generative
personalization, an under-explored topic where the learned visual concept
($V^*$) gradually shifts from its original textual meaning and comes to
dominate other concepts in multi-concept input prompts. This issue not only
reduces the semantic richness of complex input prompts like "a photo of $V^*$
wearing glasses and playing guitar" into simpler, less contextually rich forms
such as "a photo of $V^*$" but also leads to simplified output images that fail
to capture the intended concept.
  We identify the root cause as unconstrained optimisation, which allows the
learned embedding $V^*$ to drift arbitrarily in the embedding space, both in
direction and magnitude. To address this, we propose a simple yet effective
training-free method that adjusts the magnitude and direction of pre-trained
embedding at inference time, effectively mitigating the semantic collapsing
problem. Our method is broadly applicable across different personalization
methods and demonstrates significant improvements in text-image alignment in
diverse use cases. Our code is anonymously published at
https://anonymous.4open.science/r/Embedding-Adjustment.

</details>


### [384] [Generalized Linear Mode Connectivity for Transformers](https://arxiv.org/abs/2506.22712)
*Alexander Theus,Alessandro Cabodi,Sotiris Anagnostidis,Antonio Orvieto,Sidak Pal Singh,Valentina Boeva*

Main category: cs.LG

TL;DR: The paper explores symmetry-aware methods to study neural network loss landscapes, enabling the discovery of linear paths between independently trained models like Vision Transformers and GPT-2.


<details>
  <summary>Details</summary>
Motivation: Previous methods for analyzing geometry in neural network loss landscapes do not fully account for the broader symmetries beyond basic neuron permutations. Modern architectures like Transformers exhibit richer symmetries that need to be analyzed to understand model connectivity.

Method: The framework introduces a unified analysis that encompasses four symmetry classes: permutations, semi-permutations, orthogonal transformations, and general invertible maps. This generalization extends prior techniques by capturing both basic and advanced symmetrical transformations.

Result: Using this unified framework, the researchers demonstrated the existence of low- and zero-barrier linear paths connecting independently trained Vision Transformers and GPT-2 models, showcasing deeper geometrical structures.

Conclusion: Analyzing neural network loss landscapes with symmetry-aware methods reveals richer structural insights, suggesting these approaches are critical for future studies in model generalization and optimization.

Abstract: Understanding the geometry of neural network loss landscapes is a central
question in deep learning, with implications for generalization and
optimization. A striking phenomenon is linear mode connectivity (LMC), where
independently trained models can be connected by low- or zero-loss paths,
despite appearing to lie in separate loss basins. However, this is often
obscured by symmetries in parameter space -- such as neuron permutations --
which make functionally equivalent models appear dissimilar. Prior work has
predominantly focused on neuron re-ordering through permutations, but such
approaches are limited in scope and fail to capture the richer symmetries
exhibited by modern architectures such as Transformers. In this work, we
introduce a unified framework that captures four symmetry classes:
permutations, semi-permutations, orthogonal transformations, and general
invertible maps -- broadening the set of valid reparameterizations and
subsuming many previous approaches as special cases. Crucially, this
generalization enables, for the first time, the discovery of low- and
zero-barrier linear interpolation paths between independently trained Vision
Transformers and GPT-2 models. These results reveal deeper structure in the
loss landscape and underscore the importance of symmetry-aware analysis for
understanding model space geometry.

</details>


### [385] [Residual Matrix Transformers: Scaling the Size of the Residual Stream](https://arxiv.org/abs/2506.22696)
*Brian Mak,Jeffrey Flanigan*

Main category: cs.LG

TL;DR: The paper introduces the Residual Matrix Transformer (RMT), which replaces the residual stream in transformers with an outer product memory matrix to improve scalability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Transformers rely on the residual stream to store and retrieve information, but this mechanism could be optimized for better computational efficiency and scaling.

Method: The residual stream is replaced by an outer product memory matrix, allowing for adjustable scalability of the model's memory bus without changing compute or model size.

Result: The RMT reduces the computational cost (58% fewer FLOPS, 25% fewer parameters, 41% fewer training tokens) while maintaining or outperforming transformers in loss and downstream evaluations.

Conclusion: The RMT provides a more efficient and scalable alternative to traditional transformers, enabling better performance and reduced resource usage.

Abstract: The residual stream acts as a memory bus where transformer layers both store
and access features (Elhage et al., 2021). We consider changing the mechanism
for retrieving and storing information in the residual stream, and replace the
residual stream of the transformer with an outer product memory matrix
(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix
Transformer (RMT). We find that the RMT enjoys a number of attractive
properties: 1) the size of the residual stream can be scaled independently of
compute and model size, improving performance, 2) the RMT can achieve the same
loss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%
fewer training tokens tokens, and 3) the RMT outperforms the transformer on
downstream evaluations. We theoretically analyze the transformer and the RMT,
and show that the RMT allows for more efficient scaling of the residual stream,
as well as improved variance propagation properties. Code for this project can
be found at https://github.com/bmac3/residual-matrix-transformer.

</details>


### [386] [Robust Tensor Completion via Gradient Tensor Nulclear L1-L2 Norm for Traffic Data Recovery](https://arxiv.org/abs/2506.22732)
*Hao Shu,Jicheng Li,Tianyv Lei,Lijun Sun*

Main category: cs.LG

TL;DR: The paper addresses the challenge of recovering degraded spatiotemporal traffic data, proposing a novel model for simultaneous handling of missing values and noise.


<details>
  <summary>Details</summary>
Motivation: Sensor malfunctions and communication failures degrade spatiotemporal traffic data through missing values and noise, necessitating effective recovery methods for reliable data applications.

Method: The authors introduce the non-convex tensor L1-L2 norm and develop the Robust Tensor Completion via Gradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, leveraging global low-rankness and local consistency without trade-off parameters.

Result: Experiments on real-world traffic datasets reveal that RTC-GTNLN consistently outperforms state-of-the-art methods in scenarios with dual degradation challenges.

Conclusion: RTC-GTNLN provides an effective solution for recovering degraded traffic datasets, enhancing data-driven applications by addressing the challenges of missingness and noise.

Abstract: In real-world scenarios, spatiotemporal traffic data frequently experiences
dual degradation from missing values and noise caused by sensor malfunctions
and communication failures. Therefore, effective data recovery methods are
essential to ensure the reliability of downstream data-driven applications.
while classical tensor completion methods have been widely adopted, they are
incapable of modeling noise, making them unsuitable for complex scenarios
involving simultaneous data missingness and noise interference. Existing Robust
Tensor Completion (RTC) approaches offer potential solutions by separately
modeling the actual tensor data and noise. However, their effectiveness is
often constrained by the over-relaxation of convex rank surrogates and the
suboptimal utilization of local consistency, leading to inadequate model
accuracy. To address these limitations, we first introduce the tensor L1-L2
norm, a novel non-convex tensor rank surrogate that functions as an effective
low-rank representation tool. Leveraging an advanced feature fusion strategy,
we further develop the gradient tensor L1-L2 norm by incorporating the tensor
L1-L2 norm in the gradient domain. By integrating the gradient tensor nuclear
L1-L2 norm into the RTC framework, we propose the Robust Tensor Completion via
Gradient Tensor Nuclear L1-L2 Norm (RTC-GTNLN) model, which not only fully
exploits both global low-rankness and local consistency without trade-off
parameter, but also effectively handles the dual degradation challenges of
missing data and noise in traffic data. Extensive experiments conducted on
multiple real-world traffic datasets demonstrate that the RTC-GTNLN model
consistently outperforms existing state-of-the-art methods in complex recovery
scenarios involving simultaneous missing values and noise.

</details>


### [387] [FairMarket-RL: LLM-Guided Fairness Shaping for Multi-Agent Reinforcement Learning in Peer-to-Peer Markets](https://arxiv.org/abs/2506.22708)
*Shrenik Jadhav,Birva Sevak,Srijita Das,Akhtar Hussain,Wencong Su,Van-Hai Bui*

Main category: cs.LG

TL;DR: FairMarket-RL integrates Large Language Models (LLMs) with Reinforcement Learning (RL) to enhance fairness in decentralized peer-to-peer trading systems.


<details>
  <summary>Details</summary>
Motivation: Current P2P trading mechanisms lack robust frameworks to ensure fairness among sellers and buyers.

Method: The framework utilizes LLMs as fairness critics and combines them with RL-based training, incorporating fairness metrics into agent rewards through adaptive coefficients.

Result: FairMarket-RL achieves equitable trading outcomes with fairness scores exceeding 0.80, and effectively fulfills buyer demands while maintaining fair seller profits.

Conclusion: FairMarket-RL provides a scalable and fairness-aware solution for decentralized energy trading systems, demonstrating practical applicability in larger scenarios.

Abstract: Peer-to-peer (P2P) trading is increasingly recognized as a key mechanism for
decentralized market regulation, yet existing approaches often lack robust
frameworks to ensure fairness. This paper presents FairMarket-RL, a novel
hybrid framework that combines Large Language Models (LLMs) with Reinforcement
Learning (RL) to enable fairness-aware trading agents. In a simulated P2P
microgrid with multiple sellers and buyers, the LLM acts as a real-time
fairness critic, evaluating each trading episode using two metrics:
Fairness-To-Buyer (FTB) and Fairness-Between-Sellers (FBS). These fairness
scores are integrated into agent rewards through scheduled
{\lambda}-coefficients, forming an adaptive LLM-guided reward shaping loop that
replaces brittle, rule-based fairness constraints. Agents are trained using
Independent Proximal Policy Optimization (IPPO) and achieve equitable outcomes,
fulfilling over 90% of buyer demand, maintaining fair seller margins, and
consistently reaching FTB and FBS scores above 0.80. The training process
demonstrates that fairness feedback improves convergence, reduces buyer
shortfalls, and narrows profit disparities between sellers. With its
language-based critic, the framework scales naturally, and its extension to a
large power distribution system with household prosumers illustrates its
practical applicability. FairMarket-RL thus offers a scalable, equity-driven
solution for autonomous trading in decentralized energy systems.

</details>


### [388] [BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute](https://arxiv.org/abs/2506.22716)
*Dujian Ding,Ankur Mallick,Shaokun Zhang,Chi Wang,Daniel Madrigal,Mirian Del Carmen Hipolito Garcia,Menglin Xia,Laks V. S. Lakshmanan,Qingyun Wu,Victor Rühle*

Main category: cs.LG

TL;DR: The paper introduces BEST-Route, a routing framework that strategically balances cost and quality in large language model (LLM) deployments, reducing costs significantly with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: Deploying LLMs at scale is expensive, and prior query routing methods fail to effectively balance cost by overusing large models. Small models can provide competitive outputs when generating multiple responses, offering untapped opportunities for cost savings.

Method: BEST-Route dynamically routes queries based on difficulty and predefined quality thresholds while leveraging multiple response sampling from small models to enhance output quality at a lower cost.

Result: BEST-Route reduces costs by up to 60% with less than 1% drop in performance, as demonstrated in experiments on real-world datasets.

Conclusion: BEST-Route efficiently achieves a cost-quality trade-off, demonstrating its potential to optimize LLM deployment by integrating multi-response sampling into the routing process.

Abstract: Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

</details>


### [389] [Kernel Outlier Detection](https://arxiv.org/abs/2506.22994)
*Can Hakan Dağıdır,Mia Hubert,Peter J. Rousseeuw*

Main category: cs.LG

TL;DR: A novel method, kernel outlier detection (KOD), addresses high-dimensional outlier detection without relying on strict assumptions or challenging hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: The study aims to solve outlier detection limitations in high-dimensional settings, addressing issues like dependency on complex assumptions and hard-to-tune parameters.

Method: KOD uses a kernel transformation combined with projection pursuit, introducing new ensemble directions and novel result combination techniques.

Result: Empirical tests of KOD show effectiveness on three small complex datasets and four large benchmark datasets.

Conclusion: Kernel outlier detection offers a flexible and efficient solution for detecting anomalies across various datasets, showcasing its practicality and robustness.

Abstract: A new anomaly detection method called kernel outlier detection (KOD) is
proposed. It is designed to address challenges of outlier detection in
high-dimensional settings. The aim is to overcome limitations of existing
methods, such as dependence on distributional assumptions or on hyperparameters
that are hard to tune. KOD starts with a kernel transformation, followed by a
projection pursuit approach. Its novelties include a new ensemble of directions
to search over, and a new way to combine results of different direction types.
This provides a flexible and lightweight approach for outlier detection. Our
empirical evaluations illustrate the effectiveness of KOD on three small
datasets with challenging structures, and on four large benchmark datasets.

</details>


### [390] [Multimodal Atmospheric Super-Resolution With Deep Generative Models](https://arxiv.org/abs/2506.22780)
*Dibyajyoti Chakraborty,Haiwen Guan,Jason Stock,Troy Arcomano,Guido Cervone,Romit Maulik*

Main category: cs.LG

TL;DR: The paper proposes using score-based diffusion models to improve the super-resolution of atmospheric datasets by fusing low-resolution multimodal data and provides uncertainty estimates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage score-based diffusion models for fusing data from various low-fidelity sources to improve spatiotemporal reconstructions, specifically in high-dimensional dynamical systems such as atmospheric datasets.

Method: The authors extend score-based diffusion modeling for super-resolution, incorporating observed low-resolution data in a Bayesian framework. They demonstrate this with real-time sparse, multimodal sensor data for atmospheric observations.

Result: The study achieves accurate high-resolution reconstructions of the ERA5 atmospheric dataset using sparse observations and shows the model's ability to balance multiple data modalities effectively.

Conclusion: Score-based diffusion models can improve high-dimensional data reconstruction by fusing multimodal inputs. They also provide valuable uncertainty estimates, making them suitable for spatiotemporal applications.

Abstract: Score-based diffusion modeling is a generative machine learning algorithm
that can be used to sample from complex distributions. They achieve this by
learning a score function, i.e., the gradient of the log-probability density of
the data, and reversing a noising process using the same. Once trained,
score-based diffusion models not only generate new samples but also enable
zero-shot conditioning of the generated samples on observed data. This promises
a novel paradigm for data and model fusion, wherein the implicitly learned
distributions of pretrained score-based diffusion models can be updated given
the availability of online data in a Bayesian formulation. In this article, we
apply such a concept to the super-resolution of a high-dimensional dynamical
system, given the real-time availability of low-resolution and experimentally
observed sparse sensor measurements from multimodal data. Additional analysis
on how score-based sampling can be used for uncertainty estimates is also
provided. Our experiments are performed for a super-resolution task that
generates the ERA5 atmospheric dataset given sparse observations from a
coarse-grained representation of the same and/or from unstructured experimental
observations of the IGRA radiosonde dataset. We demonstrate accurate recovery
of the high dimensional state given multiple sources of low-fidelity
measurements. We also discover that the generative model can balance the
influence of multiple dataset modalities during spatiotemporal reconstructions.

</details>


### [391] [Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning](https://arxiv.org/abs/2506.23033)
*Yash Vardhan Tomar*

Main category: cs.LG

TL;DR: The paper introduces a feature-wise mixing framework to reduce bias in ML models by redistributing feature representations, achieving better performance and lower bias compared to traditional techniques.


<details>
  <summary>Details</summary>
Motivation: Bias in ML models leads to unfair outcomes, and current mitigation strategies struggle with scalability and generalizability.

Method: A feature-wise mixing framework redistributes feature representations across multiple contextual datasets and uses cross-validation with bias-sensitive metrics.

Result: The method reduced bias by 43.35% and significantly lowered MSE across classifiers, outperforming SMOTE without requiring explicit bias attribute identification.

Conclusion: Feature-wise mixing is effective in mitigating bias while maintaining predictive performance, offering a scalable and computationally efficient solution.

Abstract: Bias in predictive machine learning (ML) models is a fundamental challenge
due to the skewed or unfair outcomes produced by biased models. Existing
mitigation strategies rely on either post-hoc corrections or rigid constraints.
However, emerging research claims that these techniques can limit scalability
and reduce generalizability. To address this, this paper introduces a
feature-wise mixing framework to mitigate contextual bias. This was done by
redistributing feature representations across multiple contextual datasets. To
assess feature-wise mixing's effectiveness, four ML classifiers were trained
using cross-validation and evaluated with bias-sensitive loss functions,
including disparity metrics and mean squared error (MSE), which served as a
standard measure of predictive performance. The proposed method achieved an
average bias reduction of 43.35% and a statistically significant decrease in
MSE across all classifiers trained on mixed datasets. Additionally,
benchmarking against established bias mitigation techniques found that
feature-wise mixing consistently outperformed SMOTE oversampling and
demonstrated competitive effectiveness without requiring explicit bias
attribute identification. Feature-wise mixing efficiently avoids the
computational overhead typically associated with fairness-aware learning
algorithms. Future work could explore applying feature-wise mixing for
real-world fields where accurate predictions are necessary.

</details>


### [392] [Riemannian-Geometric Fingerprints of Generative Models](https://arxiv.org/abs/2506.22802)
*Hae Jin Song,Laurent Itti*

Main category: cs.LG

TL;DR: The paper addresses generative models' attribution by defining fingerprints using Riemannian geometry, achieving improved model attribution performance across varied datasets and architectures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle issues of model attribution, intellectual property protection, accountability, and addressing challenges like model collapse due to synthetic data.

Method: It employs Riemannian geometry to redefine generative model fingerprints and proposes a gradient-based algorithm for practical computation.

Result: The proposed definitions and methods outperform previous approaches in distinguishing generative models across diverse datasets, resolutions, architectures, and modalities.

Conclusion: The approach generalized effectively to unseen data, models, and modalities, highlighting its practical relevance and potential to improve generative model accountability and performance.

Abstract: Recent breakthroughs and rapid integration of generative models (GMs) have
sparked interest in the problem of model attribution and their fingerprints.
For instance, service providers need reliable methods of authenticating their
models to protect their IP, while users and law enforcement seek to verify the
source of generated content for accountability and trust. In addition, a
growing threat of model collapse is arising, as more model-generated data are
being fed back into sources (e.g., YouTube) that are often harvested for
training ("regurgitative training"), heightening the need to differentiate
synthetic from human data. Yet, a gap still exists in understanding generative
models' fingerprints, we believe, stemming from the lack of a formal framework
that can define, represent, and analyze the fingerprints in a principled way.
To address this gap, we take a geometric approach and propose a new definition
of artifact and fingerprint of GMs using Riemannian geometry, which allows us
to leverage the rich theory of differential geometry. Our new definition
generalizes previous work (Song et al., 2024) to non-Euclidean manifolds by
learning Riemannian metrics from data and replacing the Euclidean distances and
nearest-neighbor search with geodesic distances and kNN-based Riemannian center
of mass. We apply our theory to a new gradient-based algorithm for computing
the fingerprints in practice. Results show that it is more effective in
distinguishing a large array of GMs, spanning across 4 different datasets in 2
different resolutions (64 by 64, 256 by 256), 27 model architectures, and 2
modalities (Vision, Vision-Language). Using our proposed definition
significantly improves the performance on model attribution, as well as a
generalization to unseen datasets, model types, and modalities, suggesting its
practical efficacy.

</details>


### [393] [ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning](https://arxiv.org/abs/2506.23960)
*Mingfei Cheng,Xiaofei Xie,Renzhi Wang,Yuan Zhou,Ming Hu*

Main category: cs.LG

TL;DR: ADReFT is a repair system for autonomous driving systems focused on safety and reliability using a transformer-based model.


<details>
  <summary>Details</summary>
Motivation: Existing online repair solutions for ADSs are too conservative and lack adaptability and generalizability, which limits their effectiveness in mitigating safety risks while preserving driving quality.

Method: ADReFT uses a transformer-based model with two heads—State Monitor and Decision Adapter—for offline learning and adaptive repair. It is trained initially with supervised learning and fine-tuned using reinforcement learning.

Result: ADReFT identifies safety-critical states and generates appropriate repair decisions with improved precision and performance over traditional approaches.

Conclusion: ADReFT enhances the safety and reliability of ADSs through contextually appropriate repair strategies and avoids the limitations of overly conservative approaches.

Abstract: Autonomous Driving Systems (ADSs) continue to face safety-critical risks due
to the inherent limitations in their design and performance capabilities.
Online repair plays a crucial role in mitigating such limitations, ensuring the
runtime safety and reliability of ADSs. Existing online repair solutions
enforce ADS compliance by transforming unacceptable trajectories into
acceptable ones based on predefined specifications, such as rule-based
constraints or training datasets. However, these approaches often lack
generalizability, adaptability and tend to be overly conservative, resulting in
ineffective repairs that not only fail to mitigate safety risks sufficiently
but also degrade the overall driving experience. To address this issue, we
propose Adaptive Decision Repair (ADReFT), a novel and effective repair method
that identifies safety-critical states through offline learning from failed
tests and generates appropriate mitigation actions to improve ADS safety.
Specifically, ADReFT incorporates a transformer-based model with two joint
heads, State Monitor and Decision Adapter, designed to capture complex driving
environment interactions to evaluate state safety severity and generate
adaptive repair actions. Given the absence of oracles for state safety
identification, we first pretrain ADReFT using supervised learning with coarse
annotations, i.e., labeling states preceding violations as positive samples and
others as negative samples. It establishes ADReFT's foundational capability to
mitigate safety-critical violations, though it may result in somewhat
conservative mitigation strategies. Therefore, we subsequently finetune ADReFT
using reinforcement learning to improve its initial capability and generate
more precise and contextually appropriate repair decisions. Our evaluation
results illustrate that ADReFT achieves better repair performance.

</details>


### [394] [Efficient Algorithms for Learning and Compressing Monophonic Halfspaces in Graphs](https://arxiv.org/abs/2506.23186)
*Marco Bressan,Victor Chepoi,Emmanuel Esposito,Maximilian Thiessen*

Main category: cs.LG

TL;DR: The paper introduces monophonic halfspaces on graph vertices and provides efficient algorithms for learning tasks, contrasting them with geodesic halfspaces.


<details>
  <summary>Details</summary>
Motivation: To explore efficient algorithms for learning problems using graph-based notions of convexity, focusing on monophonic halfspaces.

Method: Developed a 2-satisfiability-based decomposition theorem to represent monophonic halfspaces as disjoint vertex subsets and designed efficient learning algorithms.

Result: Achieved efficient solutions for teaching, active, and online learning, including a polynomial-time algorithm for empirical risk minimization and a stable sample compression scheme.

Conclusion: Monophonic halfspaces are efficiently learnable with proper learners at a linear error rate under the PAC setting, answering open questions and distinguishing them from NP-hard geodesic halfspaces.

Abstract: Abstract notions of convexity over the vertices of a graph, and corresponding
notions of halfspaces, have recently gained attention from the machine learning
community. In this work we study monophonic halfspaces, a notion of graph
halfspaces defined through closure under induced paths. Our main result is a
$2$-satisfiability based decomposition theorem, which allows one to represent
monophonic halfspaces as a disjoint union of certain vertex subsets. Using this
decomposition, we achieve efficient and (nearly) optimal algorithms for various
learning problems, such as teaching, active, and online learning. Most notably,
we obtain a polynomial-time algorithm for empirical risk minimization.
Independently of the decomposition theorem, we obtain an efficient, stable, and
proper sample compression scheme. This makes monophonic halfspaces efficiently
learnable with proper learners and linear error rate $1/\varepsilon$ in the
realizable PAC setting. Our results answer open questions from the literature,
and show a stark contrast with geodesic halfspaces, for which most of the said
learning problems are NP-hard.

</details>


### [395] [BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters](https://arxiv.org/abs/2506.22809)
*Cooper Doyle*

Main category: cs.LG

TL;DR: BayesLoRA introduces a task-specific scheme to quantify uncertainty in LoRA frameworks using MC-Dropout, allowing better decision-making. It shows amplified variance outside fine-tuning distributions.


<details>
  <summary>Details</summary>
Motivation: Existing transformer-based uncertainty quantification methods aren't specialized for task-specific guardrails or downstream applications, and improvements can enhance agent behavior under uncertainty conditions.

Method: BayesLoRA integrates Monte Carlo (MC)-Dropout mechanisms within LoRA, aiming for uncertainty quantification specialized for fine-tuning workflows.

Result: The implementation exhibited amplified variance outside fine-tuning distributions mathematically and empirically, leading to reliable confidence measures.

Conclusion: BayesLoRA effectively provides tools for task-specific uncertainty management, facilitating robust and adaptable behavior from agents.

Abstract: We propose BayesLoRA, a task-specific uncertainty quantification framework
that integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike
general-purpose transformer uncertainty methods, BayesLoRA provides guardrails
tailored to downstream workflows, enabling agents to introspect and modulate
behavior under uncertainty. We demonstrate mathematically and empirically that
LoRA adapters exhibit amplified variance outside fine-tuning distributions,
yielding reliable confidence estimates for agentic decision-making.

</details>


### [396] [Not All Explanations for Deep Learning Phenomena Are Equally Valuable](https://arxiv.org/abs/2506.23286)
*Alan Jeffares,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: The paper critiques the focus on understanding isolated deep learning phenomena (e.g., double descent, grokking) and argues for leveraging them to refine broader deep learning theories.


<details>
  <summary>Details</summary>
Motivation: To question the efficiency of addressing unique deep learning phenomena as isolated cases and align research efforts more closely with practical advancements in deep learning.

Method: Analyzing outcomes from recent studies on deep learning phenomena and revisiting research norms to propose practical recommendations.

Result: The paper highlights inefficiencies in isolated studies of deep learning phenomena while emphasizing their potential for enhancing general theories.

Conclusion: Deep learning phenomena should not merely be treated as puzzles but rather as opportunities to refine broad explanatory theories, thereby advancing the overall field of deep learning.

Abstract: Developing a better understanding of surprising or counterintuitive phenomena
has constituted a significant portion of deep learning research in recent
years. These include double descent, grokking, and the lottery ticket
hypothesis -- among many others. Works in this area often develop ad hoc
hypotheses attempting to explain these observed phenomena on an isolated,
case-by-case basis. This position paper asserts that, in many prominent cases,
there is little evidence to suggest that these phenomena appear in real-world
applications and these efforts may be inefficient in driving progress in the
broader field. Consequently, we argue against viewing them as isolated puzzles
that require bespoke resolutions or explanations. However, despite this, we
suggest that deep learning phenomena do still offer research value by providing
unique settings in which we can refine our broad explanatory theories of more
general deep learning principles. This position is reinforced by analyzing the
research outcomes of several prominent examples of these phenomena from the
recent literature. We revisit the current norms in the research community in
approaching these problems and propose practical recommendations for future
research, aiming to ensure that progress on deep learning phenomena is well
aligned with the ultimate pragmatic goal of progress in the broader field of
deep learning.

</details>


### [397] [Deep learning 40 years of human migration](https://arxiv.org/abs/2506.22821)
*Thomas Gaskin,Guy J. Abel*

Main category: cs.LG

TL;DR: A novel dataset of global migration flows and stocks (1990–present) disaggregated by country of birth, developed using a deep recurrent neural network to analyze 18 covariates with validation and open access availability.


<details>
  <summary>Details</summary>
Motivation: The study aims to provide a comprehensive and detailed understanding of global migration patterns over the past four decades by leveraging advanced machine learning techniques while addressing data limitations and uncertainties.

Method: A deep recurrent neural network ensemble was trained on 18 covariates (geographic, economic, cultural, societal, and political) to estimate migration flows and stocks over time. It includes uncertainty modeling and provides confidence bounds through ensemble methods.

Result: The model achieved a significant improvement in estimating migration flows, with increased temporal resolution, outperforming traditional methods on various unseen datasets. It also identifies geographic regions needing more data collection.

Conclusion: The dataset and model offer a rich resource for migration studies, enabling more accurate and granular analysis. The authors provide open access to training data, code, and model weights to encourage further research.

Abstract: We present a novel and detailed dataset on origin-destination annual
migration flows and stocks between 230 countries and regions, spanning the
period from 1990 to the present. Our flow estimates are further disaggregated
by country of birth, providing a comprehensive picture of migration over the
last 43 years. The estimates are obtained by training a deep recurrent neural
network to learn flow patterns from 18 covariates for all countries, including
geographic, economic, cultural, societal, and political information. The
recurrent architecture of the neural network means that the entire past can
influence current migration patterns, allowing us to learn long-range temporal
correlations. By training an ensemble of neural networks and additionally
pushing uncertainty on the covariates through the trained network, we obtain
confidence bounds for all our estimates, allowing researchers to pinpoint the
geographic regions most in need of additional data collection. We validate our
approach on various test sets of unseen data, demonstrating that it
significantly outperforms traditional methods estimating five-year flows while
delivering a significant increase in temporal resolution. The model is fully
open source: all training data, neural network weights, and training code are
made public alongside the migration estimates, providing a valuable resource
for future studies of human migration.

</details>


### [398] [xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection](https://arxiv.org/abs/2506.22837)
*Kamil Faber,Marcin Pietroń,Dominik Żurek,Roberto Corizzo*

Main category: cs.LG

TL;DR: The paper introduces xLSTMAD, leveraging xLSTM for anomaly detection in multivariate time series, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the unexplored application of xLSTM in anomaly detection, particularly due to its success in time series tasks and the lack of prior work in this domain.

Method: The authors propose xLSTMAD with an encoder-decoder architecture for multivariate time series. They evaluate two methods: forecasting (xLSTMAD-F) and reconstruction (xLSTMAD-R), leveraging Mean Squared Error (MSE) and Soft Dynamic Time Warping (SoftDTW) as loss functions.

Result: xLSTMAD surpasses 23 existing anomaly detection techniques on the TSB-AD-M benchmark across 17 datasets, achieving state-of-the-art accuracy using VUS-PR metrics.

Conclusion: The research demonstrates xLSTM's strong potential in anomaly detection and opens avenues for further innovations. The authors release the code for broader use and validation.

Abstract: The recently proposed xLSTM is a powerful model that leverages expressive
multiplicative gating and residual connections, providing the temporal capacity
needed for long-horizon forecasting and representation learning. This
architecture has demonstrated success in time series forecasting, lossless
compression, and even large-scale language modeling tasks, where its linear
memory footprint and fast inference make it a viable alternative to
Transformers. Despite its growing popularity, no prior work has explored xLSTM
for anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the
first anomaly detection method that integrates a full encoder-decoder xLSTM
architecture, purpose-built for multivariate time series data. Our encoder
processes input sequences to capture historical context, while the decoder is
devised in two separate variants of the method. In the forecasting approach,
the decoder iteratively generates forecasted future values xLSTMAD-F, while the
reconstruction approach reconstructs the input time series from its encoded
counterpart xLSTMAD-R. We investigate the performance of two loss functions:
Mean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider
local reconstruction fidelity and global sequence alignment, respectively. We
evaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17
real-world datasets, using state-of-the-art challenging metrics such as VUS-PR.
In our results, xLSTM showcases state-of-the-art accuracy, outperforming 23
popular anomaly detection baselines. Our paper is the first work revealing the
powerful modeling capabilities of xLSTM for anomaly detection, paving the way
for exciting new developments on this subject. Our code is available at:
https://github.com/Nyderx/xlstmad

</details>


### [399] [Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models](https://arxiv.org/abs/2506.22845)
*Batuhan Hangun,Oguz Altun,Onder Eyecioglu*

Main category: cs.LG

TL;DR: The study investigates Quantum Neural Networks (QNNs) for predicting wind turbine power output, showing competitive performance compared to classical methods.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of Quantum Neural Networks (QNNs) in improving predictive performance for power output forecasting in renewable energy systems.

Method: Six QNN configurations based on the Z Feature Map were tested with varying ansatz structures using cross-validation and unseen dataset assessment.

Result: QNNs demonstrated competitive predictive performance and revealed the impact of dataset size and circuit complexity on outcomes in the energy domain.

Conclusion: The results support the feasibility of QNNs as a promising tool for energy prediction tasks, offering insights to researchers incorporating quantum methods.

Abstract: Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine
Learning (QML), are emerging as a powerful alternative to classical machine
learning methods. Recent studies have focused on the applicability of QNNs to
various tasks, such as time-series forecasting, prediction, and classification,
across a wide range of applications, including cybersecurity and medical
imaging. With the increased use of smart grids driven by the integration of
renewable energy systems, machine learning plays an important role in
predicting power demand and detecting system disturbances. This study provides
an in-depth investigation of QNNs for predicting the power output of a wind
turbine. We assess the predictive performance and simulation time of six QNN
configurations that are based on the Z Feature Map for data encoding and
varying ansatz structures. Through detailed cross-validation experiments and
tests on an unseen hold-out dataset, we experimentally demonstrate that QNNs
can achieve predictive performance that is competitive with, and in some cases
marginally better than, the benchmarked classical approaches. Our results also
reveal the effects of dataset size and circuit complexity on predictive
performance and simulation time. We believe our findings will offer valuable
insights for researchers in the energy domain who wish to incorporate quantum
machine learning into their work.

</details>


### [400] [Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles](https://arxiv.org/abs/2506.22848)
*Shengcai Liu,Hui Ou-yang,Zhiyuan Wang,Cheng Chen,Qijun Cai,Yew-Soon Ong,Ke Tang*

Main category: cs.LG

TL;DR: The paper addresses the challenge of learning Bayesian networks from large datasets by introducing Auto-SLE, an approach that automates the creation of structure learning ensembles, achieving significantly better accuracy.


<details>
  <summary>Details</summary>
Motivation: To overcome the unstable accuracy in divide-and-conquer approaches for Bayesian network learning and achieve scalable and consistent performance on large datasets.

Method: Development of Auto-SLE, an automatic method to design structure learning ensembles, which integrates various structure learning algorithms into a divide-and-conquer strategy.

Result: Auto-SLE improves accuracy by 30% to 225% in experiments involving datasets with up to 10,000 variables and demonstrates generalization to even larger datasets.

Conclusion: Employing automatic structure learning ensembles (SLEs) shows great promise for scalable and accurate Bayesian network structure learning especially in large-scale datasets.

Abstract: Learning the structure of Bayesian networks (BNs) from data is challenging,
especially for datasets involving a large number of variables. The recently
proposed divide-and-conquer (D\&D) strategies present a promising approach for
learning large BNs. However, they still face a main issue of unstable learning
accuracy across subproblems. In this work, we introduce the idea of employing
structure learning ensemble (SLE), which combines multiple BN structure
learning algorithms, to consistently achieve high learning accuracy. We further
propose an automatic approach called Auto-SLE for learning near-optimal SLEs,
addressing the challenge of manually designing high-quality SLEs. The learned
SLE is then integrated into a D\&D method. Extensive experiments firmly show
the superiority of our method over D\&D methods with single BN structure
learning algorithm in learning large BNs, achieving accuracy improvement
usually by 30\%$\sim$225\% on datasets involving 10,000 variables. Furthermore,
our method generalizes well to datasets with many more (e.g., 30000) variables
and different network characteristics than those present in the training data
for learning the SLE. These results indicate the significant potential of
employing (automatic learning of) SLEs for scalable BN structure learning.

</details>


### [401] [Training of Spiking Neural Networks with Expectation-Propagation](https://arxiv.org/abs/2506.23757)
*Dan Yao,Steve McLaughlin,Yoann Altmann*

Main category: cs.LG

TL;DR: This paper introduces a unified gradient-free framework for training spiking neural networks using Expectation-Propagation, enabling faster convergence and efficient handling of both deterministic and stochastic weights.


<details>
  <summary>Details</summary>
Motivation: To develop an effective training method for spiking neural networks that can handle nuisance parameters and different types of weights efficiently.

Method: A gradient-free Expectation-Propagation framework that trains spiking neural networks by learning marginal distributions and marginalizing nuisance parameters, allowing batch training for discrete and continuous weights.

Result: The framework achieves faster convergence in practice compared to gradient-based methods and successfully addresses classification and regression tasks.

Conclusion: The proposed method could lead to new and efficient training techniques for deep Bayesian networks, despite practical limitations in convergence guarantees.

Abstract: In this paper, we propose a unifying message-passing framework for training
spiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free
method is capable of learning the marginal distributions of network parameters
and simultaneously marginalizes nuisance parameters, such as the outputs of
hidden layers. This framework allows for the first time, training of discrete
and continuous weights, for deterministic and stochastic spiking networks,
using batches of training samples. Although its convergence is not ensured, the
algorithm converges in practice faster than gradient-based methods, without
requiring a large number of passes through the training data. The
classification and regression results presented pave the way for new efficient
training methods for deep Bayesian networks.

</details>


### [402] [P$^2$U: Progressive Precision Update For Efficient Model Distribution](https://arxiv.org/abs/2506.22871)
*Homayun Afrabandpey,Hamed Rezazadegan Tavakoli*

Main category: cs.LG

TL;DR: The paper introduces P$^2$U, an efficient model distribution method that transmits low-precision models with updates for bandwidth-constrained environments, showcasing improved accuracy-bandwidth-latency tradeoffs across datasets and architectures.


<details>
  <summary>Details</summary>
Motivation: The need to transmit models efficiently in bandwidth-limited environments, such as federated learning and IoT deployments.

Method: P$^2$U transmits low-bit precision models supplemented with updates representing differences between low and high-precision versions, and works with existing compression techniques.

Result: Extensive experiments across datasets and model sizes reveal that P$^2$U offers superior accuracy-bandwidth-latency trade-offs, even with aggressive quantization (e.g., 4-bit).

Conclusion: P$^2$U is a scalable and practical solution for model distribution in bandwidth-constrained settings, complementing current compression strategies for further efficiency improvements.

Abstract: Efficient model distribution is becoming increasingly critical in
bandwidth-constrained environments. In this paper, we propose a simple yet
effective approach called Progressive Precision Update (P$^2$U) to address this
problem. Instead of transmitting the original high-precision model, P$^2$U
transmits a lower-bit precision model, coupled with a model update representing
the difference between the original high-precision model and the transmitted
low precision version. With extensive experiments on various model
architectures, ranging from small models ($1 - 6$ million parameters) to a
large model (more than $100$ million parameters) and using three different data
sets, e.g., chest X-Ray, PASCAL-VOC, and CIFAR-100, we demonstrate that P$^2$U
consistently achieves better tradeoff between accuracy, bandwidth usage and
latency. Moreover, we show that when bandwidth or startup time is the priority,
aggressive quantization (e.g., 4-bit) can be used without severely compromising
performance. These results establish P$^2$U as an effective and practical
solution for scalable and efficient model distribution in low-resource
settings, including federated learning, edge computing, and IoT deployments.
Given that P$^2$U complements existing compression techniques and can be
implemented alongside any compression method, e.g., sparsification,
quantization, pruning, etc., the potential for improvement is even greater.

</details>


### [403] [Interpretable Time Series Autoregression for Periodicity Quantification](https://arxiv.org/abs/2506.22895)
*Xinyu Chen,Vassilis Digalakis Jr,Lijun Ding,Dingyi Zhuang,Jinhua Zhao*

Main category: cs.LG

TL;DR: This paper introduces a sparse autoregression framework with $
$-norm based sparsity for improved interpretability, applying fast optimization strategies and validating through multiple real-world datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the interpretability and scalability of autoregression models for complex time series data with goals like identifying periodicity and spatial-temporal patterns.

Method: The authors reformulate sparse autoregression using mixed-integer optimization and propose a decision variable pruning (DVP) strategy to expedite computations. For multidimensional time series, they design a spatially-and time-varying model and solve it using a two-stage optimization method.

Result: The DVP strategy accelerates computations significantly without sacrificing solution quality. Illustrations on mobility datasets reveal periodic behaviors, while climate datasets highlight dynamic climate trends over decades.

Conclusion: The proposed methods successfully enhance functionality, scalability, and interpretability in handling complex time series data, uncovering valuable spatial and temporal insights.

Abstract: Time series autoregression is a classical statistical model for capturing
auto-correlations and identifying temporal patterns such as periodicity and
seasonality. In this work, we propose a novel sparse autoregression framework
from an interpretable machine learning perspective and the model
interpretability for periodicity quantification is reinforced by $\ell_0$-norm
induced sparsity constraints. On the time-varying time series data, we
reformulate the sparse autoregression and convert the involved optimization
problem into a mixed-integer optimization (MIO). To accelerate it, we develop a
subspace pursuit based decision variable pruning (DVP) strategy to reduce the
search space. On the multidimensional time series that involves complicated
spatial and temporal dimensions, we propose a spatially- and time-varying
sparse autoregression model and resolve the corresponding MIO problem by
developing a two-stage optimization scheme. In particular, the proposed scheme
makes the model scalable to large problems even with millions of decision
variables. Empirically, we conduct extensive experiments to evaluate the
proposed models on real-world time series data. First, we demonstrate that the
MIO solver can be drastically accelerated through the DVP strategy, while
maintaining the same solution quality as a full MIO solver. Applying the
time-varying sparse autoregression model to ridesharing trip data, we uncover
both daily and weekly periodicities and reveal long-term changes in regularity
of human mobility. Second, we demonstrate the spatial patterns of yearly
seasonality in climate variable time series such as temperature and
precipitation across the past four decades, and our model allows to discover
dynamic climate patterns and identify climate phenomena such as El Nino in sea
surface temperature.

</details>


### [404] [Missing-Modality-Aware Graph Neural Network for Cancer Classification](https://arxiv.org/abs/2506.22901)
*Sina Tabakhi,Haiping Lu*

Main category: cs.LG

TL;DR: This paper proposes MAGNET, a graph neural network approach that effectively handles missing modalities in multimodal biological data, outperforming current methods in cancer classification tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of missing modalities in multimodal biological datasets where traditional approaches struggle due to the diversity of missing-modality patterns and scalability concerns as the number of modalities increases.

Method: MAGNET employs a patient-modality multi-head attention mechanism to fuse modality embeddings dynamically based on importance and missingness, followed by a graph neural network utilizing a patient graph with fused multimodal embeddings as node features.

Result: The proposed method significantly outperformed state-of-the-art fusion techniques in cancer classification tasks on three public multiomics datasets that included real-world missingness scenarios.

Conclusion: MAGNET offers an effective and scalable solution for predictive tasks in multimodal biological data with missing modalities, addressing variability in missing-patterns and demonstrating superior performance over existing methods.

Abstract: A key challenge in learning from multimodal biological data is missing
modalities, where all data from some modalities are missing for some patients.
Current fusion methods address this by excluding patients with missing
modalities, imputing missing modalities, or making predictions directly with
partial modalities. However, they often struggle with diverse missing-modality
patterns and the exponential growth of the number of such patterns as the
number of modalities increases. To address these limitations, we propose MAGNET
(Missing-modality-Aware Graph neural NETwork) for direct prediction with
partial modalities, which introduces a patient-modality multi-head attention
mechanism to fuse lower-dimensional modality embeddings based on their
importance and missingness. MAGNET's complexity increases linearly with the
number of modalities while adapting to missing-pattern variability. To generate
predictions, MAGNET further constructs a patient graph with fused multimodal
embeddings as node features and the connectivity determined by the modality
missingness, followed by a conventional graph neural network. Experiments on
three public multiomics datasets for cancer classification, with real-world
instead of artificial missingness, show that MAGNET outperforms the
state-of-the-art fusion methods. The data and code are available at
https://github.com/SinaTabakhi/MAGNET.

</details>


### [405] [Faster Diffusion Models via Higher-Order Approximation](https://arxiv.org/abs/2506.24042)
*Gen Li,Yuchen Zhou,Yuting Wei,Yuxin Chen*

Main category: cs.LG

TL;DR: The paper introduces a training-free diffusion model acceleration algorithm that significantly reduces score function evaluations during data distribution approximation.


<details>
  <summary>Details</summary>
Motivation: To demonstrate a method for efficiently approximating target data distributions using diffusion models without retraining or assuming smoothness/log-concavity.

Method: The authors propose a sampling algorithm inspired by high-order ODE solvers, using Lagrange interpolation and successive refinement for efficient integration.

Result: The algorithm operates with order $d^{1+2/K} \varepsilon^{-1/K}$ score function evaluations for any large integer $K$, and its performance is robust to inexact score estimation.

Conclusion: The proposed method provides a theoretical advancement in diffusion model efficiency, with broad applicability to diverse distributions and robustness to score estimation errors.

Abstract: In this paper, we explore provable acceleration of diffusion models without
any additional retraining. Focusing on the task of approximating a target data
distribution in $\mathbb{R}^d$ to within $\varepsilon$ total-variation
distance, we propose a principled, training-free sampling algorithm that
requires only the order of
  $$ d^{1+2/K} \varepsilon^{-1/K} $$
  score function evaluations (up to log factor) in the presence of accurate
scores, where $K$ is an arbitrarily large fixed integer. This result applies to
a broad class of target data distributions, without the need for assumptions
such as smoothness or log-concavity. Our theory is robust vis-a-vis inexact
score estimation, degrading gracefully as the score estimation error increases
-- without demanding higher-order smoothness on the score estimates as assumed
in previous work. The proposed algorithm draws insight from high-order ODE
solvers, leveraging high-order Lagrange interpolation and successive refinement
to approximate the integral derived from the probability flow ODE.

</details>


### [406] [Towards Time Series Generation Conditioned on Unstructured Natural Language](https://arxiv.org/abs/2506.22927)
*Jaeyun Woo,Jiseok Lee,Brian Kenji Iwana*

Main category: cs.LG

TL;DR: The paper introduces a novel approach for generating time series data from natural language descriptions.


<details>
  <summary>Details</summary>
Motivation: Time series generative AI is underdeveloped despite its importance in fields like finance and climate.

Method: Combining a diffusion model with a language model to produce time series data from text descriptions.

Result: The research successfully demonstrates the feasibility of generating time series based on natural language inputs and introduces a related public dataset.

Conclusion: This approach opens possibilities for custom forecasting, data manipulation, and other advanced applications in time series analysis, supported by a new dataset innovation.

Abstract: Generative Artificial Intelligence (AI) has rapidly become a powerful tool,
capable of generating various types of data, such as images and text. However,
despite the significant advancement of generative AI, time series generative AI
remains underdeveloped, even though the application of time series is essential
in finance, climate, and numerous fields. In this research, we propose a novel
method of generating time series conditioned on unstructured natural language
descriptions. We use a diffusion model combined with a language model to
generate time series from the text. Through the proposed method, we demonstrate
that time series generation based on natural language is possible. The proposed
method can provide various applications such as custom forecasting, time series
manipulation, data augmentation, and transfer learning. Furthermore, we
construct and propose a new public dataset for time series generation,
consisting of 63,010 time series-description pairs.

</details>


### [407] [Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime](https://arxiv.org/abs/2506.24120)
*Yuqing Wang,Shangding Gu*

Main category: cs.LG

TL;DR: This paper investigates the role of data uniformity in improving training efficiency and performance in large language models, introducing the metric $h_{\min}$ to quantify data distribution's impact.


<details>
  <summary>Details</summary>
Motivation: The motivation is to identify general and quantitative principles for data selection that consistently enhance performance, especially for complex tasks with limited prior knowledge.

Method: The authors theoretically and experimentally demonstrate that selecting uniformly distributed data points, quantified by maximizing $h_{\min}$ (minimum pairwise distance), improves training dynamics and reduces approximation error in neural networks.

Result: The study shows that data selection maximizing $h_{\min}$ accelerates training, boosts performance, and generalizes well across various datasets, model sizes, and optimization strategies.

Conclusion: Uniformity in data distribution is a valuable principle for data-driven tasks, offering both theoretical insights and practical benefits for improving large language model training.

Abstract: Data selection plays a crucial role in data-driven decision-making, including
in large language models (LLMs), and is typically task-dependent. Properties
such as data quality and diversity have been extensively studied and are known
to enhance model performance. However, it remains unclear whether there exist
other quantitative and general principles of data selection that can
consistently improve performance, especially for complex tasks with limited
prior knowledge. In this paper, we demonstrate that selecting more uniformly
distributed data can improve training efficiency while enhancing performance.
Specifically, we establish that more uniform (less biased) distribution leads
to a larger minimum pairwise distance between data points, denoted by
$h_{\min}$, and prove that a smaller $h_{\min}$ can slow down the training
dynamics of gradient descent (GD). Moreover, we theoretically show that the
approximation error of neural networks decreases as $h_{\min}$ increases. Our
analysis introduces a convergence framework for GD beyond the Neural Tangent
Kernel (NTK) regime, applicable to a broad class of architectures, including
transformers, without requiring Lipschitz smoothness. This framework further
provides theoretical justification for the use of residual connections and
function compositions in deep neural architectures. In the end, we conduct
comprehensive experiments for supervised fine-tuning across various settings,
including different optimization strategies, model sizes, and training
datasets. The results consistently demonstrate that selecting data by
maximizing pairwise distance significantly accelerates training and achieves
comparable or better performance in LLMs across diverse datasets. Code and
Datasets are available at the link:
https://github.com/SafeRL-Lab/data-uniformity.

</details>


### [408] [Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration](https://arxiv.org/abs/2506.22929)
*Chen Zhang*

Main category: cs.LG

TL;DR: The paper proposes a parallel computation architecture to overcome the computational limitations of deep learning for high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: Deep learning struggles with computational inefficiencies when applied to high-dimensional data due to the curse of dimensionality, creating a need for effective solutions.

Method: A new parallel computation architecture is introduced, leveraging space completeness to decompose high-dimensional data for distributed processing.

Result: This architecture integrates data mining and parallel-optimized machine learning methods, enhancing scientific computations for diverse data types under a unified system.

Conclusion: The proposed framework enables efficient handling of high-dimensional data, supporting advanced analysis and broad applicability across scientific domains such as medical imaging.

Abstract: While deep learning excels in natural image and language processing, its
application to high-dimensional data faces computational challenges due to the
dimensionality curse. Current large-scale data tools focus on business-oriented
descriptive statistics, lacking mathematical statistics support for advanced
analysis. We propose a parallel computation architecture based on space
completeness, decomposing high-dimensional data into dimension-independent
structures for distributed processing. This framework enables seamless
integration of data mining and parallel-optimized machine learning methods,
supporting scientific computations across diverse data types like medical and
natural images within a unified system.

</details>


### [409] [Infinite Sampling: Efficient and Stable Grouped RL Training for Large Language Models](https://arxiv.org/abs/2506.22950)
*Liangyu Wang,Huanyi Xie,Xinhai Wang,Tianjin Huang,Mengdi Li,Di Wang*

Main category: cs.LG

TL;DR: This paper introduces Infinite Sampling, a framework enhancing the scalability of Group Reward Policy Optimization (GRPO) in fine-tuning large language models by decoupling memory usage from group size.


<details>
  <summary>Details</summary>
Motivation: Scaling GRPO-based fine-tuning of large language models is challenging due to high memory usage when generating multiple responses per prompt, making it inefficient under hardware constraints.

Method: The proposed Infinite Sampling framework includes three components: (1) micro sampling groups to break down large groups into smaller memory-efficient rounds, (2) continuous sampling for better GPU utilization, and (3) a length-aware scheduler for runtime efficiency via a two-stage plan combining global grouping and runtime refill strategies.

Result: The proposed Micro Sampling Groups reduce memory usage by over 50%, and the Infinite Sampling framework increases throughput by 25%, maintaining memory efficiency and enabling stable GRPO training with larger group sizes.

Conclusion: Infinite Sampling significantly improves GRPO training's memory efficiency and throughput, making it more scalable for fine-tuning large language models on constrained hardware.

Abstract: Group-based reinforcement learning algorithms such as Group Reward Policy
Optimization (GRPO) have proven effective for fine-tuning large language models
(LLMs) with human feedback. However, generating and storing multiple responses
per prompt incurs substantial memory overhead, especially as the sample group
size increases, limiting scalability under constrained hardware.
  We propose Infinite Sampling, a framework that enables efficient and stable
GRPO training by decoupling group size from GPU memory usage. It consists of:
(1) micro sampling groups that decompose large groups into memory-feasible
rounds; (2) continuous sampling that interleaves generation across groups to
improve utilization; and (3) a length-aware scheduler combining
token-conditioned sequence length prediction with a two-stage plan: global
grouping via FPTAS and runtime refill via SJF.
  Experiments show that our Micro Sampling Groups reduce peak memory usage by
over 50% compared to full-group decoding (e.g., from 21.55 GB to 10.64 GB on
Qwen3-1.7B). Building on this, Infinite Sampling improves throughput by over
25% compared to the naive micro sampling group method, reducing decoding steps
while maintaining full-length completions and memory usage. Our hybrid
scheduling ensures efficient and stable GRPO training with larger groups under
realistic GPU memory constraints.

</details>


### [410] [Cybersecurity-Focused Anomaly Detection in Connected Autonomous Vehicles Using Machine Learning](https://arxiv.org/abs/2506.22984)
*Prathyush Kumar Reddy Lebaku,Lu Gao,Yunpeng Zhang,Zhixia Li,Yongxin Liu,Tanvir Arafin*

Main category: cs.LG

TL;DR: The study employs stacked LSTM and Random Forest models to detect anomalies in connected autonomous vehicles (CAVs) using a simulated dataset of vehicle behavior, achieving high accuracy and demonstrating effectiveness for anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety and reliability in connected autonomous vehicles (CAVs) by detecting sensor malfunctions, cyber-attacks, and environmental disruptions.

Method: Simulated vehicular behavior to generate a dataset of typical/atypical interactions. Used stacked LSTM model to capture temporal dependencies and sequence-based anomalies, and Random Forest model for ensemble-based anomaly detection.

Result: The Random Forest model achieved an R2 of 0.9830 and MAE of 5.746, while the stacked LSTM model demonstrated strong temporal prediction with an R2 of 0.9998 yet higher MAE of 82.425.

Conclusion: The models effectively detect anomalies in CAV behavior, with stacked LSTM excelling in temporal pattern learning and Random Forest providing strong interpretability and reliability.

Abstract: Anomaly detection in connected autonomous vehicles (CAVs) is crucial for
maintaining safe and reliable transportation networks, as CAVs can be
susceptible to sensor malfunctions, cyber-attacks, and unexpected environmental
disruptions. This study explores an anomaly detection approach by simulating
vehicle behavior, generating a dataset that represents typical and atypical
vehicular interactions. The dataset includes time-series data of position,
speed, and acceleration for multiple connected autonomous vehicles. We utilized
machine learning models to effectively identify abnormal driving patterns.
First, we applied a stacked Long Short-Term Memory (LSTM) model to capture
temporal dependencies and sequence-based anomalies. The stacked LSTM model
processed the sequential data to learn standard driving behaviors.
Additionally, we deployed a Random Forest model to support anomaly detection by
offering ensemble-based predictions, which enhanced model interpretability and
performance. The Random Forest model achieved an R2 of 0.9830, MAE of 5.746,
and a 95th percentile anomaly threshold of 14.18, while the stacked LSTM model
attained an R2 of 0.9998, MAE of 82.425, and a 95th percentile anomaly
threshold of 265.63. These results demonstrate the models' effectiveness in
accurately predicting vehicle trajectories and detecting anomalies in
autonomous driving scenarios.

</details>


### [411] [A Reinforcement Learning Approach for Optimal Control in Microgrids](https://arxiv.org/abs/2506.22995)
*Davide Salaorni,Federico Bianchi,Francesco Trovò,Marcello Restelli*

Main category: cs.LG

TL;DR: The paper introduces an RL-based methodology for optimizing microgrid energy management using a digital twin and leverages historical data for enhanced decision-making.


<details>
  <summary>Details</summary>
Motivation: The integration of renewable energy sources necessitates decentralized energy production and consumption solutions to transform traditional power grids.

Method: The study employs reinforcement learning (RL) with a digital twin model to analyze storage dynamics, incorporating degradation factors, based on real-world data from Italy.

Result: The RL-based strategy outperformed rule-based methods and existing RL benchmarks in optimizing microgrid energy management.

Conclusion: The approach provides a robust and intelligent solution for decentralized energy management systems in microgrids.

Abstract: The increasing integration of renewable energy sources (RESs) is transforming
traditional power grid networks, which require new approaches for managing
decentralized energy production and consumption. Microgrids (MGs) provide a
promising solution by enabling localized control over energy generation,
storage, and distribution. This paper presents a novel reinforcement learning
(RL)-based methodology for optimizing microgrid energy management.
Specifically, we propose an RL agent that learns optimal energy trading and
storage policies by leveraging historical data on energy production,
consumption, and market prices. A digital twin (DT) is used to simulate the
energy storage system dynamics, incorporating degradation factors to ensure a
realistic emulation of the analysed setting. Our approach is validated through
an experimental campaign using real-world data from a power grid located in the
Italian territory. The results indicate that the proposed RL-based strategy
outperforms rule-based methods and existing RL benchmarks, offering a robust
solution for intelligent microgrid management.

</details>


### [412] [BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs](https://arxiv.org/abs/2506.23024)
*Jerry Liu,Yasa Baig,Denise Hui Jean Lee,Rajat Vadiraj Dwaraknath,Atri Rudra,Chris Ré*

Main category: cs.LG

TL;DR: The paper introduces the Barycentric Weight Layer (BWLer), which overcomes precision limitations of Physics-informed neural networks (PINNs) for solving PDEs, significantly improving performance.


<details>
  <summary>Details</summary>
Motivation: To address the gap between the flexibility of PINNs to solve PDEs and the high accuracy demanded by scientific tasks, which current PINN architectures fail to achieve.

Method: The method involves introducing the BWLer, a barycentric polynomial interpolation-based architecture, which can be added to (BWLer-hat) or replace (explicit BWLer) the typical multi-layer perceptron (MLP). It separates solution representation from PDE loss differentiation, and addresses conditioning of PDE losses with preconditioning and spectral derivatives.

Result: The BWLer improves RMSE performance by up to 30x for convection, 10x for reaction, and 1800x for wave equations, and even achieves near-machine precision on certain benchmarks, vastly outperforming typical PINNs.

Conclusion: The paper provides a practical approach to merge the flexibility of PINNs with the precision of classical spectral solvers, making a significant leap in solving PDE problems efficiently and accurately.

Abstract: Physics-informed neural networks (PINNs) offer a flexible way to solve
partial differential equations (PDEs) with machine learning, yet they still
fall well short of the machine-precision accuracy many scientific tasks demand.
In this work, we investigate whether the precision ceiling comes from the
ill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)
architecture. We introduce the Barycentric Weight Layer (BWLer), which models
the PDE solution through barycentric polynomial interpolation. A BWLer can be
added on top of an existing MLP (a BWLer-hat) or replace it completely
(explicit BWLer), cleanly separating how we represent the solution from how we
take derivatives for the PDE loss. Using BWLer, we identify fundamental
precision limitations within the MLP: on a simple 1-D interpolation task, even
MLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above
float64 machine precision -- before any PDE terms are added. In PDE learning,
adding a BWLer lifts this ceiling and exposes a tradeoff between achievable
accuracy and the conditioning of the PDE loss. For linear PDEs we fully
characterize this tradeoff with an explicit error decomposition and navigate it
during training with spectral derivatives and preconditioning. Across five
benchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for
convection, 10x for reaction, and 1800x for wave equations while remaining
compatible with first-order optimizers. Replacing the MLP entirely lets an
explicit BWLer reach near-machine-precision on convection, reaction, and wave
problems (up to 10 billion times better than prior results) and match the
performance of standard PINNs on stiff Burgers' and irregular-geometry Poisson
problems. Together, these findings point to a practical path for combining the
flexibility of PINNs with the precision of classical spectral solvers.

</details>


### [413] [Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models](https://arxiv.org/abs/2506.23025)
*Tejas Vaidhya,Ayush Kaushal,Vineet Jain,Francis Couture Harpin,Prashant Shishodia,Majid Behbahani,Yuriy Nevmyvaka,Irina Rish*

Main category: cs.LG

TL;DR: The paper focuses on improving inference efficiency in large language models (LLMs) by employing ternary language models (TriLMs) with quantization-aware training and novel packing schemes.


<details>
  <summary>Details</summary>
Motivation: Modern GPUs struggle with memory bandwidth and capacity deficiencies during LLM inference. There is a need to reduce these inefficiencies while maintaining model performance.

Method: The authors introduce TriLMs using quantization-aware training, scaling laws analysis, novel 2-bit and 1.6-bit packing schemes, and a GPU kernel called TriRun for accelerated inference.

Result: TriLMs showed performance gains with scalability, the proposed packing schemes accelerated inference across CPU and GPU architectures, and TriRun achieved up to 5× speedup over floating-point baselines.

Conclusion: The study demonstrates practical improvements in inference efficiency, scalability, and deployability of LLMs, with open resources to encourage further exploration.

Abstract: Large language models (LLMs) are increasingly used across research and
industry applications, yet their inference efficiency remains a significant
challenge. As the computational power of modern GPU architectures continuously
improves, their memory bandwidth and capacity have not scaled proportionally,
creating a critical bottleneck during inference. To address this, we
investigate ternary language models (TriLMs) that employ quantization-aware
training to significantly reduce memory requirements. We first analyze the
scalability of TriLMs by conducting a scaling law analysis, revealing that
TriLMs benefit more from increasing training data than from scaling model
parameters. Based on this observation, we introduce Spectra-1.1, an open suite
of TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained
performance gains at scale. Furthermore, to improve inference efficiency, we
propose novel 2-bit and 1.6-bit packing schemes for ternary weights, which
demonstrate accelerated inference across various CPU architectures. Also,
building on the 2-bit packing, we develop a GPU kernel called TriRun that
accelerates end-to-end model inference by up to 5 times compared to
floating-point baselines. To encourage further exploration and development of
TriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.
Overall, our work lays the foundation for building and deploying efficient
LLMs, providing a valuable resource for the research community.

</details>


### [414] [Fragile, Robust, and Antifragile: A Perspective from Parameter Responses in Reinforcement Learning Under Stress](https://arxiv.org/abs/2506.23036)
*Zain ul Abdeen,Ming Jin*

Main category: cs.LG

TL;DR: The paper analyzes robustness in reinforcement learning (RL) policies using internal and external stresses, uncovering factors for improving RL adaptability.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand and enhance the robustness and adaptability of RL policies by identifying and leveraging critical parameters under stress conditions.

Method: A dual approach is employed: synaptic filtering to simulate internal stress by perturbing parameters, and adversarial attacks to impose external stress on agent observations. Parameters are classified as fragile, robust, or antifragile based on their impact on RL performance.

Result: The analysis reveals antifragile parameters that improve RL policy performance under stress. Results demonstrate the utility of selective parameter filtering techniques in boosting policy adaptability.

Conclusion: The findings highlight the potential to design more robust and antifragile RL systems by focusing on critical parameters, paving the way for future advancements in RL robustness research.

Abstract: This paper explores Reinforcement learning (RL) policy robustness by
systematically analyzing network parameters under internal and external
stresses. Inspired by synaptic plasticity in neuroscience, synaptic filtering
introduces internal stress by selectively perturbing parameters, while
adversarial attacks apply external stress through modified agent observations.
This dual approach enables the classification of parameters as fragile, robust,
or antifragile, based on their influence on policy performance in clean and
adversarial settings. Parameter scores are defined to quantify these
characteristics, and the framework is validated on PPO-trained agents in Mujoco
continuous control environments. The results highlight the presence of
antifragile parameters that enhance policy performance under stress,
demonstrating the potential of targeted filtering techniques to improve RL
policy adaptability. These insights provide a foundation for future
advancements in the design of robust and antifragile RL systems.

</details>


### [415] [ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation](https://arxiv.org/abs/2506.23041)
*Chengyu Dong,Huan Gui,Noveen Sachdeva,Long Jin,Ke Yin,Jingbo Shang,Lichan Hong,Ed H. Chi,Zhe Zhao*

Main category: cs.LG

TL;DR: This paper proposes methods to make knowledge distillation from pretrained Vision Transformers (ViTs) more effective by introducing mutual information-aware fine-tuning and reweighting strategies for MLP blocks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of ineffective knowledge transfer from strong, large-scale pretrained visual representation models to small, task-specific models.

Method: The method involves mutual information-aware optimization during fine-tuning and a heuristic approach of reweighting MLP blocks to counterbalance mutual information loss in highly-imbalanced datasets.

Result: The proposed approach enhances knowledge transfer, allowing small student models to better utilize information from strong pretrained models.

Conclusion: Mutual information-aware fine-tuning and reweighting strategies improve the effectiveness of knowledge transfer from pretrained ViTs to student models, especially for small or imbalanced datasets.

Abstract: Knowledge distillation from pretrained visual representation models offers an
effective approach to improve small, task-specific production models. However,
the effectiveness of such knowledge transfer drops significantly when
distilling from strong models that are pretrained in a large scale. In this
paper, we address this challenge for pretrained Vision Transformers (ViTs) by
exploring methods to fine-tune them for more effective knowledge transfer.
Motivated by the connection between mutual information and distillation
effectiveness, we propose to employ mutual information-aware optimization
during finetuning. For small or highly-imbalanced downstream datasets where
such optimization becomes less effective, we introduce a simple yet effective
heuristic of reweighting MLP blocks. This approach is inspired by our
observation that top MLP blocks are primarily responsible for mutual
information loss. Our method enables small student models to benefit from those
pretrained models among the strongest.

</details>


### [416] [ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks](https://arxiv.org/abs/2402.09146)
*Muhammad Kashif,Muhammad Shafique*

Main category: cs.LG

TL;DR: The paper introduces Residual Quanvolutional Neural Networks (ResQuNNs), addressing challenges of gradient flow in trainable quanvolutional layers using residual blocks to improve training.


<details>
  <summary>Details</summary>
Motivation: Traditional quanvolutional layers in Quanvolutional Neural Networks (QuNNs) are static and lack adaptability, limiting their performance enhancement potential.

Method: The authors propose trainable quanvolutional layers combined with a residual learning architecture (ResQuNNs), utilizing residual blocks to address gradient-based optimization challenges.

Result: Extensive experiments reveal an efficient configuration of residual blocks, enhancing gradient accessibility across all layers and leading to more effective training.

Conclusion: The placement and use of residual blocks within QuNNs is critical for improved performance, marking progress in quantum deep learning and its practical applications.

Abstract: In this paper, we present a novel framework for enhancing the performance of
Quanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional
layers and addressing the critical challenges associated with them. Traditional
quanvolutional layers, although beneficial for feature extraction, have largely
been static, offering limited adaptability. Unlike state-of-the-art, our
research overcomes this limitation by enabling training within these layers,
significantly increasing the flexibility and potential of QuNNs. However, the
introduction of multiple trainable quanvolutional layers induces complexities
in gradient-based optimization, primarily due to the difficulty in accessing
gradients across these layers. To resolve this, we propose a novel
architecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging
the concept of residual learning, which facilitates the flow of gradients by
adding skip connections between layers. By inserting residual blocks between
quanvolutional layers, we ensure enhanced gradient access throughout the
network, leading to improved training performance. Moreover, we provide
empirical evidence on the strategic placement of these residual blocks within
QuNNs. Through extensive experimentation, we identify an efficient
configuration of residual blocks, which enables gradients across all the layers
in the network that eventually results in efficient training. Our findings
suggest that the precise location of residual blocks plays a crucial role in
maximizing the performance gains in QuNNs. Our results mark a substantial step
forward in the evolution of quantum deep learning, offering new avenues for
both theoretical development and practical quantum computing applications.

</details>


### [417] [Double-Diffusion: Diffusion Conditioned Diffusion Probabilistic Model For Air Quality Prediction](https://arxiv.org/abs/2506.23053)
*Hanlin Dong,Arian Prabowo,Hao Xue,Flora D. Salim*

Main category: cs.LG

TL;DR: Double-Diffusion introduces a physics-guided approach to enhance air quality prediction using probabilistic diffusion modeling, achieving superior results while also improving computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Air quality prediction is complicated by its spatio-temporal and dynamic uncertainties, requiring a balance between deterministic physics-based modeling and stochastic approaches.

Method: The paper proposes Double-Diffusion, using physics principles as conditioning factors in a diffusion probabilistic model, integrated with a new denoiser architecture and improved sampling strategies.

Result: Double-Diffusion outperforms other probabilistic models on real-life datasets in most scenarios, with computational efficiency gains of 30-50% while improving CRPS scores by 3-12%.

Conclusion: Double-Diffusion successfully combines deterministic physics-guided modeling with stochastic prediction, offering better accuracy and efficiency in air quality forecasting.

Abstract: Air quality prediction is a challenging forecasting task due to its
spatio-temporal complexity and the inherent dynamics as well as uncertainty.
Most of the current models handle these two challenges by applying Graph Neural
Networks or known physics principles, and quantifying stochasticity through
probabilistic networks like Diffusion models. Nevertheless, finding the right
balancing point between the certainties and uncertainties remains an open
question. Therefore, we propose Double-Diffusion, a novel diffusion
probabilistic model that harnesses the power of known physics to guide air
quality forecasting with stochasticity. To the best of our knowledge, while
precedents have been made of using conditional diffusion models to predict air
pollution, this is the first attempt to use physics as a conditional generative
approach for air quality prediction. Along with a sampling strategy adopted
from image restoration and a new denoiser architecture, Double-Diffusion ranks
first in most evaluation scenarios across two real-life datasets compared with
other probabilistic models, it also cuts inference time by 50% to 30% while
enjoying an increase between 3-12% in Continuous Ranked Probabilistic Score
(CRPS).

</details>


### [418] [Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis](https://arxiv.org/abs/2506.23055)
*Hiro Taiyo Hamada,Ippei Fujisawa,Genji Kawakita,Yuki Yamada*

Main category: cs.LG

TL;DR: The paper assesses concept alignment between large language models (LLMs) and human psychological dimensions using standardized questionnaires. It finds that GPT-4 significantly outperforms GPT-3.5, BERT, and random baselines in classification accuracy.


<details>
  <summary>Details</summary>
Motivation: The study seeks to understand whether large language models internalize human psychological concepts accurately, addressing concerns about concept alignment and interpretability in AI systems.

Method: The authors used 43 standardized psychological questionnaires and pairwise similarity analysis to evaluate concept reconstruction and classification. Cluster structures were compared with original categorical labels using hierarchical clustering.

Result: GPT-4 achieved 66.2% classification accuracy, outperforming GPT-3.5 (55.9%), BERT (48.1%), and random baselines (31.9%). Additionally, GPT-4's semantic similarity scores correlated with human response patterns in psychological questionnaires.

Conclusion: Modern large language models demonstrate measurable alignment with human psychological constructs, suggesting their potential for developing interpretable AI systems and identifying representational biases.

Abstract: Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities
in producing human-like text. However, it is unclear how accurately these
models internalize concepts that shape human thought and behavior. Here, we
developed a quantitative framework to assess concept alignment between LLMs and
human psychological dimensions using 43 standardized psychological
questionnaires, selected for their established validity in measuring distinct
psychological constructs. Our method evaluates how accurately language models
reconstruct and classify questionnaire items through pairwise similarity
analysis. We compared resulting cluster structures with the original
categorical labels using hierarchical clustering. A GPT-4 model achieved
superior classification accuracy (66.2\%), significantly outperforming GPT-3.5
(55.9\%) and BERT (48.1\%), all exceeding random baseline performance (31.9\%).
We also demonstrated that the estimated semantic similarity from GPT-4 is
associated with Pearson's correlation coefficients of human responses in
multiple psychological questionnaires. This framework provides a novel approach
to evaluate the alignment of the human-LLM concept and identify potential
representational biases. Our findings demonstrate that modern LLMs can
approximate human psychological constructs with measurable accuracy, offering
insights for developing more interpretable AI systems.

</details>


### [419] [Curious Causality-Seeking Agents Learn Meta Causal World](https://arxiv.org/abs/2506.23068)
*Zhiyu Zhao,Haoxuan Li,Haifeng Zhang,Jun Wang,Francesco Faccio,Jürgen Schmidhuber,Mengyue Yang*

Main category: cs.LG

TL;DR: This paper introduces the concept of Meta-Causal Graphs as world models to address challenges in understanding shifting causal mechanisms in dynamic environments. It also proposes a Causality-Seeking Agent to identify, discover, and refine such causal structures.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the recognition that conventional world models assume stable causal mechanisms, but real-world observations often show shifting causal dynamics due to latent underlying states.

Method: The approach centers around the Meta-Causal Graph representation, composed of causal subgraphs triggered by latent meta states. A Causality-Seeking Agent employs curiosity-driven interventions and iterative refinement to explore and understand causal shifts.

Result: Experiments demonstrate the efficiency of the proposed method in capturing causal dynamics shifts and generalizing to new contexts, both on synthetic and robotic manipulation tasks.

Conclusion: The study successfully showcases a framework for dynamically understanding and representing causal structures in changing environments, highlighting its utility in both theoretical and practical applications.

Abstract: When building a world model, a common assumption is that the environment has
a single, unchanging underlying causal rule, like applying Newton's laws to
every situation. In reality, what appears as a drifting causal mechanism is
often the manifestation of a fixed underlying mechanism seen through a narrow
observational window. This brings about a problem that, when building a world
model, even subtle shifts in policy or environment states can alter the very
observed causal mechanisms. In this work, we introduce the \textbf{Meta-Causal
Graph} as world models, a minimal unified representation that efficiently
encodes the transformation rules governing how causal structures shift across
different latent world states. A single Meta-Causal Graph is composed of
multiple causal subgraphs, each triggered by meta state, which is in the latent
state space. Building on this representation, we introduce a
\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta
states that trigger each subgraph, (2) discover the corresponding causal
relationships by agent curiosity-driven intervention policy, and (3)
iteratively refine the Meta-Causal Graph through ongoing curiosity-driven
exploration and agent experiences. Experiments on both synthetic tasks and a
challenging robot arm manipulation task demonstrate that our method robustly
captures shifts in causal dynamics and generalizes effectively to previously
unseen contexts.

</details>


### [420] [Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings](https://arxiv.org/abs/2506.23145)
*Shahad Hardan,Darya Taratynova,Abdelmajid Essofi,Karthik Nandakumar,Mohammad Yaqub*

Main category: cs.LG

TL;DR: The paper introduces Forget-MI, a machine unlearning framework, to address privacy concerns in AI models trained on multimodal medical data while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Preserving privacy in AI, especially in healthcare, is difficult due to the need to remove sensitive patient data from multimodal models, which are prevalent and challenging to adapt for "machine unlearning."

Method: The authors designed Forget-MI using specialized loss functions and perturbation techniques to unlearn specific data while retaining performance on remaining data.

Result: Forget-MI outperformed baseline methods, achieving a reduction of 0.202 in Membership Inference Attack (MIA) risk and decreased AUC and F1 scores on the forgotten dataset by 0.221 and 0.305, respectively, without compromising test set performance.

Conclusion: Forget-MI constitutes a robust solution for privacy-preserving unlearning in multimodal medical data, balancing security, retention, and performance.

Abstract: Privacy preservation in AI is crucial, especially in healthcare, where models
rely on sensitive patient data. In the emerging field of machine unlearning,
existing methodologies struggle to remove patient data from trained multimodal
architectures, which are widely used in healthcare. We propose Forget-MI, a
novel machine unlearning method for multimodal medical data, by establishing
loss functions and perturbation techniques. Our approach unlearns unimodal and
joint representations of the data requested to be forgotten while preserving
knowledge from the remaining data and maintaining comparable performance to the
original model. We evaluate our results using performance on the forget
dataset, performance on the test dataset, and Membership Inference Attack
(MIA), which measures the attacker's ability to distinguish the forget dataset
from the training dataset. Our model outperforms the existing approaches that
aim to reduce MIA and the performance on the forget dataset while keeping an
equivalent performance on the test set. Specifically, our approach reduces MIA
by 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,
respectively. Additionally, our performance on the test set matches that of the
retrained model, while allowing forgetting. Code is available at
https://github.com/BioMedIA-MBZUAI/Forget-MI.git

</details>


### [421] [maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics](https://arxiv.org/abs/2506.23147)
*Jonathan Schuster,Fabian Transchel*

Main category: cs.LG

TL;DR: This paper discusses maneuverRecognition, a Python package for classifying driving maneuvers using telematic sensor data and LSTM-based models.


<details>
  <summary>Details</summary>
Motivation: Enhance telematics-based driving maneuver recognition to improve insurance personalization, road safety, fuel efficiency, and environmental impact while addressing practical tool limitations.

Method: Developed a Python package with preprocessing, modeling, and evaluation functions, integrated with a ready-to-use, modifiable LSTM-based network structure.

Result: The package was tested using real driving data collected from three individuals via smartphone sensors.

Conclusion: maneuverRecognition demonstrates utility in handling maneuver recognition tasks, facilitating efficient and accessible development of predictive telematics models.

Abstract: In the domain of vehicle telematics the automated recognition of driving
maneuvers is used to classify and evaluate driving behaviour. This not only
serves as a component to enhance the personalization of insurance policies, but
also to increase road safety, reduce accidents and the associated costs as well
as to reduce fuel consumption and support environmentally friendly driving. In
this context maneuver recognition technically requires a continuous application
of time series classification which poses special challenges to the transfer,
preprocessing and storage of telematic sensor data, the training of predictive
models, and the prediction itself. Although much research has been done in the
field of gathering relevant data or regarding the methods to build predictive
models for the task of maneuver recognition, there is a practical need for
python packages and functions that allow to quickly transform data into the
required structure as well as to build and evaluate such models. The
maneuverRecognition package was therefore developed to provide the necessary
functions for preprocessing, modelling and evaluation and also includes a ready
to use LSTM based network structure that can be modified. The implementation of
the package is demonstrated using real driving data of three different persons
recorded via smartphone sensors.

</details>


### [422] [Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data](https://arxiv.org/abs/2506.23174)
*Chen Gong,Bo Liang,Wei Gao,Chenren Xu*

Main category: cs.LG

TL;DR: The paper presents SynCheck, a scheme to address quality limitations in synthetic wireless data and improve task performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the unpredictable quality of synthetic wireless data that can degrade the performance of wireless sensing tasks.

Method: The authors propose quantifiable metrics (affinity and diversity) to assess synthetic data quality and introduce SynCheck, a quality-guided data refinement scheme during task model training.

Result: SynCheck showcased a consistent improvement in performance, achieving a 4.3% gain despite initial performance degradation from quality-oblivious approaches.

Conclusion: SynCheck effectively enhances the quality and utilization of synthetic wireless data, solving critical limitations in current generative models.

Abstract: Generative models have gained significant attention for their ability to
produce realistic synthetic data that supplements the quantity of real-world
datasets. While recent studies show performance improvements in wireless
sensing tasks by incorporating all synthetic data into training sets, the
quality of synthetic data remains unpredictable and the resulting performance
gains are not guaranteed. To address this gap, we propose tractable and
generalizable metrics to quantify quality attributes of synthetic data -
affinity and diversity. Our assessment reveals prevalent affinity limitation in
current wireless synthetic data, leading to mislabeled data and degraded task
performance. We attribute the quality limitation to generative models' lack of
awareness of untrained conditions and domain-specific processing. To mitigate
these issues, we introduce SynCheck, a quality-guided synthetic data
utilization scheme that refines synthetic data quality during task model
training. Our evaluation demonstrates that SynCheck consistently outperforms
quality-oblivious utilization of synthetic data, and achieves 4.3% performance
improvement even when the previous utilization degrades performance by 13.4%.

</details>


### [423] [Attribution assignment for deep-generative sequence models enables interpretability analysis using positive-only data](https://arxiv.org/abs/2506.23182)
*Robert Frank,Michael Widrich,Rahmad Akbar,Günter Klambauer,Geir Kjetil Sandve,Philippe A. Robert,Victor Greiff*

Main category: cs.LG

TL;DR: This paper introduces GAMA, a method to improve the interpretability of generative models in biological sequence design, enabling insights and validation without requiring negative data.


<details>
  <summary>Details</summary>
Motivation: Developing a method to extract interpretable biological insights from generative models, which was previously hindered by their lack of attribution mechanisms.

Method: The paper introduces Generative Attribution Metric Analysis (GAMA), an attribution method based on Integrated Gradients, validated using synthetic datasets and applied to antibody-antigen binding data.

Result: GAMA successfully identifies biologically relevant features in synthetic datasets and proves useful for interpreting generative sequence design in experimental data.

Conclusion: GAMA bridges the gap in interpretability of generative biological models, allowing for validation and strategic design without needing negatively labeled data.

Abstract: Generative machine learning models offer a powerful framework for therapeutic
design by efficiently exploring large spaces of biological sequences enriched
for desirable properties. Unlike supervised learning methods, which require
both positive and negative labeled data, generative models such as LSTMs can be
trained solely on positively labeled sequences, for example, high-affinity
antibodies. This is particularly advantageous in biological settings where
negative data are scarce, unreliable, or biologically ill-defined. However, the
lack of attribution methods for generative models has hindered the ability to
extract interpretable biological insights from such models. To address this
gap, we developed Generative Attribution Metric Analysis (GAMA), an attribution
method for autoregressive generative models based on Integrated Gradients. We
assessed GAMA using synthetic datasets with known ground truths to characterize
its statistical behavior and validate its ability to recover biologically
relevant features. We further demonstrated the utility of GAMA by applying it
to experimental antibody-antigen binding data. GAMA enables model
interpretability and the validation of generative sequence design strategies
without the need for negative training data.

</details>


### [424] [External Data-Enhanced Meta-Representation for Adaptive Probabilistic Load Forecasting](https://arxiv.org/abs/2506.23201)
*Haoran Li,Muhao Guo,Marija Ilic,Yang Weng,Guangchun Ruan*

Main category: cs.LG

TL;DR: The paper introduces a novel approach to residential load forecasting by dynamically adapting machine learning models using external conditions, achieving higher accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Current forecasting models fail to leverage external factors, such as weather and calendar effects, effectively due to their heterogeneity.

Method: The authors propose a meta-representation framework utilizing hypernetworks and a Mixture-of-Experts mechanism to dynamically adapt forecasting model parameters based on external conditions.

Result: The proposed model (M2oE2) outperforms state-of-the-art methods in accuracy and robustness across diverse residential load datasets, with limited computational overhead.

Conclusion: Integrating external data as meta-knowledge into forecasting models improves expressivity, adaptability, and efficiency, paving the way for better power system reliability in renewable energy contexts.

Abstract: Accurate residential load forecasting is critical for power system
reliability with rising renewable integration and demand-side flexibility.
However, most statistical and machine learning models treat external factors,
such as weather, calendar effects, and pricing, as extra input, ignoring their
heterogeneity, and thus limiting the extraction of useful external information.
We propose a paradigm shift: external data should serve as meta-knowledge to
dynamically adapt the forecasting model itself. Based on this idea, we design a
meta-representation framework using hypernetworks that modulate selected
parameters of a base Deep Learning (DL) model in response to external
conditions. This provides both expressivity and adaptability. We further
integrate a Mixture-of-Experts (MoE) mechanism to enhance efficiency through
selective expert activation, while improving robustness by filtering redundant
external inputs. The resulting model, dubbed as a Meta Mixture of Experts for
External data (M2oE2), achieves substantial improvements in accuracy and
robustness with limited additional overhead, outperforming existing
state-of-the-art methods in diverse load datasets. The dataset and source code
are publicly available at
https://github.com/haorandd/M2oE2\_load\_forecast.git.

</details>


### [425] [Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels](https://arxiv.org/abs/2506.23221)
*Bálint Horváth,Balázs Csanád Csáji*

Main category: cs.LG

TL;DR: The paper introduces a method called SGKI for estimating missing image pixels and quantifying uncertainty, leveraging RKHS and Schur complements.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of estimating missing image data along with providing uncertainty quantifications.

Method: Utilizes a statistical learning approach grounded in Reproducing Kernel Hilbert Spaces (RKHS) for simultaneously estimating missing pixels and constructing non-asymptotic confidence bands.

Result: Proposed SGKI method effectively estimates missing pixels and computes confidence bands, demonstrated through experiments on synthetic and benchmark datasets.

Conclusion: SGKI offers a robust and efficient approach for pixel estimation with uncertainty guarantees, advancing statistical learning methods in image processing.

Abstract: The paper proposes a statistical learning approach to the problem of
estimating missing pixels of images, crucial for image inpainting and
super-resolution problems. One of the main novelties of the method is that it
also provides uncertainty quantifications together with the estimated values.
Our core assumption is that the underlying data-generating function comes from
a Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on
band-limited functions, central to signal processing, which form Paley-Wiener
type RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel
Interpolation (SGKI), is an extension and refinement of a recently developed
kernel method. An advantage of SGKI is that it not only estimates the missing
pixels, but also builds non-asymptotic confidence bands for the unobserved
values, which are simultaneously guaranteed for all missing pixels. We also
show how to compute these bands efficiently using Schur complements, we discuss
a generalization to vector-valued functions, and we present a series of
numerical experiments on various datasets containing synthetically generated
and benchmark images, as well.

</details>


### [426] [Masked Gated Linear Unit](https://arxiv.org/abs/2506.23225)
*Yukito Tajima,Nakamasa Inoue,Yusuke Sekikawa,Ikuro Sato,Rio Yokota*

Main category: cs.LG

TL;DR: This paper introduces Masked Gated Linear Units (MGLUs) to address memory inefficiencies in GLUs used in Large Language Models (LLMs), offering both improved efficiency and competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: GLUs in Large Language Models demand high memory reads due to separate weight matrices for gate and value streams, creating a bottleneck in efficiency.

Method: MGLUs use the Mixture of Element-wise Gating (MoEG) to employ shared weight matrices for both gate and value streams, reducing memory needs, alongside creating FlashMGLU for hardware-level optimization.

Result: MGLUs achieved up to 19.7× inference-time speed-up over a naive PyTorch implementation and were 47% more memory-efficient and 34% faster compared to standard GLUs on the RTX5090 GPU.

Conclusion: Masked Gated Linear Units improve memory efficiency and computational speed while maintaining or surpassing downstream accuracy compared to conventional GLUs.

Abstract: Gated Linear Units (GLUs) have become essential components in the
feed-forward networks of state-of-the-art Large Language Models (LLMs).
However, they require twice as many memory reads compared to feed-forward
layers without gating, due to the use of separate weight matrices for the gate
and value streams. To address this bottleneck, we introduce Masked Gated Linear
Units (MGLUs), a novel family of GLUs with an efficient kernel implementation.
The core contribution of MGLUs include: (1) the Mixture of Element-wise Gating
(MoEG) architecture that learns multiple binary masks, each determining gate or
value assignments at the element level on a single shared weight matrix
resulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly
kernel that yields up to a 19.7 $\times$ inference-time speed-up over a naive
PyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs
despite added architectural complexity on an RTX5090 GPU. In LLM experiments,
the Swish-activated variant SwiMGLU preserves its memory advantages while
matching - or even surpassing - the downstream accuracy of the SwiGLU baseline.

</details>


### [427] [Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging](https://arxiv.org/abs/2506.23266)
*Lujun Li,Zhu Qiyuan,Jiacheng Wang,Wei Li,Hao Gu,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: The paper presents Sub-MoE, a novel framework for compressing Mixture of Experts (MoE) large language models by merging expert parameters using subspace techniques. It significantly reduces parameter scale while retaining high model performance.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenges of memory, storage, and deployment posed by the massive parameter scale of Mixture of Experts (MoE) models. Existing expert merging methods face limitations due to parameter conflicts from expert specialization.

Method: The framework, Sub-MoE, involves two main phases: (1) Adaptive Expert Clustering, which groups similar experts using K-means clustering based on cosine similarity of outputs, and (2) Subspace Expert Merging, which applies joint Singular Value Decomposition (SVD) to align and merge expert weights in a shared subspace.

Result: Sub-MoE outperformed existing expert pruning and merging techniques in extensive experiments on multiple MoE models. For example, it retained 96% and 86% of baseline performance with 25% and 50% fewer experts, respectively, on Mixtral-8x7B benchmarks.

Conclusion: Sub-MoE effectively compresses large MoE models by resolving parameter conflicts through a subspace approach, achieving significant parameter reduction while sustaining strong performance. The method is promising for optimizing model inference and deployment.

Abstract: Mixture of Experts (MoE) LLMs face significant obstacles due to their massive
parameter scale, which imposes memory, storage, and deployment challenges.
Although recent expert merging methods promise greater efficiency by
consolidating multiple experts, they are fundamentally hindered by parameter
conflicts arising from expert specialization. In this paper, we present
Sub-MoE, a novel MoE compression framework via Subspace Expert Merging. Our key
insight is to perform joint Singular Value Decomposition (SVD) on concatenated
expert weights, reducing conflicting parameters by extracting shared
$U$-matrices while enabling effective merging of the expert-specific $V$
components. Specifically, Sub-MoE consists of two innovative phases: (1)
Adaptive Expert Clustering, which groups functionally coherent experts via
K-means clustering based on cosine similarity of expert outputs; and (2)
Subspace Expert Merging, which first enforces Experts Union Decomposition to
derive the shared $U$-matrix across experts in the same group, then pursues
frequency-based merging for individual $V$-matrices, and finalizes expert
reconstruction using the merged $V$-matrix. In this way, we align and fuse
experts in a shared subspace, and can be extended with intra-expert compression
for further inference optimization. Extensive experiments on Mixtral, DeepSeek,
and Qwen-1.5|3 MoE LLMs demonstrate that our Sub-MoE significantly outperforms
existing expert pruning and merging methods. Notably, our Sub-MoE maintains
96\%|86\% of original performance with 25\%|50\% expert reduction on
Mixtral-8x7B in zero-shot benchmarks. Code will be released at
https://github.com/lliai/MoERazor.

</details>


### [428] [Predicting thinking time in Reasoning models](https://arxiv.org/abs/2506.23274)
*Hans Peter Lynsgøe Raaschou-jensen,Constanza Fierro,Anders Søgaard*

Main category: cs.LG

TL;DR: The paper addresses the challenge of unpredictable reasoning time in AI models by proposing methods to predict their 'thinking time' and enhance user experience.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle user frustration caused by the unpredictability of how long reasoning models take to complete complex tasks.

Method: The authors developed methods for both online and offline prediction of the reasoning time of large language models (LLMs).

Result: The methods aim to provide an estimate of model 'thinking time' to improve user interaction.

Conclusion: Introducing a 'progress bar for reasoning' helps mitigate user frustration and guides future research in model usability.

Abstract: Reasoning models that produce long, hidden chains of thought have emerged as
powerful tools for complex, reasoning-intensive
tasks\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,
openai2024openaio1card}. However, this paradigm introduces a new user
experience challenge: users have little insight into how much time the model
will spend reasoning before returning an answer. This unpredictability, can
lead to user frustration and is likely to compound as LLMs can produce
increasingly long tasks asynchronously
\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and
evaluate methods for both online and offline prediction of model "thinking
time," aiming to develop a practical "progress bar for reasoning." We discuss
the implications for user interaction and future research directions.

</details>


### [429] [BAPE: Learning an Explicit Bayes Classifier for Long-tailed Visual Recognition](https://arxiv.org/abs/2506.23280)
*Chaoqun Du,Yulin Wang,Shiji Song,Gao Huang*

Main category: cs.LG

TL;DR: The paper introduces BAPE, a novel method to explicitly model posterior probabilities for a Bayes classifier, specifically addressing complications arising in long-tailed data distributions.


<details>
  <summary>Details</summary>
Motivation: Deep learning often struggles with long-tailed distributions, leading to issues like gradient imbalance and sub-optimal classifiers, when using the common Softmax cross-entropy loss.

Method: The method, BAPE, explicitly models and solves the posterior parameters and incorporates a distribution adjustment technique to directly learn a Bayes classifier, bypassing gradient descent.

Result: The BAPE method significantly improves performance across various datasets (e.g., CIFAR-10-LT, CIFAR-100-LT) and adapts well to diverse test data distributions.

Conclusion: BAPE offers a practical and efficient approach for long-tailed data scenarios, ensuring a Bayes optimal solution and complementing existing deep learning techniques.

Abstract: Bayesian decision theory advocates the Bayes classifier as the optimal
approach for minimizing the risk in machine learning problems. Current deep
learning algorithms usually solve for the optimal classifier by
\emph{implicitly} estimating the posterior probabilities, \emph{e.g.}, by
minimizing the Softmax cross-entropy loss. This simple methodology has been
proven effective for meticulously balanced academic benchmark datasets.
However, it is not applicable to the long-tailed data distributions in the real
world, where it leads to the gradient imbalance issue and fails to ensure the
Bayes optimal decision rule. To address these challenges, this paper presents a
novel approach (BAPE) that provides a more precise theoretical estimation of
the data distributions by \emph{explicitly} modeling the parameters of the
posterior probabilities and solving them with point estimation. Consequently,
our method directly learns the Bayes classifier without gradient descent based
on Bayes' theorem, simultaneously alleviating the gradient imbalance and
ensuring the Bayes optimal decision rule. Furthermore, we propose a
straightforward yet effective \emph{distribution adjustment} technique. This
method enables the Bayes classifier trained from the long-tailed training set
to effectively adapt to the test data distribution with an arbitrary imbalance
factor, thereby enhancing performance without incurring additional
computational costs. In addition, we demonstrate the gains of our method are
orthogonal to existing learning approaches for long-tailed scenarios, as they
are mostly designed under the principle of \emph{implicitly} estimating the
posterior probabilities. Extensive empirical evaluations on CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist demonstrate that our method
significantly improves the generalization performance of popular deep networks,
despite its simplicity.

</details>


### [430] [Hierarchical Quantized Diffusion Based Tree Generation Method for Hierarchical Representation and Lineage Analysis](https://arxiv.org/abs/2506.23287)
*Zelin Zang,WenZhe Li,Fei Chen,Yongjie Xu,Chang Yu,Zhen Lei,Stan Z. Li*

Main category: cs.LG

TL;DR: The paper introduces HDTree, a diffusion-based method for modeling hierarchical relationships in single-cell data using a unified hierarchical codebook.


<details>
  <summary>Details</summary>
Motivation: Current methods for analyzing high-throughput single-cell differentiation trajectories are limited by computational costs, performance issues, and an inability to capture deep hierarchical relationships.

Method: The approach employs a diffusion-based framework with a hierarchical latent space and eliminates the need for specialized tree branch modules by using a unified hierarchical codebook and quantized diffusion processes.

Result: HDTree outperforms existing methods in terms of accuracy and performance when tested on both general-purpose and single-cell datasets.

Conclusion: HDTree offers a more stable and generative method for hierarchical lineage analysis, facilitating accurate modeling of cellular differentiation paths and aiding downstream biological research.

Abstract: In single-cell research, tracing and analyzing high-throughput single-cell
differentiation trajectories is crucial for understanding complex biological
processes. Key to this is the modeling and generation of hierarchical data that
represents the intrinsic structure within datasets. Traditional methods face
limitations in terms of computational cost, performance, generative capacity,
and stability. Recent VAEs based approaches have made strides in addressing
these challenges but still require specialized network modules for each tree
branch, limiting their stability and ability to capture deep hierarchical
relationships. To overcome these challenges, we introduce diffusion-based
approach called HDTree. HDTree captures tree relationships within a
hierarchical latent space using a unified hierarchical codebook and quantized
diffusion processes to model tree node transitions. This method improves
stability by eliminating branch-specific modules and enhancing generative
capacity through gradual hierarchical changes simulated by the diffusion
process. HDTree's effectiveness is demonstrated through comparisons on both
general-purpose and single-cell datasets, where it outperforms existing methods
in terms of accuracy and performance. These contributions provide a new tool
for hierarchical lineage analysis, enabling more accurate and efficient
modeling of cellular differentiation paths and offering insights for downstream
biological tasks. The code of HDTree is available at anonymous link
https://anonymous.4open.science/r/code_HDTree_review-A8DB.

</details>


### [431] [VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design](https://arxiv.org/abs/2506.23339)
*Malikussaid,Hilal Hudan Nuha*

Main category: cs.LG

TL;DR: VALID-Mol is a framework that enhances the chemical validity of molecules designed by LLMs, increasing valid outputs from 3% to 83%.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of applying Large Language Models (LLMs) to molecular design in drug discovery, particularly due to the chemically invalid or impractical molecules they often generate without domain-specific constraints.

Method: VALID-Mol integrates methodical prompt engineering, automated chemical validation, and a fine-tuned domain-adapted LLM to generate synthesizable molecules with improved properties.

Result: The framework increases the generation of valid chemical structures from 3% to 83%, with predictions indicating up to 17-fold improvement in target affinity while maintaining synthetic accessibility.

Conclusion: VALID-Mol offers a reproducible and generalizable methodology for scientifically-constrained applications of LLMs, enabling reliable molecular design with quantifiable performance improvements.

Abstract: Large Language Models (LLMs) demonstrate remarkable potential for scientific
discovery, but their application in domains requiring factual accuracy and
domain-specific constraints remains challenging. In molecular design for drug
discovery, LLMs can suggest creative molecular modifications but often produce
chemically invalid or impractical structures. We present VALID-Mol, a
systematic framework for integrating chemical validation with LLM-driven
molecular design that increases the rate of generating valid chemical
structures from 3% to 83%. Our approach combines methodical prompt engineering,
automated chemical validation, and a fine-tuned domain-adapted LLM to ensure
reliable generation of synthesizable molecules with improved properties. Beyond
the specific implementation, we contribute a generalizable methodology for
scientifically-constrained LLM applications, with quantifiable reliability
improvements. Computational predictions suggest our framework can generate
promising candidates for synthesis with up to 17-fold computationally predicted
improvements in target affinity while maintaining synthetic accessibility. We
provide a detailed analysis of our prompt engineering process, validation
architecture, and fine-tuning approach, offering a reproducible blueprint for
applying LLMs to other scientific domains where domain-specific validation is
essential.

</details>


### [432] [A case for data valuation transparency via DValCards](https://arxiv.org/abs/2506.23349)
*Keziah Naggita,Julienne LaChance*

Main category: cs.LG

TL;DR: Data valuation methods for machine learning are biased and unstable, leading to technical and ethical issues, solved by the proposed Data Valuation Cards (DValCards) framework.


<details>
  <summary>Details</summary>
Motivation: Data valuation methods have been suggested as tools to not only improve ML model performance but also fairly compensate data owners in data markets, warranting closer examination of their reliability.

Method: The authors analyzed 9 tabular classification datasets and 6 data valuation methods to investigate the effects of data pre-processing, subsampling, and group representation on data valuation metrics. They introduce the Data Valuation Cards (DValCards) framework as a transparency tool.

Result: It was shown that pre-processing can significantly alter data values, subsampling increases class imbalance, and underrepresented groups' data are often undervalued by current metrics.

Conclusion: Data valuation metrics need greater transparency to mitigate their misuse and biases. The introduction of DValCards can promote ethical and trustworthy applications of these metrics.

Abstract: Following the rise in popularity of data-centric machine learning (ML),
various data valuation methods have been proposed to quantify the contribution
of each datapoint to desired ML model performance metrics (e.g., accuracy).
Beyond the technical applications of data valuation methods (e.g., data
cleaning, data acquisition, etc.), it has been suggested that within the
context of data markets, data buyers might utilize such methods to fairly
compensate data owners. Here we demonstrate that data valuation metrics are
inherently biased and unstable under simple algorithmic design choices,
resulting in both technical and ethical implications. By analyzing 9 tabular
classification datasets and 6 data valuation methods, we illustrate how (1)
common and inexpensive data pre-processing techniques can drastically alter
estimated data values; (2) subsampling via data valuation metrics may increase
class imbalance; and (3) data valuation metrics may undervalue underrepresented
group data. Consequently, we argue in favor of increased transparency
associated with data valuation in-the-wild and introduce the novel Data
Valuation Cards (DValCards) framework towards this aim. The proliferation of
DValCards will reduce misuse of data valuation metrics, including in data
pricing, and build trust in responsible ML systems.

</details>


### [433] [Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment](https://arxiv.org/abs/2506.23358)
*Pawel Renc,Michal K. Grzeszczyk,Linglong Qian,Nassim Oufattole,Jeff Rasley,Arkadiusz Sitek*

Main category: cs.LG

TL;DR: The paper introduces Federated Timeline Synthesis (FTS), a framework for federatively training generative models on distributed electronic health records data while ensuring privacy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training AI models on sensitive and distributed electronic health records (EHR) data while ensuring privacy and scalability.

Method: FTS tokenizes patient history into Patient Health Timelines (PHTs) and uses a federated setup where each institution trains a local model. Model weights are aggregated centrally, and a Global Generator is trained on synthetic patient data generated from local models.

Result: FTS demonstrates that synthetic models trained using the Global Generator perform comparably to models trained on real data across five clinically significant prediction tasks.

Conclusion: FTS provides a scalable and privacy-preserving framework for training generative models on distributed healthcare data, with applications in prediction, simulation, and clinical decision-making.

Abstract: We present Federated Timeline Synthesis (FTS), a novel framework for training
generative foundation models across distributed timeseries data applied to
electronic health records (EHR). At its core, FTS represents patient history as
tokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding
temporal, categorical, and continuous clinical information. Each institution
trains an autoregressive transformer on its local PHTs and transmits only model
weights to a central server. The server uses the generators to synthesize a
large corpus of trajectories and train a Global Generator (GG), enabling
zero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS
on five clinically meaningful prediction tasks using MIMIC-IV data, showing
that models trained on synthetic data generated by GG perform comparably to
those trained on real data. FTS offers strong privacy guarantees, scalability
across institutions, and extensibility to diverse prediction and simulation
tasks especially in healthcare, including counterfactual inference, early
warning detection, and synthetic trial design.

</details>


### [434] [When Additive Noise Meets Unobserved Mediators: Bivariate Denoising Diffusion for Causal Discovery](https://arxiv.org/abs/2506.23374)
*Dominik Meier,Sujai Hiremath,Promit Ghosal,Kyra Gan*

Main category: cs.LG

TL;DR: This paper introduces BiDD, a robust causal discovery method for bivariate data that works effectively even under unmeasured mediation, outperforming conventional and prior approaches in synthetic and real-world tests.


<details>
  <summary>Details</summary>
Motivation: Identify causal relationships between bivariate variables in the presence of unmeasured mediators, which hinder traditional additive noise model (ANM) methods.

Method: Proposed Bivariate Denoising Diffusion (BiDD), utilizing denoising processes and independence tests to handle latent noise introduced by unobserved mediators.

Result: BiDD demonstrates improved accuracy and robustness under mediator-corrupted data in synthetic and real-world experiments, compared to standard and existing methods.

Conclusion: BiDD offers a consistent, practical approach for causal discovery in both mediator-corrupted and mediator-free datasets with theoretical backing and empirical validation.

Abstract: Distinguishing cause and effect from bivariate observational data is a
foundational problem in many disciplines, but challenging without additional
assumptions. Additive noise models (ANMs) are widely used to enable
sample-efficient bivariate causal discovery. However, conventional ANM-based
methods fail when unobserved mediators corrupt the causal relationship between
variables. This paper makes three key contributions: first, we rigorously
characterize why standard ANM approaches break down in the presence of
unmeasured mediators. Second, we demonstrate that prior solutions for hidden
mediation are brittle in finite sample settings, limiting their practical
utility. To address these gaps, we propose Bivariate Denoising Diffusion (BiDD)
for causal discovery, a method designed to handle latent noise introduced by
unmeasured mediators. Unlike prior methods that infer directionality through
mean squared error loss comparisons, our approach introduces a novel
independence test statistic: during the noising and denoising processes for
each variable, we condition on the other variable as input and evaluate the
independence of the predicted noise relative to this input. We prove asymptotic
consistency of BiDD under the ANM, and conjecture that it performs well under
hidden mediation. Experiments on synthetic and real-world data demonstrate
consistent performance, outperforming existing methods in mediator-corrupted
settings while maintaining strong performance in mediator-free settings.

</details>


### [435] [Do LLMs Dream of Discrete Algorithms?](https://arxiv.org/abs/2506.23408)
*Claudionor Coelho Jr,Yanen Li,Philip Tee*

Main category: cs.LG

TL;DR: The paper proposes an approach combining Large Language Models (LLMs) with logic-based reasoning modules to improve their performance in logical reasoning and complex decision-making tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle in domains requiring strict logical reasoning, discrete decisions, and interpretability due to their reliance on probabilistic inference.

Method: The authors integrate LLMs with logic modules using Prolog predicates and composable tools, enabling decomposition of tasks into verifiable subtasks and mitigating issues such as hallucinations.

Result: The framework improves precision, coverage, and documentation in reasoning tasks, as demonstrated on the DABStep benchmark.

Conclusion: Combining LLMs with modular logic components enhances scalability, reliability, interpretability, and engineering rigor in AI systems.

Abstract: Large Language Models (LLMs) have rapidly transformed the landscape of
artificial intelligence, enabling natural language interfaces and dynamic
orchestration of software components. However, their reliance on probabilistic
inference limits their effectiveness in domains requiring strict logical
reasoning, discrete decision-making, and robust interpretability. This paper
investigates these limitations and proposes a neurosymbolic approach that
augments LLMs with logic-based reasoning modules, particularly leveraging
Prolog predicates and composable toolsets. By integrating first-order logic and
explicit rule systems, our framework enables LLMs to decompose complex queries
into verifiable sub-tasks, orchestrate reliable solutions, and mitigate common
failure modes such as hallucination and incorrect step decomposition. We
demonstrate the practical benefits of this hybrid architecture through
experiments on the DABStep benchmark, showing improved precision, coverage, and
system documentation in multi-step reasoning tasks. Our results indicate that
combining LLMs with modular logic reasoning restores engineering rigor,
enhances system reliability, and offers a scalable path toward trustworthy,
interpretable AI agents across complex domains.

</details>


### [436] [BenchMake: Turn any scientific data set into a reproducible benchmark](https://arxiv.org/abs/2506.23419)
*Amanda S Barnard*

Main category: cs.LG

TL;DR: The paper presents BenchMake, a tool that transforms scientific datasets into benchmarks using deterministic methods for robust evaluation across data modalities.


<details>
  <summary>Details</summary>
Motivation: The scarcity of benchmark datasets in computational science hampers the evaluation of new methods due to the uniqueness and rapid evolution of these domains.

Method: Developing BenchMake, which employs non-negative matrix factorization to isolate challenging dataset instances for testing, ensuring statistical significance and diversity.

Result: BenchMake was validated on ten publicly available datasets across diverse modalities, demonstrating superior testing set creation compared to established and random splits.

Conclusion: BenchMake offers a standardized and reliable approach to generate benchmarks, facilitating robust comparisons in computational science domains.

Abstract: Benchmark data sets are a cornerstone of machine learning development and
applications, ensuring new methods are robust, reliable and competitive. The
relative rarity of benchmark sets in computational science, due to the
uniqueness of the problems and the pace of change in the associated domains,
makes evaluating new innovations difficult for computational scientists. In
this paper a new tool is developed and tested to potentially turn any of the
increasing numbers of scientific data sets made openly available into a
benchmark accessible to the community. BenchMake uses non-negative matrix
factorisation to deterministically identify and isolate challenging edge cases
on the convex hull (the smallest convex set that contains all existing data
instances) and partitions a required fraction of matched data instances into a
testing set that maximises divergence and statistical significance, across
tabular, graph, image, signal and textual modalities. BenchMake splits are
compared to establish splits and random splits using ten publicly available
benchmark sets from different areas of science, with different sizes, shapes,
distributions.

</details>


### [437] [Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting](https://arxiv.org/abs/2506.23424)
*Heitor R. Medeiros,Hossein Sharifi-Noghabi,Gabriel L. Oliveira,Saghar Irandoust*

Main category: cs.LG

TL;DR: The paper introduces PETSA, a method for parameter-efficient test-time adaptation of forecasting models that maintains competitive or superior performance while reducing memory and compute costs.


<details>
  <summary>Details</summary>
Motivation: Existing test-time adaptation techniques for non-stationary time series require updating the full model, leading to high resource demands.

Method: PETSA leverages low-rank adapters and dynamic gating for calibration, accompanied by a specialized loss function to enhance robustness, preserve periodicity, and align structures.

Result: PETSA outperforms or matches baseline methods across various forecasting benchmarks, while significantly reducing parameter usage.

Conclusion: The approach demonstrates the feasibility of resource-efficient test-time model adaptation, promoting adaptability without sacrificing accuracy or efficiency.

Abstract: Real-world time series often exhibit a non-stationary nature, degrading the
performance of pre-trained forecasting models. Test-Time Adaptation (TTA)
addresses this by adjusting models during inference, but existing methods
typically update the full model, increasing memory and compute costs. We
propose PETSA, a parameter-efficient method that adapts forecasters at test
time by only updating small calibration modules on the input and output. PETSA
uses low-rank adapters and dynamic gating to adjust representations without
retraining. To maintain accuracy despite limited adaptation capacity, we
introduce a specialized loss combining three components: (1) a robust term, (2)
a frequency-domain term to preserve periodicity, and (3) a patch-wise
structural term for structural alignment. PETSA improves the adaptability of
various forecasting backbones while requiring fewer parameters than baselines.
Experimental results on benchmark datasets show that PETSA achieves competitive
or better performance across all horizons. Our code is available at:
https://github.com/BorealisAI/PETSA

</details>


### [438] [Enhancing Insider Threat Detection Using User-Based Sequencing and Transformer Encoders](https://arxiv.org/abs/2506.23446)
*Mohamed Elbasheer,Adewale Akinfaderin*

Main category: cs.LG

TL;DR: This paper introduces the User-Based Sequencing (UBS) approach for insider threat detection using temporal sequences and a Transformer Encoder, showing state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Insider threat detection is challenging because insiders have authorized access and often exhibit subtle behaviors that evading detection.

Method: The User-Based Sequencing (UBS) approach transforms user activity from the CERT dataset into temporal sequences for deep sequence modeling using a Transformer Encoder. Anomaly scores are produced based on reconstruction errors and evaluated with three unsupervised outlier detection methods.

Result: The UBS-Transformer pipeline achieves exceptional performance metrics, including 96.61% accuracy and 99.43% recall, while substantially outperforming other baselines.

Conclusion: Sequential user behavior modeling with advanced techniques like Transformer Encoders is highly effective for detecting insider threats, achieving state-of-the-art results.

Abstract: Insider threat detection presents unique challenges due to the authorized
status of malicious actors and the subtlety of anomalous behaviors. Existing
machine learning methods often treat user activity as isolated events, thereby
failing to leverage sequential dependencies in user behavior. In this study, we
propose a User-Based Sequencing (UBS) methodology, transforming the CERT
insider threat dataset into structured temporal sequences suitable for deep
sequential modeling. We deploy a Transformer Encoder architecture to model
benign user activity and employ its reconstruction errors as anomaly scores.
These scores are subsequently evaluated using three unsupervised outlier
detection algorithms: One-Class SVM (OCSVM), Local Outlier Factor (LOF), and
Isolation Forest (iForest). Across four rigorously designed test sets,
including combinations of multiple CERT dataset releases, our UBS-Transformer
pipeline consistently achieves state-of-the-art performance - notably 96.61%
accuracy, 99.43% recall, 96.38% F1-score, 95.00% AUROC, and exceptionally low
false negative (0.0057) and false positive (0.0571) rates. Comparative analyses
demonstrate that our approach substantially outperforms tabular and
conventional autoencoder baselines, underscoring the efficacy of sequential
user modeling and advanced anomaly detection in the insider threat domain.

</details>


### [439] [Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification](https://arxiv.org/abs/2506.23462)
*Manaswi Kulahara,Gautam Siddharth Kashyap,Nipun Joshi,Arpita Soni*

Main category: cs.LG

TL;DR: The paper introduces DisasterNet-LLM, a specialized model that integrates multimodal data for disaster classification, achieving superior performance metrics.


<details>
  <summary>Details</summary>
Motivation: Traditional disaster management methods face challenges integrating multimodal data like images, weather records, and textual reports.

Method: The paper leverages advanced pretraining, cross-modal attention mechanisms, and adaptive transformers to design DisasterNet-LLM.

Result: DisasterNet-LLM outperforms state-of-the-art models with 89.5% accuracy, 88.0% F1 score, 0.92% AUC, and 0.88% BERTScore in multimodal disaster classification.

Conclusion: DisasterNet-LLM is effective in comprehensive disaster analysis and demonstrates significant improvements in classification accuracy.

Abstract: Effective disaster management requires timely and accurate insights, yet
traditional methods struggle to integrate multimodal data such as images,
weather records, and textual reports. To address this, we propose
DisasterNet-LLM, a specialized Large Language Model (LLM) designed for
comprehensive disaster analysis. By leveraging advanced pretraining,
cross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM
excels in disaster classification. Experimental results demonstrate its
superiority over state-of-the-art models, achieving higher accuracy of 89.5%,
an F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal
disaster classification tasks.

</details>


### [440] [Reconciling Attribute and Structural Anomalies for Improved Graph Anomaly Detection](https://arxiv.org/abs/2506.23469)
*Chunjing Xiao,Jiahui Lu,Xovee Xu,Fan Zhou,Tianshu Xie,Wei Lu,Lifeng Xu*

Main category: cs.LG

TL;DR: The paper introduces TripleAD, a graph anomaly detection framework that tackles the tug-of-war between detecting attribute and structural anomalies using a triple-channel approach with mutual distillation.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised models for graph anomaly detection struggle to adequately detect both attribute and structural anomalies due to conflicting requirements, leading to suboptimal results.

Method: TripleAD uses a triple-channel approach with distinct modules for attribute, structural, and mixed anomaly detection, incorporating techniques like multiscale attribute estimation, link-enhanced structure estimation, and a new attribute-mixed curvature indicator. A mutual distillation strategy fosters collaboration between channels.

Result: Experiments revealed that TripleAD significantly outperformed other benchmark approaches in accurately detecting graph anomalies.

Conclusion: TripleAD effectively mitigates the interference between different anomaly types, enabling superior performance and advancing graph anomaly detection frameworks.

Abstract: Graph anomaly detection is critical in domains such as healthcare and
economics, where identifying deviations can prevent substantial losses.
Existing unsupervised approaches strive to learn a single model capable of
detecting both attribute and structural anomalies. However, they confront the
tug-of-war problem between two distinct types of anomalies, resulting in
suboptimal performance. This work presents TripleAD, a mutual
distillation-based triple-channel graph anomaly detection framework. It
includes three estimation modules to identify the attribute, structural, and
mixed anomalies while mitigating the interference between different types of
anomalies. In the first channel, we design a multiscale attribute estimation
module to capture extensive node interactions and ameliorate the over-smoothing
issue. To better identify structural anomalies, we introduce a link-enhanced
structure estimation module in the second channel that facilitates information
flow to topologically isolated nodes. The third channel is powered by an
attribute-mixed curvature, a new indicator that encapsulates both attribute and
structural information for discriminating mixed anomalies. Moreover, a mutual
distillation strategy is introduced to encourage communication and
collaboration between the three channels. Extensive experiments demonstrate the
effectiveness of the proposed TripleAD model against strong baselines.

</details>


### [441] [Sample Margin-Aware Recalibration of Temperature Scaling](https://arxiv.org/abs/2506.23492)
*Haolan Guo,Linwei Tao,Haoyang Luo,Minjing Dong,Chang Xu*

Main category: cs.LG

TL;DR: The paper introduces SMART, a recalibration technique for neural networks to enhance prediction reliability while being data-efficient and lightweight.


<details>
  <summary>Details</summary>
Motivation: Modern neural networks are overconfident, posing risks in safety-critical areas. Existing recalibration approaches either introduce high bias (e.g., Temperature Scaling) or suffer from high variance due to noisy inputs and insufficient validation data.

Method: The authors propose SMART, a method that recalibrates logits using the 'logit gap' (the margin between the top two logits), acting as a denoised uncertainty signal. It incorporates a SoftECE objective that adapts binning to balance bias and variance, improving calibration stability, even with limited data.

Result: SMART outperforms existing parametric methods in calibration performance across diverse datasets and architectures, achieving state-of-the-art results with fewer parameters.

Conclusion: SMART provides a principled and efficient recalibration solution, ensuring robust uncertainty quantification for neural network predictions even with minimal computational cost.

Abstract: Recent advances in deep learning have significantly improved predictive
accuracy. However, modern neural networks remain systematically overconfident,
posing risks for deployment in safety-critical scenarios. Current post-hoc
calibration methods face a fundamental dilemma: global approaches like
Temperature Scaling apply uniform adjustments across all samples, introducing
high bias despite computational efficiency, while more expressive methods that
operate on full logit distributions suffer from high variance due to noisy
high-dimensional inputs and insufficient validation data. To address these
challenges, we propose Sample Margin-Aware Recalibration of Temperature
(SMART), a lightweight, data-efficient recalibration method that precisely
scales logits based on the margin between the top two logits -- termed the
logit gap. Specifically, the logit gap serves as a denoised, scalar signal
directly tied to decision boundary uncertainty, providing a robust indicator
that avoids the noise inherent in high-dimensional logit spaces while
preserving model prediction invariance. Meanwhile, SMART employs a novel
soft-binned Expected Calibration Error (SoftECE) objective that balances model
bias and variance through adaptive binning, enabling stable parameter updates
even with extremely limited calibration data. Extensive evaluations across
diverse datasets and architectures demonstrate that SMART achieves
state-of-the-art calibration performance even with substantially fewer
parameters compared to existing parametric methods, offering a principled,
robust, and highly efficient solution for practical uncertainty quantification
in neural network predictions. The source code is available at:
https://anonymous.4open.science/r/SMART-8B11.

</details>


### [442] [FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization](https://arxiv.org/abs/2506.23516)
*Seung-Wook Kim,Seongyeol Kim,Jiah Kim,Seowon Ji,Se-Ho Lee*

Main category: cs.LG

TL;DR: The paper presents FedWSQ, a novel federated learning framework integrating weight standardization and a new quantization method (DANUQ) to address challenges like data heterogeneity and communication constraints.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges like data heterogeneity and limited communication, which impact performance. The paper aims to address these issues.

Method: FedWSQ combines weight standardization to improve robustness against biases and the distribution-aware non-uniform quantization method to reduce errors and communication overhead.

Result: FedWSQ achieves high model accuracy and reduces communication costs in federated learning, as shown through experiments on benchmark datasets under challenging scenarios.

Conclusion: FedWSQ outperforms existing federated learning methods, addressing key challenges of extreme data heterogeneity and low communication capacity effectively.

Abstract: Federated learning (FL) often suffers from performance degradation due to key
challenges such as data heterogeneity and communication constraints. To address
these limitations, we present a novel FL framework called FedWSQ, which
integrates weight standardization (WS) and the proposed distribution-aware
non-uniform quantization (DANUQ). WS enhances FL performance by filtering out
biased components in local updates during training, thereby improving the
robustness of the model against data heterogeneity and unstable client
participation. In addition, DANUQ minimizes quantization errors by leveraging
the statistical properties of local model updates. As a result, FedWSQ
significantly reduces communication overhead while maintaining superior model
accuracy. Extensive experiments on FL benchmark datasets demonstrate that
FedWSQ consistently outperforms existing FL methods across various challenging
FL settings, including extreme data heterogeneity and ultra-low-bit
communication scenarios.

</details>


### [443] [Both Asymptotic and Non-Asymptotic Convergence of Quasi-Hyperbolic Momentum using Increasing Batch Size](https://arxiv.org/abs/2506.23544)
*Kento Imaizumi,Hideaki Iiduka*

Main category: cs.LG

TL;DR: This study investigates Quasi-hyperbolic momentum (QHM), a generalized momentum method, for its effectiveness in stochastic nonconvex optimization like deep neural networks. It concludes that increasing batch size without decaying learning rate improves training outcomes.


<details>
  <summary>Details</summary>
Motivation: While momentum methods excel in deterministic convex optimization, their theoretical effectiveness in stochastic nonconvex contexts, such as deep neural networks, is poorly understood, motivating this study.

Method: The paper analyzes mini-batch QHM under scenarios of increasing batch size and/or decaying learning rate. It performs asymptotic and non-asymptotic convergence analyses through experiments.

Result: Asymptotic convergence requires either increasing batch size or a decaying learning rate, but the latter harms non-asymptotic convergence. Experiments show increasing batch size benefits neural network training without needing decayed learning rates.

Conclusion: Increasing batch size, without decaying the learning rate, is an effective strategy for training deep neural networks using QHM, improving both theoretical understanding and practical application.

Abstract: Momentum methods were originally introduced for their superiority to
stochastic gradient descent (SGD) in deterministic settings with convex
objective functions. However, despite their widespread application to deep
neural networks -- a representative case of stochastic nonconvex optimization
-- the theoretical justification for their effectiveness in such settings
remains limited. Quasi-hyperbolic momentum (QHM) is an algorithm that
generalizes various momentum methods and has been studied to better understand
the class of momentum-based algorithms as a whole. In this paper, we provide
both asymptotic and non-asymptotic convergence results for mini-batch QHM with
an increasing batch size. We show that achieving asymptotic convergence
requires either a decaying learning rate or an increasing batch size. Since a
decaying learning rate adversely affects non-asymptotic convergence, we
demonstrate that using mini-batch QHM with an increasing batch size -- without
decaying the learning rate -- can be a more effective strategy. Our experiments
show that even a finite increase in batch size can provide benefits for
training neural networks.

</details>


### [444] [A unified framework on the universal approximation of transformer-type architectures](https://arxiv.org/abs/2506.23551)
*Jingpu Cheng,Qianxiao Li,Ting Lin,Zuowei Shen*

Main category: cs.LG

TL;DR: This paper establishes a unified theoretical framework to prove universal approximation property (UAP) in transformer-type architectures, linking attention mechanisms to UAP and extending the scope of prior results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of general theoretical frameworks connecting attention mechanisms in transformer architectures with universal approximation property, which is crucial for their expressiveness in diverse tasks.

Method: The authors introduce a framework based on token distinguishability and analyticity of the attention layer, allowing for non-constructive proofs of UAP across a broad range of transformer models, including kernel-based and sparse attention mechanisms.

Result: The framework proves UAP for various attention mechanisms and extends or generalizes prior works, while enabling the design of transformer architectures with guaranteed UAP properties.

Conclusion: The study provides both theoretical insights and practical tools for designing expressive transformer architectures, advancing our understanding of UAP in models that employ attention mechanisms.

Abstract: We investigate the universal approximation property (UAP) of transformer-type
architectures, providing a unified theoretical framework that extends prior
results on residual networks to models incorporating attention mechanisms. Our
work identifies token distinguishability as a fundamental requirement for UAP
and introduces a general sufficient condition that applies to a broad class of
architectures. Leveraging an analyticity assumption on the attention layer, we
can significantly simplify the verification of this condition, providing a
non-constructive approach in establishing UAP for such architectures. We
demonstrate the applicability of our framework by proving UAP for transformers
with various attention mechanisms, including kernel-based and sparse attention
mechanisms. The corollaries of our results either generalize prior works or
establish UAP for architectures not previously covered. Furthermore, our
framework offers a principled foundation for designing novel transformer
architectures with inherent UAP guarantees, including those with specific
functional symmetries. We propose examples to illustrate these insights.

</details>


### [445] [Transition Matching: Scalable and Flexible Generative Modeling](https://arxiv.org/abs/2506.23589)
*Neta Shaul,Uriel Singer,Itai Gat,Yaron Lipman*

Main category: cs.LG

TL;DR: This paper introduces Transition Matching (TM), a novel generative framework that integrates diffusion/flow models and autoregressive generation, offering new flexibility and high-quality media generation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the well-explored limitations of diffusion and flow matching models by introducing a unified generative approach leveraging capabilities of autoregressive models.

Method: The paper proposed Transition Matching (TM), decomposing complex tasks into simpler Markov transitions. It introduced three variants: Difference Transition Matching (DTM), Autoregressive Transition Matching (ARTM), and Full History Transition Matching (FHTM).

Result: DTM achieved state-of-the-art results in image generation and sampling efficiency. FHTM matched or surpassed non-causal methods for text-to-image generation while maintaining causality, while ARTM demonstrated competitive quality compared to continuous autoregressive approaches.

Conclusion: TM represents a flexible and unifying generative paradigm, significantly advancing both diffusion/flow and continuous AR generation with high performance on image and text-to-image tasks.

Abstract: Diffusion and flow matching models have significantly advanced media
generation, yet their design space is well-explored, somewhat limiting further
improvements. Concurrently, autoregressive (AR) models, particularly those
generating continuous tokens, have emerged as a promising direction for
unifying text and media generation. This paper introduces Transition Matching
(TM), a novel discrete-time, continuous-state generative paradigm that unifies
and advances both diffusion/flow models and continuous AR generation. TM
decomposes complex generation tasks into simpler Markov transitions, allowing
for expressive non-deterministic probability transition kernels and arbitrary
non-continuous supervision processes, thereby unlocking new flexible design
avenues. We explore these choices through three TM variants: (i) Difference
Transition Matching (DTM), which generalizes flow matching to discrete-time by
directly learning transition probabilities, yielding state-of-the-art image
quality and text adherence as well as improved sampling efficiency. (ii)
Autoregressive Transition Matching (ARTM) and (iii) Full History Transition
Matching (FHTM) are partially and fully causal models, respectively, that
generalize continuous AR methods. They achieve continuous causal AR generation
quality comparable to non-causal approaches and potentially enable seamless
integration with existing AR text generation techniques. Notably, FHTM is the
first fully causal model to match or surpass the performance of flow-based
methods on text-to-image task in continuous domains. We demonstrate these
contributions through a rigorous large-scale comparison of TM variants and
relevant baselines, maintaining a fixed architecture, training data, and
hyperparameters.

</details>


### [446] [When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series](https://arxiv.org/abs/2506.23596)
*Min-Yeong Park,Won-Jeong Lee,Seong Tae Kim,Gyeong-Moon Park*

Main category: cs.LG

TL;DR: The paper introduces Anomaly to Prompt (A2P), a novel framework for predicting future anomalies in time series data, employing anomaly-aware forecasting and synthetic anomaly prompting.


<details>
  <summary>Details</summary>
Motivation: To address the lack of effective solutions for predicting specific future time points of anomalies, known as Anomaly Prediction (AP), as existing methods are inadequate for this task.

Method: The A2P framework combines Anomaly-Aware Forecasting (AAF) to learn relationships of anomalies and Synthetic Anomaly Prompting (SAP) with a learnable Anomaly Prompt Pool (APP) to simulate diverse anomaly patterns.

Result: A2P demonstrates its superiority over state-of-the-art methods across multiple real-world datasets in accurately predicting future anomalies.

Conclusion: The proposed A2P framework is highly effective for anomaly prediction tasks, advancing the capability of forecasting models in detecting future abnormal events.

Abstract: Recently, forecasting future abnormal events has emerged as an important
scenario to tackle real-world necessities. However, the solution of predicting
specific future time points when anomalies will occur, known as Anomaly
Prediction (AP), remains under-explored. Existing methods dealing with time
series data fail in AP, focusing only on immediate anomalies or failing to
provide precise predictions for future anomalies. To address the AP task, we
propose a novel framework called Anomaly to Prompt (A2P), comprised of
Anomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To
enable the forecasting model to forecast abnormal time points, we adopt a
strategy to learn the relationships of anomalies. For the robust detection of
anomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)
that simulates diverse anomaly patterns using signal adaptive prompt.
Comprehensive experiments on multiple real-world datasets demonstrate the
superiority of A2P over state-of-the-art methods, showcasing its ability to
predict future anomalies. Our implementation code is available at
https://github.com/KU-VGI/AP.

</details>


### [447] [A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data](https://arxiv.org/abs/2506.23629)
*Xin Liao,Bing Yang,Cai Yu*

Main category: cs.LG

TL;DR: The paper addresses challenges in imputation of High-Dimensional and Sparse (HDS) water quality data using a novel Nonlinear Low-rank Representation model (NLR) with CNNs, achieving superior estimation accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional data imputation methods fail to effectively handle missing water quality data, which often exhibits high dimensionality and sparsity, limiting decision-making and ecological protection.

Method: The authors propose a Nonlinear Low-rank Representation model (NLR) utilizing CNNs for capturing temporal dependencies and nonlinear interactions to enhance data imputation accuracy.

Result: The model is tested on three real-world water quality datasets, demonstrating significant performance improvements over existing imputation methods.

Conclusion: The proposed approach provides an effective solution for imputing water quality monitoring data in dynamically changing environments, improving decision-making and environmental monitoring.

Abstract: The integrity of Water Quality Data (WQD) is critical in environmental
monitoring for scientific decision-making and ecological protection. However,
water quality monitoring systems are often challenged by large amounts of
missing data due to unavoidable problems such as sensor failures and
communication delays, which further lead to water quality data becoming
High-Dimensional and Sparse (HDS). Traditional data imputation methods are
difficult to depict the potential dynamics and fail to capture the deep data
features, resulting in unsatisfactory imputation performance. To effectively
address the above issues, this paper proposes a Nonlinear Low-rank
Representation model (NLR) with Convolutional Neural Networks (CNN) for
imputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing
temporal features to model the temporal dependence of data between time slots,
and b) Extracting nonlinear interactions and local patterns to mine
higher-order relationships features and achieve deep fusion of multidimensional
information. Experimental studies on three real water quality datasets
demonstrate that the proposed model significantly outperforms existing
state-of-the-art data imputation models in terms of estimation accuracy. It
provides an effective approach for handling water quality monitoring data in
complex dynamic environments.

</details>


### [448] [Learning Modular Exponentiation with Transformers](https://arxiv.org/abs/2506.23679)
*David Demitri Africa,Sara M. Kapoor,Theo Simon Sorg*

Main category: cs.LG

TL;DR: This study explores how Transformer models learn modular exponentiation, finding that the models develop specialized computational circuits for numerical reasoning and arithmetic tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of mechanistic interpretability of modular exponentiation—a fundamental operation in number theory and cryptography—and explore how machine learning models learn such numerical reasoning tasks.

Method: The researchers trained a 4-layer encoder-decoder Transformer model for modular exponentiation. Analysis methods included principled sampling, PCA-based embedding analysis, and activation patching to understand the internal mechanisms.

Result: Training with reciprocal operands improved performance dramatically, leading to sudden generalization across moduli. The emergence of arithmetic structures was observed in grokking-like dynamics, where attention heads in the final layer sufficed for regular exponentiation.

Conclusion: Transformer models likely learn modular arithmetic using specialized computational circuits, improving interpretability and potentially leading to efficient neural approaches for modular arithmetic tasks.

Abstract: Modular exponentiation is crucial to number theory and cryptography, yet
remains largely unexplored from a mechanistic interpretability standpoint. We
train a 4-layer encoder-decoder Transformer model to perform this operation and
investigate the emergence of numerical reasoning during training. Utilizing
principled sampling strategies, PCA-based embedding analysis, and activation
patching, we examine how number-theoretic properties are encoded within the
model. We find that reciprocal operand training leads to strong performance
gains, with sudden generalization across related moduli. These synchronized
accuracy surges reflect grokking-like dynamics, suggesting the model
internalizes shared arithmetic structure. We also find a subgraph consisting
entirely of attention heads in the final layer sufficient to achieve full
performance on the task of regular exponentiation. These results suggest that
transformer models learn modular arithmetic through specialized computational
circuits, paving the way for more interpretable and efficient neural approaches
to modular exponentiation.

</details>


### [449] [DABstep: Data Agent Benchmark for Multi-step Reasoning](https://arxiv.org/abs/2506.23719)
*Alex Egg,Martin Iglesias Goyanes,Friso Kingma,Andreu Mora,Leandro von Werra,Thomas Wolf*

Main category: cs.LG

TL;DR: The paper introduces DABstep, a benchmark for evaluating AI agents on realistic multi-step data analysis tasks focusing on iterative problem-solving and contextual reasoning.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the gap in evaluating AI agents on realistic, complex data analysis tasks that involve multi-step reasoning over documentation.

Method: The benchmark involves over 450 real-world financial data analysis challenges, combining code-based processing, contextual reasoning, and a factoid-style system for automatic scoring.

Result: Even the best large language model (LLM)-based agent achieved only 14.55% accuracy on the most challenging tasks, indicating substantial room for improvement.

Conclusion: DABstep serves as a critical tool to assess and advance autonomous data analysis capabilities in AI agents through iterative reasoning and heterogeneous data sources.

Abstract: We introduce DABstep, a novel benchmark for evaluating AI agents on realistic
multi-step data analysis tasks. DABstep comprises over 450 real-world
challenges derived from a financial analytics platform, requiring models to
combine code-based data processing with contextual reasoning over heterogeneous
documentation. Each task demands an iterative, multi-step problem-solving
approach, testing capabilities in data manipulation, cross-referencing multiple
sources, and precise result reporting. The benchmark provides a factoid-style
answer format with automatic correctness checks for objective scoring at scale.
We evaluate leading LLM-based agents, revealing a substantial performance gap:
even the best agent achieves only 14.55% accuracy on the hardest tasks. We
detail our benchmark's design, dataset composition, task formulation,
evaluation protocol, report baseline results and analyze failure modes. DABstep
is released with a public leaderboard and toolkit to accelerate research in
autonomous data analysis.

</details>


### [450] [System-Embedded Diffusion Bridge Models](https://arxiv.org/abs/2506.23726)
*Bartlomiej Sobieski,Matthew Tivnan,Yuang Wang,Siyeop Yoon,Pengfei Jin,Dufan Wu,Quanzheng Li,Przemyslaw Biecek*

Main category: cs.LG

TL;DR: This paper introduces System Embedded Diffusion Bridge Models (SDBs), a supervised approach for solving linear inverse problems using score-based generative models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve upon existing approaches to inverse problems by integrating structural knowledge of the measurement system, which is often overlooked in supervised methods.

Method: The authors propose a method that embeds the linear measurement system directly into the coefficients of a matrix-valued stochastic differential equation (SDE).

Result: The proposed SDBs show consistent improvement in solving linear inverse problems and robust generalization under system misspecification.

Conclusion: System Embedded Diffusion Bridge Models provide a more principled and effective way to leverage known measurement models, enabling enhanced performance and generalization for real-world inverse problems.

Abstract: Solving inverse problems -- recovering signals from incomplete or noisy
measurements -- is fundamental in science and engineering. Score-based
generative models (SGMs) have recently emerged as a powerful framework for this
task. Two main paradigms have formed: unsupervised approaches that adapt
pretrained generative models to inverse problems, and supervised bridge methods
that train stochastic processes conditioned on paired clean and corrupted data.
While the former typically assume knowledge of the measurement model, the
latter have largely overlooked this structural information. We introduce System
embedded Diffusion Bridge Models (SDBs), a new class of supervised bridge
methods that explicitly embed the known linear measurement system into the
coefficients of a matrix-valued SDE. This principled integration yields
consistent improvements across diverse linear inverse problems and demonstrates
robust generalization under system misspecification between training and
deployment, offering a promising solution to real-world applications.

</details>


### [451] [Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models](https://arxiv.org/abs/2506.23731)
*Michel Meintz,Jan Dubiński,Franziska Boenisch,Adam Dziedzic*

Main category: cs.LG

TL;DR: The paper addresses the issue of unauthorized use of generated images in training new models by proposing a watermarking method for image autoregressive models (IARs) that retains its detectability even after being used for training new models. It introduces the concept of radioactivity and evaluates the method effectively.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing popularity of image generative models and the high costs associated with dataset collection and curation. The need arises to address unauthorized usage of generated images for training new models.

Method: The authors analyze existing methods for watermarking diffusion models (DMs) and find them inadequate regarding radioactivity. For IARs, the authors propose a novel radioactive watermarking technique inspired by large language models' methods, tailored for IARs' autoregressive model paradigm.

Result: The experimental results demonstrate the proposed watermarking method's success in preserving radioactivity within IARs, ensuring robust tracking of provenance and deterring unauthorized use of generated images.

Conclusion: The developed watermarking method emphasizes the importance of radioactivity in watermarking for IARs, successfully enabling provenance tracking and tackling misuse issues, outperforming existing watermarking methods for diffusion models.

Abstract: Image generative models have become increasingly popular, but training them
requires large datasets that are costly to collect and curate. To circumvent
these costs, some parties may exploit existing models by using the generated
images as training data for their own models. In general, watermarking is a
valuable tool for detecting unauthorized use of generated images. However, when
these images are used to train a new model, watermarking can only enable
detection if the watermark persists through training and remains identifiable
in the outputs of the newly trained model - a property known as radioactivity.
We analyze the radioactivity of watermarks in images generated by diffusion
models (DMs) and image autoregressive models (IARs). We find that existing
watermarking methods for DMs fail to retain radioactivity, as watermarks are
either erased during encoding into the latent space or lost in the
noising-denoising process (during the training in the latent space). Meanwhile,
despite IARs having recently surpassed DMs in image generation quality and
efficiency, no radioactive watermarking methods have been proposed for them. To
overcome this limitation, we propose the first watermarking method tailored for
IARs and with radioactivity in mind - drawing inspiration from techniques in
large language models (LLMs), which share IARs' autoregressive paradigm. Our
extensive experimental evaluation highlights our method's effectiveness in
preserving radioactivity within IARs, enabling robust provenance tracking, and
preventing unauthorized use of their generated images.

</details>


### [452] [Model-driven Stochastic Trace Clustering](https://arxiv.org/abs/2506.23776)
*Jari Peeperkorn,Johannes De Smedt,Jochen De Weerdt*

Main category: cs.LG

TL;DR: The paper proposes a trace clustering method that uses stochastic process models and entropic relevance to improve clustering performance and model interpretability in process discovery.


<details>
  <summary>Details</summary>
Motivation: Current trace clustering techniques fail to effectively incorporate stochasticity, limiting their ability to capture real-world execution dynamics in process discovery.

Method: A model-driven trace clustering method optimized for stochastic process models, incorporating entropic relevance as a conformance metric to evaluate direct-follow probabilities.

Result: The proposed method is computationally efficient, scales linearly with input size, improves interpretability, and outperforms existing techniques, particularly when stochasticity is considered.

Conclusion: Considering stochastic elements in process models leads to more accurate clustering, better represents process behavior, and results in more interpretable models with clearer control-flow patterns.

Abstract: Process discovery algorithms automatically extract process models from event
logs, but high variability often results in complex and hard-to-understand
models. To mitigate this issue, trace clustering techniques group process
executions into clusters, each represented by a simpler and more understandable
process model. Model-driven trace clustering improves on this by assigning
traces to clusters based on their conformity to cluster-specific process
models. However, most existing clustering techniques rely on either no process
model discovery, or non-stochastic models, neglecting the frequency or
probability of activities and transitions, thereby limiting their capability to
capture real-world execution dynamics. We propose a novel model-driven trace
clustering method that optimizes stochastic process models within each cluster.
Our approach uses entropic relevance, a stochastic conformance metric based on
directly-follows probabilities, to guide trace assignment. This allows
clustering decisions to consider both structural alignment with a cluster's
process model and the likelihood that a trace originates from a given
stochastic process model. The method is computationally efficient, scales
linearly with input size, and improves model interpretability by producing
clusters with clearer control-flow patterns. Extensive experiments on public
real-life datasets show that our method outperforms existing alternatives in
representing process behavior and reveals how clustering performance rankings
can shift when stochasticity is considered.

</details>


### [453] [Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling](https://arxiv.org/abs/2506.23782)
*Xiaoyang Li,Linwei Tao,Haohui Lu,Minjing Dong,Junbin Gao,Chang Xu*

Main category: cs.LG

TL;DR: The paper introduces Wavelet-Aware Temperature Scaling (WATS), a post-hoc calibration framework for Graph Neural Networks (GNNs), enabling improved confidence estimation using heat-kernel graph wavelet features. WATS enhances performance without model retraining or relying on coarse neighbor statistics.


<details>
  <summary>Details</summary>
Motivation: Graph Neural Networks (GNNs) often lack reliable confidence estimates despite their strong predictive ability, limiting their use in safety-critical applications. Existing calibration methods fail to address the structural complexity of graph topology.

Method: WATS uses node-specific temperatures derived from heat-kernel graph wavelet features in a post-hoc calibration manner. It avoids retraining models and does not require neighboring data, leveraging the scalability and sensitivity of graph wavelets.

Result: Extensive evaluations on seven datasets and two GNN backbones show WATS achieves the lowest Expected Calibration Error (ECE), outperforming baseline methods by up to 42.3% in ECE and reducing calibration variance by an average of 17.24%.

Conclusion: Wavelet-Aware Temperature Scaling (WATS) offers a computationally efficient and structurally aware calibration method, effectively bridging the gap in confidence estimation for GNNs while scaling robustly across various graph structures.

Abstract: Graph Neural Networks (GNNs) have demonstrated strong predictive performance
on relational data; however, their confidence estimates often misalign with
actual predictive correctness, posing significant limitations for deployment in
safety-critical settings. While existing graph-aware calibration methods seek
to mitigate this limitation, they primarily depend on coarse one-hop
statistics, such as neighbor-predicted confidence, or latent node embeddings,
thereby neglecting the fine-grained structural heterogeneity inherent in graph
topology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a
post-hoc calibration framework that assigns node-specific temperatures based on
tunable heat-kernel graph wavelet features. Specifically, WATS harnesses the
scalability and topology sensitivity of graph wavelets to refine confidence
estimates, all without necessitating model retraining or access to neighboring
logits or predictions. Extensive evaluations across seven benchmark datasets
with varying graph structures and two GNN backbones demonstrate that WATS
achieves the lowest Expected Calibration Error (ECE) among all compared
methods, outperforming both classical and graph-specific baselines by up to
42.3\% in ECE and reducing calibration variance by 17.24\% on average compared
with graph-specific methods. Moreover, WATS remains computationally efficient,
scaling well across graphs of diverse sizes and densities. Code will be
released based on publication.

</details>


### [454] [KAIROS: Scalable Model-Agnostic Data Valuation](https://arxiv.org/abs/2506.23799)
*Jiongli Zhu,Parjanya Prajakta Prashant,Alex Cloninger,Babak Salimi*

Main category: cs.LG

TL;DR: The paper introduces KAIROS, a model-agnostic data valuation framework that is efficient, accurate, and scalable, outperforming existing methods in utility ranking for training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods for data valuation are inadequate due to their dependence on biased models, costly retraining, or approximation errors in ranking, motivating the need for a scalable and reliable alternative.

Method: KAIROS uses Maximum Mean Discrepancy (MMD)-based influence scores to estimate the utility of data points, avoiding retraining, achieving closed-form solutions, and ensuring efficient updates with theoretical guarantees.

Result: KAIROS demonstrates superior performance over state-of-the-art methods in tasks involving noise, mislabeling, and poisoning, providing better accuracy, runtime efficiency, and reliable rankings.

Conclusion: KAIROS is a robust and scalable solution for data valuation in AI pipelines, delivering significant advancements over current methods in both performance and practicality.

Abstract: Training data increasingly shapes not only model accuracy but also regulatory
compliance and market valuation of AI assets. Yet existing valuation methods
remain inadequate: model-based techniques depend on a single fitted model and
inherit its biases, while algorithm-based approaches such as Data Shapley
require costly retrainings at web scale. Recent Wasserstein-based
model-agnostic methods rely on approximations that misrank examples relative to
their true leave-one-out (LOO) utility. We introduce KAIROS, a scalable,
model-agnostic valuation framework that assigns each example a distributional
influence score: its contribution to the Maximum Mean Discrepancy (MMD) between
the empirical training distribution and a clean reference set. Unlike
Wasserstein surrogates, our MMD-based influence admits a closed-form solution
that faithfully approximates the exact LOO ranking within $O(1/N^2)$ error,
requires no retraining, and naturally extends to conditional kernels for
unified label- and feature-error detection. Moreover, KAIROS supports efficient
online updates: when a new batch of size m arrives, all scores can be updated
in $O(mN)$ time, delivering up to 50x speedup without compromising ranking
quality. Empirical evaluations on noise, mislabeling, and poisoning benchmarks
show that KAIROS consistently outperforms state-of-the-art model-, Shapley-,
and Wasserstein-based baselines in both accuracy and runtime. We provide
rigorous theoretical guarantees, including symmetry for reproducible rankings
and density-separation for interpretable thresholds.

</details>


### [455] [Towards the Training of Deeper Predictive Coding Neural Networks](https://arxiv.org/abs/2506.23800)
*Chang Qi,Matteo Forasassi,Thomas Lukasiewicz,Tommaso Salvatori*

Main category: cs.LG

TL;DR: This paper identifies and resolves performance degradation issues in deep predictive coding networks using equilibrium propagation, achieving results comparable to backpropagation for models exceeding seven layers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the significant performance degradation observed in predictive coding networks with depth greater than five to seven layers, limiting their scalability and application in complex tasks.

Method: The authors introduce two precision-weighting optimization methods for latent variable energy balancing during the relaxation phase and a novel weight update mechanism to mitigate error accumulation in deeper layers.

Result: The proposed methods result in substantial test accuracy improvements on image classification tasks for deep networks, achieving performances comparable to backpropagation.

Conclusion: Understanding and refining the relaxation phase is critical for scaling predictive coding networks trained with equilibrium propagation, paving the way for their use in more complex problems.

Abstract: Predictive coding networks trained with equilibrium propagation are neural
models that perform inference through an iterative energy minimization process.
Previous studies have demonstrated their effectiveness in shallow
architectures, but show significant performance degradation when depth exceeds
five to seven layers. In this work, we show that the reason behind this
degradation is due to exponentially imbalanced errors between layers during
weight updates, and predictions from the previous layer not being effective in
guiding updates in deeper layers. We address the first issue by introducing two
novel methods to optimize the latent variables that use precision-weighting to
re-balance the distribution of energy among layers during the `relaxation
phase', and the second issue by proposing a novel weight update mechanism that
reduces error accumulation in deeper layers. Empirically, we test our methods
on a large number of image classification tasks, resulting in large
improvements in test accuracy across networks with more than seven layers, with
performances comparable to those of backprop on similar models. These findings
suggest that a better understanding of the relaxation phase is important to
train models using equilibrium propagation at scale, and open new possibilities
for their application in complex tasks.

</details>


### [456] [Adaptive Out-of-Control Point Pattern Detection in Sequential Random Finite Set Observations](https://arxiv.org/abs/2506.23802)
*Konstantinos Bourazas,Savvas Papaioannou,Panayiotis Kolios*

Main category: cs.LG

TL;DR: The paper presents a new adaptive anomaly detection framework for sequential Random Finite Set (RFS) data, which learns normal behaviors and detects anomalies using Power Discounting posterior distributions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of monitoring and detecting anomalies in sequential random finite set observations in dynamic environments.

Method: The method involves developing RFS-based Power Discounting Posteriors that can adapt to systematic data changes and perform anomaly detection using a novel predictive posterior density function.

Result: The approach's efficacy is validated through extensive simulations demonstrating its qualitative and quantitative effectiveness.

Conclusion: The proposed RFS-based framework proves effective for adaptive anomaly detection, robustly identifying abnormal behaviors in point pattern data.

Abstract: In this work we introduce a novel adaptive anomaly detection framework
specifically designed for monitoring sequential random finite set (RFS)
observations. Our approach effectively distinguishes between In-Control data
(normal) and Out-Of-Control data (anomalies) by detecting deviations from the
expected statistical behavior of the process. The primary contributions of this
study include the development of an innovative RFS-based framework that not
only learns the normal behavior of the data-generating process online but also
dynamically adapts to behavioral shifts to accurately identify abnormal point
patterns. To achieve this, we introduce a new class of RFS-based posterior
distributions, named Power Discounting Posteriors (PD), which facilitate
adaptation to systematic changes in data while enabling anomaly detection of
point pattern data through a novel predictive posterior density function. The
effectiveness of the proposed approach is demonstrated by extensive qualitative
and quantitative simulation experiments.

</details>


### [457] [SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration](https://arxiv.org/abs/2506.23803)
*Dmitry Kovalev*

Main category: cs.LG

TL;DR: This paper revisits SGD with AdaGrad-type preconditioning, addressing its convergence and introducing novel accelerations using Nesterov momentum.


<details>
  <summary>Details</summary>
Motivation: To analyze and improve SGD with adaptive preconditioning methods, like AdaGrad, to enhance their performance and efficiency.

Method: The paper develops a unified convergence theory under specific smoothness/noise assumptions, explores connections between recent algorithms, and incorporates Nesterov momentum for acceleration.

Result: It recovers state-of-the-art results for many adaptive gradient methods, establishes connections among algorithms, and demonstrates provable acceleration.

Conclusion: AdaGrad-type methods can benefit from both diagonal preconditioning and momentum, providing insight into their practical effectiveness, such as in Adam optimizer.

Abstract: In this paper, we revisit stochastic gradient descent (SGD) with AdaGrad-type
preconditioning. Our contributions are twofold. First, we develop a unified
convergence analysis of SGD with adaptive preconditioning under anisotropic or
matrix smoothness and noise assumptions. This allows us to recover
state-of-the-art convergence results for several popular adaptive gradient
methods, including AdaGrad-Norm, AdaGrad, and ASGO/One-sided Shampoo. In
addition, we establish the fundamental connection between two recently proposed
algorithms, Scion and DASGO, and provide the first theoretical guarantees for
the latter. Second, we show that the convergence of methods like AdaGrad and
DASGO can be provably accelerated beyond the best-known rates using Nesterov
momentum. Consequently, we obtain the first theoretical justification that
AdaGrad-type algorithms can simultaneously benefit from both diagonal
preconditioning and momentum, which may provide an ultimate explanation for the
practical efficiency of Adam.

</details>


### [458] [Supercm: Revisiting Clustering for Semi-Supervised Learning](https://arxiv.org/abs/2506.23824)
*Durgesh Singh,Ahcene Boubekki,Robert Jenssen,Michael C. Kampffmeyer*

Main category: cs.LG

TL;DR: This paper proposes a novel semi-supervised learning (SSL) method combining clustering and simplicity.


<details>
  <summary>Details</summary>
Motivation: Recent SSL advancements rely heavily on complex methods like consistency regularization, untapped explicit clustering opportunities.

Method: The proposed method uses a differentiable clustering module guided by labeled data in an end-to-end framework.

Result: The model outperforms a supervised-only baseline and complements existing SSL techniques.

Conclusion: Incorporating clustering-based strategies simplifies SSL training while improving performance, with potential compatibility for enhancing other methods.

Abstract: The development of semi-supervised learning (SSL) has in recent years largely
focused on the development of new consistency regularization or entropy
minimization approaches, often resulting in models with complex training
strategies to obtain the desired results. In this work, we instead propose a
novel approach that explicitly incorporates the underlying clustering
assumption in SSL through extending a recently proposed differentiable
clustering module. Leveraging annotated data to guide the cluster centroids
results in a simple end-to-end trainable deep SSL approach. We demonstrate that
the proposed model improves the performance over the supervised-only baseline
and show that our framework can be used in conjunction with other SSL methods
to further boost their performance.

</details>


### [459] [EFPI: Elastic Formation and Position Identification in Football (Soccer) using Template Matching and Linear Assignment](https://arxiv.org/abs/2506.23843)
*Joris Bekkers*

Main category: cs.LG

TL;DR: This paper introduces EFPI, a method for recognizing football formations using spatiotemporal data and cost minimization techniques.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve tactical analysis in football by providing a reliable way to identify team formations and player positions systematically.

Method: The method involves using static formation templates, scaling player positions to fit templates, employing linear sum assignment for position matching, and minimizing costs to determine the formation.

Result: EFPI effectively identifies team formations and player positions from tracking data, working for both individual frames and extended game sequences.

Conclusion: The EFPI method is an accurate, flexible, and open-source solution for formation recognition in football, supporting both individual and extended time analyses.

Abstract: Understanding team formations and player positioning is crucial for tactical
analysis in football (soccer). This paper presents a flexible method for
formation recognition and player position assignment in football using
predefined static formation templates and cost minimization from spatiotemporal
tracking data, called EFPI. Our approach employs linear sum assignment to
optimally match players to positions within a set of template formations by
minimizing the total distance between actual player locations and template
positions, subsequently selecting the formation with the lowest assignment
cost. To improve accuracy, we scale actual player positions to match the
dimensions of these formation templates in both width and length. While the
method functions effectively on individual frames, it extends naturally to
larger game segments such as complete periods, possession sequences or specific
intervals (e.g. 10 second intervals, 5 minute intervals etc.). Additionally, we
incorporate an optional stability parameter that prevents unnecessary formation
changes when assignment costs differ only marginally between time segments.
EFPI is available as open-source code through the unravelsports Python package.

</details>


### [460] [Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts](https://arxiv.org/abs/2506.23845)
*Kenny Peng,Rajiv Movva,Jon Kleinberg,Emma Pierson,Nikhil Garg*

Main category: cs.LG

TL;DR: Sparse autoencoders (SAEs) are useful for discovering unknown concepts, despite skepticism about their effectiveness for acting on known concepts.


<details>
  <summary>Details</summary>
Motivation: To reconcile conflicting views about the usefulness of sparse autoencoders by clarifying their strengths and limitations in distinct application areas.

Method: The paper categorizes the roles of SAEs into analyzing known versus discovering unknown concepts and highlights their utility in specific domains like ML fairness and health sciences.

Result: The authors show that SAEs are powerful for applications such as ML interpretability, fairness, safety, and social/health sciences while addressing existing skepticism.

Conclusion: SAEs should be viewed as tools for discovery in specific domains, rather than universally effective for all purposes, which resolves prior conflicting narratives.

Abstract: While sparse autoencoders (SAEs) have generated significant excitement, a
series of negative results have added to skepticism about their usefulness.
Here, we establish a conceptual distinction that reconciles competing
narratives surrounding SAEs. We argue that while SAEs may be less effective for
acting on known concepts, SAEs are powerful tools for discovering unknown
concepts. This distinction cleanly separates existing negative and positive
results, and suggests several classes of SAE applications. Specifically, we
outline use cases for SAEs in (i) ML interpretability, explainability,
fairness, auditing, and safety, and (ii) social and health sciences.

</details>


### [461] [When Plants Respond: Electrophysiology and Machine Learning for Green Monitoring Systems](https://arxiv.org/abs/2506.23872)
*Eduard Buss,Till Aust,Heiko Hamann*

Main category: cs.LG

TL;DR: The study integrates plants into a biohybrid system using a device (PhytoNode) to monitor plant electrophysiology for environmental applications, achieving high classification accuracy with automated machine learning.


<details>
  <summary>Details</summary>
Motivation: To harness plants' natural sensor abilities for environmental monitoring and precision agriculture by linking their biological signals to artificial devices.

Method: Developed the PhytoNode wearable device to record plant electrophysiological signals, deploying *Hedera helix* outdoors for five months and analyzing the data with automated machine learning models.

Result: Achieved classification performance with macro F1 scores up to 95%, with AutoML surpassing manual methods, demonstrating improved accuracy through feature selection.

Conclusion: This work demonstrates scalable plant-based biohybrid systems suitable for monitoring environmental conditions in real-world, outdoor settings.

Abstract: Living plants, while contributing to ecological balance and climate
regulation, also function as natural sensors capable of transmitting
information about their internal physiological states and surrounding
conditions. This rich source of data provides potential for applications in
environmental monitoring and precision agriculture. With integration into
biohybrid systems, we establish novel channels of physiological signal flow
between living plants and artificial devices. We equipped *Hedera helix* with a
plant-wearable device called PhytoNode to continuously record the plant's
electrophysiological activity. We deployed plants in an uncontrolled outdoor
environment to map electrophysiological patterns to environmental conditions.
Over five months, we collected data that we analyzed using state-of-the-art and
automated machine learning (AutoML). Our classification models achieve high
performance, reaching macro F1 scores of up to 95 percent in binary tasks.
AutoML approaches outperformed manual tuning, and selecting subsets of
statistical features further improved accuracy. Our biohybrid living system
monitors the electrophysiology of plants in harsh, real-world conditions. This
work advances scalable, self-sustaining, and plant-integrated living biohybrid
systems for sustainable environmental monitoring.

</details>


### [462] [Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic](https://arxiv.org/abs/2506.23875)
*Yuta Sato,Kazuhiko Kawamoto,Hiroshi Kera*

Main category: cs.LG

TL;DR: This paper proposes a method to reorder decoder input tokens in Transformers to optimize their learning on arithmetic tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the step-by-step reasoning ability in Transformers by reordering input tokens into sequences that facilitate easier learning.

Method: It trains a Transformer on target sequences of various orders, identifies learning-friendly orders based on early loss reduction, and employs a two-stage hierarchical approach for inter- and intra-block token reordering.

Result: The method successfully discovers optimized input sequences for order-sensitive arithmetic tasks, including reproducing reverse-digit order for multiplication tasks.

Conclusion: Reordering tokens into learning-friendly sequences significantly improves Transformer performance on arithmetic tasks, highlighting the importance of sequence order in reasoning.

Abstract: The chain of thought is fundamental in Transformers, which is to perform
step-by-step reasoning. Besides what intermediate steps work, the order of
these steps critically affects the difficulty of the reasoning. This study
addresses a novel task of unraveling chain of thought - reordering decoder
input tokens to a learning-friendly sequence for Transformers to learn
arithmetic tasks. The proposed pipeline first trains a Transformer on a mixture
of target sequences arranged in different orders and then identifies benign
orders as those with fast loss drops in the early stage. As the search space
grows factorially with sequence length, we propose a two-stage hierarchical
approach for inter- and intra-block reordering. Experiments on four
order-sensitive arithmetic tasks show that our method identifies a
learning-friendly order out of a few billion candidates. Notably, on the
multiplication task, it recovered the reverse-digit order reported in prior
studies.

</details>


### [463] [Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System](https://arxiv.org/abs/2506.23923)
*Miguel Camacho-Sánchez,Fernando García-Torres,Jesper John Lisegaard,Rocío del Amor,Sankhya Mohanty,Valery Naranjo*

Main category: cs.LG

TL;DR: The paper proposes a reinforcement learning (RL) strategy using Proximal Policy Optimisation (PPO) to control resin flow in resin infusion (RI) processes, ensuring uniform impregnation of fibre reinforcements for better composite quality.


<details>
  <summary>Details</summary>
Motivation: To address the critical challenge of controlling resin flow dynamics in resin infusion and resin transfer moulding processes for producing defect-free, high-performance fibre-reinforced polymer composites.

Method: The authors developed an RL-based control strategy using Proximal Policy Optimisation (PPO) within simulated process environments to synchronise resin flow fronts across two inlets and a single outlet in partially observable conditions.

Result: The RL strategy demonstrated effective resin flow convergence, reducing defects such as residual porosities and dry spots in fibre-reinforced polymers.

Conclusion: Reinforcement learning offers a promising approach to improve process control and enhance the structural quality of polymer composite manufacturing.

Abstract: Resin infusion (RI) and resin transfer moulding (RTM) are critical processes
for the manufacturing of high-performance fibre-reinforced polymer composites,
particularly for large-scale applications such as wind turbine blades.
Controlling the resin flow dynamics in these processes is critical to ensure
the uniform impregnation of the fibre reinforcements, thereby preventing
residual porosities and dry spots that impact the consequent structural
integrity of the final component. This paper presents a reinforcement learning
(RL) based strategy, established using process simulations, for synchronising
the different resin flow fronts in an infusion scenario involving two resin
inlets and a single outlet. Using Proximal Policy Optimisation (PPO), our
approach addresses the challenge of managing the fluid dynamics in a partially
observable environment. The results demonstrate the effectiveness of the RL
approach in achieving an accurate flow convergence, highlighting its potential
towards improving process control and product quality in composites
manufacturing.

</details>


### [464] [Bridging the Gap with Retrieval-Augmented Generation: Making Prosthetic Device User Manuals Available in Marginalised Languages](https://arxiv.org/abs/2506.23958)
*Ikechukwu Ogbonna,Lesley Davidson,Soumya Banerjee,Abhishek Dasgupta,Laurence Kenney,Vikranth Harthikote Nagaraja*

Main category: cs.LG

TL;DR: Developed an AI-powered framework that translates complex medical documents, like prosthetic device manuals, into marginalized languages for improved accessibility.


<details>
  <summary>Details</summary>
Motivation: Millions in African countries face healthcare access barriers due to language and literacy gaps, exacerbated by inaccessible medical documentation formats.

Method: Utilized a Retrieval-Augmented Generation pipeline and advanced NLP models for multilingual translation and query-based question-answering.

Result: The framework enables real-time translation and localized responses to medical queries using languages such as Pidgin dialect, ensuring better accessibility and understanding.

Conclusion: The system empowers underserved populations, enabling informed healthcare decisions through accessible and localized medical documentation.

Abstract: Millions of people in African countries face barriers to accessing healthcare
due to language and literacy gaps. This research tackles this challenge by
transforming complex medical documents -- in this case, prosthetic device user
manuals -- into accessible formats for underserved populations. This case study
in cross-cultural translation is particularly pertinent/relevant for
communities that receive donated prosthetic devices but may not receive the
accompanying user documentation. Or, if available online, may only be available
in formats (e.g., language and readability) that are inaccessible to local
populations (e.g., English-language, high resource settings/cultural context).
The approach is demonstrated using the widely spoken Pidgin dialect, but our
open-source framework has been designed to enable rapid and easy extension to
other languages/dialects. This work presents an AI-powered framework designed
to process and translate complex medical documents, e.g., user manuals for
prosthetic devices, into marginalised languages. The system enables users --
such as healthcare workers or patients -- to upload English-language medical
equipment manuals, pose questions in their native language, and receive
accurate, localised answers in real time. Technically, the system integrates a
Retrieval-Augmented Generation (RAG) pipeline for processing and semantic
understanding of the uploaded manuals. It then employs advanced Natural
Language Processing (NLP) models for generative question-answering and
multilingual translation. Beyond simple translation, it ensures accessibility
to device instructions, treatment protocols, and safety information, empowering
patients and clinicians to make informed healthcare decisions.

</details>


### [465] [UMA: A Family of Universal Models for Atoms](https://arxiv.org/abs/2506.23971)
*Brandon M. Wood,Misko Dzamba,Xiang Fu,Meng Gao,Muhammed Shuaibi,Luis Barroso-Luque,Kareem Abdelmaqsoud,Vahe Gharakhanyan,John R. Kitchin,Daniel S. Levine,Kyle Michel,Anuroop Sriram,Taco Cohen,Abhishek Das,Ammar Rizvi,Sushree Jagriti Sahoo,Zachary W. Ulissi,C. Lawrence Zitnick*

Main category: cs.LG

TL;DR: This paper presents Universal Models for Atoms (UMA), addressing the need for fast and accurate computation in atomic simulations, trained on the largest dataset to date of 3D atomic structures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to advance computational capabilities in applications such as drug discovery, energy storage, and semiconductor manufacturing by creating models that improve speed, accuracy, and generalization in atomic simulations.

Method: The UMA models use architectural designs like mixture of linear experts and are trained on a massive dataset of 3D atomic structures across multiple chemical domains. Empirical scaling laws are developed to guide model capacity growth.

Result: UMA models perform on par or better than specialized models across diverse applications, without requiring fine-tuning, showcasing their generalist capability.

Conclusion: The UMA models enable fast, accurate computations and generalization across domains, pushing boundaries in atomic simulations, and their release will accelerate advancements in AI-driven workflows in science and engineering.

Abstract: The ability to quickly and accurately compute properties from atomic
simulations is critical for advancing a large number of applications in
chemistry and materials science including drug discovery, energy storage, and
semiconductor manufacturing. To address this need, Meta FAIR presents a family
of Universal Models for Atoms (UMA), designed to push the frontier of speed,
accuracy, and generalization. UMA models are trained on half a billion unique
3D atomic structures (the largest training runs to date) by compiling data
across multiple chemical domains, e.g. molecules, materials, and catalysts. We
develop empirical scaling laws to help understand how to increase model
capacity alongside dataset size to achieve the best accuracy. The UMA small and
medium models utilize a novel architectural design we refer to as mixture of
linear experts that enables increasing model capacity without sacrificing
speed. For example, UMA-medium has 1.4B parameters but only ~50M active
parameters per atomic structure. We evaluate UMA models on a diverse set of
applications across multiple domains and find that, remarkably, a single model
without any fine-tuning can perform similarly or better than specialized
models. We are releasing the UMA code, weights, and associated data to
accelerate computational workflows and enable the community to continue to
build increasingly capable AI models.

</details>


### [466] [A Scalable Approach for Safe and Robust Learning via Lipschitz-Constrained Networks](https://arxiv.org/abs/2506.23977)
*Zain ul Abdeen,Vassilis Kekatos,Ming Jin*

Main category: cs.LG

TL;DR: This paper proposes a convex training framework for neural networks that enforces robustness via Lipschitz constraints, addressing scalability issues using a randomized subspace method.


<details>
  <summary>Details</summary>
Motivation: To enhance the robustness of neural networks in safety-critical applications by constraining their global Lipschitz constant, overcoming issues like non-convex formulations and poor scalability.

Method: The proposed method reparameterizes neural networks using loop transformation, employs semidefinite relaxation for convex admissibility, and utilizes a Randomized Subspace Linear Matrix Inequalities (RS-LMI) approach to improve scalability.

Result: Empirical evaluations on MNIST, CIFAR-10, and ImageNet show competitive accuracy, improved Lipschitz bounds, and better runtime efficiency compared to prior methods.

Conclusion: The framework provides a scalable, certifiably robust training approach for Lipschitz-constrained neural networks, emphasizing both rigor and practicality.

Abstract: Certified robustness is a critical property for deploying neural networks
(NN) in safety-critical applications. A principle approach to achieving such
guarantees is to constrain the global Lipschitz constant of the network.
However, accurate methods for Lipschitz-constrained training often suffer from
non-convex formulations and poor scalability due to reliance on global
semidefinite programs (SDPs). In this letter, we propose a convex training
framework that enforces global Lipschitz constraints via semidefinite
relaxation. By reparameterizing the NN using loop transformation, we derive a
convex admissibility condition that enables tractable and certifiable training.
While the resulting formulation guarantees robustness, its scalability is
limited by the size of global SDP. To overcome this, we develop a randomized
subspace linear matrix inequalities (RS-LMI) approach that decomposes the
global constraints into sketched layerwise constraints projected onto
low-dimensional subspaces, yielding a smooth and memory-efficient training
objective. Empirical results on MNIST, CIFAR-10, and ImageNet demonstrate that
the proposed framework achieves competitive accuracy with significantly
improved Lipschitz bounds and runtime performance.

</details>


### [467] [LLM Agents Are the Antidote to Walled Gardens](https://arxiv.org/abs/2506.23978)
*Samuele Marro,Philip Torr*

Main category: cs.LG

TL;DR: The paper discusses how LLM-based agents enable 'universal interoperability' between digital services, reducing monopolistic practices but bringing new risks.


<details>
  <summary>Details</summary>
Motivation: To address the dominance of closed, proprietary platforms and limited data exchange in today's application layer.

Method: Propose AI-mediated adapters for data format translation and interface interactions, disrupting the need for API-based interoperability.

Result: Demonstrate that LLM-based agents make interoperability cheaper and undermine monopolistic behaviors, but also introduce security risks and technical debt.

Conclusion: The ML community should adopt universal interoperability while developing frameworks to address potential downsides like security and technical risks.

Abstract: While the Internet's core infrastructure was designed to be open and
universal, today's application layer is dominated by closed, proprietary
platforms. Open and interoperable APIs require significant investment, and
market leaders have little incentive to enable data exchange that could erode
their user lock-in. We argue that LLM-based agents fundamentally disrupt this
status quo. Agents can automatically translate between data formats and
interact with interfaces designed for humans: this makes interoperability
dramatically cheaper and effectively unavoidable. We name this shift universal
interoperability: the ability for any two digital services to exchange data
seamlessly using AI-mediated adapters. Universal interoperability undermines
monopolistic behaviours and promotes data portability. However, it can also
lead to new security risks and technical debt. Our position is that the ML
community should embrace this development while building the appropriate
frameworks to mitigate the downsides. By acting now, we can harness AI to
restore user freedom and competitive markets without sacrificing security.

</details>


### [468] [The Jacobian and Hessian of the Kullback-Leibler Divergence between Multivariate Gaussian Distributions (Technical Report)](https://arxiv.org/abs/2506.23996)
*Juan Maroñas*

Main category: cs.LG

TL;DR: This paper derives Jacobian and Hessian matrices for the Kullback-Leibler (KL) divergence between multivariate Gaussians using first and second-order differentials, presenting both results and detailed derivations.


<details>
  <summary>Details</summary>
Motivation: To calculate Jacobian and Hessian matrices of the KL divergence between multivariate Gaussian distributions for enhanced analytical insights, with a focus on clarity and educational value.

Method: Utilizes theories from Magnus (1999) and Minka to derive Jacobian and Hessian matrices using first and second-order differentials, supported by detailed step-by-step derivations and references.

Result: Successfully provides analytical derivations of the Jacobian and Hessian matrices of the KL divergence for multivariate Gaussian distributions, clarified through examples and theoretical references.

Conclusion: The paper offers a thorough and didactic guide to deriving key mathematical structures for the KL divergence, filling gaps in understanding and aiding future applications in analytical and statistical methodologies.

Abstract: This document shows how to obtain the Jacobian and Hessian matrices of the
Kullback-Leibler divergence between two multivariate Gaussian distributions,
using the first and second-order differentials. The presented derivations are
based on the theory presented by \cite{magnus99}. I've also got great
inspiration from some of the derivations in \cite{minka}.
  Since I pretend to be at most didactic, the document is split into a summary
of results and detailed derivations on each of the elements involved, with
specific references to the tricks used in the derivations, and to many of the
underlying concepts.

</details>


### [469] [The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models](https://arxiv.org/abs/2506.24000)
*Lijun Sheng,Jian Liang,Ran He,Zilei Wang,Tieniu Tan*

Main category: cs.LG

TL;DR: The paper introduces TTA-VLM, a benchmark for evaluating test-time adaptation (TTA) methods on vision-language models (VLMs) with diverse metrics and reproducible framework.


<details>
  <summary>Details</summary>
Motivation: Address limitations of current TTA research, such as poor evaluation consistency, to enable fair comparison and deeper insights into TTA methods for VLMs.

Method: Develop a benchmark called TTA-VLM that supports multiple TTA methods and evaluates them across diverse metrics—including accuracy and robustness—on 15 datasets.

Result: Find existing TTA methods offer limited benefits, perform poorly with training-time fine-tuning methods, and often reduce model trustworthiness.

Conclusion: TTA-VLM serves as a valuable tool for fair comparison and comprehensive analysis of TTA methods, encouraging the development of more reliable approaches.

Abstract: Test-time adaptation (TTA) methods have gained significant attention for
enhancing the performance of vision-language models (VLMs) such as CLIP during
inference, without requiring additional labeled data. However, current TTA
researches generally suffer from major limitations such as duplication of
baseline results, limited evaluation metrics, inconsistent experimental
settings, and insufficient analysis. These problems hinder fair comparisons
between TTA methods and obscure their practical strengths and weaknesses. To
address these challenges, we introduce TTA-VLM, a comprehensive benchmark for
evaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7
online TTA methods within a unified and reproducible framework, and evaluates
them across 15 widely used datasets. Unlike prior studies focused solely on
CLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid
loss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA
to assess generality. Beyond classification accuracy, TTA-VLM incorporates
various evaluation metrics, including robustness, calibration,
out-of-distribution detection, and stability, enabling a more holistic
assessment of TTA methods. Through extensive experiments, we find that 1)
existing TTA methods produce limited gains compared to the previous pioneering
work; 2) current TTA methods exhibit poor collaboration with training-time
fine-tuning methods; 3) accuracy gains frequently come at the cost of reduced
model trustworthiness. We release TTA-VLM to provide fair comparison and
comprehensive evaluation of TTA methods for VLMs, and we hope it encourages the
community to develop more reliable and generalizable TTA strategies.

</details>


### [470] [Provably Efficient and Agile Randomized Q-Learning](https://arxiv.org/abs/2506.24005)
*He Wang,Xingyu Xu,Yuejie Chi*

Main category: cs.LG

TL;DR: This paper introduces RandomizedQ, a novel Q-learning variant for episodic tabular RL, combining sampling-based exploration and step-wise policy updates, achieving competitive empirical and theoretical results.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address computational inefficiency and slow learning in provable RL algorithms, particularly in the context of model-free Bayesian-based exploration, which lacks theoretical grounding.

Method: RandomizedQ applies sampling-based exploration with step-wise policy updates, ensuring responsiveness while guaranteeing provable regret bounds.

Result: The proposed algorithm achieves an $
widetilde{O}(\sqrt{H^5SAT})$ regret bound and demonstrates exceptional empirical performance compared to existing Q-learning variants.

Conclusion: RandomizedQ bridges refined theoretical insight and practical efficiency, showing promise as an effective exploration method for RL tasks.

Abstract: While Bayesian-based exploration often demonstrates superior empirical
performance compared to bonus-based methods in model-based reinforcement
learning (RL), its theoretical understanding remains limited for model-free
settings. Existing provable algorithms either suffer from computational
intractability or rely on stage-wise policy updates which reduce responsiveness
and slow down the learning process. In this paper, we propose a novel variant
of Q-learning algorithm, refereed to as RandomizedQ, which integrates
sampling-based exploration with agile, step-wise, policy updates, for episodic
tabular RL. We establish an $\widetilde{O}(\sqrt{H^5SAT})$ regret bound, where
$S$ is the number of states, $A$ is the number of actions, $H$ is the episode
length, and $T$ is the total number of episodes. In addition, we present a
logarithmic regret bound under a mild positive sub-optimality condition on the
optimal Q-function. Empirically, RandomizedQ exhibits outstanding performance
compared to existing Q-learning variants with both bonus-based and
Bayesian-based exploration on standard benchmarks.

</details>


### [471] [Bridging Theory and Practice in Link Representation with Graph Neural Networks](https://arxiv.org/abs/2506.24018)
*Veronica Lachi,Francesco Ferrini,Antonio Longa,Bruno Lepri,Andrea Passerini,Manfred Jaeger*

Main category: cs.LG

TL;DR: This paper studies the expressive power of Graph Neural Networks (GNNs) for link prediction, introducing a unifying framework and synthetic benchmark for analysis.


<details>
  <summary>Details</summary>
Motivation: Most prior theoretical work on GNNs focuses on graph-level representations, neglecting the expressiveness needed for node pair/link-level tasks.

Method: The authors developed the $k_\phi$-$k_\rho$-$m$ framework, which generalizes existing link models, derived an expressiveness hierarchy, and proposed evaluation protocols for benchmarking link-level performance.

Result: Expressive GNN models outperform simpler ones in challenging settings with high graph symmetry, although they may underperform on standard benchmarks.

Conclusion: Expressiveness matters in practical applications and dataset-aware model selection is critical for link prediction tasks.

Abstract: Graph Neural Networks (GNNs) are widely used to compute representations of
node pairs for downstream tasks such as link prediction. Yet, theoretical
understanding of their expressive power has focused almost entirely on
graph-level representations. In this work, we shift the focus to links and
provide the first comprehensive study of GNN expressiveness in link
representation. We introduce a unifying framework, the $k_\phi$-$k_\rho$-$m$
framework, that subsumes existing message-passing link models and enables
formal expressiveness comparisons. Using this framework, we derive a hierarchy
of state-of-the-art methods and offer theoretical tools to analyze future
architectures. To complement our analysis, we propose a synthetic evaluation
protocol comprising the first benchmark specifically designed to assess
link-level expressiveness. Finally, we ask: does expressiveness matter in
practice? We use a graph symmetry metric that quantifies the difficulty of
distinguishing links and show that while expressive models may underperform on
standard benchmarks, they significantly outperform simpler ones as symmetry
increases, highlighting the need for dataset-aware model selection.

</details>


### [472] [Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies](https://arxiv.org/abs/2506.24093)
*Paul Wachter,Lukas Niehaus,Julius Schöning*

Main category: cs.LG

TL;DR: This paper studies the effectiveness of mixing synthetic and real data for ANN training to address domain gaps and evaluates the strategies across tasks and architectures.


<details>
  <summary>Details</summary>
Motivation: Synthetic data is cost-effective for ANN training, but domain gaps with real data hinder real-world performance.

Method: The study systematically evaluates mixing strategies for synthetic and real data on three architectures and hybrid datasets with varying proportions.

Result: The findings provide insights into the impact of synthetic and real data components and their influence on robustness and efficacy.

Conclusion: Optimizing synthetic data usage for ANN training can enhance generalizability, robustness, and applicability in real-world scenarios.

Abstract: Synthetic data has emerged as a cost-effective alternative to real data for
training artificial neural networks (ANN). However, the disparity between
synthetic and real data results in a domain gap. That gap leads to poor
performance and generalization of the trained ANN when applied to real-world
scenarios. Several strategies have been developed to bridge this gap, which
combine synthetic and real data, known as mixed training using hybrid datasets.
While these strategies have been shown to mitigate the domain gap, a systematic
evaluation of their generalizability and robustness across various tasks and
architectures remains underexplored. To address this challenge, our study
comprehensively analyzes two widely used mixing strategies on three prevalent
architectures and three distinct hybrid datasets. From these datasets, we
sample subsets with varying proportions of synthetic to real data to
investigate the impact of synthetic and real components. The findings of this
paper provide valuable insights into optimizing the use of synthetic data in
the training process of any ANN, contributing to enhancing robustness and
efficacy.

</details>


### [473] [Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives](https://arxiv.org/abs/2506.24124)
*Dong Sixun,Fan Wei,Teresa Wu,Fu Yanjie*

Main category: cs.LG

TL;DR: The paper introduces a multimodal approach for time series forecasting by converting numerical inputs into visual and textual forms and aligning them using contrastive learning for improved representation and performance.


<details>
  <summary>Details</summary>
Motivation: Traditional unimodal numerical time series forecasting struggles with capturing high-level semantic patterns due to their dense and unstructured nature. This paper aims to harness multimodal perspectives, such as visual and textual representations, for improved pattern recognition and forecasting.

Method: The proposed method uses a multimodal contrastive learning framework to transform numerical time series data into visual and textual representations. The alignment of these modalities in a shared semantic space is achieved through contrastive learning. Additionally, a variate selection module is introduced to identify key informative variables for multivariate forecasting.

Result: The multimodal approach outperforms strong unimodal and cross-modal baselines across fifteen short-term and six long-term forecasting benchmarks. This highlights the advantage of the proposed multimodal alignment framework.

Conclusion: Multimodal alignment, leveraging structured visual and textual representations, significantly enhances time series forecasting capabilities, especially in capturing richer and complementary representations compared to unimodal approaches.

Abstract: Time series forecasting traditionally relies on unimodal numerical inputs,
which often struggle to capture high-level semantic patterns due to their dense
and unstructured nature. While recent approaches have explored representing
time series as text using large language models (LLMs), these methods remain
limited by the discrete nature of token sequences and lack the perceptual
intuition humans typically apply, such as interpreting visual patterns. In this
paper, we propose a multimodal contrastive learning framework that transforms
raw time series into structured visual and textual perspectives. Rather than
using natural language or real-world images, we construct both modalities
directly from numerical sequences. We then align these views in a shared
semantic space via contrastive learning, enabling the model to capture richer
and more complementary representations. Furthermore, we introduce a variate
selection module that leverages the aligned representations to identify the
most informative variables for multivariate forecasting. Extensive experiments
on fifteen short-term and six long-term forecasting benchmarks demonstrate that
our approach consistently outperforms strong unimodal and cross-modal
baselines, highlighting the effectiveness of multimodal alignment in enhancing
time series forecasting. Code is available at:
https://github.com/Ironieser/TimesCLIP.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [474] [Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation](https://arxiv.org/abs/2506.23717)
*Xingting Yao,Qinghao Hu,Fei Zhou,Tielong Liu,Gang Li,Peisong Wang,Jian Cheng*

Main category: cs.NE

TL;DR: The paper proposes a novel adaptive bit allocation strategy for spiking neural networks (SNNs) to enhance efficiency and accuracy by dynamically tuning bit widths and temporal lengths in a layer-wise fashion.


<details>
  <summary>Details</summary>
Motivation: To overcome the rising memory and computational demands in multi-bit SNNs, the study aims to allocate resources more effectively by leveraging the varying importance of layers, thus avoiding inefficiencies.

Method: The authors introduce a parametric approach where bit widths of weights/spikes and temporal lengths are learnable through gradients. A refined spiking neuron is proposed to address challenges of variable bit widths and temporal lengths, along with a step-size renewal mechanism to mitigate quantization errors.

Result: The approach demonstrates enhanced efficiency and accuracy on static datasets (CIFAR, ImageNet) and dynamic datasets (CIFAR-DVS, DVS-GESTURE), achieving up to 2.69% accuracy improvement and 4.16× lower bit usage compared to baselines on ImageNet.

Conclusion: The adaptive bit allocation strategy significantly improves resource allocation and quantization efficiency in SNNs without compromising accuracy, marking a step forward in energy-efficient AI solutions.

Abstract: Multi-bit spiking neural networks (SNNs) have recently become a heated
research spot, pursuing energy-efficient and high-accurate AI. However, with
more bits involved, the associated memory and computation demands escalate to
the point where the performance improvements become disproportionate. Based on
the insight that different layers demonstrate different importance and extra
bits could be wasted and interfering, this paper presents an adaptive bit
allocation strategy for direct-trained SNNs, achieving fine-grained layer-wise
allocation of memory and computation resources. Thus, SNN's efficiency and
accuracy can be improved. Specifically, we parametrize the temporal lengths and
the bit widths of weights and spikes, and make them learnable and controllable
through gradients. To address the challenges caused by changeable bit widths
and temporal lengths, we propose the refined spiking neuron, which can handle
different temporal lengths, enable the derivation of gradients for temporal
lengths, and suit spike quantization better. In addition, we theoretically
formulate the step-size mismatch problem of learnable bit widths, which may
incur severe quantization errors to SNN, and accordingly propose the step-size
renewal mechanism to alleviate this issue. Experiments on various datasets,
including the static CIFAR and ImageNet and the dynamic CIFAR-DVS and
DVS-GESTURE, demonstrate that our methods can reduce the overall memory and
computation cost while achieving higher accuracy. Particularly, our
SEWResNet-34 can achieve a 2.69\% accuracy gain and 4.16$\times$ lower bit
budgets over the advanced baseline work on ImageNet. This work will be fully
open-sourced.

</details>


### [475] [Marker Gene Method : Identifying Stable Solutions in a Dynamic Environment](https://arxiv.org/abs/2506.23734)
*Hao Shi,Xi Li,Fangfang Xie*

Main category: cs.NE

TL;DR: This paper proposes the Marker Gene Method (MGM) to improve the stability of Competitive Co-evolutionary Algorithms (CCEAs) by introducing a "marker gene" and adaptive weighting mechanisms, demonstrating strong theoretical and empirical performance.


<details>
  <summary>Details</summary>
Motivation: To address issues like intransitivity and the Red Queen effect that hinder stable convergence in Competitive Co-evolutionary Algorithms (CCEAs).

Method: The Marker Gene Method (MGM) introduces a 'marker gene' as a dynamic benchmark along with an adaptive weighting mechanism to stabilize CCEAs. Mathematical proofs and a Memory Pool (MP) extension further enhance performance.

Result: MGM stabilizes the Rock-Paper-Scissors game, improves C-RMOEA/D performance on ZDT benchmarks, and addresses challenges in complex games like the Shapley Biased Game with the MP extension.

Conclusion: MGM provides a robust, theoretically grounded framework for improving the stability and performance of CCEAs in competitive and complex environments.

Abstract: Competitive Co-evolutionary Algorithms (CCEAs) are often hampered by complex
dynamics like intransitivity and the Red Queen effect, leading to unstable
convergence. To counter these challenges, this paper introduces the Marker Gene
Method (MGM), a framework that establishes stability by using a 'marker gene'
as a dynamic benchmark and an adaptive weighting mechanism to balance
exploration and exploitation. We provide rigorous mathematical proofs
demonstrating that MGM creates strong attractors near Nash Equilibria within
the Strictly Competitive Game framework. Empirically, MGM demonstrates its
efficacy across a spectrum of challenges: it stabilizes the canonical
Rock-Paper-Scissors game, significantly improves the performance of C-RMOEA/D
on ZDT benchmarks, and, when augmented with a Memory Pool (MP) extension, it
successfully tames the notoriously pathological Shapley Biased Game. This work
presents a theoretically sound and empirically validated framework that
substantially enhances the stability and robustness of CCEAs in complex
competitive environments.

</details>


### [476] [More Efficient Real-Valued Gray-Box Optimization through Incremental Distribution Estimation in RV-GOMEA](https://arxiv.org/abs/2506.23738)
*Renzo J. Scholman,Tanja Alderliesten,Peter A. N. Bosman*

Main category: cs.NE

TL;DR: The paper investigates using incremental distribution estimation to improve the efficiency of RV-GOMEA for solving problems with overlapping dependencies.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency of RV-GOMEA, particularly in cases where overlapping dependencies in problems are present, by exploring the possibility of incremental distribution estimation.

Method: The authors integrate incremental learning mechanisms for the Gaussian distribution into RV-GOMEA. They compare its efficiency against RV-GOMEA and VKD-CMA-ES on benchmark problems with varying dependency overlaps.

Result: Results indicate that incremental distribution estimation reduces the required number of evaluations. Specifically, it reduces evaluations by a factor of up to 1.5 with problem-specific population tuning, and by 2-3 with generic population sizing.

Conclusion: Incremental distribution estimation enhances RV-GOMEA's efficiency significantly, particularly when population sizes are appropriately configured, making it a promising approach for further refinement.

Abstract: The Gene-pool Optimal Mixing EA (GOMEA) family of EAs offers a specific means
to exploit problem-specific knowledge through linkage learning, i.e.,
inter-variable dependency detection, expressed using subsets of variables, that
should undergo joint variation. Such knowledge can be exploited if faster
fitness evaluations are possible when only a few variables are changed in a
solution, enabling large speed-ups. The recent-most version of Real-Valued
GOMEA (RV-GOMEA) can learn a conditional linkage model during optimization
using fitness-based linkage learning, enabling fine-grained dependency
exploitation in learning and sampling a Gaussian distribution. However, while
the most efficient Gaussian-based EAs, like NES and CMA-ES, employ incremental
learning of the Gaussian distribution rather than performing full re-estimation
every generation, the recent-most RV-GOMEA version does not employ such
incremental learning. In this paper, we therefore study whether incremental
distribution estimation can lead to efficiency enhancements of RV-GOMEA. We
consider various benchmark problems with varying degrees of overlapping
dependencies. We find that, compared to RV-GOMEA and VKD-CMA-ES, the required
number of evaluations to reach high-quality solutions can be reduced by a
factor of up to 1.5 if population sizes are tuned problem-specifically, while a
reduction by a factor of 2-3 can be achieved with generic population-sizing
guidelines.

</details>


### [477] [Unsupervised Sparse Coding-based Spiking Neural Network for Real-time Spike Sorting](https://arxiv.org/abs/2506.24041)
*Alexis Melot,Sean U. N. Wood,Yannick Coffinier,Pierre Yger,Fabien Alibart*

Main category: cs.NE

TL;DR: The paper introduces Neuromorphic Sparse Sorter (NSS), a compact spiking neural network for efficient, real-time spike sorting at the edge, enabling higher neural decoding performance.


<details>
  <summary>Details</summary>
Motivation: Spike sorting is fundamental for decoding neural signals efficiently in BMIs, with challenges in achieving real-time, low-power, and high-performance decoding at the edge.

Method: They propose NSS, a two-layer spiking neural network utilizing Locally Competitive Algorithm (LCA) for real-time, unsupervised spike sorting with custom neuron models leveraging multi-bit spike coding on Intel's Loihi 2.

Result: Evaluations on tetrode signals demonstrated NSS achieving an F1-score of 77%, outperforming existing pipelines like WaveClus3 while consuming only 8.6mW and processing in 0.25ms per inference.

Conclusion: NSS shows significant advancements in real-time spike sorting, emphasizing its potential for low-power BMIs and edge computing platforms with improved accuracy and efficiency.

Abstract: Spike sorting is a crucial step in decoding multichannel extracellular neural
signals, enabling the identification of individual neuronal activity. A key
challenge in brain-machine interfaces (BMIs) is achieving real-time, low-power
spike sorting at the edge while keeping high neural decoding performance. This
study introduces the Neuromorphic Sparse Sorter (NSS), a compact two-layer
spiking neural network optimized for efficient spike sorting. NSS leverages the
Locally Competitive Algorithm (LCA) for sparse coding to extract relevant
features from noisy events with reduced computational demands. NSS learns to
sort detected spike waveforms in an online fashion and operates entirely
unsupervised. To exploit multi-bit spike coding capabilities of neuromorphic
platforms like Intel's Loihi 2, a custom neuron model was implemented, enabling
flexible power-performance trade-offs via adjustable spike bit-widths.
Evaluations on simulated and real-world tetrode signals with biological drift
showed NSS outperformed established pipelines such as WaveClus3 and PCA+KMeans.
With 2-bit graded spikes, NSS on Loihi 2 outperformed NSS implemented with
leaky integrate-and-fire neuron and achieved an F1-score of 77% (+10%
improvement) while consuming 8.6mW (+1.65mW) when tested on a drifting
recording, with a computational processing time of 0.25ms (+60 us) per
inference.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [478] [Data-Driven Power Modeling and Monitoring via Hardware Performance Counter Tracking](https://arxiv.org/abs/2506.23672)
*Sergio Mazzola,Gabriele Ara,Thomas Benz,Björn Forsberg,Tommaso Cucinotta,Luca Benini*

Main category: cs.PF

TL;DR: The paper introduces a novel power modeling methodology for heterogeneous hardware systems, achieving high accuracy (7.5% error for power, 1.3% for energy) with minimal runtime overhead (0.7%). The models are integrated into a Linux kernel-based framework for real-time power measurements.


<details>
  <summary>Details</summary>
Motivation: The need for energy-efficient designs in embedded systems under real-time constraints, compounded by the challenges posed by hardware heterogeneity and parallelism.

Method: Simple, state-specific power models based on Performance Monitoring Counters (PMCs) are developed and combined into a comprehensive framework integrated into the Linux kernel using the Runmeter monitoring tool.

Result: The approach achieves state-of-the-art power and energy estimation accuracy with negligible runtime overhead, enabling responsive real-time measurements.

Conclusion: This methodology facilitates dynamic adaptations in hardware and software stacks, improving workload-aware DVFS and closed-loop task scheduling in power-constrained systems.

Abstract: Energy-centric design is paramount in the current embedded computing era: use
cases require increasingly high performance at an affordable power budget,
often under real-time constraints. Hardware heterogeneity and parallelism help
address the efficiency challenge, but greatly complicate online power
consumption assessments, which are essential for dynamic hardware and software
stack adaptations. We introduce a novel power modeling methodology with
state-of-the-art accuracy, low overhead, and high responsiveness, whose
implementation does not rely on microarchitectural details. Our methodology
identifies the Performance Monitoring Counters (PMCs) with the highest linear
correlation to the power consumption of each hardware sub-system, for each
Dynamic Voltage and Frequency Scaling (DVFS) state. The individual, simple
models are composed into a complete model that effectively describes the power
consumption of the whole system, achieving high accuracy and low overhead. Our
evaluation reports an average estimation error of 7.5% for power consumption
and 1.3% for energy. We integrate these models in the Linux kernel with
Runmeter, an open-source, PMC-based monitoring framework. Runmeter manages PMC
sampling and processing, enabling the execution of our power models at runtime.
With a worst-case time overhead of only 0.7%, Runmeter provides responsive and
accurate power measurements directly in the kernel. This information can be
employed for actuation policies in workload-aware DVFS and power-aware,
closed-loop task scheduling.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [479] [Verifying Properties of Index Arrays in a Purely-Functional Data-Parallel Language](https://arxiv.org/abs/2506.23058)
*Nikolaj Hey Hinnerskov,Robert Schenck,Cosmin E. Oancea*

Main category: cs.PL

TL;DR: The paper introduces a method for verifying properties of data-parallel programs using index functions and algebraic reasoning, enabling optimizations and correctness guarantees.


<details>
  <summary>Details</summary>
Motivation: Existing techniques struggle to verify properties of data-parallel programs with non-linear indexing, which hampers optimization and correctness.

Method: Programs are treated as index function transformations. Properties are proved by converting them into algebraic inequalities and solving them using a Fourier-Motzkin-based approach.

Result: Implementation in Futhark verifies properties in an average time of 1 second for seven applications. Case studies show significant GPU speedups due to reduced dynamic verification.

Conclusion: The approach is practical, enabling automated, meaningful program guarantees that enhance correctness and compiler optimization without restricting expressiveness.

Abstract: This paper presents a novel approach to automatically verify properties of
pure data-parallel programs with non-linear indexing -- expressed as pre- and
post-conditions on functions. Programs consist of nests of second-order array
combinators (e.g., map, scan, and scatter) and loops. The key idea is to
represent arrays as index functions: programs are index function
transformations over which properties are propagated and inferred. Our
framework proves properties on index functions by distilling them into
algebraic (in)equalities and discharging them to a Fourier-Motzkin-based
solver. The framework is practical and accessible: properties are not
restricted to a decidable logic, but instead are carefully selected to express
practically useful guarantees that can be automatically reasoned about and
inferred. These guarantees extend beyond program correctness and can be
exploited by the entire compiler pipeline for optimization. We implement our
system in the pure data-parallel language Futhark and demonstrate its
practicality on seven applications, reporting an average verification time of 1
second. Two case studies show how eliminating dynamic verification in GPU
programs results in significant speedups.

</details>


### [480] [A Denotational Semantics for Quantum Loops](https://arxiv.org/abs/2506.23320)
*Nicola Assolini,Alessandra Di Pierro*

Main category: cs.PL

TL;DR: The paper proposes a mathematical framework to understand high-level quantum programming concepts, specifically quantum-controlled branching and iteration.


<details>
  <summary>Details</summary>
Motivation: To provide a conceptual and mathematical understanding of quantum control flow and loops in quantum programming.

Method: Introduces a denotational domain to define mathematical semantics for quantum-controlled branching and iteration.

Result: Demonstrates conceptual coherence evolution of quantum systems implementing high-level quantum programming constructs.

Conclusion: The proposed semantics enhance understanding at the abstraction level for quantum programming and quantum control flow systems.

Abstract: Programming a quantum computer, i.e., implementing quantum algorithms on a
quantum processor-based copmputer architecture, is a task that can be addressed
(just as for classical computers) at different levels of abstraction. This
paper proposes a denotational semantics for high-level quantum programming
constructs, focusing on the conceptual meaning of quantum-controlled branching
and iteration. We introduce a denotational domain where a mathematical meaning
of a quantum control flow with loops can be defined, which reflects the
coherent evolution of the quantum system implementing the program.

</details>


### [481] [Compiling a Q# Subset to QASM 3.0 in TypeScript via a JSON Based IR](https://arxiv.org/abs/2506.23407)
*Marcus Edwards*

Main category: cs.PL

TL;DR: The paper introduces a TypeScript-based toolchain for converting Q# to QASM 3.0, including lexer, parser, and compiler.


<details>
  <summary>Details</summary>
Motivation: To provide a web-compatible alternative to Microsoft's Q# compiler toolchain.

Method: Developed a TypeScript-based lexer, parser, and subset-supporting compiler; tested on Q# examples and compared with existing tools.

Result: The toolchain successfully processes Q# programs and is functional in web environments.

Conclusion: This TypeScript-written toolchain expands Q# compilation compatibility for web-based applications.

Abstract: We implement a compile toolchain from Q# to QASM 3.0 including a
full-featured lexer and parser implementation, as well as a compiler that
supports a subset of Q# features. The lexer, parser and compiler are shown to
work with various input Q# programs and the implementation is compared against
existing Q# compile tools. Unlike the Microsoft implementation of the official
Q# compile toolchain, our implementation is written in TypeScript in order to
port functionality to web environments.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [482] [Conversations with Andrea: Visitors' Opinions on Android Robots in a Museum](https://arxiv.org/abs/2506.22466)
*Marcel Heisler,Christian Becker-Asano*

Main category: cs.RO

TL;DR: An autonomous android robot, Andrea, had conversations with museum visitors in Germany over six days, gathering opinions about its use-cases, general perceptions, and improvement suggestions. Gender cues via voice and wig changes showed no significant impact on visitor perception.


<details>
  <summary>Details</summary>
Motivation: Explore real-world visitor interactions with an autonomous android robot to understand possible use-cases and gather feedback for system improvement.

Method: Set up Andrea in a public museum for fully autonomous interactions with visitors across six days, using structured interviews and analyzing system logs.

Result: Visitor feedback showed positive overall perception. Desired improvements included multilingual support, quicker responses, and museum-related information provision. Gender cue changes did not significantly affect perceptions.

Conclusion: Insights from real-world interactions will guide improvements to make Andrea a practical, useful application in public settings like museums.

Abstract: The android robot Andrea was set up at a public museum in Germany for six
consecutive days to have conversations with visitors, fully autonomously. No
specific context was given, so visitors could state their opinions regarding
possible use-cases in structured interviews, without any bias. Additionally the
44 interviewees were asked for their general opinions of the robot, their
reasons (not) to interact with it and necessary improvements for future use.
The android's voice and wig were changed between different days of operation to
give varying cues regarding its gender. This did not have a significant impact
on the positive overall perception of the robot. Most visitors want the robot
to provide information about exhibits in the future, while opinions on other
roles, like a receptionist, were both wanted and explicitly not wanted by
different visitors. Speaking more languages (than only English) and faster
response times were the improvements most desired. These findings from the
interviews are in line with an analysis of the system logs, which revealed,
that after chitchat and personal questions, most of the 4436 collected requests
asked for information related to the museum and to converse in a different
language. The valuable insights gained from these real-world interactions are
now used to improve the system to become a useful real-world application.

</details>


### [483] [Unsupervised Discovery of Behavioral Primitives from Sensorimotor Dynamic Functional Connectivity](https://arxiv.org/abs/2506.22473)
*Fernando Diaz Ledezma,Valentin Marcel,Matej Hoffmann*

Main category: cs.RO

TL;DR: The paper proposes a framework to analyze sensorimotor information in robots and animals using dynamic functional connectivity, revealing motion primitives for potential use in robot learning and human movement analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to uncover the underlying structure in high-dimensional sensorimotor time series data from robots or animals, enabling understanding and interpretation of these dynamic processes.

Method: The study uses instantaneous mutual information to determine functional connectivity between sensory signals and applies an infinite relational model to identify sensorimotor modules. Non-negative matrix factorization is used to decompose connectivity patterns into additive factors.

Result: The method revealed the motion primitives or movement synergies of an agent, representing the building blocks for interpreting sensorimotor spaces and facilitating behavior selection.

Conclusion: The framework offers a novel approach to understanding sensorimotor dynamics in robots, with potential applications in robot learning, human movement trajectory analysis, and brain signal interpretation.

Abstract: The movements of both animals and robots give rise to streams of
high-dimensional motor and sensory information. Imagine the brain of a newborn
or the controller of a baby humanoid robot trying to make sense of unprocessed
sensorimotor time series. Here, we present a framework for studying the dynamic
functional connectivity between the multimodal sensory signals of a robotic
agent to uncover an underlying structure. Using instantaneous mutual
information, we capture the time-varying functional connectivity (FC) between
proprioceptive, tactile, and visual signals, revealing the sensorimotor
relationships. Using an infinite relational model, we identified sensorimotor
modules and their evolving connectivity. To further interpret these dynamic
interactions, we employed non-negative matrix factorization, which decomposed
the connectivity patterns into additive factors and their corresponding
temporal coefficients. These factors can be considered the agent's motion
primitives or movement synergies that the agent can use to make sense of its
sensorimotor space and later for behavior selection. In the future, the method
can be deployed in robot learning as well as in the analysis of human movement
trajectories or brain signals.

</details>


### [484] [DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios](https://arxiv.org/abs/2506.22494)
*Shihong Ling,Yue Wan,Xiaowei Jia,Na Du*

Main category: cs.RO

TL;DR: Introduces DriveBLIP2, an improved vision-language model using targeted attention for better explainability in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: To overcome the inability of existing vision-language models to handle complex, multi-object environments effectively in real-time driving scenarios.

Method: Developed DriveBLIP2 with an Attention Map Generator to focus on key objects in video frames, improving the explainability of decision-making processes.

Result: Demonstrated superior performance on the DRAMA dataset through improved BLEU, ROUGE, CIDEr, and SPICE scores compared to baseline models.

Conclusion: Targeted attention mechanisms can enhance the explainability and contextual relevance of vision-language models in critical autonomous driving applications.

Abstract: This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT
architecture, to generate accurate and contextually relevant explanations for
emerging driving scenarios. While existing vision-language models perform well
in general tasks, they encounter difficulties in understanding complex,
multi-object environments, particularly in real-time applications such as
autonomous driving, where the rapid identification of key objects is crucial.
To address this limitation, an Attention Map Generator is proposed to highlight
significant objects relevant to driving decisions within critical video frames.
By directing the model's focus to these key regions, the generated attention
map helps produce clear and relevant explanations, enabling drivers to better
understand the vehicle's decision-making process in critical situations.
Evaluations on the DRAMA dataset reveal significant improvements in explanation
quality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared
to baseline models. These findings underscore the potential of targeted
attention mechanisms in vision-language models for enhancing explainability in
real-time autonomous driving.

</details>


### [485] [Directed Shape Morphing using Kirigami-enhanced Thermoplastics](https://arxiv.org/abs/2506.22572)
*Mrunmayi Mungekar,Sanjith Menon,M. Ravi Shankar,M. Khalid Jawed*

Main category: cs.RO

TL;DR: The paper introduces a straightforward method to turn flat plastic sheets into intricate 3D structures using heat and simple tools.


<details>
  <summary>Details</summary>
Motivation: To provide an accessible and adaptable method for creating complex self-morphing structures without detailed process control.

Method: Combining heat-shrinkable thermoplastics with Kirigami patterns to create bilayer composites that morph into 3D shapes using uniform heating.

Result: The method successfully produces various intricate shapes confirmed by finite element simulations, driven by strain mismatch in the composites.

Conclusion: This approach enables scalable manufacturing and adaptive design of self-morphing structures by simplifying material-morphing processes using low-information stimuli.

Abstract: We present a simple, accessible method for autonomously transforming flat
plastic sheets into intricate three-dimensional structures using only uniform
heating and common tools such as household ovens and scissors. Our approach
combines heat-shrinkable thermoplastics with Kirigami patterns tailored to the
target 3D shape, creating bilayer composites that morph into a wide range of
complex structures, e.g., bowls, pyramids, and even custom ergonomic surfaces
like mouse covers. Critically, the transformation is driven by a
low-information stimulus (uniform heat) yet produces highly intricate shapes
through programmed geometric design. The morphing behavior, confirmed by finite
element simulations, arises from strain mismatch between the contracting
thermoplastic layer and the constraining Kirigami layer. By decoupling material
composition from mechanical response, this method avoids detailed process
control and enables a broad class of self-morphing structures, offering a
versatile platform for adaptive design and scalable manufacturing.

</details>


### [486] [Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding](https://arxiv.org/abs/2506.22593)
*Antonello Longo,Chanyoung Chung,Matteo Palieri,Sung-Kyun Kim,Ali Agha,Cataldo Guaragnella,Shehryar Khattak*

Main category: cs.RO

TL;DR: This paper introduces Pixels-to-Graph (Pix2G), a method for creating structured scene graphs from image pixels and LiDAR data, aimed at improving human-robot cooperation under compute constraints.


<details>
  <summary>Details</summary>
Motivation: The need for efficient human-robot collaboration in risky tasks because humans work better with compact, high-level representations of environments, unlike standard robotic 3D planning.

Method: Pix2G generates structured scene graphs from real-time data using image pixels and LiDAR maps, operating entirely on CPU to meet resource constraints.

Result: The method produces structured multi-layer graphs combining denoised 2D maps and segmented 3D point clouds, tested successfully on complex environments using NASA JPL NeBula-Spot robots.

Conclusion: Pix2G provides a lightweight and efficient solution for bridging human-readable maps and robotic spatial data for real-time exploration in resource-limited systems.

Abstract: Autonomous robots are increasingly playing key roles as support platforms for
human operators in high-risk, dangerous applications. To accomplish challenging
tasks, an efficient human-robot cooperation and understanding is required.
While typically robotic planning leverages 3D geometric information, human
operators are accustomed to a high-level compact representation of the
environment, like top-down 2D maps representing the Building Information Model
(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap
between human readable 2D BIM and the robot 3D maps. In this work, we introduce
Pixels-to-Graph (Pix2G), a novel lightweight method to generate structured
scene graphs from image pixels and LiDAR maps in real-time for the autonomous
exploration of unknown environments on resource-constrained robot platforms. To
satisfy onboard compute constraints, the framework is designed to perform all
operation on CPU only. The method output are a de-noised 2D top-down
environment map and a structure-segmented 3D pointcloud which are seamlessly
connected using a multi-layer graph abstracting information from object-level
up to the building-level. The proposed method is quantitatively and
qualitatively evaluated during real-world experiments performed using the NASA
JPL NeBula-Spot legged robot to autonomously explore and map cluttered garage
and urban office like environments in real-time.

</details>


### [487] [Robust Peg-in-Hole Assembly under Uncertainties via Compliant and Interactive Contact-Rich Manipulation](https://arxiv.org/abs/2506.22766)
*Yiting Chen,Kenneth Kimble,Howard H. Qian,Podshara Chanrungmaneekul,Robert Seney,Kaiyu Hang*

Main category: cs.RO

TL;DR: The paper introduces a manipulation system to perform robust peg-in-hole assembly under unstructured and uncertain conditions without relying on precise perception, using environmental contact constraints and an uncertainty-absorbing approach.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of performing robotic peg-in-hole assembly, which is critical for industry but difficult due to perceptual and physical uncertainties exceeding clearance tolerances.

Method: The method involves leveraging contact mechanics and compliance to iteratively localize the target hole, refine insertion motions through contact constraints, and conceptualizing these actions as manipulation funnels for uncertainty absorption.

Result: The results show that the proposed system generalizes well across various scales, shapes, and materials in diverse peg-in-hole scenarios, validated through extensive real-world experiments, including those on a NIST Assembly Task Board.

Conclusion: The paper concludes that the proposed system provides a robust and learning-free solution to peg-in-hole assembly, suitable for unstructured and uncertain conditions.

Abstract: Robust and adaptive robotic peg-in-hole assembly under tight tolerances is
critical to various industrial applications. However, it remains an open
challenge due to perceptual and physical uncertainties from contact-rich
interactions that easily exceed the allowed clearance. In this paper, we study
how to leverage contact between the peg and its matching hole to eliminate
uncertainties in the assembly process under unstructured settings. By examining
the role of compliance under contact constraints, we present a manipulation
system that plans collision-inclusive interactions for the peg to 1)
iteratively identify its task environment to localize the target hole and 2)
exploit environmental contact constraints to refine insertion motions into the
target hole without relying on precise perception, enabling a robust solution
to peg-in-hole assembly. By conceptualizing the above process as the
composition of funneling in different state spaces, we present a formal
approach to constructing manipulation funnels as an uncertainty-absorbing
paradigm for peg-in-hole assembly. The proposed system effectively generalizes
across diverse peg-in-hole scenarios across varying scales, shapes, and
materials in a learning-free manner. Extensive experiments on a NIST Assembly
Task Board (ATB) and additional challenging scenarios validate its robustness
in real-world applications.

</details>


### [488] [Learning Efficient Robotic Garment Manipulation with Standardization](https://arxiv.org/abs/2506.22769)
*Changshi Zhou,Feng Luan,Jiarui Hu,Shaoqiang Meng,Zhipeng Wang,Yanchao Dong,Yanmin Zhou,Bin He*

Main category: cs.RO

TL;DR: The paper introduces APS-Net, a robotic method for unfolding and standardizing garments by optimizing coverage, alignment, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Efficient garment manipulation is hindered by complex dynamics and self-occlusion, requiring a standardized approach to simplify tasks like folding and ironing.

Method: APS-Net uses a dual-arm, multi-primitive policy for dynamic fling to unfold garments and pick-and-place functions for precise alignment, guided by a factorized reward function and optimized action modules.

Result: APS-Net shows superior metrics in garment coverage (+3.9%), IoU (+5.2%), and distance reduction (-0.14 KD) compared to existing methods during simulation and improves folding efficiency in real-world scenarios.

Conclusion: Integrating garment unfolding and standardization provides significant advantages in robotic manipulation, streamlining downstream tasks like folding and ironing.

Abstract: Garment manipulation is a significant challenge for robots due to the complex
dynamics and potential self-occlusion of garments. Most existing methods of
efficient garment unfolding overlook the crucial role of standardization of
flattened garments, which could significantly simplify downstream tasks like
folding, ironing, and packing. This paper presents APS-Net, a novel approach to
garment manipulation that combines unfolding and standardization in a unified
framework. APS-Net employs a dual-arm, multi-primitive policy with dynamic
fling to quickly unfold crumpled garments and pick-and-place (p and p) for
precise alignment. The purpose of garment standardization during unfolding
involves not only maximizing surface coverage but also aligning the garment's
shape and orientation to predefined requirements. To guide effective robot
learning, we introduce a novel factorized reward function for standardization,
which incorporates garment coverage (Cov), keypoint distance (KD), and
intersection-over-union (IoU) metrics. Additionally, we introduce a spatial
action mask and an Action Optimized Module to improve unfolding efficiency by
selecting actions and operation points effectively. In simulation, APS-Net
outperforms state-of-the-art methods for long sleeves, achieving 3.9 percent
better coverage, 5.2 percent higher IoU, and a 0.14 decrease in KD (7.09
percent relative reduction). Real-world folding tasks further demonstrate that
standardization simplifies the folding process. Project page: see
https://hellohaia.github.io/APS/

</details>


### [489] [SPI-BoTER: Error Compensation for Industrial Robots via Sparse Attention Masking and Hybrid Loss with Spatial-Physical Information](https://arxiv.org/abs/2506.22788)
*Xuao Hou,Yongquan Jia,Shijin Zhang,Yuqiang Wu*

Main category: cs.RO

TL;DR: The paper introduces SPI-BoTER, a spatial-physical informed attention network, to enhance trajectory accuracy for industrial robots using minimal sample data. It achieves greater precision and generalization compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current error compensation approaches for industrial robots suffer from simplified modeling, lack of physical consistency, and excessive data requirements, making it challenging to achieve high accuracy and generalization.

Method: The proposed SPI-BoTER model integrates robotic kinematics with a Transformer architecture featuring sparse self-attention masks and uses a hybrid loss function with spatial and physical information. It incorporates gradient descent optimization for inverse joint angle compensation.

Result: Experimental results on a small-sample UR5 robot dataset show a 35.16% error reduction over traditional DNNs, achieving a 3D positioning error of 0.2515 mm and convergence of 0.01 mm accuracy in 147 iterations.

Conclusion: The proposed method offers a physically interpretable and data-adaptive solution for precision tasks in intelligent manufacturing, presenting a significant advancement in industrial robot control.

Abstract: The widespread application of industrial robots in fields such as cutting and
welding has imposed increasingly stringent requirements on the trajectory
accuracy of end-effectors. However, current error compensation methods face
several critical challenges, including overly simplified mechanism modeling, a
lack of physical consistency in data-driven approaches, and substantial data
requirements. These issues make it difficult to achieve both high accuracy and
strong generalization simultaneously. To address these challenges, this paper
proposes a Spatial-Physical Informed Attention Residual Network (SPI-BoTER).
This method integrates the kinematic equations of the robotic manipulator with
a Transformer architecture enhanced by sparse self-attention masks. A
parameter-adaptive hybrid loss function incorporating spatial and physical
information is employed to iteratively optimize the network during training,
enabling high-precision error compensation under small-sample conditions.
Additionally, inverse joint angle compensation is performed using a gradient
descent-based optimization method. Experimental results on a small-sample
dataset from a UR5 robotic arm (724 samples, with a train:test:validation split
of 8:1:1) demonstrate the superior performance of the proposed method. It
achieves a 3D absolute positioning error of 0.2515 mm with a standard deviation
of 0.15 mm, representing a 35.16\% reduction in error compared to conventional
deep neural network (DNN) methods. Furthermore, the inverse angle compensation
algorithm converges to an accuracy of 0.01 mm within an average of 147
iterations. This study presents a solution that combines physical
interpretability with data adaptability for high-precision control of
industrial robots, offering promising potential for the reliable execution of
precision tasks in intelligent manufacturing.

</details>


### [490] [Hierarchical Vision-Language Planning for Multi-Step Humanoid Manipulation](https://arxiv.org/abs/2506.22827)
*André Schakkal,Ben Zandonati,Zhutian Yang,Navid Azizan*

Main category: cs.RO

TL;DR: The paper proposes a hierarchical framework combining vision-language planning and reinforcement learning to enable humanoid robots to successfully perform complex multi-step manipulation tasks. The system demonstrated a high success rate in real-world trials.


<details>
  <summary>Details</summary>
Motivation: To enhance humanoid robots' ability to reliably perform complex multi-step tasks in industrial and household settings.

Method: The framework consists of three layers: a reinforcement learning-based low-level controller, imitation learning mid-level skill policies, and a vision-language high-level planning module for task execution and monitoring.

Result: The proposed system was validated on a humanoid robot in real-world pick-and-place tasks, achieving a 72.5% success rate across 40 trials.

Conclusion: The hierarchical system is effective for multi-step manipulation tasks, with vision-language models proving valuable for skill planning and task monitoring.

Abstract: Enabling humanoid robots to reliably execute complex multi-step manipulation
tasks is crucial for their effective deployment in industrial and household
environments. This paper presents a hierarchical planning and control framework
designed to achieve reliable multi-step humanoid manipulation. The proposed
system comprises three layers: (1) a low-level RL-based controller responsible
for tracking whole-body motion targets; (2) a mid-level set of skill policies
trained via imitation learning that produce motion targets for different steps
of a task; and (3) a high-level vision-language planning module that determines
which skills should be executed and also monitors their completion in real-time
using pretrained vision-language models (VLMs). Experimental validation is
performed on a Unitree G1 humanoid robot executing a non-prehensile
pick-and-place task. Over 40 real-world trials, the hierarchical system
achieved a 72.5% success rate in completing the full manipulation sequence.
These experiments confirm the feasibility of the proposed hierarchical system,
highlighting the benefits of VLM-based skill planning and monitoring for
multi-step manipulation scenarios. See https://vlp-humanoid.github.io/ for
video demonstrations of the policy rollout.

</details>


### [491] [Safe Reinforcement Learning with a Predictive Safety Filter for Motion Planning and Control: A Drifting Vehicle Example](https://arxiv.org/abs/2506.22894)
*Bei Zhou,Baha Zarrouki,Mattia Piccinini,Cheng Hu,Lei Xie,Johannes Betz*

Main category: cs.RO

TL;DR: The paper proposes a Safe Reinforcement Learning (RL)-based motion planner for autonomous drifting by combining RL with drift dynamics and a Predictive Safety Filter (PSF), improving performance and safety.


<details>
  <summary>Details</summary>
Motivation: Current methods face instability and unpredictability during autonomous drifting, particularly at high speeds, leaving safety during learning and deployment insufficiently addressed.

Method: The approach integrates a model-based drift dynamics RL agent and incorporates a Predictive Safety Filter (PSF) to adjust actions online for safety during drifting maneuvers.

Result: Simulations on Matlab-Carsim show improved drift performance, reduced tracking errors, and better computational efficiency compared to traditional methods.

Conclusion: The method effectively enhances safety-critical drifting abilities in autonomous vehicles, addressing key limitations of existing techniques.

Abstract: Autonomous drifting is a complex and crucial maneuver for safety-critical
scenarios like slippery roads and emergency collision avoidance, requiring
precise motion planning and control. Traditional motion planning methods often
struggle with the high instability and unpredictability of drifting,
particularly when operating at high speeds. Recent learning-based approaches
have attempted to tackle this issue but often rely on expert knowledge or have
limited exploration capabilities. Additionally, they do not effectively address
safety concerns during learning and deployment. To overcome these limitations,
we propose a novel Safe Reinforcement Learning (RL)-based motion planner for
autonomous drifting. Our approach integrates an RL agent with model-based drift
dynamics to determine desired drift motion states, while incorporating a
Predictive Safety Filter (PSF) that adjusts the agent's actions online to
prevent unsafe states. This ensures safe and efficient learning, and stable
drift operation. We validate the effectiveness of our method through
simulations on a Matlab-Carsim platform, demonstrating significant improvements
in drift performance, reduced tracking errors, and computational efficiency
compared to traditional methods. This strategy promises to extend the
capabilities of autonomous vehicles in safety-critical maneuvers.

</details>


### [492] [Energy-Constrained Resilient Multi-Robot Coverage Control](https://arxiv.org/abs/2506.22942)
*Kartik A. Pant,Jaehyeok Kim,James M. Goppert,Inseok Hwang*

Main category: cs.RO

TL;DR: The paper proposes a resilient and energy-aware multi-robot control design that maintains network connectivity during disruptions caused by robots leaving for battery charging.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in multi-robot coverage control, especially when robots leave the mission to recharge, disrupting network operations.

Method: Modeling the multi-robot system as a hybrid system with three modes (coverage, return-to-base, recharge) and designing guard conditions to ensure smooth mode transitions. Additionally, introducing an energy-aware bearing rigid network topology design for maintaining resilience.

Result: A resilient approach that systematically manages network connectivity and energy constraints was validated through numerical simulations.

Conclusion: The proposed method achieves desired coverage performance and maintains network connectivity in multi-robot systems, even during disruptions due to energy constraints.

Abstract: The problem of multi-robot coverage control becomes significantly challenging
when multiple robots leave the mission space simultaneously to charge their
batteries, disrupting the underlying network topology for communication and
sensing. To address this, we propose a resilient network design and control
approach that allows robots to achieve the desired coverage performance while
satisfying energy constraints and maintaining network connectivity throughout
the mission. We model the combined motion, energy, and network dynamics of the
multirobot systems (MRS) as a hybrid system with three modes, i.e., coverage,
return-to-base, and recharge, respectively. We show that ensuring the energy
constraints can be transformed into designing appropriate guard conditions for
mode transition between each of the three modes. Additionally, we present a
systematic procedure to design, maintain, and reconfigure the underlying
network topology using an energy-aware bearing rigid network design, enhancing
the structural resilience of the MRS even when a subset of robots departs to
charge their batteries. Finally, we validate our proposed method using
numerical simulations.

</details>


### [493] [SPICE-HL3: Single-Photon, Inertial, and Stereo Camera dataset for Exploration of High-Latitude Lunar Landscapes](https://arxiv.org/abs/2506.22956)
*David Rodríguez-Martínez,Dave van der Meer,Junlin Song,Abishek Bera,C. J. Pérez-del-Pulgar,Miguel Angel Olivares-Mendez*

Main category: cs.RO

TL;DR: This paper presents a dataset from LunaLab emulating high-latitude lunar visual conditions, supporting robotic navigation research.


<details>
  <summary>Details</summary>
Motivation: Robotic navigation in high-latitude lunar regions faces challenges due to complex lighting conditions dominated by low sunlight angles and dynamic shadows.

Method: A dataset was created using LunaLab, which mimics lunar lighting conditions. Data includes images, inertial measurements, and wheel odometry from diverse scenarios featuring different illumination setups and robot speeds.

Result: The dataset contains 88 sequences with 1.3M images collected using various sensors and cameras, enabling the study of visual-based perception tasks under challenging conditions.

Conclusion: This resource facilitates the validation of autonomous navigation systems and supports lunar research applications, particularly for degraded visual environments.

Abstract: Exploring high-latitude lunar regions presents an extremely challenging
visual environment for robots. The low sunlight elevation angle and minimal
light scattering result in a visual field dominated by a high dynamic range
featuring long, dynamic shadows. Reproducing these conditions on Earth requires
sophisticated simulators and specialized facilities. We introduce a unique
dataset recorded at the LunaLab from the SnT - University of Luxembourg, an
indoor test facility designed to replicate the optical characteristics of
multiple lunar latitudes. Our dataset includes images, inertial measurements,
and wheel odometry data from robots navigating seven distinct trajectories
under multiple illumination scenarios, simulating high-latitude lunar
conditions from dawn to night time with and without the aid of headlights,
resulting in 88 distinct sequences containing a total of 1.3M images. Data was
captured using a stereo RGB-inertial sensor, a monocular monochrome camera, and
for the first time, a novel single-photon avalanche diode (SPAD) camera. We
recorded both static and dynamic image sequences, with robots navigating at
slow (5 cm/s) and fast (50 cm/s) speeds. All data is calibrated, synchronized,
and timestamped, providing a valuable resource for validating perception tasks
from vision-based autonomous navigation to scientific imaging for future lunar
missions targeting high-latitude regions or those intended for robots operating
across perceptually degraded environments. The dataset can be downloaded from
https://zenodo.org/records/13970078?preview=1, and a visual overview is
available at https://youtu.be/d7sPeO50_2I. All supplementary material can be
found at https://github.com/spaceuma/spice-hl3.

</details>


### [494] [Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making](https://arxiv.org/abs/2506.23023)
*M. Youssef Abdelhamid,Lennart Vater,Zlatan Ajanovic*

Main category: cs.RO

TL;DR: This paper presents a novel framework, SAD-RL, for improving decision-making algorithms in automated driving using hierarchical reinforcement learning (HRL) in a scenario-based setup.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of safety and efficiency in decision-making for highly automated driving systems, which need to operate in complex and open environments.

Method: The authors proposed SAD-RL, an integration of hierarchical reinforcement learning with a scenario-based environment. A high-level policy handles maneuver selection, while a low-level controller performs those actions. Training scenarios are designed to include rare and challenging situations.

Result: The experiments demonstrated that agents trained with SAD-RL achieve safe and efficient behavior in both simple and complex driving scenarios. Additionally, ablation studies confirmed the necessity of both HRL and scenario diversity.

Conclusion: Scenario-based training and hierarchical RL are critical for improving the robustness and efficiency of decision-making systems in highly automated driving.

Abstract: Developing decision-making algorithms for highly automated driving systems
remains challenging, since these systems have to operate safely in an open and
complex environments. Reinforcement Learning (RL) approaches can learn
comprehensive decision policies directly from experience and already show
promising results in simple driving tasks. However, current approaches fail to
achieve generalizability for more complex driving tasks and lack learning
efficiency. Therefore, we present Scenario-based Automated Driving
Reinforcement Learning (SAD-RL), the first framework that integrates
Reinforcement Learning (RL) of hierarchical policy in a scenario-based
environment. A high-level policy selects maneuver templates that are evaluated
and executed by a low-level control logic. The scenario-based environment
allows to control the training experience for the agent and to explicitly
introduce challenging, but rate situations into the training process. Our
experiments show that an agent trained using the SAD-RL framework can achieve
safe behaviour in easy as well as challenging situations efficiently. Our
ablation studies confirmed that both HRL and scenario diversity are essential
for achieving these results.

</details>


### [495] [Event-based Stereo Visual-Inertial Odometry with Voxel Map](https://arxiv.org/abs/2506.23078)
*Zhaoxing Zhang,Xiaoxiang Wang,Chengliang Zhang,Yangyang Guo,Zikang Yuan,Xin Yang*

Main category: cs.RO

TL;DR: The paper introduces Voxel-ESVIO, a system that improves event-based stereo visual-inertial odometry by using voxel-based map management for better noise filtering and state estimation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of noise in event camera data which impacts the accuracy of map points used for visual odometry.

Method: The proposed method integrates voxel-based point selection and voxel-aware point management to efficiently filter and update high-quality 3D map points, optimizing them on a per-voxel basis.

Result: The system outperforms existing state-of-the-art methods in accuracy and computational efficiency, as tested on three public benchmarks.

Conclusion: Voxel-ESVIO effectively enhances state estimation in event-based visual odometry by providing a noise-resilient map management approach, combining accuracy and efficiency.

Abstract: The event camera, renowned for its high dynamic range and exceptional
temporal resolution, is recognized as an important sensor for visual odometry.
However, the inherent noise in event streams complicates the selection of
high-quality map points, which critically determine the precision of state
estimation. To address this challenge, we propose Voxel-ESVIO, an event-based
stereo visual-inertial odometry system that utilizes voxel map management,
which efficiently filter out high-quality 3D points. Specifically, our
methodology utilizes voxel-based point selection and voxel-aware point
management to collectively optimize the selection and updating of map points on
a per-voxel basis. These synergistic strategies enable the efficient retrieval
of noise-resilient map points with the highest observation likelihood in
current frames, thereby ensureing the state estimation accuracy. Extensive
evaluations on three public benchmarks demonstrate that our Voxel-ESVIO
outperforms state-of-the-art methods in both accuracy and computational
efficiency.

</details>


### [496] [Minimizing Acoustic Noise: Enhancing Quiet Locomotion for Quadruped Robots in Indoor Applications](https://arxiv.org/abs/2506.23114)
*Zhanxiang Cao,Buqing Nie,Yang Zhang,Yue Gao*

Main category: cs.RO

TL;DR: This paper proposes motion control algorithms to reduce locomotion noise in quadruped robots for noise-sensitive indoor environments, achieving an average 8 dBA reduction.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked problem of noise pollution caused by quadruped robots in noise-sensitive indoor environments, such as service and healthcare settings.

Method: The study integrates optimized gait design with tailored control strategies to minimize noise emissions during robot locomotion.

Result: Experimental findings indicate that the proposed system reduces noise emissions by an average of 8 dBA during robot movement in indoor settings.

Conclusion: The proposed approach enhances quadruped robots' capability for silent operation, boosting their applicability for noise-sensitive indoor settings.

Abstract: Recent advancements in quadruped robot research have significantly improved
their ability to traverse complex and unstructured outdoor environments.
However, the issue of noise generated during locomotion is generally
overlooked, which is critically important in noise-sensitive indoor
environments, such as service and healthcare settings, where maintaining low
noise levels is essential. This study aims to optimize the acoustic noise
generated by quadruped robots during locomotion through the development of
advanced motion control algorithms. To achieve this, we propose a novel
approach that minimizes noise emissions by integrating optimized gait design
with tailored control strategies. This method achieves an average noise
reduction of approximately 8 dBA during movement, thereby enhancing the
suitability of quadruped robots for deployment in noise-sensitive indoor
environments. Experimental results demonstrate the effectiveness of this
approach across various indoor settings, highlighting the potential of
quadruped robots for quiet operation in noise-sensitive environments.

</details>


### [497] [Learning Motion Skills with Adaptive Assistive Curriculum Force in Humanoid Robots](https://arxiv.org/abs/2506.23125)
*Zhanxiang Cao,Yang Zhang,Buqing Nie,Huangxuan Lin,Haoyang Li,Yue Gao*

Main category: cs.RO

TL;DR: The paper proposes A2CF (Adaptive Assistive Curriculum Force), a dual-agent method to train humanoid robots for complex tasks by using assistive forces that are reduced as proficiency improves.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of learning policies for complex humanoid tasks by drawing inspiration from how humans rely on external support to learn skills.

Method: A2CF involves training a dual-agent system where one agent applies adaptive assistive forces to guide a humanoid robot initially and decreases the support as skills improve.

Result: The approach accelerates convergence by 30%, reduces failure rates by over 40%, and produces robust, support-free policies in tasks such as walking, dancing, and performing backflips.

Conclusion: The adaptive use of assistive forces enables humanoid robots to learn complex skills faster and more effectively both in simulations and real-world experiments.

Abstract: Learning policies for complex humanoid tasks remains both challenging and
compelling. Inspired by how infants and athletes rely on external support--such
as parental walkers or coach-applied guidance--to acquire skills like walking,
dancing, and performing acrobatic flips, we propose A2CF: Adaptive Assistive
Curriculum Force for humanoid motion learning. A2CF trains a dual-agent system,
in which a dedicated assistive force agent applies state-dependent forces to
guide the robot through difficult initial motions and gradually reduces
assistance as the robot's proficiency improves. Across three
benchmarks--bipedal walking, choreographed dancing, and backflip--A2CF achieves
convergence 30% faster than baseline methods, lowers failure rates by over 40%,
and ultimately produces robust, support-free policies. Real-world experiments
further demonstrate that adaptively applied assistive forces significantly
accelerate the acquisition of complex skills in high-dimensional robotic
control.

</details>


### [498] [ParticleFormer: A 3D Point Cloud World Model for Multi-Object, Multi-Material Robotic Manipulation](https://arxiv.org/abs/2506.23126)
*Suning Huang,Qianzhong Chen,Xiaohan Zhang,Jiankai Sun,Mac Schwager*

Main category: cs.RO

TL;DR: ParticleFormer introduces a Transformer-based 3D world model to predict dynamics in multi-material, multi-object robotic interactions, outperforming leading baselines across scenarios.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in existing 3D world models, such as single-material dynamics modeling and dependence on complex 3D scene reconstruction for training, thereby enabling better robotic manipulation in diverse environments.

Method: Proposed ParticleFormer, a Transformer-based model trained with a hybrid point cloud reconstruction loss to capture both global and local dynamics features. It trains directly from real-world robot perception data without requiring elaborate scene reconstruction.

Result: ParticleFormer achieves higher accuracy in dynamics prediction and reduced rollout error in visuomotor tasks, validated through six simulation and three real-world experiments.

Conclusion: ParticleFormer offers a robust and efficient solution for modeling robotic dynamics in complex, multi-material scenarios, pushing forward state-of-the-art performance in manipulation tasks.

Abstract: 3D world models (i.e., learning-based 3D dynamics models) offer a promising
approach to generalizable robotic manipulation by capturing the underlying
physics of environment evolution conditioned on robot actions. However,
existing 3D world models are primarily limited to single-material dynamics
using a particle-based Graph Neural Network model, and often require
time-consuming 3D scene reconstruction to obtain 3D particle tracks for
training. In this work, we present ParticleFormer, a Transformer-based point
cloud world model trained with a hybrid point cloud reconstruction loss,
supervising both global and local dynamics features in multi-material,
multi-object robot interactions. ParticleFormer captures fine-grained
multi-object interactions between rigid, deformable, and flexible materials,
trained directly from real-world robot perception data without an elaborate
scene reconstruction. We demonstrate the model's effectiveness both in 3D scene
forecasting tasks, and in downstream manipulation tasks using a Model
Predictive Control (MPC) policy. In addition, we extend existing dynamics
learning benchmarks to include diverse multi-material, multi-object interaction
scenarios. We validate our method on six simulation and three real-world
experiments, where it consistently outperforms leading baselines by achieving
superior dynamics prediction accuracy and less rollout error in downstream
visuomotor tasks. Experimental videos are available at
https://particleformer.github.io/.

</details>


### [499] [Flatness-based Finite-Horizon Multi-UAV Formation Trajectory Planning and Directionally Aware Collision Avoidance Tracking](https://arxiv.org/abs/2506.23129)
*Hossein B. Jond,Logan Beaver,Martin Jiroušek,Naiemeh Ahmadlou,Veli Bakırcıoğlu,Martin Saska*

Main category: cs.RO

TL;DR: This paper proposes a novel finite-time formation control scheme for UAV teams that eliminates reliance on numerical methods by leveraging differential flatness for collision-free optimal control.


<details>
  <summary>Details</summary>
Motivation: Managing collision-free and optimal formation control for UAV teams is currently challenging due to reliance on sensitive numerical methods in existing approaches.

Method: The proposed approach formulates a finite-time optimal control problem solved via Pontryagin's principle and incorporates a collision-avoidance strategy that prioritizes UAVs in forward paths while deprioritizing those with minimal collision risks.

Result: Simulation tests on a four-UAV team (re)formation problem demonstrate the effectiveness of the proposed control framework.

Conclusion: This framework provides an efficient and reliable solution for UAV trajectory formation and collision avoidance without using computationally sensitive numerical methods.

Abstract: Collision-free optimal formation control of unmanned aerial vehicle (UAV)
teams is challenging. The state-of-the-art optimal control approaches often
rely on numerical methods sensitive to initial guesses. This paper presents an
innovative collision-free finite-time formation control scheme for multiple
UAVs leveraging the differential flatness of the UAV dynamics, eliminating the
need for numerical methods. We formulate a finite-time optimal control problem
to plan a formation trajectory for feasible initial states. This formation
trajectory planning optimal control problem involves a collective performance
index to meet the formation requirements of achieving relative positions and
velocity consensus. It is solved by applying Pontryagin's principle.
Subsequently, a collision-constrained regulating problem is addressed to ensure
collision-free tracking of the planned formation trajectory. The tracking
problem incorporates a directionally aware collision avoidance strategy that
prioritizes avoiding UAVs in the forward path and relative approach. It assigns
lower priority to those on the sides with an oblique relative approach and
disregards UAVs behind and not in the relative approach. The simulation results
for a four-UAV team (re)formation problem confirm the efficacy of the proposed
control scheme.

</details>


### [500] [DexH2R: A Benchmark for Dynamic Dexterous Grasping in Human-to-Robot Handover](https://arxiv.org/abs/2506.23152)
*Youzhuo Wang,Jiayi Ye,Chuyang Xiao,Yiming Zhong,Heng Tao,Hang Yu,Yumeng Liu,Jingyi Yu,Yuexin Ma*

Main category: cs.RO

TL;DR: This paper introduces DexH2R, a real-world dataset for human-to-robot handovers using a dexterous robotic hand, addressing the limitations of synthetic and static datasets.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle the challenge of effective dynamic dexterous grasping in human-robot collaboration, hindered by the lack of realistic human-to-robot handover datasets.

Method: The authors developed the DexH2R dataset with teleoperation techniques for natural movement data and proposed DynamicGrasp for evaluating dynamic handovers, alongside a comparison of existing approaches.

Result: The dataset includes diverse objects, natural dynamic motion data, rich sensors, and effective annotations, along with performance analysis of multiple state-of-the-art methods for dynamic handovers.

Conclusion: This benchmark dataset and proposed solution are positioned to facilitate progress in human-robot handover research by providing realistic datasets and robust evaluation metrics.

Abstract: Handover between a human and a dexterous robotic hand is a fundamental yet
challenging task in human-robot collaboration. It requires handling dynamic
environments and a wide variety of objects and demands robust and adaptive
grasping strategies. However, progress in developing effective dynamic
dexterous grasping methods is limited by the absence of high-quality,
real-world human-to-robot handover datasets. Existing datasets primarily focus
on grasping static objects or rely on synthesized handover motions, which
differ significantly from real-world robot motion patterns, creating a
substantial gap in applicability. In this paper, we introduce DexH2R, a
comprehensive real-world dataset for human-to-robot handovers, built on a
dexterous robotic hand. Our dataset captures a diverse range of interactive
objects, dynamic motion patterns, rich visual sensor data, and detailed
annotations. Additionally, to ensure natural and human-like dexterous motions,
we utilize teleoperation for data collection, enabling the robot's movements to
align with human behaviors and habits, which is a crucial characteristic for
intelligent humanoid robots. Furthermore, we propose an effective solution,
DynamicGrasp, for human-to-robot handover and evaluate various state-of-the-art
approaches, including auto-regressive models and diffusion policy methods,
providing a thorough comparison and analysis. We believe our benchmark will
drive advancements in human-to-robot handover research by offering a
high-quality dataset, effective solutions, and comprehensive evaluation
metrics.

</details>


### [501] [Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models](https://arxiv.org/abs/2506.23164)
*Maarten Hugenholtz,Anna Meszaros,Jens Kober,Zlatan Ajanovic*

Main category: cs.RO

TL;DR: The paper addresses the issue of mode collapse in multimodal prediction models for autonomous vehicles by proposing a new evaluation framework with novel metrics.


<details>
  <summary>Details</summary>
Motivation: The need to improve safety in autonomous vehicles by addressing mode collapse in multimodal prediction models, which poses risks due to inaccurate or narrow prediction of potential human interactions.

Method: The authors propose a novel evaluation framework that introduces metrics for mode collapse, correctness, and coverage, analyzing the sequential dimension of multi-agent trajectory predictions.

Result: Their analysis of four multi-agent trajectory prediction models reveals that mode collapse occurs, particularly in predicting the correct interaction mode just before these interactions become inevitable.

Conclusion: The proposed framework and metrics provide a new tool to evaluate and improve the safety and accuracy of prediction models, advancing the goal of safer autonomous driving systems.

Abstract: Autonomous Vehicle decisions rely on multimodal prediction models that
account for multiple route options and the inherent uncertainty in human
behavior. However, models can suffer from mode collapse, where only the most
likely mode is predicted, posing significant safety risks. While existing
methods employ various strategies to generate diverse predictions, they often
overlook the diversity in interaction modes among agents. Additionally,
traditional metrics for evaluating prediction models are dataset-dependent and
do not evaluate inter-agent interactions quantitatively. To our knowledge, none
of the existing metrics explicitly evaluates mode collapse. In this paper, we
propose a novel evaluation framework that assesses mode collapse in joint
trajectory predictions, focusing on safety-critical interactions. We introduce
metrics for mode collapse, mode correctness, and coverage, emphasizing the
sequential dimension of predictions. By testing four multi-agent trajectory
prediction models, we demonstrate that mode collapse indeed happens. When
looking at the sequential dimension, although prediction accuracy improves
closer to interaction events, there are still cases where the models are unable
to predict the correct interaction mode, even just before the interaction mode
becomes inevitable. We hope that our framework can help researchers gain new
insights and advance the development of more consistent and accurate prediction
models, thus enhancing the safety of autonomous driving systems.

</details>


### [502] [InfGen: Scenario Generation as Next Token Group Prediction](https://arxiv.org/abs/2506.23316)
*Zhenghao Peng,Yuxin Liu,Bolei Zhou*

Main category: cs.RO

TL;DR: The paper introduces InfGen, a framework for generating realistic and adaptable traffic scenarios in an autoregressive manner using transformer models for training autonomous driving systems.


<details>
  <summary>Details</summary>
Motivation: Existing traffic simulation methods have limitations in modeling dynamic, long-horizon, evolving-agent scenarios, prompting a need for more interactive and realistic approaches.

Method: InfGen models traffic using a sequence-to-sequence token representation and a transformer-based architecture, allowing real-time insertion of agents and infinite generation of agent states and trajectories.

Result: InfGen creates diverse and adaptive traffic behaviors, improving the robustness and generalization of reinforcement learning policies trained in its simulated scenarios.

Conclusion: InfGen is validated as a high-fidelity simulation environment for autonomous driving, with superior capabilities over traditional static or log-replay methods.

Abstract: Realistic and interactive traffic simulation is essential for training and
evaluating autonomous driving systems. However, most existing data-driven
simulation methods rely on static initialization or log-replay data, limiting
their ability to model dynamic, long-horizon scenarios with evolving agent
populations. We propose InfGen, a scenario generation framework that outputs
agent states and trajectories in an autoregressive manner. InfGen represents
the entire scene as a sequence of tokens, including traffic light signals,
agent states, and motion vectors, and uses a transformer model to simulate
traffic over time. This design enables InfGen to continuously insert new agents
into traffic, supporting infinite scene generation. Experiments demonstrate
that InfGen produces realistic, diverse, and adaptive traffic behaviors.
Furthermore, reinforcement learning policies trained in InfGen-generated
scenarios achieve superior robustness and generalization, validating its
utility as a high-fidelity simulation environment for autonomous driving. More
information is available at https://metadriverse.github.io/infgen/.

</details>


### [503] [Simplifying Data-Driven Modeling of the Volume-Flow-Pressure Relationship in Hydraulic Soft Robotic Actuators](https://arxiv.org/abs/2506.23326)
*Sang-Yoep Lee,Leonardo Zamora Yanez,Jacob Rogatinsky,Vi T. Vo,Tanvi Shingade,Tommaso Ranzani*

Main category: cs.RO

TL;DR: This study uses data-driven approaches with exponential, polynomial, and neural network models to predict the pressure dynamics in hydraulic soft actuators, highlighting the effectiveness of simpler multivariate polynomial models.


<details>
  <summary>Details</summary>
Motivation: Physics-based models struggle to capture the nonlinear dynamics of soft robotic systems, necessitating exploration of efficient data-driven approaches for modeling pressure-flow relationships.

Method: Regression analyses were performed on a stacked balloon actuator system using exponential, polynomial, and neural network models with and without autoregressive inputs.

Result: Multivariate polynomials were found to be particularly effective, providing accurate predictions with fewer parameters compared to complex models.

Conclusion: The study offers a low-complexity, high-accuracy modeling solution suitable for real-time applications in soft robotics, with broader benefits for techniques requiring explicit analytical models.

Abstract: Soft robotic systems are known for their flexibility and adaptability, but
traditional physics-based models struggle to capture their complex, nonlinear
behaviors. This study explores a data-driven approach to modeling the
volume-flow-pressure relationship in hydraulic soft actuators, focusing on
low-complexity models with high accuracy. We perform regression analysis on a
stacked balloon actuator system using exponential, polynomial, and neural
network models with or without autoregressive inputs. The results demonstrate
that simpler models, particularly multivariate polynomials, effectively predict
pressure dynamics with fewer parameters. This research offers a practical
solution for real-time soft robotics applications, balancing model complexity
and computational efficiency. Moreover, the approach may benefit various
techniques that require explicit analytical models.

</details>


### [504] [Moving Matter: Using a Single, Simple Robot to Reconfigure a Connected Set of Building Blocks](https://arxiv.org/abs/2506.23333)
*Javier Garcia,Jonas Friemel,Ramin Kosfeld,Michael Yannuzzi,Peter Kramer,Christian Rieck,Christian Scheffer,Arne Schmidt,Harm Kube,Dan Biediger,Sándor P. Fekete,Aaron T. Becker*

Main category: cs.RO

TL;DR: This paper evaluates methods for reconfiguring connected tile shapes using a single robot, implementing a recent algorithm and comparing it with existing heuristics.


<details>
  <summary>Details</summary>
Motivation: To optimize and evaluate methods for reconfiguration of tile structures using a single robot in scenarios requiring connectivity and efficiency.

Method: The authors implemented the histogram-based algorithm proposed by Becker et al. (CCCG 2025) along with two heuristic algorithms, tested in both simulated and real-world settings using an inchworm-type robot.

Result: The work provides a comparative analysis of the histogram-based algorithm against heuristic approaches, showcasing its performance in simulation and practical scenarios.

Conclusion: Reconfiguration performance can be improved using the histogram-based algorithm, especially under constraints of maintaining connectivity and well-separated start/target configurations.

Abstract: We implement and evaluate different methods for the reconfiguration of a
connected arrangement of tiles into a desired target shape, using a single
active robot that can move along the tile structure. This robot can pick up,
carry, or drop off one tile at a time, but it must maintain a single connected
configuration at all times.
  Becker et al. (CCCG 2025) recently proposed an algorithm that uses histograms
as canonical intermediate configurations, guaranteeing performance within a
constant factor of the optimal solution if the start and target configuration
are well-separated. We implement and evaluate this algorithm, both in a
simulated and practical setting, using an inchworm type robot to compare it
with two existing heuristic algorithms.

</details>


### [505] [Safe and Performant Deployment of Autonomous Systems via Model Predictive Control and Hamilton-Jacobi Reachability Analysis](https://arxiv.org/abs/2506.23346)
*Hao Wang,Armand Jordana,Ludovic Righetti,Somil Bansal*

Main category: cs.RO

TL;DR: This paper introduces a framework combining model predictive control (MPC) and Hamilton-Jacobi (HJ) reachability to optimize autonomous systems' task performance while adhering to safety constraints, demonstrating effectiveness in simulations.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in achieving both task performance and safety for autonomous systems, as existing methods often fail to balance these aspects.

Method: The paper employs a hybrid framework combining MPC for task optimization and HJ reachability for safety assurance, ensuring recursive feasibility and scalability for high-dimensional systems.

Result: Simulation tests using a 4D Dubins Car and a 6 Degree of Freedom (Dof) Kuka iiwa manipulator showed substantial improvements in safety constraint satisfaction compared to baseline approaches.

Conclusion: The proposed framework successfully balances task optimization and safety adherence for autonomous systems, offering scalability and enhanced safety performance in simulations.

Abstract: While we have made significant algorithmic developments to enable autonomous
systems to perform sophisticated tasks, it remains difficult for them to
perform tasks effective and safely. Most existing approaches either fail to
provide any safety assurances or substantially compromise task performance for
safety. In this work, we develop a framework, based on model predictive control
(MPC) and Hamilton-Jacobi (HJ) reachability, to optimize task performance for
autonomous systems while respecting the safety constraints. Our framework
guarantees recursive feasibility for the MPC controller, and it is scalable to
high-dimensional systems. We demonstrate the effectiveness of our framework
with two simulation studies using a 4D Dubins Car and a 6 Dof Kuka iiwa
manipulator, and the experiments show that our framework significantly improves
the safety constraints satisfaction of the systems over the baselines.

</details>


### [506] [Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop](https://arxiv.org/abs/2506.23351)
*Tianxing Chen,Kaixuan Wang,Zhaohui Yang,Yuhao Zhang,Zanxin Chen,Baijun Chen,Wanxi Dong,Ziyuan Liu,Dong Chen,Tianshuo Yang,Haibao Yu,Xiaokang Yang,Yusen Qin,Zhiqiang Xie,Yao Mu,Ping Luo,Tian Nian,Weiliang Deng,Yiheng Ge,Yibin Liu,Zixuan Li,Dehui Wang,Zhixuan Liang,Haohui Xie,Rijie Zeng,Yunfei Ge,Peiqing Cong,Guannan He,Zhaoming Han,Ruocheng Yin,Jingxiang Guo,Lunkai Lin,Tianling Xu,Hongzhe Bi,Xuewu Lin,Tianwei Lin,Shujie Luo,Keyu Li,Ziyan Zhao,Ke Fan,Heyang Xu,Bo Peng,Wenlong Gao,Dongjiang Li,Feng Jin,Hui Shen,Jinming Li,Chaowei Cui,Yuchen,Yaxin Peng,Lingdong Zeng,Wenlong Dong,Tengfei Li,Weijie Ke,Jun Chen,Erdemt Bao,Tian Lan,Tenglong Liu,Jin Yang,Huiping Zhuang,Baozhi Jia,Shuai Zhang,Zhengfeng Zou,Fangheng Guan,Tianyi Jia,Ke Zhou,Hongjiu Zhang,Yating Han,Cheng Fang,Yixian Zou,Chongyang Xu,Qinglun Zhang,Shen Cheng,Xiaohe Wang,Ping Tan,Haoqiang Fan,Shuaicheng Liu,Jiaheng Chen,Chuxuan Huang,Chengliang Lin,Kaijun Luo,Boyu Yue,Yi Liu,Jinyu Chen,Zichang Tan,Liming Deng,Shuo Xu,Zijian Cai,Shilong Yin,Hao Wang,Hongshan Liu,Tianyang Li,Long Shi,Ran Xu,Huilin Xu,Zhengquan Zhang,Congsheng Xu,Jinchang Yang,Feng Xu*

Main category: cs.RO

TL;DR: The paper discusses the RoboTwin Dual-Arm Collaboration Challenge, a competition focused on advancing collaborative dual-arm manipulation using novel solutions and platforms.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of single-arm systems and promote advancements in collaborative dual-arm manipulation for tasks requiring higher complexity, such as handling rigid, deformable, and tactile-sensitive objects.

Method: The challenge included three competitive stages (two simulation rounds and one real-world round) where 64 global teams tackled a series of 17 dual-arm manipulation tasks using platforms like RoboTwin Simulation and AgileX COBOT-Magic Robot.

Result: Participants developed high-performing solutions like SEM and AnchorDP3, offering valuable insights into generalizable bimanual policy learning, and contributed to progress in the field.

Conclusion: Insights and results gained from this challenge aim to support further research and development of robust and transferable bimanual robot manipulation policies.

Abstract: Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in
robotics, driven by the need for autonomous systems that can perceive, reason,
and act in complex physical environments. While single-arm systems have shown
strong task performance, collaborative dual-arm systems are essential for
handling more intricate tasks involving rigid, deformable, and
tactile-sensitive objects. To advance this goal, we launched the RoboTwin
Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on
the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot
platform, the competition consisted of three stages: Simulation Round 1,
Simulation Round 2, and a final Real-World Round. Participants totally tackled
17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based
scenarios. The challenge attracted 64 global teams and over 400 participants,
producing top-performing solutions like SEM and AnchorDP3 and generating
valuable insights into generalizable bimanual policy learning. This report
outlines the competition setup, task design, evaluation methodology, key
findings and future direction, aiming to support future research on robust and
generalizable bimanual manipulation policies. The Challenge Webpage is
available at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.

</details>


### [507] [GS-NBV: a Geometry-based, Semantics-aware Viewpoint Planning Algorithm for Avocado Harvesting under Occlusions](https://arxiv.org/abs/2506.23369)
*Xiao'ao Song,Konstantinos Karydis*

Main category: cs.RO

TL;DR: The paper tackles automated avocado harvesting challenges by presenting a geometry-based, semantics-aware viewpoint-planning algorithm. It uses occlusion detection and a picking score metric to optimize viewpoints, achieving remarkable success rates in simulations.


<details>
  <summary>Details</summary>
Motivation: Automated harvesting of fruits like avocados faces challenges due to their irregular shape, substantial weight, and complex growing environments, necessitating innovative approaches to identify effective picking points.

Method: The proposed method involves detecting partially occluded avocados, constraining viewpoint searches to a 1D circle using geometric data, sampling four points, and introducing a picking score for selecting efficient viewpoints.

Result: The algorithm demonstrated a 100% success rate in simulations with heavy occlusions, outperforming two state-of-the-art methods in efficiency and robustness.

Conclusion: The study's geometry-based and semantics-aware viewpoint-planning strategy is effective for improving automated harvesting processes in unstructured and occluded environments, validated by its impressive simulation results.

Abstract: Efficient identification of picking points is critical for automated fruit
harvesting. Avocados present unique challenges owing to their irregular shape,
weight, and less-structured growing environments, which require specific
viewpoints for successful harvesting. We propose a geometry-based,
semantics-aware viewpoint-planning algorithm to address these challenges. The
planning process involves three key steps: viewpoint sampling, evaluation, and
execution. Starting from a partially occluded view, the system first detects
the fruit, then leverages geometric information to constrain the viewpoint
search space to a 1D circle, and uniformly samples four points to balance the
efficiency and exploration. A new picking score metric is introduced to
evaluate the viewpoint suitability and guide the camera to the next-best view.
We validate our method through simulation against two state-of-the-art
algorithms. Results show a 100% success rate in two case studies with
significant occlusions, demonstrating the efficiency and robustness of our
approach. Our code is available at https://github.com/lineojcd/GSNBV

</details>


### [508] [A Model Predictive Control Framework to Enhance Safety and Quality in Mobile Additive Manufacturing Systems](https://arxiv.org/abs/2506.23400)
*Yifei Li,Joshua A. Robbins,Guha Manogharan,Herschel C. Pangborn,Ilya Kovalenko*

Main category: cs.RO

TL;DR: This paper focuses on improving manufacturing with mobile robots integrated with Additive Manufacturing (AM) to enhance flexibility, reduce lead times, and achieve better print quality.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the growing demand for flexible and on-demand manufacturing, where traditional AM falls short in scalability, lead times, and print precision.

Method: The authors propose a model predictive control framework for mobile AM platforms, providing safe navigation and ensuring high print quality in dynamic production environments.

Result: Three case studies were conducted to test the system's feasibility and reliability.

Conclusion: Integrating mobile robots with AM systems can tackle challenges in modern manufacturing by improving flexibility, safety, and print precision in a dynamic production setting.

Abstract: In recent years, the demand for customized, on-demand production has grown in
the manufacturing sector. Additive Manufacturing (AM) has emerged as a
promising technology to enhance customization capabilities, enabling greater
flexibility, reduced lead times, and more efficient material usage. However,
traditional AM systems remain constrained by static setups and human worker
dependencies, resulting in long lead times and limited scalability. Mobile
robots can improve the flexibility of production systems by transporting
products to designated locations in a dynamic environment. By integrating AM
systems with mobile robots, manufacturers can optimize travel time for
preparatory tasks and distributed printing operations. Mobile AM robots have
been deployed for on-site production of large-scale structures, but often
neglect critical print quality metrics like surface roughness. Additionally,
these systems do not have the precision necessary for producing small,
intricate components. We propose a model predictive control framework for a
mobile AM platform that ensures safe navigation on the plant floor while
maintaining high print quality in a dynamic environment. Three case studies are
used to test the feasibility and reliability of the proposed systems.

</details>


### [509] [Risk-Based Filtering of Valuable Driving Situations in the Waymo Open Motion Dataset](https://arxiv.org/abs/2506.23433)
*Tim Puphal,Vipul Ramtekkar,Kenji Nishimiya*

Main category: cs.RO

TL;DR: The paper proposes a risk-based filtering method to identify valuable driving situations in large datasets, leveraging a probabilistic risk model to consider direct and indirect vehicle interactions.


<details>
  <summary>Details</summary>
Motivation: Enhance the improvement of automated vehicle software by utilizing driving data with significant road user interactions.

Method: A probabilistic risk model is employed to detect high-risk situations, accounting for both first-order and second-order interactions between vehicles.

Result: Experiments show effectiveness of the approach in identifying complex driving situations, outperforming baseline interaction metrics with enriched data quality.

Conclusion: The filtering method proves to be a powerful tool for better automated vehicle testing, and the risk data is made publicly available.

Abstract: Improving automated vehicle software requires driving data rich in valuable
road user interactions. In this paper, we propose a risk-based filtering
approach that helps identify such valuable driving situations from large
datasets. Specifically, we use a probabilistic risk model to detect high-risk
situations. Our method stands out by considering a) first-order situations
(where one vehicle directly influences another and induces risk) and b)
second-order situations (where influence propagates through an intermediary
vehicle). In experiments, we show that our approach effectively selects
valuable driving situations in the Waymo Open Motion Dataset. Compared to the
two baseline interaction metrics of Kalman difficulty and Tracks-To-Predict
(TTP), our filtering approach identifies complex and complementary situations,
enriching the quality in automated vehicle testing. The risk data is made
open-source: https://github.com/HRI-EU/RiskBasedFiltering.

</details>


### [510] [MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments](https://arxiv.org/abs/2506.23514)
*Sai Krishna Ghanta,Ramviyas Parasuraman*

Main category: cs.RO

TL;DR: This paper presents MGPRL, a novel distributed framework for multi-robot relative localization using Wi-Fi RSSI signals and convex hull-based estimation, providing high accuracy and computational efficiency, suitable for GPS-denied and resource-limited environments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing multi-robot relative localization systems, which rely on costly, short-range sensors like cameras or LiDARs, leading to high computational overhead and challenges in disjoint environments.

Method: The authors develop MGPRL, leveraging co-regionalized multi-output Gaussian Processes for RSSI field prediction, uncertainty-aware multi-AP localization, and convex hull-based alignment for relative pose estimation. This process uses Wi-Fi RSSI signals and requires no pre-calibration or offline fingerprinting.

Result: MGPRL was evaluated in simulations and real-world experiments against state-of-the-art methods, demonstrating better localization accuracy and computational efficiency.

Conclusion: MGPRL offers a computationally efficient and accurate solution for multi-robot relative localization in GPS-denied environments using ubiquitous Wi-Fi RSSI signals. The system is released as an open-source ROS package.

Abstract: Relative localization is a crucial capability for multi-robot systems
operating in GPS-denied environments. Existing approaches for multi-robot
relative localization often depend on costly or short-range sensors like
cameras and LiDARs. Consequently, these approaches face challenges such as high
computational overhead (e.g., map merging) and difficulties in disjoint
environments. To address this limitation, this paper introduces MGPRL, a novel
distributed framework for multi-robot relative localization using convex-hull
of multiple Wi-Fi access points (AP). To accomplish this, we employ
co-regionalized multi-output Gaussian Processes for efficient Radio Signal
Strength Indicator (RSSI) field prediction and perform uncertainty-aware
multi-AP localization, which is further coupled with weighted convex hull-based
alignment for robust relative pose estimation. Each robot predicts the RSSI
field of the environment by an online scan of APs in its environment, which are
utilized for position estimation of multiple APs. To perform relative
localization, each robot aligns the convex hull of its predicted AP locations
with that of the neighbor robots. This approach is well-suited for devices with
limited computational resources and operates solely on widely available Wi-Fi
RSSI measurements without necessitating any dedicated pre-calibration or
offline fingerprinting. We rigorously evaluate the performance of the proposed
MGPRL in ROS simulations and demonstrate it with real-world experiments,
comparing it against multiple state-of-the-art approaches. The results showcase
that MGPRL outperforms existing methods in terms of localization accuracy and
computational efficiency. Finally, we open source MGPRL as a ROS package
https://github.com/herolab-uga/MGPRL.

</details>


### [511] [Online Human Action Detection during Escorting](https://arxiv.org/abs/2506.23573)
*Siddhartha Mondal,Avik Mitra,Chayan Sarkar*

Main category: cs.RO

TL;DR: The paper introduces a novel neural network architecture to improve robotic escorting services in crowded indoor environments, enabling real-time person re-identification and human action prediction.


<details>
  <summary>Details</summary>
Motivation: Current robotic escorting systems struggle in crowded environments due to limited understanding of human movement dynamics, lacking abilities to detect and interpret human actions or disruptions effectively.

Method: A new neural network architecture is proposed that combines person re-identification with action prediction, enabling real-time adjustments to escorting robots' behavior.

Result: Comparative evaluations against baseline systems show that the proposed model is more efficient and effective in handling escorting tasks in complex environments.

Conclusion: The system demonstrates significant potential for improving robotic escorting services in real-world crowded indoor settings.

Abstract: The deployment of robot assistants in large indoor spaces has seen
significant growth, with escorting tasks becoming a key application. However,
most current escorting robots primarily rely on navigation-focused strategies,
assuming that the person being escorted will follow without issue. In crowded
environments, this assumption often falls short, as individuals may struggle to
keep pace, become obstructed, get distracted, or need to stop unexpectedly. As
a result, conventional robotic systems are often unable to provide effective
escorting services due to their limited understanding of human movement
dynamics. To address these challenges, an effective escorting robot must
continuously detect and interpret human actions during the escorting process
and adjust its movement accordingly. However, there is currently no existing
dataset designed specifically for human action detection in the context of
escorting. Given that escorting often occurs in crowded environments, where
other individuals may enter the robot's camera view, the robot also needs to
identify the specific human it is escorting (the subject) before predicting
their actions. Since no existing model performs both person re-identification
and action prediction in real-time, we propose a novel neural network
architecture that can accomplish both tasks. This enables the robot to adjust
its speed dynamically based on the escortee's movements and seamlessly resume
escorting after any disruption. In comparative evaluations against strong
baselines, our system demonstrates superior efficiency and effectiveness,
showcasing its potential to significantly improve robotic escorting services in
complex, real-world scenarios.

</details>


### [512] [Passage-traversing optimal path planning with sampling-based algorithms](https://arxiv.org/abs/2506.23614)
*Jing Huang,Hao Su,Kwok Wai Samuel Au*

Main category: cs.RO

TL;DR: This paper introduces passage-traversing optimal path planning (PTOPP) which optimizes paths' traversal through constrained passages using proximity graph-based methods.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to address the limitations of existing clearance-based path planning methods by optimizing the accessible free space along the entire path for better efficiency and configurability.

Method: A novel proximity graph-based method is proposed for detecting sparse but informative passages, followed by environment decomposition. These techniques are integrated into sampling-based algorithms for PTOPP to enable efficient path planning.

Result: The proposed PTOPP approach demonstrates superior performance in terms of configurability, solution optimality, and efficiency when compared to traditional clearance-based methods.

Conclusion: PTOPP provides a robust, versatile, and efficient solution to optimize the accessible free space, offering improvements over conventional path planning approaches and addressing broader path planning problems.

Abstract: This paper introduces a new paradigm of optimal path planning, i.e.,
passage-traversing optimal path planning (PTOPP), that optimizes paths'
traversed passages for specified optimization objectives. In particular, PTOPP
is utilized to find the path with optimal accessible free space along its
entire length, which represents a basic requirement for paths in robotics. As
passages are places where free space shrinks and becomes constrained, the core
idea is to leverage the path's passage traversal status to characterize its
accessible free space comprehensively. To this end, a novel passage detection
and free space decomposition method using proximity graphs is proposed,
enabling fast detection of sparse but informative passages and environment
decompositions. Based on this preprocessing, optimal path planning with
accessible free space objectives or constraints is formulated as PTOPP problems
compatible with sampling-based optimal planners. Then, sampling-based
algorithms for PTOPP, including their dependent primitive procedures, are
developed leveraging partitioned environments for fast passage traversal check.
All these methods are implemented and thoroughly tested for effectiveness and
efficiency validation. Compared to existing approaches, such as clearance-based
methods, PTOPP demonstrates significant advantages in configurability, solution
optimality, and efficiency, addressing prior limitations and incapabilities. It
is believed to provide an efficient and versatile solution to accessible free
space optimization over conventional avenues and more generally, to a broad
class of path planning problems that can be formulated as PTOPP.

</details>


### [513] [Towards Universal Shared Control in Teleoperation Without Haptic Feedback](https://arxiv.org/abs/2506.23624)
*Max Grobbel,Tristan Schneider,Sören Hohmann*

Main category: cs.RO

TL;DR: This study proposes a solution for VR teleoperation without haptic feedback by optimizing user input for collision-free robot trajectories, achieving low-latency control while suppressing liquid slosh.


<details>
  <summary>Details</summary>
Motivation: Non-haptic VR controllers lack essential motion feedback for teleoperating, making certain operations challenging, such as handling liquids.

Method: The paper integrates multi-objective optimization into the user-controller input to convert it into collision-free joint trajectories for a UR5e robotic arm while minimizing liquid motion within a glass.

Result: The system achieved an average planning latency of 13 milliseconds, ensuring real-time operational capability.

Conclusion: The proposed approach effectively resolves motion feedback limitation in teleoperation and demonstrates potential for augmenting to additional control objectives beyond liquid slosh suppression.

Abstract: Teleoperation with non-haptic VR controllers deprives human operators of
critical motion feedback. We address this by embedding a multi-objective
optimization problem that converts user input into collision-free UR5e joint
trajectories while actively suppressing liquid slosh in a glass. The controller
maintains 13 ms average planning latency, confirming real-time performance and
motivating the augmentation of this teleoperation approach to further
objectives.

</details>


### [514] [A comprehensive control architecture for semi-autonomous dual-arm robots in agriculture settings](https://arxiv.org/abs/2506.23723)
*Jozsef Palmieri,Paolo Di Lillo,Stefano Chiaverini,Alessandro Marino*

Main category: cs.RO

TL;DR: The paper introduces a control architecture for a dual-arm mobile robot used in autonomous and semi-autonomous grape harvesting in vineyards under the CANOPIES project.


<details>
  <summary>Details</summary>
Motivation: The need for robotic systems designed for complex agricultural tasks that integrate efficient perception and control mechanisms.

Method: The method involves using a 16-DOF dual-arm mobile robot with a Hierarchical Quadratic Programming (HQP) approach to prioritize and execute tasks, including managing uncertainties and interaction forces.

Result: The robotic system was validated through extensive lab testing and real-world vineyard trials, demonstrating successful autonomous and semi-autonomous grape harvesting.

Conclusion: The integration of HQP for task management and interaction forces shows promise for advancing agricultural robotics, particularly in complex settings like vineyards, blending human assistance with automation.

Abstract: The adoption of mobile robotic platforms in complex environments, such as
agricultural settings, requires these systems to exhibit a flexible yet
effective architecture that integrates perception and control. In such
scenarios, several tasks need to be accomplished simultaneously, ranging from
managing robot limits to performing operational tasks and handling human
inputs. The purpose of this paper is to present a comprehensive control
architecture for achieving complex tasks such as robotized harvesting in
vineyards within the framework of the European project CANOPIES. In detail, a
16-DOF dual-arm mobile robot is employed, controlled via a Hierarchical
Quadratic Programming (HQP) approach capable of handling both equality and
inequality constraints at various priorities to harvest grape bunches selected
by the perception system developed within the project. Furthermore, given the
complexity of the scenario and the uncertainty in the perception system, which
could potentially lead to collisions with the environment, the handling of
interaction forces is necessary. Remarkably, this was achieved using the same
HQP framework. This feature is further leveraged to enable semi-autonomous
operations, allowing a human operator to assist the robotic counterpart in
completing harvesting tasks. Finally, the obtained results are validated
through extensive testing conducted first in a laboratory environment to prove
individual functionalities, then in a real vineyard, encompassing both
autonomous and semi-autonomous grape harvesting operations.

</details>


### [515] [PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?](https://arxiv.org/abs/2506.23725)
*Atharva Gundawar,Som Sagar,Ransalu Senanayake*

Main category: cs.RO

TL;DR: The paper introduces PAC Bench, a benchmark to evaluate Vision-Language Models (VLMs) on their understanding of physical concepts critical for robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Current Vision-Language Models (VLMs) frequently lack a deep understanding of intrinsic physical properties, action affordances, and constraints, which are prerequisites for effective robot manipulation tasks.

Method: The authors developed PAC Bench, a benchmark consisting of over 30,000 annotations, including real-world images, humanoid-view scenarios, and simulated constraint scenarios across four tasks. It is designed to systematically evaluate the physical understanding of VLMs.

Result: Performance evaluations of existing VLMs on PAC Bench reveal significant gaps in their ability to understand physical concepts, indicating their current limitations in robotic applications.

Conclusion: PAC Bench offers a standardized and comprehensive framework for evaluating and improving the physical reasoning capabilities of VLMs, aiming to enhance their suitability for robotic tasks.

Abstract: Vision-Language Models (VLMs) are increasingly pivotal for generalist robot
manipulation, enabling tasks such as physical reasoning, policy generation, and
failure detection. However, their proficiency in these high-level applications
often assumes a deep understanding of low-level physical prerequisites, a
capability that remains largely unverified. For robots to perform actions
reliably, they must comprehend intrinsic object properties (e.g., material,
weight), action affordances (e.g., graspable, stackable), and physical
constraints (e.g., stability, reachability, or an object's state, such as being
closed). Despite the widespread use of VLMs in manipulation tasks, we argue
that off-the-shelf models may lack this granular, physically grounded
understanding, as such prerequisites are often overlooked during training.
  To address this critical gap, we introduce PAC Bench, a comprehensive
benchmark designed to systematically evaluate VLMs on their understanding of
core Properties, Affordances, and Constraints (PAC) from a task executability
perspective. PAC Bench features a diverse dataset with over 30,000 annotations,
comprising 673 real-world images (115 object classes, 15 property types, and 1
to 3 affordances defined per class), 100 real-world humanoid-view scenarios,
and 120 unique simulated constraint scenarios across four tasks.
  Our evaluations reveal significant gaps in the ability of current VLMs to
grasp fundamental physical concepts, highlighting limitations in their
suitability for reliable robot manipulation and pointing to key areas for
targeted research. PAC Bench also serves as a standardized benchmark for
rigorously evaluating physical reasoning in VLMs and guiding the development of
more robust, physically grounded models for robotic applications.
  Project Page: https://pacbench.github.io/

</details>


### [516] [Validation of AI-Based 3D Human Pose Estimation in a Cyber-Physical Environment](https://arxiv.org/abs/2506.23739)
*Lisa Marie Otto,Michael Kaiser,Daniel Seebacher,Steffen Müller*

Main category: cs.RO

TL;DR: The paper explores advanced testing methods for automated vehicles interacting with pedestrians and cyclists by using a cyber-physical test environment and validating human pose estimation techniques.


<details>
  <summary>Details</summary>
Motivation: Improving interaction safety and realism between automated vehicles and vulnerable road users (VRUs) in urban scenarios demands new testing approaches.

Method: A test environment combining a Vehicle-in-the-Loop (ViL) test bench and motion laboratory is used to validate HPE with comparative analysis between real-world and virtual settings, utilizing Unreal Engine 5 for virtual representations.

Result: Human pose estimation in real-world and cyber-physical testing conditions shows alignment for stable motion patterns but faces inaccuracies in dynamic scenarios and complex cyclist postures.

Conclusion: The research refines cyber-physical testing methods for validating AI-based vehicle perception and interaction models in urban environments, highlighting challenges with dynamic movements and occlusions.

Abstract: Ensuring safe and realistic interactions between automated driving systems
and vulnerable road users (VRUs) in urban environments requires advanced
testing methodologies. This paper presents a test environment that combines a
Vehiclein-the-Loop (ViL) test bench with a motion laboratory, demonstrating the
feasibility of cyber-physical (CP) testing of vehicle-pedestrian and
vehicle-cyclist interactions. Building upon previous work focused on pedestrian
localization, we further validate a human pose estimation (HPE) approach
through a comparative analysis of real-world (RW) and virtual representations
of VRUs. The study examines the perception of full-body motion using a
commercial monocular camera-based 3Dskeletal detection AI. The virtual scene is
generated in Unreal Engine 5, where VRUs are animated in real time and
projected onto a screen to stimulate the camera. The proposed stimulation
technique ensures the correct perspective, enabling realistic vehicle
perception. To assess the accuracy and consistency of HPE across RW and CP
domains, we analyze the reliability of detections as well as variations in
movement trajectories and joint estimation stability. The validation includes
dynamic test scenarios where human avatars, both walking and cycling, are
monitored under controlled conditions. Our results show a strong alignment in
HPE between RW and CP test conditions for stable motion patterns, while notable
inaccuracies persist under dynamic movements and occlusions, particularly for
complex cyclist postures. These findings contribute to refining CP testing
approaches for evaluating next-generation AI-based vehicle perception and to
enhancing interaction models of automated vehicles and VRUs in CP environments.

</details>


### [517] [Motion Tracking with Muscles: Predictive Control of a Parametric Musculoskeletal Canine Model](https://arxiv.org/abs/2506.23768)
*Vittorio La Barbera,Steven Bohez,Leonard Hasenclever,Yuval Tassa,John R. Hutchinson*

Main category: cs.RO

TL;DR: The paper presents a novel procedural musculoskeletal model of a dog, validated with EMG data and designed for bridging biomechanics, robotics, and neuroscience.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the need for a robust platform to study muscle actuation and neuromuscular control across biomechanics, robotics, and computational neuroscience.

Method: A dog musculoskeletal model is procedurally generated from 3D muscle meshes, coupled with a motion-capture locomotion task and an enhanced muscle dynamics model for differentiable control.

Result: The model's muscle activation patterns were validated against experimental canine EMG data, showing its ability to replicate realistic neuromuscular behaviors.

Conclusion: The paper provides a valuable tool for interdisciplinary research, with plans to release the model and datasets for further academic investigations.

Abstract: We introduce a novel musculoskeletal model of a dog, procedurally generated
from accurate 3D muscle meshes. Accompanying this model is a motion
capture-based locomotion task compatible with a variety of control algorithms,
as well as an improved muscle dynamics model designed to enhance convergence in
differentiable control frameworks. We validate our approach by comparing
simulated muscle activation patterns with experimentally obtained
electromyography (EMG) data from previous canine locomotion studies. This work
aims to bridge gaps between biomechanics, robotics, and computational
neuroscience, offering a robust platform for researchers investigating muscle
actuation and neuromuscular control.We plan to release the full model along
with the retargeted motion capture clips to facilitate further research and
development.

</details>


### [518] [Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving](https://arxiv.org/abs/2506.23771)
*Guizhe Jin,Zhuoren Li,Bo Leng,Ran Yu,Lu Xiong*

Main category: cs.RO

TL;DR: The paper proposes a multi-timescale hierarchical reinforcement learning (RL) approach for autonomous driving (AD) that increases driving quality in terms of efficiency, safety, and action consistency.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based autonomous driving methods lack a well-structured policy design, leading to either fluctuating driving behavior or the inability to achieve unified optimality of behavior and control.

Method: The paper introduces a hierarchical policy structure where high-level RL policies generate long-timescale motion guidance, and low-level RL policies produce short-timescale control commands. Hybrid actions are used for multimodal behaviors, and a hierarchical safety mechanism is incorporated.

Result: Experimental evaluations in simulation and on real-world datasets show significant improvements in driving efficiency, safety, and action consistency using the proposed approach.

Conclusion: The multi-timescale hierarchical RL framework effectively addresses the limitations of previous methods, providing a unified and safer autonomous driving system.

Abstract: Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)
and shows clear advantages. However, most RL-based AD methods overlook policy
structure design. An RL policy that only outputs short-timescale vehicle
control commands results in fluctuating driving behavior due to fluctuations in
network outputs, while one that only outputs long-timescale driving goals
cannot achieve unified optimality of driving behavior and control. Therefore,
we propose a multi-timescale hierarchical reinforcement learning approach. Our
approach adopts a hierarchical policy structure, where high- and low-level RL
policies are unified-trained to produce long-timescale motion guidance and
short-timescale control commands, respectively. Therein, motion guidance is
explicitly represented by hybrid actions to capture multimodal driving
behaviors on structured road and support incremental low-level extend-state
updates. Additionally, a hierarchical safety mechanism is designed to ensure
multi-timescale safety. Evaluation in simulator-based and HighD dataset-based
highway multi-lane scenarios demonstrates that our approach significantly
improves AD performance, effectively increasing driving efficiency, action
consistency and safety.

</details>


### [519] [Data-Driven Predictive Planning and Control for Aerial 3D Inspection with Back-face Elimination](https://arxiv.org/abs/2506.23781)
*Savvas Papaioannou,Panayiotis Kolios,Christos G. Panayiotou,Marios M. Polycarpou*

Main category: cs.RO

TL;DR: This paper introduces a novel approach that integrates perception, planning, and control into a single predictive framework for automated 3D inspection with drones.


<details>
  <summary>Details</summary>
Motivation: Traditional unmanned aerial system (UAS) inspection techniques treat perception, planning, and control separately and are often limited by short-horizon planning. This paper seeks to address these issues and simplify the application to various drones.

Method: The authors propose a comprehensive data-driven predictive control framework that integrates perception, planning, and control. It uses input-output data rather than known UAS dynamics and incorporates 3D computer graphics' back-face elimination for optimizing inspection trajectories.

Result: The proposed method enables accurate, long-horizon 3D inspection trajectories that can be generated online, potentially outperforming traditional approaches in both versatility and functionality.

Conclusion: This integrated framework not only simplifies the application to off-the-shelf black-box UASs but also enhances the efficiency and accuracy of automated inspections.

Abstract: Automated inspection with Unmanned Aerial Systems (UASs) is a transformative
capability set to revolutionize various application domains. However, this task
is inherently complex, as it demands the seamless integration of perception,
planning, and control which existing approaches often treat separately.
Moreover, it requires accurate long-horizon planning to predict action
sequences, in contrast to many current techniques, which tend to be myopic. To
overcome these limitations, we propose a 3D inspection approach that unifies
perception, planning, and control within a single data-driven predictive
control framework. Unlike traditional methods that rely on known UAS dynamic
models, our approach requires only input-output data, making it easily
applicable to off-the-shelf black-box UASs. Our method incorporates back-face
elimination, a visibility determination technique from 3D computer graphics,
directly into the control loop, thereby enabling the online generation of
accurate, long-horizon 3D inspection trajectories.

</details>


### [520] [World4Omni: A Zero-Shot Framework from Image Generation World Model to Robotic Manipulation](https://arxiv.org/abs/2506.23919)
*Haonan Chen,Bangjun Wang,Jingxiang Guo,Tianrui Zhang,Yiwen Hou,Xuchuan Huang,Chenrui Tie,Lin Shao*

Main category: cs.RO

TL;DR: The paper introduces a framework using a pre-trained multimodal image-generation model to enable general-purpose robotic manipulation with strong performance and no task-specific training.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of data efficiency and generalization in robotic manipulation.

Method: A multimodal image-generation model is used as a world model to provide open-ended future state predictions, guiding policy learning. It is coupled with zero-shot low-level control modules.

Result: The method performs effectively across various manipulation tasks in both simulated and real-world settings without task-specific training, data collection, or fine-tuning.

Conclusion: The proposed framework effectively leverages a pre-trained model for general-purpose robotic manipulation and achieves substantial performance without requiring additional resources.

Abstract: Improving data efficiency and generalization in robotic manipulation remains
a core challenge. We propose a novel framework that leverages a pre-trained
multimodal image-generation model as a world model to guide policy learning. By
exploiting its rich visual-semantic representations and strong generalization
across diverse scenes, the model generates open-ended future state predictions
that inform downstream manipulation. Coupled with zero-shot low-level control
modules, our approach enables general-purpose robotic manipulation without
task-specific training. Experiments in both simulation and real-world
environments demonstrate that our method achieves effective performance across
a wide range of manipulation tasks with no additional data collection or
fine-tuning. Supplementary materials are available on our website:
https://world4omni.github.io/.

</details>


### [521] [Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning](https://arxiv.org/abs/2506.23944)
*Fuhang Kuang,Jiacheng You,Yingdong Hu,Tong Zhang,Chuan Wen,Yang Gao*

Main category: cs.RO

TL;DR: This paper introduces a domain adaptation framework to mitigate proprioception distribution shifts in imitation learning for robotic tasks, improving model robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Imitation learning models in robotics face a performance drop due to the divergence in proprioceptive state distributions between training and deployment (proprioception shift problem).

Method: The authors use Wasserstein distance to measure and minimize the gap between training and deployment distributions of proprioceptive states by adding noise proportionate to this distance.

Result: The proposed method enhances robustness against proprioception shifts and outperforms baselines, including naive proprioception removal, in robotic manipulation experiments.

Conclusion: Bridging the proprioception shift with a domain adaptation framework enables leveraging proprioception effectively while maintaining or improving imitation learning performance.

Abstract: Imitation learning models for robotic tasks typically rely on multi-modal
inputs, such as RGB images, language, and proprioceptive states. While
proprioception is intuitively important for decision-making and obstacle
avoidance, simply incorporating all proprioceptive states leads to a surprising
degradation in imitation learning performance. In this work, we identify the
underlying issue as the proprioception shift problem, where the distributions
of proprioceptive states diverge significantly between training and deployment.
To address this challenge, we propose a domain adaptation framework that
bridges the gap by utilizing rollout data collected during deployment. Using
Wasserstein distance, we quantify the discrepancy between expert and rollout
proprioceptive states and minimize this gap by adding noise to both sets of
states, proportional to the Wasserstein distance. This strategy enhances
robustness against proprioception shifts by aligning the training and
deployment distributions. Experiments on robotic manipulation tasks demonstrate
the efficacy of our method, enabling the imitation policy to leverage
proprioception while mitigating its adverse effects. Our approach outperforms
the naive solution which discards proprioception, and other baselines designed
to address distributional shifts.

</details>


### [522] [Predictive Risk Analysis and Safe Trajectory Planning for Intelligent and Connected Vehicles](https://arxiv.org/abs/2506.23999)
*Zeyu Han,Mengchi Cai,Chaoyi Chen,Qingwen Meng,Guangwei Wang,Ying Liu,Qing Xu,Jianqiang Wang,Keqiang Li*

Main category: cs.RO

TL;DR: This paper introduces a framework for predictive risk analysis and safe trajectory planning in autonomous vehicles.


<details>
  <summary>Details</summary>
Motivation: Existing risk assessment theories in autonomous driving only focus on current information, neglecting future risk prediction.

Method: The authors propose a local risk-aware algorithm to predict future object trajectories, followed by spatiotemporal-discretized predictive risk analysis for safe trajectory generation.

Result: Simulation and vehicle experiments demonstrate the framework's effectiveness and real-time applicability in safe trajectory planning.

Conclusion: The proposed framework provides a promising approach for enhancing safety and real-time implementation in autonomous vehicle trajectory planning.

Abstract: The safe trajectory planning of intelligent and connected vehicles is a key
component in autonomous driving technology. Modeling the environment risk
information by field is a promising and effective approach for safe trajectory
planning. However, existing risk assessment theories only analyze the risk by
current information, ignoring future prediction. This paper proposes a
predictive risk analysis and safe trajectory planning framework for intelligent
and connected vehicles. This framework first predicts future trajectories of
objects by a local risk-aware algorithm, following with a
spatiotemporal-discretised predictive risk analysis using the prediction
results. Then the safe trajectory is generated based on the predictive risk
analysis. Finally, simulation and vehicle experiments confirm the efficacy and
real-time practicability of our approach.

</details>


### [523] [Exploring Accelerated Skill Acquisition via Tandem Training for Colonoscopy](https://arxiv.org/abs/2506.24046)
*Olivia Richards,Keith L. Obstein,Nabil Simaan*

Main category: cs.RO

TL;DR: The study introduces a dual-control colonoscope training system to facilitate hands-on guidance for novice endoscopists, demonstrating initial effectiveness in skill acquisition.


<details>
  <summary>Details</summary>
Motivation: Traditional colonoscopy training methods require significant time and hand-off techniques, hindering skill development. There's a need for tools allowing real-time, in-hand expert guidance for novices.

Method: The paper proposes a tandem training system with a telemanipulated control that allows dual expert-novice control over colonoscope angulation. Control toggles automatically between expert and novice users during procedures.

Result: Preliminary user studies conducted with novice and expert endoscopists showed the device's effectiveness in helping novices acquire colonoscopy skills more efficiently.

Conclusion: The tandem training system can enhance skill acquisition for novices and may enable personalized and responsive teaching methods in colonoscopy training.

Abstract: New endoscopists require a large volume of expert-proctored colonoscopies to
attain minimal competency. Developing multi-fingered, synchronized control of a
colonoscope requires significant time and exposure to the device. Current
training methods inhibit this development by relying on tool hand-off for
expert demonstrations. There is a need for colonoscopy training tools that
enable in-hand expert guidance in real-time. We present a new concept of a
tandem training system that uses a telemanipulated preceptor colonoscope to
guide novice users as they perform a colonoscopy. This system is capable of
dual-control and can automatically toggle between expert and novice control of
a standard colonoscope's angulation control wheels. Preliminary results from a
user study with novice and expert users show the effectiveness of this device
as a skill acquisition tool. We believe that this device has the potential to
accelerate skill acquisition for colonoscopy and, in the future, enable
individualized instruction and responsive teaching through bidirectional
actuation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [524] [Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision](https://arxiv.org/abs/2506.22656)
*Jiangping Huang,Dongming Jin,Weisong Sun,Yang Liu,Zhi Jin*

Main category: cs.SE

TL;DR: A multi-agent framework, KGMAF, is proposed to improve automated requirements development by addressing current gaps in software engineering.


<details>
  <summary>Details</summary>
Motivation: Current automation systems in software engineering neglect the complexities of requirements tasks and focus heavily on code development.

Method: The authors propose KGMAF, a framework consisting of six specialized agents and an artifact pool, defining the operations, knowledge, and design of these components.

Result: The case study demonstrates KGMAF's practical applicability and potential impact in real-world scenarios.

Conclusion: KGMAF could significantly enhance automated requirements development and open new research opportunities, especially in the context of LLMs.

Abstract: This paper envisions a knowledge-guided multi-agent framework named KGMAF for
automated requirements development. KGMAF aims to address gaps in current
automation systems for SE, which prioritize code development and overlook the
complexities of requirements tasks. KGMAF is composed of six specialized agents
and an artifact pool to improve efficiency and accuracy. Specifically, KGMAF
outlines the functionality, actions, and knowledge of each agent and provides
the conceptual design of the artifact pool. Our case study highlights the
potential of KGMAF in real-world scenarios. Finally, we outline several
research opportunities for implementing and enhancing automated requirements
development using multi-agent systems. We believe that KGMAF will play a
pivotal role in shaping the future of automated requirements development in the
era of LLMs.

</details>


### [525] [An LLM-assisted approach to designing software architectures using ADD](https://arxiv.org/abs/2506.22688)
*Humberto Cervantes,Rick Kazman,Yuanfang Cai*

Main category: cs.SE

TL;DR: This study introduces a method for using Large Language Models (LLMs) to assist in software architecture design by integrating them into the Attribute-Driven Design (ADD) framework.


<details>
  <summary>Details</summary>
Motivation: Software architecture design is traditionally dependent on expert judgment, and the paper aims to explore how LLMs can be effectively leveraged to support this intricate process.

Method: The researchers proposed a structured approach where an LLM is guided by an explicit ADD description, an architect persona simulation, and a defined iteration plan to collaboratively produce architecture designs alongside human architects.

Result: Case studies show that the LLM-assisted ADD process produces architecture designs closely resembling established solutions, while partially addressing architectural drivers. The results highlight both the capabilities and limitations of LLMs in this field.

Conclusion: Although promising, LLMs in architecture design still require significant human oversight and iterative refinement to ensure effective and reliable solutions.

Abstract: Designing effective software architectures is a complex, iterative process
that traditionally relies on expert judgment. This paper proposes an approach
for Large Language Model (LLM)-assisted software architecture design using the
Attribute-Driven Design (ADD) method. By providing an LLM with an explicit
description of ADD, an architect persona, and a structured iteration plan, our
method guides the LLM to collaboratively produce architecture artifacts with a
human architect. We validate the approach through case studies, comparing
generated designs against proven solutions and evaluating them with
professional architects. Results show that our LLM-assisted ADD process can
generate architectures closely aligned with established solutions and partially
satisfying architectural drivers, highlighting both the promise and current
limitations of using LLMs in architecture design. Our findings emphasize the
importance of human oversight and iterative refinement when leveraging LLMs in
this domain.

</details>


### [526] [P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code](https://arxiv.org/abs/2506.22703)
*Wali Mohammad Abdullah,Azmain Kabir*

Main category: cs.SE

TL;DR: P4OMP utilizes Retrieval-Augmented Generation (RAG) to enhance the generation of OpenMP parallel code using GPT-3.5-Turbo, achieving higher compilation success rates than baseline methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in ensuring syntactic correctness and reliability in generating OpenMP parallel code using LLMs, particularly for C/C++ programs.

Method: P4OMP adopts a retrieval-based prompting method, using instructional knowledge from OpenMP tutorials, and leverages GPT-3.5-Turbo without model fine-tuning for generating parallel code.

Result: P4OMP achieves 100% compilation success in parallelizable benchmark cases, outperforming the baseline in avoiding scoping errors and syntax issues.

Conclusion: The modular pipeline of P4OMP demonstrates improved reliability, scalability, and applicability of OpenMP code generation using LLMs, making it highly suitable for HPC applications.

Abstract: We present P4OMP, a retrieval-augmented framework for transforming serial
C/C++ code into OpenMP-annotated parallel code using large language models
(LLMs). To our knowledge, this is the first system to apply retrieval-based
prompting for OpenMP pragma correctness without model fine-tuning or compiler
instrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with
structured instructional knowledge from OpenMP tutorials to improve the
reliability of prompt-driven code generation. By grounding generation in the
retrieved context, P4OMP improves syntactic correctness compared to baseline
prompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,
GPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world
C++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.
P4OMP achieves 100% compilation success on all parallelizable cases, while the
baseline fails to compile in 20 out of 108 cases. Six cases that rely on
non-random-access iterators or thread-unsafe constructs are excluded due to
fundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP
consistently avoids scoping errors, syntactic misuse, and invalid directive
combinations that commonly affect baseline-generated code. We further
demonstrate strong runtime scaling across seven compute-intensive benchmarks on
an HPC cluster. P4OMP offers a robust, modular pipeline that significantly
improves the reliability and applicability of LLM-generated OpenMP code.

</details>


### [527] [Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation](https://arxiv.org/abs/2506.22776)
*Sen Fang,Weiyuan Ding,Antonio Mastropaolo,Bowen Xu*

Main category: cs.SE

TL;DR: Quantization improves robustness in large language models (LLMs) for code generation tasks.


<details>
  <summary>Details</summary>
Motivation: Quantization has been widely used to compress LLMs and reduce computation costs, but its impact on robustness remains unclear.

Method: Systematic robustness analysis of quantized LLMs in code generation tasks using adversarial attacks and noise perturbations on models across four LLM families.

Result: Quantized LLMs show higher robustness, with 51.59% of adversarial tests and better tolerance to noise compared to full-precision models.

Conclusion: Quantization not only enhances computational efficiency but also improves LLM robustness, benefiting code generation tasks and deployment strategies.

Abstract: Quantization has emerged as a mainstream method for compressing Large
Language Models (LLMs), reducing memory requirements and accelerating inference
without architectural modifications. While existing research primarily focuses
on evaluating the effectiveness of quantized LLMs compared to their original
counterparts, the impact on robustness remains largely unexplored.In this
paper, we present the first systematic investigation of how quantization
affects the robustness of LLMs in code generation tasks. Through extensive
experiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and
StarCoder) with parameter scales ranging from 350M to 33B, we evaluate
robustness from dual perspectives: adversarial attacks on input prompts and
noise perturbations on model architecture. Our findings challenge conventional
wisdom by demonstrating that quantized LLMs often exhibit superior robustness
compared to their full-precision counterparts, with 51.59% versus 42.86% of our
adversarial experiments showing better resilience in quantized LLMs. Similarly,
our noise perturbation experiments also confirm that LLMs after quantitation
generally withstand higher levels of weight disturbances. These results suggest
that quantization not only reduces computational requirements but can actually
enhance LLMs' reliability in code generation tasks, providing valuable insights
for developing more robust and efficient LLM deployment strategies.

</details>


### [528] [RAILS: Retrieval-Augmented Intelligence for Learning Software Development](https://arxiv.org/abs/2506.22742)
*Wali Mohammad Abdullah,Md. Morshedul Islam,Devraj Parmar,Happy Hasmukhbhai Patel,Sindhuja Prabhakaran,Baidya Saha*

Main category: cs.SE

TL;DR: RAILS improves software development by augmenting LLM prompts with retrieved context and compiler feedback, outperforming baseline methods in resolving Java import errors.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce incorrect or incomplete code due to limited access to project-specific or external documentation.

Method: RAILS enhances LLM capabilities using FAISS and OpenAI embeddings for semantic retrieval of Java resources, combined with iterative compiler feedback loops.

Result: RAILS successfully resolved 78 real-world Java import errors, outperforming standard LLM prompting by retaining developer intent and avoiding hallucinations.

Conclusion: RAILS effectively addresses code generation challenges with retrieval-augmented methods and iterative feedback, paving the way for future multi-language and IDE integrations.

Abstract: Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to
assist software development, yet they often produce incomplete code or
incorrect imports, especially when lacking access to external or
project-specific documentation. We introduce RAILS (Retrieval-Augmented
Intelligence for Learning Software Development), a framework that augments LLM
prompts with semantically retrieved context from curated Java resources using
FAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop
guided by compiler feedback to refine suggestions. We evaluated RAILS on 78
real-world Java import error cases spanning standard libraries, GUI APIs,
external tools, and custom utilities. Despite using the same LLM, RAILS
outperforms baseline prompting by preserving intent, avoiding hallucinations,
and surfacing correct imports even when libraries are unavailable locally.
Future work will integrate symbolic filtering via PostgreSQL and extend support
to other languages and IDEs.

</details>


### [529] [On the Feasibility of Deduplicating Compiler Bugs with Bisection](https://arxiv.org/abs/2506.23281)
*Xintong Zhou,Zhenyang Xu,Chengnian Sun*

Main category: cs.SE

TL;DR: This paper explores bisection for compiler bug deduplication, proposing a new method, BugLens, which reduces human effort compared to existing techniques.


<details>
  <summary>Details</summary>
Motivation: The need for effective compiler bug deduplication methods arises due to challenges in identifying duplicate bugs from random testing outputs, which is computationally costly and lacks generalizability in prior approaches.

Method: The authors investigate bisection as a duplicate detection mechanism, incorporate it in a new method called BugLens, and enhance it by identifying bug-triggering optimizations to improve accuracy.

Result: BugLens outperformed existing tools (Tamer and D3) on four datasets, achieving average human effort savings of 26.98% and 9.64% for the same bug identification tasks.

Conclusion: Bisection, as implemented in BugLens, offers a lightweight and generalizable solution for compiler bug deduplication, making it more practical for real-world applications.

Abstract: Random testing has proven to be an effective technique for compiler
validation. However, the debugging of bugs identified through random testing
presents a significant challenge due to the frequent occurrence of duplicate
test programs that expose identical compiler bugs. The process to identify
duplicates is a practical research problem known as bug deduplication. Prior
methodologies for compiler bug deduplication primarily rely on program analysis
to extract bug-related features for duplicate identification, which can result
in substantial computational overhead and limited generalizability. This paper
investigates the feasibility of employing bisection, a standard debugging
procedure largely overlooked in prior research on compiler bug deduplication,
for this purpose. Our study demonstrates that the utilization of bisection to
locate failure-inducing commits provides a valuable criterion for
deduplication, albeit one that requires supplementary techniques for more
accurate identification. Building on these results, we introduce BugLens, a
novel deduplication method that primarily uses bisection, enhanced by the
identification of bug-triggering optimizations to minimize false negatives.
Empirical evaluations conducted on four real-world datasets demonstrate that
BugLens significantly outperforms the state-of-the-art analysis-based
methodologies Tamer and D3 by saving an average of 26.98% and 9.64% human
effort to identify the same number of distinct bugs. Given the inherent
simplicity and generalizability of bisection, it presents a highly practical
solution for compiler bug deduplication in real-world applications.

</details>


### [530] [Privacy-Preserving Methods for Bug Severity Prediction](https://arxiv.org/abs/2506.22752)
*Havvanur Dervişoğlu,Ruşen Halepmollası,Elif Eyvaz*

Main category: cs.SE

TL;DR: This paper explores bug severity prediction through source code metrics and LLMs, comparing centralized learning, federated learning, and synthetic data generation approaches. Results show that federated learning and synthetic data methods perform comparably to centralized models while enabling privacy-preserving practices.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in bug severity prediction, specifically the data-sharing constraints and lack of labeled data faced in industrial applications, by exploring privacy-preserving techniques.

Method: The study used source code metrics and Large Language Models (LLMs) for bug severity prediction on two well-known datasets. It investigated three approaches: centralized learning, federated learning, and synthetic data generation, measuring their comparative performances.

Result: Experimental results suggest that the federated learning and synthetic data generation methods achieve comparable performance to centralized models without requiring data sharing.

Conclusion: Privacy-preserving methods like federated learning and synthetic data generation can effectively facilitate bug severity prediction, particularly in data-sensitive industrial scenarios.

Abstract: Bug severity prediction is a critical task in software engineering as it
enables more efficient resource allocation and prioritization in software
maintenance. While AI-based analyses and models significantly require access to
extensive datasets, industrial applications face challenges due to data-sharing
constraints and the limited availability of labeled data. In this study, we
investigate method-level bug severity prediction using source code metrics and
Large Language Models (LLMs) with two widely used datasets. We compare the
performance of models trained using centralized learning, federated learning,
and synthetic data generation. Our experimental results, obtained using two
widely recognized software defect datasets, indicate that models trained with
federated learning and synthetic data achieve comparable results to centrally
trained models without data sharing. Our finding highlights the potential of
privacy-preserving approaches such as federated learning and synthetic data
generation to enable effective bug severity prediction in industrial context
where data sharing is a major challenge.
  The source code and dataset are available at our GitHub repository:
https://github.com/drvshavva/EASE2025-Privacy-Preserving-Methods-for-Bug-Severity-Prediction.

</details>


### [531] [What Challenges Do Developers Face When Using Verification-Aware Programming Languages?](https://arxiv.org/abs/2506.23696)
*Francisco Oliveira,Alexandra Mendes,Carolina Carreira*

Main category: cs.SE

TL;DR: This paper examines why Verification-Aware (VA) programming languages, which provide strong correctness guarantees, are not widely adopted by developers.


<details>
  <summary>Details</summary>
Motivation: To investigate the barriers to adopting VA languages and improve their usability and accessibility since they offer significant benefits for software reliability.

Method: The study analyzes developer discussions on public forums using topic modeling techniques and complements this analysis with a developer survey.

Result: Key obstacles to the adoption of VA languages include steep learning curves, usability challenges, and inadequate tool interfaces and educational materials.

Conclusion: The study suggests actionable recommendations like improving tool interfaces, providing better educational resources, and integrating VA languages into common development environments to boost their adoption.

Abstract: Software reliability is critical in ensuring that the digital systems we
depend on function correctly. In software development, increasing software
reliability often involves testing. However, for complex and critical systems,
developers can use Design by Contract (DbC) methods to define precise
specifications that software components must satisfy. Verification-Aware (VA)
programming languages support DbC and formal verification at compile-time or
run-time, offering stronger correctness guarantees than traditional testing.
However, despite the strong guarantees provided by VA languages, their adoption
remains limited. In this study, we investigate the barriers to adopting VA
languages by analyzing developer discussions on public forums using topic
modeling techniques. We complement this analysis with a developer survey to
better understand the practical challenges associated with VA languages. Our
findings reveal key obstacles to adoption, including steep learning curves and
usability issues. Based on these insights, we identify actionable
recommendations to improve the usability and accessibility of VA languages. Our
findings suggest that simplifying tool interfaces, providing better educational
materials, and improving integration with everyday development environments
could improve the usability and adoption of these languages. Our work provides
actionable insights for improving the usability of VA languages and making
verification tools more accessible.

</details>


### [532] [Generating Privacy Stories From Software Documentation](https://arxiv.org/abs/2506.23014)
*Wilder Baldwin,Shashank Chintakuntla,Shreyah Parajuli,Ali Pourghasemi,Ryan Shanz,Sepideh Ghanavati*

Main category: cs.SE

TL;DR: The paper introduces a new method to extract privacy behaviors and create privacy user stories using Large Language Models (LLMs) and techniques like chain-of-thought prompting.


<details>
  <summary>Details</summary>
Motivation: Analysts and developers often neglect privacy concerns, treating them as secondary to security, which risks non-compliance and user privacy violations.

Method: The authors use Chain-of-Thought prompting, In-Context Learning, and LLMs (such as GPT-4o and Llama 3) to extract privacy behaviors from software documents and generate privacy requirements as user stories.

Result: The proposed approach achieves over 0.8 F1 scores for identifying privacy behaviors and generating user stories, with further performance gains available through model parameter tuning.

Conclusion: LLMs are effective tools for identifying and generating privacy requirements in the software development lifecycle, and their performance can be improved through optimization.

Abstract: Research shows that analysts and developers consider privacy as a security
concept or as an afterthought, which may lead to non-compliance and violation
of users' privacy. Most current approaches, however, focus on extracting legal
requirements from the regulations and evaluating the compliance of software and
processes with them. In this paper, we develop a novel approach based on
chain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language
Models (LLMs) to extract privacy behaviors from various software documents
prior to and during software development, and then generate privacy
requirements in the format of user stories. Our results show that most commonly
used LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and
generate privacy user stories with F1 scores exceeding 0.8. We also show that
the performance of these models could be improved through parameter-tuning. Our
findings provide insight into using and optimizing LLMs for generating privacy
requirements given software documents created prior to or throughout the
software development lifecycle.

</details>


### [533] [Guiding AI to Fix Its Own Flaws: An Empirical Study on LLM-Driven Secure Code Generation](https://arxiv.org/abs/2506.23034)
*Hao Yan,Swapneel Suhas Vaidya,Xiaokuan Zhang,Ziyu Yao*

Main category: cs.SE

TL;DR: This paper explores the security issues of Large Language Models (LLMs) in code generation, their ability to repair vulnerabilities, and strategies for improvement.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical issue of insecure code generation by LLMs, including the lack of robust methods to improve security and repair vulnerabilities.

Method: The authors conducted evaluations on proprietary and open-weight LLMs using established benchmarks and analyzed their tendencies to produce insecure code, capability to use vulnerability hints, and effectiveness in fixing vulnerabilities with feedback.

Result: The study found that LLMs often produce insecure code but advanced models can leverage vulnerability hints and feedback to significantly improve code security.

Conclusion: Advanced LLMs can become more effective in secure code generation and vulnerability repair when guided by well-designed hints and feedback, highlighting the need for developers to implement these strategies.

Abstract: Large Language Models (LLMs) have become powerful tools for automated code
generation. However, these models often overlook critical security practices,
which can result in the generation of insecure code that contains
vulnerabilities-weaknesses or flaws in the code that attackers can exploit to
compromise a system. However, there has been limited exploration of strategies
to guide LLMs in generating secure code and a lack of in-depth analysis of the
effectiveness of LLMs in repairing code containing vulnerabilities. In this
paper, we present a comprehensive evaluation of state-of-the-art LLMs by
examining their inherent tendencies to produce insecure code, their capability
to generate secure code when guided by self-generated vulnerability hints, and
their effectiveness in repairing vulnerabilities when provided with different
levels of feedback. Our study covers both proprietary and open-weight models
across various scales and leverages established benchmarks to assess a wide
range of vulnerability types. Through quantitative and qualitative analyses, we
reveal that although LLMs are prone to generating insecure code, advanced
models can benefit from vulnerability hints and fine-grained feedback to avoid
or fix vulnerabilities. We also provide actionable suggestions to developers to
reduce vulnerabilities when using LLMs for code generation.

</details>


### [534] [HF-DGF: Hybrid Feedback Guided Directed Grey-box Fuzzing](https://arxiv.org/abs/2506.23063)
*Guangfa Lyu,Zhenzhong Cao,Xiaofei Ren,Fengyu Wang*

Main category: cs.SE

TL;DR: The paper introduces HF-DGF, a new directed grey-box fuzzing (DGF) framework that significantly outperforms existing tools in speed and directionality for crash reproduction and patch testing, using hybrid feedback and advanced algorithms.


<details>
  <summary>Details</summary>
Motivation: Current DGF tools are limited by inadequate runtime feedback, reducing their efficiency in reaching target locations and exploring state space during crash reproduction and patch testing.

Method: HF-DGF improves DGF via a hybrid feedback mechanism combining control-flow distance, value-flow influence score, and slice coverage, supported by backward-stepping algorithms and selective instrumentation strategies.

Result: HF-DGF demonstrates faster crash reproduction than leading tools (AFL, AFLGo, WindRanger, DAFL, and Beacon) on average, achieving up to 73.75 times faster performance with superior directionality.

Conclusion: The HF-DGF framework provides a more efficient and directional approach to DGF, improving runtime feedback and outperforming existing fuzzing tools in crash reproduction and static analysis.

Abstract: Directed Grey-box Fuzzing (DGF) has emerged as a widely adopted technique for
crash reproduction and patch testing, leveraging its capability to precisely
navigate toward target locations and exploit vulnerabilities. However, current
DGF tools are constrained by insufficient runtime feedback, limiting their
efficiency in reaching targets and exploring state spaces. This study presents
HF-DGF, a novel directed grey-box fuzzing framework. Its seed scheduling is
guided by a hybrid feedback mechanism integrating control-flow distance,
value-flow influence score, and slice coverage. To enable precise control-flow
distance feedback, we propose a backward-stepping algorithm to calculate basic
block-level seed distances on a virtual inter-procedural control-flow graph
(ICFG). For effective state space exploration, we introduce value-flow
influence and a corresponding metric, the value-flow influence score.
Additionally, to mitigate runtime overhead from hybrid feedback, we adopt a
novel selective instrumentation strategy. Evaluations on 41 real-world
vulnerabilities show HF-DGF outperforms existing tools: it achieves crash
reproduction 5.05 times faster than AFL, 5.79 times faster than AFLGo, 73.75
times faster than WindRanger, 2.56 times faster than DAFL, and 8.45 times
faster than Beacon on average. Notably, when all fuzzers triggered crashes,
HF-DGF exhibited the lowest code coverage, demonstrating superior
directionality and efficiency. It also surpasses AFLGo, WindRanger, DAFL, and
Beacon in static analysis efficiency.

</details>


### [535] [Repair Ingredients Are All You Need: Improving Large Language Model-Based Program Repair via Repair Ingredients Search](https://arxiv.org/abs/2506.23100)
*Jiayi Zhang,Kai Huang,Jian Zhang,Yang Liu,Chunyang Chen*

Main category: cs.SE

TL;DR: The paper introduces ReinFix, a framework that enhances Large Language Models (LLMs) for automated program repair by integrating contextually relevant repair ingredients and achieves state-of-the-art results on popular benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches for automated program repair lack capability in generating contextually relevant and accurate patches, as they often disregard important repair ingredients.

Method: ReinFix employs a two-phase framework: 1) During the reasoning phase, it incorporates static analysis tools to retrieve internal repair ingredients for root cause analysis. 2) During the solution phase, it utilizes historical bug fixes (external ingredients) to guide patch generation based on bug patterns.

Result: ReinFix demonstrates superior performance by fixing 146 bugs on Defects4J V1.2 (32 more than baselines) and 38 additional bugs on Defects4J V2.0 compared to the state-of-the-art.

Conclusion: ReinFix significantly improves automated program repair by equipping LLMs with both internal and external repair ingredients, establishing itself as the leading approach even on recent benchmarks free from data leakage risks.

Abstract: Automated Program Repair (APR) techniques aim to automatically fix buggy
programs. Among these, Large Language Model-based (LLM-based) approaches have
shown great promise. Recent advances demonstrate that directly leveraging LLMs
can achieve leading results. However, these techniques remain suboptimal in
generating contextually relevant and accurate patches, as they often overlook
repair ingredients crucial for practical program repair. In this paper, we
propose ReinFix, a novel framework that enables LLMs to autonomously search for
repair ingredients throughout both the reasoning and solution phases of bug
fixing. In the reasoning phase, ReinFix integrates static analysis tools to
retrieve internal ingredients, such as variable definitions, to assist the LLM
in root cause analysis when it encounters difficulty understanding the context.
During the solution phase, when the LLM lacks experience in fixing specific
bugs, ReinFix searches for external ingredients from historical bug fixes with
similar bug patterns, leveraging both the buggy code and its root cause to
guide the LLM in identifying appropriate repair actions, thereby increasing the
likelihood of generating correct patches. Evaluations on two popular benchmarks
(Defects4J V1.2 and V2.0) demonstrate the effectiveness of our approach over
SOTA baselines. Notably, ReinFix fixes 146 bugs, which is 32 more than the
baselines on Defects4J V1.2. On Defects4J V2.0, ReinFix fixes 38 more bugs than
the SOTA. Importantly, when evaluating on the recent benchmarks that are free
of data leakage risk, ReinFix also maintains the best performance.

</details>


### [536] [From Release to Adoption: Challenges in Reusing Pre-trained AI Models for Downstream Developers](https://arxiv.org/abs/2506.23234)
*Peerachai Banyongrakkul,Mansooreh Zahedi,Patanamon Thongtanunam,Christoph Treude,Haoyu Gao*

Main category: cs.SE

TL;DR: The paper studies challenges developers face when reusing pre-trained models (PTMs) in software systems, identifying seven categories of issues and highlighting their resolution complexities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the underexplored challenges developers encounter when reusing PTMs in downstream software projects, despite the widespread success of PTMs in various fields.

Method: The authors created and analyzed a dataset of 840 PTM-related issues from 31 OSS GitHub projects, developing a comprehensive taxonomy of challenges and performing resolution time analysis.

Result: Seven categories of PTM-related challenges were identified, with findings showing PTM-related issues take significantly longer for resolution compared to non-PTM issues.

Conclusion: The paper highlights the need for better tools, support, and research resources for developers working with PTMs, offering insights for future research directions and practical improvements.

Abstract: Pre-trained models (PTMs) have gained widespread popularity and achieved
remarkable success across various fields, driven by their groundbreaking
performance and easy accessibility through hosting providers. However, the
challenges faced by downstream developers in reusing PTMs in software systems
are less explored. To bridge this knowledge gap, we qualitatively created and
analyzed a dataset of 840 PTM-related issue reports from 31 OSS GitHub
projects. We systematically developed a comprehensive taxonomy of PTM-related
challenges that developers face in downstream projects. Our study identifies
seven key categories of challenges that downstream developers face in reusing
PTMs, such as model usage, model performance, and output quality. We also
compared our findings with existing taxonomies. Additionally, we conducted a
resolution time analysis and, based on statistical tests, found that
PTM-related issues take significantly longer to be resolved than issues
unrelated to PTMs, with significant variation across challenge categories. We
discuss the implications of our findings for practitioners and possibilities
for future research.

</details>


### [537] [Improving vulnerability type prediction and line-level detection via adversarial training-based data augmentation and multi-task learning](https://arxiv.org/abs/2506.23534)
*Siyu Chen,Jiongyi Yang,Xiang Chen,Menglin Zheng,Minnan Wei,Xiaolin Ju*

Main category: cs.SE

TL;DR: This paper addresses the challenges in detecting software vulnerabilities by proposing a unified approach combining Embedding-Layer Driven Adversarial Training (EDAT) and Multi-task Learning (MTL), achieving superior results in vulnerability type prediction and line-level detection compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Escalating software vulnerabilities and cyberattacks demand automated detection methods, but challenges such as limited labeled data, class imbalance, and the independent treatment of related tasks hinder progress.

Method: The study integrates EDAT to enhance robustness via adversarial perturbations and MTL to exploit shared semantics and inter-task correlations for Vulnerability Type Prediction (VTP) and Line-level Vulnerability Detection (LVD).

Result: The proposed unified approach improves VTP accuracy, precision, recall, and F1-score, especially for rare types. It also enhances LVD accuracy while reducing false positives, outperforming existing methods.

Conclusion: The integration of EDAT with MTL offers an effective unified solution for both VTP and LVD, showing promise for further research and refinement.

Abstract: Context: Software vulnerabilities pose a significant threat to modern
software systems, as evidenced by the growing number of reported
vulnerabilities and cyberattacks. These escalating trends underscore the urgent
need for effective approaches that can automatically detect and understand
software vulnerabilities. Objective: However, the scarcity of labeled samples
and the class imbalance issue in vulnerability datasets present significant
challenges for both Vulnerability Type Prediction (VTP) and Line-level
Vulnerability Detection (LVD), especially for rare yet critical vulnerability
types. Moreover, most existing studies treat VTP and LVD as independent tasks,
overlooking their inherent correlation, which limits the potential to leverage
shared semantic patterns across tasks. Methods: To address these limitations,
we propose a unified approach that integrates Embedding-Layer Driven
Adversarial Training (EDAT) with Multi-task Learning (MTL). Specifically, EDAT
enhances model robustness by introducing adversarial perturbations to
identifier embeddings, guided by semantic importance. Meanwhile, MTL improves
overall performance by leveraging shared representations and inter-task
correlations between VTP and LVD. Results: Extensive experiments demonstrate
that our proposed approach outperforms state-of-the-art baselines on both VTP
and LVD tasks. For VTP, it yields notable improvements in accuracy, precision,
recall, and F1-score, particularly in identifying rare vulnerability types.
Similarly, for LVD, our approach enhances line-level detection accuracy while
significantly reducing false positives. Conclusion: Our study demonstrates that
combining EDAT with MTL provides a unified solution that improves performance
on both tasks and warrants further investigation.

</details>


### [538] [Comparative Analysis of the Code Generated by Popular Large Language Models (LLMs) for MISRA C++ Compliance](https://arxiv.org/abs/2506.23535)
*Malik Muhammad Umer*

Main category: cs.SE

TL;DR: The paper conducts a comparative analysis of code generated by popular Large Language Models (LLMs) to evaluate their compliance with MISRA C++ standards in safety-critical domains.


<details>
  <summary>Details</summary>
Motivation: Safety-critical systems require adherence to rigorous software engineering and coding standards to prevent catastrophic failures. With advancements in AI-driven code generation, it's important to assess LLM-generated code for compliance with these standards.

Method: The study compares C++ code produced by LLMs such as ChatGPT, Google Gemini, DeepSeek, Meta AI, and Microsoft Copilot, analyzing it against MISRA C++ coding guidelines.

Result: Findings reveal how well various LLMs conform to MISRA C++ standards, highlighting their strengths and weaknesses in safety-critical code generation.

Conclusion: LLMs have potential in generating code for safety-critical systems, but their output must undergo stringent compliance checks to meet industry standards like MISRA C++.

Abstract: Safety-critical systems are engineered systems whose failure or malfunction
could result in catastrophic consequences. The software development for
safety-critical systems necessitates rigorous engineering practices and
adherence to certification standards like DO-178C for avionics. DO-178C is a
guidance document which requires compliance to well-defined software coding
standards like MISRA C++ to enforce coding guidelines that prevent the use of
ambiguous, unsafe, or undefined constructs. Large Language Models (LLMs) have
demonstrated significant capabilities in automatic code generation across a
wide range of programming languages, including C++. Despite their impressive
performance, code generated by LLMs in safety-critical domains must be
carefully analyzed for conformance to MISRA C++ coding standards. In this
paper, I have conducted a comparative analysis of the C++ code generated by
popular LLMs including: OpenAI ChatGPT, Google Gemini, DeepSeek, Meta AI, and
Microsoft Copilot for compliance with MISRA C++.

</details>


### [539] [QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration](https://arxiv.org/abs/2506.23644)
*Junze Hu,Xiangyu Jin,Yizhe Zeng,Yuling Liu,Yunpeng Li,Dan Du,Kaiyu Xie,Hongsong Zhu*

Main category: cs.SE

TL;DR: QLPro is a framework combining large language models (LLMs) and static analysis for comprehensive vulnerability detection, outperforming existing tools like CodeQL.


<details>
  <summary>Details</summary>
Motivation: Enhance vulnerability detection in open-source projects by addressing the limitations of current static analysis tools.

Method: Developed QLPro by integrating large language models with static analysis tools, and evaluated it using the newly constructed JavaTest dataset.

Result: QLPro detected 41 vulnerabilities (compared to 24 by CodeQL) and uncovered 6 previously unknown vulnerabilities, including 2 0-days.

Conclusion: QLPro demonstrates improved efficacy in vulnerability detection compared to traditional tools, highlighting its potential for enhancing security in open-source software.

Abstract: We introduce QLPro, a vulnerability detection framework that systematically
integrates LLMs and static analysis tools to enable comprehensive vulnerability
detection across entire open-source projects.We constructed a new dataset,
JavaTest, comprising 10 open-source projects from GitHub with 62 confirmed
vulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only
24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro
discovered 6 previously unknown vulnerabilities, 2 of which have been confirmed
as 0-days.

</details>


### [540] [Towards a Science of Developer eXperience (DevX)](https://arxiv.org/abs/2506.23715)
*Benoit Combemale*

Main category: cs.SE

TL;DR: The paper argues for recognizing Developer eXperience (DevX) as an essential field of research in software engineering.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored human aspects of software creation, despite the technical advances in software engineering.

Method: The paper identifies rationales, enablers, and interdisciplinary intersections while providing a roadmap for scientific challenges in DevX.

Result: It highlights the importance of DevX in collaborative and diverse development environments for fostering productivity and sustainable practices.

Conclusion: The authors call for a research focus on DevX to develop human-centered software engineering approaches and enhance developer productivity.

Abstract: As software continues to permeate nearly every facet of modern life, the
complexity and ubiquity of digital services underscore the need for
sustainable, effective, and inclusive software development practices. Although
software engineering has made significant progress in technical challenges
since its inception, the human experience of those involved in software
creation, broadly defined as developers, remains underexplored. This column
advocates for the formal recognition of Developer eXperience (DevX) as a
distinct research field. We argue that DevX profoundly influences critical
development activities and overall productivity, especially as development
becomes increasingly collaborative and diverse in terms of application domains.
Building on existing efforts to measure and enhance DevX, we identify key
rationales, scientific enablers, and interdisciplinary intersections that
support this emerging discipline. We also outline the core scientific
challenges ahead, aiming to call for actions from the research community and to
promote more human-centered approaches to software engineering.

</details>


### [541] [A Survey of LLM-based Automated Program Repair: Taxonomies, Design Paradigms, and Applications](https://arxiv.org/abs/2506.23749)
*Boyang Yang,Zijian Cai,Fengling Liu,Bach Le,Lingming Zhang,Tegawendé F. Bissyandé,Yang Liu,Haoye Tian*

Main category: cs.SE

TL;DR: The paper categorizes 63 recent LLM-based Automated Program Repair (APR) systems into four paradigms, explores their trade-offs, challenges, and proposes directions for improvement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assess and classify advancements in LLM-based APR, highlighting key paradigms, challenges, and opportunities for improvement in scalability, efficiency, and reliability.

Method: The authors survey 63 LLM-based APR systems and categorize them into four paradigms: fine-tuning, prompting, procedural pipelines, and agentic frameworks, analyzing their trade-offs and contexts.

Result: The analysis reveals strengths and weaknesses of each paradigm, clarifying trade-offs such as task alignment vs. training cost and rapid deployment vs. design limitations. Persistent challenges like verifying semantic correctness are also discussed.

Conclusion: Future research should focus on human feedback integration, repository-aware retrieval, enhanced code analysis, and cost-efficient approaches to make LLM-based APR more reliable and scalable.

Abstract: Large language models (LLMs) are reshaping automated program repair (APR). We
categorize the recent 63 LLM-based APR systems published from January 2022 to
June 2025 into four paradigms, and show how retrieval- or analysis-augmented
contexts strengthen any of them. This taxonomy clarifies key trade-offs:
fine-tuning delivers strong task alignment at high training cost; prompting
enables rapid deployment but is limited by prompt design and context windows;
procedural pipelines offer reproducible control with moderate overhead; agentic
frameworks tackle multi-hunk or cross-file bugs at the price of increased
latency and complexity. Persistent challenges include verifying semantic
correctness beyond test suites, repairing repository-scale defects, and
lowering the costs of LLMs. We outline research directions that combine
lightweight human feedback, repository-aware retrieval, code analysis, and
cost-aware planning to advance reliable and efficient LLM-based APR.

</details>


### [542] [Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead](https://arxiv.org/abs/2506.23762)
*Hongzhou Rao,Yanjie Zhao,Xinyi Hou,Shenao Wang,Haoyu Wang*

Main category: cs.SE

TL;DR: This paper systematically analyzes challenges in large language model (LLM) development using a software engineering (SE) perspective, identifying key challenges across six phases and proposing future research directions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address gaps in understanding the challenges and solutions in LLM development throughout its lifecycle, especially from a SE perspective, for better academic and industrial advancements.

Method: The authors analyze the LLM lifecycle through six phases: requirements engineering, dataset construction, model development and enhancement, testing and evaluation, deployment and operations, and maintenance and evolution. They identify challenges and suggest potential research directions for each phase.

Result: The study highlights key challenges encountered in each phase of the LLM development lifecycle and provides future-oriented research directions to overcome these challenges.

Conclusion: This research emphasizes the significance of a systematic SE perspective in LLM development, offering insights that can advance both research and practice in AI-driven systems.

Abstract: The rapid advancement of large language models (LLMs) has redefined
artificial intelligence (AI), pushing the boundaries of AI research and
enabling unbounded possibilities for both academia and the industry. However,
LLM development faces increasingly complex challenges throughout its lifecycle,
yet no existing research systematically explores these challenges and solutions
from the perspective of software engineering (SE) approaches. To fill the gap,
we systematically analyze research status throughout the LLM development
lifecycle, divided into six phases: requirements engineering, dataset
construction, model development and enhancement, testing and evaluation,
deployment and operations, and maintenance and evolution. We then conclude by
identifying the key challenges for each phase and presenting potential research
directions to address these challenges. In general, we provide valuable
insights from an SE perspective to facilitate future advances in LLM
development.

</details>


### [543] [Requirements for Active Assistance of Natural Questions in Software Architecture](https://arxiv.org/abs/2506.23898)
*Diogo Lemos,Ademar Aguiar,Neil B. Harrison*

Main category: cs.SE

TL;DR: This study explores how natural questions affect architectural design and proposes an environment to manage them better using AI and knowledge tools.


<details>
  <summary>Details</summary>
Motivation: To address the mismanagement or neglect of natural questions in architectural design, which leads to issues like knowledge loss and inefficient resource use.

Method: Combined literature review, a requirements workshop, three design iterations, and expert surveys to study the lifecycle of natural questions and design an effective environment.

Result: Proposed a lifecycle for natural questions, identified key requirements, and validated the design through expert feedback, ensuring improved architecture knowledge preservation and decision-making.

Conclusion: Managing natural questions effectively with an adaptable, AI-integrated environment can significantly improve collaboration, decision-making, and knowledge retention in architecture.

Abstract: Natural questions are crucial to shaping key architectural decisions and
preserving architectural knowledge. They arise organically during the
architectural design process, often resulting from the existing architectural
experience of the designer and the distinctive characteristics of the system
being designed. However, natural questions are often mismanaged or ignored,
which can lead to architectural drift, knowledge loss, inefficient resource
use, or poor understandability of the system's architecture. We aim to better
understand the lifecycle of natural questions, its key requirements, challenges
and difficulties, and then to envision an assisted environment to properly
support it. The environment should be adaptable and responsive to real-world
constraints and uncertainties by seamlessly integrating knowledge management
tools and artificial intelligence techniques into software development
workflows. Based on existing literature, a requirements workshop, and three
design iterations, we proposed a lifecycle for natural questions and elicited
essential functional and non-functional requirements for such an environment.
At last, the results of a survey conducted with experts helped to analyze and
validate the elicited requirements and proposed features for the environment to
enhance collaboration, decision-making, and the preservation of architectural
knowledge more effectively than conventional methods.

</details>


### [544] [Green Metrics Tool: Measuring for fun and profit](https://arxiv.org/abs/2506.23967)
*Geerd-Dietger Hoffmann,Verena Majuntke*

Main category: cs.SE

TL;DR: The paper introduces the Green Metrics Tool (GMT) for evaluating and optimizing the environmental impact of software by measuring its resource consumption effectively.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for computational resources necessitates the evaluation of software resource consumption to optimize usage and reduce carbon emissions.

Method: The authors propose a framework called the Green Metrics Tool (GMT) that uses a containerized, controlled, reproducible, life cycle-based approach to measure resource consumption during key software phases.

Result: GMT features include visualization, comparability, and rule- and LLM-based optimizations, demonstrating the tool's capabilities in providing actionable insights.

Conclusion: The GMT has the potential to assist developers and researchers in minimizing the environmental footprint of their software through better understanding and optimization of resource usage.

Abstract: The environmental impact of software is gaining increasing attention as the
demand for computational resources continues to rise. In order to optimize
software resource consumption and reduce carbon emissions, measuring and
evaluating software is a first essential step. In this paper we discuss what
metrics are important for fact base decision making. We introduce the Green
Metrics Tool (GMT), a novel framework for accurately measuring the resource
consumption of software. The tool provides a containerized, controlled, and
reproducible life cycle-based approach, assessing the resource use of software
during key phases. Finally, we discuss GMT features like visualization,
comparability and rule- and LLM-based optimisations highlighting its potential
to guide developers and researchers in reducing the environmental impact of
their software.

</details>


### [545] [STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems](https://arxiv.org/abs/2506.23995)
*Mingfei Cheng,Renzhi Wang,Xiaofei Xie,Yuan Zhou,Lei Ma*

Main category: cs.SE

TL;DR: This paper introduces STCLocker, a testing technique aimed at generating deadlock scenarios in multi-autonomous-vehicle settings, surpassing existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the gap in testing multi-AV systems for deadlock conditions, a critical coordination failure inadequately explored by existing ADS testing techniques.

Method: STCLocker employs Deadlock Oracle to detect deadlocks, and uses Conflict Feedback alongside Conflict-aware Scenario Generation to guide AVs into spatio-temporal conflicts for testing purposes.

Result: STCLocker outperformed the best-existing baseline in generating Deadlock Scenarios across tests on two different ADSs, Roach and OpenCDA.

Conclusion: STCLocker presents a robust solution for testing the cooperative performance of multi-AV systems, enhancing the reliability of ADSs in deadlock-prone scenarios.

Abstract: Autonomous Driving System (ADS) testing is essential to ensure the safety and
reliability of autonomous vehicles (AVs) before deployment. However, existing
techniques primarily focus on evaluating ADS functionalities in single-AV
settings. As ADSs are increasingly deployed in multi-AV traffic, it becomes
crucial to assess their cooperative performance, particularly regarding
deadlocks, a fundamental coordination failure in which multiple AVs enter a
circular waiting state indefinitely, resulting in motion planning failures.
Despite its importance, the cooperative capability of ADSs to prevent deadlocks
remains insufficiently underexplored. To address this gap, we propose the first
dedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,
STCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs
controlled by the ADS under test are in a circular wait state. STCLocker
consists of three key components: Deadlock Oracle, Conflict Feedback, and
Conflict-aware Scenario Generation. Deadlock Oracle provides a reliable
black-box mechanism for detecting deadlock cycles among multiple AVs within a
given scenario. Conflict Feedback and Conflict-aware Scenario Generation
collaborate to actively guide AVs into simultaneous competition over spatial
conflict resources (i.e., shared passing regions) and temporal competitive
behaviors (i.e., reaching the conflict region at the same time), thereby
increasing the effectiveness of generating conflict-prone deadlocks. We
evaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,
a module-based ADS supporting cooperative communication. Experimental results
show that, on average, STCLocker generates more DLS than the best-performing
baseline.

</details>


### [546] [Bug Fixing with Broader Context: Enhancing LLM-Based Program Repair via Layered Knowledge Injection](https://arxiv.org/abs/2506.24015)
*Ramtin Ehsani,Esteban Parra,Sonia Haiduc,Preetha Chatterjee*

Main category: cs.SE

TL;DR: The paper proposes a framework for improving automated program repair (APR) by incrementally injecting structured bug, repository, and project knowledge into large language models (LLMs), leading to a 79% bug fix rate on a BugsInPy dataset and revealing the effectiveness of varying levels of context for different bug types.


<details>
  <summary>Details</summary>
Motivation: Existing automated program repair methods often fail to resolve many bugs because they lack broader repository and project-level context typically used by developers in real-world scenarios.

Method: The authors propose a three-layered knowledge injection framework for LLMs. The first layer (Bug Knowledge Layer) includes local bug-specific information, the second layer (Repository Knowledge Layer) adds structural dependencies and related files, and the third layer (Project Knowledge Layer) incorporates broader project-level insights like documentation and past bug fixes.

Result: Using the framework on 314 bugs in the BugsInPy dataset, Llama 3.3 achieved a 79% fix rate, a 23% improvement over existing methods. Improvements were evident across all bug types with repository knowledge, while only some types benefited from additional project-level information.

Conclusion: Layered injection of structured knowledge significantly improves automated program repair performance. However, issues like complex and isolated bugs remain a challenge, indicating a need for adaptive and interactive APR systems in the future.

Abstract: Prompting LLMs with bug-related context (e.g., error messages, stack traces)
improves automated program repair, but many bugs still remain unresolved. In
real-world projects, developers often rely on broader repository and
project-level context beyond the local code to resolve such bugs. In this
paper, we investigate how automatically extracting and providing such knowledge
can improve LLM-based program repair. We propose a layered knowledge injection
framework that incrementally augments LLMs with structured context. It starts
with the Bug Knowledge Layer, which includes information such as the buggy
function and failing tests; expands to the Repository Knowledge Layer, which
adds structural dependencies, related files, and commit history; and finally
injects the Project Knowledge Layer, which incorporates relevant details from
documentation and previously fixed bugs. We evaluate this framework on a
dataset of 314 bugs from BugsInPy using two LLMs (Llama 3.3 and GPT-4o-mini),
and analyze fix rates across six bug types. By progressively injecting
knowledge across layers, our approach achieves a fix rate of 79% (250/314)
using Llama 3.3, a significant improvement of 23% over previous work. All bug
types show improvement with the addition of repository-level context, while
only a subset benefit further from project-level knowledge, highlighting that
different bug types require different levels of contextual information for
effective repair. We also analyze the remaining unresolved bugs and find that
more complex and structurally isolated bugs, such as Program Anomaly and GUI
bugs, remain difficult even after injecting all available information. Our
results show that layered context injection improves program repair and suggest
the need for interactive and adaptive APR systems.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [547] [Hemispheric-Specific Coupling Improves Modeling of Functional Connectivity Using Wilson-Cowan Dynamics](https://arxiv.org/abs/2506.22951)
*Ramiro Plüss,Hernán Villota,Patricio Orio*

Main category: q-bio.NC

TL;DR: The paper introduces a hemispheric-specific coupling scheme to the Wilson-Cowan model and demonstrates improved alignment between simulated and empirical brain connectivity.


<details>
  <summary>Details</summary>
Motivation: To improve the biological accuracy of large-scale brain network models by addressing hemispheric asymmetries in structural connectivity.

Method: Using the Wilson-Cowan model, the authors introduced hemispheric-specific coupling to differentiate between intra- and inter-hemispheric structural connections. They applied the model to empirical cortical connectomes and fMRI data from both control and schizophrenia groups.

Result: The hemispheric-asymmetric coupling approach improved the correlation between simulated and empirical functional connectivity data.

Conclusion: Hemispheric-specific coupling enhances the anatomical realism of large-scale neural mass models, demonstrating its importance in understanding brain connectivity.

Abstract: Large-scale neural mass models have been widely used to simulate
resting-state brain activity from structural connectivity. In this work, we
extend a well-established Wilson--Cowan framework by introducing a novel
hemispheric-specific coupling scheme that differentiates between
intra-hemispheric and inter-hemispheric structural interactions. We apply this
model to empirical cortical connectomes and resting-state fMRI data from
matched control and schizophrenia groups. Simulated functional connectivity is
computed from the band-limited envelope correlations of regional excitatory
activity and compared against empirical functional connectivity matrices. Our
results show that incorporating hemispheric asymmetries enhances the
correlation between simulated and empirical functional connectivity,
highlighting the importance of anatomically-informed coupling strategies in
improving the biological realism of large-scale brain network models.

</details>


### [548] [Distinct Modes of Functional Neural Organization in Autism: Insights from Dynamical Systems Analysis of Resting-State EEG](https://arxiv.org/abs/2506.23013)
*Sungwoo Ahn,Leonid L Rubchinsky,Evie A Malaia*

Main category: q-bio.NC

TL;DR: The study compares brain network dynamics in autistic and neurotypical young adults using a dynamical systems approach on EEG, revealing more unstable neural dynamics in autistic individuals.


<details>
  <summary>Details</summary>
Motivation: To understand fine-grained differences in neural dynamics and connectivity between autistic and neurotypical individuals, which are not captured by traditional EEG measures.

Method: Resting-state EEG data from autistic and neurotypical young adults were analyzed using a dynamical systems approach focusing on Lyapunov exponents, phase synchronization, and network efficiency.

Result: Autistic participants showed less stable neural dynamics, weaker synchronization, and lower functional network efficiency. Neural dynamics in autistic individuals improved when closing their eyes.

Conclusion: Autistic brains may exhibit distinct neural dynamic patterns reflecting lifelong sensory adaptations, which influence both resting-state brain activity and cognitive strategies.

Abstract: While differences in patterns of functional connectivity and neural
synchronization have been reported between individuals on the autism spectrum
and neurotypical peers at various age stages, these differences appear to be
subtle and may not be captured by typical quantitative measures of EEG. We used
the dynamical systems approach to analyze resting-state EEG to investigate
fine-grained spatiotemporal organization of brain networks in autistic and
neurotypical young adults. While power spectra showed minimal group
differences, autistic participants exhibited higher Lyapunov exponents
(indicating less stable neural dynamics), weaker phase synchronization, and
lower clustering/efficiency of functional networks during eyes-open resting
state, suggesting more random and less stably connected neural dynamics in
comparison to those of neurotypical peers. Closing the eyes regularized neural
dynamics in autistic but not neurotypical participants, with increases in
synchrony strength, transient desynchronization patterning, and functional
connectivity observed in the autistic group. The results point to the distinct
modes of neural dynamics organization that could reflect life-long adaptations
to sensory inputs that shape both resting-state neural activity and cognitive
processing strategies.

</details>


### [549] [Neural Langevin Machine: a local asymmetric learning rule can be creative](https://arxiv.org/abs/2506.23546)
*Zhendong Yu,Weizhong Huang,Haiping Huang*

Main category: q-bio.NC

TL;DR: The paper introduces the concept of a neural Langevin machine, leveraging fixed points in recurrent neural networks for generative modeling.


<details>
  <summary>Details</summary>
Motivation: Explores biologically relevant generative model inspired by brain circuits that enables continuous sampling and learning processes.

Method: Utilizes neural Langevin dynamics, derived from Boltzmann-Gibbs measures, coupled with asymmetric plasticity for training.

Result: Demonstrates an interpretable and efficient generative model with biologically plausible mechanisms.

Conclusion: Proposes neural Langevin machines as a promising generative model with applications in creative continuous sampling and biological mimicry.

Abstract: Fixed points of recurrent neural networks can be leveraged to store and
generate information. These fixed points can be captured by the Boltzmann-Gibbs
measure, which leads to neural Langevin dynamics that can be used for sampling
and learning a real dataset. We call this type of generative model neural
Langevin machine, which is interpretable due to its analytic form of
distribution and is simple to train. Moreover, the learning process is derived
as a local asymmetric plasticity rule, bearing biological relevance. Therefore,
one can realize a continuous sampling of creative dynamics in a neural network,
mimicking an imagination process in brain circuits. This neural Langevin
machine may be another promising generative model, at least in its strength in
circuit-based sampling and biologically plausible learning rule.

</details>


### [550] [Attention acts to suppress goal-based conflict under high competition](https://arxiv.org/abs/1610.09431)
*Omar Claflin*

Main category: q-bio.NC

TL;DR: Top-down attention suppresses neural signals for both relevant and irrelevant stimuli under high competition to reduce irrelevant feedforward signals.


<details>
  <summary>Details</summary>
Motivation: Investigate how top-down attention operates under high competition where multiple stimuli vie for neural representation simultaneously.

Method: Experimental analysis of neural signals during conditions of high competition, with stimuli sharing receptive fields and differing goals.

Result: Within 100 ms of stimulus onset, top-down attention suppresses neural signals for both task-relevant and irrelevant stimuli.

Conclusion: Top-down attention non-selectively engages to mitigate the feedforward signal of irrelevant stimuli under competitive conditions.

Abstract: It is known that when multiple stimuli are present, top-down attention
selectively enhances the neural signal in the visual cortex for task-relevant
stimuli, but this has been tested only under conditions of minimal competition
of visual attention. Here we show during high competition, that is, two stimuli
in a shared receptive field possessing opposing modulatory goals, top-down
attention suppresses both task-relevant and irrelevant neural signals within
100 ms of stimuli onset. This non-selective engagement of top-down attentional
resources serves to reduce the feedforward signal representing irrelevant
stimuli.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [551] [Strategic A/B testing via Maximum Probability-driven Two-armed Bandit](https://arxiv.org/abs/2506.22536)
*Yu Zhang,Shanshan Zhao,Bokui Wan,Jinjuan Wang,Xiaodong Yan*

Main category: stat.ML

TL;DR: This paper proposes an innovative two-armed bandit approach to improve the detection of minor average treatment effects, leveraging counterfactual frameworks and strategic permutation methods for enhanced statistical power.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle to detect minor treatment effects in large-scale applications, limiting sensitivity and failing to address small discrepancies effectively.

Method: This approach introduces a maximum probability-driven two-armed bandit process using weighted mean volatility statistics and permutation methods, designed to control Type I error and employ central limit theorem strategies for improved null and alternative hypothesis distributions.

Result: The experimental results show improved A/B testing efficiency, significantly enhancing statistical power while reducing experimental costs.

Conclusion: The methodology provides a robust solution for detecting minor effects with high statistical power, paving the way for more cost-effective and accurate large-scale applications.

Abstract: Detecting a minor average treatment effect is a major challenge in
large-scale applications, where even minimal improvements can have a
significant economic impact. Traditional methods, reliant on normal
distribution-based or expanded statistics, often fail to identify such minor
effects because of their inability to handle small discrepancies with
sufficient sensitivity. This work leverages a counterfactual outcome framework
and proposes a maximum probability-driven two-armed bandit (TAB) process by
weighting the mean volatility statistic, which controls Type I error. The
implementation of permutation methods further enhances the robustness and
efficacy. The established strategic central limit theorem (SCLT) demonstrates
that our approach yields a more concentrated distribution under the null
hypothesis and a less concentrated one under the alternative hypothesis,
greatly improving statistical power. The experimental results indicate a
significant improvement in the A/B testing, highlighting the potential to
reduce experimental costs while maintaining high statistical power.

</details>


### [552] [Adjoint Schrödinger Bridge Sampler](https://arxiv.org/abs/2506.22565)
*Guan-Horng Liu,Jaemoo Choi,Yongxin Chen,Benjamin Kurt Miller,Ricky T. Q. Chen*

Main category: stat.ML

TL;DR: This paper introduces the Adjoint Schr"odinger Bridge Sampler (ASBS), a new diffusion-based method for learning to sample from Boltzmann distributions, which is more scalable and avoids the need for target sample estimation during training.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based methods for sampling from Boltzmann distributions often suffer from complexity and inefficiency due to reliance on importance-weighted estimation or complicated learning processes.

Method: The authors propose ASBS, a diffusion sampling technique grounded in the Schr"odinger Bridge model that employs stochastic optimal control theory and Adjoint Matching to efficiently train sampling models without requiring target samples during training.

Result: ASBS achieves enhanced sampling efficiency and generalizes previous methods (like Adjoint Sampling) by relaxing constraints such as the memoryless condition. Experiments show its effectiveness in various tasks like classical energy sampling, molecular conformer generation, and Boltzmann distribution modeling.

Conclusion: ASBS addresses scalability and training inefficiencies in diffusion sampling, offering a robust and generalized solution for sampling from complex distributions, thus expanding its practical applicability and efficiency.

Abstract: Computational methods for learning to sample from the Boltzmann distribution
-- where the target distribution is known only up to an unnormalized energy
function -- have advanced significantly recently. Due to the lack of explicit
target samples, however, prior diffusion-based methods, known as diffusion
samplers, often require importance-weighted estimation or complicated learning
processes. Both trade off scalability with extensive evaluations of the energy
and model, thereby limiting their practical usage. In this work, we propose
Adjoint Schr\"odinger Bridge Sampler (ASBS), a new diffusion sampler that
employs simple and scalable matching-based objectives yet without the need to
estimate target samples during training. ASBS is grounded on a mathematical
model -- the Schr\"odinger Bridge -- which enhances sampling efficiency via
kinetic-optimal transportation. Through a new lens of stochastic optimal
control theory, we demonstrate how SB-based diffusion samplers can be learned
at scale via Adjoint Matching and prove convergence to the global solution.
Notably, ASBS generalizes the recent Adjoint Sampling (Havens et al., 2025) to
arbitrary source distributions by relaxing the so-called memoryless condition
that largely restricts the design space. Through extensive experiments, we
demonstrate the effectiveness of ASBS on sampling from classical energy
functions, amortized conformer generation, and molecular Boltzmann
distributions.

</details>


### [553] [Bayesian Invariance Modeling of Multi-Environment Data](https://arxiv.org/abs/2506.22675)
*Luhuan Wu,Mingzhang Yin,Yixin Wang,John P. Cunningham,David M. Blei*

Main category: stat.ML

TL;DR: This paper introduces Bayesian Invariant Prediction (BIP), a probabilistic model to identify invariant features, improving accuracy and scalability over existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of previous invariant prediction methods such as hypothesis testing or regularized optimization, and to provide a probabilistic approach to identifying invariant features that support generalization and reveal causal mechanisms.

Method: The authors encode invariant feature indices as a latent variable in a Bayesian framework and recover them through posterior inference. They introduce a variational approximation (VI-BIP) for computational efficiency when dealing with many features.

Result: BIP is shown to produce consistent posterior results and faster convergence with increased environment heterogeneity. Experiments demonstrate BIP and VI-BIP outperform existing methods in accuracy and scalability.

Conclusion: BIP and its variational approximation offer a more precise and scalable solution to invariant prediction, facilitating better generalization and causal understanding.

Abstract: Invariant prediction [Peters et al., 2016] analyzes feature/outcome data from
multiple environments to identify invariant features - those with a stable
predictive relationship to the outcome. Such features support generalization to
new environments and help reveal causal mechanisms. Previous methods have
primarily tackled this problem through hypothesis testing or regularized
optimization. Here we develop Bayesian Invariant Prediction (BIP), a
probabilistic model for invariant prediction. BIP encodes the indices of
invariant features as a latent variable and recover them by posterior
inference. Under the assumptions of Peters et al. [2016], the BIP posterior
targets the true invariant features. We prove that the posterior is consistent
and that greater environment heterogeneity leads to faster posterior
contraction. To handle many features, we design an efficient variational
approximation called VI-BIP. In simulations and real data, we find that BIP and
VI-BIP are more accurate and scalable than existing methods for invariant
prediction.

</details>


### [554] [CN-SBM: Categorical Block Modelling For Primary and Residual Copy Number Variation](https://arxiv.org/abs/2506.22963)
*Kevin Lam,William Daniels,J Maxwell Douglas,Daniel Lai,Samuel Aparicio,Benjamin Bloem-Reddy,Yongjin Park*

Main category: stat.ML

TL;DR: The paper introduces a new probabilistic framework (CN-SBM) to analyze genetic cancer data by clustering based on copy number variants, achieving better results than existing models and aiding in identifying clinically relevant cancer subtypes.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the need for a model that can effectively analyze and interpret noisy genome-wide copy number variations (CNVs) in cancer research, particularly for tracking clonal evolution in tumors.

Method: The authors developed the Copy Number Stochastic Block Model (CN-SBM), a bipartite categorical block model that clusters samples and regions based on discrete CNV states, leveraging a scalable variational inference algorithm for large datasets.

Result: CN-SBM performs better than existing methods on simulated and real datasets. Application to TCGA glioma data uncovered clinically relevant subtypes and residual variations that enhanced patient classification and survival analysis.

Conclusion: CN-SBM provides a scalable, interpretable framework for analyzing CNV data, contributing insights into tumor heterogeneity and improving cancer prognosis and patient stratification.

Abstract: Cancer is a genetic disorder whose clonal evolution can be monitored by
tracking noisy genome-wide copy number variants. We introduce the Copy Number
Stochastic Block Model (CN-SBM), a probabilistic framework that jointly
clusters samples and genomic regions based on discrete copy number states using
a bipartite categorical block model. Unlike models relying on Gaussian or
Poisson assumptions, CN-SBM respects the discrete nature of CNV calls and
captures subpopulation-specific patterns through block-wise structure. Using a
two-stage approach, CN-SBM decomposes CNV data into primary and residual
components, enabling detection of both large-scale chromosomal alterations and
finer aberrations. We derive a scalable variational inference algorithm for
application to large cohorts and high-resolution data. Benchmarks on simulated
and real datasets show improved model fit over existing methods. Applied to
TCGA low-grade glioma data, CN-SBM reveals clinically relevant subtypes and
structured residual variation, aiding patient stratification in survival
analysis. These results establish CN-SBM as an interpretable, scalable
framework for CNV analysis with direct relevance for tumor heterogeneity and
prognosis.

</details>


### [555] [AICO: Feature Significance Tests for Supervised Learning](https://arxiv.org/abs/2506.23396)
*Kay Giesecke,Enguerrand Horel,Chartsiri Jirachotkulthorn*

Main category: stat.ML

TL;DR: The paper introduces a method to assess the importance of input features in supervised learning algorithms using a model- and distribution-agnostic approach.


<details>
  <summary>Details</summary>
Motivation: Many supervised learning algorithms are opaque, impeding scientific discovery and their deployment in sensitive domains. A method to explain feature importance is needed to address this challenge.

Method: The authors propose a feature significance test through masking feature values and employing a randomized sign test. The technique avoids retraining or auxiliary models and provides exact p-values and confidence intervals.

Result: The proposed method demonstrated statistical and computational efficiency in synthetic tasks and proved practically useful in real-world applications.

Conclusion: This method advances explainability in machine learning by providing an efficient and reliable way to assess feature importance, applicable across various models and distributions.

Abstract: The opacity of many supervised learning algorithms remains a key challenge,
hindering scientific discovery and limiting broader deployment -- particularly
in high-stakes domains. This paper develops model- and distribution-agnostic
significance tests to assess the influence of input features in any regression
or classification algorithm. Our method evaluates a feature's incremental
contribution to model performance by masking its values across samples. Under
the null hypothesis, the distribution of performance differences across a test
set has a non-positive median. We construct a uniformly most powerful,
randomized sign test for this median, yielding exact p-values for assessing
feature significance and confidence intervals with exact coverage for
estimating population-level feature importance. The approach requires minimal
assumptions, avoids model retraining or auxiliary models, and remains
computationally efficient even for large-scale, high-dimensional settings.
Experiments on synthetic tasks validate its statistical and computational
advantages, and applications to real-world data illustrate its practical
utility.

</details>


### [556] [DPOT: A DeepParticle method for Computation of Optimal Transport with convergence guarantee](https://arxiv.org/abs/2506.23429)
*Yingyuan Li,Aokun Wang,Zhongjian Wang*

Main category: stat.ML

TL;DR: The paper introduces a machine learning approach for computing optimal transport maps using DeepParticle methods, validated by theoretical and numerical results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of computing optimal transport maps between continuous distributions efficiently and accurately.

Method: A novel machine learning method involving DeepParticle techniques with a min-min optimization strategy is proposed, avoiding restrictions on network structures.

Result: Theoretical guarantees of weak convergence and error bounds are established, and numerical experiments corroborate the approach's efficacy, including on real-world tasks.

Conclusion: The study confirms the successful application of the proposed method in calculating optimal transport maps, backed by theoretical validations and practical performance.

Abstract: In this work, we propose a novel machine learning approach to compute the
optimal transport map between two continuous distributions from their unpaired
samples, based on the DeepParticle methods. The proposed method leads to a
min-min optimization during training and does not impose any restriction on the
network structure. Theoretically we establish a weak convergence guarantee and
a quantitative error bound between the learned map and the optimal transport
map. Our numerical experiments validate the theoretical results and the
effectiveness of the new approach, particularly on real-world tasks.

</details>


### [557] [Minimax Optimal Two-Stage Algorithm For Moment Estimation Under Covariate Shift](https://arxiv.org/abs/2506.23453)
*Zhen Zhang,Xin Liu,Shaoli Wang,Jiaye Teng*

Main category: stat.ML

TL;DR: The paper addresses the problem of estimating moments in covariate shift scenarios with known distributions, proposing an optimal two-stage algorithm and extending it to unknown distributions using a double robust estimator.


<details>
  <summary>Details</summary>
Motivation: Covariate shift frequently occurs in real-world scenarios, making accurate estimation of unknown function moments crucial but under-explored.

Method: The proposed method includes a two-stage algorithm: training an optimal estimator under the source distribution followed by likelihood ratio reweighting for calibration, with a robust extension for unknown distributions.

Result: The algorithm achieves the minimax optimal bound (up to a logarithmic factor), and numerical studies verify its theoretical claims and practical efficacy.

Conclusion: The paper introduces a novel approach for moment estimation under covariate shift, achieving theoretical limits and demonstrating robustness in practical scenarios.

Abstract: Covariate shift occurs when the distribution of input features differs
between the training and testing phases. In covariate shift, estimating an
unknown function's moment is a classical problem that remains under-explored,
despite its common occurrence in real-world scenarios. In this paper, we
investigate the minimax lower bound of the problem when the source and target
distributions are known. To achieve the minimax optimal bound (up to a
logarithmic factor), we propose a two-stage algorithm. Specifically, it first
trains an optimal estimator for the function under the source distribution, and
then uses a likelihood ratio reweighting procedure to calibrate the moment
estimator. In practice, the source and target distributions are typically
unknown, and estimating the likelihood ratio may be unstable. To solve this
problem, we propose a truncated version of the estimator that ensures double
robustness and provide the corresponding upper bound. Extensive numerical
studies on synthetic examples confirm our theoretical findings and further
illustrate the effectiveness of our proposed method.

</details>


### [558] [Test of partial effects for Frechet regression on Bures-Wasserstein manifolds](https://arxiv.org/abs/2506.23487)
*Haoshu Xu,Hongzhe Li*

Main category: stat.ML

TL;DR: The paper introduces a novel statistical test for evaluating partial effects in Frechet regression on Bures Wasserstein manifolds, with provable theoretical and practical effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the need for a rigorous statistical test for assessing partial effects in the complex setting of Frechet regression on Bures Wasserstein manifolds.

Method: The method involves a sample-splitting strategy where one subsample is used to fit the Frechet regression model, and the other subsample is used to construct the test statistic. Theoretical analysis ensures convergence properties and effectiveness.

Result: The test statistic converges asymptotically to a weighted mixture of chi-squared components. Nominal asymptotic size is achieved, and worst-case power converges to one uniformly.

Conclusion: The proposed test shows both strong theoretical foundations and practical applicability, demonstrated through simulations and real-world data analysis.

Abstract: We propose a novel test for assessing partial effects in Frechet regression
on Bures Wasserstein manifolds. Our approach employs a sample splitting
strategy: the first subsample is used to fit the Frechet regression model,
yielding estimates of the covariance matrices and their associated optimal
transport maps, while the second subsample is used to construct the test
statistic. We prove that this statistic converges in distribution to a weighted
mixture of chi squared components, where the weights correspond to the
eigenvalues of an integral operator defined by an appropriate RKHS kernel. We
establish that our procedure achieves the nominal asymptotic size and
demonstrate that its worst-case power converges uniformly to one. Through
extensive simulations and a real data application, we illustrate the test's
finite-sample accuracy and practical utility.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [559] [Seeding neural network quantum states with tensor network states](https://arxiv.org/abs/2506.23550)
*Ryui Kaneko,Shimpei Goto*

Main category: cond-mat.str-el

TL;DR: The paper introduces an approach to convert matrix product states (MPSs) into neural network wave functions through CP decomposition, useful for quantum ground-state calculations.


<details>
  <summary>Details</summary>
Motivation: To efficiently generate initial neural network quantum states for quantum many-body systems and improve the accuracy of ground-state calculations.

Method: The authors use Canonical Polyadic (CP) decomposition on matrix product states (MPSs) to convert them into restricted Boltzmann machine wave functions with multinomial hidden units.

Result: The method achieved efficient initialization and improved closeness to the ground states, demonstrated on a transverse-field Ising model.

Conclusion: This CP decomposition-based approach is effective and has potential applications in complex quantum many-body systems with intricate ground-state wave function structures.

Abstract: We find an efficient approach to approximately convert matrix product states
(MPSs) into restricted Boltzmann machine wave functions consisting of a
multinomial hidden unit through a canonical polyadic (CP) decomposition of the
MPSs. This method allows us to generate well-behaved initial neural network
quantum states for quantum many-body ground-state calculations in polynomial
time of the number of variational parameters and systematically shorten the
distance between the initial states and the ground states with increasing the
rank of the CP decomposition. We demonstrate the efficiency of our method by
taking the transverse-field Ising model as an example and discuss possible
applications of our method to more general quantum many-body systems in which
the ground-state wave functions possess complex nodal structures.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [560] [Programming Soft Robots with Flexible Mechanical Metamaterials](https://arxiv.org/abs/1906.00306)
*Ahmad Rafsanjani,Katia Bertoldi,André R. Studart*

Main category: cond-mat.soft

TL;DR: Highly deformable mechanical metamaterials improve the functionality of soft robots.


<details>
  <summary>Details</summary>
Motivation: To investigate how mechanical metamaterials can enhance soft robot performance.

Method: The paper studies the behavior and application of deformable mechanical metamaterials in robotics.

Result: Demonstration of metamaterials' potential in improving soft robots' capabilities.

Conclusion: Mechanical metamaterials are valuable advancements for enhancing soft robot efficiency and functionality.

Abstract: The complex behavior of highly deformable mechanical metamaterials can
substantially enhance the performance of soft robots.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [561] [Not quite a piece of CHERI-cake: Are new digital security by design architectures usable?](https://arxiv.org/abs/2506.23682)
*Maysara Alhindi,Joseph Hallett*

Main category: cs.CR

TL;DR: The paper explores CHERI, a digital security-by-design architecture aimed at improving memory safety, and identifies usability challenges faced by developers using it.


<details>
  <summary>Details</summary>
Motivation: The authors aim to examine the impact of CHERI on programming practices, specifically how it changes assumptions about C language and hardware implementation, while addressing usability challenges.

Method: A usability study was conducted where developers ported software to CHERI to observe their reactions to warnings, errors, and documentation changes.

Result: The study found developers struggle with CHERI’s warning/error mechanisms and the limited scope of its documentation.

Conclusion: While CHERI enhances memory safety, there are significant usability barriers, particularly in error displays and documentation, that need addressing to improve developer experience.

Abstract: A digital security-by-design computer architecture, like CHERI, lets you
program without fear of buffer overflows or other memory safety errors, but
CHERI also rewrites some of the assumptions about how C works and how
fundamental types (such as pointers) are implemented in hardware. We conducted
a usability study to examine how developers react to the changes required by
CHERI when porting software to run on it. We find that developers struggle with
CHERI's display of warnings and errors and a lack of diverse documentation.

</details>


### [562] [Detect \& Score: Privacy-Preserving Misbehaviour Detection and Contribution Evaluation in Federated Learning](https://arxiv.org/abs/2506.23583)
*Marvin Xhemrishi,Alexandre Graell i Amat,Balázs Pejó*

Main category: cs.CR

TL;DR: This study enhances federated learning with secure aggregation by integrating two methods (QI and FedGT) to improve both misbehavior detection (MD) and contribution evaluation (CE).


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges in federated learning, specifically in detecting malicious client behaviors and evaluating individual client contributions, compounded by secure aggregation protocols.

Method: The researchers combine two approaches—QI for contribution evaluation and FedGT for misbehavior detection—to address the limitations of both and create a unified solution.

Result: The integrated approach demonstrates better MD and CE performance in experiments compared to using QI or FedGT individually.

Conclusion: The study successfully combines QI and FedGT to achieve robust misbehavior detection and accurate contribution evaluation in federated learning settings.

Abstract: Federated learning with secure aggregation enables private and collaborative
learning from decentralised data without leaking sensitive client information.
However, secure aggregation also complicates the detection of malicious client
behaviour and the evaluation of individual client contributions to the
learning. To address these challenges, QI (Pejo et al.) and FedGT (Xhemrishi et
al.) were proposed for contribution evaluation (CE) and misbehaviour detection
(MD), respectively. QI, however, lacks adequate MD accuracy due to its reliance
on the random selection of clients in each training round, while FedGT lacks
the CE ability. In this work, we combine the strengths of QI and FedGT to
achieve both robust MD and accurate CE. Our experiments demonstrate superior
performance compared to using either method independently.

</details>


### [563] [VERA: Variational Inference Framework for Jailbreaking Large Language Models](https://arxiv.org/abs/2506.22666)
*Anamika Lochab,Lu Yan,Patrick Pynadath,Xiangyu Zhang,Ruqi Zhang*

Main category: cs.CR

TL;DR: The paper proposes VERA, an attacker LLM framework for generating adversarial prompts to uncover vulnerabilities in black-box API-only large language models.


<details>
  <summary>Details</summary>
Motivation: API-only access to leading LLMs necessitates effective methods for identifying model vulnerabilities, given the limitations of current jailbreaking strategies like genetic algorithms.

Method: The paper introduces VERA, a variational inference-based framework which trains a small attacker LLM to approximate the target LLM's posterior and generate diverse jailbreak prompts without re-optimization.

Result: VERA demonstrates strong performance across multiple state-of-the-art target LLMs, producing effective and diverse adversarial prompts.

Conclusion: Probabilistic inference methods, like VERA, can efficiently and effectively characterize vulnerabilities in black-box LLMs, offering a robust alternative to previous techniques.

Abstract: The rise of API-only access to state-of-the-art LLMs highlights the need for
effective black-box jailbreak methods to identify model vulnerabilities in
real-world settings. Without a principled objective for gradient-based
optimization, most existing approaches rely on genetic algorithms, which are
limited by their initialization and dependence on manually curated prompt
pools. Furthermore, these methods require individual optimization for each
prompt, failing to provide a comprehensive characterization of model
vulnerabilities. To address this gap, we introduce VERA: Variational infErence
fRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a
variational inference problem, training a small attacker LLM to approximate the
target LLM's posterior over adversarial prompts. Once trained, the attacker can
generate diverse, fluent jailbreak prompts for a target query without
re-optimization. Experimental results show that VERA achieves strong
performance across a range of target LLMs, highlighting the value of
probabilistic inference for adversarial prompt generation.

</details>


### [564] [General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers](https://arxiv.org/abs/2506.22706)
*Arun Ramamurthy,Neil Dhir*

Main category: cs.CR

TL;DR: This paper investigates methods for creating generalizable autonomous cybersecurity defense systems that adapt to dynamic network changes.


<details>
  <summary>Details</summary>
Motivation: Current ACD systems often fail because they rely on static or stationary network dynamics, limiting their adaptive capabilities in real-world scenarios with dynamic topologies.

Method: The authors explore designing agents capable of learning policies that generalize across various dynamic network environments.

Result: Findings suggest that agents trained to handle evolving network conditions can mitigate overfitting and improve generalization to unseen topologies.

Conclusion: Developing general ACD systems enhances cybersecurity defense capabilities by enabling adaptive responses to intricate and evolving network threats.

Abstract: In the face of evolving cyber threats such as malware, ransomware and
phishing, autonomous cybersecurity defense (ACD) systems have become essential
for real-time threat detection and response with optional human intervention.
However, existing ACD systems rely on limiting assumptions, particularly the
stationarity of the underlying network dynamics. In real-world scenarios,
network topologies can change due to actions taken by attackers or defenders,
system failures, or time evolution of networks, leading to failures in the
adaptive capabilities of current defense agents. Moreover, many agents are
trained on static environments, resulting in overfitting to specific
topologies, which hampers their ability to generalize to out-of-distribution
network topologies. This work addresses these challenges by exploring methods
for developing agents to learn generalizable policies across dynamic network
environments -- general ACD (GACD).

</details>


### [565] [Threadbox: Sandboxing for Modular Security](https://arxiv.org/abs/2506.23683)
*Maysara Alhindi,Joseph Hallett*

Main category: cs.CR

TL;DR: The paper explores challenges with existing OS sandboxing mechanisms and proposes a new approach called Threadbox, enabling modular thread-specific sandboxes.


<details>
  <summary>Details</summary>
Motivation: Developers often need to extensively refactor their applications to adapt to existing sandboxing models in operating systems, creating practical challenges.

Method: The authors propose Threadbox, a novel mechanism that creates modular, independent sandboxes applicable to threads and specific functions.

Result: Threadbox demonstrates its utility and limitations through case studies showcasing improved sandbox flexibility for diverse scenarios.

Conclusion: Threadbox offers a promising alternative to traditional sandboxing mechanisms, allowing greater modularity and reducing challenges in application adaptation.

Abstract: There are many sandboxing mechanisms provided by operating systems to limit
what resources applications can access, however, sometimes the use of these
mechanisms requires developers to refactor their code to fit the sandboxing
model. In this work, we investigate what makes existing sandboxing mechanisms
challenging to apply to certain types of applications, and propose Threadbox, a
sandboxing mechanism that enables having modular and independent sandboxes, and
can be applied to threads and sandbox specific functions. We present case
studies to illustrate the applicability of the idea and discuss its
limitations.

</details>


### [566] [An ontological lens on attack trees: Toward adequacy and interoperability](https://arxiv.org/abs/2506.23841)
*Ítalo Oliveira,Stefano M. Nicoletti,Gal Engelberg,Mattia Fumagalli,Dan Klein,Giancarlo Guizzardi*

Main category: cs.CR

TL;DR: Attack Trees (AT) are widely used for security analysis, but their lack of strong ontological foundations results in four key issues: ambiguous terms, missing critical concepts, insufficient modeling guidance, and lack of semantic interoperability.


<details>
  <summary>Details</summary>
Motivation: To analyze the limitations of the Attack Trees (AT) language in security modeling and identify ontological shortcomings that hamper its effectiveness.

Method: This paper employs an ontological analysis grounded in the Common Ontology of Value and Risk (COVER), which is based on the Unified Foundational Ontology (UFO), to investigate AT's ontological inadequacies.

Result: The study identifies four significant shortcomings in AT: ambiguous syntax, a deficit of domain-specific concepts, insufficient guidance for goal decomposition, and a lack of semantic interoperability.

Conclusion: The analysis highlights that overcoming these limitations requires a broader approach to risk management modeling and suggests existing incremental solutions as interim fixes.

Abstract: Attack Trees (AT) are a popular formalism for security analysis. They are
meant to display an attacker's goal decomposed into attack steps needed to
achieve it and compute certain security metrics (e.g., attack cost,
probability, and damage). ATs offer three important services: (a) conceptual
modeling capabilities for representing security risk management scenarios, (b)
a qualitative assessment to find root causes and minimal conditions of
successful attacks, and (c) quantitative analyses via security metrics
computation under formal semantics, such as minimal time and cost among all
attacks. Still, the AT language presents limitations due to its lack of
ontological foundations, thus compromising associated services. Via an
ontological analysis grounded in the Common Ontology of Value and Risk (COVER)
-- a reference core ontology based on the Unified Foundational Ontology (UFO)
-- we investigate the ontological adequacy of AT and reveal four significant
shortcomings: (1) ambiguous syntactical terms that can be interpreted in
various ways; (2) ontological deficit concerning crucial domain-specific
concepts; (3) lacking modeling guidance to construct ATs decomposing a goal;
(4) lack of semantic interoperability, resulting in ad hoc stand-alone tools.
We also discuss existing incremental solutions and how our analysis paves the
way for overcoming those issues through a broader approach to risk management
modeling.

</details>


### [567] [Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions](https://arxiv.org/abs/2506.23866)
*Jason Kayembe,Iness Ben Guirat,Jan Tobias Mühlberg*

Main category: cs.CR

TL;DR: The paper examines the environmental consequences of cloud-based email services using a framework analyzing energy consumption and carbon emissions. Results indicate that privacy-focused services such as Proton Mail and self-hosted solutions are more environmentally efficient than advertising-driven models like Gmail and Outlook.


<details>
  <summary>Details</summary>
Motivation: To investigate the link between privacy-centric service models and their environmental sustainability, hypothesizing that services protecting user privacy are more energy-efficient than advertising-funded alternatives.

Method: A systematic framework was created to measure energy use and carbon emissions of email services during automated usage scenarios. The study applies this to analyze three platforms—Microsoft Outlook, Google Mail, and Proton Mail—plus a self-hosted email server with and without end-to-end encryption.

Result: The self-hosted email solution was the most energy-efficient, with 33% lower emissions compared to Gmail, even accounting for encryption overheads. Among commercial providers, Proton Mail showed superior efficiency, outperforming Outlook and Gmail, though ad-blocking reduced Outlook emissions slightly.

Conclusion: Privacy-driven email services not only protect user data but also have a lower environmental impact, highlighting their dual advantages of sustainability and privacy. Self-hosted and privacy-focused services are more efficient than advertising-based alternatives.

Abstract: In this paper, we explore the intersection of privacy, security, and
environmental sustainability in cloud-based office solutions, focusing on
quantifying user- and network-side energy use and associated carbon emissions.
We hypothesise that privacy-focused services are typically more
energy-efficient than those funded through data collection and advertising. To
evaluate this, we propose a framework that systematically measures
environmental costs based on energy usage and network data traffic during
well-defined, automated usage scenarios. To test our hypothesis, we first
analyse how underlying architectures and business models, such as monetisation
through personalised advertising, contribute to the environmental footprint of
these services. We then explore existing methodologies and tools for software
environmental impact assessment. We apply our framework to three mainstream
email services selected to reflect different privacy policies, from
ad-supported tracking-intensive models to privacy-focused designs: Microsoft
Outlook, Google Mail (Gmail), and Proton Mail. We extend this comparison to a
self-hosted email solution, evaluated with and without end-to-end encryption.
We show that the self-hosted solution, even with 14% of device energy and 15%
of emissions overheads from PGP encryption, remains the most energy-efficient,
saving up to 33% of emissions per session compared to Gmail. Among commercial
providers, Proton Mail is the most efficient, saving up to 0.1 gCO2 e per
session compared to Outlook, whose emissions can be further reduced by 2%
through ad-blocking.

</details>


### [568] [SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning](https://arxiv.org/abs/2506.22506)
*Momin Ahmad Khan,Yasra Chandio,Fatima Muhammad Anwar*

Main category: cs.CR

TL;DR: This paper introduces and addresses backdoor attacks in Federated Prompt Learning for large vision-language models like CLIP.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the security vulnerabilities in Federated Prompt Learning, specifically the susceptibility to backdoor attacks.

Method: The authors propose SABRE-FL, an embedding-space anomaly detector trained offline to filter poisoned updates without requiring client data or labels.

Result: SABRE-FL outperforms baseline defenses by significantly reducing backdoor accuracy while maintaining clean accuracy across diverse datasets.

Conclusion: Robust prompt learning in federated systems is critical, and SABRE-FL provides an effective solution against backdoor threats.

Abstract: Federated Prompt Learning has emerged as a communication-efficient and
privacy-preserving paradigm for adapting large vision-language models like CLIP
across decentralized clients. However, the security implications of this setup
remain underexplored. In this work, we present the first study of backdoor
attacks in Federated Prompt Learning. We show that when malicious clients
inject visually imperceptible, learnable noise triggers into input images, the
global prompt learner becomes vulnerable to targeted misclassification while
still maintaining high accuracy on clean inputs. Motivated by this
vulnerability, we propose SABRE-FL, a lightweight, modular defense that filters
poisoned prompt updates using an embedding-space anomaly detector trained
offline on out-of-distribution data. SABRE-FL requires no access to raw client
data or labels and generalizes across diverse datasets. We show, both
theoretically and empirically, that malicious clients can be reliably
identified and filtered using an embedding-based detector. Across five diverse
datasets and four baseline defenses, SABRE-FL outperforms all baselines by
significantly reducing backdoor accuracy while preserving clean accuracy,
demonstrating strong empirical performance and underscoring the need for robust
prompt learning in future federated systems.

</details>


### [569] [In-context learning for the classification of manipulation techniques in phishing emails](https://arxiv.org/abs/2506.22515)
*Antony Dalmiere,Guillaume Auriol,Vincent Nicomette,Pascal Marchand*

Main category: cs.CR

TL;DR: The study explores using GPT-4o-mini with In-Context Learning (ICL) for detecting 40 specific manipulation techniques in French phishing emails, achieving an accuracy of 0.76.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for phishing detection often ignore the psychological manipulation tactics used by attackers. The researchers aimed to fill this gap by classifying phishing emails based on manipulation techniques.

Method: The authors utilized GPT-4o-mini with few-shot examples in an In-Context Learning (ICL) setting, applying this strategy to a dataset of real-world French phishing emails and validating against a human-annotated test set.

Result: The study demonstrated that the approach effectively identified common manipulation techniques like Baiting, Curiosity Appeal, and Request For Minor Favor, achieving an accuracy of 0.76.

Conclusion: ICL using LLMs like GPT-4o-mini shows significant potential for detailed phishing email analysis, enhancing understanding of attacker strategies and psychological manipulation methods.

Abstract: Traditional phishing detection often overlooks psychological manipulation.
This study investigates using Large Language Model (LLM) In-Context Learning
(ICL) for fine-grained classification of phishing emails based on a taxonomy of
40 manipulation techniques. Using few-shot examples with GPT-4o-mini on
real-world French phishing emails (SignalSpam), we evaluated performance
against a human-annotated test set (100 emails). The approach effectively
identifies prevalent techniques (e.g., Baiting, Curiosity Appeal, Request For
Minor Favor) with a promising accuracy of 0.76. This work demonstrates ICL's
potential for nuanced phishing analysis and provides insights into attacker
strategies.

</details>


### [570] [A Survey on Model Extraction Attacks and Defenses for Large Language Models](https://arxiv.org/abs/2506.22521)
*Kaixiang Zhao,Lincan Li,Kaize Ding,Neil Zhenqiang Gong,Yue Zhao,Yushun Dong*

Main category: cs.CR

TL;DR: The paper surveys model extraction attacks on large language models (LLMs) and defenses, offering a taxonomy and highlighting key research directions.


<details>
  <summary>Details</summary>
Motivation: To address the growing threat of model extraction attacks on large language models, which compromise intellectual property and user privacy.

Method: The authors categorize attacks (functionality extraction, training data extraction, prompt-targeted attacks) and defense strategies (model, data, and prompt protection), proposing specialized evaluation metrics.

Result: The analysis identifies key limitations in current approaches and outlines new directions like integrated attack methods and adaptive defenses.

Conclusion: The work provides a framework for understanding and improving the security of LLMs, beneficial to researchers and security professionals.

Abstract: Model extraction attacks pose significant security threats to deployed
language models, potentially compromising intellectual property and user
privacy. This survey provides a comprehensive taxonomy of LLM-specific
extraction attacks and defenses, categorizing attacks into functionality
extraction, training data extraction, and prompt-targeted attacks. We analyze
various attack methodologies including API-based knowledge distillation, direct
querying, parameter recovery, and prompt stealing techniques that exploit
transformer architectures. We then examine defense mechanisms organized into
model protection, data privacy protection, and prompt-targeted strategies,
evaluating their effectiveness across different deployment scenarios. We
propose specialized metrics for evaluating both attack effectiveness and
defense performance, addressing the specific challenges of generative language
models. Through our analysis, we identify critical limitations in current
approaches and propose promising research directions, including integrated
attack methodologies and adaptive defense mechanisms that balance security with
model utility. This work serves NLP researchers, ML engineers, and security
professionals seeking to protect language models in production environments.

</details>


### [571] [Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks](https://arxiv.org/abs/2506.22722)
*Anmin Fu,Fanyu Meng,Huaibing Peng,Hua Ma,Zhi Zhang,Yifeng Zheng,Willy Susilo,Yansong Gao*

Main category: cs.CR

TL;DR: UniGuard is a unified online detection framework designed to simultaneously address adversarial examples (AEs) and backdoor attacks during inference by analyzing propagation trajectories with LSTM and spectral transformation.


<details>
  <summary>Details</summary>
Motivation: To create a unified solution capable of detecting both adversarial examples and backdoor attacks in deep learning systems, overcoming current challenges in efficiently and effectively identifying these sophisticated threats.

Method: UniGuard detects threats by treating propagation trajectories of adversarial inputs in deep learning models as time-series signals, utilizing LSTM and spectrum transformation to highlight subtle differences from benign trajectories.

Result: UniGuard outperforms state-of-the-art methods tailored to either adversarial example detection or backdoor detection across diverse modalities, model architectures, and attack strategies.

Conclusion: UniGuard sets a new benchmark for unified detection frameworks by successfully addressing threats from adversarial examples and backdoor attacks simultaneously, offering unmatched efficiency and effectiveness against current state-of-the-art methods.

Abstract: The proposed UniGuard is the first unified online detection framework capable
of simultaneously addressing adversarial examples and backdoor attacks.
UniGuard builds upon two key insights: first, both AE and backdoor attacks have
to compromise the inference phase, making it possible to tackle them
simultaneously during run-time via online detection. Second, an adversarial
input, whether a perturbed sample in AE attacks or a trigger-carrying sample in
backdoor attacks, exhibits distinctive trajectory signatures from a benign
sample as it propagates through the layers of a DL model in forward inference.
The propagation trajectory of the adversarial sample must deviate from that of
its benign counterpart; otherwise, the adversarial objective cannot be
fulfilled. Detecting these trajectory signatures is inherently challenging due
to their subtlety; UniGuard overcomes this by treating the propagation
trajectory as a time-series signal, leveraging LSTM and spectrum transformation
to amplify differences between adversarial and benign trajectories that are
subtle in the time domain. UniGuard exceptional efficiency and effectiveness
have been extensively validated across various modalities (image, text, and
audio) and tasks (classification and regression), ranging from diverse model
architectures against a wide range of AE attacks and backdoor attacks,
including challenging partial backdoors and dynamic triggers. When compared to
SOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED
(IEEE SP 24) specific for backdoor detection, UniGuard consistently
demonstrates superior performance, even when matched against each method's
strengths in addressing their respective threats-each SOTA fails to parts of
attack strategies while UniGuard succeeds for all.

</details>


### [572] [Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models](https://arxiv.org/abs/2506.24056)
*Tung-Ling Li,Hongliang Liu*

Main category: cs.CR

TL;DR: Logit-gap steering is a novel jailbreak framework that manipulates RLHF-aligned language models through an efficient single-pass vocabulary analysis, boosting attack success rates substantially while maintaining coherence.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome refusal-affirmation gaps in RLHF-aligned models, enabling efficient jailbreaks to probe language model safety and alignment artifacts.

Method: The paper introduces a forward-computable scoring system that combines logit-gap adjustments, KL penalty, and reward shift proxies, enabling quick 'sort-sum-stop' suffix generation.

Result: The method achieves 80-100% one-shot attack success rates on checkpoints ranging from 0.5B to 70B, requiring significantly fewer model calls than traditional methods.

Conclusion: The approach efficiently uncovers vulnerabilities in language model alignments and provides insights into the internal effects of safety tuning.

Abstract: We introduce logit-gap steering, a fast jailbreak framework that casts the
refusal-affirmation gap of RLHF-aligned language models as a single pass over
the vocabulary. A forward-computable score blends gap reduction with
lightweight proxies for KL penalty and reward shift, allowing a "sort-sum-stop"
sweep to complete in under a second and return a short suffix--two orders of
magnitude fewer model calls than beam or gradient attacks. The same suffix
generalises to unseen prompts and scales from 0.5 B to 70 B checkpoints,
lifting one-shot attack success from baseline levels to 80-100% while
preserving topical coherence. Beyond efficiency, these suffixes expose
sentence-boundary reward cliffs and other alignment artefacts, offering a
lightweight probe into how safety tuning reshapes internal representations.

</details>


### [573] [A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance](https://arxiv.org/abs/2506.22949)
*Ehsan Hallaji,Vaishnavi Shanmugam,Roozbeh Razavi-Far,Mehrdad Saif*

Main category: cs.CR

TL;DR: The paper explores using 13 Semi-Supervised Learning algorithms to detect Distributed Denial of Service (DDoS) attacks in scenarios with class imbalance and limited labeled data.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in detecting DDoS attacks due to class imbalance and insufficient labeled real-world samples.

Method: Evaluate 13 state-of-the-art Semi-Supervised Learning techniques for DDoS detection across various conditions, emphasizing their effectiveness and limitations.

Result: The study identifies how well these algorithms perform under extreme conditions and their practical efficacy.

Conclusion: Findings contribute to developing Intrusion Detection Systems that are resilient to class imbalance and capable of handling partially labeled datasets.

Abstract: One of the most difficult challenges in cybersecurity is eliminating
Distributed Denial of Service (DDoS) attacks. Automating this task using
artificial intelligence is a complex process due to the inherent class
imbalance and lack of sufficient labeled samples of real-world datasets. This
research investigates the use of Semi-Supervised Learning (SSL) techniques to
improve DDoS attack detection when data is imbalanced and partially labeled. In
this process, 13 state-of-the-art SSL algorithms are evaluated for detecting
DDoS attacks in several scenarios. We evaluate their practical efficacy and
shortcomings, including the extent to which they work in extreme environments.
The results will offer insight into designing intelligent Intrusion Detection
Systems (IDSs) that are robust against class imbalance and handle partially
labeled data.

</details>


### [574] [MetaCipher: A General and Extensible Reinforcement Learning Framework for Obfuscation-Based Jailbreak Attacks on Black-Box LLMs](https://arxiv.org/abs/2506.22557)
*Boyuan Chen,Minghao Shao,Abdul Basit,Siddharth Garg,Muhammad Shafique*

Main category: cs.CR

TL;DR: MetaCipher introduces a novel method to conduct sophisticated jailbreak attacks on LLMs using dynamic, reinforcement-learning-based cipher selection, achieving high attack success rates against both non-reasoning and reasoning-capable LLMs.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the vulnerability of LLMs to obfuscation-based attacks, wherein encrypted prompts are used to bypass conventional safety mechanisms that cannot decode ciphered content.

Method: MetaCipher employs a modular framework with reinforcement learning to dynamically select optimal encryption strategies from a pool of ciphers, maximizing attack success and adaptability across various LLM targets and tasks.

Result: MetaCipher achieves over 92% attack success rate on standard benchmarks for non-reasoning LLMs and over 74% ASR against reasoning-capable LLMs, outperforming prior obfuscation-based methods.

Conclusion: MetaCipher demonstrates long-term robustness and adaptability, highlighting the need for evolved safety mechanisms in LLMs to combat such sophisticated jailbreak strategies.

Abstract: The growing capabilities of large language models (LLMs) have exposed them to
increasingly sophisticated jailbreak attacks. Among these, obfuscation-based
attacks -- which encrypt malicious content to evade detection -- remain highly
effective. By leveraging the reasoning ability of advanced LLMs to interpret
encrypted prompts, such attacks circumvent conventional defenses that rely on
keyword detection or context filtering. These methods are very difficult to
defend against, as existing safety mechanisms are not designed to interpret or
decode ciphered content. In this work, we propose \textbf{MetaCipher}, a novel
obfuscation-based jailbreak framework, along with a reinforcement
learning-based dynamic cipher selection mechanism that adaptively chooses
optimal encryption strategies from a cipher pool. This approach enhances
jailbreak effectiveness and generalizability across diverse task types, victim
LLMs, and safety guardrails. Our framework is modular and extensible by design,
supporting arbitrary cipher families and accommodating evolving adversarial
strategies. We complement our method with a large-scale empirical analysis of
cipher performance across multiple victim LLMs. Within as few as 10 queries,
MetaCipher achieves over 92\% attack success rate (ASR) on most recent standard
malicious prompt benchmarks against state-of-the-art non-reasoning LLMs, and
over 74\% ASR against reasoning-capable LLMs, outperforming all existing
obfuscation-based jailbreak methods. These results highlight the long-term
robustness and adaptability of our approach, making it more resilient than
prior methods in the face of advancing safety measures.

</details>


### [575] [A User-Centric, Privacy-Preserving, and Verifiable Ecosystem for Personal Data Management and Utilization](https://arxiv.org/abs/2506.22606)
*Osama Zafar,Mina Namazi,Yuqiao Xu,Youngjin Yoo,Erman Ayday*

Main category: cs.CR

TL;DR: The paper proposes a decentralized, privacy-preserving system addressing issues tied to centralized personal data management, leveraging advanced technologies like secure enclaves and federated learning.


<details>
  <summary>Details</summary>
Motivation: To address significant privacy, security, and autonomy concerns tied to centralized management of personal data in sectors like education, healthcare, and finance.

Method: The design of a decentralized architecture using privacy-enhancing technologies such as secure enclaves and federated learning to enable secure computations, data verification, and privacy-preserving sharing.

Result: The system provides complete data control to users and enables secure, credible, and privacy-preserving functionalities across multiple data types and use cases.

Conclusion: Decentralized architectures with advanced privacy technologies can overcome the privacy and security vulnerabilities of traditional systems, ensuring greater user autonomy and data protection.

Abstract: In the current paradigm of digital personalized services, the centralized
management of personal data raises significant privacy concerns, security
vulnerabilities, and diminished individual autonomy over sensitive information.
Despite their efficiency, traditional centralized architectures frequently fail
to satisfy rigorous privacy requirements and expose users to data breaches and
unauthorized access risks. This pressing challenge calls for a fundamental
paradigm shift in methodologies for collecting, storing, and utilizing personal
data across diverse sectors, including education, healthcare, and finance.
  This paper introduces a novel decentralized, privacy-preserving architecture
that handles heterogeneous personal information, ranging from educational
credentials to health records and financial data. Unlike traditional models,
our system grants users complete data ownership and control, allowing them to
selectively share information without compromising privacy. The architecture's
foundation comprises advanced privacy-enhancing technologies, including secure
enclaves and federated learning, enabling secure computation, verification, and
data sharing. The system supports diverse functionalities, including local
computation, model training, and privacy-preserving data sharing, while
ensuring data credibility and robust user privacy.

</details>


### [576] [Efficient Cybersecurity Assessment Using SVM and Fuzzy Evidential Reasoning for Resilient Infrastructure](https://arxiv.org/abs/2506.22938)
*Zaydon L. Ali,Wassan Saad Abduljabbar Hayale,Israa Ibraheem Al_Barazanchi,Ravi Sekhar,Pritesh Shah,Sushma Parihar*

Main category: cs.CR

TL;DR: The paper addresses issues in selecting encryption models and assessing digital information security by proposing a risk assessment model using SVM and fuzzy evidential reasoning.


<details>
  <summary>Details</summary>
Motivation: The increasing problem of protecting sensitive digital information with advancements in hypermedia knowledge necessitates better methods for identifying suitable security protocols.

Method: The authors propose a security phase exposure model using Support Vector Machine (SVM) and fuzzy evidential reasoning (ER) to enable systematic risk assessment.

Result: The proposed model is analyzed using metrics such as recall, F1 score, and accuracy, demonstrating its effectiveness in faster and precise identification of security algorithms.

Conclusion: The model provides a systematic means of assessing security algorithms and overcoming uncertainties in data processing and security, offering practical utility for systematic risk evaluations.

Abstract: With current advancement in hybermedia knowledges, the privacy of digital
information has developed a critical problem. To overawed the susceptibilities
of present security protocols, scholars tend to focus mainly on efforts on
alternation of current protocols. Over past decade, various proposed encoding
models have been shown insecurity, leading to main threats against significant
data. Utilizing the suitable encryption model is very vital means of guard
against various such, but algorithm is selected based on the dependency of data
which need to be secured. Moreover, testing potentiality of the security
assessment one by one to identify the best choice can take a vital time for
processing. For faster and precisive identification of assessment algorithm, we
suggest a security phase exposure model for cipher encryption technique by
invoking Support Vector Machine (SVM). In this work, we form a dataset using
usual security components like contrast, homogeneity. To overcome the
uncertainty in analysing the security and lack of ability of processing data to
a risk assessment mechanism. To overcome with such complications, this paper
proposes an assessment model for security issues using fuzzy evidential
reasoning (ER) approaches. Significantly, the model can be utilised to process
and assemble risk assessment data on various aspects in systematic ways. To
estimate the performance of our framework, we have various analyses like,
recall, F1 score and accuracy.

</details>


### [577] [From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows](https://arxiv.org/abs/2506.23260)
*Mohamed Amine Ferrag,Norbert Tihanyi,Djallel Hamouda,Leandros Maglaras,Merouane Debbah*

Main category: cs.CR

TL;DR: This paper develops a unified threat model for ecosystems of LLM-driven autonomous AI agents, highlighting vulnerabilities and proposing future research directions for secure and robust workflows.


<details>
  <summary>Details</summary>
Motivation: The integration of LLMs with structured functions has dramatically advanced AI agent capabilities. However, the rapid adoption of these technologies has created security vulnerabilities and a lack of cohesive discovery mechanisms.

Method: The authors cataloged over thirty attack techniques and organized them into four domains (Input Manipulation, Model Compromise, System and Privacy Attacks, and Protocol Vulnerabilities). They assessed real-world scenarios and defenses for each threat.

Result: A structured threat model was established, and representative scenarios were analyzed. Key open challenges and research directions were highlighted, aiming at securing LLM-agent communications.

Conclusion: The paper provides a foundational reference for improving the robustness of LLM-agent ecosystems, offering insights into securing interactions and minimizing vulnerabilities in these systems.

Abstract: Autonomous AI agents powered by large language models (LLMs) with structured
function-calling interfaces have dramatically expanded capabilities for
real-time data retrieval, complex computation, and multi-step orchestration.
Yet, the explosive proliferation of plugins, connectors, and inter-agent
protocols has outpaced discovery mechanisms and security practices, resulting
in brittle integrations vulnerable to diverse threats. In this survey, we
introduce the first unified, end-to-end threat model for LLM-agent ecosystems,
spanning host-to-tool and agent-to-agent communications, formalize adversary
capabilities and attacker objectives, and catalog over thirty attack
techniques. Specifically, we organized the threat model into four domains:
Input Manipulation (e.g., prompt injections, long-context hijacks, multimodal
adversarial inputs), Model Compromise (e.g., prompt- and parameter-level
backdoors, composite and encrypted multi-backdoors, poisoning strategies),
System and Privacy Attacks (e.g., speculative side-channels, membership
inference, retrieval poisoning, social-engineering simulations), and Protocol
Vulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent
Communication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent
(A2A) protocol). For each category, we review representative scenarios, assess
real-world feasibility, and evaluate existing defenses. Building on our threat
taxonomy, we identify key open challenges and future research directions, such
as securing MCP deployments through dynamic trust management and cryptographic
provenance tracking; designing and hardening Agentic Web Interfaces; and
achieving resilience in multi-agent and federated environments. Our work
provides a comprehensive reference to guide the design of robust defense
mechanisms and establish best practices for resilient LLM-agent workflows.

</details>


### [578] [Securing AI Systems: A Guide to Known Attacks and Impacts](https://arxiv.org/abs/2506.23296)
*Naoto Kiribuchi,Kengo Zenitani,Takayuki Semitsu*

Main category: cs.CR

TL;DR: This paper provides an overview of 11 major adversarial attack types targeting predictive and generative AI systems, linking them to impacts and the CIA security triad.


<details>
  <summary>Details</summary>
Motivation: The need to address increasing AI-specific security threats and provide foundational knowledge for mitigating such risks.

Method: Identifying major attack types, mapping them to their impacts, and contextualizing them within the CIA (confidentiality, integrity, availability) security framework.

Result: Created a structured understanding of AI-specific adversarial threats and their implications for system security.

Conclusion: Equips stakeholders with the knowledge needed to identify and defend against AI-specific vulnerabilities, contributing to improved AI system security.

Abstract: Embedded into information systems, artificial intelligence (AI) faces
security threats that exploit AI-specific vulnerabilities. This paper provides
an accessible overview of adversarial attacks unique to predictive and
generative AI systems. We identify eleven major attack types and explicitly
link attack techniques to their impacts -- including information leakage,
system compromise, and resource exhaustion -- mapped to the confidentiality,
integrity, and availability (CIA) security triad. We aim to equip researchers,
developers, security practitioners, and policymakers, even those without
specialized AI security expertise, with foundational knowledge to recognize
AI-specific risks and implement effective defenses, thereby enhancing the
overall security posture of AI systems.

</details>


### [579] [Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance](https://arxiv.org/abs/2506.23314)
*Joner Assolin,Gabriel Canto,Diego Kreutz,Eduardo Feitosa,Hendrio Bragança,Angelo Nogueira,Vanderson Rocha*

Main category: cs.CR

TL;DR: MH-AutoML is a framework for Android malware detection automating the ML process while enhancing transparency and interpretability, outperforming existing solutions.


<details>
  <summary>Details</summary>
Motivation: Current AutoML systems are limited by black-box nature, lack of interpretability, and traceability in cybersecurity contexts.

Method: Created MH-AutoML, a domain-specific tool integrating interpretability, debugging, and experiment tracking within complete ML automation for Android malware detection.

Result: MH-AutoML displayed superior recall rates compared to seven leading AutoML frameworks while maintaining computational efficiency.

Conclusion: MH-AutoML balances performance, explainability, and efficiency, making it ideal for Android security applications.

Abstract: Malware detection in Android systems requires both cybersecurity expertise
and machine learning (ML) techniques. Automated Machine Learning (AutoML) has
emerged as an approach to simplify ML development by reducing the need for
specialized knowledge. However, current AutoML solutions typically operate as
black-box systems with limited transparency, interpretability, and experiment
traceability. To address these limitations, we present MH-AutoML, a
domain-specific framework for Android malware detection. MH-AutoML automates
the entire ML pipeline, including data preprocessing, feature engineering,
algorithm selection, and hyperparameter tuning. The framework incorporates
capabilities for interpretability, debugging, and experiment tracking that are
often missing in general-purpose solutions. In this study, we compare MH-AutoML
against seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT,
HyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML
achieves better recall rates while providing more transparency and control. The
framework maintains computational efficiency comparable to other solutions,
making it suitable for cybersecurity applications where both performance and
explainability matter.

</details>


### [580] [SoK: Semantic Privacy in Large Language Models](https://arxiv.org/abs/2506.23603)
*Baihe Ma,Yanna Jiang,Xu Wang,Guangshen Yu,Qin Wang,Caijun Sun,Chen Li,Xuelei Qi,Ying He,Wei Ni,Ren Ping Liu*

Main category: cs.CR

TL;DR: The paper introduces 'semantic privacy' risks in LLMs, analyzes attack vectors and current defenses, identifies critical gaps, and provides research directions for improved semantic-level privacy techniques.


<details>
  <summary>Details</summary>
Motivation: To address the inadequacy of traditional privacy measures in protecting implicit, contextual, or inferable information in LLMs and propose a framework for analyzing semantic privacy risks.

Method: Developed a lifecycle-centric framework to study privacy risks across the operational stages of LLMs, categorized attack vectors, assessed current defenses, and identified gaps and challenges.

Result: Found significant weaknesses in current privacy defenses, particularly in protecting against contextual inference and latent representation leakage.

Conclusion: Highlighted urgent challenges in semantic privacy, advocated for improved techniques, and aimed to guide future research toward addressing these critical gaps in privacy-preserving strategies for LLMs.

Abstract: As Large Language Models (LLMs) are increasingly deployed in sensitive
domains, traditional data privacy measures prove inadequate for protecting
information that is implicit, contextual, or inferable - what we define as
semantic privacy. This Systematization of Knowledge (SoK) introduces a
lifecycle-centric framework to analyze how semantic privacy risks emerge across
input processing, pretraining, fine-tuning, and alignment stages of LLMs. We
categorize key attack vectors and assess how current defenses, such as
differential privacy, embedding encryption, edge computing, and unlearning,
address these threats. Our analysis reveals critical gaps in semantic-level
protection, especially against contextual inference and latent representation
leakage. We conclude by outlining open challenges, including quantifying
semantic leakage, protecting multimodal inputs, balancing de-identification
with generation quality, and ensuring transparency in privacy enforcement. This
work aims to inform future research on designing robust, semantically aware
privacy-preserving techniques for LLMs.

</details>


### [581] [gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures](https://arxiv.org/abs/2506.23634)
*Youjeong Noh,Joon-Young Paik,Jingun Kwon,Eun-Sun Cho*

Main category: cs.CR

TL;DR: The paper introduces a novel guided MBA deobfuscation framework (gMBA) using a truth table to efficiently recover obfuscated code.


<details>
  <summary>Details</summary>
Motivation: To address the growing misuse of MBA obfuscation by malware developers and the limitations of traditional deobfuscation methods that ignore semantic information.

Method: Proposes a truth table-based semantic representation for MBA expressions and a Transformer-based neural Seq2Seq model incorporating semantic guidance.

Result: Experimental results show that the proposed framework significantly improves deobfuscation performance by using internal semantics.

Conclusion: Incorporating expression semantics is critical for recovering obfuscated code effectively, as demonstrated by the proposed gMBA approach.

Abstract: Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by
converting programs into forms that are more complex to analyze. However, MBA
has been increasingly exploited by malware developers to evade detection and
cause significant real-world problems. Traditional MBA deobfuscation methods
often consider these expressions as part of a black box and overlook their
internal semantic information. To bridge this gap, we propose a truth table,
which is an automatically constructed semantic representation of an
expression's behavior that does not rely on external resources. The truth table
is a mathematical form that represents the output of expression for all
possible combinations of input. We also propose a general and extensible guided
MBA deobfuscation framework (gMBA) that modifies a Transformer-based neural
encoder-decoder Seq2Seq architecture to incorporate this semantic guidance.
Experimental results and in-depth analysis show that integrating expression
semantics significantly improves performance and highlights the importance of
internal semantic expressions in recovering obfuscated code to its original
form.

</details>


### [582] [Differentially Private Synthetic Data Release for Topics API Outputs](https://arxiv.org/abs/2506.23855)
*Travis Dick,Alessandro Epasto,Adel Javanmard,Josh Karlin,Andres Munoz Medina,Vahab Mirrokni,Sergei Vassilvitskii,Peilin Zhong*

Main category: cs.CR

TL;DR: The paper addresses the lack of publicly available data for studying Privacy-Preserving Ads APIs by introducing a novel methodology to create synthetic, realistic, and private API output datasets.


<details>
  <summary>Details</summary>
Motivation: The authors aim to overcome challenges in reliably studying the privacy properties of Privacy-Preserving Ads APIs due to limited access to realistic datasets, as privacy concerns restrict public dataset sharing.

Method: A methodology involving differential privacy to compute statistics from real API traces, design a parameterized distribution, and generate synthetic data resembling real API outputs was developed. This ensures privacy protection while enabling accurate analysis.

Result: The authors successfully produced a differentially-private dataset modeled after the Topics API from Google Chrome's Privacy Sandbox. The synthetic dataset maintains low re-identification risks and closely resembles real API outputs.

Conclusion: The release of this anonymized dataset fosters transparency and supports future research on Privacy-Preserving Ads APIs. The approach offers a balance between data realism and user privacy protection.

Abstract: The analysis of the privacy properties of Privacy-Preserving Ads APIs is an
area of research that has received strong interest from academics, industry,
and regulators. Despite this interest, the empirical study of these methods is
hindered by the lack of publicly available data. Reliable empirical analysis of
the privacy properties of an API, in fact, requires access to a dataset
consisting of realistic API outputs; however, privacy concerns prevent the
general release of such data to the public.
  In this work, we develop a novel methodology to construct synthetic API
outputs that are simultaneously realistic enough to enable accurate study and
provide strong privacy protections. We focus on one Privacy-Preserving Ads
APIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a
methodology to generate a differentially-private dataset that closely matches
the re-identification risk properties of the real Topics API data. The use of
differential privacy provides strong theoretical bounds on the leakage of
private user information from this release.
  Our methodology is based on first computing a large number of
differentially-private statistics describing how output API traces evolve over
time. Then, we design a parameterized distribution over sequences of API traces
and optimize its parameters so that they closely match the statistics obtained.
Finally, we create the synthetic data by drawing from this distribution.
  Our work is complemented by an open-source release of the anonymized dataset
obtained by this methodology. We hope this will enable external researchers to
analyze the API in-depth and replicate prior and future work on a realistic
large-scale dataset. We believe that this work will contribute to fostering
transparency regarding the privacy properties of Privacy-Preserving Ads APIs.

</details>


### [583] [RawMal-TF: Raw Malware Dataset Labeled by Type and Family](https://arxiv.org/abs/2506.23909)
*David Bálik,Martin Jureček,Mark Stamp*

Main category: cs.CR

TL;DR: This paper creates a novel malware dataset labeled at both type and family levels and demonstrates high classification performance using machine learning techniques.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of malware classification by building a robust and fine-grained dataset labeled at both malware type and family levels, enhancing the understanding of malware identification.

Method: The authors collected raw binary data from sources like VirusShare and MalwareBazaar, labeled the malware data using family and type-level information, processed it using static analysis focusing on Portable Executable headers, and evaluated classification tasks using Random Forest, XGBoost, and SVM models.

Result: The models achieved high accuracy in binary classification (up to 98.98%), interclass classification (up to 97.5% for type and 93.7% for family levels), and multiclass classification (up to 81.1% for type and 73.4% for family levels).

Conclusion: This study provides an advanced dataset and demonstrates the effectiveness of machine learning techniques for malware classification, emphasizing type and family-level labeling to enable more detailed analysis and future research development.

Abstract: This work addresses the challenge of malware classification using machine
learning by developing a novel dataset labeled at both the malware type and
family levels. Raw binaries were collected from sources such as VirusShare, VX
Underground, and MalwareBazaar, and subsequently labeled with family
information parsed from binary names and type-level labels integrated from
ClarAVy. The dataset includes 14 malware types and 17 malware families, and was
processed using a unified feature extraction pipeline based on static analysis,
particularly extracting features from Portable Executable headers, to support
advanced classification tasks. The evaluation was focused on three key
classification tasks. In the binary classification of malware versus benign
samples, Random Forest and XGBoost achieved high accuracy on the full datasets,
reaching 98.5% for type-based detection and 98.98% for family-based detection.
When using truncated datasets of 1,000 samples to assess performance under
limited data conditions, both models still performed strongly, achieving 97.6%
for type-based detection and 98.66% for family-based detection. For interclass
classification, which distinguishes between malware types or families, the
models reached up to 97.5% accuracy on type-level tasks and up to 93.7% on
family-level tasks. In the multiclass classification setting, which assigns
samples to the correct type or family, SVM achieved 81.1% accuracy on type
labels, while Random Forest and XGBoost reached approximately 73.4% on family
labels. The results highlight practical trade-offs between accuracy and
computational cost, and demonstrate that labeling at both the type and family
levels enables more fine-grained and insightful malware classification. The
work establishes a robust foundation for future research on advanced malware
detection and classification.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [584] [CMOS+X: Stacking Persistent Embedded Memories based on Oxide Transistors upon GPGPU Platforms](https://arxiv.org/abs/2506.23405)
*Faaiq Waqar,Ming-Yen Lee,Seongwon Yoon,Seongkwang Lim,Shimeng Yu*

Main category: cs.ET

TL;DR: This paper proposes a new memory topology based on amorphous oxide semiconductor (AOS) transistors for GPGPUs, delivering higher energy efficiency and performance gains compared to traditional SRAM.


<details>
  <summary>Details</summary>
Motivation: The scaling challenges and increasing power consumption of SRAM limit advancements in GPGPU memory systems, inhibiting raw arithmetic throughput and compute density.

Method: The study introduces CMOS+X integration of AOS transistors into capacitive memory topologies, leveraging 3D-integrated memory designs. A detailed evaluation of tradeoffs in density and power efficiency is performed.

Result: The proposed AOS-based multi-ported gain-cell delivers three times the read ports at ~76% of SRAM's footprint and 70% lower standby power. Benchmarks show performance-per-watt improvements up to 5.2x and 8% higher IPC on average.

Conclusion: AOS transistor-based memory designs offer a compelling path to overcome SRAM limitations, significantly enhancing GPGPU performance and energy efficiency across diverse workloads.

Abstract: In contemporary general-purpose graphics processing units (GPGPUs), the
continued increase in raw arithmetic throughput is constrained by the
capabilities of the register file (single-cycle) and last-level cache (high
bandwidth), which require the delivery of operands at a cadence demanded by
wide single-instruction multiple-data (SIMD) lanes. Enhancing the capacity,
density, or bandwidth of these memories can unlock substantial performance
gains; however, the recent stagnation of SRAM bit-cell scaling leads to
inequivalent losses in compute density.
  To address the challenges posed by SRAM's scaling and leakage power
consumption, this paper explores the potential CMOS+X integration of amorphous
oxide semiconductor (AOS) transistors in capacitive, persistent memory
topologies (e.g., 1T1C eDRAM, 2T0C/3T0C Gain Cell) as alternative cells in
multi-ported and high-bandwidth banked GPGPU memories. A detailed study of the
density and energy tradeoffs of back-end-of-line (BEOL) integrated memories
utilizing monolithic 3D (M3D)-integrated multiplexed arrays is conducted, while
accounting for the macro-level limitations of integrating AOS candidate
structures proposed by the device community (an aspect often overlooked in
prior work). By exploiting the short lifetime of register operands, we propose
a multi-ported AOS gain-cell capable of delivering 3x the read ports in ~76% of
the footprint of SRAM with over 70% lower standby power, enabling enhancements
to compute capacity, such as larger warp sizes or processor counts. Benchmarks
run on a validated NVIDIA Ampere-class GPU model, using a modified version of
Accel-Sim, demonstrate improvements of up to 5.2x the performance per watt and
an average 8% higher geometric mean instruction per cycle (IPC) on various
compute- and memory-bound tasks.

</details>


### [585] [Towards the "Digital Me": A vision of authentic Conversational Agents powered by personal Human Digital Twins](https://arxiv.org/abs/2506.23826)
*Lluís C. Coll,Martin W. Lauer-Schmaltz,Philip Cash,John P. Hansen,Anja Maier*

Main category: cs.ET

TL;DR: The paper presents an HDT system integrating conversational AI and dynamic data to emulate individual conversational styles with personal experiences, while addressing ethical concerns.


<details>
  <summary>Details</summary>
Motivation: To elevate Human Digital Twins from decision-support tools to interactive, evolving digital counterparts by leveraging advanced conversational AI and dynamic data modeling.

Method: Introduced an HDT system architecture based on large language models, employing techniques like context-aware memory retrieval, neural plasticity-inspired consolidation, and adaptive learning.

Result: The system replicates unique conversational styles and dynamically integrates personal memories, experiences, and opinions to enrich interactions.

Conclusion: The approach marks progress towards authentic virtual personas but underscores the need to responsibly address ethical considerations like privacy and accountability.

Abstract: Human Digital Twins (HDTs) have traditionally been conceptualized as
data-driven models designed to support decision-making across various domains.
However, recent advancements in conversational AI open new possibilities for
HDTs to function as authentic, interactive digital counterparts of individuals.
This paper introduces a novel HDT system architecture that integrates large
language models with dynamically updated personal data, enabling it to mirror
an individual's conversational style, memories, and behaviors. To achieve this,
our approach implements context-aware memory retrieval, neural
plasticity-inspired consolidation, and adaptive learning mechanisms, creating a
more natural and evolving digital persona. The resulting system does not only
replicate an individual's unique conversational style depending on who they are
speaking with, but also enriches responses with dynamically captured personal
experiences, opinions, and memories. While this marks a significant step toward
developing authentic virtual counterparts, it also raises critical ethical
concerns regarding privacy, accountability, and the long-term implications of
persistent digital identities. This study contributes to the field of HDTs by
describing our novel system architecture, demonstrating its capabilities, and
discussing future directions and emerging challenges to ensure the responsible
and ethical development of HDTs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [586] [Bridging Physical and Digital Worlds: Embodied Large AI for Future Wireless Systems](https://arxiv.org/abs/2506.24009)
*Xinquan Wang,Fenghao Zhu,Zhaohui Yang,Chongwen Huang,Xiaoming Chen,Zhaoyang Zhang,Sami Muhaidat,Mérouane Debbah*

Main category: cs.IT

TL;DR: This paper introduces the concept of Wireless Embodied Large AI (WELAI) to address limitations in current AI models for wireless systems.


<details>
  <summary>Details</summary>
Motivation: Current AI models for wireless networks fail to handle real-time dynamics, non-stationary environments, and lack the ability for environmental probing.

Method: The authors propose shifting from passive observation to active embodiment with WELAI, detailing its design principles, system structure, applications, and validating its effectiveness through a case study.

Result: The study demonstrates that WELAI effectively handles wireless system challenges and provides an adaptive, robust, and autonomous solution.

Conclusion: WELAI represents a transformative approach for next-generation wireless systems, highlighting the need for further research into adaptive and autonomous wireless capabilities.

Abstract: Large artificial intelligence (AI) models offer revolutionary potential for
future wireless systems, promising unprecedented capabilities in network
optimization and performance. However, current paradigms largely overlook
crucial physical interactions. This oversight means they primarily rely on
offline datasets, leading to difficulties in handling real-time wireless
dynamics and non-stationary environments. Furthermore, these models often lack
the capability for active environmental probing. This paper proposes a
fundamental paradigm shift towards wireless embodied large AI (WELAI), moving
from passive observation to active embodiment. We first identify key challenges
faced by existing models, then we explore the design principles and system
structure of WELAI. Besides, we outline prospective applications in
next-generation wireless. Finally, through an illustrative case study, we
demonstrate the effectiveness of WELAI and point out promising research
directions for realizing adaptive, robust, and autonomous wireless systems.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [587] [Neural models of multiscale systems: conceptual limitations, stochastic parametrizations, and a climate application](https://arxiv.org/abs/2506.22552)
*Fabrizio Falasca*

Main category: nlin.CD

TL;DR: This paper investigates limitations in data-driven models for dynamical systems, highlighting neural emulators' struggles with capturing statistics and responses under partial observations, while emphasizing the need for physically grounded modeling strategies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations that data-driven neural models face in capturing both equilibrium statistics and responses to external perturbations in multiscale dynamical systems like climate models.

Method: The study analyzes both low-dimensional and high-dimensional systems, constructing neural stochastic models under scenarios with full and partial state observations, and proposes physically based strategies such as coarse-graining and stochastic parameterizations.

Result: The models perform well under full observations but face challenges with partial observations, particularly in identifying key variables and parameterizing unobserved dynamics. This highlights inherent limitations in data-driven approaches.

Conclusion: Physically informed methodologies, rather than purely data-driven approaches, are necessary for effectively modeling complex dynamical systems, with practical applications demonstrated in climate emulation scenarios.

Abstract: This work explores key conceptual limitations in data-driven modeling of
multiscale dynamical systems, focusing on neural emulators and stochastic
climate modeling. A skillful climate model should capture both stationary
statistics and responses to external perturbations. While current
autoregressive neural models often reproduce the former, they typically
struggle with the latter. We begin by analyzing a low-dimensional dynamical
system to expose, by analogy, fundamental limitations that persist in
high-dimensional settings. Specifically, we construct neural stochastic models
under two scenarios: one where the full state vector is observed, and another
with only partial observations (i.e. a subset of variables). In the first case,
the models accurately capture both equilibrium statistics and forced responses
in ensemble mean and variance. In the more realistic case of partial
observations, two key challenges emerge: (i) identifying the \textit{proper}
variables to model, and (ii) parameterizing the influence of unobserved degrees
of freedom. These issues are not specific to neural networks but reflect
fundamental limitations of data-driven modeling and the need to target the slow
dynamics of the system. We argue that physically grounded strategies -- such as
coarse-graining and stochastic parameterizations -- are critical, both
conceptually and practically, for the skillful emulation of complex systems
like the coupled climate system. Building on these insights, we turn to a more
realistic application: a stochastic reduced neural model of the sea surface
temperature field and the net radiative flux at the top of the atmosphere,
assessing its stationary statistics, response to temperature forcing, and
interpretability.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [588] [Explainable AI for Comprehensive Risk Assessment for Financial Reports: A Lightweight Hierarchical Transformer Network Approach](https://arxiv.org/abs/2506.23767)
*Xue Wen Tan,Stanley Kok*

Main category: q-fin.RM

TL;DR: The paper introduces TinyXRA, an efficient transformer-based model for assessing financial risk from company 10-K reports, providing state-of-the-art accuracy and explainable predictions.


<details>
  <summary>Details</summary>
Motivation: To address the inadequacies of standard financial risk models and provide an explainable, computationally efficient approach for evaluating U.S. company risks.

Method: TinyXRA builds on TinyBERT as the encoder and incorporates dynamic attention-based word cloud visualization, skewness, kurtosis, Sortino ratio, and triplet loss for risk quartile classification.

Result: TinyXRA outperformed existing models in predictive accuracy over a 7-year test period, demonstrated scalability for real-time processing, and delivered transparent risk visualizations.

Conclusion: TinyXRA provides an innovative, interpretable, and scalable solution for financial risk assessment, offering practical insights while acknowledging limitations and suggesting avenues for future research.

Abstract: Every publicly traded U.S. company files an annual 10-K report containing
critical insights into financial health and risk. We propose Tiny eXplainable
Risk Assessor (TinyXRA), a lightweight and explainable transformer-based model
that automatically assesses company risk from these reports. Unlike prior work
that relies solely on the standard deviation of excess returns (adjusted for
the Fama-French model), which indiscriminately penalizes both upside and
downside risk, TinyXRA incorporates skewness, kurtosis, and the Sortino ratio
for more comprehensive risk assessment. We leverage TinyBERT as our encoder to
efficiently process lengthy financial documents, coupled with a novel dynamic,
attention-based word cloud mechanism that provides intuitive risk visualization
while filtering irrelevant terms. This lightweight design ensures scalable
deployment across diverse computing environments with real-time processing
capabilities for thousands of financial documents which is essential for
production systems with constrained computational resources. We employ triplet
loss for risk quartile classification, improving over pairwise loss approaches
in existing literature by capturing both the direction and magnitude of risk
differences. Our TinyXRA achieves state-of-the-art predictive accuracy across
seven test years on a dataset spanning 2013-2024, while providing transparent
and interpretable risk assessments. We conduct comprehensive ablation studies
to evaluate our contributions and assess model explanations both quantitatively
by systematically removing highly attended words and sentences, and
qualitatively by examining explanation coherence. The paper concludes with
findings, practical implications, limitations, and future research directions.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [589] [Report on NSF Workshop on Science of Safe AI](https://arxiv.org/abs/2506.22492)
*Rajeev Alur,Greg Durrett,Hadas Kress-Gazit,Corina Păsăreanu,René Vidal*

Main category: cs.CY

TL;DR: A workshop report discussing AI safety challenges and proposing a research agenda for safe, trustworthy AI systems.


<details>
  <summary>Details</summary>
Motivation: To address challenges in AI safety and trustworthiness, especially in complex machine learning models, and to develop safer AI systems.

Method: Conducting a workshop involving NSF-funded and other researchers to discuss AI safety and propose relevant research methods, tools, and theories.

Result: A new research agenda was formulated with a focus on creating theoretical foundations, methodologies, and tools for safe AI systems.

Conclusion: Advancing AI safety is essential for trustworthy AI systems, and collaborative efforts are key to achieving this objective.

Abstract: Recent advances in machine learning, particularly the emergence of foundation
models, are leading to new opportunities to develop technology-based solutions
to societal problems. However, the reasoning and inner workings of today's
complex AI models are not transparent to the user, and there are no safety
guarantees regarding their predictions. Consequently, to fulfill the promise of
AI, we must address the following scientific challenge: how to develop AI-based
systems that are not only accurate and performant but also safe and
trustworthy?
  The criticality of safe operation is particularly evident for autonomous
systems for control and robotics, and was the catalyst for the Safe Learning
Enabled Systems (SLES) program at NSF. For the broader class of AI
applications, such as users interacting with chatbots and clinicians receiving
treatment recommendations, safety is, while no less important, less
well-defined with context-dependent interpretations. This motivated the
organization of a day-long workshop, held at University of Pennsylvania on
February 26, 2025, to bring together investigators funded by the NSF SLES
program with a broader pool of researchers studying AI safety. This report is
the result of the discussions in the working groups that addressed different
aspects of safety at the workshop. The report articulates a new research agenda
focused on developing theory, methods, and tools that will provide the
foundations of the next generation of AI-enabled systems.

</details>


### [590] [Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety](https://arxiv.org/abs/2506.22496)
*Y. Du*

Main category: cs.CY

TL;DR: The paper identifies gambling-like risk-taking behaviors in large language models (LLMs) and proposes a framework to mitigate these issues by leveraging behavioral economics and prospect theory.


<details>
  <summary>Details</summary>
Motivation: To address systematic risk behaviors in LLMs such as overconfidence, loss-chasing, and misjudgment of probabilities, which can compromise performance and reliability.

Method: The paper introduces the Risk-Aware Response Generation (RARG) framework, which incorporates risk-calibrated training, loss-aversion mechanisms, and uncertainty-aware decision-making inspired by gambling psychology research.

Result: Experimental results show an 18.7% reduction in overconfidence bias, a 24.3% reduction in loss-chasing tendencies, and improved risk calibration.

Conclusion: This study establishes a novel framework for understanding and reducing gambling-like behaviors in AI, enhancing their decision-making reliability.

Abstract: Large Language Models (LLMs) exhibit systematic risk-taking behaviors
analogous to those observed in gambling psychology, including overconfidence
bias, loss-chasing tendencies, and probability misjudgment. Drawing from
behavioral economics and prospect theory, we identify and formalize these
"gambling-like" patterns where models sacrifice accuracy for high-reward
outputs, exhibit escalating risk-taking after errors, and systematically
miscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG)
framework, incorporating insights from gambling research to address these
behavioral biases through risk-calibrated training, loss-aversion mechanisms,
and uncertainty-aware decision making. Our approach introduces novel evaluation
paradigms based on established gambling psychology experiments, including AI
adaptations of the Iowa Gambling Task and probability learning assessments.
Experimental results demonstrate measurable reductions in gambling-like
behaviors: 18.7\% decrease in overconfidence bias, 24.3\% reduction in
loss-chasing tendencies, and improved risk calibration across diverse
scenarios. This work establishes the first systematic framework for
understanding and mitigating gambling psychology patterns in AI systems.

</details>


### [591] [Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship](https://arxiv.org/abs/2506.22497)
*Craig Steven Wright*

Main category: cs.CY

TL;DR: The paper proposes reimagining peer review as open, identity-linked public commentary instead of traditional anonymous methods.


<details>
  <summary>Details</summary>
Motivation: To address issues in academic validation like anonymity, delay, and gatekeeping.

Method: Employs blockchain for transparency and AI for generating iterative evaluations in an open scholarly commentary framework.

Result: Introduced a system that rewards intellectual contributions, tracks epistemic shifts, and provides clear reputational dynamics.

Conclusion: The proposed model shifts academic knowledge assessment from being static to dynamic and applicable across various fields.

Abstract: This paper reconceptualises peer review as structured public commentary.
Traditional academic validation is hindered by anonymity, latency, and
gatekeeping. We propose a transparent, identity-linked, and reproducible system
of scholarly evaluation anchored in open commentary. Leveraging blockchain for
immutable audit trails and AI for iterative synthesis, we design a framework
that incentivises intellectual contribution, captures epistemic evolution, and
enables traceable reputational dynamics. This model empowers fields from
computational science to the humanities, reframing academic knowledge as a
living process rather than a static credential.

</details>


### [592] [Ask before you Build: Rethinking AI-for-Good in Human Trafficking Interventions](https://arxiv.org/abs/2506.22512)
*Pratheeksha Nair,Gabriel Lefebvre,Sophia Garrel,Maryam Molamohammadi,Reihaneh Rabbany*

Main category: cs.CY

TL;DR: This paper critiques the use of AI for human trafficking solutions, proposing a new ethical assessment framework called Radical Questioning (RQ).


<details>
  <summary>Details</summary>
Motivation: To address the ethical concerns involved in deploying AI for addressing social issues, especially those affecting marginalized communities.

Method: Introduces the Radical Questioning (RQ) framework, a five-step upstream ethical assessment tool, applied to a case study in AI for human trafficking.

Result: RQ exposes overlooked sociocultural complexities and directs efforts away from surveillance-based approaches to survivor empowerment tools.

Conclusion: The five-step RQ framework offers a pre-design ethical evaluation tool that prioritizes responsibility and can help shift AI ethics towards relational and reflexive norms in broader contexts.

Abstract: AI for good initiatives often rely on the assumption that technical
interventions can resolve complex social problems. In the context of human
trafficking (HT), such techno-solutionism risks oversimplifying exploitation,
reinforcing power imbalances and causing harm to the very communities AI claims
to support. In this paper, we introduce the Radical Questioning (RQ) framework
as a five step, pre-project ethical assessment tool to critically evaluate
whether AI should be built at all, especially in domains involving marginalized
populations and entrenched systemic injustice. RQ does not replace principles
based ethics but precedes it, offering an upstream, deliberative space to
confront assumptions, map power, and consider harms before design. Using a case
study in AI for HT, we demonstrate how RQ reveals overlooked sociocultural
complexities and guides us away from surveillance based interventions toward
survivor empowerment tools. While developed in the context of HT, RQ's five
step structure can generalize to other domains, though the specific questions
must be contextual. This paper situates RQ within a broader AI ethics
philosophy that challenges instrumentalist norms and centers relational,
reflexive responsibility.

</details>


### [593] [Computational Analysis of Climate Policy](https://arxiv.org/abs/2506.22449)
*Carolyn Hicks*

Main category: cs.CY

TL;DR: The thesis evaluates the impact of the Climate Emergency movement on local government policies using GPT-4, finding that councils with Climate Emergency Declarations (CEDs) prioritize climate-specific measures more than those without.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this thesis is to assess the potential of large language models, like GPT-4, for answering complex policy questions and to explore whether the Climate Emergency movement has influenced local governments' climate policies.

Method: A system named PALLM, built with GPT-4, was used to analyze climate policy documents of local governments in Victoria, Australia. The analysis was validated by policymakers and applied to compare councils with and without Climate Emergency Declarations.

Result: Councils with Climate Emergency Declarations exhibited more climate-specific and recent policies, focusing on urgency, prioritization, and equity, compared to councils without such declarations.

Conclusion: The study concludes that GPT-4 can perform high-level policy analysis and facilitate nuanced research. Local governments with Climate Emergency Declarations show greater climate policy engagement, demonstrating the movement's influence.

Abstract: This thesis explores the impact of the Climate Emergency movement on local
government climate policy, using computational methods. The Climate Emergency
movement sought to accelerate climate action at local government level through
the mechanism of Climate Emergency Declarations (CEDs), resulting in a series
of commitments from councils to treat climate change as an emergency. With the
aim of assessing the potential of current large language models to answer
complex policy questions, I first built and configured a system named PALLM
(Policy Analysis with a Large Language Model), using the OpenAI model GPT-4.
This system is designed to apply a conceptual framework for climate emergency
response plans to a dataset of climate policy documents. I validated the
performance of this system with the help of local government policymakers, by
generating analyses of the climate policies of 11 local governments in Victoria
and assessing the policymakers' level of agreement with PALLM's responses.
Having established that PALLM's performance is satisfactory, I used it to
conduct a large-scale analysis of current policy documents from local
governments in the state of Victoria, Australia. This thesis presents the
methodology and results of this analysis, comparing the results for councils
which have passed a CED to those which did not. This study finds that GPT-4 is
capable of high-level policy analysis, with limitations including a lack of
reliable attribution, and can also enable more nuanced analysis by researchers.
Its use in this research shows that councils which have passed a CED are more
likely to have a recent and climate-specific policy, and show more attention to
urgency, prioritisation, and equity and social justice, than councils which
have not. It concludes that the ability to assess policy documents at scale
opens up exciting new opportunities for policy researchers.

</details>


### [594] [Theories of "Sexuality" in Natural Language Processing Bias Research](https://arxiv.org/abs/2506.22481)
*Jacob Hobbs*

Main category: cs.CY

TL;DR: The paper investigates the misrepresentation of queer sexualities in NLP systems and provides insights to improve bias analysis.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the overlooked analysis of how queer sexualities are encoded and misrepresented by NLP systems.

Method: Through a survey of 55 articles, it examines the definitions and methodologies for analyzing sexuality-based NLP biases.

Result: The study finds a lack of clear definitions for sexuality and identifies conflations between sexual and gender identities that misrepresent queerness.

Conclusion: Recommends engaging more deeply with queer communities and interdisciplinary literature to improve NLP bias analyses.

Abstract: In recent years, significant advancements in the field of Natural Language
Processing (NLP) have positioned commercialized language models as
wide-reaching, highly useful tools. In tandem, there has been an explosion of
multidisciplinary research examining how NLP tasks reflect, perpetuate, and
amplify social biases such as gender and racial bias. A significant gap in this
scholarship is a detailed analysis of how queer sexualities are encoded and
(mis)represented by both NLP systems and practitioners. Following previous work
in the field of AI fairness, we document how sexuality is defined and
operationalized via a survey and analysis of 55 articles that quantify
sexuality-based NLP bias. We find that sexuality is not clearly defined in a
majority of the literature surveyed, indicating a reliance on assumed or
normative conceptions of sexual/romantic practices and identities. Further, we
find that methods for extracting biased outputs from NLP technologies often
conflate gender and sexual identities, leading to monolithic conceptions of
queerness and thus improper quantifications of bias. With the goal of improving
sexuality-based NLP bias analyses, we conclude with recommendations that
encourage more thorough engagement with both queer communities and
interdisciplinary literature.

</details>


### [595] [A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models](https://arxiv.org/abs/2506.22493)
*Sadia Kamal,Lalu Prasad Yadav Prakash,S M Rafiuddin,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen,Sagnik Ray Choudhury*

Main category: cs.CY

TL;DR: The paper evaluates the reliability of political compass tests for determining language models' political orientations, addressing the impact of variations like prompts and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: The rise of political compass tests to gauge the biases of large language models necessitates an evaluation of these tools' reliability.

Method: The authors analyze standard generation parameter effects, prompt variations, fine-tuning procedures, and datasets to measure their impact on the model's political leaning scores.

Result: While standard generation parameters have little impact, prompt variations and fine-tuning significantly influence PCT outcomes. Fine-tuning on politically loaded datasets does not skew results.

Conclusion: The reliability of PCT tools warrants further scrutiny, and understanding how LLMs encode political leanings is crucial for unbiased assessments.

Abstract: Political Compass Test (PCT) or similar questionnaires have been used to
quantify LLM's political leanings. Building on a recent line of work that
examines the validity of PCT tests, we demonstrate that variation in standard
generation parameters does not significantly impact the models' PCT scores.
However, external factors such as prompt variations and fine-tuning
individually and in combination affect the same. Finally, we demonstrate that
when models are fine-tuned on text datasets with higher political content than
others, the PCT scores are not differentially affected. This calls for a
thorough investigation into the validity of PCT and similar tests, as well as
the mechanism by which political leanings are encoded in LLMs.

</details>


### [596] [Red Teaming for Generative AI, Report on a Copyright-Focused Exercise Completed in an Academic Medical Center](https://arxiv.org/abs/2506.22523)
*James Wen,Sahil Nalawade,Zhiwei Liang,Catherine Bielick,Marisa Ferrara Boston,Alexander Chowdhury,Adele Collin,Luigi De Angelis,Jacob Ellen,Heather Frase,Rodrigo R. Gameiro,Juan Manuel Gutierrez,Pooja Kadam,Murat Keceli,Srikanth Krishnamurthy,Anne Kwok,Yanan Lance Lu,Heather Mattie,Liam G. McCoy,Katherine Miller,Allison C. Morgan,Marlene Louisa Moerig,Trang Nguyen,Alexander Owen-Post,Alex D. Ruiz,Sreekar Reddy Puchala,Soujanya Samineni,Takeshi Tohyama,Varun Ullanat,Carmine Valenza,Camilo Velez,Pengcheng Wang,Anna Wuest,Yuxiang Zhou,Yingde Zhu,Jason M. Johnson,Jennifer Willcox,Francis J. Vitiello,Leo Anthony G. Celi,Renato Umeton*

Main category: cs.CY

TL;DR: The abstract outlines results from a red-teaming event to analyze GPT4DFCI's capability to reproduce copyrighted content and fabrications. While exact quotations were reproduced for books, target news articles, scientific papers, and medical records showed no precise reproduction, but exhibited fabrication. A mitigation strategy has been implemented in the next version of the tool.


<details>
  <summary>Details</summary>
Motivation: The paper aims to assess whether AI models, such as GPT4DFCI designed by Dana-Farber Cancer Institute in collaboration with Microsoft, could violate copyright laws by reproducing protected content.

Method: Red teaming tests were conducted by intentionally probing the AI model to reproduce copyrighted material from sources like books, news, scientific articles, and electronic health records.

Result: The tests showed that GPT4DFCI could output exact quotes from copyrighted books but failed to reproduce the content accurately from target news articles, scientific papers, and medical records. However, instances of fabricated data were observed.

Conclusion: A mitigation strategy was rolled out in GPT4DFCI v2.8.2 to limit the reproduction of copyrighted and fabricated data. The event calls for similar testing to verify AI tools' compliance with ethical and legal standards.

Abstract: Generative AI is present in multiple industries. Dana-Farber Cancer
Institute, in partnership with Microsoft, has created an internal AI tool,
GPT4DFCI. Together we hosted a red teaming event to assess whether the
underlying GPT models that support the tool would output copyrighted data. Our
teams focused on reproducing content from books, news articles, scientific
articles, and electronic health records. We found isolated instances where
GPT4DFCI was able to identify copyrighted material and reproduce exact quotes
from famous books which indicates that copyrighted material was in the training
data. The model was not able to reproduce content from our target news article,
scientific article, or electronic health records. However, there were instances
of fabrication. As a result of this event, a mitigation strategy is in
production in GPT4DFCI v2.8.2, deployed on January 21, 2025. We hope this
report leads to similar events in which AI software tools are stress-tested to
assess the perimeter of their legal and ethical usage.

</details>


### [597] [From Model Design to Organizational Design: Complexity Redistribution and Trade-Offs in Generative AI](https://arxiv.org/abs/2506.22440)
*Sharique Hasan,Alexander Oettl,Sampsa Samila*

Main category: cs.CY

TL;DR: The paper introduces the Generality-Accuracy-Simplicity (GAS) framework to analyze how large language models (LLMs) impact organizations and competition, highlighting the redistribution of complexity to organizational infrastructure and workflows, thus creating new challenges and opportunities.


<details>
  <summary>Details</summary>
Motivation: To address the evolving strategic and organizational impacts of AI, particularly how large language models reshape complexity and competition, moving beyond traditional cost-reduction perspectives.

Method: The authors introduce the GAS framework to analyze trade-offs involving generality, accuracy, and simplicity, and illustrate how LLMs shift complexity from users to organizations.

Result: The study highlights that simplicity for users leads to infrastructural, compliance, and managerial challenges within organizations, redefining competitive advantage strategies around mastering redistributed complexity.

Conclusion: Competitive advantage in the AI era depends not on adopting LLMs but on effectively managing the complexity they redistribute, through abstraction design, workflow alignment, and complementary expertise.

Abstract: This paper introduces the Generality-Accuracy-Simplicity (GAS) framework to
analyze how large language models (LLMs) are reshaping organizations and
competitive strategy. We argue that viewing AI as a simple reduction in input
costs overlooks two critical dynamics: (a) the inherent trade-offs among
generality, accuracy, and simplicity, and (b) the redistribution of complexity
across stakeholders. While LLMs appear to defy the traditional trade-off by
offering high generality and accuracy through simple interfaces, this
user-facing simplicity masks a significant shift of complexity to
infrastructure, compliance, and specialized personnel. The GAS trade-off,
therefore, does not disappear but is relocated from the user to the
organization, creating new managerial challenges, particularly around accuracy
in high-stakes applications. We contend that competitive advantage no longer
stems from mere AI adoption, but from mastering this redistributed complexity
through the design of abstraction layers, workflow alignment, and complementary
expertise. This study advances AI strategy by clarifying how scalable cognition
relocates complexity and redefines the conditions for technology integration.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [598] [FuzzCoh: Robust Canonical Coherence-Based Fuzzy Clustering of Multivariate Time Series](https://arxiv.org/abs/2506.22861)
*Ziling Ma,Mara Sherlin Talento,Ying Sun,Hernando Ombao*

Main category: stat.AP

TL;DR: The paper proposes a fuzzy clustering method leveraging Kendall's tau-based canonical coherence to analyze EEG data, addressing challenges like noise, and ambiguous clustering.


<details>
  <summary>Details</summary>
Motivation: Understanding brain functions via EEG data clustering is complex due to non-stationary dependencies, noisy data, and overlap in cognitive states.

Method: Utilizing Kendall's tau-based canonical coherence for frequency-specific feature extraction, projecting EEG data, and applying fuzzy clustering to capture spectral and spatial dynamics.

Result: The approach effectively clusters EEG data, identifying latent cognitive states like alertness and drowsiness and their spectral dependencies.

Conclusion: The method efficiently handles high-dimensional, noisy time series data and has wide applications, including neuroscience and wearable sensing.

Abstract: Brain cognitive and sensory functions are often associated with
electrophysiological activity at specific frequency bands. Clustering
multivariate time series (MTS) data like EEGs is important for understanding
brain functions but challenging due to complex non-stationary
cross-dependencies, gradual transitions between cognitive states, noisy
measurements, and ambiguous cluster boundaries. To address these issues, we
develop a robust fuzzy clustering framework in the spectral domain. Our method
leverages Kendall's tau-based canonical coherence, which extracts meaningful
frequency-specific monotonic relationships between groups of channels or
regions. KenCoh effectively captures dominant coherence structures while
remaining robust against outliers and noise, making it suitable for real EEG
datasets that typically contain artifacts. Our method first projects each MTS
object onto vectors derived from the KenCoh estimates (i.e, canonical
directions), which capture relevant information on the connectivity structure
of oscillatory signals in predefined frequency bands. These spectral features
are utilized to determine clusters of epochs using a fuzzy partitioning
strategy, accommodating gradual transitions and overlapping class structure.
Lastly, we demonstrate the effectiveness of our approach to EEG data where
latent cognitive states such as alertness and drowsiness exhibit
frequency-specific dynamics and ambiguity. Our method captures both spectral
and spatial features by locating the frequency-dependent structure and brain
functional connectivity. Built on the KenCoh framework for fuzzy clustering, it
handles the complexity of high-dimensional time series data and is broadly
applicable to domains such as neuroscience, wearable sensing, environmental
monitoring, and finance.

</details>


### [599] [Learning Individual Reproductive Behavior from Aggregate Fertility Rates via Neural Posterior Estimation](https://arxiv.org/abs/2506.22607)
*Daniel Ciganda,Ignacio Campón,Iñaki Permanyer,Jakob H Macke*

Main category: stat.AP

TL;DR: This paper introduces a Bayesian framework using Sequential Neural Posterior Estimation (SNPE) to infer individual-level reproductive behaviors from aggregate fertility data.


<details>
  <summary>Details</summary>
Motivation: Current fertility measures, such as age-specific fertility rates (ASFRs), fail to reveal the underlying individual-level behaviors influencing reproductive change.

Method: The study develops a likelihood-free Bayesian framework that combines individual-based reproductive modeling with SNPE, utilizing ASFRs and age profiles of planned versus unplanned births for inference.

Result: The framework replicates observed fertility schedules and effectively predicts out-of-sample metrics like age at first sex, inter-birth intervals, and family-size ideals, tested on data from the U.S. and several Latin American countries.

Conclusion: The approach provides valuable synthetic life histories for demographic forecasts and creates potential for constructing demographic digital twins.

Abstract: While age-specific fertility rates (ASFRs) provide the most extensive record
of reproductive change, their aggregate nature masks the underlying behavioral
mechanisms that ultimately drive fertility trends. To recover these mechanisms,
we develop a likelihood-free Bayesian framework that couples an
individual-level model of the reproductive process with Sequential Neural
Posterior Estimation (SNPE). This allows us to infer eight behavioral and
biological parameters from just two aggregate series: ASFRs and the age-profile
of planned versus unplanned births. Applied to U.S. National Survey of Family
Growth cohorts and to Demographic and Health Survey cohorts from Colombia, the
Dominican Republic, and Peru, the method reproduces observed fertility
schedules and, critically, predicts out-of-sample micro-level distributions of
age at first sex, inter-birth intervals, and family-size ideals, none of which
inform the estimation step. Because the fitted model yields complete synthetic
life histories, it enables behaviorally explicit population forecasts and
supports the construction of demographic digital twins.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [600] [Deep Learning for Optical Misalignment Diagnostics in Multi-Lens Imaging Systems](https://arxiv.org/abs/2506.23173)
*Tomer Slor,Dean Oren,Shira Baneth,Tom Coen,Haim Suchowski*

Main category: physics.optics

TL;DR: The study introduces two deep learning methods for diagnosing misalignments in multi-lens systems using optical measurements, showing high accuracy in error prediction.


<details>
  <summary>Details</summary>
Motivation: Precise alignment of multi-lens systems is crucial for optimal performance, and traditional methods are inefficient and slow, necessitating automated solutions.

Method:  Two approaches were employed: ray-traced spot diagrams to predict 5-DOF errors and a physics-based simulation pipeline using synthetic grayscale images for 4-DOF error estimation.

Result: The methods achieved high accuracy in predicting alignment errors—0.031mm in lateral translation and 0.011° in tilt for the ray-traced technique—and validated performance for both two- and six-lens systems with the simulation model.

Conclusion: Deep learning-based techniques offer promising tools for automating alignment diagnostics in imaging systems, potentially transforming manufacturing and quality control processes.

Abstract: In the rapidly evolving field of optical engineering, precise alignment of
multi-lens imaging systems is critical yet challenging, as even minor
misalignments can significantly degrade performance. Traditional alignment
methods rely on specialized equipment and are time-consuming processes,
highlighting the need for automated and scalable solutions. We present two
complementary deep learning-based inverse-design methods for diagnosing
misalignments in multi-element lens systems using only optical measurements.
First, we use ray-traced spot diagrams to predict five-degree-of-freedom
(5-DOF) errors in a 6-lens photographic prime, achieving a mean absolute error
of 0.031mm in lateral translation and 0.011$^\circ$ in tilt. We also introduce
a physics-based simulation pipeline that utilizes grayscale synthetic camera
images, enabling a deep learning model to estimate 4-DOF, decenter and tilt
errors in both two- and six-lens multi-lens systems. These results show the
potential to reshape manufacturing and quality control in precision imaging.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [601] [Hierarchical Characterization of Brain Dynamics via State Space-based Vector Quantization](https://arxiv.org/abs/2506.22952)
*Yanwu Yang,Thomas Wolfers*

Main category: eess.IV

TL;DR: This paper introduces HST, a novel tokenization method for fMRI brain dynamics, using a hierarchical state space and enhanced VQ-VAE to quantify metastability and transitions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges in studying brain dynamics through fMRI, particularly the insufficient representation of brain state transitions and metastability in existing tokenization methods.

Method: The study proposes a Hierarchical State space-based Tokenization network (HST), enhancing VQ-VAE with quantization error feedback and clustering for improved metastability representation.

Result: HST was validated on two public fMRI datasets, showcasing superior performance in quantifying hierarchical brain dynamics and potential applications in disease diagnosis.

Conclusion: This method enables deeper exploration of brain dynamics, enhancing metastability analysis and providing a robust framework for further neurological research.

Abstract: Understanding brain dynamics through functional Magnetic Resonance Imaging
(fMRI) remains a fundamental challenge in neuroscience, particularly in
capturing how the brain transitions between various functional states.
Recently, metastability, which refers to temporarily stable brain states, has
offered a promising paradigm to quantify complex brain signals into
interpretable, discretized representations. In particular, compared to
cluster-based machine learning approaches, tokenization approaches leveraging
vector quantization have shown promise in representation learning with powerful
reconstruction and predictive capabilities. However, most existing methods
ignore brain transition dependencies and lack a quantification of brain
dynamics into representative and stable embeddings. In this study, we propose a
Hierarchical State space-based Tokenization network, termed HST, which
quantizes brain states and transitions in a hierarchical structure based on a
state space-based model. We introduce a refined clustered Vector-Quantization
Variational AutoEncoder (VQ-VAE) that incorporates quantization error feedback
and clustering to improve quantization performance while facilitating
metastability with representative and stable token representations. We validate
our HST on two public fMRI datasets, demonstrating its effectiveness in
quantifying the hierarchical dynamics of the brain and its potential in disease
diagnosis and reconstruction performance. Our method offers a promising
framework for the characterization of brain dynamics, facilitating the analysis
of metastability.

</details>


### [602] [FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation](https://arxiv.org/abs/2506.22580)
*Vasilis Siomos,Jonathan Passerat-Palmbach,Giacomo Tarroni*

Main category: eess.IV

TL;DR: FedCLAM, a novel Federated Learning approach, addresses client-specific challenges and image heterogeneity in medical imaging by introducing adaptive momentum, personalized dampening, and intensity alignment loss.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges of device and population variability in medical imaging—which degrade federated learning model performance—and improve existing aggregation methods that lack adaptability.

Method: FedCLAM introduces client-adaptive momentum terms based on client-specific loss reduction, a personalized dampening factor to reduce overfitting, and intensity alignment loss to standardize heterogeneous image intensity distributions.

Result: FedCLAM outperforms eight state-of-the-art methods in medical segmentation tasks across two datasets, demonstrating its effectiveness.

Conclusion: The proposed FedCLAM framework enhances federated learning by adapting to inter-institutional disparities and image heterogeneity, thus improving medical imaging model performance.

Abstract: Federated learning is a decentralized training approach that keeps data under
stakeholder control while achieving superior performance over isolated
training. While inter-institutional feature discrepancies pose a challenge in
all federated settings, medical imaging is particularly affected due to diverse
imaging devices and population variances, which can diminish the global model's
effectiveness. Existing aggregation methods generally fail to adapt across
varied circumstances. To address this, we propose FedCLAM, which integrates
\textit{client-adaptive momentum} terms derived from each client's loss
reduction during local training, as well as a \textit{personalized dampening
factor} to curb overfitting. We further introduce a novel \textit{intensity
alignment} loss that matches predicted and ground-truth foreground
distributions to handle heterogeneous image intensity profiles across
institutions and devices. Extensive evaluations on two datasets show that
FedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,
underscoring its efficacy. The code is available at
https://github.com/siomvas/FedCLAM.

</details>


### [603] [High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning](https://arxiv.org/abs/2506.22532)
*Mark Wrobel,Michele Pascale,Tina Yao,Ruaraidh Campbell,Elena Milano,Michael Quail,Jennifer Steeden,Vivek Muthurangu*

Main category: eess.IV

TL;DR: The study aims to transform 2D free-breathing real-time CMR cine images into isotropic 3D segmented cine datasets using deep learning, showing good agreement with conventional imaging while reducing acquisition and reconstruction times.


<details>
  <summary>Details</summary>
Motivation: Conventional CMR methods are time-consuming and less accommodating for pediatric and congenital heart disease patients due to their reliance on 2D breath-hold imaging and static 3D imaging. The motivation is to create a faster and more patient-friendly imaging approach.

Method: The study trained four deep learning models to perform interslice contrast correction, respiratory motion correction, super-resolution, and segmentation of cardiovascular structures. These models were validated on real-time cine CMR data from 10 patients.

Result: Their method successfully produced isotropic 3D cine images from 2D data within less than a minute, achieving good agreement in ventricular volumes and vessel diameters compared to conventional imaging.

Conclusion: The proposed approach reduces the time required for CMR imaging while delivering comparable quantitative results, suggesting its potential utility for faster and efficient CMR workflows in clinical practice.

Abstract: Background: Conventional cardiovascular magnetic resonance (CMR) in
paediatric and congenital heart disease uses 2D, breath-hold, balanced steady
state free precession (bSSFP) cine imaging for assessment of function and
cardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for
anatomical assessment. Our aim is to concatenate a stack 2D free-breathing
real-time cines and use Deep Learning (DL) to create an isotropic a fully
segmented 3D cine dataset from these images. Methods: Four DL models were
trained on open-source data that performed: a) Interslice contrast correction;
b) Interslice respiratory motion correction; c) Super-resolution (slice
direction); and d) Segmentation of right and left atria and ventricles (RA, LA,
RV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients
undergoing routine cardiovascular examination, our method was validated on
prospectively acquired sagittal stacks of real-time cine images. Quantitative
metrics (ventricular volumes and vessel diameters) and image quality of the 3D
cines were compared to conventional breath hold cine and whole heart imaging.
Results: All real-time data were successfully transformed into 3D cines with a
total post-processing time of <1 min in all cases. There were no significant
biases in any LV or RV metrics with reasonable limits of agreement and
correlation. There is also reasonable agreement for all vessel diameters,
although there was a small but significant overestimation of RPA diameter.
Conclusion: We have demonstrated the potential of creating a 3D-cine data from
concatenated 2D real-time cine images using a series of DL models. Our method
has short acquisition and reconstruction times with fully segmented data being
available within 2 minutes. The good agreement with conventional imaging
suggests that our method could help to significantly speed up CMR in clinical
practice.

</details>


### [604] [CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation](https://arxiv.org/abs/2506.23121)
*Xinlei Yu,Chanmiao Wang,Hui Jin,Ahmed Elazab,Gangyong Jia,Xiang Wan,Changqing Zou,Ruiquan Ge*

Main category: eess.IV

TL;DR: CRISP-SAM2 enhances multi-organ medical segmentation by leveraging cross-modal interaction and semantic prompting strategies, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in current multi-organ segmentation techniques, such as inaccuracies, reliance on geometric prompts, and spatial information loss.

Method: CRISP-SAM2 uses cross-modal contextualized semantics through a progressive cross-attention mechanism, semantic prompting replacing geometric prompts, and a similarity-sorting self-updating strategy paired with mask-refining to improve segmentation performance.

Result: Comparative experiments on seven public datasets show CRISP-SAM2 outperforms other segmentation models, highlighting its efficiency in overcoming existing limitations.

Conclusion: CRISP-SAM2 provides a significant advancement in multi-organ medical segmentation using textual descriptions of organs, confirming its superior accuracy and adaptability in processing medical images.

Abstract: Multi-organ medical segmentation is a crucial component of medical image
processing, essential for doctors to make accurate diagnoses and develop
effective treatment plans. Despite significant progress in this field, current
multi-organ segmentation models often suffer from inaccurate details,
dependence on geometric prompts and loss of spatial information. Addressing
these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal
Interaction and Semantic Prompting based on SAM2. This model represents a
promising approach to multi-organ medical segmentation guided by textual
descriptions of organs. Our method begins by converting visual and textual
inputs into cross-modal contextualized semantics using a progressive
cross-attention interaction mechanism. These semantics are then injected into
the image encoder to enhance the detailed understanding of visual information.
To eliminate reliance on geometric prompts, we use a semantic prompting
strategy, replacing the original prompt encoder to sharpen the perception of
challenging targets. In addition, a similarity-sorting self-updating strategy
for memory and a mask-refining process is applied to further adapt to medical
imaging and enhance localized details. Comparative experiments conducted on
seven public datasets indicate that CRISP-SAM2 outperforms existing models.
Extensive analysis also demonstrates the effectiveness of our method, thereby
confirming its superior performance, especially in addressing the limitations
mentioned earlier. Our code is available at:
https://github.com/YU-deep/CRISP\_SAM2.git.

</details>


### [605] [CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation](https://arxiv.org/abs/2506.22882)
*Qilong Xing,Zikai Song,Yuteng Ye,Yuke Chen,Youjia Zhang,Na Feng,Junqing Yu,Wei Yang*

Main category: eess.IV

TL;DR: This paper introduces the Collaborative Anatomy Diffusion (CA-Diff) framework, leveraging anatomical information to improve brain MRI segmentation and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of existing CNN and transformer-based methods in delineating complex brain structures from MRI images and the inadequacies of current diffusion models when directly applied to brain MRI due to neglecting anatomical information.

Method: The proposed CA-Diff framework integrates distance field as an auxiliary anatomical condition for global spatial context, utilizes a collaborative diffusion process for joint distribution modeling with anatomical structures, introduces a consistency loss to refine relationships, and implements a time-adapted channel attention module to improve U-Net feature fusion.

Result: CA-Diff demonstrates superior performance in brain MRI segmentation tasks, surpassing existing SOTA approaches in accuracy.

Conclusion: Incorporating anatomical context through the CA-Diff framework significantly enhances segmentation performance, providing a promising avenue for brain morphology evaluation.

Abstract: Segmentation of brain structures from MRI is crucial for evaluating brain
morphology, yet existing CNN and transformer-based methods struggle to
delineate complex structures accurately. While current diffusion models have
shown promise in image segmentation, they are inadequate when applied directly
to brain MRI due to neglecting anatomical information. To address this, we
propose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating
spatial anatomical features to enhance segmentation accuracy of the diffusion
model. Specifically, we introduce distance field as an auxiliary anatomical
condition to provide global spatial context, alongside a collaborative
diffusion process to model its joint distribution with anatomical structures,
enabling effective utilization of anatomical features for segmentation.
Furthermore, we introduce a consistency loss to refine relationships between
the distance field and anatomical structures and design a time adapted channel
attention module to enhance the U-Net feature fusion procedure. Extensive
experiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.

</details>


### [606] [Score-based Diffusion Model for Unpaired Virtual Histology Staining](https://arxiv.org/abs/2506.23184)
*Anran Liu,Xiaofei Wang,Jing Cai,Chao Li*

Main category: eess.IV

TL;DR: The paper presents an MI-guided score-based diffusion model for unpaired virtual staining from H&E to IHC images, overcoming key limitations in staining style decomposition, control, and structural consistency.


<details>
  <summary>Details</summary>
Motivation: To address challenges in current virtual staining methods, such as the need for better decomposition of staining style and tissue structure, adaptable staining processes, and rigorous structural consistency for non-pixel-aligned images.

Method: The study introduces a mutual-information (MI)-guided score-based diffusion model, with a global MI-guided energy function, a timestep-customized reverse diffusion process, and a local MI-driven contrastive learning strategy.

Result: Extensive experiments show the model's superiority over state-of-the-art techniques in virtual staining, producing better structural consistency and controllable results.

Conclusion: The proposed method demonstrates strong potential for biomedical applications in efficiently generating IHC images, with a commitment to open-source its codes upon paper acceptance.

Abstract: Hematoxylin and eosin (H&E) staining visualizes histology but lacks
specificity for diagnostic markers. Immunohistochemistry (IHC) staining
provides protein-targeted staining but is restricted by tissue availability and
antibody specificity. Virtual staining, i.e., computationally translating the
H&E image to its IHC counterpart while preserving the tissue structure, is
promising for efficient IHC generation. Existing virtual staining methods still
face key challenges: 1) effective decomposition of staining style and tissue
structure, 2) controllable staining process adaptable to diverse tissue and
proteins, and 3) rigorous structural consistency modelling to handle the
non-pixel-aligned nature of paired H&E and IHC images. This study proposes a
mutual-information (MI)-guided score-based diffusion model for unpaired virtual
staining. Specifically, we design 1) a global MI-guided energy function that
disentangles the tissue structure and staining characteristics across
modalities, 2) a novel timestep-customized reverse diffusion process for
precise control of the staining intensity and structural reconstruction, and 3)
a local MI-driven contrastive learning strategy to ensure the cellular level
structural consistency between H&E-IHC images. Extensive experiments
demonstrate the our superiority over state-of-the-art approaches, highlighting
its biomedical potential. Codes will be open-sourced upon acceptance.

</details>


### [607] [Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation](https://arxiv.org/abs/2506.23334)
*Hongyi Pan,Ziliang Hong,Gorkem Durak,Ziyue Xu,Ulas Bagci*

Main category: eess.IV

TL;DR: The paper introduces a generative AI-based data augmentation framework to improve federated learning (FL) for breast cancer diagnosis using ultrasound images, addressing challenges of limited and non-IID data.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance and generalization of federated learning in medical domains where data privacy restricts data exchanges and datasets are often limited and non-IID.

Method: The study proposes using two class-specific Deep Convolutional Generative Adversarial Networks (DCGANs) for creating synthetic images and integrates them into FL frameworks (FedAvg and FedProx). Federated experiments used three public breast ultrasound datasets: BUSI, BUS-BRA, and UDIAT.

Result: Incorporating synthetic data improved the average AUC from 0.9206 to 0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. However, excessive synthetic data reduced performance.

Conclusion: Generative AI-based data augmentation can improve federated learning performance in breast ultrasound image classification but requires careful balancing of real and synthetic data.

Abstract: Federated learning (FL) has emerged as a promising paradigm for
collaboratively training deep learning models across institutions without
exchanging sensitive medical data. However, its effectiveness is often hindered
by limited data availability and non-independent, identically distributed data
across participating clients, which can degrade model performance and
generalization. To address these challenges, we propose a generative AI based
data augmentation framework that integrates synthetic image sharing into the
federated training process for breast cancer diagnosis via ultrasound images.
Specifically, we train two simple class-specific Deep Convolutional Generative
Adversarial Networks: one for benign and one for malignant lesions. We then
simulate a realistic FL setting using three publicly available breast
ultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are
adopted as baseline FL algorithms. Experimental results show that incorporating
a suitable number of synthetic images improved the average AUC from 0.9206 to
0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that
excessive use of synthetic data reduced performance, underscoring the
importance of maintaining a balanced ratio of real and synthetic samples. Our
findings highlight the potential of generative AI based data augmentation to
enhance FL results in the breast ultrasound image classification task.

</details>


### [608] [Physics informed guided diffusion for accelerated multi-parametric MRI reconstruction](https://arxiv.org/abs/2506.23311)
*Perla Mayo,Carolin M. Pirkl,Alin Achim,Bjoern Menze,Mohammad Golbabaee*

Main category: eess.IV

TL;DR: MRF-DiPh is a novel physics-informed denoising diffusion method for enhanced tissue mapping in MRI, leveraging advanced reconstruction techniques.


<details>
  <summary>Details</summary>
Motivation: Address challenges in MRI tissue mapping like accuracy and constraints adherence in accelerated acquisitions.

Method: Combines proximal splitting formulation with pretrained denoising diffusion models while enforcing physical constraints during reconstruction.

Result: Demonstrates higher accuracy and fidelity in brain scan parameter maps compared to deep learning and compressed sensing baselines.

Conclusion: MRF-DiPh effectively solves MRI inverse problems with improved accuracy and maintains physical consistency, aiding medical imaging reliability.

Abstract: We introduce MRF-DiPh, a novel physics informed denoising diffusion approach
for multiparametric tissue mapping from highly accelerated, transient-state
quantitative MRI acquisitions like Magnetic Resonance Fingerprinting (MRF). Our
method is derived from a proximal splitting formulation, incorporating a
pretrained denoising diffusion model as an effective image prior to regularize
the MRF inverse problem. Further, during reconstruction it simultaneously
enforces two key physical constraints: (1) k-space measurement consistency and
(2) adherence to the Bloch response model. Numerical experiments on in-vivo
brain scans data show that MRF-DiPh outperforms deep learning and compressed
sensing MRF baselines, providing more accurate parameter maps while better
preserving measurement fidelity and physical model consistency-critical for
solving reliably inverse problems in medical imaging.

</details>


### [609] [UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound](https://arxiv.org/abs/2506.23490)
*Junxuan Yu,Yaofei Duan,Yuhao Huang,Yu Wang,Rongbo Ling,Weihao Luo,Ang Zhang,Jingxian Xu,Qiongying Ni,Yongsong Zhou,Binghan Li,Haoran Dou,Liping Liu,Yanfen Chu,Feng Geng,Zhe Sheng,Zhifeng Ding,Dingxin Zhang,Rui Huang,Yuhang Zhang,Xiaowei Xu,Tao Tan,Dong Ni,Zhongshan Gou,Xin Yang*

Main category: eess.IV

TL;DR: This paper introduces UltraTwin, a generative framework to obtain 3D cardiac anatomical models from sparse 2D ultrasound data, addressing challenges like limited data and noise.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of 2D and 3D ultrasound in cardiac examinations by enabling accurate 3D anatomical reconstructions that aid clinical applications.

Method: The method involves a generative framework using a real-world paired dataset, a coarse-to-fine hierarchical reconstruction scheme, and an implicit autoencoder for topology-aware constraints.

Result: UltraTwin achieves high-quality reconstruction of cardiac anatomical twins, outperforming competing methods.

Conclusion: UltraTwin demonstrates strong potential in advancing personalized cardiac care via enhanced anatomical twin modeling.

Abstract: Echocardiography is routine for cardiac examination. However, 2D ultrasound
(US) struggles with accurate metric calculation and direct observation of 3D
cardiac structures. Moreover, 3D US is limited by low resolution, small field
of view and scarce availability in practice. Constructing the cardiac
anatomical twin from 2D images is promising to provide precise treatment
planning and clinical quantification. However, it remains challenging due to
the rare paired data, complex structures, and US noises. In this study, we
introduce a novel generative framework UltraTwin, to obtain cardiac anatomical
twin from sparse multi-view 2D US. Our contribution is three-fold. First,
pioneered the construction of a real-world and high-quality dataset containing
strictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we
propose a coarse-to-fine scheme to achieve hierarchical reconstruction
optimization. Last, we introduce an implicit autoencoder for topology-aware
constraints. Extensive experiments show that UltraTwin reconstructs
high-quality anatomical twins versus strong competitors. We believe it advances
anatomical twin modeling for potential applications in personalized cardiac
care.

</details>


### [610] [Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI](https://arxiv.org/abs/2506.23506)
*Bowen Xin,Rohan Hickey,Tamara Blake,Jin Jin,Claire E Wainwright,Thomas Benkert,Alto Stemmer,Peter Sly,David Coman,Jason Dowling*

Main category: eess.IV

TL;DR: This paper introduces an AI-assisted Pixel-level Lung (APL) scoring method for assessing structural lung damage in CF using UTE-MRI, achieving faster and more accurate results than prior methods.


<details>
  <summary>Details</summary>
Motivation: The motivation was to provide a radiation-free, quantifiable, and accurate method to assess structural lung damage in cystic fibrosis (CF) using UTE-MRI, addressing the lack of robust scoring systems for MRI in this context.

Method: The method involves a 5-stage APL system: 1) loading images, 2) AI-assisted lung segmentation, 3) sampling lung-bounded slices, 4) conducting pixel-level annotations, and 5) producing quantified reports.

Result: APL scoring demonstrated 8.2 minutes per subject analysis time, which is over twice as fast as grid-level scoring, and showed improved statistical accuracy (p=0.021) with a strong correlation to grid-level scoring (R=0.973, p=5.85e-9).

Conclusion: This method streamlines UTE lung MRI workflows in clinical settings, offers fast and accurate scoring, and may be extendable to additional MRI sequences and lung diseases.

Abstract: Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE)
represents a recent breakthrough in lung structure imaging, providing image
resolution and quality comparable to computed tomography (CT). Due to the
absence of ionising radiation, MRI is often preferred over CT in paediatric
diseases such as cystic fibrosis (CF), one of the most common genetic disorders
in Caucasians. To assess structural lung damage in CF imaging, CT scoring
systems provide valuable quantitative insights for disease diagnosis and
progression. However, few quantitative scoring systems are available in
structural lung MRI (e.g., UTE-MRI). To provide fast and accurate
quantification in lung MRI, we investigated the feasibility of novel Artificial
intelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring
consists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3)
lung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification
and reporting. The results shows that our APL scoring took 8.2 minutes per
subject, which was more than twice as fast as the previous grid-level scoring.
Additionally, our pixel-level scoring was statistically more accurate
(p=0.021), while strongly correlating with grid-level scoring (R=0.973,
p=5.85e-9). This tool has great potential to streamline the workflow of UTE
lung MRI in clinical settings, and be extended to other structural lung MRI
sequences (e.g., BLADE MRI), and for other lung diseases (e.g.,
bronchopulmonary dysplasia).

</details>


### [611] [A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation](https://arxiv.org/abs/2506.23584)
*Renjie Liang,Zhengkang Fan,Jinqian Pan,Chenkun Sun,Russell Terry,Jie Xu*

Main category: eess.IV

TL;DR: The paper proposes a two-stage framework for generating renal radiology reports from CT scans, combining feature extraction and vision-language modeling.


<details>
  <summary>Details</summary>
Motivation: Generating radiology reports from CT images is challenging due to the complexity of medical imaging and the variability in clinical documentation.

Method: A two-stage framework is introduced: 1) feature extraction using a multi-task model for lesion attributes, and 2) report generation using a fine-tuned vision-language model combining these features with CT images.

Result: The model surpasses random baselines in all evaluated abnormality types, producing reports that align with key clinical findings and reasonable textual accuracy.

Conclusion: The study proves the feasibility of feature-informed report generation in renal imaging and suggests potential for extending the approach to 3D CT volumes for enhanced clinical fidelity.

Abstract: Generating radiology reports from CT scans remains a complex task due to the
nuanced nature of medical imaging and the variability in clinical
documentation. In this study, we propose a two-stage framework for generating
renal radiology reports from 2D CT slices. First, we extract structured
abnormality features using a multi-task learning model trained to identify
lesion attributes such as location, size, enhancement, and attenuation. These
extracted features are subsequently combined with the corresponding CT image
and fed into a fine-tuned vision-language model to generate natural language
report sentences aligned with clinical findings. We conduct experiments on a
curated dataset of renal CT studies with manually annotated
sentence-slice-feature triplets and evaluate performance using both
classification metrics and natural language generation metrics. Our results
demonstrate that the proposed model outperforms random baselines across all
abnormality types, and the generated reports capture key clinical content with
reasonable textual accuracy. This exploratory work highlights the feasibility
of modular, feature-informed report generation for renal imaging. Future
efforts will focus on extending this pipeline to 3D CT volumes and further
improving clinical fidelity in multimodal medical AI systems.

</details>


### [612] [Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound](https://arxiv.org/abs/2506.23721)
*Gijs Luijten,Roberto Maria Scardigno,Lisle Faray de Paiva,Peter Hoyer,Jens Kleesiek,Domenico Buongiorno,Vitoantonio Bevilacqua,Jan Egger*

Main category: eess.IV

TL;DR: The paper proposes a system that uses deep learning (DL) for automated real-time kidney measurements via ultrasound (US) and enhances usability with augmented reality (AR) projected on HoloLens-2.


<details>
  <summary>Details</summary>
Motivation: Ultrasound is widely used in clinical settings due to its accessibility and safety but has usability challenges. These include a steep learning curve, time-consuming manual measurements, and cognitive disruptions caused by shifting focus between the patient and screen.

Method: The authors integrate DL-based semantic segmentation for real-time kidney measurements and combine it with AR to project the US display onto the clinician's field of view. Two AR-DL pipelines on HoloLens-2 are presented: a wireless API-based setup and a universal video-output-based setup.

Result: The proposed system was evaluated using the Open Kidney Dataset and various segmentation models like nnU-Net and YOLO. Usability, feasibility, and accuracy of real-time measurements were tested, and open-source pipelines were developed for broader applications.

Conclusion: This AR-enhanced and DL-assisted US system improves kidney measurement accuracy, ergonomics, and training, making it particularly advantageous for point-of-care diagnostics and clinical training.

Abstract: Ultrasound (US) is widely accessible and radiation-free but has a steep
learning curve due to its dynamic nature and non-standard imaging planes.
Additionally, the constant need to shift focus between the US screen and the
patient poses a challenge. To address these issues, we integrate deep learning
(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric
measurements, which are essential for clinical assessment but are traditionally
time-consuming and prone to fatigue. This automation allows clinicians to
concentrate on image interpretation rather than manual measurements.
Complementing DL, augmented reality (AR) enhances the usability of US by
projecting the display directly into the clinician's field of view, improving
ergonomics and reducing the cognitive load associated with screen-to-patient
transitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one
streams directly via the application programming interface for a wireless
setup, while the other supports any US device with video output for broader
accessibility. We evaluate RT feasibility and accuracy using the Open Kidney
Dataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with
MedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model
implementations, measurement algorithms, and a Wi-Fi-based streaming solution,
enhancing US training and diagnostics, especially in point-of-care settings.

</details>


### [613] [ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand Challenge](https://arxiv.org/abs/2506.22790)
*Yixu Chen,Bowen Chen,Hai Wei,Alan C. Bovik,Baojun Li,Wei Sun,Linhan Cao,Kang Fu,Dandan Zhu,Jun Jia,Menghan Hu,Xiongkuo Min,Guangtao Zhai,Dounia Hammou,Fei Yin,Rafal Mantiuk,Amritha Premkumar,Prajit T Rajendran,Vignesh V Menon*

Main category: eess.IV

TL;DR: This paper discusses the ICME 2025 Grand Challenge on video quality measurement for generalizable HDR and SDR methods, showcasing the need for robust Video Quality Assessment (VQA) approaches.


<details>
  <summary>Details</summary>
Motivation: The rapid development of HDR and SDR video technologies has highlighted a gap in effective and consistent performance of existing VQA models across dynamic ranges and distortion types.

Method: The challenge invited researchers to submit VQA models capable of addressing both HDR and SDR content. Teams competed in Full Reference (FR) and No Reference (NR) tracks, with final evaluations comparing submitted models against the VMAF baseline.

Result: Four methods surpassed the VMAF baseline, with the top-performing model achieving state-of-the-art performance, thus setting a new standard for HDR/SDR content assessment.

Conclusion: The challenge successfully fostered innovation in video quality assessment models, emphasizing the importance of developing techniques applicable to both HDR and SDR content for robust performance.

Abstract: This paper reports IEEE International Conference on Multimedia \& Expo (ICME)
2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement.
With the rapid development of video technology, especially High Dynamic Range
(HDR) and Standard Dynamic Range (SDR) contents, the need for robust and
generalizable Video Quality Assessment (VQA) methods has become increasingly
demanded. Existing VQA models often struggle to deliver consistent performance
across varying dynamic ranges, distortion types, and diverse content. This
challenge was established to benchmark and promote VQA approaches capable of
jointly handling HDR and SDR content. In the final evaluation phase, five teams
submitted seven models along with technical reports to the Full Reference (FR)
and No Reference (NR) tracks. Among them, four methods outperformed VMAF
baseline, while the top-performing model achieved state-of-the-art performance,
setting a new benchmark for generalizable video quality assessment.

</details>


### [614] [MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation](https://arxiv.org/abs/2506.23102)
*Sunggu Kyung,Jinyoung Seo,Hyunseok Lim,Dongyeong Kim,Hyungbin Park,Jimin Sung,Jihyun Kim,Wooyoung Jo,Yoojin Nam,Namkug Kim*

Main category: eess.IV

TL;DR: MedRegion-CT introduces a region-focused framework for chest CT report generation, emphasizing detailed region-specific features to improve clinical relevance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing CT-based report generation methods struggle with capturing region-specific details, leading to missed abnormalities.

Method: MedRegion-CT proposes three innovations: a region token pooling method using pretrained vision models, universal segmentation with pseudo-masks, and extracting patient-specific attributes for enriching context.

Result: MedRegion-CT achieves state-of-the-art performance in report generation tasks, excelling in both natural language generation quality and clinical relevance.

Conclusion: The framework enhances report generation by focusing on regional and patient-specific features, offering improved interpretability and benchmarking success.

Abstract: The recent release of RadGenome-Chest CT has significantly advanced CT-based
report generation. However, existing methods primarily focus on global
features, making it challenging to capture region-specific details, which may
cause certain abnormalities to go unnoticed. To address this, we propose
MedRegion-CT, a region-focused Multi-Modal Large Language Model (MLLM)
framework, featuring three key innovations. First, we introduce Region
Representative ($R^2$) Token Pooling, which utilizes a 2D-wise pretrained
vision model to efficiently extract 3D CT features. This approach generates
global tokens representing overall slice features and region tokens
highlighting target areas, enabling the MLLM to process comprehensive
information effectively. Second, a universal segmentation model generates
pseudo-masks, which are then processed by a mask encoder to extract
region-centric features. This allows the MLLM to focus on clinically relevant
regions, using six predefined region masks. Third, we leverage segmentation
results to extract patient-specific attributions, including organ size,
diameter, and locations. These are converted into text prompts, enriching the
MLLM's understanding of patient-specific contexts. To ensure rigorous
evaluation, we conducted benchmark experiments on report generation using the
RadGenome-Chest CT. MedRegion-CT achieved state-of-the-art performance,
outperforming existing methods in natural language generation quality and
clinical relevance while maintaining interpretability. The code for our
framework is publicly available.

</details>


### [615] [Multi-Source COVID-19 Detection via Variance Risk Extrapolation](https://arxiv.org/abs/2506.23208)
*Runtian Yuan,Qingqiu Li,Junlin Hou,Jilan Xu,Yuejie Zhang,Rui Feng,Hao Chen*

Main category: eess.IV

TL;DR: This paper addresses the challenge of classifying chest CT scans into COVID and Non-COVID categories across data from distinct institutions, tackling domain shifts through innovative techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address classification challenges in chest CT scans for COVID-19 detection, particularly focusing on domain shift due to variations in medical imaging protocols, scanners, and patient populations.

Method: The method involves incorporating Variance Risk Extrapolation (VREx) to minimize performance variance across domains, alongside Mixup data augmentation to improve generalization by interpolating inputs and labels of training samples.

Result: The proposed method achieved an average macro F1 score of 0.96 across four sources on the validation set, exhibiting strong cross-domain generalization.

Conclusion: The approach effectively mitigates domain shift issues in multi-source chest CT data classification, offering promising tools for robust COVID-19 detection across medical institutions.

Abstract: We present our solution for the Multi-Source COVID-19 Detection Challenge,
which aims to classify chest CT scans into COVID and Non-COVID categories
across data collected from four distinct hospitals and medical centers. A major
challenge in this task lies in the domain shift caused by variations in imaging
protocols, scanners, and patient populations across institutions. To enhance
the cross-domain generalization of our model, we incorporate Variance Risk
Extrapolation (VREx) into the training process. VREx encourages the model to
maintain consistent performance across multiple source domains by explicitly
minimizing the variance of empirical risks across environments. This
regularization strategy reduces overfitting to center-specific features and
promotes learning of domain-invariant representations. We further apply Mixup
data augmentation to improve generalization and robustness. Mixup interpolates
both the inputs and labels of randomly selected pairs of training samples,
encouraging the model to behave linearly between examples and enhancing its
resilience to noise and limited data. Our method achieves an average macro F1
score of 0.96 across the four sources on the validation set, demonstrating
strong generalization.

</details>


### [616] [Improving Myocardial Infarction Detection via Synthetic ECG Pretraining](https://arxiv.org/abs/2506.23259)
*Lachin Naghashyar*

Main category: eess.IV

TL;DR: This paper proposes a deep learning-based pipeline for myocardial infarction (MI) detection using synthetic ECG data, addressing data scarcity challenges.


<details>
  <summary>Details</summary>
Motivation: Accurate early diagnosis of MI from ECGs is a clinical priority, but the lack of large labeled datasets hinders deep learning approaches.

Method: The pipeline synthesizes realistic 12-lead ECGs with tunable MI features and pre-trains classifiers using self-supervised learning combining autoencoding and classification objectives.

Result: Synthetic ECGs enhance classification performance, particularly in low-data scenarios, with improvements in AUC up to 4%.

Conclusion: Controlled synthetic ECGs improve MI detection and provide a viable solution where real clinical ECG data is scarce.

Abstract: Myocardial infarction is a major cause of death globally, and accurate early
diagnosis from electrocardiograms (ECGs) remains a clinical priority. Deep
learning models have shown promise for automated ECG interpretation, but
require large amounts of labeled data, which are often scarce in practice. We
propose a physiology-aware pipeline that (i) synthesizes 12-lead ECGs with
tunable MI morphology and realistic noise, and (ii) pre-trains recurrent and
transformer classifiers with self-supervised masked-autoencoding plus a joint
reconstruction-classification objective. We validate the realism of synthetic
ECGs via statistical and visual analysis, confirming that key morphological
features are preserved. Pretraining on synthetic data consistently improved
classification performance, particularly in low-data settings, with AUC gains
of up to 4 percentage points. These results show that controlled synthetic ECGs
can help improve MI detection when real clinical data is limited.

</details>


### [617] [BPD-Neo: An MRI Dataset for Lung-Trachea Segmentation with Clinical Data for Neonatal Bronchopulmonary Dysplasia](https://arxiv.org/abs/2506.23305)
*Rachit Saluja,Arzu Kovanlikaya,Candace Chien,Lauren Kathryn Blatt,Jeffrey M. Perlman,Stefan Worgall,Mert R. Sabuncu,Jonathan P. Dyke*

Main category: eess.IV

TL;DR: The paper presents a dataset of neonatal lung MRI scans and corresponding semantic segmentations to aid in BPD diagnosis using advanced image processing.


<details>
  <summary>Details</summary>
Motivation: To provide a non-invasive, radiation-free, and sedation-free alternative for diagnosing BPD while offering detailed insights into its mechanisms.

Method: High-resolution 3D MRI data acquired via free-breathing StarVIBE sequences were paired with semantic segmentation algorithms and baseline models for lung and trachea analysis.

Result: A dataset of 40 neonatal MRI scans, most of whom are diagnosed with BPD, along with clinical data and validated segmentation models, was developed.

Conclusion: The dataset has the potential to support further advancements in neonatal lung imaging and improve BPD diagnosis.

Abstract: Bronchopulmonary dysplasia (BPD) is a common complication among preterm
neonates, with portable X-ray imaging serving as the standard diagnostic
modality in neonatal intensive care units (NICUs). However, lung magnetic
resonance imaging (MRI) offers a non-invasive alternative that avoids sedation
and radiation while providing detailed insights into the underlying mechanisms
of BPD. Leveraging high-resolution 3D MRI data, advanced image processing and
semantic segmentation algorithms can be developed to assist clinicians in
identifying the etiology of BPD. In this dataset, we present MRI scans paired
with corresponding semantic segmentations of the lungs and trachea for 40
neonates, the majority of whom are diagnosed with BPD. The imaging data consist
of free-breathing 3D stack-of-stars radial gradient echo acquisitions, known as
the StarVIBE series. Additionally, we provide comprehensive clinical data and
baseline segmentation models, validated against clinical assessments, to
support further research and development in neonatal lung imaging.

</details>


### [618] [SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting](https://arxiv.org/abs/2506.23309)
*Yiming Huang,Long Bai,Beilei Cui,Kun Yuan,Guankun Wang,Mobarakol Islam,Nicolas Padoy,Nassir Navab,Hongliang Ren*

Main category: eess.IV

TL;DR: This paper introduces SurgTPGS, a novel method for real-time text-promptable 3D surgical scene understanding, which combines advancements in 3D reconstruction, segmentation, and vision-language models to enhance precision and safety in surgical practices.


<details>
  <summary>Details</summary>
Motivation: The motivation is the lack of support for real-time, text-promptable 3D queries in surgical planning and intra-operative guidance, which are essential for identifying and interacting with tools and anatomical structures in complex surgical environments.

Method: The authors propose SurgTPGS, which integrates a 3D semantics feature learning strategy leveraging the Segment Anything model and advanced vision-language models. It incorporates semantic-aware deformation tracking for precise reconstruction and semantic region-aware optimization to enhance quality and smoothness.

Result: SurgTPGS outperforms state-of-the-art methods in experiments on two real-world surgical datasets, showcasing its ability to provide superior 3D surgical scene reconstruction and understanding.

Conclusion: SurgTPGS has the potential to revolutionize surgical systems by enabling text-promptable 3D scene comprehension, enhancing surgical precision, and advancing safety. Its code is publicly available for future research and development.

Abstract: In contemporary surgical research and practice, accurately comprehending 3D
surgical scenes with text-promptable capabilities is particularly crucial for
surgical planning and real-time intra-operative guidance, where precisely
identifying and interacting with surgical tools and anatomical structures is
paramount. However, existing works focus on surgical vision-language model
(VLM), 3D reconstruction, and segmentation separately, lacking support for
real-time text-promptable 3D queries. In this paper, we present SurgTPGS, a
novel text-promptable Gaussian Splatting method to fill this gap. We introduce
a 3D semantics feature learning strategy incorporating the Segment Anything
model and state-of-the-art vision-language models. We extract the segmented
language features for 3D surgical scene reconstruction, enabling a more
in-depth understanding of the complex surgical environment. We also propose
semantic-aware deformation tracking to capture the seamless deformation of
semantic features, providing a more precise reconstruction for both texture and
semantic features. Furthermore, we present semantic region-aware optimization,
which utilizes regional-based semantic information to supervise the training,
particularly promoting the reconstruction quality and semantic smoothness. We
conduct comprehensive experiments on two real-world surgical datasets to
demonstrate the superiority of SurgTPGS over state-of-the-art methods,
highlighting its potential to revolutionize surgical practices. SurgTPGS paves
the way for developing next-generation intelligent surgical systems by
enhancing surgical precision and safety. Our code is available at:
https://github.com/lastbasket/SurgTPGS.

</details>


### [619] [FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction](https://arxiv.org/abs/2506.23466)
*Qiqing Liu,Guoquan Wei,Zekun Zhou,Yiyang Wen,Liu Shi,Qiegen Liu*

Main category: eess.IV

TL;DR: This paper introduces FD-DiT, a frequency-domain diffusion transformer tailored for improved low-dose CT image reconstruction, ensuring better noise and artifact suppression over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle image artifacts and detail loss in low-dose computed tomography (LDCT) scans caused by quantum and electronic noise, which can impact diagnostic accuracy.

Method: FD-DiT integrates diffusion transformer strategies and frequency decoupling techniques, focusing on denoising high-frequency noise and utilizing hybrid networks, sparse local attention, and dynamic fusion for reconstruction.

Result: Experimental results show FD-DiT significantly outperforms state-of-the-art methods in noise and artifact reduction at identical LDCT dose levels.

Conclusion: FD-DiT enhances LDCT image reconstruction by integrating advanced denoising techniques, offering superior diagnostic-quality imaging with lower radiation exposure.

Abstract: Low-dose computed tomography (LDCT) reduces radiation exposure but suffers
from image artifacts and loss of detail due to quantum and electronic noise,
potentially impacting diagnostic accuracy. Transformer combined with diffusion
models has been a promising approach for image generation. Nevertheless,
existing methods exhibit limitations in preserving finegrained image details.
To address this issue, frequency domain-directed diffusion transformer (FD-DiT)
is proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy
that progressively introduces noise until the distribution statistically aligns
with that of LDCT data, followed by denoising processing. Furthermore, we
employ a frequency decoupling technique to concentrate noise primarily in
high-frequency domain, thereby facilitating effective capture of essential
anatomical structures and fine details. A hybrid denoising network is then
utilized to optimize the overall data reconstruction process. To enhance the
capability in recognizing high-frequency noise, we incorporate sliding sparse
local attention to leverage the sparsity and locality of shallow-layer
information, propagating them via skip connections for improving feature
representation. Finally, we propose a learnable dynamic fusion strategy for
optimal component integration. Experimental results demonstrate that at
identical dose levels, LDCT images reconstructed by FD-DiT exhibit superior
noise and artifact suppression compared to state-of-the-art methods.

</details>


### [620] [AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm](https://arxiv.org/abs/2506.23537)
*Xinyue Li,Zhangkai Ni,Wenhan Yang*

Main category: eess.IV

TL;DR: This paper introduces AFUNet, a learning-based deep unfolding network model for multi-exposure HDR reconstruction, combining alignment and fusion subtasks iteratively for enhanced performance.


<details>
  <summary>Details</summary>
Motivation: Existing HDR reconstruction methods rely heavily on empirical designs, lacking a theoretical foundation, which affects reliability.

Method: The paper proposes AFUNet, which systematically decouples HDR reconstruction into alignment and fusion subtasks optimized iteratively based on the Maximum A Posteriori (MAP) estimation framework.

Result: AFUNet achieves superior HDR reconstruction performance, bridging misalignments and exposure discrepancies, and consistently outperforms state-of-the-art approaches in qualitative and quantitative evaluations.

Conclusion: The proposed AFUNet method for HDR reconstruction effectively combines mathematical foundation and practical efficiency, demonstrating a significant advancement over existing approaches.

Abstract: Existing learning-based methods effectively reconstruct HDR images from
multi-exposure LDR inputs with extended dynamic range and improved detail, but
they rely more on empirical design rather than theoretical foundation, which
can impact their reliability. To address these limitations, we propose the
cross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR
reconstruction is systematically decoupled into two interleaved subtasks --
alignment and fusion -- optimized through alternating refinement, achieving
synergy between the two subtasks to enhance the overall performance. Our method
formulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP)
estimation perspective, explicitly incorporating spatial correspondence priors
across LDR images and naturally bridging the alignment and fusion subproblems
through joint constraints. Building on the mathematical foundation, we
reimagine traditional iterative optimization through unfolding -- transforming
the conventional solution process into an end-to-end trainable AFUNet with
carefully designed modules that work progressively. Specifically, each
iteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that
alternates between a Spatial Alignment Module (SAM) for alignment and a Channel
Fusion Module (CFM) for adaptive feature fusion, progressively bridging
misaligned content and exposure discrepancies. Extensive qualitative and
quantitative evaluations demonstrate AFUNet's superior performance,
consistently surpassing state-of-the-art methods. Our code is available at:
https://github.com/eezkni/AFUNet

</details>


### [621] [Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation](https://arxiv.org/abs/2506.23664)
*Fangyijie Wang,Kevin Whelan,Félix Balado,Guénolé Silvestre,Kathleen M. Curran*

Main category: eess.IV

TL;DR: This study uses mask-guided diffusion models for generating synthetic fetal head ultrasound images and segmentation masks to augment datasets, achieving state-of-the-art segmentation using minimal real data.


<details>
  <summary>Details</summary>
Motivation: Current medical image analysis faces challenges due to privacy concerns and the high cost of manual labeling by clinical experts, necessitating solutions like synthetic data generation.

Method: A novel mask-guided generative AI approach employing diffusion models was used to generate synthetic fetal ultrasound images paired with segmentation masks, integrated with dataset augmentation for fine-tuning the Segment Anything Model.

Result: Using synthetic data, the model achieved Dice Scores of 94.66% and 94.38% for segmentation in Spanish and African cohorts, respectively, demonstrating its effectiveness even with limited real data.

Conclusion: The proposed method effectively generates synthetic medical data, capturing real image features and enabling superior segmentation performance, offering a promising solution for data-scarce medical imaging applications.

Abstract: Medical image data is less accessible than in other domains due to privacy
and regulatory constraints. In addition, labeling requires costly,
time-intensive manual image annotation by clinical experts. To overcome these
challenges, synthetic medical data generation offers a promising solution.
Generative AI (GenAI), employing generative deep learning models, has proven
effective at producing realistic synthetic images. This study proposes a novel
mask-guided GenAI approach using diffusion models to generate synthetic fetal
head ultrasound images paired with segmentation masks. These synthetic pairs
augment real datasets for supervised fine-tuning of the Segment Anything Model
(SAM). Our results show that the synthetic data captures real image features
effectively, and this approach reaches state-of-the-art fetal head
segmentation, especially when trained with a limited number of real image-mask
pairs. In particular, the segmentation reaches Dice Scores of 94.66\% and
94.38\% using a handful of ultrasound images from the Spanish and African
cohorts, respectively. Our code, models, and data are available on GitHub.

</details>


### [622] [MedSAM-CA: A CNN-Augmented ViT with Attention-Enhanced Multi-Scale Fusion for Medical Image Segmentation](https://arxiv.org/abs/2506.23700)
*Peiting Tian,Xi Chen,Haixia Bi,Fan Li*

Main category: eess.IV

TL;DR: The paper introduces MedSAM-CA, a method for medical image segmentation that minimizes reliance on large datasets by enhancing the MedSAM model with architectural refinements, achieving high accuracy even in low-resource settings.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve the accuracy of medical image segmentation in low-resource clinical settings, addressing challenges like the need for large annotated datasets and difficult imaging scenarios.

Method: MedSAM-CA incorporates two novel mechanisms: CBR-Net, which adds convolutional attention for boundary refinement, and Atte-FFB, which fuses multi-level features to enhance delineation accuracy. These innovations are integrated with MedSAM.

Result: When tested on dermoscopy datasets, MedSAM-CA achieved 94.43% Dice with only 2% of the full training data and reached 97.25% of the performance obtained with full-data training.

Conclusion: MedSAM-CA demonstrates strong potential in addressing limitations of existing segmentation techniques, offering highly accurate results in scenarios with limited annotated data and challenging imaging conditions.

Abstract: Medical image segmentation plays a crucial role in clinical diagnosis and
treatment planning, where accurate boundary delineation is essential for
precise lesion localization, organ identification, and quantitative assessment.
In recent years, deep learning-based methods have significantly advanced
segmentation accuracy. However, two major challenges remain. First, the
performance of these methods heavily relies on large-scale annotated datasets,
which are often difficult to obtain in medical scenarios due to privacy
concerns and high annotation costs. Second, clinically challenging scenarios,
such as low contrast in certain imaging modalities and blurry lesion boundaries
caused by malignancy, still pose obstacles to precise segmentation. To address
these challenges, we propose MedSAM-CA, an architecture-level fine-tuning
approach that mitigates reliance on extensive manual annotations by adapting
the pretrained foundation model, Medical Segment Anything (MedSAM). MedSAM-CA
introduces two key components: the Convolutional Attention-Enhanced Boundary
Refinement Network (CBR-Net) and the Attention-Enhanced Feature Fusion Block
(Atte-FFB). CBR-Net operates in parallel with the MedSAM encoder to recover
boundary information potentially overlooked by long-range attention mechanisms,
leveraging hierarchical convolutional processing. Atte-FFB, embedded in the
MedSAM decoder, fuses multi-level fine-grained features from skip connections
in CBR-Net with global representations upsampled within the decoder to enhance
boundary delineation accuracy. Experiments on publicly available datasets
covering dermoscopy, CT, and MRI imaging modalities validate the effectiveness
of MedSAM-CA. On dermoscopy dataset, MedSAM-CA achieves 94.43% Dice with only
2% of full training data, reaching 97.25% of full-data training performance,
demonstrating strong effectiveness in low-resource clinical settings.

</details>


### [623] [MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction](https://arxiv.org/abs/2506.23701)
*Lingtong Zhang,Mengdie Song,Xiaohan Hao,Huayu Mai,Bensheng Qiu*

Main category: eess.IV

TL;DR: The paper introduces Multi-domain Diffusion Prior Guidance (MDPG), which enhances MRI reconstruction by using latent diffusion models (LDMs) and a novel fusion strategy.


<details>
  <summary>Details</summary>
Motivation: The difficulty in achieving high-fidelity, data-consistent MRI reconstructions with existing generative models motivates this work.

Method: The authors propose a Visual-Mamba-based backbone and integrate pre-trained latent diffusion models (LDMs) with novel techniques like Latent Guided Attention (LGA) and a Dual-domain Fusion Branch (DFB). A k-space regularization strategy is also introduced.

Result: Extensive experiments conducted on two public MRI datasets confirm the method’s effectiveness in achieving accurate reconstructions.

Conclusion: The proposed framework improves MRI reconstruction quality by effectively incorporating prior knowledge and enhancing data consistency.

Abstract: Magnetic Resonance Imaging (MRI) reconstruction is essential in medical
diagnostics. As the latest generative models, diffusion models (DMs) have
struggled to produce high-fidelity images due to their stochastic nature in
image domains. Latent diffusion models (LDMs) yield both compact and detailed
prior knowledge in latent domains, which could effectively guide the model
towards more effective learning of the original data distribution. Inspired by
this, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by
pre-trained LDMs to enhance data consistency in MRI reconstruction tasks.
Specifically, we first construct a Visual-Mamba-based backbone, which enables
efficient encoding and reconstruction of under-sampled images. Then pre-trained
LDMs are integrated to provide conditional priors in both latent and image
domains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion
in multi-level latent domains. Simultaneously, to effectively utilize a prior
in both the k-space and image domain, under-sampled images are fused with
generated full-sampled images by the Dual-domain Fusion Branch (DFB) for
self-adaption guidance. Lastly, to further enhance the data consistency, we
propose a k-space regularization strategy based on the non-auto-calibration
signal (NACS) set. Extensive experiments on two public MRI datasets fully
demonstrate the effectiveness of the proposed methodology. The code is
available at https://github.com/Zolento/MDPG.

</details>


### [624] [Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos](https://arxiv.org/abs/2506.23759)
*Zheng Fang,Xiaoming Qi,Chun-Mei Feng,Jialun Pei,Weixin Si,Yueming Jin*

Main category: eess.IV

TL;DR: The paper introduces FedST, a novel Federated Learning (FL) approach for surgical instrument segmentation, enhancing collaboration between multiple sites without centralizing data.


<details>
  <summary>Details</summary>
Motivation: To address the lack of effective Federated Learning techniques specifically tailored for surgical instrument segmentation in diverse scenarios and leverage synthetic data efficiently.

Method: The FedST model incorporates a Representation Separation and Cooperation mechanism for local-site training and Synthesis-based Explicit Representation Quantification for global-server training. It decouples embedding layers for personalized background handling while ensuring consistent instrument representation.

Result: The proposed FedST method improves segmentation accuracy by strategically utilizing synthetic data, enhancing both site-specific adaptability and generalization across surgical datasets.

Conclusion: FedST effectively leverages surgical domain characteristics to boost segmentation performance in Federated Learning settings, presenting a scalable solution for collaborative surgical data science research.

Abstract: Surgical instrument segmentation under Federated Learning (FL) is a promising
direction, which enables multiple surgical sites to collaboratively train the
model without centralizing datasets. However, there exist very limited FL works
in surgical data science, and FL methods for other modalities do not consider
inherent characteristics in surgical domain: i) different scenarios show
diverse anatomical backgrounds while highly similar instrument representation;
ii) there exist surgical simulators which promote large-scale synthetic data
generation with minimal efforts. In this paper, we propose a novel Personalized
FL scheme, Spatio-Temporal Representation Decoupling and Enhancement (FedST),
which wisely leverages surgical domain knowledge during both local-site and
global-server training to boost segmentation. Concretely, our model embraces a
Representation Separation and Cooperation (RSC) mechanism in local-site
training, which decouples the query embedding layer to be trained privately, to
encode respective backgrounds. Meanwhile, other parameters are optimized
globally to capture the consistent representations of instruments, including
the temporal layer to capture similar motion patterns. A textual-guided channel
selection is further designed to highlight site-specific features, facilitating
model adapta tion to each site. Moreover, in global-server training, we propose
Synthesis-based Explicit Representation Quantification (SERQ), which defines an
explicit representation target based on synthetic data to synchronize the model
convergence during fusion for improving model generalization.

</details>


### [625] [ShapeKit](https://arxiv.org/abs/2506.24003)
*Junqi Liu,Dongli He,Wenxuan Li,Ningyu Wang,Alan L. Yuille,Zongwei Zhou*

Main category: eess.IV

TL;DR: A new toolkit, ShapeKit, is presented to improve anatomical segmentation accuracy by over 8% without retraining models.


<details>
  <summary>Details</summary>
Motivation: To address the limited efficacy of model architecture changes in improving segmentation accuracy and to highlight the importance of shape-focused methods.

Method: Introducing ShapeKit, a flexible, integrable toolkit designed to refine anatomical shapes in segmentation tasks without modifying model architecture.

Result: The toolkit improves segmentation performance by over 8%, outperforming changes in model architecture which yield less than 3% improvement.

Conclusion: Shape-focused tools like ShapeKit are valuable in improving medical segmentation and deserve more attention from the research community.

Abstract: In this paper, we present a practical approach to improve anatomical shape
accuracy in whole-body medical segmentation. Our analysis shows that a
shape-focused toolkit can enhance segmentation performance by over 8%, without
the need for model re-training or fine-tuning. In comparison, modifications to
model architecture typically lead to marginal gains of less than 3%. Motivated
by this observation, we introduce ShapeKit, a flexible and easy-to-integrate
toolkit designed to refine anatomical shapes. This work highlights the
underappreciated value of shape-based tools and calls attention to their
potential impact within the medical segmentation community.

</details>


### [626] [C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism](https://arxiv.org/abs/2506.24074)
*Mayank V. Golhar,Lucas Sebastian Galeano Fretes,Loren Ayers,Venkata S. Akshintala,Taylor L. Bobrow,Nicholas J. Durr*

Main category: eess.IV

TL;DR: This paper introduces C3VDv2, a realistic, high-definition 3D colonoscopy video dataset designed to improve 3D reconstruction algorithm evaluation by providing diverse scenarios and ground truth data.


<details>
  <summary>Details</summary>
Motivation: The development of computer vision methods for colonoscopy diagnostics is hindered by the lack of 3D datasets for training and validation to ensure accurate performance.

Method: The dataset includes 192 high-fidelity 3D video recordings from 60 silicone colon phantoms, accompanied by detailed ground truth data for 169 videos and simulated screenings, along with challenging scenarios.

Result: C3VDv2 offers diverse, realistic environment conditions such as fecal debris, lens obstructions, and rapid camera motion and includes a comprehensive range of metadata to evaluate reconstruction algorithms.

Conclusion: C3VDv2 provides a meaningful step forward, offering a high-quality, diverse, and realistic dataset for advancing colonoscopy-related 3D computer vision technologies.

Abstract: Computer vision techniques have the potential to improve the diagnostic
performance of colonoscopy, but the lack of 3D colonoscopy datasets for
training and validation hinders their development. This paper introduces
C3VDv2, the second version (v2) of the high-definition Colonoscopy 3D Video
Dataset, featuring enhanced realism designed to facilitate the quantitative
evaluation of 3D colon reconstruction algorithms. 192 video sequences were
captured by imaging 60 unique, high-fidelity silicone colon phantom segments.
Ground truth depth, surface normals, optical flow, occlusion,
six-degree-of-freedom pose, coverage maps, and 3D models are provided for 169
colonoscopy videos. Eight simulated screening colonoscopy videos acquired by a
gastroenterologist are provided with ground truth poses. The dataset includes
15 videos featuring colon deformations for qualitative assessment. C3VDv2
emulates diverse and challenging scenarios for 3D reconstruction algorithms,
including fecal debris, mucous pools, blood, debris obscuring the colonoscope
lens, en-face views, and fast camera motion. The enhanced realism of C3VDv2
will allow for more robust and representative development and evaluation of 3D
reconstruction algorithms.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [627] [Service Placement in Small Cell Networks Using Distributed Best Arm Identification in Linear Bandits](https://arxiv.org/abs/2506.22480)
*Mariam Yahya,Aydin Sezgin,Setareh Maghsudi*

Main category: cs.NI

TL;DR: This paper addresses service placement in multi-access edge computing (MEC) by introducing a distributed linear bandit-based best-arm identification algorithm, optimizing service deployment to minimize user delay.


<details>
  <summary>Details</summary>
Motivation: The growing reliance on computation-intensive services in small cell networks leads to high latency in cloud-based access. MEC provides a potential solution by delivering services through local small base stations, but deciding which services to deploy locally faces challenges due to limited edge capacity, unknown demand, and dynamic network conditions.

Method: The problem is modeled as a linear bandit problem, where SBSs act as agents and services as arms. The authors develop a distributed, adaptive, multi-agent best-arm identification (BAI) algorithm with a fixed-confidence setting. This algorithm enables SBSs to collaborate and identify the optimal service placement to minimize total user delay.

Result: The algorithm effectively identifies the optimal service placement with the desired confidence and achieves near-optimal speedup, with learning rounds reducing proportionally to the number of SBSs. Simulations validate the approach, and theoretical analysis examines sample complexity and communication overhead.

Conclusion: The proposed algorithm successfully addresses the challenges of service placement in MEC, enabling efficient local deployment of services while reducing user delays, and demonstrates scalability and effectiveness under collaborative settings.

Abstract: As users in small cell networks increasingly rely on computation-intensive
services, cloud-based access often results in high latency. Multi-access edge
computing (MEC) mitigates this by bringing computational resources closer to
end users, with small base stations (SBSs) serving as edge servers to enable
low-latency service delivery. However, limited edge capacity makes it
challenging to decide which services to deploy locally versus in the cloud,
especially under unknown service demand and dynamic network conditions. To
tackle this problem, we model service demand as a linear function of service
attributes and formulate the service placement task as a linear bandit problem,
where SBSs act as agents and services as arms. The goal is to identify the
service that, when placed at the edge, offers the greatest reduction in total
user delay compared to cloud deployment. We propose a distributed and adaptive
multi-agent best-arm identification (BAI) algorithm under a fixed-confidence
setting, where SBSs collaborate to accelerate learning. Simulations show that
our algorithm identifies the optimal service with the desired confidence and
achieves near-optimal speedup, as the number of learning rounds decreases
proportionally with the number of SBSs. We also provide theoretical analysis of
the algorithm's sample complexity and communication overhead.

</details>


### [628] [Reliable Image Transmission in CPS-based Pub/Sub](https://arxiv.org/abs/2506.22875)
*Everson Flores,Bruna Guterres,Thomaz Pereira Junior,Paula Barros,Alberto Cabral,Cristiana Lima Dora,Marcelo Malheiros,Marcelo Pias*

Main category: cs.NI

TL;DR: This study investigates the performance of MQTT protocol for real-time image transmission in industrial IoT and CPS systems under high-traffic and network interruption scenarios.


<details>
  <summary>Details</summary>
Motivation: There is a literature gap regarding the effectiveness of MQTT for image sharing in critical IoT and CPS applications, especially under challenging network conditions.

Method: The study evaluates a distributed MQTT-based system through controlled testbed experiments to test performance under network interruptions and high traffic.

Result: Reliability is sustained under normal conditions, while recovery depends on the failure point. The system avoids duplicate errors and adapts to increasing demands, showcasing its robustness for industrial use.

Conclusion: MQTT-based systems are suitable for industrial IoT applications, providing efficient and resilient data handling even under challenging conditions.

Abstract: Developments in communication and automation have driven the expansion of
distributed networks, essential for IoT and CPS development in industrial
applications requiring reliable image processing and real-time adaptability.
Although broadly adopted, there is a literature gap regarding the performance
of MQTT protocol for image sharing and transmission under high-traffic
scenarios with intermittent connectivity, restricting its use in critical IoT
and CPS applications. In this context, the present work examines the
reliability of real-time image transmission in IoT and CPS industrial systems
that utilize the MQTT-based publish/subscribe communication model. It focuses
on scenarios with network interruptions and high data traffic, evaluating the
performance of a distributed system through a series of controlled testbed
validation experiments. Experimental validation demonstrated that while the
MQTT-based system sustains reliable transmission under normal conditions, its
recovery capability depends on the failure point, with complete restoration
occurring when disruptions affect the Orchestrator Node and partial recovery
when the Producer Node or Broker are affected. The study also confirmed that
the system prevents duplicate errors and adapts well to increasing network
demands, reinforcing its suitability for industrial applications that require
efficient and resilient data handling.

</details>


### [629] [Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI](https://arxiv.org/abs/2506.22477)
*Huiwen Han*

Main category: cs.NI

TL;DR: This paper introduces an IoT-based robotic platform integrating LLMs, generative AI, edge computing, and 5G to improve intelligence, autonomy, and scalability, with case studies across industries.


<details>
  <summary>Details</summary>
Motivation: To enhance the capabilities of IoT systems and robotics through greater real-time decision-making, autonomy, and adaptability, using advanced technologies.

Method: The approach integrates LLMs, generative AI, edge computing, and 5G into a unified IoT architecture. Validation is through case studies in industries like manufacturing and healthcare.

Result: The platform demonstrated significant improvements in workflow optimization, productivity, and scalability across diverse industrial applications.

Conclusion: The research underscores the transformative potential of LLMs and generative AI in shaping the future of IoT and robotics, highlighting broad industrial and societal impacts.

Abstract: This paper introduces an innovative design for robotic operating platforms,
underpinned by a transformative Internet of Things (IoT) architecture,
seamlessly integrating cutting-edge technologies such as large language models
(LLMs), generative AI, edge computing, and 5G networks. The proposed platform
aims to elevate the intelligence and autonomy of IoT systems and robotics,
enabling them to make real-time decisions and adapt dynamically to changing
environments. Through a series of compelling case studies across industries
including smart manufacturing, healthcare, and service sectors, this paper
demonstrates the substantial potential of IoT-enabled robotics to optimize
operational workflows, enhance productivity, and deliver innovative, scalable
solutions. By emphasizing the roles of LLMs and generative AI, the research
highlights how these technologies drive the evolution of intelligent robotics
and IoT, shaping the future of industry-specific advancements. The findings not
only showcase the transformative power of these technologies but also offer a
forward-looking perspective on their broader societal and industrial
implications, positioning them as catalysts for next-generation automation and
technological convergence.

</details>


### [630] [AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space](https://arxiv.org/abs/2506.22487)
*Amar Khelloufi,Huansheng Ning,Sahraoui Dhelim,Jianguo Ding*

Main category: cs.NI

TL;DR: The paper surveys how Artificial General Intelligence (AGI) can address critical challenges in the Internet of Everything (IoX) through advancements across sensing, network, and application layers.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address bottlenecks in Cyber-Physical-Social Thinking ecosystems by integrating IoX with AGI for improved data management, network protocols, and decision-making frameworks.

Method: Systematic review of AGI-enhanced IoX research, focusing on adaptive sensor fusion at the sensing layer, protocol optimization at the network layer, and decision-making frameworks at the application layer.

Result: Key findings highlight AGI strategies such as adaptive sensor fusion and semantic modeling that mitigate sensing-layer data overload, network protocol heterogeneity, and application-layer identity explosion.

Conclusion: AGI can significantly enhance IoX performance, with future research needed in computation scalability, ethical governance, and system validation.

Abstract: The integration of the Internet of Everything (IoX) and Artificial General
Intelligence (AGI) has given rise to a transformative paradigm aimed at
addressing critical bottlenecks across sensing, network, and application layers
in Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide
a systematic and comprehensive review of AGI-enhanced IoX research, focusing on
three key components: sensing-layer data management, network-layer protocol
optimization, and application-layer decision-making frameworks. Specifically,
this survey explores how AGI can mitigate IoX bottlenecks challenges by
leveraging adaptive sensor fusion, edge preprocessing, and selective attention
mechanisms at the sensing layer, while resolving network-layer issues such as
protocol heterogeneity and dynamic spectrum management, neuro-symbolic
reasoning, active inference, and causal reasoning, Furthermore, the survey
examines AGI-enabled frameworks for managing identity and relationship
explosion. Key findings suggest that AGI-driven strategies, such as adaptive
sensor fusion, edge preprocessing, and semantic modeling, offer novel solutions
to sensing-layer data overload, network-layer protocol heterogeneity, and
application-layer identity explosion. The survey underscores the importance of
cross-layer integration, quantum-enabled communication, and ethical governance
frameworks for future AGI-enabled IoX systems. Finally, the survey identifies
unresolved challenges, such as computational requirements, scalability, and
real-world validation, calling for further research to fully realize AGI's
potential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is
emerging as a critical research field at the intersection of interconnected
systems and advanced AI.

</details>


### [631] [Offline Reinforcement Learning for Mobility Robustness Optimization](https://arxiv.org/abs/2506.22793)
*Pegah Alizadeh,Anastasios Giovanidis,Pradeepa Ramachandra,Vasileios Koutsoukis,Osama Arouk*

Main category: cs.NI

TL;DR: This paper explores using offline reinforcement learning methods to improve Mobility Robustness Optimization (MRO) in New Radio networks, achieving up to 7% improvement over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance the Mobility Robustness Optimization (MRO) process by leveraging offline reinforcement learning to address limitations of traditional rule-based algorithms.

Method: The study applies offline reinforcement learning methods, specifically Decision Transformers and Conservative Q-Learning, to learn optimal Cell Individual Offset tuning using input features such as failures and handover issues.

Result: Offline reinforcement learning methods outperform traditional rule-based MRO algorithms, demonstrating up to 7% improved performance in New Radio networks operating at a 3500 MHz carrier frequency.

Conclusion: Offline reinforcement learning is a promising approach for MRO, allowing for both performance improvements and operational flexibility by enabling diverse objective functions using the same datasets.

Abstract: In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm
and study the possibility of learning the optimal Cell Individual Offset tuning
using offline Reinforcement Learning. Such methods make use of collected
offline datasets to learn the optimal policy, without further exploration. We
adapt and apply a sequence-based method called Decision Transformers as well as
a value-based method called Conservative Q-Learning to learn the optimal policy
for the same target reward as the vanilla rule-based MRO. The same input
features related to failures, ping-pongs, and other handover issues are used.
Evaluation for realistic New Radio networks with 3500 MHz carrier frequency on
a traffic mix including diverse user service types and a specific tunable
cell-pair shows that offline-RL methods outperform rule-based MRO, offering up
to 7% improvement. Furthermore, offline-RL can be trained for diverse objective
functions using the same available dataset, thus offering operational
flexibility compared to rule-based methods.

</details>


### [632] [Geminet: Learning the Duality-based Iterative Process for Lightweight Traffic Engineering in Changing Topologies](https://arxiv.org/abs/2506.23640)
*Ximeng Liu,Shizhen Zhao,Xinbing Wang*

Main category: cs.NI

TL;DR: Geminet proposes a scalable and lightweight ML-based Traffic Engineering framework addressing topology changes, reducing memory usage, and enhancing scalability compared to existing methods like HARP.


<details>
  <summary>Details</summary>
Motivation: Current ML-based TE approaches fail to handle topology changes or suffer from poor scalability due to high computational and memory requirements.

Method: Geminet uses a topology-agnostic gradient-descent process and shifts optimization from path-level routing weights to edge-level dual variables, reducing memory and computational overhead.

Result: Geminet is significantly smaller in neural network size (0.04% to 7% of others), handles topology variations effectively, uses under 10 GiB of memory, and achieves 5.45 times faster convergence compared to HARP.

Conclusion: Geminet addresses the impracticalities of ML-based TE for large-scale usage, improving efficiency and scalability without performance compromises.

Abstract: Recently, researchers have explored ML-based Traffic Engineering (TE),
leveraging neural networks to solve TE problems traditionally addressed by
optimization. However, existing ML-based TE schemes remain impractical: they
either fail to handle topology changes or suffer from poor scalability due to
excessive computational and memory overhead. To overcome these limitations, we
propose Geminet, a lightweight and scalable ML-based TE framework that can
handle changing topologies. Geminet is built upon two key insights: (i) a
methodology that decouples neural networks from topology by learning an
iterative gradient-descent-based adjustment process, as the update rule of
gradient descent is topology-agnostic, relying only on a few gradient-related
quantities; (ii) shifting optimization from path-level routing weights to
edge-level dual variables, reducing memory consumption by leveraging the fact
that edges are far fewer than paths. Evaluations on WAN and data center
datasets show that Geminet significantly improves scalability. Its neural
network size is only 0.04% to 7% of existing schemes, while handling topology
variations as effectively as HARP, a state-of-the-art ML-based TE approach,
without performance degradation. When trained on large-scale topologies,
Geminet consumes under 10 GiB of memory, more than eight times less than the
80-plus GiB required by HARP, while achieving 5.45 times faster convergence
speed, demonstrating its potential for large-scale deployment.

</details>


### [633] [The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking](https://arxiv.org/abs/2506.23628)
*Antonio Ojea*

Main category: cs.NI

TL;DR: The paper proposes Kubernetes Network Drivers (KNDs), a new approach to address the challenges in Kubernetes networking for high-performance AI/ML workloads and Telco infrastructure.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in Kubernetes networking that cannot meet the demands of high-performance AI/ML applications and evolving Telco infrastructure.

Method: The method involves creating Kubernetes Network Drivers (KNDs) using advances like Dynamic Resource Allocation (DRA), Node Resource Interface (NRI), and changes in the OCI Runtime Specification for modular and declarative network management.

Result: The implementation, DraNet, allows for declarative attachment of advanced network interfaces such as RDMA devices, offering significant performance improvements for AI/ML workloads.

Conclusion: This new architecture enhances cloud-native application performance, reduces operational complexity, supports future Telco solutions, and lays the foundation for a diverse ecosystem of KNDs.

Abstract: Traditional Kubernetes networking struggles to meet the escalating demands of
AI/ML and evolving Telco infrastructure. This paper introduces Kubernetes
Network Drivers (KNDs), a transformative, modular, and declarative architecture
designed to overcome current imperative provisioning and API limitations. KNDs
integrate network resource management into Kubernetes' core by utilizing
Dynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements,
and upcoming OCI Runtime Specification changes. Our DraNet implementation
demonstrates declarative attachment of network interfaces, including Remote
Direct Memory Access (RDMA) devices, significantly boosting high-performance
AI/ML workloads. This capability enables sophisticated cloud-native
applications and lays crucial groundwork for future Telco solutions, fostering
a "galaxy" of specialized KNDs for enhanced application delivery and reduced
operational complexity.

</details>


### [634] [Learning Constraints Directly from Network Data](https://arxiv.org/abs/2506.23964)
*Hongyu Hè,Minhao Jin,Maria Apostolaki*

Main category: cs.NI

TL;DR: The paper introduces NetNomos, a system that extracts network data rules as logic constraints for improved data quality, ML robustness, and semantic understanding. It outperforms baselines in speed and rule discovery.


<details>
  <summary>Details</summary>
Motivation: To formalize network data rules as logic constraints for improving synthetic data quality, reducing ML brittleness, and enhancing semantic understanding, as existing manual or ML-dependent methods are incomplete and unreliable.

Method: NetNomos formulates rule extraction as a constraint modeling problem and employs a lattice-based search structured by constraint specificity and succinctness to efficiently learn logic constraints from raw network measurements.

Result: NetNomos achieves better performance than baseline methods, learning all benchmark rules (even for rare data points) in under three hours, while baselines discover less than 25% of the rules and take several days.

Conclusion: NetNomos is effective for discovering network constraints, enabling use cases such as synthetic data generation validation, anomaly detection, and telemetry imputation, outperforming traditional methods in efficiency and accuracy.

Abstract: Network data conforms to a wide range of rules that arise from protocols,
design principles, and deployment decisions (e.g., a packet's queuing delay
must be less than its end-to-end delay). Formalizing such rules as logic
constraints can (i) improve the quality of synthetic data, (ii) reduce the
brittleness of machine learning (ML) models, and (iii) improve semantic
understanding of network measurements. However, these benefits remain out of
reach if rule extraction is manual or solely reliant on ML, as both approaches
yield incomplete, unreliable, and/or inaccurate rules.
  This paper formulates rule extraction as a constraint modeling problem and
introduces NetNomos that learns propositional logic constraints directly from
raw network measurements. Constraint modeling in this domain is uniquely
challenging due to the scale of the data, the inherent learning complexity and
passive environment, and the lack of ground truth supervision. NetNomos
addresses these challenges via a lattice-based search structured by constraint
specificity and succinctness. Our approach reduces learning complexity from
superquadratic to logarithmic and enables efficient traversal in combinatorial
search space.
  Our evaluations on diverse network datasets show that NetNomos learns all
benchmark rules, including those associated with as little as 0.01% of data
points, in under three hours. In contrast, baseline methods discover less than
25% of the rules and require several days to run. Through three case studies,
we show that: NetNomos (i) finds rule violations in the outputs of all seven
synthetic traffic generators, hence can be used to assess and guide their
generation process; (ii) detects semantic differences in traffic, hence can be
used for anomaly detection; and (iii) automatically finds rules used for
telemetry imputation, hence can support monitoring through inference.

</details>


### [635] [Wireless Home Automation Using Social Networking Websites](https://arxiv.org/abs/2506.22482)
*Divya Alok Gupta,Dwith Chenna,B. Aditya Vighnesh Ramakanth*

Main category: cs.NI

TL;DR: The paper proposes a home automation system leveraging Twitter for secure authentication and user interaction to control domestic appliances.


<details>
  <summary>Details</summary>
Motivation: To address challenges in current Wireless Home Automation Systems (WHAS) such as security, user-friendliness, and managing diverse appliances with a single interface.

Method: The system integrates secure authentication through social networking platforms like Twitter and monitors user activities on the network to control home appliances.

Result: A comparison is made with traditional systems, showing the advantages of this approach in terms of security and functionality.

Conclusion: The proposed system enhances functionality and usability in home automation by incorporating social media-based secure authentication mechanisms.

Abstract: With the advent of Internet of Things, Wireless Home Automation Systems WHAS
are gradually gaining popularity. These systems are faced with multiple
challenges such as security; controlling a variety of home appliances with a
single interface and user friendliness. In this paper we propose a system that
uses secure authentication systems of social networking websites such as
Twitter, tracks the end-users activities on the social network and then control
his or her domestic appliances. At the end, we highlight the applications of
the proposed WHAS and compare the advantages of our proposed system over
traditional home automation systems.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [636] [Supervised Diffusion-Model-Based PET Image Reconstruction](https://arxiv.org/abs/2506.24034)
*George Webber,Alexander Hammers,Andrew P King,Andrew J Reader*

Main category: physics.med-ph

TL;DR: This paper introduces a supervised diffusion model (DM) algorithm for PET image reconstruction, showing improvements over state-of-the-art methods and enabling better uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: To improve PET image reconstruction by addressing limitations of unsupervised diffusion models, which lack explicit interaction modeling between the DM prior and noisy measurement data, potentially affecting reconstruction accuracy.

Method: Proposed a supervised DM-based algorithm that enforces non-negativity of PET's Poisson likelihood model and accommodates wide PET image intensity ranges. It also enables better posterior sampling and includes ablation studies to analyze model components and dependencies.

Result: Experimental results on realistic brain PET phantoms indicate that the proposed method outperforms or matches state-of-the-art deep learning techniques across varying dose levels. It demonstrates improved posterior sampling accuracy and extends to fully 3D PET with results on real brain PET data.

Conclusion: The proposed supervised DM-based method enhances PET reconstruction accuracy, offers robust uncertainty estimation, and presents practical applicability in 3D PET imaging, advancing the capabilities of DM-based approaches for this domain.

Abstract: Diffusion models (DMs) have recently been introduced as a regularizing prior
for PET image reconstruction, integrating DMs trained on high-quality PET
images with unsupervised schemes that condition on measured data. While these
approaches have potential generalization advantages due to their independence
from the scanner geometry and the injected activity level, they forgo the
opportunity to explicitly model the interaction between the DM prior and noisy
measurement data, potentially limiting reconstruction accuracy. To address
this, we propose a supervised DM-based algorithm for PET reconstruction. Our
method enforces the non-negativity of PET's Poisson likelihood model and
accommodates the wide intensity range of PET images. Through experiments on
realistic brain PET phantoms, we demonstrate that our approach outperforms or
matches state-of-the-art deep learning-based methods quantitatively across a
range of dose levels. We further conduct ablation studies to demonstrate the
benefits of the proposed components in our model, as well as its dependence on
training data, parameter count, and number of diffusion steps. Additionally, we
show that our approach enables more accurate posterior sampling than
unsupervised DM-based methods, suggesting improved uncertainty estimation.
Finally, we extend our methodology to a practical approach for fully 3D PET and
present example results from real [$^{18}$F]FDG brain PET data.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [637] [Sampling and Identity-Testing Without Approximate Tensorization of Entropy](https://arxiv.org/abs/2506.23456)
*William Gay,William He,Nicholas Kocurek,Ryan O'Donnell*

Main category: math.ST

TL;DR: The paper explores challenges in high-dimensional statistics for mixtures of distributions with approximate tensorization of entropy, providing efficient methods for sampling and identity testing.


<details>
  <summary>Details</summary>
Motivation: To address the complexities in tasks such as identity-testing and sampling for mixtures of distributions that lack approximate tensorization of entropy (ATE).

Method: The authors extend existing results on modified log-Sobolev inequalities to handle mixtures of distributions that satisfy such inequalities. They also design new algorithms for identity-testing under the coordinate-conditional sampling model.

Result: They demonstrate fast mixing of Glauber dynamics with optimal sample complexity for these mixture distributions and provide efficient identity-testing methods, along with algorithmic enhancements over prior work.

Conclusion: The study advances practical methods for tackling mixtures of high-dimensional distributions, offering theoretical as well as algorithmic contributions to facilitate faster and more efficient statistical tasks.

Abstract: Certain tasks in high-dimensional statistics become easier when the
underlying distribution satisfies a local-to-global property called approximate
tensorization of entropy (ATE). For example, the Glauber dynamics Markov chain
of an ATE distribution mixes fast and can produce approximate samples in a
small amount of time, since such a distribution satisfies a modified
log-Sobolev inequality. Moreover, identity-testing for an ATE distribution
requires few samples if the tester is given coordinate conditional access to
the unknown distribution, as shown by Blanca, Chen, \v{S}tefankovi\v{c}, and
Vigoda (COLT 2023).
  A natural class of distributions that do not satisfy ATE consists of mixtures
of (few) distributions that do satisfy ATE. We study the complexity of
identity-testing and sampling for these distributions. Our main results are the
following:
  1. We show fast mixing of Glauber dynamics from a data-based initialization,
with optimal sample complexity, for mixtures of distributions satisfying
modified log-Sobolev inequalities. This extends work of Huang, Koehler, Lee,
Mohanty, Rajaraman, Vuong, and Wu (STOC 2025, COLT 2025) for mixtures of
distributions satisfying Poincar\'e inequalities.
  2. Answering an open question posed by Blanca et al., we give efficient
identity-testers for mixtures of ATE distributions in the
coordinate-conditional sampling access model. We also give some simplifications
and improvements to the original algorithm of Blanca et al.

</details>


### [638] [Lower bounds for trace estimation via Block Krylov and other methods](https://arxiv.org/abs/2506.22701)
*Shi Jie Yu*

Main category: math.ST

TL;DR: The paper explores theoretical bounds for estimating matrix function traces with Hutchinson's method and Block Krylov techniques, analyzing approximation costs and limitations.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical limits and computational efficiency of trace estimation methods when combined with polynomial approximations.

Method: Analyzed theoretical bounds through polynomial approximations and studied how many Krylov steps are sufficient for specific matrix functions, alongside developing lower query limits for trace estimation.

Result: Upper bounds for Krylov steps were derived for functions like $A^{-1/2}$ and $A^{-1}$, while lower limits were established for $	ext{tr}(W^{-p})$ involving Wishart matrices.

Conclusion: The study connects Krylov steps and polynomial degree to computation cost, providing insights into theoretical and practical efficiency for trace estimation methods.

Abstract: This paper studies theoretical lower bounds for estimating the trace of a
matrix function, $\text{tr}(f(A))$, focusing on methods that use Hutchinson's
method along with Block Krylov techniques. These methods work by approximating
matrix-vector products like $f(A)V$ using a Block Krylov subspace. This is
closely related to approximating functions with polynomials. We derive
theoretical upper bounds on how many Krylov steps are needed for functions such
as $A^{-1/2}$ and $A^{-1}$ by analyzing the upper bounds from the polynomial
approximation of their scalar equivalent. In addition, we also develop lower
limits on the number of queries needed for trace estimation, specifically for
$\text{tr}(W^{-p})$ where $W$ is a Wishart matrix. Our study clarifies the
connection between the number of steps in Block Krylov methods and the degree
of the polynomial used for approximation. This links the total cost of trace
estimation to basic limits in polynomial approximation and how much information
is needed for the computation.

</details>


### [639] [On Universality of Non-Separable Approximate Message Passing Algorithms](https://arxiv.org/abs/2506.23010)
*Max Lovig,Tianhao Wang,Zhou Fan*

Main category: math.ST

TL;DR: The paper explores universality in Approximate Message Passing (AMP) algorithms with non-separable nonlinearities for data beyond Gaussian assumptions, proposing conditions for universal state evolution.


<details>
  <summary>Details</summary>
Motivation: Mean-field characterizations provide valuable insights into iterative algorithm dynamics; however, the universality of such characterization for non-separable algorithms has been limited to specific data types.

Method: The study introduces conditions like the Bounded Composition Property (BCP) and BCP-approximability, enabling universal state evolution for matrices with non-Gaussian entries in AMP algorithms.

Result: The paper proves that several important classes of non-separable nonlinearities meet the BCP-approximability condition, ensuring universality for AMP algorithms with such nonlinearities.

Conclusion: The findings broaden the scope of universal state evolution characterizations to non-separable AMP algorithms, advancing understanding in statistical learning and iterative methods.

Abstract: Mean-field characterizations of first-order iterative algorithms -- including
Approximate Message Passing (AMP), stochastic and proximal gradient descent,
and Langevin diffusions -- have enabled a precise understanding of learning
dynamics in many statistical applications. For algorithms whose non-linearities
have a coordinate-separable form, it is known that such characterizations enjoy
a degree of universality with respect to the underlying data distribution.
However, mean-field characterizations of non-separable algorithm dynamics have
largely remained restricted to i.i.d. Gaussian or rotationally-invariant data.
  In this work, we initiate a study of universality for non-separable AMP
algorithms. We identify a general condition for AMP with polynomial
non-linearities, in terms of a Bounded Composition Property (BCP) for their
representing tensors, to admit a state evolution that holds universally for
matrices with non-Gaussian entries. We then formalize a condition of
BCP-approximability for Lipschitz AMP algorithms to enjoy a similar universal
guarantee. We demonstrate that many common classes of non-separable
non-linearities are BCP-approximable, including local denoisers, spectral
denoisers for generic signals, and compositions of separable functions with
generic linear maps, implying the universality of state evolution for AMP
algorithms employing these non-linearities.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [640] [Learning Truthful Mechanisms without Discretization](https://arxiv.org/abs/2506.22911)
*Yunxuan Ma,Siqiang Wang,Zhijian Duan,Yukun Cheng,Xiaotie Deng*

Main category: cs.GT

TL;DR: The paper introduces TEDI, a novel algorithm for designing truthful and efficient mechanisms without relying on outcome discretization.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for learning mechanisms often rely on discretization, which becomes inefficient as the problem size increases. The paper seeks to address this issue with a new formulation.

Method: TEDI uses a new network architecture called Partial GroupMax Network to parameterize pricing rules, which are trained using specialized techniques like the covariance trick and continuous sampling.

Result: Theoretical and experimental results show that TEDI ensures truthfulness, expressiveness, and dimension-insensitivity, while achieving competitive or superior performance compared to state-of-the-art methods.

Conclusion: TEDI offers a new solution for learning mechanisms without discretization, improving efficiency and sparking potential developments in mechanism design and differentiable economics.

Abstract: This paper introduces TEDI (Truthful, Expressive, and Dimension-Insensitive
approach), a discretization-free algorithm to learn truthful and
utility-maximizing mechanisms. Existing learning-based approaches often rely on
discretization of outcome spaces to ensure truthfulness, which leads to
inefficiency with increasing problem size. To address this limitation, we
formalize the concept of pricing rules, defined as functions that map outcomes
to prices. Based on this concept, we propose a novel menu mechanism, which can
be equivalent to a truthful direct mechanism under specific conditions. The
core idea of TEDI lies in its parameterization of pricing rules using Partial
GroupMax Network, a new network architecture designed to universally
approximate partial convex functions. To learn optimal pricing rules, we
develop novel training techniques, including covariance trick and continuous
sampling, to derive unbiased gradient estimators compatible with first-order
optimization. Theoretical analysis establishes that TEDI guarantees
truthfulness, full expressiveness, and dimension-insensitivity. Experimental
evaluation in the studied auction setting demonstrates that TEDI achieves
strong performance, competitive with or exceeding state-of-the-art methods.
  This work presents the first approaches to learn truthful mechanisms without
outcome discretization, thereby enhancing algorithmic efficiency. The proposed
concepts, network architecture, and learning techniques might offer potential
value and provide new insights for automated mechanism design and
differentiable economics.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [641] [TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity](https://arxiv.org/abs/2506.23484)
*Yuzhuo Chen,Zehua Ma,Han Fang,Weiming Zhang,Nenghai Yu*

Main category: cs.MM

TL;DR: This paper introduces TAG-WM, a watermarking method designed for tamper detection and localization in AI-generated content (AIGC), while ensuring image quality and robustness.


<details>
  <summary>Details</summary>
Motivation: AI-generated content introduces risks such as copyright issues and malicious tampering, necessitating robust watermarking solutions that maintain image quality and proactively localize tampering.

Method: TAG-WM includes four modules: dual-mark joint sampling (DMJS) for watermark embedding, watermark latent reconstruction (WLR), dense variation region detector (DVRD) for tampering detection, and tamper-aware decoding (TAD) for guided recovery.

Result: Experiments show TAG-WM achieves state-of-the-art performance in robustness against tampering, accurate tampering localization, and maintains lossless generation quality with a watermark capacity of 256 bits.

Conclusion: TAG-WM effectively addresses the challenges of watermark robustness and proactive tampering detection while supporting high-quality AI-generated images.

Abstract: AI-generated content (AIGC) enables efficient visual creation but raises
copyright and authenticity risks. As a common technique for integrity
verification and source tracing, digital image watermarking is regarded as a
potential solution to above issues. Among these, watermarking methods capable
of preserving the generation quality are receiving increased attention.
However, the proliferation and high performance of generative image editing
applications have elevated the risks of malicious tampering, creating new
demands. 1) The tamper robustness of current lossless visual quality watermarks
remains constrained by the modification-sensitive diffusion inversion process,
necessitating enhanced robustness. 2) The improved tampering quality and rapid
iteration cycles render passive tampering detection methods inadequate, making
proactive tampering localization capability a desired feature for watermarks.
To address these requirements, this paper proposes a Tamper-Aware Generative
image WaterMarking method named TAG-WM. The proposed method comprises four key
modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright
and localization watermarks into the latent space while preserving generative
quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a
dense variation region detector (DVRD) leveraging diffusion inversion
sensitivity to identify tampered areas via statistical deviation analysis, and
the tamper-aware decoding (TAD) guided by localization results. The
experimental results indicate that TAG-WM achieves SOTA tampering robustness
and tampering localization capability with distortions while maintaining
lossless generation quality and a considerable capacity of 256 bits.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [642] [Correlated Mutations for Integer Programming](https://arxiv.org/abs/2506.22526)
*Ofer M. Shir,Michael Emmerich*

Main category: math.OC

TL;DR: The paper introduces Integer Evolution Strategies (IESs) as a heuristic method for Integer Programming, focusing on discrete search using the $
\ell_1$-norm and evaluating mutation distributions. It finds the Double Geometric (DG) distribution superior for unbounded integer variables and highlights its empirical performance benefits.


<details>
  <summary>Details</summary>
Motivation: Despite recent advancements in Integer Programming theory, heuristics dominate due to the complexity of the problem. The study aims to establish discrete IESs by addressing the limitations of continuous operators and reconsidering metrics for integer lattice search.

Method: The authors explore mutation distributions for integer decision variables using theoretical analysis and numerical simulations. They evaluate uniform, binomial, truncated normal (TN), and double geometric (DG) distributions, examining entropy functions and scalability for correlated mutations.

Result: The paper demonstrates that the DG mutation distribution is theoretically and empirically superior for integer search problems. Numerical simulations show its advantage over other strategies, especially for non-separable quadratic Integer Programming problems.

Conclusion: The paper concludes that adopting the $
\ell_1$-norm and DG mutation distribution significantly improves Integer Evolution Strategies. While replacing TN with DG is beneficial, the adoption of the $
\ell_1$-norm is deemed a more crucial innovation.

Abstract: Even with the recent theoretical advancements that dramatically reduced the
complexity of Integer Programming (IP), heuristics remain the dominant
problem-solvers for this difficult category. This study seeks to establish the
groundwork for Integer Evolution Strategies (IESs), a class of randomized
search heuristics inherently designed for continuous spaces. IESs already excel
in treating IP in practice, but accomplish it via discretization and by
applying sophisticated patches to their continuous operators, while
persistently using the $\ell_2$-norm as their operation pillar. We lay
foundations for discrete search, by adopting the $\ell_1$-norm, accounting for
the suitable step-size, and questioning alternative measures to quantify
correlations over the integer lattice. We focus on mutation distributions for
unbounded integer decision variables. We briefly discuss a couple of candidate
discrete probabilities induced by the uniform and binomial distributions, which
we show to possess less appealing theoretical properties, and then narrow down
to the Truncated Normal (TN) and Double Geometric (DG) distributions. We
explore their theoretical properties, including entropy functions, and propose
a procedure to generate scalable correlated mutation distributions. Our
investigations are accompanied by extensive numerical simulations, which
consistently support the claim that the DG distribution is better suited for
unbounded integer search. We link our theoretical perspective to empirical
evidence indicating that an IES with correlated DG mutations outperformed other
strategies over non-separable quadratic IP. We conclude that while the
replacement of the default TN distribution by the DG is theoretically justified
and practically beneficial, the truly crucial change lies in adopting the
$\ell_1$-norm over the $\ell_2$-norm.

</details>


### [643] [Proving the Limited Scalability of Centralized Distributed Optimization via a New Lower Bound Construction](https://arxiv.org/abs/2506.23836)
*Alexander Tyurin*

Main category: math.OC

TL;DR: The paper investigates the scalability of centralized distributed optimization in federated learning, addressing constraints in scaling communication and runtime. It establishes theoretical limitations using a new lower bound framework and a worst-case function construction.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the goal of understanding the scalability of distributed optimization algorithms in federated learning, particularly when accounting for both computation and communication overheads.

Method: The approach involves theoretical analysis using a newly constructed worst-case function and a lower bound framework. The framework relies on a concentration of a random sum to derive fundamental limits.

Result: The study proves that it's infeasible to design methods using unbiased random sparsification compressors to scale server-side communication and variance-dependent runtime terms better than poly-logarithmically in $n$ (number of workers).

Conclusion: The findings highlight inherent limitations in distributed optimization scalability, even in ideally homogeneous setups, challenging the design of efficient federated learning algorithms.

Abstract: We consider centralized distributed optimization in the classical federated
learning setup, where $n$ workers jointly find an $\varepsilon$-stationary
point of an $L$-smooth, $d$-dimensional nonconvex function $f$, having access
only to unbiased stochastic gradients with variance $\sigma^2$. Each worker
requires at most $h$ seconds to compute a stochastic gradient, and the
communication times from the server to the workers and from the workers to the
server are $\tau_{s}$ and $\tau_{w}$ seconds per coordinate, respectively. One
of the main motivations for distributed optimization is to achieve scalability
with respect to $n$. For instance, it is well known that the distributed
version of SGD has a variance-dependent runtime term $\frac{h \sigma^2 L
\Delta}{n \varepsilon^2},$ which improves with the number of workers $n,$ where
$\Delta = f(x^0) - f^*,$ and $x^0 \in R^d$ is the starting point. Similarly,
using unbiased sparsification compressors, it is possible to reduce both the
variance-dependent runtime term and the communication runtime term. However,
once we account for the communication from the server to the workers
$\tau_{s}$, we prove that it becomes infeasible to design a method using
unbiased random sparsification compressors that scales both the server-side
communication runtime term $\tau_{s} d \frac{L \Delta}{\varepsilon}$ and the
variance-dependent runtime term $\frac{h \sigma^2 L \Delta}{\varepsilon^2},$
better than poly-logarithmically in $n$, even in the homogeneous (i.i.d.) case,
where all workers access the same distribution. To establish this result, we
construct a new "worst-case" function and develop a new lower bound framework
that reduces the analysis to the concentration of a random sum, for which we
prove a concentration bound. These results reveal fundamental limitations in
scaling distributed optimization, even under the homogeneous assumption.

</details>


### [644] [Deep neural networks can provably solve Bellman equations for Markov decision processes without the curse of dimensionality](https://arxiv.org/abs/2506.22851)
*Arnulf Jentzen,Konrad Kleinberg,Thomas Kruse*

Main category: math.OC

TL;DR: The paper investigates using deep neural networks (DNNs) with leaky ReLU activation to approximate $Q$-functions in Markov decision processes (MDPs) with infinite time horizons. The authors establish that these DNN approximations have error bounds that grow only polynomially with state space dimension and error tolerance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of efficiently solving $Q$-functions in MDPs, fundamental models for decision-making under uncertainty, by employing modern neural network approaches to make these computations feasible and accurate in high-dimensional settings.

Method: The authors use deep neural networks with leaky ReLU activation to approximate payoff functions, transition dynamics, and solutions of Bellman equations. They leverage the full-history recursive multilevel fixed-point (MLFP) approximation scheme to ensure accuracy and efficiency.

Result: The paper proves that the $Q$-functions in MDPs can be approximated by DNNs with parameters that grow polynomially concerning the state space dimension and the error tolerance. This establishes feasibility in computational and error management.

Conclusion: The study demonstrates that DNNs can effectively approximate $Q$-functions in MDPs, providing a scalable and accurate framework for reinforcement learning applications in complex systems.

Abstract: Discrete time stochastic optimal control problems and Markov decision
processes (MDPs) are fundamental models for sequential decision-making under
uncertainty and as such provide the mathematical framework underlying
reinforcement learning theory. A central tool for solving MDPs is the Bellman
equation and its solution, the so-called $Q$-function. In this article, we
construct deep neural network (DNN) approximations for $Q$-functions associated
to MDPs with infinite time horizon and finite control set $A$. More
specifically, we show that if the the payoff function and the random transition
dynamics of the MDP can be suitably approximated by DNNs with leaky rectified
linear unit (ReLU) activation, then the solutions $Q_d\colon \mathbb R^d\to
\mathbb R^{|A|}$, $d\in \mathbb{N}$, of the associated Bellman equations can
also be approximated in the $L^2$-sense by DNNs with leaky ReLU activation
whose numbers of parameters grow at most polynomially in both the dimension
$d\in \mathbb{N}$ of the state space and the reciprocal $1/\varepsilon$ of the
prescribed error $\varepsilon\in (0,1)$. Our proof relies on the recently
introduced full-history recursive multilevel fixed-point (MLFP) approximation
scheme.

</details>


### [645] [Hindsight-Guided Momentum (HGM) Optimizer: An Approach to Adaptive Learning Rate](https://arxiv.org/abs/2506.22479)
*Krisanu Sarkar*

Main category: math.OC

TL;DR: The paper introduces Hindsight-Guided Momentum (HGM), a novel optimization algorithm that uses directional consistency to adaptively scale learning rates and improve performance, particularly in non-convex settings.


<details>
  <summary>Details</summary>
Motivation: Current adaptive optimization methods only adjust based on gradient magnitude, overlooking directional information, which can reflect the optimization path's local curvature and consistency.

Method: HGM evaluates the cosine similarity between the current gradient and accumulated momentum to adjust learning rates. This mechanism increases rates for coherent directions and decreases them in noisy or oscillating regions.

Result: The HGM optimizer accelerates convergence in smooth areas of the loss surface while maintaining stability in sharp or erratic regions, improving responsiveness and performance compared to existing optimizers.

Conclusion: HGM is a computationally efficient and effective enhancement over standard optimization methods, offering particular benefits for complex non-convex problems like deep neural network training.

Abstract: We introduce Hindsight-Guided Momentum (HGM), a first-order optimization
algorithm that adaptively scales learning rates based on the directional
consistency of recent updates. Traditional adaptive methods, such as Adam or
RMSprop , adapt learning dynamics using only the magnitude of gradients, often
overlooking important geometric cues.Geometric cues refer to directional
information, such as the alignment between current gradients and past updates,
which reflects the local curvature and consistency of the optimization path.
HGM addresses this by incorporating a hindsight mechanism that evaluates the
cosine similarity between the current gradient and accumulated momentum. This
allows it to distinguish between coherent and conflicting gradient directions,
increasing the learning rate when updates align and reducing it in regions of
oscillation or noise. The result is a more responsive optimizer that
accelerates convergence in smooth regions of the loss surface while maintaining
stability in sharper or more erratic areas. Despite this added adaptability,
the method preserves the computational and memory efficiency of existing
optimizers.By more intelligently responding to the structure of the
optimization landscape, HGM provides a simple yet effective improvement over
existing approaches, particularly in non-convex settings like that of deep
neural network training.

</details>


### [646] [Maximum Dispersion, Maximum Concentration: Enhancing the Quality of MOP Solutions](https://arxiv.org/abs/2506.22568)
*Gladston Moreira,Ivan Meneghini,Elzabeth Wanner*

Main category: math.OC

TL;DR: This paper improves multi-objective optimization by balancing solution dispersion in the decision space with convergence in specific regions of the objective space, using a Region of Interest (ROI) approach.


<details>
  <summary>Details</summary>
Motivation: To address challenges in multi-objective optimization, such as balancing diversity and convergence, while avoiding bias caused by clustering solutions in particular regions of the decision space.

Method: The paper introduces a Region of Interest (ROI) defined by a cone reflecting decision-maker preferences for convergence in the objective space. Simultaneously, it uses a uniformity measure to enhance solution dispersion in the decision space.

Result: Experiments show that the proposed approach improves multi-objective optimization by generating solutions that balance dispersion in the decision space with concentration in the objective space, reducing bias.

Conclusion: The approach effectively enhances the quality of solutions for multi-objective optimization by improving diversity and avoiding clustering bias, contributing to better Pareto-optimal solution searches.

Abstract: Multi-objective optimization problems (MOPs) often require a trade-off
between conflicting objectives, maximizing diversity and convergence in the
objective space. This study presents an approach to improve the quality of MOP
solutions by optimizing the dispersion in the decision space and the
convergence in a specific region of the objective space. Our approach defines a
Region of Interest (ROI) based on a cone representing the decision maker's
preferences in the objective space, while enhancing the dispersion of solutions
in the decision space using a uniformity measure. Combining solution
concentration in the objective space with dispersion in the decision space
intensifies the search for Pareto-optimal solutions while increasing solution
diversity. When combined, these characteristics improve the quality of
solutions and avoid the bias caused by clustering solutions in a specific
region of the decision space. Preliminary experiments suggest that this method
enhances multi-objective optimization by generating solutions that effectively
balance dispersion and concentration, thereby mitigating bias in the decision
space.

</details>


### [647] [Consensus-based optimization for closed-box adversarial attacks and a connection to evolution strategies](https://arxiv.org/abs/2506.24048)
*Tim Roith,Leon Bungert,Philipp Wacker*

Main category: math.OC

TL;DR: Consensus-based optimization (CBO) is examined for closed-box adversarial attacks and connected to natural evolution strategies (NES), with experiments showing superior performance in some cases.


<details>
  <summary>Details</summary>
Motivation: To improve understanding and efficiency of closed-box adversarial attacks using gradient-free optimization techniques.

Method: The study connects CBO's consensus hopping with NES and compares their performance while drawing parallels to gradient-based methods.

Result: CBO outperforms NES and other evolutionary strategies experimentally in specific scenarios related to adversarial attacks.

Conclusion: CBO has the potential to enhance closed-box adversarial attack strategies, offering superior results over NES in certain cases.

Abstract: Consensus-based optimization (CBO) has established itself as an efficient
gradient-free optimization scheme, with attractive mathematical properties,
such as mean-field convergence results for non-convex loss functions. In this
work, we study CBO in the context of closed-box adversarial attacks, which are
imperceptible input perturbations that aim to fool a classifier, without
accessing its gradient. Our contribution is to establish a connection between
the so-called consensus hopping as introduced by Riedl et al. and natural
evolution strategies (NES) commonly applied in the context of adversarial
attacks and to rigorously relate both methods to gradient-based optimization
schemes. Beyond that, we provide a comprehensive experimental study that shows
that despite the conceptual similarities, CBO can outperform NES and other
evolutionary strategies in certain scenarios.

</details>


### [648] [Denoising Multi-Color QR Codes and Stiefel-Valued Data by Relaxed Regularizations](https://arxiv.org/abs/2506.22826)
*Robert Beinert,Jonas Bresch*

Main category: math.OC

TL;DR: This paper extends a recent convex denoising framework to handle multi-binary and Stiefel-valued data, supported by synthetic experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to expand existing denoising methods, such as total variation and Tikhonov models, which have already been applied to manifold-valued data, to new data types like multi-binary and Stiefel-valued data.

Method: The approach involves embedding the manifold-valued data in Euclidean ambient space, encoding non-convex manifolds with fixed-rank matrices, and introducing a convex relaxation for easier computation. The authors extend these methods to handle multi-binary and Stiefel-valued data.

Result: The paper successfully develops and tests TV- and Tikhonov-based denoising methods for the new data types through synthetic experiments.

Conclusion: The proposed models are numerically efficient and versatile, showcasing their potential for practical applications like multi-color QR codes and video-based recognition.

Abstract: The handling of manifold-valued data, for instance, plays a central role in
color restoration tasks relying on circle- or sphere-valued color models, in
the study of rotational or directional information related to the special
orthogonal group, and in Gaussian image processing, where the pixel statistics
are interpreted as values on the hyperbolic sheet. Especially, to denoise these
kind of data, there have been proposed several generalizations of total
variation (TV) and Tikhonov-type denoising models incorporating the underlying
manifolds. Recently, a novel, numerically efficient denoising approach has been
introduced, where the data are embedded in an Euclidean ambient space, the
non-convex manifolds are encoded by a series of positive semi-definite,
fixed-rank matrices, and the rank constraint is relaxed to obtain a
convexification that can be solved using standard algorithms from convex
analysis. The aim of the present paper is to extent this approach to new kinds
of data like multi-binary and Stiefel-valued data. Multi-binary data can, for
instance, be used to model multi-color QR codes whereas Stiefel-valued data
occur in image and video-based recognition. For both new data types, we propose
TV- and Tikhonov-based denoising modelstogether with easy-to-solve
convexification. All derived methods are evaluated on proof-of-concept,
synthetic experiments.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [649] [Data-Driven Self-Supervised Learning for the Discovery of Solution Singularity for Partial Differential Equations](https://arxiv.org/abs/2506.23344)
*Difeng Cai,Paulina Sepúlveda*

Main category: math.NA

TL;DR: The paper addresses challenges in detecting singularities in scientific computing and introduces a self-supervised learning framework to estimate singularity locations using unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Singularities pose a major challenge in scientific computing, as they can reduce the efficiency of numerical schemes. This problem worsens when singularity locations are unknown, demanding adaptive methods to optimize computational costs.

Method: The paper proposes a self-supervised learning framework utilizing a filtering procedure as a pretext task, with two specific filtering methods: $k$ nearest neighbors and kernel density estimation, enabling effective singularity location estimation.

Result: Numerical examples demonstrate the issues of raw data usage and highlight the approach's robustness against input perturbations, label corruption, and varying singularity types like interior circles and boundary layers.

Conclusion: The proposed SSL framework successfully addresses the singularity detection challenge, offering a data-driven solution that enhances adaptive numerical methods in scientific computing.

Abstract: The appearance of singularities in the function of interest constitutes a
fundamental challenge in scientific computing. It can significantly undermine
the effectiveness of numerical schemes for function approximation, numerical
integration, and the solution of partial differential equations (PDEs), etc.
The problem becomes more sophisticated if the location of the singularity is
unknown, which is often encountered in solving PDEs. Detecting the singularity
is therefore critical for developing efficient adaptive methods to reduce
computational costs in various applications. In this paper, we consider
singularity detection in a purely data-driven setting. Namely, the input only
contains given data, such as the vertex set from a mesh. To overcome the
limitation of the raw unlabeled data, we propose a self-supervised learning
(SSL) framework for estimating the location of the singularity. A key component
is a filtering procedure as the pretext task in SSL, where two filtering
methods are presented, based on $k$ nearest neighbors and kernel density
estimation, respectively. We provide numerical examples to illustrate the
potential pathological or inaccurate results due to the use of raw data without
filtering. Various experiments are presented to demonstrate the ability of the
proposed approach to deal with input perturbation, label corruption, and
different kinds of singularities such interior circle, boundary layer,
concentric semicircles, etc.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [650] [GaussMaster: An LLM-based Database Copilot System](https://arxiv.org/abs/2506.23322)
*Wei Zhou,Ji Sun,Xuanhe Zhou,Guoliang Li,Luyang Liu,Hao Wu,Tianyuan Wang*

Main category: cs.DB

TL;DR: GaussMaster introduces an LLM-based system for autonomous database maintenance, reducing human intervention across 34 scenarios in real-world deployments.


<details>
  <summary>Details</summary>
Motivation: DBAs face heavy workloads in SQL tuning, deployment, diagnosis, and repair, needing efficient tools for holistic database maintenance.

Method: GaussMaster uses an LLM-based approach, Tree-of-Thought analysis, and automated tooling to diagnose and resolve database issues.

Result: GaussMaster achieved zero human intervention in 34 maintenance scenarios, particularly effective in the banking sector.

Conclusion: GaussMaster demonstrates potential to transform autonomous database systems by comprehensively addressing maintenance needs and reducing reliance on manual DBA operations.

Abstract: In the financial industry, data is the lifeblood of operations, and DBAs
shoulder significant responsibilities for SQL tuning, database deployment,
diagnosis, and service repair. In recent years, both database vendors and
customers have increasingly turned to autonomous database platforms in an
effort to alleviate the heavy workload of DBAs. However, existing autonomous
database platforms are limited in their capabilities, primarily addressing
single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual
intervention remains a necessity for comprehensive database maintenance.
GaussMaster aims to revolutionize this landscape by introducing an LLM-based
database copilot system. This innovative solution is designed not only to
assist developers in writing efficient SQL queries but also to provide
comprehensive care for database services. When database instances exhibit
abnormal behavior, GaussMaster is capable of orchestrating the entire
maintenance process automatically. It achieves this by analyzing hundreds of
metrics and logs, employing a Tree-of-thought approach to identify root causes,
and invoking appropriate tools to resolve issues. We have successfully
implemented GaussMaster in real-world scenarios, such as the banking industry,
where it has achieved zero human intervention for over 34 database maintenance
scenarios. In this paper, we present significant improvements in these tasks
with code at https://gitcode.com/opengauss/openGauss-GaussMaster.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [651] [Doubly robust estimation of causal effects for random object outcomes with continuous treatments](https://arxiv.org/abs/2506.22754)
*Satarupa Bhattacharjee,Bing Li,Xiao Wu,Lingzhou Xue*

Main category: stat.ME

TL;DR: The paper develops a causal inference framework for non-Euclidean data using Hilbert space embeddings, addressing continuous treatments and demonstrating its applications in environmental studies.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing presence of complex, non-Euclidean data in modern applications and the need for a robust causal inference framework to analyze such data.

Method: The authors propose using Hilbert space embeddings to estimate Fréchet means and map causal effects, complemented by a nonparametric, doubly-debiased approach with cross-fitted estimators and conformal inference techniques.

Result: The framework demonstrates strong asymptotic properties, robustness, and interpretability, validated through numerical experiments and a study on particulate matter exposure's impact on age-at-death distributions across U.S. counties.

Conclusion: The work extends traditional causal inference methods to handle abstract metric spaces and complex data, enhancing its relevance for a wide range of scientific applications.

Abstract: Causal inference is central to statistics and scientific discovery, enabling
researchers to identify cause-and-effect relationships beyond associations.
While traditionally studied within Euclidean spaces, contemporary applications
increasingly involve complex, non-Euclidean data structures that reside in
abstract metric spaces, known as random objects, such as images, shapes,
networks, and distributions. This paper introduces a novel framework for causal
inference with continuous treatments applied to non-Euclidean data. To address
the challenges posed by the lack of linear structures, we leverage Hilbert
space embeddings of the metric spaces to facilitate Fr\'echet mean estimation
and causal effect mapping. Motivated by a study on the impact of exposure to
fine particulate matter on age-at-death distributions across U.S. counties, we
propose a nonparametric, doubly-debiased causal inference approach for outcomes
as random objects with continuous treatments. Our framework can accommodate
moderately high-dimensional vector-valued confounders and derive efficient
influence functions for estimation to ensure both robustness and
interpretability. We establish rigorous asymptotic properties of the
cross-fitted estimators and employ conformal inference techniques for
counterfactual outcome prediction. Validated through numerical experiments and
applied to real-world environmental data, our framework extends causal
inference methodologies to complex data structures, broadening its
applicability across scientific disciplines.

</details>


<div id='stat.OT'></div>

# stat.OT [[Back]](#toc)

### [652] [Treatment, evidence, imitation, and chat](https://arxiv.org/abs/2506.23040)
*Samuel J. Weisenthal*

Main category: stat.OT

TL;DR: The paper examines the role of large language models (LLMs) in aiding medical decision-making and explores their application specifically to treatment and chat-based problems, identifying challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To evaluate the potential of LLMs in medical decision-making, gain insights on their applicability to treatment tasks, and address challenges tied to evidence-based approaches.

Method: The authors analyze the treatment decision-making process, compare it with chat-based tasks, and examine possible uses of LLMs while mapping challenges against evidence-based medicine principles.

Result: The analysis reveals significant challenges for LLMs in solving medical decision-making tasks, particularly in aligning with evidence-based medicine and addressing imitation issues.

Conclusion: While LLMs have potential in aiding medical decision-making, their practical application requires resolving challenges tied to evidence-based medicine and understanding task-specific needs like treatment and chat problems.

Abstract: Large language models are thought to have potential to aid in medical
decision making. We investigate this here. We start with the treatment problem,
the patient's core medical decision-making task, which is solved in
collaboration with a healthcare provider. We discuss approaches to solving the
treatment problem, including -- within evidence-based medicine -- trials and
observational data. We then discuss the chat problem, and how this differs from
the treatment problem -- in particular as it relates to imitation. We then
discuss how a large language model might be used to solve the treatment problem
and highlight some of the challenges that emerge. We finally discuss how these
challenges relate to evidence-based medicine, and how this might inform next
steps.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [653] [Deep Hedging to Manage Tail Risk](https://arxiv.org/abs/2506.22611)
*Yuming Ma*

Main category: q-fin.PM

TL;DR: The paper extends the Deep Hedging framework to address portfolio tail-risk hedging using deep neural networks for convex-risk minimization.


<details>
  <summary>Details</summary>
Motivation: To advance the field of risk management by addressing tail-risk hedging challenges through the integration of modern machine learning techniques with financial models.

Method: The authors employ deep neural networks to parameterize Convex-risk Minimization (CVaR/ES) and evaluate its efficacy using crisis-era bootstrap market simulators configured with realistic financial constraints.

Result: The proposed framework demonstrates significant reductions in 99% CVaR over one-day horizons, along with robustness and adaptability in real-market simulations.

Conclusion: This research provides a practical, friction-aware approach to tail-risk hedging, showing its feasibility and effectiveness in realistic market setups.

Abstract: Extending Buehler et al.'s 2019 Deep Hedging paradigm, we innovatively employ
deep neural networks to parameterize convex-risk minimization (CVaR/ES) for the
portfolio tail-risk hedging problem. Through comprehensive numerical
experiments on crisis-era bootstrap market simulators -- customizable with
transaction costs, risk budgets, liquidity constraints, and market impact --
our end-to-end framework not only achieves significant one-day 99% CVaR
reduction but also yields practical insights into friction-aware strategy
adaptation, demonstrating robustness and operational viability in realistic
markets.

</details>


### [654] [Can We Reliably Predict the Fed's Next Move? A Multi-Modal Approach to U.S. Monetary Policy Forecasting](https://arxiv.org/abs/2506.22763)
*Fiona Xiao Jingyi,Lili Liu*

Main category: q-fin.PM

TL;DR: This paper enhances forecasting of U.S. federal funds rate using hybrid models integrating textual and structured data, achieving improved predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address shortcomings in traditional methods for forecasting central bank policies by exploring ways to better capture forward-looking cues from Federal Reserve communications.

Method: The researchers employed a multi-modal framework combining structured economic indicators with unstructured textual data using models such as XGBoost classifiers, FinBERT sentiment analysis, and hybrid approaches.

Result: Hybrid models outperform unimodal approaches, with the combination of TF-IDF features and economic indicators in an XGBoost classifier yielding the best test AUC of 0.83. FinBERT sentiment analysis improves ranking but struggles in classification.

Conclusion: Integrating textual and structured data transparently through hybrid models offers accuracy and interpretability, making it valuable for forecasting monetary policies.

Abstract: Forecasting central bank policy decisions remains a persistent challenge for
investors, financial institutions, and policymakers due to the wide-reaching
impact of monetary actions. In particular, anticipating shifts in the U.S.
federal funds rate is vital for risk management and trading strategies.
Traditional methods relying only on structured macroeconomic indicators often
fall short in capturing the forward-looking cues embedded in central bank
communications.
  This study examines whether predictive accuracy can be enhanced by
integrating structured data with unstructured textual signals from Federal
Reserve communications. We adopt a multi-modal framework, comparing traditional
machine learning models, transformer-based language models, and deep learning
architectures in both unimodal and hybrid settings.
  Our results show that hybrid models consistently outperform unimodal
baselines. The best performance is achieved by combining TF-IDF features of
FOMC texts with economic indicators in an XGBoost classifier, reaching a test
AUC of 0.83. FinBERT-based sentiment features marginally improve ranking but
perform worse in classification, especially under class imbalance. SHAP
analysis reveals that sparse, interpretable features align more closely with
policy-relevant signals.
  These findings underscore the importance of integrating textual and
structured signals transparently. For monetary policy forecasting, simpler
hybrid models can offer both accuracy and interpretability, delivering
actionable insights for researchers and decision-makers.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [655] [Investigating an Overfitting and Degeneration Phenomenon in Self-Supervised Multi-Pitch Estimation](https://arxiv.org/abs/2506.23371)
*Frank Cwitkowitz,Zhiyao Duan*

Main category: eess.AS

TL;DR: This paper explores enhancing supervised multi-pitch estimation by incorporating self-supervised objectives, leading to improved performance under specific conditions, but also reveals issues with overfitting.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in Multi-Pitch Estimation (MPE) posed by supervised learning, which requires annotated data that is challenging to collect, and to leverage self-supervised techniques for improved performance.

Method: The authors propose combining supervised learning methods for MPE with self-supervised objectives that exploit pitch-invariant and pitch-equivariant properties in a joint training framework.

Result: The joint training approach improves performance under closed training conditions but uncovers overfitting issues when incorporating broader data for self-supervision.

Conclusion: While joint supervised and self-supervised training yields promising results, challenges such as overfitting to supervised data and degrading performance on self-supervised data need further investigation.

Abstract: Multi-Pitch Estimation (MPE) continues to be a sought after capability of
Music Information Retrieval (MIR) systems, and is critical for many
applications and downstream tasks involving pitch, including music
transcription. However, existing methods are largely based on supervised
learning, and there are significant challenges in collecting annotated data for
the task. Recently, self-supervised techniques exploiting intrinsic properties
of pitch and harmonic signals have shown promise for both monophonic and
polyphonic pitch estimation, but these still remain inferior to supervised
methods. In this work, we extend the classic supervised MPE paradigm by
incorporating several self-supervised objectives based on pitch-invariant and
pitch-equivariant properties. This joint training results in a substantial
improvement under closed training conditions, which naturally suggests that
applying the same objectives to a broader collection of data will yield further
improvements. However, in doing so we uncover a phenomenon whereby our model
simultaneously overfits to the supervised data while degenerating on data used
for self-supervision only. We demonstrate and investigate this and offer our
insights on the underlying problem.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [656] [An Interpretable Transformer-Based Foundation Model for Cross-Procedural Skill Assessment Using Raw fNIRS Signals](https://arxiv.org/abs/2506.22476)
*A. Subedi,S. De,L. Cavuoto,S. Schwaitzberg,M. Hackett,J. Norfleet*

Main category: eess.SP

TL;DR: The paper presents a transformer-based model for skill assessment using fNIRS signals, which generalizes effectively across tasks and individuals, achieving high accuracy even with minimal data on new tasks.


<details>
  <summary>Details</summary>
Motivation: To develop a model for objective skill assessment in cognitive-motor tasks that generalizes across different tasks, individuals, and contexts without relying on extensive preprocessing.

Method: A transformer-based model trained on minimally processed fNIRS signals using self-supervised learning, pretrained on surgical tasks, and validated on a novel task with lightweight adapters.

Result: The model achieved >88% classification accuracy on all tasks, an MCC >0.91 on ETI, and >87% AUC on a new procedure using fewer than 30 labeled samples.

Conclusion: The proposed model demonstrates strong task generalization, interpretability through novel attention mechanisms, and provides insights into neural processes underlying cognitive performance.

Abstract: Objective skill assessment in high-stakes procedural environments requires
models that not only decode underlying cognitive and motor processes but also
generalize across tasks, individuals, and experimental contexts. While prior
work has demonstrated the potential of functional near-infrared spectroscopy
(fNIRS) for evaluating cognitive-motor performance, existing approaches are
often task-specific, rely on extensive preprocessing, and lack robustness to
new procedures or conditions. Here, we introduce an interpretable
transformer-based foundation model trained on minimally processed fNIRS signals
for cross-procedural skill assessment. Pretrained using self-supervised
learning on data from laparoscopic surgical tasks and endotracheal intubation
(ETI), the model achieves greater than 88% classification accuracy on all
tasks, with Matthews Correlation Coefficient exceeding 0.91 on ETI. It
generalizes to a novel emergency airway procedure--cricothyrotomy--using fewer
than 30 labeled samples and a lightweight (less than 2k parameter) adapter
module, attaining an AUC greater than 87%. Interpretability is achieved via a
novel channel attention mechanism--developed specifically for fNIRS--that
identifies functionally coherent prefrontal sub-networks validated through
ablation studies. Temporal attention patterns align with task-critical phases
and capture stress-induced changes in neural variability, offering insight into
dynamic cognitive states.

</details>


### [657] [Optical Waveguide-based Spider Web Enables Resilient Impact Detection and Localization](https://arxiv.org/abs/2506.22472)
*Dylan Wilson,Marco Pontin,Peter Walters,Perla Maiolino*

Main category: eess.SP

TL;DR: The paper introduces a spider web-inspired optical waveguide system leveraging vibrations to detect and localize impulses. It uses clear TPU waveguides configured in a bioinspired radial-spoke design, transmitting light, and detecting losses induced by vibrations for real-time monitoring.


<details>
  <summary>Details</summary>
Motivation: The researchers were motivated by the multifunctional sensing capabilities of spider webs and aimed to replicate these properties for technological systems, particularly focusing on adaptive sensing applications.

Method: They designed six TPU radial waveguides connected by a spiral thread to mimic spider web geometry. Vibrational signals were detected through induced light transmission losses, which were monitored by LEDs and photo-diodes. System parameters like waveguide tension and impulse location were optimized.

Result: Experiments showed a 5 ms delay in vibration transfer between neighboring radii, enabling better impulse localization. The developed algorithm proved effective even under sensor failure conditions, making the system robust.

Conclusion: Spider web-inspired optical waveguide structures have significant potential for resilient and adaptive sensing applications, with implications for fields like soft robotics and structural monitoring.

Abstract: Spiders use their webs as multifunctional tools that enable capturing and
localizing prey and more general environmental sensing through vibrations.
Inspired by their biological function, we present a spider web-inspired optical
waveguide system for resilient impulse detection and localization. The
structure consists of six clear thermoplastic polyurethane (TPU) waveguides
arranged radially and interconnected by a spiral TPU thread, mimicking orb
spider webs. Light transmission losses, induced by vibrations, are measured via
coupled LEDs and photo-diodes, allowing real-time detection. We systematically
characterize individual waveguides, analyzing key parameters such as tension,
impulse position, and break angle to optimize vibrational response. The
complete system is validated through controlled experiments, revealing a 5 ms
propagation delay in vibration transfer between adjacent radii, enhancing
localization capabilities. We demonstrate a robust impulse detection and
localization algorithm leveraging time delay analysis, achieving reliable event
identification even in cases of sensor failure. This study highlights the
potential of bioinspired optical waveguide structures for adaptive sensing,
with applications in soft robotics, structural monitoring, and environmental
sensing.

</details>


### [658] [Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems](https://arxiv.org/abs/2506.22448)
*Yu Ma,Xingyu Zhou,Xiao Li,Le Liang,Shi Jin*

Main category: eess.SP

TL;DR: This paper presents an unsupervised learning-based approach to optimize resource allocation in RIS-assisted MISO-OFDMA systems, achieving near-optimal performance with significantly reduced runtime.


<details>
  <summary>Details</summary>
Motivation: To address resource allocation challenges in RIS-assisted 6G wireless systems and improve efficiency and performance in transmission systems.

Method: The authors propose a two-stage framework combining two neural networks: BeamNet (for RIS phase shift prediction using CSI) and AllocationNet (for resource block allocation). Techniques like quantization and the Gumbel-softmax trick are employed to ensure differentiability under constraints. A customized loss function and phased training further improve performance.

Result: The framework achieves 99.93% of the sum rate of the traditional SCA baseline while using only 0.036% of its runtime. It also demonstrates robustness against varying channel and user conditions.

Conclusion: The presented approach offers an effective, computationally efficient solution for joint design of RIS-aided wireless systems, advancing the integration of RIS in practical 6G applications.

Abstract: Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless
systems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA
system, addressing resource allocation challenges. A two-stage unsupervised
learning-based framework is proposed to jointly design RIS phase shifts, BS
beamforming, and resource block (RB) allocation. The framework includes
BeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which
allocates RBs using equivalent CSI derived from BeamNet outputs. Active
beamforming is implemented via maximum ratio transmission and water-filling. To
handle discrete constraints while ensuring differentiability, quantization and
the Gumbel-softmax trick are adopted. A customized loss and phased training
enhance performance under QoS constraints. Simulations show the method achieves
99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and
it remains robust across varying channel and user conditions.

</details>


### [659] [A Complex UNet Approach for Non-Invasive Fetal ECG Extraction Using Single-Channel Dry Textile Electrodes](https://arxiv.org/abs/2506.22457)
*Iulia Orvas,Andrei Radu,Alessandra Galli,Ana Neacsu,Elisabetta Peri*

Main category: eess.SP

TL;DR: The paper introduces an AI-based method to extract fetal electrocardiogram (fECG) from single-channel dry textile electrode recordings, overcoming challenges like noise and motion artifacts. This approach achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Continuous pregnancy monitoring is essential for minimizing complications, and fECG offers a promising tool for fetal health assessment outside hospitals. Dry textile electrodes provide comfort but introduce significant noise and artifacts.

Method: The paper utilizes simulated datasets mimicking real-world conditions and proposes a novel pipeline using Complex UNet, a complex-valued denoising network that processes both magnitude and phase spectrogram components.

Result: The proposed method outperformed traditional approaches in fECG extraction and R-peak detection from simulated and real recordings, establishing state-of-the-art accuracy.

Conclusion: This AI-driven solution enables accurate fECG extraction using minimal, non-invasive dry textile electrodes, marking progress toward independent, home-based fetal monitoring systems.

Abstract: Continuous, non-invasive pregnancy monitoring is crucial for minimising
potential complications. The fetal electrocardiogram (fECG) represents a
promising tool for assessing fetal health beyond clinical environments.
Home-based monitoring necessitates the use of a minimal number of comfortable
and durable electrodes, such as dry textile electrodes. However, this setup
presents many challenges, including increased noise and motion artefacts, which
complicate the accurate extraction of fECG signals. To overcome these
challenges, we introduce a pioneering method for extracting fECG from
single-channel recordings obtained using dry textile electrodes using AI
techniques. We created a new dataset by simulating abdominal recordings,
including noise closely resembling real-world characteristics of in-vivo
recordings through dry textile electrodes, alongside mECG and fECG. To ensure
the reliability of the extracted fECG, we propose an innovative pipeline based
on a complex-valued denoising network, Complex UNet. Unlike previous approaches
that focused solely on signal magnitude, our method processes both real and
imaginary components of the spectrogram, addressing phase information and
preventing incongruous predictions. We evaluated our novel pipeline against
traditional, well-established approaches, on both simulated and real data in
terms of fECG extraction and R-peak detection. The results showcase that our
suggested method achieves new state-of-the-art results, enabling an accurate
extraction of fECG morphology across all evaluated settings. This method is the
first to effectively extract fECG signals from single-channel recordings using
dry textile electrodes, making a significant advancement towards a fully
non-invasive and self-administered fECG extraction solution.

</details>


### [660] [Heart rate and respiratory rate prediction from noisy real-world smartphone based on Deep Learning methods](https://arxiv.org/abs/2506.22460)
*Ibne Farabi Shihab*

Main category: eess.SP

TL;DR: The paper investigates using mobile phone videos for heart rate (HR) and respiratory rate (RR) estimation in daily-life settings, and proposes a 3D deep CNN model to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Current methods for estimating heart rate and respiratory rate from mobile phone videos are accurate in controlled laboratory settings but fail in generalizing to daily-life applications.

Method: The researchers collected a large dataset of mobile phone videos during daily life with ground truth HR and RR labels, then developed a novel 3D deep CNN to estimate these vital signs.

Result: The proposed 3D deep CNN reduced the estimation error for HR by 68% and for RR by 75%, outperforming traditional algorithms.

Conclusion: Deep learning methods, particularly CNN-based approaches, provide a significant performance improvement and should be used for estimating vital signs in real-world scenarios.

Abstract: Using mobile phone video of the fingertip as a data source for estimating
vital signs such as heart rate (HR) and respiratory rate (RR) during daily life
has long been suggested. While existing literature indicates that these
estimates are accurate to within several beats or breaths per minute, the data
used to draw these conclusions are typically collected in laboratory
environments under careful experimental control, and yet the results are
assumed to generalize to daily life. In an effort to test it, a team of
researchers collected a large dataset of mobile phone video recordings made
during daily life and annotated with ground truth HR and RR labels from N=111
participants. They found that traditional algorithm performance on the
fingerprint videos is worse than previously reported (7 times and 13 times
worse for RR and HR, respectively). Fortunately, recent advancements in deep
learning, especially in convolutional neural networks (CNNs), offer a promising
solution to improve this performance. This study proposes a new method for
estimating HR and RR using a novel 3D deep CNN, demonstrating a reduced error
in estimated HR by 68% and RR by 75%. These promising results suggest that
regressor-based deep learning approaches should be used in estimating HR and
RR.

</details>


### [661] [Machine Learning for Proactive Groundwater Management: Early Warning and Resource Allocation](https://arxiv.org/abs/2506.22461)
*Chuan Li,Ruoxuan Yang*

Main category: eess.SP

TL;DR: The paper introduces a machine learning pipeline to predict groundwater levels using extensive French groundwater data.


<details>
  <summary>Details</summary>
Motivation: Traditional groundwater monitoring faces challenges like sparse data and delayed outputs, necessitating innovative approaches.

Method: The study utilizes a machine learning pipeline via AutoGluon's ensemble framework, integrating geospatial preprocessing, feature engineering, and automated model selection.

Result: The model achieves weighted F1 scores of 0.927 on validation data and 0.67 on challenging temporally distinct test data.

Conclusion: This machine learning framework offers a scalable and effective solution for improving groundwater monitoring and response strategies worldwide.

Abstract: Groundwater supports ecosystems, agriculture, and drinking water supplies
worldwide, yet effective monitoring remains challenging due to sparse data,
computational constraints, and delayed outputs from traditional approaches. We
develop a machine learning pipeline that predicts groundwater level categories
using climate data, hydro-meteorological records, and physiographic attributes
processed through AutoGluon's automated ensemble framework. Our approach
integrates geospatial preprocessing, domain-driven feature engineering, and
automated model selection to overcome conventional monitoring limitations.
Applied to a large-scale French dataset (n $>$ 3,440,000 observations from
1,500+ wells), the model achieves weighted F\_1 scores of 0.927 on validation
data and 0.67 on temporally distinct test data. Scenario-based evaluations
demonstrate practical utility for early warning systems and water allocation
decisions under changing climate conditions. The open-source implementation
provides a scalable framework for integrating machine learning into national
groundwater monitoring networks, enabling more responsive and data-driven water
management strategies.

</details>


### [662] [Privacy-aware IoT Fall Detection Services For Aging in Place](https://arxiv.org/abs/2506.22462)
*Abdallah Lakhdari,Jiajie Li,Amani Abusafia,Athman Bouguettaya*

Main category: eess.SP

TL;DR: This paper proposes an IoT-based framework using UWB radar sensors and an FD-GPT model for accurate elderly fall detection, achieving over 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: To provide a solution for fall detection among the growing elderly population while addressing challenges like data scarcity and privacy concerns.

Method: The approach combines an IoT-based architecture with UWB radar sensors, a Fall Detection Generative Pre-trained Transformer (FD-GPT) for data augmentation, and a specialized protocol to gather realistic datasets.

Result: The proposed method achieves 90.72% accuracy and 89.33% precision in distinguishing fall events from regular daily activities, based on experimental validation.

Conclusion: The novel FDaaS framework can effectively support elderly individuals in independent living, balancing privacy, data availability, and detection performance.

Abstract: Fall detection is critical to support the growing elderly population,
projected to reach 2.1 billion by 2050. However, existing methods often face
data scarcity challenges or compromise privacy. We propose a novel IoT-based
Fall Detection as a Service (FDaaS) framework to assist the elderly in living
independently and safely by accurately detecting falls. We design a
service-oriented architecture that leverages Ultra-wideband (UWB) radar sensors
as an IoT health-sensing service, ensuring privacy and minimal intrusion. We
address the challenges of data scarcity by utilizing a Fall Detection
Generative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.
We developed a protocol to collect a comprehensive dataset of the elderly daily
activities and fall events. This resulted in a real dataset that carefully
mimics the elderly's routine. We rigorously evaluate and compare various models
using this dataset. Experimental results show our approach achieves 90.72%
accuracy and 89.33% precision in distinguishing between fall events and regular
activities of daily living.

</details>


### [663] [Dimensionality Reduction on IoT Monitoring Data of Smart Building for Energy Consumption Forecasting](https://arxiv.org/abs/2506.22468)
*Konstantinos Koutras,Agorakis Bompotas,Constantinos Halkiopoulos,Athanasios Kalogeras,Christos Alexakos*

Main category: eess.SP

TL;DR: The paper explores the correlation of environmental variables and energy consumption in an IoT-enabled smart office to optimize machine learning input parameters, ensuring accurate predictions with reduced data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance energy consumption prediction in IoT networks by identifying statistical correlations among environmental variables to reduce data input for low-resource edge computing devices.

Method: The study conducted ninety hypothesis tests across three environmental variables to assess their correlation with energy consumption, identifying statistically strong or weak relationships for optimization.

Result: Strong or semi-strong correlations were found between energy consumption and two environmental variables, while the correlation with a third variable was weak, enabling input parameter optimization.

Conclusion: The proposed methodology allows the identification and exclusion of weakly correlated variables, preserving prediction accuracy while reducing computational load for IoT edge devices.

Abstract: The Internet of Things (IoT) plays a major role today in smart building
infrastructures, from simple smart-home applications, to more sophisticated
industrial type installations. The vast amounts of data generated from relevant
systems can be processed in different ways revealing important information.
This is especially true in the era of edge computing, when advanced data
analysis and decision-making is gradually moving to the edge of the network
where devices are generally characterised by low computing resources. In this
context, one of the emerging main challenges is related to maintaining data
analysis accuracy even with less data that can be efficiently handled by low
resource devices. The present work focuses on correlation analysis of data
retrieved from a pilot IoT network installation monitoring a small smart office
by means of environmental and energy consumption sensors. The research
motivation was to find statistical correlation between the monitoring variables
that will allow the use of machine learning (ML) prediction algorithms for
energy consumption reducing input parameters. For this to happen, a series of
hypothesis tests for the correlation of three different environmental variables
with the energy consumption were carried out. A total of ninety tests were
performed, thirty for each pair of variables. In these tests, p-values showed
the existence of strong or semi-strong correlation with two environmental
variables, and of a weak correlation with a third one. Using the proposed
methodology, we manage without examining the entire data set to exclude weak
correlated variables while keeping the same score of accuracy.

</details>


### [664] [Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses](https://arxiv.org/abs/2506.22495)
*He-Yang Xu,Hongxiang Gao,Yuwen Li,Xiu-Shen Wei,Chengyu Liu*

Main category: eess.SP

TL;DR: The paper investigates Simplicity Bias (SB) in supervised ECG models, proposes a self-supervised learning method to mitigate SB, and demonstrates its efficacy with state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: ECG models often overfit to dominant patterns and miss subtle but critical diagnostic cues due to Simplicity Bias, which negatively impacts clinical performance.

Method: A self-supervised learning (SSL) framework using Temporal-Frequency Aware Filters for dynamic feature capture and Multi-Grained Prototype Reconstruction for detailed representation learning.

Result: The proposed method achieved state-of-the-art performance by reducing Simplicity Bias across three downstream tasks and six datasets with improved diagnostic accuracy.

Conclusion: Self-supervised learning is a promising approach for overcoming Simplicity Bias in ECG analysis, offering improved diagnostic capabilities and advancing ECG-based research with curated datasets.

Abstract: The diagnostic value of electrocardiogram (ECG) lies in its dynamic
characteristics, ranging from rhythm fluctuations to subtle waveform
deformations that evolve across time and frequency domains. However, supervised
ECG models tend to overfit dominant and repetitive patterns, overlooking
fine-grained but clinically critical cues, a phenomenon known as Simplicity
Bias (SB), where models favor easily learnable signals over subtle but
informative ones. In this work, we first empirically demonstrate the presence
of SB in ECG analyses and its negative impact on diagnostic performance, while
simultaneously discovering that self-supervised learning (SSL) can alleviate
it, providing a promising direction for tackling the bias. Following the SSL
paradigm, we propose a novel method comprising two key components: 1)
Temporal-Frequency aware Filters to capture temporal-frequency features
reflecting the dynamic characteristics of ECG signals, and 2) building on this,
Multi-Grained Prototype Reconstruction for coarse and fine representation
learning across dual domains, further mitigating SB. To advance SSL in ECG
analyses, we curate a large-scale multi-site ECG dataset with 1.53 million
recordings from over 300 clinical centers. Experiments on three downstream
tasks across six ECG datasets demonstrate that our method effectively reduces
SB and achieves state-of-the-art performance. Code and dataset will be released
publicly.

</details>


### [665] [Microelectrode Signal Dynamics as Biomarkers of Subthalamic Nucleus Entry on Deep Brain Stimulation: A Nonlinear Feature Approach](https://arxiv.org/abs/2506.22454)
*Ana Luiza S. Tavares,Artur Pedro M. Neto,Francinaldo L. Gomes,Paul Rodrigo dos Reis,Arthur G. da Silva,Antonio P. Junior,Bruno D. Gomes*

Main category: eess.SP

TL;DR: The study improves Deep Brain Stimulation (DBS) efficacy by proposing an entropy and nonlinear dynamic-based framework for accurate intraoperative localization of the subthalamic nucleus (STN) during Parkinson's surgery, achieving high classification metrics.


<details>
  <summary>Details</summary>
Motivation: Current DBS procedures often rely on subjective interpretation of microelectrode recordings (MERs) to locate the STN, which can impact precision and efficacy. The study aims to develop a data-driven method for enhancing accuracy.

Method: The paper preprocesses MER data with artifact correction, extracts features using recurrence quantification analysis, nonlinear, and entropy metrics, and tests multiple supervised classifiers through 10-fold cross-validation and statistical methods.

Result: The entropy and nonlinear features were most discriminative, with the Extra Trees classifier achieving an F1-score of 0.902±0.027 and ROC AUC of 0.887±0.055. Generalization metrics on hold-out data showed F1=0.922 and ROC AUC=0.941.

Conclusion: Entropy and nonlinear signal analysis reliably distinguish intra-STN from extra-STN activity, offering a robust framework for real-time, data-driven decision support in DBS surgeries.

Abstract: Accurate intraoperative localization of the subthalamic nucleus (STN) is
essential for the efficacy of Deep Brain Stimulation (DBS) in patients with
Parkinson's disease. While microelectrode recordings (MERs) provide rich
electrophysiological information during DBS electrode implantation, current
localization practices often rely on subjective interpretation of signal
features. In this study, we propose a quantitative framework that leverages
nonlinear dynamics and entropy-based metrics to classify neural activity
recorded inside versus outside the STN. MER data from three patients were
preprocessed using a robust artifact correction pipeline, segmented, and
labelled based on surgical annotations. A comprehensive set of recurrence
quantification analysis, nonlinear, and entropy features were extracted from
each segment. Multiple supervised classifiers were trained on every combination
of feature domains using stratified 10-fold cross-validation, followed by
statistical comparison using paired Wilcoxon signed-rank tests with
Holm-Bonferroni correction. The combination of entropy and nonlinear features
yielded the highest discriminative power, and the Extra Trees classifier
emerged as the best model with a cross-validated F1-score of 0.902+/-0.027 and
ROC AUC of 0.887+/-0.055. Final evaluation on a 20% hold-out test set confirmed
robust generalization (F1= 0.922, ROC AUC = 0.941). These results highlight the
potential of nonlinear and entropy signal descriptors in supporting real-time,
data-driven decision-making during DBS surgeries

</details>


### [666] [Data Normalization Strategies for EEG Deep Learning](https://arxiv.org/abs/2506.22455)
*Dung Truong,Arnaud Delorme*

Main category: eess.SP

TL;DR: This paper investigates the impact of normalization strategies on both supervised and self-supervised EEG deep learning tasks, showing that different approaches are optimal for different task paradigms.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding optimal normalization strategies in the emerging paradigm of large-scale pretraining (e.g., self-supervised learning) for EEG deep learning applications.

Method: Systematic evaluation of different normalization granularities (recording vs. window level) and scopes (cross-channel vs. within-channel) on supervised (age and gender prediction) and self-supervised tasks using high-density EEG data from 2,836 individuals.

Result: Window-level within-channel normalization is optimal for supervised tasks, while cross-channel normalization or minimal normalization at the window level is more effective for self-supervised learning.

Conclusion: Task-specific normalization choices are essential, and there is no universal normalization strategy that generalizes across learning paradigms. These findings are critical for designing robust EEG deep learning pipelines for future large-scale model training.

Abstract: Normalization is a critical yet often overlooked component in the
preprocessing pipeline for EEG deep learning applications. The rise of
large-scale pretraining paradigms such as self-supervised learning (SSL)
introduces a new set of tasks whose nature is substantially different from
supervised training common in EEG deep learning applications. This raises new
questions about optimal normalization strategies for the applicable task. In
this study, we systematically evaluate the impact of normalization granularity
(recording vs. window level) and scope (cross-channel vs. within-channel) on
both supervised (age and gender prediction) and self-supervised (Contrastive
Predictive Coding) tasks. Using high-density resting-state EEG from 2,836
subjects in the Healthy Brain Network dataset, we show that optimal
normalization strategies differ significantly between training paradigms.
Window-level within-channel normalization yields the best performance in
supervised tasks, while minimal or cross-channel normalization at the window
level is more effective for SSL. These results underscore the necessity of
task-specific normalization choices and challenge the assumption that a
universal normalization strategy can generalize across learning settings. Our
findings provide practical insights for developing robust EEG deep learning
pipelines as the field shifts toward large-scale, foundation model training.

</details>


### [667] [Physics-Embedded Neural Networks for sEMG-based Continuous Motion Estimation](https://arxiv.org/abs/2506.22459)
*Wending Heng,Chaoyuan Liang,Yihui Zhao,Zhiqiang Zhang,Glen Cooper,Zhenhong Li*

Main category: eess.SP

TL;DR: This paper introduces a Physics-Embedded Neural Network (PENN) to improve human motion intention decoding using surface electromyography (sEMG) by combining musculoskeletal models with data-driven methods.


<details>
  <summary>Details</summary>
Motivation: Existing sEMG-based motion estimation methods face challenges due to reliance on subject-specific musculoskeletal models or purely data-driven approaches lacking physiological consistency.

Method: This paper proposes the PENN model, integrating interpretable musculoskeletal dynamics with residual learning, a recursive temporal structure, and lightweight convolutional neural networks for refinement.

Result: Experimental results on six subjects show that PENN achieves better accuracy than baseline methods in both RMSE and $R^2$ metrics.

Conclusion: The PENN framework successfully improves motion estimation by balancing physiological consistency and estimation accuracy, proving robust in practical applications.

Abstract: Accurately decoding human motion intentions from surface electromyography
(sEMG) is essential for myoelectric control and has wide applications in
rehabilitation robotics and assistive technologies. However, existing
sEMG-based motion estimation methods often rely on subject-specific
musculoskeletal (MSK) models that are difficult to calibrate, or purely
data-driven models that lack physiological consistency. This paper introduces a
novel Physics-Embedded Neural Network (PENN) that combines interpretable MSK
forward-dynamics with data-driven residual learning, thereby preserving
physiological consistency while achieving accurate motion estimation. The PENN
employs a recursive temporal structure to propagate historical estimates and a
lightweight convolutional neural network for residual correction, leading to
robust and temporally coherent estimations. A two-phase training strategy is
designed for PENN. Experimental evaluations on six healthy subjects show that
PENN outperforms state-of-the-art baseline methods in both root mean square
error (RMSE) and $R^2$ metrics.

</details>


### [668] [Zero-Shot EEG-to-Gait Decoding via Phase-Aware Representation Learning](https://arxiv.org/abs/2506.22488)
*Xi Fu,Weibang Jiang,Rui Liu,Gernot R. Müller-Putz,Cuntai Guan*

Main category: eess.SP

TL;DR: The paper introduces NeuroDyGait, a domain-generalizable framework for decoding lower-limb motion from EEG signals, addressing issues of variability in subjects and sessions.


<details>
  <summary>Details</summary>
Motivation: To enable accurate and scalable EEG-to-motion decoding for advancing BCI applications in movement intent recognition and control.

Method: The paper employs structured contrastive representation learning, semantic alignment between EEG and motion embeddings, multi-cycle gait reconstruction, and a dynamic decoding mechanism for inter-session generalization.

Result: NeuroDyGait achieves superior zero-shot performance in cross-subject gait decoding, strong phase-detection, and eliminates the need for adaptation on unseen individuals.

Conclusion: Relational domain learning in NeuroDyGait enables scalable, target-free deployment of BCIs, offering significant advancements in motion decoding frameworks.

Abstract: Accurate decoding of lower-limb motion from EEG signals is essential for
advancing brain-computer interface (BCI) applications in movement intent
recognition and control. However, challenges persist in achieving causal,
phase-consistent predictions and in modeling both inter- and intra-subject
variability. To address these issues, we propose NeuroDyGait, a
domain-generalizable EEG-to-motion decoding framework that leverages structured
contrastive representation learning and relational domain modeling. The
proposed method employs relative contrastive learning to achieve semantic
alignment between EEG and motion embeddings. Furthermore, a multi-cycle gait
reconstruction objective is introduced to enforce temporal coherence and
maintain biomechanical consistency. To promote inter-session generalization,
during fine-tuning, a domain dynamic decoding mechanism adaptively assigns
session-specific prediction heads and learns to mix their outputs based on
inter-session relationships. NeuroDyGait enables zero-shot motion prediction
for unseen individuals without requiring adaptation and achieves superior
performance in cross-subject gait decoding on benchmark datasets. Additionally,
it demonstrates strong phase-detection capabilities even without explicit phase
supervision during training. These findings highlight the potential of
relational domain learning in enabling scalable, target-free deployment of
BCIs.

</details>


### [669] [MENGLAN: Multiscale Enhanced Nonparametric Gas Analyzer with Lightweight Architecture and Networks](https://arxiv.org/abs/2506.22490)
*Zhenke Duan,Jiqun Pan,Jiani Tu*

Main category: eess.SP

TL;DR: MENGLAN is a newly proposed model for detecting ethylene concentrations in mixed gases, offering improved accuracy, reduced computational demand, and practical deployability.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for detecting ethylene concentrations are expensive, complex, and impractical, creating the need for a more efficient and deployable solution.

Method: MENGLAN incorporates a combination of innovative components: a dual-stream structure, Hybrid Multi-Head Attention mechanism, and Feature Reactivation Module for real-time, lightweight, and high-precision predictions.

Result: MENGLAN demonstrates superior predictive performance, lower computational requirements, and better deployability compared to existing approaches.

Conclusion: The study concludes that MENGLAN effectively addresses the limitations of traditional methods and provides a feasible option for chemical production safety and health monitoring.

Abstract: Accurate detection of ethylene concentrations in mixed gases is crucial in
chemical production for safety and health purposes. Traditional methods are
hindered by high cost and complexity, limiting their practical application.
This study proposes MENGLAN, a Multiscale Enhanced Nonparametric Gas Analyzer
that integrates a dual-stream structure, a Hybrid Multi-Head Attention
mechanism, and a Feature Reactivation Module to enable real-time, lightweight,
and high-precision ethylene concentration prediction. Results show that MENGLAN
achieves superior performance, reduced computational demand, and enhanced
deployability compared to existing methods.

</details>


### [670] [Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver](https://arxiv.org/abs/2506.23203)
*Feng Shu,Jiatong Bai,Di Wu,Wei Zhu,Bin Deng,Fuhui Zhou,Jiangzhou Wang*

Main category: eess.SP

TL;DR: The paper focuses on advancing direction-of-arrival (DOA) sensing in 6G massive H$^2$AD MIMO systems by proposing a lightweight CRLB-ratio-weight fusion (WF) method and a multi-branch deep neural network (MBDNN) to enhance performance and reduce complexity.


<details>
  <summary>Details</summary>
Motivation: Designing a low-complexity and high-performance method for fusing target direction values in massive H$^2$AD-based 6G networks is challenging due to the need for reduced dependence on prior knowledge and computational resource.

Method: The paper introduces a CRLB-ratio-weight fusion (WF) method to approximate inverse CRLB using antenna number reciprocals, and a multi-branch deep neural network (MBDNN) for improved DOA sensing, integrating subarray-specific branches with a shared regression module for angle fusion.

Result: The CRLB-ratio-WF method achieves DOA sensing performance comparable to CRLB-based methods, while reducing reliance on prior knowledge. The MBDNN outperforms CRLB-ratio-WF in low-SNR scenarios, achieving significantly higher estimation accuracy at SNR = -15 dB.

Conclusion: The proposed lightweight CRLB-ratio-WF and MBDNN method effectively address the challenges of DOA sensing in massive H$^2$AD systems, offering high accuracy and low complexity solutions, especially in low-SNR conditions.

Abstract: As a green MIMO structure, massive H$^2$AD is viewed as a potential
technology for the future 6G wireless network. For such a structure, it is a
challenging task to design a low-complexity and high-performance fusion of
target direction values sensed by different sub-array groups with fewer use of
prior knowledge. To address this issue, a lightweight Cramer-Rao lower bound
(CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse
CRLB of each subarray using antenna number reciprocals to eliminate real-time
CRLB computation. This reduces complexity and prior knowledge dependence while
preserving fusion performance. Moreover, a multi-branch deep neural network
(MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by
leveraging candidate angles from multiple subarrays. The subarray-specific
branch networks are integrated with a shared regression module to effectively
eliminate pseudo-solutions and fuse true angles. Simulation results show that
the proposed CRLB-ratio-WF method achieves DOA sensing performance comparable
to CRLB-based methods, while significantly reducing the reliance on prior
knowledge. More notably, the proposed MBDNN has superior performance in low-SNR
ranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in
estimation accuracy compared to CRLB-ratio-WF method.

</details>


### [671] [Differentiable Radar Ambiguity Functions: Mathematical Formulation and Computational Implementation](https://arxiv.org/abs/2506.22935)
*Marc Bara Iniesta*

Main category: eess.SP

TL;DR: The paper introduces a differentiable radar ambiguity function framework (GRAF) compatible with modern machine learning, solving challenges like complex-valued gradient computation.


<details>
  <summary>Details</summary>
Motivation: Radar waveform design faces limitations due to non-differentiable ambiguity functions, hindering integration with machine learning frameworks.

Method: The paper employs Wirtinger calculus, parallelized FFT operations, and numerical stable pipelines to enable differentiability in radar ambiguity functions.

Result: GRAF achieves gradient flow through the ambiguity function computation pipeline and demonstrates suitability for neural-network-based waveform optimization.

Conclusion: This work bridges radar signal processing with machine learning by providing a foundation for gradient-based optimization in radar applications.

Abstract: The ambiguity function is fundamental to radar waveform design,
characterizing range and Doppler resolution capabilities. However, its
traditional formulation involves non-differentiable operations, preventing
integration with gradient-based optimization methods and modern machine
learning frameworks. This paper presents the first complete mathematical
framework and computational implementation for differentiable radar ambiguity
functions. Our approach addresses the fundamental technical challenges that
have prevented the radar community from leveraging automatic differentiation:
proper handling of complex-valued gradients using Wirtinger calculus, efficient
computation through parallelized FFT operations, numerical stability throughout
cascaded operations, and composability with arbitrary differentiable
operations. We term this approach GRAF (Gradient-based Radar Ambiguity
Functions), which reformulates the ambiguity function computation to maintain
mathematical equivalence while enabling gradient flow through the entire
pipeline. The resulting implementation provides a general-purpose
differentiable ambiguity function compatible with modern automatic
differentiation frameworks, enabling new research directions including neural
network-based waveform generation with ambiguity constraints, end-to-end
optimization of radar systems, and integration of classical radar theory with
modern deep learning. We provide complete implementation details and
demonstrate computational efficiency suitable for practical applications. This
work establishes the mathematical and computational foundation for applying
modern machine learning techniques to radar waveform design, bridging classical
radar signal processing with automatic differentiation frameworks.

</details>


### [672] [SegmentAnyMuscle: A universal muscle segmentation model across different locations in MRI](https://arxiv.org/abs/2506.22467)
*Roy Colglazier,Jisoo Lee,Haoyu Dong,Hanxue Gu,Yaqian Chen,Joseph Cao,Zafer Yildiz,Zhonghao Liu,Nicholas Konz,Jichen Yang,Jikai Zhang,Yuwen Chen,Lin Li,Adrian Camarena,Maciej A. Mazurowski*

Main category: eess.SP

TL;DR: This paper presents a deep learning model for automated muscle segmentation in MRI, achieving high accuracy across diverse imaging settings, and releases it publicly for research use.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of obtaining precise quantitative muscle measurements from MRI, which are crucial predictors of health outcomes.

Method: The paper utilizes a deep learning algorithm trained on 316 MRIs to segment muscles and tested its performance across standard and challenging MRI sequences.

Result: The model achieved an average Dice Similarity Coefficient of 88.45% on common sequences and 86.21% on less common sequences with abnormalities, demonstrating robustness.

Conclusion: The study concludes that automated deep learning muscle segmentation in MRI is feasible, consistent, and valuable for advancing research into musculature and health relationships.

Abstract: The quantity and quality of muscles are increasingly recognized as important
predictors of health outcomes. While MRI offers a valuable modality for such
assessments, obtaining precise quantitative measurements of musculature remains
challenging. This study aimed to develop a publicly available model for muscle
segmentation in MRIs and demonstrate its applicability across various
anatomical locations and imaging sequences. A total of 362 MRIs from 160
patients at a single tertiary center (Duke University Health System, 2016-2020)
were included, with 316 MRIs from 114 patients used for model development. The
model was tested on two separate sets: one with 28 MRIs representing common
sequence types, achieving an average Dice Similarity Coefficient (DSC) of
88.45%, and another with 18 MRIs featuring less frequent sequences and
abnormalities such as muscular atrophy, hardware, and significant noise,
achieving 86.21% DSC. These results demonstrate the feasibility of a fully
automated deep learning algorithm for segmenting muscles on MRI across diverse
settings. The public release of this model enables consistent, reproducible
research into the relationship between musculature and health.

</details>


### [673] [Post-processing of EEG-based Auditory Attention Decoding Decisions via Hidden Markov Models](https://arxiv.org/abs/2506.24024)
*Nicolas Heintz,Tom Francart,Alexander Bertrand*

Main category: eess.SP

TL;DR: This paper improves auditory attention decoding (AAD) algorithms by integrating a hidden Markov model (HMM), which accounts for temporal attention patterns and improves accuracy and responsiveness.


<details>
  <summary>Details</summary>
Motivation: Current AAD algorithms have accuracy limitations, making them impractical for real-world applications, especially in identifying focused speakers in noisy environments.

Method: The researchers integrated a hidden Markov model (HMM) into AAD algorithms to exploit temporal attention patterns, assuming listeners are less likely to frequently switch their focus between speakers.

Result: The incorporation of HMM enhanced the accuracy and responsiveness of AAD algorithms in both real-time and offline contexts, outperforming other postprocessing techniques.

Conclusion: The HMM framework provides a computationally efficient and operationally intuitive solution for improving AAD, making it applicable for practical scenarios.

Abstract: Auditory attention decoding (AAD) algorithms exploit brain signals, such as
electroencephalography (EEG), to identify which speaker a listener is focusing
on in a multi-speaker environment. While state-of-the-art AAD algorithms can
identify the attended speaker on short time windows, their predictions are
often too inaccurate for practical use. In this work, we propose augmenting AAD
with a hidden Markov model (HMM) that models the temporal structure of
attention. More specifically, the HMM relies on the fact that a subject is much
less likely to switch attention than to keep attending the same speaker at any
moment in time. We show how a HMM can significantly improve existing AAD
algorithms in both causal (real-time) and non-causal (offline) settings. We
further demonstrate that HMMs outperform existing postprocessing approaches in
both accuracy and responsiveness, and explore how various factors such as
window length, switching frequency, and AAD accuracy influence overall
performance. The proposed method is computationally efficient, intuitive to use
and applicable in both real-time and offline settings.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [674] [Beyond Code: The Multidimensional Impacts of Large Language Models in Software Development](https://arxiv.org/abs/2506.22704)
*Sardar Fatooreh Bonabi,Sarah Bana,Tingting Nian,Vijay Gurbaxani*

Main category: econ.GN

TL;DR: The paper investigates the impact of Large Language Models (LLMs) on Open-Source Software development and finds that LLM access improves productivity, knowledge sharing, and skill acquisition, varying by developer experience and context.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the mechanisms and effects of LLMs, like ChatGPT, on OSS developers' productivity, knowledge exchange, and skill enhancement.

Method: The study uses a natural experiment involving a temporary ChatGPT ban in Italy and a Difference-in-Differences framework with data from GitHub developers in Italy, France, and Portugal.

Result: Results show that ChatGPT access boosts developer productivity by 6.4%, knowledge sharing by 9.6%, and skill acquisition by 8.4%, with variations based on experience and context.

Conclusion: Strategically deploying LLMs can benefit developers at different experience levels and improve broader organizational productivity and collaboration.

Abstract: Large language models (LLMs) are poised to significantly impact software
development, especially in the Open-Source Software (OSS) sector. To understand
this impact, we first outline the mechanisms through which LLMs may influence
OSS through code development, collaborative knowledge transfer, and skill
development. We then empirically examine how LLMs affect OSS developers' work
in these three key areas. Leveraging a natural experiment from a temporary
ChatGPT ban in Italy, we employ a Difference-in-Differences framework with
two-way fixed effects to analyze data from all OSS developers on GitHub in
three similar countries, Italy, France, and Portugal, totaling 88,022 users. We
find that access to ChatGPT increases developer productivity by 6.4%, knowledge
sharing by 9.6%, and skill acquisition by 8.4%. These benefits vary
significantly by user experience level: novice developers primarily experience
productivity gains, whereas more experienced developers benefit more from
improved knowledge sharing and accelerated skill acquisition. In addition, we
find that LLM-assisted learning is highly context-dependent, with the greatest
benefits observed in technically complex, fragmented, or rapidly evolving
contexts. We show that the productivity effects of LLMs extend beyond direct
code generation to include enhanced collaborative learning and knowledge
exchange among developers; dynamics that are essential for gaining a holistic
understanding of LLMs' impact in OSS. Our findings offer critical managerial
implications: strategically deploying LLMs can accelerate novice developers'
onboarding and productivity, empower intermediate developers to foster
knowledge sharing and collaboration, and support rapid skill acquisition,
together enhancing long-term organizational productivity and agility.

</details>


<div id='astro-ph.GA'></div>

# astro-ph.GA [[Back]](#toc)

### [675] [Simulation-based population inference of LISA's Galactic binaries: Bypassing the global fit](https://arxiv.org/abs/2506.22543)
*Rahul Srinivasan,Enrico Barausse,Natalia Korsakova,Roberto Trotta*

Main category: astro-ph.GA

TL;DR: The authors propose a novel method to infer parameters of Galactic double white dwarfs directly from LISA data compressed into frequency series using normalizing flows, avoiding traditional global fits.


<details>
  <summary>Details</summary>
Motivation: Current LISA data analysis relies on a computationally expensive global fit procedure, which complicates the process of extracting astrophysical information from overlapping resolved and unresolved gravitational wave sources.

Method: A simulation-based approach is introduced, where a custom frequency series compression and normalizing flow are trained on simulated LISA data from Galactic double white dwarfs to infer population posterior distributions efficiently.

Result: This method successfully extracts population parameter information from resolved and unresolved sources simultaneously, offering computational efficiency and extensibility to other source classes and noise scenarios.

Conclusion: The proposed approach simplifies LISA data analysis, bypasses the global fit, and can adapt to various source types and noise conditions, making it versatile for future applications.

Abstract: The Laser Interferometer Space Antenna (LISA) is expected to detect thousands
of individually resolved gravitational wave sources, overlapping in time and
frequency, on top of unresolved astrophysical and/or primordial backgrounds.
Disentangling resolved sources from backgrounds and extracting their parameters
in a computationally intensive "global fit" is normally regarded as a necessary
step toward reconstructing the properties of the underlying astrophysical
populations. Here, we show that it is possible to infer the properties of the
most numerous population of LISA sources - Galactic double white dwarfs -
directly from the frequency (or, equivalently, time) strain series, by using a
simulation-based approach that bypasses the global fit entirely. By training a
normalizing flow on a custom-designed compression of simulated LISA frequency
series from the Galactic double white dwarf population, we demonstrate how to
infer the posterior distribution of population parameters (e.g., mass function,
frequency, and spatial distributions). This allows for extracting information
on the population parameters from both resolved and unresolved sources
simultaneously and in a computationally efficient manner. Our approach to
target population properties directly can be readily extended to other source
classes (e.g., massive and stellar-mass black holes, extreme mass ratio
inspirals), provided fast simulations are available, and to scenarios involving
non-Gaussian or non-stationary noise (e.g., data gaps).

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [676] [Learning robust parameter inference and density reconstruction in flyer plate impact experiments](https://arxiv.org/abs/2506.23914)
*Evan Bell,Daniel A. Serino,Ben S. Southworth,Trevor Wilcox,Marc L. Klasky*

Main category: physics.comp-ph

TL;DR: This paper explores a machine learning approach to estimate material properties (EoS and crush model parameters) from radiographic observations, focusing on porous materials impacted at varied velocities.


<details>
  <summary>Details</summary>
Motivation: Inferring physical parameters like density from radiographs is challenging because radiography does not directly observe key state variables, limiting traditional parameter estimation methods.

Method: The authors combine low and high impact velocity datasets and utilize a generative machine learning framework to produce posterior distributions of material parameters from radiographs.

Result: The method effectively estimates EoS and crush model parameters, enabling accurate and physically consistent density reconstructions in hydrodynamic simulations, even with model mismatches and unseen physics.

Conclusion: This approach represents a significant advancement in using experimental radiographs for parameter estimation, promoting robust and reliable material property inference despite noisy or unseen conditions.

Abstract: Estimating physical parameters or material properties from experimental
observations is a common objective in many areas of physics and material
science. In many experiments, especially in shock physics, radiography is the
primary means of observing the system of interest. However, radiography does
not provide direct access to key state variables, such as density, which
prevents the application of traditional parameter estimation approaches. Here
we focus on flyer plate impact experiments on porous materials, and resolving
the underlying parameterized equation of state (EoS) and crush porosity model
parameters given radiographic observation(s). We use machine learning as a tool
to demonstrate with high confidence that using only high impact velocity data
does not provide sufficient information to accurately infer both EoS and crush
model parameters, even with fully resolved density fields or a dynamic sequence
of images. We thus propose an observable data set consisting of low and high
impact velocity experiments/simulations that capture different regimes of
compaction and shock propagation, and proceed to introduce a generative machine
learning approach which produces a posterior distribution of physical
parameters directly from radiographs. We demonstrate the effectiveness of the
approach in estimating parameters from simulated flyer plate impact
experiments, and show that the obtained estimates of EoS and crush model
parameters can then be used in hydrodynamic simulations to obtain accurate and
physically admissible density reconstructions. Finally, we examine the
robustness of the approach to model mismatches, and find that the learned
approach can provide useful parameter estimates in the presence of
out-of-distribution radiographic noise and previously unseen physics, thereby
promoting a potential breakthrough in estimating material properties from
experimental radiographic images.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [677] [Reachability in symmetric VASS](https://arxiv.org/abs/2506.23578)
*Łukasz Kamiński,Sławomir Lasota*

Main category: cs.FL

TL;DR: The paper explores the reachability of symmetric vector addition systems with states (VASS), demonstrating PSPACE complexity for symmetric groups and addressing other group invariants.


<details>
  <summary>Details</summary>
Motivation: Understanding the impact of group symmetry on VASS reachability and addressing complexities such as the open problem in data VASS.

Method: Analyzing symmetric VASS where transitions follow group permutations, exploring various groups like symmetric, alternating, and cyclic.

Result: PSPACE complexity is established for symmetric groups in VASS, improving substantially over Ackermannian complexity seen in general VASS.

Conclusion: Group symmetry significantly reduces computational complexity in VASS reachability, with potential insights for data VASS reachability problems.

Abstract: We investigate the reachability problem in symmetric vector addition systems
with states (VASS), where transitions are invariant under a group of
permutations of coordinates. One extremal case, the trivial groups, yields
general VASS. In another extremal case, the symmetric groups, we show that the
reachability problem can be solved in PSPACE, regardless of the dimension of
input VASS (to be contrasted with Ackermannian complexity in general VASS). We
also consider other groups, in particular alternating and cyclic ones.
Furthermore, motivated by the open status of the reachability problem in data
VASS, we estimate the gain in complexity when the group arises as a combination
of the trivial and symmetric groups.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [678] [Arnoldi Singular Vector perturbations for machine learning weather prediction](https://arxiv.org/abs/2506.22450)
*Jens Winkler,Michael Denhard*

Main category: physics.ao-ph

TL;DR: This paper explores how errors in initial conditions affect machine learning weather predictions (MLWP) using a novel Arnoldi-Singular Vector (A-SV) method to identify error growth directions, specifically with Huawei's Pangu Weather model.


<details>
  <summary>Details</summary>
Motivation: To improve the reliability of machine learning weather forecasts by understanding how errors grow under initial condition perturbations, enabling the design of better ensemble prediction systems.

Method: A-SV method generates perturbations using Krylov subspaces without requiring linear or adjoint models. It observes error growth iteratively by applying the forecast model to perturbed states and identifies leading modes for growing errors.

Result: A-SV successfully found meaningful perturbation patterns that grow from the start of a forecast rollout in the Pangu Weather model. The perturbations showed local unstable modes useful for initializing prediction ensembles.

Conclusion: The A-SV method provides dynamically relevant perturbations for MLWP and could enhance ensemble initialization. It draws parallels to the denoising aspect of diffusion-based ML models like GenCast.

Abstract: Since weather forecasts are fundamentally uncertain, reliable decision making
requires information on the likelihoods of future weather scenarios. We explore
the sensitivity of machine learning weather prediction (MLWP) using the 24h
Pangu Weather ML model of Huawei to errors in the initial conditions with a
specific kind of Singular Vector (SV) perturbations. Our Arnoldi-SV (A-SV)
method does not need linear nor adjoint model versions and is applicable to
numerical weather prediction (NWP) as well as MLWP. It observes error growth
within a given optimization time window by iteratively applying a forecast
model to perturbed model states. This creates a Krylov subspace, implicitly
based on a matrix operator, which approximates the local error growth. Each
iteration adds new dimensions to the Krylov space and its leading right SVs are
expected to turn into directions of growing errors. We show that A-SV indeed
finds dynamically meaningful perturbation patterns for the 24h Pangu Weather
model, which grow right from the beginning of the forecast rollout. These
perturbations describe local unstable modes and could be a basis to initialize
MLWP ensembles. Since we start A-SV from random noise perturbations, the
algorithm transforms noise into perturbations conditioned on a given reference
state - a process that is akin to the denoising process of the generic
diffusion based ML model of GenCast, therefor we briefly discuss similarities
and differences.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [679] [Segmented Operations using Matrix Multiplications](https://arxiv.org/abs/2506.23906)
*Aleksandros Sobczyk,Giuseppe Sorrentino,Anastasios Zouzias*

Main category: cs.DS

TL;DR: The paper introduces MMV-RAM, a computational model designed for matrix multiplication accelerators, addressing their underutilization beyond dense matrix multiplications.


<details>
  <summary>Details</summary>
Motivation: The work aims to address the stagnation in algorithm analysis for matrix multiplication architectures due to the absence of suitable theoretical models.

Method: The MMV-RAM computational model incorporates a specialized unit capable of performing matrix multiplications in a single parallel step, alongside a rigorous theoretical analysis and balanced computational power.

Result: The study develops algorithms for segmented scan and sum, achieving significant theoretical speedups compared to vector-only approaches within MMV-RAM.

Conclusion: MMV-RAM offers a theoretical foundation for optimizing computations on matrix multiplication accelerators, demonstrating both worst-case complexity improvements and practical potential for efficient implementation.

Abstract: Specialized computational units that perform small matrix multiplications as
primitive operations are typically present in modern accelerators. However,
these units are often underutilized for many fundamental operations besides
dense matrix multiplications. The analysis of algorithms for such architectures
is currently stagnated due to the lack of a rigorous theoretical model of
computation that captures their characteristics. In this work, we propose
MMV-RAM, a computational model tailored to matrix multiplication accelerators.
MMV-RAM judiciously extends the Vector-RAM model with an additional processing
unit that multiplies two matrices of sizes $n\times s$ and $s\times s$ in a
single parallel step, where $s$ is a model parameter. We provide a detailed
theoretical analysis of the model, and carefully balance the computational
power between the matrix and vector units, guided by the circuit complexity
lower bound that parity is not in AC[0].
  In MMV-RAM, we study algorithms for segmented scan and sum, two fundamental
parallel primitives. We propose a segmented scan algorithm that uses matrix
multiplications to perform speculative block-scan computations, which runs in
$O(\log_s(n))$ steps. In contrast, we show that any algorithm that uses only
the vector unit of MMV-RAM requires
$\Omega\left(\frac{\log_2(n)}{\log_2\log_2(n)}\right)$ steps. We further apply
these techniques to obtain similar theoretical speedups for element-wise vector
multiplication and matrix multiplication. Beyond the worst-case complexity
analysis, we propose algorithms for segmented operations that could lead to
highly efficient and pragmatic implementations. For example, we observe that
segmented sum is a combination of three elementary parallel primitives: scan,
compress, and vector differentiation. As a case study, we implement...

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [680] [Overparametrized models with posterior drift](https://arxiv.org/abs/2506.23619)
*Guillaume Coqueret,Martial Laguerre*

Main category: q-fin.ST

TL;DR: This paper studies how changes in data patterns affect forecasting accuracy in complex machine learning models, especially for stock market predictions.


<details>
  <summary>Details</summary>
Motivation: Explore how machine learning models perform when data patterns change over time, particularly in scenarios like financial markets.

Method: Analyze the impact of posterior drift and evaluate factors like periods and bandwidths in equity premium forecasting.

Result: Long holding periods yield variable returns, small bandwidths cause inconsistency, while large bandwidths are stable but less attractive risk-wise.

Conclusion: Caution is advised when using large linear machine learning models for stock market forecasting.

Abstract: This paper investigates the impact of posterior drift on out-of-sample
forecasting accuracy in overparametrized machine learning models. We document
the loss in performance when the loadings of the data generating process change
between the training and testing samples. This matters crucially in settings in
which regime changes are likely to occur, for instance, in financial markets.
Applied to equity premium forecasting, our results underline the sensitivity of
a market timing strategy to sub-periods and to the bandwidth parameters that
control the complexity of the model. For the average investor, we find that
focusing on holding periods of 15 years can generate very heterogeneous
returns, especially for small bandwidths. Large bandwidths yield much more
consistent outcomes, but are far less appealing from a risk-adjusted return
standpoint. All in all, our findings tend to recommend cautiousness when
resorting to large linear models for stock market predictions.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [681] [Spectral Bias in Variational Quantum Machine Learning](https://arxiv.org/abs/2506.22555)
*Callum Duffy,Marcin Jastrzebski*

Main category: quant-ph

TL;DR: The paper studies spectral bias in parameterized quantum circuits (PQCs), showing how spectral bias correlates with the redundancy of Fourier coefficients and is influenced by data encoding schemes, gradient magnitudes, and perturbation robustness.


<details>
  <summary>Details</summary>
Motivation: To analyze spectral bias in quantum machine learning, particularly the frequency-dependent rate of convergence, and understand how Fourier coefficients' properties in PQCs contribute to this bias.

Method: The authors leverage the Fourier series representation of PQCs, analytically link spectral bias to Fourier coefficient redundancy determined by data encoding schemes, and perform empirical validations with multiple encoding schemes.

Result: They show a direct correlation between Fourier coefficients' redundancy and gradient magnitudes during training in PQCs. Redundant circuits are more robust to random parameter perturbations. Additionally, choices in parameter initialization and entanglement structure impact convergence rates.

Conclusion: Understanding spectral bias and its relationship with redundancy in PQCs guides better circuit design, encoding schemes, and initialization/entanglement strategies to optimize learning and robustness.

Abstract: In this work, we investigate the phenomenon of spectral bias in quantum
machine learning, where, in classical settings, models tend to fit
low-frequency components of a target function earlier during training than
high-frequency ones, demonstrating a frequency-dependent rate of convergence.
We study this effect specifically in parameterised quantum circuits (PQCs).
Leveraging the established formulation of PQCs as Fourier series, we prove that
spectral bias in this setting arises from the ``redundancy'' of the Fourier
coefficients, which denotes the number of terms in the analytical form of the
model contributing to the same frequency component. The choice of data encoding
scheme dictates the degree of redundancy for a Fourier coefficient. We find
that the magnitude of the Fourier coefficients' gradients during training
strongly correlates with the coefficients' redundancy. We then further
demonstrate this empirically with three different encoding schemes.
Additionally, we demonstrate that PQCs with greater redundancy exhibit
increased robustness to random perturbations in their parameters at the
corresponding frequencies. We investigate how design choices affect the ability
of PQCs to learn Fourier sums, focusing on parameter initialization scale and
entanglement structure, finding large initializations and low-entanglement
schemes tend to slow convergence.

</details>


### [682] [Tensor Train Quantum State Tomography using Compressed Sensing](https://arxiv.org/abs/2506.23560)
*Shakir Showkat Sofi,Charlotte Vermeylen,Lieven De Lathauwer*

Main category: quant-ph

TL;DR: The paper addresses the inefficiency of standard quantum state tomography (QST) methods by proposing a low-rank block tensor train decomposition for improved computational and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper lies in overcoming the impracticality of standard QST methods due to the exponential growth of parameters in state representation, particularly as quantum systems scale.

Method: The paper introduces a low-rank block tensor train decomposition to parameterize quantum states, enabling efficient estimation and representation of quantum systems.

Result: The proposed method demonstrates computationally and memory-efficient QST for a broad range of quantum states, including pure states, nearly pure states, and ground states of Hamiltonians.

Conclusion: The low-rank block tensor train decomposition framework offers a scalable and efficient solution for QST, making it applicable to evaluating quantum devices and systems with complex states.

Abstract: Quantum state tomography (QST) is a fundamental technique for estimating the
state of a quantum system from measured data and plays a crucial role in
evaluating the performance of quantum devices. However, standard estimation
methods become impractical due to the exponential growth of parameters in the
state representation. In this work, we address this challenge by parameterizing
the state using a low-rank block tensor train decomposition and demonstrate
that our approach is both memory- and computationally efficient. This framework
applies to a broad class of quantum states that can be well approximated by
low-rank decompositions, including pure states, nearly pure states, and ground
states of Hamiltonians.

</details>


### [683] [SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks](https://arxiv.org/abs/2506.24081)
*Rahul Kumar,Wenqi Wei,Ying Mao,Junaid Farooq,Ying Wang,Juntao Chen*

Main category: quant-ph

TL;DR: The paper introduces SQUASH, a stealthy quantum circuit-level attack using SWAP gates to sabotage Hybrid Quantum Neural Networks (HQNNs).


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of HQNNs to circuit-level attacks and expose potential risks in quantum machine learning systems.

Method: SQUASH manipulates the variational quantum circuit structure of HQNNs by inserting SWAP gates. This causes qubit misalignments without needing access to training data or detectable input perturbations.

Result: SQUASH reduces classification accuracy by up to 74.08% in untargeted attacks and up to 79.78% for targeted attacks.

Conclusion: The study underscores the critical vulnerability of HQNNs to circuit-level threats and calls for the development of more robust quantum architectures against such adversarial interventions.

Abstract: We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to
sabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks.
SQUASH is executed by inserting SWAP gate(s) into the variational quantum
circuit of the victim HQNN. Unlike conventional noise-based or adversarial
input attacks, SQUASH directly manipulates the circuit structure, leading to
qubit misalignment and disrupting quantum state evolution. This attack is
highly stealthy, as it does not require access to training data or introduce
detectable perturbations in input states. Our results demonstrate that SQUASH
significantly degrades classification performance, with untargeted SWAP attacks
reducing accuracy by up to 74.08\% and targeted SWAP attacks reducing target
class accuracy by up to 79.78\%. These findings reveal a critical vulnerability
in HQNN implementations, underscoring the need for more resilient architectures
against circuit-level adversarial interventions.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [684] [Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling](https://arxiv.org/abs/2504.15071)
*Louis Bradshaw,Simon Colton*

Main category: cs.SD

TL;DR: This paper presents a dataset of over one million MIDI files transcribed from audio recordings using a multi-stage pipeline and statistical metadata analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a substantial resource of transcribed piano audio performances to aid research and applications in music and audio processing.

Method: The paper uses a multi-stage pipeline for data collection, involving language model-based autonomous crawling and scoring, and audio classifier-based pruning and segmentation.

Result: The authors produced a dataset consisting of over one million distinct MIDI files, representing approximately 100,000 hours of transcribed performances.

Conclusion: The research provides a valuable dataset and detailed analysis techniques, with potential for broad applications in audio and music analysis.

Abstract: We introduce an extensive new dataset of MIDI files, created by transcribing
audio recordings of piano performances into their constituent notes. The data
pipeline we use is multi-stage, employing a language model to autonomously
crawl and score audio recordings from the internet based on their metadata,
followed by a stage of pruning and segmentation using an audio classifier. The
resulting dataset contains over one million distinct MIDI files, comprising
roughly 100,000 hours of transcribed audio. We provide an in-depth analysis of
our techniques, offering statistical insights, and investigate the content by
extracting metadata tags, which we also provide. Dataset available at
https://github.com/loubbrad/aria-midi.

</details>


### [685] [You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties](https://arxiv.org/abs/2506.23367)
*Paige Tuttösí,H. Henny Yeung,Yue Wang,Jean-Julien Aucouturier,Angelica Lim*

Main category: cs.SD

TL;DR: The paper introduces Matcha-TTS, a text-to-speech system for second-language speakers that uses vowel duration differences to improve intelligibility.


<details>
  <summary>Details</summary>
Motivation: Provide a TTS system that specifically caters to second-language (L2) English speakers by addressing intelligibility challenges and emotional considerations.

Method: Using American English vowel duration differences (tense vs lax vowels), the system applies a 'clarity mode' to improve transcription accuracy and emotional perception.

Result: French-L1, English-L2 listeners experienced reduced transcription errors (9.15%) and found the clarity mode encouraging and respectful. Whisper-ASR did not offer adequate intelligibility assessment.

Conclusion: Matcha-TTS improves intelligibility for L2 speakers while accounting for emotional and perceptual factors, but further exploration is needed for intelligibility assessment mechanisms.

Abstract: We present the first text-to-speech (TTS) system tailored to second language
(L2) speakers. We use duration differences between American English tense
(longer) and lax (shorter) vowels to create a "clarity mode" for Matcha-TTS.
Our perception studies showed that French-L1, English-L2 listeners had fewer
(at least 9.15%) transcription errors when using our clarity mode, and found it
more encouraging and respectful than overall slowed down speech. Remarkably,
listeners were not aware of these effects: despite the decreased word error
rate in clarity mode, listeners still believed that slowing all target words
was the most intelligible, suggesting that actual intelligibility does not
correlate with perceived intelligibility. Additionally, we found that
Whisper-ASR did not use the same cues as L2 speakers to differentiate difficult
vowels and is not sufficient to assess the intelligibility of TTS systems for
these individuals.

</details>


### [686] [Efficient Interleaved Speech Modeling through Knowledge Distillation](https://arxiv.org/abs/2506.23670)
*Mohammadmahdi Nouriborji,Morteza Rohanian*

Main category: cs.SD

TL;DR: The paper presents TinyWave, a family of compact, expressive speech generation models that compress large multimodal transformers by 3x with minimal performance loss, achieving high accuracy in speech-to-speech and interleaved speech-text tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of deploying oversized speech language models on hardware with size and latency limitations, enabling broader real-time and low-resource applications.

Method: The paper employs layer-aligned distillation, matching hidden states, attention maps, and softened logits to compress large multimodal transformers into smaller, more efficient models, specifically creating the TinyWave family with 2B parameters.

Result: TinyWave demonstrates competitive performance: within 1.4 normalized perplexity points of teacher models on Libri-Light, and 93-97% accuracy of teacher model performance on spoken StoryCloze and SALMon tasks, outperforming similarly sized baselines.

Conclusion: TinyWave offers a scalable solution for compact, expressive speech generation suitable for real-time and low-resource scenarios, with released models and tools supporting reproducible research.

Abstract: Current speech language models exceed the size and latency constraints of
many deployment environments. We build compact, expressive speech generation
models through layer-aligned distillation, matching hidden states, attention
maps, and softened logits to compress large multimodal transformers by 3x with
minimal loss in performance. We introduce TinyWave, a family of 2B-parameter
models for speech-to-speech and interleaved speech-text generation, trained on
50,000 hours of public audio. TinyWave supports (i) speech-only generation
using phonetic or expressive tokens and (ii) mixed speech-text continuations.
Evaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity
points of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%
of the teacher's performance, outperforming size-matched baselines. These
models are optimized for deployment on commodity hardware, enabling
applications in real-time conversational agents, assistive technologies, and
low-resource environments. We release models, training code, and evaluation
scripts to support reproducible research on compact, expressive speech
generation.

</details>


### [687] [WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing](https://arxiv.org/abs/2506.22789)
*Oguzhan Baser,Ahmet Ege Tanriverdi,Kaan Kale,Sandeep P. Chinchali,Sriram Vishwanath*

Main category: cs.SD

TL;DR: This paper introduces WavShape, a framework for speech embeddings that reduces sensitive attribute leakage while maintaining task-relevant information to promote fairness and privacy in speech systems.


<details>
  <summary>Details</summary>
Motivation: Sensitive attributes in speech embeddings can lead to biased model training and privacy risks, prompting a need for optimized representation learning that balances fairness, privacy, and task-relevance.

Method: The authors employ mutual information estimation, specifically utilizing the Donsker-Varadhan formulation, to guide an encoder that filters sensitive information while preserving data important for downstream speech tasks.

Result: WavShape demonstrated a reduction of mutual information between embeddings and sensitive attributes by up to 81%, while preserving 97% of the task-relevant information across three datasets.

Conclusion: This study advances the development of fair, privacy-aware, and efficient speech systems by integrating information theory and self-supervised learning, achieving both fairness and utility in speech representation learning.

Abstract: Speech embeddings often retain sensitive attributes such as speaker identity,
accent, or demographic information, posing risks in biased model training and
privacy leakage. We propose WavShape, an information-theoretic speech
representation learning framework that optimizes embeddings for fairness and
privacy while preserving task-relevant information. We leverage mutual
information (MI) estimation using the Donsker-Varadhan formulation to guide an
MI-based encoder that systematically filters sensitive attributes while
maintaining speech content essential for downstream tasks. Experimental results
on three known datasets show that WavShape reduces MI between embeddings and
sensitive attributes by up to 81% while retaining 97% of task-relevant
information. By integrating information theory with self-supervised speech
models, this work advances the development of fair, privacy-aware, and
resource-efficient speech systems.

</details>


### [688] [TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure](https://arxiv.org/abs/2506.23094)
*Qi He,Gus Xia,Ziyu Wang*

Main category: cs.SD

TL;DR: The paper introduces TOMI, a hierarchical planning method for deep music generation, capable of producing coherent full-song multi-track electronic music.


<details>
  <summary>Details</summary>
Motivation: To explore concept hierarchies in music generation, enabling the structuring of musical ideas across time and space into complete compositions.

Method: A TOMI-based model is developed using an instruction-tuned foundation LLM and represents music with a four-dimensional sparse space involving clips, sections, tracks, and transformations.

Result: The TOMI-based model generates high-quality, structurally coherent multi-track electronic music and supports interactive human-AI co-creation via integration with the REAPER workstation.

Conclusion: The approach improves electronic music generation in terms of quality and structural coherence, showcasing potential for enhanced human-AI music collaboration.

Abstract: Hierarchical planning is a powerful approach to model long sequences
structurally. Aside from considering hierarchies in the temporal structure of
music, this paper explores an even more important aspect: concept hierarchy,
which involves generating music ideas, transforming them, and ultimately
organizing them--across musical time and space--into a complete composition. To
this end, we introduce TOMI (Transforming and Organizing Music Ideas) as a
novel approach in deep music generation and develop a TOMI-based model via
instruction-tuned foundation LLM. Formally, we represent a multi-track
composition process via a sparse, four-dimensional space characterized by clips
(short audio or MIDI segments), sections (temporal positions), tracks
(instrument layers), and transformations (elaboration methods). Our model is
capable of generating multi-track electronic music with full-song structure,
and we further integrate the TOMI-based model with the REAPER digital audio
workstation, enabling interactive human-AI co-creation. Experimental results
demonstrate that our approach produces higher-quality electronic music with
stronger structural coherence compared to baselines.

</details>


### [689] [XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs](https://arxiv.org/abs/2506.23325)
*Yitian Gong,Luozhijie Jin,Ruifan Deng,Dong Zhang,Xin Zhang,Qinyuan Cheng,Zhaoye Fei,Shimin Li,Xipeng Qiu*

Main category: cs.SD

TL;DR: The paper proposes XY-Tokenizer, a novel speech codec that balances semantic richness and acoustic fidelity, addressing limitations in existing codecs.


<details>
  <summary>Details</summary>
Motivation: Existing speech codecs struggle to simultaneously preserve rich semantic information and high-quality audio reconstruction when interfacing speech signals with large language models.

Method: XY-Tokenizer leverages multi-stage, multi-task learning to address the trade-offs between semantic and acoustic capabilities in speech language modeling.

Result: XY-Tokenizer achieves comparable performance to state-of-the-art codecs, excelling in both semantic tasks and acoustic fidelity, with a speaker similarity score of 0.83.

Conclusion: XY-Tokenizer successfully balances semantic and acoustic functions, demonstrating its potential as an improved codec for speech language models, and its implementation is accessible online.

Abstract: Speech codecs serve as bridges between speech signals and large language
models. An ideal codec for speech language models should not only preserve
acoustic information but also capture rich semantic information. However,
existing speech codecs struggle to balance high-quality audio reconstruction
with ease of modeling by language models. In this study, we analyze the
limitations of previous codecs in balancing semantic richness and acoustic
fidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict
between semantic and acoustic capabilities through multi-stage, multi-task
learning. Experimental results demonstrate that XY-Tokenizer achieves
performance in both semantic and acoustic tasks comparable to that of
state-of-the-art codecs operating at similar bitrates, even though those
existing codecs typically excel in only one aspect. Specifically, XY-Tokenizer
achieves strong text alignment, surpassing distillation-based semantic modeling
methods such as SpeechTokenizer and Mimi, while maintaining a speaker
similarity score of 0.83 between reconstructed and original audio. The
reconstruction performance of XY-Tokenizer is comparable to that of BigCodec,
the current state-of-the-art among acoustic-only codecs, which achieves a
speaker similarity score of 0.84 at a similar bitrate. Code and models are
available at https://github.com/gyt1145028706/XY-Tokenizer.

</details>


### [690] [From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection](https://arxiv.org/abs/2506.23437)
*Stefano Giacomelli,Marco Giordano,Claudia Rinaldi,Fabio Graziosi*

Main category: cs.SD

TL;DR: The paper introduces E2PANNs, a lightweight neural network optimized for emergency vehicle siren detection, achieving state-of-the-art performance while being computationally efficient and suitable for embedded hardware.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the limitations of current emergency vehicle (EV) siren recognition systems, such as the absence of large-scale curated datasets and high computational requirements of existing models.

Method: The authors propose E2PANNs, a lightweight neural network derived from the PANNs framework, specifically fine-tuned using a dedicated AudioSet EV dataset and evaluated on multiple reference datasets. The model was tested with ablation studies, cross-domain benchmarking, and real-time deployment on edge hardware.

Result: The results demonstrate that E2PANNs achieve state-of-the-art performance in EV siren detection, with high computational efficiency and real-time viability on embedded devices.

Conclusion: E2PANNs provide an effective solution for binary EV siren detection with strong performance, low computational costs, and applicability for safety-critical smart city and autonomous driving systems.

Abstract: Accurate recognition of Emergency Vehicle (EV) sirens is critical for the
integration of intelligent transportation systems, smart city monitoring
systems, and autonomous driving technologies. Modern automatic solutions are
limited by the lack of large scale, curated datasets and by the computational
demands of state of the art sound event detection models. This work introduces
E2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight
Convolutional Neural Network architecture derived from the PANNs framework,
specifically optimized for binary EV siren detection. Leveraging our dedicated
subset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across
multiple reference datasets and test its viability on embedded hardware. The
experimental campaign includes ablation studies, cross-domain benchmarking, and
real-time inference deployment on edge device. Interpretability analyses
exploiting Guided Backpropagation and ScoreCAM algorithms provide insights into
the model internal representations and validate its ability to capture distinct
spectrotemporal patterns associated with different types of EV sirens. Real
time performance is assessed through frame wise and event based detection
metrics, as well as a detailed analysis of false positive activations. Results
demonstrate that E2PANNs establish a new state of the art in this research
domain, with high computational efficiency, and suitability for edge-based
audio monitoring and safety-critical applications.

</details>


### [691] [Scaling Self-Supervised Representation Learning for Symbolic Piano Performance](https://arxiv.org/abs/2506.23869)
*Louis Bradshaw,Honglu Fan,Alexander Spangher,Stella Biderman,Simon Colton*

Main category: cs.SD

TL;DR: The paper explores generative autoregressive transformer models trained on symbolic piano transcriptions, achieving strong results in music generation, classification, and embedding tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the understanding and application of transformer models in symbolic music tasks, addressing generation coherence, classification, and representational abilities with limited data.

Method: The researchers first pretrained models on a large dataset of 60,000 hours of symbolic music and fine-tuned them on a smaller, high-quality dataset. They adapted the SimCLR framework for contrastive MIDI embeddings and evaluated models on music continuation and benchmark tasks.

Result: The generative model demonstrated superior coherence in music continuation compared to symbolic methods and remained competitive with audio generation models. In classification, contrastive MIDI embeddings achieved state-of-the-art results, requiring minimal labeled data for downstream applications.

Conclusion: Generative transformer models with pretraining on vast datasets, followed by fine-tuning, show promise for symbolic music tasks, including generation and efficient classification with compact labeled datasets.

Abstract: We study the capabilities of generative autoregressive transformer models
trained on large amounts of symbolic solo-piano transcriptions. After first
pretraining on approximately 60,000 hours of music, we use a comparatively
smaller, high-quality subset, to finetune models to produce musical
continuations, perform symbolic classification tasks, and produce
general-purpose contrastive MIDI embeddings by adapting the SimCLR framework to
symbolic music. When evaluating piano continuation coherence, our generative
model outperforms leading symbolic generation techniques and remains
competitive with proprietary audio generation models. On MIR classification
benchmarks, frozen representations from our contrastive model achieve
state-of-the-art results in linear probe experiments, while direct finetuning
demonstrates the generalizability of pretrained representations, often
requiring only a few hundred labeled examples to specialize to downstream
tasks.

</details>


### [692] [Emergent musical properties of a transformer under contrastive self-supervised learning](https://arxiv.org/abs/2506.23873)
*Yuexuan Kong,Gabriel Meseguer-Brocal,Vincent Lostanlen,Mathieu Lagrange,Romain Hennequin*

Main category: cs.SD

TL;DR: The paper explores the surprising effectiveness of contrastive self-supervised learning (SSL) with transformers for local tasks in music information retrieval, like chord estimation, challenging conventional assumptions.


<details>
  <summary>Details</summary>
Motivation: To challenge the widely held belief that contrastive-trained self-supervised models are unsuitable for local music information retrieval tasks and explore their potential capabilities.

Method: The authors use a lightweight vision transformer (ViT-1D) that processes one-dimensional patches in the time-frequency domain, trained using the normalized temperature-scaled cross-entropy loss (NT-Xent), to investigate its performance on MIR tasks.

Result: Sequence tokens in the transformer showed unexpected utility for local tasks, despite not being explicitly trained for them, while global tasks benefitted from combining class and sequence tokens. Emergent features like onsets were observed, highlighting transformative capacities.

Conclusion: The study advances understanding of transformers in MIR, revealing overlooked potentials of contrastive SSL for local and global tasks and shedding light on musical feature emergence through layer analyses.

Abstract: In music information retrieval (MIR), contrastive self-supervised learning
for general-purpose representation models is effective for global tasks such as
automatic tagging. However, for local tasks such as chord estimation, it is
widely assumed that contrastively trained general-purpose self-supervised
models are inadequate and that more sophisticated SSL is necessary; e.g.,
masked modeling. Our paper challenges this assumption by revealing the
potential of contrastive SSL paired with a transformer in local MIR tasks. We
consider a lightweight vision transformer with one-dimensional patches in the
time--frequency domain (ViT-1D) and train it with simple contrastive SSL
through normalized temperature-scaled cross-entropy loss (NT-Xent). Although
NT-Xent operates only over the class token, we observe that, potentially thanks
to weight sharing, informative musical properties emerge in ViT-1D's sequence
tokens. On global tasks, the temporal average of class and sequence tokens
offers a performance increase compared to the class token alone, showing useful
properties in the sequence tokens. On local tasks, sequence tokens perform
unexpectedly well, despite not being specifically trained for. Furthermore,
high-level musical features such as onsets emerge from layer-wise attention
maps and self-similarity matrices show different layers capture different
musical dimensions. Our paper does not focus on improving performance but
advances the musical interpretation of transformers and sheds light on some
overlooked abilities of contrastive SSL paired with transformers for sequence
modeling in MIR.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [693] [Diversity by Design: Addressing Mode Collapse Improves scRNA-seq Perturbation Modeling on Well-Calibrated Metrics](https://arxiv.org/abs/2506.22641)
*Gabriel M. Mejia,Henry E. Miller,Francis J. A. Leblanc,Bo Wang,Brendan Swain,Lucas Paulo de Lima Camillo*

Main category: q-bio.GN

TL;DR: The paper addresses flaws in performance metrics for single-cell perturbation response models, demonstrating how these metrics reward biased predictions over genuine biological changes.


<details>
  <summary>Details</summary>
Motivation: Benchmark evaluations for single-cell perturbation models show unexpected results where predicting the dataset mean outperforms sophisticated approaches, raising concerns about metric bias.

Method: The study analyzes in silico simulations and real-world datasets to demonstrate metric artifacts. They propose DEG-aware metrics and baselines, including WMSE and weighted delta R².

Result: Introducing DEG-aware metrics and loss functions resolved the mode collapse issue while significantly improving the model evaluation process.

Conclusion: The proposed metrics and calibration methods correct biases in model assessments, providing fair evaluation tools and improving model predictions for biological applications.

Abstract: Recent benchmarks reveal that models for single-cell perturbation response
are often outperformed by simply predicting the dataset mean. We trace this
anomaly to a metric artifact: control-referenced deltas and unweighted error
metrics reward mode collapse whenever the control is biased or the biological
signal is sparse. Large-scale \textit{in silico} simulations and analysis of
two real-world perturbation datasets confirm that shared reference shifts, not
genuine biological change, drives high performance in these evaluations. We
introduce differentially expressed gene (DEG)-aware metrics, weighted
mean-squared error (WMSE) and weighted delta $R^{2}$ ($R^{2}_{w}(\Delta)$) with
respect to all perturbations, that measure error in niche signals with high
sensitivity. We further introduce negative and positive performance baselines
to calibrate these metrics. With these improvements, the mean baseline sinks to
null performance while genuine predictors are correctly rewarded. Finally, we
show that using WMSE as a loss function reduces mode collapse and improves
model performance.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [694] [Density, asymmetry and citation dynamics in scientific literature](https://arxiv.org/abs/2506.23366)
*Nathaniel Imel,Zachary Hafen*

Main category: cs.DL

TL;DR: The study explores how the similarity of a scientific paper to prior research, measured through semantic metrics (density and asymmetry), impacts its eventual citation rates. Evidence suggests density carries modest predictive capability, while asymmetry has little impact.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying factors influencing scientific reward, such as citation rates, in the context of balancing novel ideas with established knowledge.

Method: Developed two semantic metrics, density and asymmetry, to represent the similarity of papers in their surrounding literature. Used Bayesian hierarchical regression to analyze citation impact across 53,000 publications spanning multiple disciplines and document embedding techniques.

Result: Density shows small yet meaningful predictive utility for citation rates, while asymmetry offers no improvement in model accuracy. Adding density-based predictors enhances the reliability of existing baseline models.

Conclusion: Semantic density carries actionable insights regarding citation behavior in scientific work, presenting a new avenue to analyze the interplay between research novelty and its recognition in academic communities. Asymmetry adds limited value in this context.

Abstract: Scientific behavior is often characterized by a tension between building upon
established knowledge and introducing novel ideas. Here, we investigate whether
this tension is reflected in the relationship between the similarity of a
scientific paper to previous research and its eventual citation rate. To
operationalize similarity to previous research, we introduce two complementary
metrics to characterize the local geometry of a publication's semantic
neighborhood: (1) \emph{density} ($\rho$), defined as the ratio between a fixed
number of previously-published papers and the minimum distance enclosing those
papers in a semantic embedding space, and (2) asymmetry ($\alpha$), defined as
the average directional difference between a paper and its nearest neighbors.
We tested the predictive relationship between these two metrics and its
subsequent citation rate using a Bayesian hierarchical regression approach,
surveying $\sim 53,000$ publications across nine academic disciplines and five
different document embeddings. While the individual effects of $\rho$ on
citation count are small and variable, incorporating density-based predictors
consistently improves out-of-sample prediction when added to baseline models.
These results suggest that the density of a paper's surrounding scientific
literature may carry modest but informative signals about its eventual impact.
Meanwhile, we find no evidence that publication asymmetry improves model
predictions of citation rates. Our work provides a scalable framework for
linking document embeddings to scientometric outcomes and highlights new
questions regarding the role that semantic similarity plays in shaping the
dynamics of scientific reward.

</details>


### [695] [Persistence Paradox in Dynamic Science](https://arxiv.org/abs/2506.22729)
*Honglin Bao,Kai Li*

Main category: cs.DL

TL;DR: This paper examines how persistence in science, particularly during paradigm shifts like the deep learning revolution, can hinder adaptation and impact, with adaptable scientists achieving greater success.


<details>
  <summary>Details</summary>
Motivation: To challenge the conventional view of persistence as always virtuous in science by exploring its contextual impact, especially during paradigm shifts.

Method: Analyzed 20-year career trajectories of over 5,000 machine learning scientists, focusing on research focus evolution and scientific impact during the deep learning paradigm shift after AlexNet (2012).

Result: Leading research venues shifted to deep learning from traditional methods. Scientists who adapted slowly due to prior success or affiliations faced a rigidity penalty, while those who pivoted strategically experienced higher success and impact.

Conclusion: Scientific breakthroughs, like the deep learning revolution, reconfigure power dynamics in a field. Persistence might hinder progress without strategic adaptation, impacting scientific influence.

Abstract: Persistence is often regarded as a virtue in science. In this paper, however,
we challenge this conventional view by highlighting its contextual nature,
particularly how persistence can become a liability during periods of paradigm
shift. We focus on the deep learning revolution catalyzed by AlexNet in 2012.
Analyzing the 20-year career trajectories of over 5,000 scientists who were
active in top machine learning venues during the preceding decade, we examine
how their research focus and output evolved. We first uncover a dynamic period
in which leading venues increasingly prioritized cutting-edge deep learning
developments that displaced relatively traditional statistical learning
methods. Scientists responded to these changes in markedly different ways.
Those who were previously successful or affiliated with old teams adapted more
slowly, experiencing what we term a rigidity penalty - a reluctance to embrace
new directions leading to a decline in scientific impact, as measured by
citation percentile rank. In contrast, scientists who pursued strategic
adaptation - selectively pivoting toward emerging trends while preserving weak
connections to prior expertise - reaped the greatest benefits. Taken together,
our macro- and micro-level findings show that scientific breakthroughs act as
mechanisms that reconfigure power structures within a field.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [696] [CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding](https://arxiv.org/abs/2506.23075)
*Yuchen Zhou,Jiamin Wu,Zichen Ren,Zhouheng Yao,Weiheng Lu,Kunyu Peng,Qihao Zheng,Chunfeng Song,Wanli Ouyang,Chao Gou*

Main category: cs.HC

TL;DR: The paper introduces CSBrain, a model that significantly improves EEG decoding by incorporating cross-scale spatiotemporal features.


<details>
  <summary>Details</summary>
Motivation: Existing EEG decoding models ignore the multi-scale spatiotemporal structure of brain activity, leading to suboptimal representation and weak generalization.

Method: The authors propose CSBrain, which utilizes Cross-scale Spatiotemporal Tokenization (CST) to aggregate diverse features and Structured Sparse Attention (SSA) to capture dependencies while avoiding noise. CST and SSA are combined in a layered structure for better integration.

Result: CSBrain outperformed task-specific and foundational models across 11 EEG tasks using 16 diverse datasets, proving its superior ability in generalized EEG decoding.

Conclusion: The study highlights the importance of cross-scale spatiotemporal modeling for EEG decoding and positions CSBrain as a seminal foundation model in brain-AI integration.

Abstract: Understanding and decoding brain activity from electroencephalography (EEG)
signals is a fundamental challenge in neuroscience and AI, with applications in
cognition, emotion recognition, diagnosis, and brain-computer interfaces. While
recent EEG foundation models advance generalized decoding via unified
architectures and large-scale pretraining, they adopt a scale-agnostic dense
modeling paradigm inherited from NLP and vision. This design neglects a core
property of neural activity: cross-scale spatiotemporal structure. EEG task
patterns span a wide range of temporal and spatial scales, from short bursts to
slow rhythms, and from localized cortical responses to distributed
interactions. Ignoring this diversity leads to suboptimal representations and
weak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain
foundation model for generalized EEG decoding. CSBrain introduces: (i)
Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale
features from localized temporal windows and anatomical brain regions into
compact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which
captures cross-window and cross-region dependencies, enhancing scale diversity
while removing spurious correlations. CST and SSA are alternately stacked to
progressively integrate multi-scale dependencies. Experiments on 11 EEG tasks
across 16 datasets show that CSBrain consistently outperforms task-specific and
foundation model baselines. These results establish cross-scale modeling as a
key inductive bias and position CSBrain as a robust backbone for future
brain-AI research.

</details>


### [697] [Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics](https://arxiv.org/abs/2506.22520)
*Mustafa Demir,Jacob Miratsky,Jonathan Nguyen,Chun Kit Chan,Punya Mishra,Abhishek Singharoy*

Main category: cs.HC

TL;DR: The paper investigates AI tutor teammates' role in enhancing curiosity-driven engagement and learning effectiveness in molecular visualization tasks, showing promise as adaptive educators.


<details>
  <summary>Details</summary>
Motivation: To explore how AI tutor teammates can stimulate student curiosity and engagement in interactive molecular dynamics learning tasks.

Method: Implemented a Wizard-of-Oz paradigm where a human controlled AI behavior through a language model. Conducted mixed-methods exploratory study involving 11 students performing molecular visualization tasks and analyzed student-AI interactions using CRQA metrics.

Result: High-performing teams demonstrated better task completion, engagement, and deeper understanding. Advanced student questions were linked to AI's curiosity-triggering behaviors, highlighting cognitive complexity.

Conclusion: AI has the potential to act as both an educator and teammate by providing adaptive feedback that sustains epistemic curiosity and engagement, indicating promise for future educational technologies.

Abstract: This study examines the impact of an Artificial Intelligence tutor teammate
(AI) on student curiosity-driven engagement and learning effectiveness during
Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics
platform. It explores the role of the AI's curiosity-triggering and response
behaviors in stimulating and sustaining student curiosity, affecting the
frequency and complexity of student-initiated questions. The study further
assesses how AI interventions shape student engagement, foster discovery
curiosity, and enhance team performance within the IMD learning environment.
Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI
tutor teammate's behavior through a large language model. By employing a
mixed-methods exploratory design, a total of 11 high school students
participated in four IMD tasks that involved molecular visualization and
calculations, which increased in complexity over a 60-minute period. Team
performance was evaluated through real-time observation and recordings, whereas
team communication was measured by question complexity and AI's
curiosity-triggering and response behaviors. Cross Recurrence Quantification
Analysis (CRQA) metrics reflected structural alignment in coordination and were
linked to communication behaviors. High-performing teams exhibited superior
task completion, deeper understanding, and increased engagement. Advanced
questions were associated with AI curiosity-triggering, indicating heightened
engagement and cognitive complexity. CRQA metrics highlighted dynamic
synchronization in student-AI interactions, emphasizing structured yet adaptive
engagement to promote curiosity. These proof-of-concept findings suggest that
the AI's dual role as a teammate and educator indicates its capacity to provide
adaptive feedback, sustaining engagement and epistemic curiosity.

</details>


### [698] [Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions](https://arxiv.org/abs/2506.22941)
*Kaixuan Wang,Jason T. Jacques,Chenxin Diao*

Main category: cs.HC

TL;DR: This paper explores how large language models (LLMs) can responsibly support the information needs of People Who Use Drugs (PWUD) while addressing ethical and operational challenges.


<details>
  <summary>Details</summary>
Motivation: Access to accurate harm reduction information is critical for PWUD, but traditional online platforms fail to address their complex needs due to limitations in adaptability, accessibility, and stigmatization.

Method: The researchers conducted a qualitative workshop with stakeholders, including academics, harm reduction practitioners, and an online community moderator, to evaluate LLMs' capabilities, identify use cases, and delineate design considerations.

Result: The study found LLMs offer potential advantages such as responsive, multilingual, and less stigmatizing information delivery. However, their success depends on addressing ethical concerns, contextual understanding, and operational boundaries.

Conclusion: Collaboration with experts and PWUD in designing LLM systems is essential for ensuring safety, utility, and ethical alignment in their role within the harm reduction ecosystem.

Abstract: Access to accurate and actionable harm reduction information can directly
impact the health outcomes of People Who Use Drugs (PWUD), yet existing online
channels often fail to meet their diverse and dynamic needs due to limitations
in adaptability, accessibility, and the pervasive impact of stigma. Large
Language Models (LLMs) present a novel opportunity to enhance information
provision, but their application in such a high-stakes domain is under-explored
and presents socio-technical challenges. This paper investigates how LLMs can
be responsibly designed to support the information needs of PWUD. Through a
qualitative workshop involving diverse stakeholder groups (academics, harm
reduction practitioners, and an online community moderator), we explored LLM
capabilities, identified potential use cases, and delineated core design
considerations. Our findings reveal that while LLMs can address some existing
information barriers (e.g., by offering responsive, multilingual, and
potentially less stigmatising interactions), their effectiveness is contingent
upon overcoming challenges related to ethical alignment with harm reduction
principles, nuanced contextual understanding, effective communication, and
clearly defined operational boundaries. We articulate design pathways
emphasising collaborative co-design with experts and PWUD to develop LLM
systems that are helpful, safe, and responsibly governed. This work contributes
empirically grounded insights and actionable design considerations for the
responsible development of LLMs as supportive tools within the harm reduction
ecosystem.

</details>


### [699] [Against 'softmaxing' culture](https://arxiv.org/abs/2506.22968)
*Daniel Mwesigwa*

Main category: cs.HC

TL;DR: AI is averaging out linguistic differences, homogenizing culture. This paper critiques current evaluation approaches and proposes conceptual shifts for analyzing AI's cultural impact.


<details>
  <summary>Details</summary>
Motivation: Understanding how large AI models homogenize language and culture to better align systems with cultural nuances.

Method: Conceptual proposal emphasizing shifts in evaluation questions: focusing on 'when is culture?' instead of 'what is culture?', and situating cultural universals relative to specifics.

Result: Proposes a framework to refine evaluation methods, aiming for better responsiveness to the complexities of culture.

Conclusion: The paper advocates for evaluation approaches that prioritize cultural complexity over technical requirements for culturally aligned AI systems.

Abstract: AI is flattening culture. Evaluations of "culture" are showing the myriad
ways in which large AI models are homogenizing language and culture, averaging
out rich linguistic differences into generic expressions. I call this
phenomenon "softmaxing culture," and it is one of the fundamental challenges
facing AI evaluations today. Efforts to improve and strengthen evaluations of
culture are central to the project of cultural alignment in large AI systems.
This position paper argues that machine learning (ML) and human-computer
interaction (HCI) approaches to evaluation are limited. I propose two key
shifts. First, instead of asking "what is culture?" at the start of system
evaluations, I propose beginning with the question: "when is culture?" Second,
while I acknowledge the philosophical claim that cultural universals exist, the
challenge is not simply to describe them, but to situate them in relation to
their particulars. Taken together, these conceptual shifts invite evaluation
approaches that move beyond technical requirements, toward perspectives more
responsive to the complexities of culture.

</details>


### [700] [Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs](https://arxiv.org/abs/2506.23458)
*Xiaoxiao Yang,Chan Feng,Jiancheng Chen*

Main category: cs.HC

TL;DR: The paper introduces MuseCogNet, a framework for improving the accuracy and reliability of portable EEG-based cognitive load detection using joint learning techniques.


<details>
  <summary>Details</summary>
Motivation: The use of portable EEG devices like Muse headbands for brain-computer interface applications faces performance limitations due to low signal fidelity and decoding accuracy caused by non-stationary portable EEG signals.

Method: The authors propose a joint learning framework combining self-supervised EEG reconstruction loss with supervised cross-entropy loss to identify robust neurophysiological and task-specific patterns.

Result: MuseCogNet outperformed existing methods on a public Muse dataset, demonstrating enhanced accuracy and usability for neurocognitive monitoring.

Conclusion: MuseCogNet offers a feasible and effective solution for neurocognitive monitoring in real-world settings by addressing signal fidelity and decoding accuracy using a unified learning approach.

Abstract: Portable and wearable consumer-grade electroencephalography (EEG) devices,
like Muse headbands, offer unprecedented mobility for daily brain-computer
interface (BCI) applications, including cognitive load detection. However, the
exacerbated non-stationarity in portable EEG signals constrains data fidelity
and decoding accuracy, creating a fundamental trade-off between portability and
performance. To mitigate such limitation, we propose MuseCogNet (Muse-based
Cognitive Network), a unified joint learning framework integrating
self-supervised and supervised training paradigms. In particular, we introduce
an EEG-grounded self-supervised reconstruction loss based on average pooling to
capture robust neurophysiological patterns, while cross-entropy loss refines
task-specific cognitive discriminants. This joint learning framework resembles
the bottom-up and top-down attention in humans, enabling MuseCogNet to
significantly outperform state-of-the-art methods on a publicly available Muse
dataset and establish an implementable pathway for neurocognitive monitoring in
ecological settings.

</details>


### [701] [Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2506.23678)
*Rock Yuren Pang,K. J. Kevin Feng,Shangbin Feng,Chu Li,Weijia Shi,Yulia Tsvetkov,Jeffrey Heer,Katharina Reinecke*

Main category: cs.HC

TL;DR: This paper presents Interactive Reasoning, a system for organizing and refining the chain-of-thought content of large language models (LLMs), allowing users to interactively review, modify, and improve their outputs.


<details>
  <summary>Details</summary>
Motivation: LLMs' chain-of-thought reasoning improves output quality but is overly verbose, unstructured, and lacks user interactivity for feedback and customization.

Method: The authors introduced Interactive Reasoning, implemented in the prototype Hippo, to visualize reasoning as a hierarchy of topics and allow interactive user review and modification.

Result: In a user study with 16 participants, Interactive Reasoning helped users detect and address errors, customize the model's outputs, and understand its reasoning better.

Conclusion: Interactive Reasoning enhances LLMs by integrating user oversight, providing a new paradigm for AI-assisted decision making and improving output review and steering.

Abstract: The output quality of large language models (LLMs) can be improved via
"reasoning": generating segments of chain-of-thought (CoT) content to further
condition the model prior to producing user-facing output. While these chains
contain valuable information, they are verbose and lack explicit organization,
making them tedious to review. Moreover, they lack opportunities for user
feedback, such as to remove unwanted considerations, add desired ones, or
clarify unclear assumptions. We introduce Interactive Reasoning, an interaction
design that visualizes chain-of-thought outputs as a hierarchy of topics and
enables user review and modification. We implement interactive reasoning in
Hippo, a prototype for AI-assisted decision making in the face of uncertain
trade-offs. In a user study with 16 participants, we find that interactive
reasoning in Hippo allows users to quickly identify and interrupt erroneous
generations, efficiently steer the model towards customized responses, and
better understand both model reasoning and model outputs. Our work contributes
to a new paradigm that incorporates user oversight into LLM reasoning
processes.

</details>


### [702] [Autonomy by Design: Preserving Human Autonomy in AI Decision-Support](https://arxiv.org/abs/2506.23952)
*Stefan Buijsman,Sarah Carter,Juan Pablo Bermúdez*

Main category: cs.HC

TL;DR: The study explores how AI decision-support systems impact domain-specific autonomy, comprising skilled competence and authentic value-formation. It identifies challenges and suggests a socio-technical framework to preserve autonomy while benefiting from AI.


<details>
  <summary>Details</summary>
Motivation: AI systems are increasingly integrated into decision-making processes in various domains, but their effects on domain-specific autonomy, which is critical for self-governed actions within specific expertise areas, are not well understood.

Method: The paper analyzes prior research and empirical cases from medical, financial, and educational sectors to identify the impacts of AI on skilled competence and authentic value-formation. It proposes design patterns to address these issues.

Result: The absence of reliable failure indicators and unconscious value shifts were found to erode domain-specific autonomy. The proposed solutions include socio-technical patterns like role specification, defeater mechanisms, and reflective support.

Conclusion: AI systems can either enhance or undermine human agency in specialized domains. By adopting the suggested framework, developers can design AI systems that preserve domain-specific autonomy while leveraging AI's abilities.

Abstract: AI systems increasingly support human decision-making across domains of
professional, skill-based, and personal activity. While previous work has
examined how AI might affect human autonomy globally, the effects of AI on
domain-specific autonomy -- the capacity for self-governed action within
defined realms of skill or expertise -- remain understudied. We analyze how AI
decision-support systems affect two key components of domain-specific autonomy:
skilled competence (the ability to make informed judgments within one's domain)
and authentic value-formation (the capacity to form genuine domain-relevant
values and preferences). By engaging with prior investigations and analyzing
empirical cases across medical, financial, and educational domains, we
demonstrate how the absence of reliable failure indicators and the potential
for unconscious value shifts can erode domain-specific autonomy both
immediately and over time. We then develop a constructive framework for
autonomy-preserving AI support systems. We propose specific socio-technical
design patterns -- including careful role specification, implementation of
defeater mechanisms, and support for reflective practice -- that can help
maintain domain-specific autonomy while leveraging AI capabilities. This
framework provides concrete guidance for developing AI systems that enhance
rather than diminish human agency within specialized domains of action.

</details>


### [703] [The Impact of AI on Educational Assessment: A Framework for Constructive Alignment](https://arxiv.org/abs/2506.23815)
*Patrick Stokkink*

Main category: cs.HC

TL;DR: Artificial Intelligence tools, particularly Large Language Models, significantly impact education and assessments, requiring adaptation in alignment with Bloom's Taxonomy and Constructive Alignment theory.


<details>
  <summary>Details</summary>
Motivation: The increasing usage of AI tools like LLM in education raises concerns about the validity of current assessment methods to evaluate student performance effectively.

Method: A theoretical framework grounded in Constructive Alignment theory and Bloom's taxonomy is developed to analyze AI's specific impacts on learning objectives and assessment methods.

Result: AI affects different Bloom levels of learning objectives distinctively, suggesting that assessments should be adjusted accordingly and standardized guidelines developed to reduce lecturer bias.

Conclusion: Adapting educational assessments to integrate AI responsibly entails structured guidelines and training lecturers about AI's capabilities and limitations. This ensures consistency across faculties in assessment methods.

Abstract: The influence of Artificial Intelligence (AI), and specifically Large
Language Models (LLM), on education is continuously increasing. These models
are frequently used by students, giving rise to the question whether current
forms of assessment are still a valid way to evaluate student performance and
comprehension. The theoretical framework developed in this paper is grounded in
Constructive Alignment (CA) theory and Bloom's taxonomy for defining learning
objectives. We argue that AI influences learning objectives of different Bloom
levels in a different way, and assessment has to be adopted accordingly.
Furthermore, in line with Bloom's vision, formative and summative assessment
should be aligned on whether the use of AI is permitted or not.
  Although lecturers tend to agree that education and assessment need to be
adapted to the presence of AI, a strong bias exists on the extent to which
lecturers want to allow for AI in assessment. This bias is caused by a
lecturer's familiarity with AI and specifically whether they use it themselves.
To avoid this bias, we propose structured guidelines on a university or faculty
level, to foster alignment among the staff. Besides that, we argue that
teaching staff should be trained on the capabilities and limitations of AI
tools. In this way, they are better able to adapt their assessment methods.

</details>


### [704] [Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks](https://arxiv.org/abs/2506.23016)
*Tomás Silva Santos Rocha,Anastasiia Mikhailova,Moreno I. Coco,José Santos-Victor*

Main category: cs.HC

TL;DR: The study explores using eye-tracking data and deep learning to differentiate Mild Cognitive Impairment (MCI) from Healthy Controls (HC).


<details>
  <summary>Details</summary>
Motivation: The global rise in dementia highlights the need for scalable and automated tools to diagnose conditions like Mild Cognitive Impairment (MCI).

Method: Participants performed a visual memory task while eye-tracking data was collected. A VTNet-based deep learning model analyzed time series and spatial data, incorporating scan paths and heatmaps.

Result: The model achieved 68% sensitivity and 76% specificity using $700×700$ resolution heatmaps, comparable to prior Alzheimer's studies.

Conclusion: This approach shows promise as a diagnostic tool for MCI but requires refinement and testing with standardized, long-term tasks.

Abstract: The global prevalence of dementia is projected to double by 2050,
highlighting the urgent need for scalable diagnostic tools. This study utilizes
digital cognitive tasks with eye-tracking data correlated with memory processes
to distinguish between Healthy Controls (HC) and Mild Cognitive Impairment
(MCI), a precursor to dementia. A deep learning model based on VTNet was
trained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who
performed a visual memory task. The model utilizes both time series and spatial
data derived from eye-tracking. It was modified to incorporate scan paths, heat
maps, and image content. These modifications also enabled testing parameters
such as image resolution and task performance, analyzing their impact on model
performance. The best model, utilizing $700\times700px$ resolution heatmaps,
achieved 68% sensitivity and 76% specificity. Despite operating under more
challenging conditions (e.g., smaller dataset size, shorter task duration, or a
less standardized task), the model's performance is comparable to an
Alzheimer's study using similar methods (70% sensitivity and 73% specificity).
These findings contribute to the development of automated diagnostic tools for
MCI. Future work should focus on refining the model and using a standardized
long-term visual memory task.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [705] [Minimax and Bayes Optimal Best-arm Identification: Adaptive Experimental Design for Treatment Choice](https://arxiv.org/abs/2506.24007)
*Masahiro Kato*

Main category: econ.EM

TL;DR: This paper introduces an adaptive experimental design for selecting the best treatment arm efficiently, utilizing a two-stage treatment-allocation process followed by a treatment-choice phase, achieving optimal performance for minimizing regret.


<details>
  <summary>Details</summary>
Motivation: The study addresses the need for efficient methods to identify the best treatment arm in adaptive experimental settings, where clear suboptimal arms can be eliminated early.

Method: The method consists of a two-stage adaptive treatment-allocation process: a uniform pilot stage to eliminate suboptimal arms and estimate variances, followed by proportional allocation in the second stage and a treatment-choice phase selecting the arm with the highest sample mean.

Result: The designed experiment achieves asymptotically optimal performance for both minimax and Bayesian objectives in terms of simple regret, with theoretical upper bounds matching lower bounds up to exact constants.

Conclusion: This experiment achieves sharp efficiency limits and does not require separate tuning for minimax and Bayesian objectives, making it highly effective for treatment arm identification.

Abstract: This study investigates adaptive experimental design for treatment choice,
also known as fixed-budget best-arm identification. We consider an adaptive
procedure consisting of a treatment-allocation phase followed by a
treatment-choice phase, and we design an adaptive experiment for this setup to
efficiently identify the best treatment arm, defined as the one with the
highest expected outcome. In our designed experiment, the treatment-allocation
phase consists of two stages. The first stage is a pilot phase, where we
allocate each treatment arm uniformly with equal proportions to eliminate
clearly suboptimal arms and estimate outcome variances. In the second stage, we
allocate treatment arms in proportion to the variances estimated in the first
stage. After the treatment-allocation phase, the procedure enters the
treatment-choice phase, where we choose the treatment arm with the highest
sample mean as our estimate of the best treatment arm. We prove that this
single design is simultaneously asymptotically minimax and Bayes optimal for
the simple regret, with upper bounds that match our lower bounds up to exact
constants. Therefore, our designed experiment achieves the sharp efficiency
limits without requiring separate tuning for minimax and Bayesian objectives.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [706] [Evaluating and Improving Large Language Models for Competitive Program Generation](https://arxiv.org/abs/2506.22954)
*Minnan Wei,Ziming Li,Xiang Chen,Menglin Zheng,Ziyan Qu,Cheng Yu,Siyu Chen,Xiaolin Ju*

Main category: cs.SI

TL;DR: The study evaluates and enhances large language models (LLMs) for competitive programming problems, utilizing a curated benchmark and error-correction strategies for improved problem-solving success.


<details>
  <summary>Details</summary>
Motivation: Previous LLM evaluation for competitive programming used simplistic prompts and datasets vulnerable to data leakage, with limited focus on algorithm variety and difficulty.

Method: Researchers collected 117 real-world competitive programming problems from ICPC/CCPC contests, filtered them to 80, and tested LLM DeepSeek-R1 with basic prompts via OJ platforms. They then applied a two-phase improvement approach: multi-turn error repair and information-augmented regeneration.

Result: With basic prompts, only 5 problems were solved correctly. Error categorization identified general and specialized errors. By applying improvement strategies, the acceptance rate increased to 46 out of 80 problems.

Conclusion: The study demonstrates that targeted improvement frameworks, including dialogue-based repair and regeneration phases, significantly enhance LLM performance in solving competitive programming challenges.

Abstract: Context: Due to the demand for strong algorithmic reasoning, complex logic
implementation, and strict adherence to input/output formats and resource
constraints, competitive programming generation by large language models (LLMs)
is considered the most challenging problem in current LLM-based code
generation. However, previous studies often evaluate LLMs using simple prompts
and benchmark datasets prone to data leakage. Moreover, prior work has limited
consideration of the diversity in algorithm types and difficulty levels.
Objective: In this study, we aim to evaluate and improve LLMs in solving
real-world competitive programming problems. Methods: We initially collect 117
problems from nine regional ICPC/CCPC contests held in 2024 and design four
filtering criteria to construct a curated benchmark consisting of 80 problems.
Leveraging DeepSeek-R1 as the LLM, we evaluate its competitive program
generation capabilities through the online judge (OJ) platforms, guided by a
carefully designed basic prompt. For incorrect submissions, we construct a
fine-grained error taxonomy and then propose a targeted improvement framework
by combining a multi-turn dialogue-based repair phase and an
information-augmented regeneration phase. Results: Experimental results show
that only 5 out of 80 problems are fully accepted when using basic prompts. For
the unsolved problems, we construct the error taxonomy, including general
errors (such as design, boundary, condition, data type, syntax, and
input/output errors) and specialized errors (such as those in mathematical
problems, greedy algorithms, and graph theories). After applying our proposed
improvement strategies, we substantially increased the number of correct
solutions, with 46 out of 80 problems successfully accepted.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [707] [VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding](https://arxiv.org/abs/2506.22799)
*Minchao Jiang,Shunyu Jia,Jiaming Gu,Xiaoyuan Lu,Guangming Zhu,Anqi Dong,Liang Zhang*

Main category: cs.GR

TL;DR: The paper proposes VoteSplat, a novel 3D scene understanding framework that combines 3D Gaussian Splatting and Hough voting for improving tasks such as object localization and semantic mapping in 3D scenes.


<details>
  <summary>Details</summary>
Motivation: Existing 3D Gaussian Splatting methods focus on rendering quality and efficiency but lack scene understanding capabilities and come with high training costs. The authors aim to address these limitations to enable more comprehensive 3D scene understanding.

Method: The framework integrates the Segment Anything Model (SAM) for extracting objects and generating 2D vote maps, embeds spatial offsets into Gaussian primitives for constructing 3D spatial votes, and maps 2D image semantics to 3D point clouds. Depth distortion constraints are added to refine the depth localization.

Result: VoteSplat demonstrates its effectiveness through experiments on open-vocabulary 3D instance localization, 3D point cloud understanding, click-based 3D object localization, hierarchical segmentation, and detailed ablation studies.

Conclusion: VoteSplat enhances 3D scene understanding by making it both efficient and semantically rich while reducing computational costs. Its open-vocabulary capability and focus on instance segmentation highlight its versatility.

Abstract: 3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time
rendering for novel view synthesis of 3D scenes. However, existing methods
focus primarily on geometric and appearance modeling, lacking deeper scene
understanding while also incurring high training costs that complicate the
originally streamlined differentiable rendering pipeline. To this end, we
propose VoteSplat, a novel 3D scene understanding framework that integrates
Hough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized
for instance segmentation, extracting objects, and generating 2D vote maps. We
then embed spatial offset vectors into Gaussian primitives. These offsets
construct 3D spatial votes by associating them with 2D image votes, while depth
distortion constraints refine localization along the depth axis. For
open-vocabulary object localization, VoteSplat maps 2D image semantics to 3D
point clouds via voting points, reducing training costs associated with
high-dimensional CLIP features while preserving semantic unambiguity. Extensive
experiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D
instance localization, 3D point cloud understanding, click-based 3D object
localization, hierarchical segmentation, and ablation studies. Our code is
available at https://sy-ja.github.io/votesplat/

</details>


### [708] [ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes](https://arxiv.org/abs/2506.21629)
*Chenhao Zhang,Yezhi Shen,Fengqing Zhu*

Main category: cs.GR

TL;DR: The paper introduces ICP-3DGS, a method enhancing camera pose estimation and scene reconstruction for neural rendering without requiring prior 3D structural information.


<details>
  <summary>Details</summary>
Motivation: Neural rendering methods like NeRFs and 3D Gaussian Splatting struggle in outdoor scenarios due to dependence on preprocessed camera poses and 3D priors.

Method: The paper integrates Iterative Closest Point (ICP) with optimization-based refinement for accurate camera pose estimation and uses voxel-based scene densification for large-scale scene guidance.

Result: The proposed ICP-3DGS method outperforms existing approaches in camera pose estimation and novel view synthesis for both indoor and outdoor scenes of different scales.

Conclusion: ICP-3DGS overcomes limitations in existing neural rendering techniques by providing a more effective solution for scene reconstruction and novel view synthesis in challenging scenarios.

Abstract: In recent years, neural rendering methods such as NeRFs and 3D Gaussian
Splatting (3DGS) have made significant progress in scene reconstruction and
novel view synthesis. However, they heavily rely on preprocessed camera poses
and 3D structural priors from structure-from-motion (SfM), which are
challenging to obtain in outdoor scenarios. To address this challenge, we
propose to incorporate Iterative Closest Point (ICP) with optimization-based
refinement to achieve accurate camera pose estimation under large camera
movements. Additionally, we introduce a voxel-based scene densification
approach to guide the reconstruction in large-scale scenes. Experiments
demonstrate that our approach ICP-3DGS outperforms existing methods in both
camera pose estimation and novel view synthesis across indoor and outdoor
scenes of various scales. Source code is available at
https://github.com/Chenhao-Z/ICP-3DGS.

</details>


### [709] [Navigating with Annealing Guidance Scale in Diffusion Space](https://arxiv.org/abs/2506.24108)
*Shai Yehezkel,Omer Dahary,Andrey Voynov,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: The paper introduces an annealing guidance scheduler that dynamically adjusts the guidance scale in denoising diffusion models, enhancing image quality and text alignment without added computational cost.


<details>
  <summary>Details</summary>
Motivation: To improve the balancing mechanism (guidance scale) in Classifier-Free Guidance for better convergence in generating high-quality and text-aligned images via diffusion models.

Method: The annealing guidance scheduler adjust guidance scale dynamically during sampling, leveraging the conditional noisy signal and learning a scheduling policy.

Result: The proposed scheduler enhances both image quality and adherence to text prompts, outperforming standard Classifier-Free Guidance in text-to-image generation while maintaining computational efficiency.

Conclusion: The annealing guidance scheduler provides a better trade-off between prompt alignment and image quality with no added memory or activation cost, making it a practical replacement for current CFG methods.

Abstract: Denoising diffusion models excel at generating high-quality images
conditioned on text prompts, yet their effectiveness heavily relies on careful
guidance during the sampling process. Classifier-Free Guidance (CFG) provides a
widely used mechanism for steering generation by setting the guidance scale,
which balances image quality and prompt alignment. However, the choice of the
guidance scale has a critical impact on the convergence toward a visually
appealing and prompt-adherent image. In this work, we propose an annealing
guidance scheduler which dynamically adjusts the guidance scale over time based
on the conditional noisy signal. By learning a scheduling policy, our method
addresses the temperamental behavior of CFG. Empirical results demonstrate that
our guidance scheduler significantly enhances image quality and alignment with
the text prompt, advancing the performance of text-to-image generation.
Notably, our novel scheduler requires no additional activations or memory
consumption, and can seamlessly replace the common classifier-free guidance,
offering an improved trade-off between prompt alignment and quality.

</details>


### [710] [Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions](https://arxiv.org/abs/2506.22973)
*AmirHossein Naghi Razlighi,Elaheh Badali Golezani,Shohreh Kasaei*

Main category: cs.GR

TL;DR: The paper introduces a lossy compression method for 3D Gaussian Splatting using learnable confidence scores, effectively reducing computational costs while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: To address the high storage and computational overhead caused by millions of splats in 3D Gaussian Splatting.

Method: The authors propose using learnable confidence scores modeled as Beta distributions and reconstruction-aware losses to optimize and prune low-confidence splats.

Result: The method achieves favorable trade-offs between compression and fidelity compared to previous frameworks, and introduces a confidence-based metric for scene quality assessment.

Conclusion: The proposed method significantly reduces the overhead of 3D Gaussian Splatting without compromising visual fidelity, and it can be integrated with any variant of Gaussian Splatting.

Abstract: 3D Gaussian Splatting enables high-quality real-time rendering but often
produces millions of splats, resulting in excessive storage and computational
overhead. We propose a novel lossy compression method based on learnable
confidence scores modeled as Beta distributions. Each splat's confidence is
optimized through reconstruction-aware losses, enabling pruning of
low-confidence splats while preserving visual fidelity. The proposed approach
is architecture-agnostic and can be applied to any Gaussian Splatting variant.
In addition, the average confidence values serve as a new metric to assess the
quality of the scene. Extensive experiments demonstrate favorable trade-offs
between compression and fidelity compared to prior work. Our code and data are
publicly available at
https://github.com/amirhossein-razlighi/Confident-Splatting

</details>


### [711] [GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering](https://arxiv.org/abs/2506.23957)
*Zinuo You,Stamatios Georgoulis,Anpei Chen,Siyu Tang,Dengxin Dai*

Main category: cs.GR

TL;DR: The paper introduces GaVS, a novel approach to video stabilization leveraging 3D-grounded reconstruction and rendering to address issues like geometric distortions and poor generalization in prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing video stabilization techniques often suffer from challenges such as geometric distortions, excessive cropping, and poor generalization, which degrade user experience.

Method: GaVS reimagines video stabilization as a temporally-consistent 'local reconstruction and rendering' problem using 3D camera pose data, Gaussian Splatting primitives, photometric supervision, and scene extrapolation to stabilize video frames without excessive cropping.

Result: The method is proven competitive or superior to state-of-the-art 2D and 2.5D stabilization methods in task metrics and geometric consistency, with significant qualitative improvements validated by a user study.

Conclusion: GaVS effectively addresses prevalent issues in video stabilization, notably outperforming existing methods in both quantitative metrics and qualitative evaluations, offering a robust, geometry-consistent solution.

Abstract: Video stabilization is pivotal for video processing, as it removes unwanted
shakiness while preserving the original user motion intent. Existing
approaches, depending on the domain they operate, suffer from several issues
(e.g. geometric distortions, excessive cropping, poor generalization) that
degrade the user experience. To address these issues, we introduce
\textbf{GaVS}, a novel 3D-grounded approach that reformulates video
stabilization as a temporally-consistent `local reconstruction and rendering'
paradigm. Given 3D camera poses, we augment a reconstruction model to predict
Gaussian Splatting primitives, and finetune it at test-time, with multi-view
dynamics-aware photometric supervision and cross-frame regularization, to
produce temporally-consistent local reconstructions. The model are then used to
render each stabilized frame. We utilize a scene extrapolation module to avoid
frame cropping. Our method is evaluated on a repurposed dataset, instilled with
3D-grounded information, covering samples with diverse camera motions and scene
dynamics. Quantitatively, our method is competitive with or superior to
state-of-the-art 2D and 2.5D approaches in terms of conventional task metrics
and new geometry consistency. Qualitatively, our method produces noticeably
better results compared to alternatives, validated by the user study.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [712] [A Correlation-Based Design of RIS for Reduced Power Consumption and Simplified Control Circuitry](https://arxiv.org/abs/2506.22702)
*Zina Mohamed,Ammar B. Kouki,Sonia Aïssa*

Main category: eess.SY

TL;DR: The paper introduces Connected-RIS, a novel design for reconfigurable intelligent surfaces (RIS) that simplifies hardware and reduces energy consumption by leveraging phase shift correlation between surface elements.


<details>
  <summary>Details</summary>
Motivation: To reduce hardware complexity and energy consumption in RIS-based wireless communication systems.

Method: Conducted correlation analysis of phase shift values based on azimuth angles within a coverage of $-80^{\circ}$ to $80^{\circ}$. Correlated elements share control signals and are used to develop Connected-RIS structure.

Result: Connected-RIS achieved significant hardware simplification, reducing load impedances and control signals by 83-98%, and power consumption by 86-92%, compared to fully controlled RIS structures.

Conclusion: Connected-RIS enables effective passive beamforming while lowering hardware costs and achieving substantial energy savings, without compromising radio coverage gain.

Abstract: Aiming at simplifying the hardware structure and reducing the energy
consumption in wireless communication via reconfigurable intelligent surfaces
(RIS), this paper introduces a novel RIS design founded on the correlation
between the phase shift values of the surface elements. First, a correlation
analysis is conducted, considering the azimuth angle of a target device within
a coverage region spanning from $-80^{\circ}$ to $80^{\circ}$. The correlation
is demonstrated for different deployment cases, creating the basis for the new
RIS structure, termed Connected-RIS, where correlated elements are designed to
share the same control signal. The fundamental performance of the proposed
design is then analyzed in terms of control signals, power consumption, and
communication system performance, comparing it to two RIS structures with full
control: one with the same size as the proposed design, and the other employing
the minimum number of elements necessary to satisfy the fair coverage
criterion. The correlation-based RIS design enables three-dimensional passive
beamforming and significantly reduces the number of required load impedances
and control signals, thereby lowering the hardware cost and simplifying the
control circuitry. It also achieves substantial power savings as compared to
the baseline schemes, while maintaining sufficient gain for a fair radio
coverage. For instance, numerical simulations demonstrate that the proposed
design reduces the power consumption by almost 86-92\% and the control signals
by 83-98\% compared to operation with fully controlled RIS.

</details>


### [713] [Momentum-based Accelerated Algorithm for Distributed Optimization under Sector-Bound Nonlinearity](https://arxiv.org/abs/2506.22855)
*Mohammadreza Doostmohammadian,Hamid R. Rabiee*

Main category: eess.SY

TL;DR: The paper introduces an accelerated consensus-based distributed algorithm for non-convex optimization that improves convergence using momentum, addresses sector-bound nonlinearities, and adapts to dynamic network setups.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of centralized methods and improve decentralized learning efficiency and robustness in the presence of nonlinearities and dynamic network conditions.

Method: The authors employ the heavy-ball method for momentum, gradient-tracking techniques, perturbation theory, and eigen-spectrum analysis.

Result: They prove convergence despite sector-bound nonlinearities and non-convexity, while enabling applicability to time-varying, directed networks.

Conclusion: The paper provides an effective framework for distributed optimization in dynamic environments with nonlinear constraints, ensuring improved convergence and robustness.

Abstract: Distributed optimization advances centralized machine learning methods by
enabling parallel and decentralized learning processes over a network of
computing nodes. This work provides an accelerated consensus-based distributed
algorithm for locally non-convex optimization using the gradient-tracking
technique. The proposed algorithm (i) improves the convergence rate by adding
momentum towards the optimal state using the heavy-ball method, while (ii)
addressing general sector-bound nonlinearities over the information-sharing
network. The link nonlinearity includes any sign-preserving odd sector-bound
mapping, for example, log-scale data quantization or clipping in practical
applications. For admissible momentum and gradient-tracking parameters, using
perturbation theory and eigen-spectrum analysis, we prove convergence even in
the presence of sector-bound nonlinearity and for locally non-convex cost
functions. Further, in contrast to most existing weight-stochastic algorithms,
we adopt weight-balanced (WB) network design. This WB design and
perturbation-based analysis allow to handle dynamic directed network of agents
to address possible time-varying setups due to link failures or packet drops.

</details>


### [714] [Hierarchical Decentralized Stochastic Control for Cyber-Physical Systems](https://arxiv.org/abs/2506.22971)
*Kesav Kazam Ramachandran Anantharaman,Rahul Meshram*

Main category: eess.SY

TL;DR: The paper introduces a hierarchical decentralized control architecture for Cyber-Physical Systems with global and local controllers based on Markov Decision Processes (MDPs). It explores two optimization frameworks for local controllers, compares their performance, and establishes optimal policy conditions.


<details>
  <summary>Details</summary>
Motivation: To improve control strategies for Cyber-Physical Systems with decentralized architectures, considering both global constraints and local autonomy.

Method: Proposes a two-timescale hierarchical setup with a global controller optimizing over infinite horizons and local controllers using two frameworks, COpt (infinite-horizon MDP) and FOpt (finite-horizon MDP). Analyses policies, value functions, and interrelationships between the frameworks.

Result: Establishes the existence of optimal policies for both frameworks, provides bounds on value function differences, and identifies conditions under which the two frameworks achieve the same optimal values.

Conclusion: The proposed architectures with two optimization frameworks offer a versatile control strategy, balancing global optimization and local autonomy, thus enhancing systematic decision-making in Cyber-Physical Systems.

Abstract: This paper presents a two-timescale hierarchical decentralized architecture
for control of Cyber-Physical Systems. The architecture consists of $N$
independent sub-processes, a global controller, and $N$ local controllers, each
formulated as a Markov Decision Process (MDP). The global controller, operating
at a slower timescale optimizes the infinite-horizon discounted cumulative
reward under budget constraints. For the local controllers, operating at a
faster timescale, we propose two different optimization frameworks, namely the
COpt and FOpt. In the COpt framework, the local controller also optimizes an
infinite-horizon MDP, while in the FOpt framework, the local controller
optimizes a finite-horizon MDP. The FOpt framework mimics a federal structure,
where the local controllers have more autonomy in their decision making. First,
the existence of stationary deterministic optimal policies for both these
frameworks is established. Then, various relationships between the two
frameworks are studied, including a bound on the difference between the two
optimal value functions. Additionally, sufficiency conditions are provided such
that the two frameworks lead to the same optimal values.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [715] [Teaching a Language Model to Speak the Language of Tools](https://arxiv.org/abs/2506.23394)
*Simeon Emanuilov*

Main category: cs.IR

TL;DR: This paper adapts language models to robustly use tools in non-English languages, particularly Bulgarian, by training on function-calling examples, improving accuracy and response quality.


<details>
  <summary>Details</summary>
Motivation: Most multilingual models fail to reliably use external tools in non-English languages, often showing issues like language confusion and incorrect function-call structures.

Method: The authors trained the BgGPT model on a bilingual dataset of 10,035 function-calling examples and introduced TUCAN to support accurate and standardized tool use.

Result: TUCAN improved function-calling accuracy by up to 28.75% compared to base models, while maintaining strong language understanding.

Conclusion: The study provides a replicable methodology to enable robust tool-integration beyond English-centric language systems and publishes resources for other languages.

Abstract: External tool integration through function-calling is essential for practical
language model applications, yet most multilingual models lack reliable
tool-use capabilities in non-English languages. Even state-of-the-art
multilingual models struggle with determining when to use tools and generating
the structured outputs required for function calls, often exhibiting language
confusion when prompted in lower-resource languages. This work presents a
methodology for adapting existing language models to enable robust tool use in
any target language, using Bulgarian as a case study. The approach involves
continued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a
novel bilingual dataset of 10,035 function-calling examples designed to support
standardized protocols like MCP (Model Context Protocol). The research
introduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to
28.75% improvement in function-calling accuracy over base models while
preserving core language understanding, as verified on established Bulgarian
benchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready
response formatting with clean, parsable function calls, contrasting with the
verbose and inconsistent outputs of base models. The models, evaluation
framework, and dataset are released to enable replication for other languages.
This work demonstrates a practical approach for extending tool-augmented
capabilities beyond English-centric systems.

</details>


### [716] [Interact2Vec -- An efficient neural network-based model for simultaneously learning users and items embeddings in recommender systems](https://arxiv.org/abs/2506.22648)
*Pedro R. Pires,Tiago A. Almeida*

Main category: cs.IR

TL;DR: Interact2Vec is a neural network-based model for recommender systems that creates user and item embeddings using only implicit feedback.


<details>
  <summary>Details</summary>
Motivation: Recommender systems face challenges such as high data dimensionality and sparseness, with existing solutions requiring complex architectures or unavailable content data.

Method: The model adopts NLP-inspired optimization strategies for embedding generation and evaluates its performance in both extrinsic recommendation tasks and intrinsic embedding quality.

Result: Interact2Vec was competitive in 30% of tested datasets while reducing training time by 274% compared to other embedding-based models, and demonstrated strong intrinsic quality in similarity analysis.

Conclusion: Interact2Vec is an efficient and competitive approach for generating embeddings, particularly advantageous in low-resource scenarios.

Abstract: Over the past decade, recommender systems have experienced a surge in
popularity. Despite notable progress, they grapple with challenging issues,
such as high data dimensionality and sparseness. Representing users and items
as low-dimensional embeddings learned via neural networks has become a leading
solution. However, while recent studies show promising results, many approaches
rely on complex architectures or require content data, which may not always be
available. This paper presents Interact2Vec, a novel neural network-based model
that simultaneously learns distributed embeddings for users and items while
demanding only implicit feedback. The model employs state-of-the-art strategies
that natural language processing models commonly use to optimize the training
phase and enhance the final embeddings. Two types of experiments were conducted
regarding the extrinsic and intrinsic quality of the model. In the former, we
benchmarked the recommendations generated by Interact2Vec's embeddings in a
top-$N$ ranking problem, comparing them with six other recommender algorithms.
The model achieved the second or third-best results in 30\% of the datasets,
being competitive with other recommenders, and has proven to be very efficient
with an average training time reduction of 274\% compared to other
embedding-based models. Later, we analyzed the intrinsic quality of the
embeddings through similarity tables. Our findings suggest that Interact2Vec
can achieve promising results, especially on the extrinsic task, and is an
excellent embedding-generator model for scenarios of scarce computing
resources, enabling the learning of item and user embeddings simultaneously and
efficiently.

</details>


### [717] [Enhancing Live Broadcast Engagement: A Multi-modal Approach to Short Video Recommendations Using MMGCN and User Preferences](https://arxiv.org/abs/2506.23085)
*Saeid Aghasoleymani Najafabadi*

Main category: cs.IR

TL;DR: The paper presents a novel multi-modal recommendation system using MMGCN to improve live broadcast engagement. It integrates user preferences, video content, and contextual data, showing superior performance over baseline models in several datasets.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current recommendation systems and enhance content discovery and user engagement for live broadcasts.

Method: The paper employs Multi-modal Graph Convolutional Networks (MMGCN) with a hybrid of collaborative and content-based filtering techniques. It uses user interaction data, video content features, and contextual information for personalized recommendations.

Result: The proposed MMGCN-based model outperformed baseline models like DeepFM, LightGBM, and XGBoost in terms of F1 scores across Kwai (0.574), TikTok (0.506), and MovieLens (0.197).

Conclusion: The study highlights the effectiveness of incorporating multi-modal integration and user-centric approaches for building advanced recommender systems that enhance audience engagement on live broadcast platforms.

Abstract: The purpose of this paper is to explore a multi-modal approach to enhancing
live broadcast engagement by developing a short video recommendation system
that incorporates Multi-modal Graph Convolutional Networks (MMGCN) with user
preferences. In order to provide personalized recommendations tailored to
individual interests, the proposed system takes into account user interaction
data, video content features, and contextual information. With the aid of a
hybrid approach combining collaborative filtering and content-based filtering
techniques, the system is able to capture nuanced relationships between users,
video attributes, and engagement patterns. Three datasets are used to evaluate
the effectiveness of the system: Kwai, TikTok, and MovieLens. Compared to
baseline models, such as DeepFM, Wide & Deep, LightGBM, and XGBoost, the
proposed MMGCN-based model shows superior performance. A notable feature of the
proposed model is that it outperforms all baseline methods in capturing diverse
user preferences and making accurate, personalized recommendations, resulting
in a Kwai F1 score of 0.574, a Tiktok F1 score of 0.506, and a MovieLens F1
score of 0.197. We emphasize the importance of multi-modal integration and
user-centric approaches in advancing recommender systems, emphasizing the role
they play in enhancing content discovery and audience interaction on live
broadcast platforms.

</details>


### [718] [Multi-task Offline Reinforcement Learning for Online Advertising in Recommender Systems](https://arxiv.org/abs/2506.23090)
*Langming Liu,Wanyu Wang,Chi Zhang,Bo Li,Hongzhi Yin,Xuetao Wei,Wenbo Su,Bo Zheng,Xiangyu Zhao*

Main category: cs.IR

TL;DR: The paper introduces MTORL, a multi-task offline RL model to improve online advertising by solving challenges like overestimation and distributional shifts.


<details>
  <summary>Details</summary>
Motivation: Current offline reinforcement learning methods struggle with challenges such as sparse advertising scenarios, overestimation, and budget constraints.

Method: The authors propose a Markov Decision Process framework and develop a causal state encoder with causal attention mechanisms. They employ multi-task learning for both channel recommendation and budget allocation.

Result: MTORL outperforms state-of-the-art methods in experiments conducted on both offline and online environments.

Conclusion: MTORL effectively addresses key challenges in online advertising, providing a robust model for channel recommendation and budget allocation in sparse scenarios.

Abstract: Online advertising in recommendation platforms has gained significant
attention, with a predominant focus on channel recommendation and budget
allocation strategies. However, current offline reinforcement learning (RL)
methods face substantial challenges when applied to sparse advertising
scenarios, primarily due to severe overestimation, distributional shifts, and
overlooking budget constraints. To address these issues, we propose MTORL, a
novel multi-task offline RL model that targets two key objectives. First, we
establish a Markov Decision Process (MDP) framework specific to the nuances of
advertising. Then, we develop a causal state encoder to capture dynamic user
interests and temporal dependencies, facilitating offline RL through
conditional sequence modeling. Causal attention mechanisms are introduced to
enhance user sequence representations by identifying correlations among causal
states. We employ multi-task learning to decode actions and rewards,
simultaneously addressing channel recommendation and budget allocation.
Notably, our framework includes an automated system for integrating these tasks
into online advertising. Extensive experiments on offline and online
environments demonstrate MTORL's superiority over state-of-the-art methods.

</details>


### [719] [Compositions of Variant Experts for Integrating Short-Term and Long-Term Preferences](https://arxiv.org/abs/2506.23170)
*Jaime Hieu Do,Trung-Hoang Le,Hady W. Lauw*

Main category: cs.IR

TL;DR: The paper introduces Compositions of Variant Experts (CoVE), a framework for combining short- and long-term user preferences to enhance personalized sequential recommendations.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of leveraging both short-term session context and long-term cumulative user behavior to improve the relevance and timeliness of recommendations.

Method: The proposed CoVE framework dynamically integrates specialized recommendation models (experts) for both short-term and long-term preferences. Empirical studies on real-world datasets were conducted.

Result: Experiments illustrate the effectiveness of CoVE in improving recommendation performance compared to traditional systems across diverse datasets. Ablation studies confirm the contributions of different expert types.

Conclusion: The research demonstrates that combining short- and long-term preferences through the CoVE framework significantly enhances recommendation system performance, offering a promising direction for future applications.

Abstract: In the online digital realm, recommendation systems are ubiquitous and play a
crucial role in enhancing user experience. These systems leverage user
preferences to provide personalized recommendations, thereby helping users
navigate through the paradox of choice. This work focuses on personalized
sequential recommendation, where the system considers not only a user's
immediate, evolving session context, but also their cumulative historical
behavior to provide highly relevant and timely recommendations. Through an
empirical study conducted on diverse real-world datasets, we have observed and
quantified the existence and impact of both short-term (immediate and
transient) and long-term (enduring and stable) preferences on users' historical
interactions. Building on these insights, we propose a framework that combines
short- and long-term preferences to enhance recommendation performance, namely
Compositions of Variant Experts (CoVE). This novel framework dynamically
integrates short- and long-term preferences through the use of different
specialized recommendation models (i.e., experts). Extensive experiments
showcase the effectiveness of the proposed methods and ablation studies further
investigate the impact of variant expert types.

</details>


### [720] [Learning to Rank with Variable Result Presentation Lengths](https://arxiv.org/abs/2506.23319)
*Norman Knyazev,Harrie Oosterhuis*

Main category: cs.IR

TL;DR: This paper proposes a new ranking strategy considering document presentation lengths to optimize user engagement and relevance perception.


<details>
  <summary>Details</summary>
Motivation: Current Learning to Rank (LTR) methods fail to account for how varying document presentation lengths can influence user attention and relevance perception.

Method: The paper introduces VLPL, a Plackett-Luce-based list-wise optimization method, to jointly determine both the order and presentation length of documents.

Result: Semi-synthetic experiments reveal that VLPL effectively balances exposure and attractiveness, outperforming fixed-length and simpler length-aware methods.

Conclusion: Integrating document presentation length in LTR shows significant advantages, illustrating challenges and opportunities for improving ranking systems.

Abstract: Learning to Rank (LTR) methods generally assume that each document in a top-K
ranking is presented in an equal format. However, previous work has shown that
users' perceptions of relevance can be changed by varying presentations, i.e.,
allocating more vertical space to some documents to provide additional textual
or image information. Furthermore, presentation length can also redirect
attention, as users are more likely to notice longer presentations when
scrolling through results. Deciding on the document presentation lengths in a
fixed vertical space ranking is an important problem that has not been
addressed by existing LTR methods.
  We address this gap by introducing the variable presentation length ranking
task, where simultaneously the ordering of documents and their presentation
length is decided. Despite being a generalization of standard ranking, we show
that this setting brings significant new challenges: Firstly, the probability
ranking principle no longer applies to this setting, and secondly, the problem
cannot be divided into separate ordering and length selection tasks.
  We therefore propose VLPL - a new family of Plackett-Luce list-wise gradient
estimation methods for the joint optimization of document ordering and lengths.
Our semi-synthetic experiments show that VLPL can effectively balance the
expected exposure and attractiveness of all documents, achieving the best
performance across different ranking settings. Furthermore, we observe that
even simple length-aware methods can achieve significant performance
improvements over fixed-length models. Altogether, our theoretical and
empirical results highlight the importance and difficulties of combining
document presentation with LTR.

</details>


### [721] [KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation, And Try-On](https://arxiv.org/abs/2506.23471)
*Thanh-Tung Phan-Nguyen,Khoi-Nguyen Nguyen-Ngoc,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.IR

TL;DR: The paper introduces KiseKloset, an advanced system for outfit retrieval, recommendation, and virtual try-on, boasting high user satisfaction at 84%.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the online shopping experience through better outfit recommendations and virtual try-on capabilities, addressing customer and retailer needs such as personalization and reducing return costs.

Method: The system employs two outfit retrieval methods (similar item retrieval and text-guided retrieval), a novel transformer for recommendations, optimized search pipelines with approximate algorithms, and a lightweight virtual try-on framework.

Result: The user study reported that 84% of participants found the KiseKloset system highly useful, significantly improving their online shopping experience.

Conclusion: KiseKloset successfully integrates outfit retrieval, recommendation, and virtual try-on, offering realistic visualization and improving user satisfaction in the e-commerce fashion domain.

Abstract: The global fashion e-commerce industry has become integral to people's daily
lives, leveraging technological advancements to offer personalized shopping
experiences, primarily through recommendation systems that enhance customer
engagement through personalized suggestions. To improve customers' experience
in online shopping, we propose a novel comprehensive KiseKloset system for
outfit retrieval, recommendation, and try-on. We explore two approaches for
outfit retrieval: similar item retrieval and text feedback-guided item
retrieval. Notably, we introduce a novel transformer architecture designed to
recommend complementary items from diverse categories. Furthermore, we enhance
the overall performance of the search pipeline by integrating approximate
algorithms to optimize the search process. Additionally, addressing the crucial
needs of online shoppers, we employ a lightweight yet efficient virtual try-on
framework capable of real-time operation, memory efficiency, and maintaining
realistic outputs compared to its predecessors. This virtual try-on module
empowers users to visualize specific garments on themselves, enhancing the
customers' experience and reducing costs associated with damaged items for
retailers. We deployed our end-to-end system for online users to test and
provide feedback, enabling us to measure their satisfaction levels. The results
of our user study revealed that 84% of participants found our comprehensive
system highly useful, significantly improving their online shopping experience.

</details>
