<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 46]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 63]
- [cs.CV](#cs.CV) [Total: 88]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 92]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.PL](#cs.PL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 26]
- [cs.SE](#cs.SE) [Total: 18]
- [q-bio.NC](#q-bio.NC) [Total: 4]
- [stat.ML](#stat.ML) [Total: 15]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.CR](#cs.CR) [Total: 10]
- [econ.GN](#econ.GN) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 2]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [cs.IR](#cs.IR) [Total: 7]
- [eess.IV](#eess.IV) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [eess.AS](#eess.AS) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [math.OC](#math.OC) [Total: 3]
- [q-bio.GN](#q-bio.GN) [Total: 2]
- [cs.CY](#cs.CY) [Total: 4]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI Agents for the Dhumbal Card Game: A Comparative Study](https://arxiv.org/abs/2510.11736)
*Sahaj Raj Malla*

Main category: cs.AI

TL;DR: The study analyzes AI agents for the culturally significant Dhumbal card game, comparing rule-based, search-based, and learning-based strategies. The Aggressive rule-based agent performed best with an 88.3% win rate.


<details>
  <summary>Details</summary>
Motivation: The paper aims to evaluate AI strategies for Dhumbal, a multiplayer card game with imperfect information, and explore how AI can contribute to digital preservation of cultural games.

Method: The study formalizes Dhumbal's mechanics and tests rule-based (e.g., Aggressive, Conservative), search-based (e.g., MCTS, ISMCTS), and learning-based (e.g., DQN, PPO) AI agents through within-category and cross-category tournaments using statistical methods like Welch's t-test and Cohen's d.

Result: The Aggressive rule-based agent had the highest win rate at 88.3%, outperforming search-based (ISMCTS) and learning-based (PPO) agents. Performance was measured by various metrics like win rates and decision efficiency.

Conclusion: Rule-based strategies, particularly the Aggressive heuristic, are highly effective in Dhumbal, offering insights into heuristic efficacy in imperfect information settings. The research framework and open-source code also aid in cultural game preservation.

Abstract: This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a
culturally significant multiplayer card game with imperfect information,
through a systematic comparison of rule-based, search-based, and learning-based
strategies. We formalize Dhumbal's mechanics and implement diverse agents,
including heuristic approaches (Aggressive, Conservative, Balanced,
Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and
Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning
approaches including Deep Q-Network (DQN) and Proximal Policy Optimization
(PPO), and a random baseline. Evaluation involves within-category tournaments
followed by a cross-category championship. Performance is measured via win
rate, economic outcome, Jhyap success, cards discarded per round, risk
assessment, and decision efficiency. Statistical significance is assessed using
Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95%
confidence intervals (CI). Across 1024 simulated rounds, the rule-based
Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]),
outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of
Jhyap declarations. The study contributes a reproducible AI framework, insights
into heuristic efficacy under partial information, and open-source code,
thereby advancing AI research and supporting digital preservation of cultural
games.

</details>


### [2] [Tensor Logic: The Language of AI](https://arxiv.org/abs/2510.12269)
*Pedro Domingos*

Main category: cs.AI

TL;DR: The paper introduces 'tensor logic,' a programming language designed to unify neural and symbolic AI, overcoming limitations in existing tools like PyTorch, TensorFlow, LISP, and Prolog.


<details>
  <summary>Details</summary>
Motivation: AI lacks a unified language that combines the capabilities of neural and symbolic AI. Existing tools like PyTorch and TensorFlow are not intrinsically designed for AI, while traditional AI languages lack scalability and learning support.

Method: The paper proposes tensor logic as a new language, grounded in tensor equations, unifying logical rules and Einstein summation. It demonstrates key AI implementations and explores new capabilities like reasoning in embedding space.

Result: Tensor logic enables the seamless integration of neural, symbolic, and statistical AI techniques, showing potential for scalable and reliable AI applications.

Conclusion: Tensor logic combines the strengths of diverse AI approaches, paving the way for integrated systems that are scalable, learnable, reliable, and transparent, potentially broadening AI adoption.

Abstract: Progress in AI is hindered by the lack of a programming language with all the
requisite features. Libraries like PyTorch and TensorFlow provide automatic
differentiation and efficient GPU implementation, but are additions to Python,
which was never intended for AI. Their lack of support for automated reasoning
and knowledge acquisition has led to a long and costly series of hacky attempts
to tack them on. On the other hand, AI languages like LISP an Prolog lack
scalability and support for learning. This paper proposes tensor logic, a
language that solves these problems by unifying neural and symbolic AI at a
fundamental level. The sole construct in tensor logic is the tensor equation,
based on the observation that logical rules and Einstein summation are
essentially the same operation, and all else can be reduced to them. I show how
to elegantly implement key forms of neural, symbolic and statistical AI in
tensor logic, including transformers, formal reasoning, kernel machines and
graphical models. Most importantly, tensor logic makes new directions possible,
such as sound reasoning in embedding space. This combines the scalability and
learnability of neural networks with the reliability and transparency of
symbolic reasoning, and is potentially a basis for the wider adoption of AI.

</details>


### [3] [Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations](https://arxiv.org/abs/2510.11822)
*Suryaansh Jain,Umair Z. Ahmed,Shubham Sahai,Ben Leong*

Main category: cs.AI

TL;DR: This paper addresses the limitation of LLM-as-a-judge due to positive bias when evaluating outputs. It proposes a minority-veto strategy and a regression-based framework tailored to reduce bias and improve evaluation accuracy.


<details>
  <summary>Details</summary>
Motivation: Frequent new LLMs compel developers to evaluate switching, but human evaluation is costly, and automated LLM-based evaluations are biased, necessitating improved evaluation techniques.

Method: Proposed a minority-veto strategy to counter bias and a regression-based method using limited human-annotated data to directly model validator bias.

Result: On a Python program feedback task, the regression approach reduced maximum absolute error to 1.2%, doubling the performance of the best ensemble model.

Conclusion: The proposed solutions significantly enhance reliability and precision in automated LLM evaluation by addressing bias and ensuring scalability.

Abstract: New Large Language Models (LLMs) become available every few weeks, and modern
application developers confronted with the unenviable task of having to decide
if they should switch to a new model. While human evaluation remains the gold
standard, it is costly and unscalable. The state-of-the-art approach is to use
LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw:
LLMs exhibit a strong positive bias. We provide empirical evidence showing that
while LLMs can identify valid outputs with high accuracy (i.e., True Positive
Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True
Negative Rate <25%). This systematic bias, coupled with class imbalance, often
leads to inflated reliability scores.
  While ensemble-based methods like majority voting can help, we show that they
are not good enough. We introduce an optimal minority-veto strategy that is
resilient to missing data and mitigates this bias to a large extent. For
scenarios requiring even higher precision, we propose a novel regression-based
framework that directly models the validator bias using a small set of
human-annotated ground truth data. On a challenging code feedback task over 366
high-school Python programs, our regression approach reduces the maximum
absolute error to just 1.2%, achieving a 2x improvement over the
best-performing ensemble of 14 state-of-the-art LLMs.

</details>


### [4] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: The paper introduces the Holistic Agent Leaderboard (HAL) to standardize AI agent evaluation, focusing on real-world reliability over benchmark performance.


<details>
  <summary>Details</summary>
Motivation: Existing AI agent evaluations face challenges that limit understanding of agent performance in practical scenarios.

Method: HAL standardizes evaluation by orchestrating parallel tests on hundreds of VMs, conducts comprehensive analysis across models, scaffolds, and benchmarks, and uses LLM-aided log inspection to detect unique behaviors.

Result: Evaluations of 21,730 agent rollouts revealed surprising insights, such as reduced accuracy with increased reasoning efforts, and uncovered unusual agent behaviors.

Conclusion: HAL sets a standardized framework for evaluating AI agents, aiming to shift focus from scoring well on benchmarks to performing reliably in real-world tasks.

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [5] [CGBench: Benchmarking Language Model Scientific Reasoning for Clinical Genetics Research](https://arxiv.org/abs/2510.11985)
*Owen Queen,Harrison G. Zhang,James Zou*

Main category: cs.AI

TL;DR: This paper introduces CGBench, a benchmark to assess generative language models in interpreting scientific literature, especially for clinical genetics.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of manual, labor-intensive approaches in interpreting genetic literature and the lack of real-world relevance in existing LM benchmarks.

Method: CGBench was built using ClinGen data to evaluate LMs on tasks such as experimental result extraction, evidence judgment, and task categorization. It tested 8 different LMs across reasoning and high-level interpretation tasks.

Result: The study found that while reasoning models perform well in fine-grained tasks, non-reasoning models excel in high-level interpretations. However, all models struggle with aspects such as hallucinations and misinterpretation.

Conclusion: CGBench highlights the strengths and weaknesses of LMs in interpreting scientific literature, suggesting paths for improvement in AI applications for clinical genetics and beyond.

Abstract: Variant and gene interpretation are fundamental to personalized medicine and
translational biomedicine. However, traditional approaches are manual and
labor-intensive. Generative language models (LMs) can facilitate this process,
accelerating the translation of fundamental research into clinically-actionable
insights. While existing benchmarks have attempted to quantify the capabilities
of LMs for interpreting scientific data, these studies focus on narrow tasks
that do not translate to real-world research. To meet these challenges, we
introduce CGBench, a robust benchmark that tests reasoning capabilities of LMs
on scientific publications. CGBench is built from ClinGen, a resource of
expert-curated literature interpretations in clinical genetics. CGBench
measures the ability to 1) extract relevant experimental results following
precise protocols and guidelines, 2) judge the strength of evidence, and 3)
categorize and describe the relevant outcome of experiments. We test 8
different LMs and find that while models show promise, substantial gaps exist
in literature interpretation, especially on fine-grained instructions.
Reasoning models excel in fine-grained tasks but non-reasoning models are
better at high-level interpretations. Finally, we measure LM explanations
against human explanations with an LM judge approach, revealing that models
often hallucinate or misinterpret results even when correctly classifying
evidence. CGBench reveals strengths and weaknesses of LMs for precise
interpretation of scientific publications, opening avenues for future research
in AI for clinical genetics and science more broadly.

</details>


### [6] [Asking Clarifying Questions for Preference Elicitation With Large Language Models](https://arxiv.org/abs/2510.12015)
*Ali Montazeralghaem,Guy Tennenholtz,Craig Boutilier,Ofer Meshi*

Main category: cs.AI

TL;DR: The paper presents a method for training Large Language Models (LLMs) to ask sequential clarifying questions for better user preference elicitation.


<details>
  <summary>Details</summary>
Motivation: LLMs for recommendation systems need to elicit user preferences effectively for personalization, especially with limited user history.

Method: A two-stage process inspired by diffusion models is introduced to train LLMs to ask clarifying questions. The forward process generates and removes answers to simulate noise, while the reverse process trains the model to ask effective questions to 'denoise' the user profile.

Result: The method enhances the ability of LLMs to ask sequential clarifying questions, improving preference elicitation.

Conclusion: The proposed approach successfully trains LLMs to generate effective clarifying questions, advancing their capability in user preference elicitation.

Abstract: Large Language Models (LLMs) have made it possible for recommendation systems
to interact with users in open-ended conversational interfaces. In order to
personalize LLM responses, it is crucial to elicit user preferences, especially
when there is limited user history. One way to get more information is to
present clarifying questions to the user. However, generating effective
sequential clarifying questions across various domains remains a challenge. To
address this, we introduce a novel approach for training LLMs to ask sequential
questions that reveal user preferences. Our method follows a two-stage process
inspired by diffusion models. Starting from a user profile, the forward process
generates clarifying questions to obtain answers and then removes those answers
step by step, serving as a way to add ``noise'' to the user profile. The
reverse process involves training a model to ``denoise'' the user profile by
learning to ask effective clarifying questions. Our results show that our
method significantly improves the LLM's proficiency in asking funnel questions
and eliciting user preferences effectively.

</details>


### [7] [CausalTrace: A Neurosymbolic Causal Analysis Agent for Smart Manufacturing](https://arxiv.org/abs/2510.12033)
*Chathurangi Shyalika,Aryaman Sharma,Fadi El Kalach,Utkarshani Jaimini,Cory Henson,Ramy Harik,Amit Sheth*

Main category: cs.AI

TL;DR: The paper introduces CausalTrace, a neurosymbolic causal analysis module, integrated into an industrial CoPilot to enhance trust, explanation, and prediction in manufacturing environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing AI systems in manufacturing environments, which often function as black boxes and lack integrated prediction, explanation, and causal reasoning capabilities, reducing trustworthiness and utility.

Method: The authors developed CausalTrace, which combines data-driven causal analysis with industrial ontologies and knowledge graphs, and offers features like causal discovery, counterfactual reasoning, and root cause analysis. It was evaluated using multiple causal methods and the C3AN framework.

Result: CausalTrace demonstrated high agreement with domain experts and outstanding performance in root cause analysis, with metrics like ROUGE-1 (0.91), MAP@3 (94%), PR@2 (97%), and MRR (0.92). It also scored 4.59/5 in trustworthiness evaluations.

Conclusion: CausalTrace successfully integrates causal reasoning into industrial AI systems, providing precise, reliable, and interpretable decision-support tools for real-time applications.

Abstract: Modern manufacturing environments demand not only accurate predictions but
also interpretable insights to process anomalies, root causes, and potential
interventions. Existing AI systems often function as isolated black boxes,
lacking the seamless integration of prediction, explanation, and causal
reasoning required for a unified decision-support solution. This fragmentation
limits their trustworthiness and practical utility in high-stakes industrial
environments. In this work, we present CausalTrace, a neurosymbolic causal
analysis module integrated into the SmartPilot industrial CoPilot. CausalTrace
performs data-driven causal analysis enriched by industrial ontologies and
knowledge graphs, including advanced functions such as causal discovery,
counterfactual reasoning, and root cause analysis (RCA). It supports real-time
operator interaction and is designed to complement existing agents by offering
transparent, explainable decision support. We conducted a comprehensive
evaluation of CausalTrace using multiple causal assessment methods and the C3AN
framework (i.e. Custom, Compact, Composite AI with Neurosymbolic Integration),
which spans principles of robustness, intelligence, and trustworthiness. In an
academic rocket assembly testbed, CausalTrace achieved substantial agreement
with domain experts (ROUGE-1: 0.91 in ontology QA) and strong RCA performance
(MAP@3: 94%, PR@2: 97%, MRR: 0.92, Jaccard: 0.92). It also attained 4.59/5 in
the C3AN evaluation, demonstrating precision and reliability for live
deployment.

</details>


### [8] [Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](https://arxiv.org/abs/2510.12047)
*Soohan Lim,Joonghyuk Hahn,Hyunwoo Park,Sang-Ki Ko,Yo-Sub Han*

Main category: cs.AI

TL;DR: The paper introduces PACT, a framework to evaluate and improve large language models (LLMs) in adhering to software contracts (rules for handling ill-formed inputs) in code generation, along with functional correctness.


<details>
  <summary>Details</summary>
Motivation: Existing code generation benchmarks lack evaluation of adherence to software contracts, which are critical for assessing robustness and reliability of real-world software.

Method: PACT introduces a test-suite with contract-focused violations, employs varied prompting conditions, and develops metrics to evaluate and enhance contract adherence and functionality in LLM-generated code.

Result: PACT demonstrates the effectiveness of prompting augmentation in improving contract adherence and reveals overlooked errors in traditional benchmarks, providing a more reliable and interpretable evaluation of LLMs.

Conclusion: PACT enhances the robustness and reliability assessment of LLM-generated code by addressing both functional correctness and contract adherence, advancing the development of better performing models.

Abstract: Prevailing code generation benchmarks, such as HumanEval+ and MBPP+,
primarily evaluate large language models (LLMs) with pass@k on functional
correctness using well-formed inputs. However, they ignore a crucial aspect of
real-world software: adherence to contracts-the preconditions and validity
constraints that dictate how ill-formed inputs must be rejected. This critical
oversight means that existing benchmarks fail to measure, and models
consequently fail to generate, truly robust and reliable code snippets. We
introduce PACT, a program assessment and contract-adherence evaluation
framework, to bridge this gap. PACT is the first framework designed to
systematically evaluate and enhance contract-adherence in LLM-generated code
snippets alongside functional correctness. PACT's contributions are threefold:
First, it provides a comprehensive test-suite corpus focused on contract
violations, extending HumanEval+ and MBPP+. Second, it enables a systematic
analysis of code generation under varied prompting conditions. This analysis
demonstrates that augmenting prompts with contract-violating test cases
significantly enhance a model's ability to respect contracts compared to using
contract description alone. Finally, it introduces novel metrics to rigorously
quantify contract adherence in both test generation and code generation. By
revealing critical errors that conventional benchmarks overlook, PACT provides
the rigorous and interpretable metrics to evaluate the robustness of
LLM-generated code snippets in both functionality and contract-adherence.Our
code and data are available at https://github.com/suhanmen/PACT.

</details>


### [9] [Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response](https://arxiv.org/abs/2510.12061)
*Yiheng Chen,Lingyao Li,Zihui Ma,Qikai Hu,Yilun Zhu,Min Deng,Runlong Yu*

Main category: cs.AI

TL;DR: This paper introduces a Geospatial Awareness Layer (GAL) to enhance disaster response models by integrating geospatial data, improving semantic context and interpretability in recommendations.


<details>
  <summary>Details</summary>
Motivation: Existing disaster response approaches lack semantic context, geographic awareness, and interpretability, limiting their effectiveness in decision-making during emergencies such as wildfires.

Method: The authors propose adding a Geospatial Awareness Layer (GAL) to LLMs to ground them in geospatial data. GAL integrates structured information on infrastructure, demographics, terrain, and weather into a perception script for data-driven recommendations.

Result: In testing on wildfire scenarios using multiple LLMs, geospatially grounded agents equipped with GAL showcased enhanced performance, surpassing traditional baselines in resource allocation and decision-making.

Conclusion: The proposed GAL framework improves the applicability and efficiency of LLMs in disaster management scenarios and has the potential to be adapted for various hazard types like floods and hurricanes.

Abstract: Effective disaster response is essential for safeguarding lives and property.
Existing statistical approaches often lack semantic context, generalize poorly
across events, and offer limited interpretability. While Large language models
(LLMs) provide few-shot generalization, they remain text-bound and blind to
geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL)
that grounds LLM agents in structured earth data. Starting from raw wildfire
detections, GAL automatically retrieves and integrates infrastructure,
demographic, terrain, and weather information from external geodatabases,
assembling them into a concise, unit-annotated perception script. This enriched
context enables agents to produce evidence-based resource-allocation
recommendations (e.g., personnel assignments, budget allocations), further
reinforced by historical analogs and daily change signals for incremental
updates. We evaluate the framework in real wildfire scenarios across multiple
LLM models, showing that geospatially grounded agents can outperform baselines.
The proposed framework can generalize to other hazards such as floods and
hurricanes.

</details>


### [10] [ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://arxiv.org/abs/2510.12063)
*Sunzhu Li,Zhiyu Lin,Shuling Yang,Jiale Zhao,Wei Chen*

Main category: cs.AI

TL;DR: ThinkPilot improves large reasoning models' (LRMs) reasoning through a training-free framework using think-prefixes to optimize efficiency, safety, and instruction-following.


<details>
  <summary>Details</summary>
Motivation: Existing LRMs suffer from inefficient and off-target reasoning, and current methods to improve them lack adaptability and actionable solutions.

Method: ThinkPilot employs an evolutionary process to create think-prefixes—specific instructions that guide reasoning behaviors, using a taxonomy of behaviors for tailored optimization.

Result: Extensive experiments show ThinkPilot enhances efficiency in reasoning, drastically improves safety, strengthens instruction adherence, and works well alongside training-based methods.

Conclusion: ThinkPilot reliably optimizes LRMs' reasoning by aligning behavioral distributions with task-specific demands, offering a scalable and actionable solution for LRM improvement.

Abstract: Large Reasoning Models (LRMs) are powerful, but they still suffer from
inefficient and off-target reasoning. Currently, training-free methods are
limited to either rigid heuristics or descriptive, non-actionable analyses. In
this paper, we introduce ThinkPilot, a training-free framework that
automatically optimizes LRMs reasoning. It uses an evolutionary process to
generate think-prefixes, which are instructions that evolve driven by a
taxonomy of reasoning behaviors to guide models toward superior performance.
Extensive experiments demonstrate ThinkPilot's broad effectiveness: it
significantly improves the accuracy-length trade-off for efficient reasoning,
drastically improves safety (for example, cutting the StrongREJECT score of
DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction
following. It also synergizes with existing training-based methods. Our
analysis reveals that think-prefixes can reliably control LRMs' reasoning
behaviors, and that different tasks have strong preferences for specific
behavioral distributions. By automatically identifying and eliciting these
behaviors, ThinkPilot provides a generalizable framework for aligning LRMs
reasoning with task demands. Data and code are available at
https://github.com/teqkilla/ThinkPilot

</details>


### [11] [AI Agents as Universal Task Solvers](https://arxiv.org/abs/2510.12066)
*Alessandro Achille,Stefano Soatto*

Main category: cs.AI

TL;DR: This paper explores the concept of AI agents as computational systems capable of reasoning and learning, proposing a shift from inductive to transductive learning focused on time reduction for solving tasks.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address fundamental questions about the universality of AI reasoning agents, how they learn, and the relationship between model size, training data, and reasoning capabilities.

Method: The authors reinterpret AI agents as stochastic dynamical systems and emphasize transductive learning, focusing on understanding algorithmic structures to minimize time in solving new tasks. They derive theoretical results connecting optimal speed-up and algorithmic information and analyze scaling behaviors in reasoning models.

Result: The study provides theoretical insights into the relationship between training and inference time, showing power-law scaling and illustrating the limitations of scaling model size, which can lead to brute-force behavior without true intelligence.

Conclusion: The authors argue that optimizing reasoning models should prioritize time efficiency rather than solely focusing on accuracy or size, challenging traditional concepts of learning and scaling in AI.

Abstract: AI reasoning agents are already able to solve a variety of tasks by deploying
tools, simulating outcomes of multiple hypotheses and reflecting on them. In
doing so, they perform computation, although not in the classical sense --
there is no program being executed. Still, if they perform computation, can AI
agents be universal? Can chain-of-thought reasoning solve any computable task?
How does an AI Agent learn to reason? Is it a matter of model size? Or training
dataset size?
  In this work, we reinterpret the role of learning in the context of AI
Agents, viewing them as compute-capable stochastic dynamical systems, and
highlight the role of time in a foundational principle for learning to reason.
In doing so, we propose a shift from classical inductive learning to
transductive learning -- where the objective is not to approximate the
distribution of past data, but to capture their algorithmic structure to reduce
the time needed to find solutions to new tasks.
  Transductive learning suggests that, counter to Shannon's theory, a key role
of information in learning is about reduction of time rather than
reconstruction error. In particular, we show that the optimal speed-up that a
universal solver can achieve using past data is tightly related to their
algorithmic information. Using this, we show a theoretical derivation for the
observed power-law scaling of inference time versus training time. We then show
that scaling model size can lead to behaviors that, while improving accuracy on
benchmarks, fail any reasonable test of intelligence, let alone
super-intelligence: In the limit of infinite space and time, large models can
behave as savants, able to brute-force through any task without any insight.
Instead, we argue that the key quantity to optimize when scaling reasoning
models is time, whose critical role in learning has so far only been indirectly
considered.

</details>


### [12] [HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory](https://arxiv.org/abs/2510.12067)
*Junyi Xie,Yuankun Jiao,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: The paper introduces HiCoTraj, a framework using LLMs for zero-shot demographic inference from mobility data, leveraging natural language representations and hierarchical reasoning to improve interpretability and generalizability.


<details>
  <summary>Details</summary>
Motivation: Demographic inference from mobility patterns is essential for applications like public health and urban planning, but current methods lack interpretability and generalizability due to reliance on labeled data.

Method: HiCoTraj employs large language models (LLMs) for zero-shot learning by transforming trajectory data into natural language representations and using hierarchical chain-of-thought reasoning for demographic inference.

Result: HiCoTraj demonstrates competitive zero-shot inferencing performance on real-world trajectory data for various demographic attributes.

Conclusion: The approach addresses challenges in labeled data scarcity and improves transparency and reasoning in demographic inference, offering a generalizable and interpretable solution.

Abstract: Inferring demographic attributes such as age, sex, or income level from human
mobility patterns enables critical applications such as targeted public health
interventions, equitable urban planning, and personalized transportation
services. Existing mobility-based demographic inference studies heavily rely on
large-scale trajectory data with demographic labels, leading to limited
interpretability and poor generalizability across different datasets and user
groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical
Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs'
zero-shot learning and semantic understanding capabilities to perform
demographic inference without labeled training data. HiCoTraj transforms
trajectories into semantically rich, natural language representations by
creating detailed activity chronicles and multi-scale visiting summaries. Then
HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically
guide LLMs through three cognitive stages: factual feature extraction,
behavioral pattern analysis, and demographic inference with structured output.
This approach addresses the scarcity challenge of labeled demographic data
while providing transparent reasoning chains. Experimental evaluation on
real-world trajectory data demonstrates that HiCoTraj achieves competitive
performance across multiple demographic attributes in zero-shot scenarios.

</details>


### [13] [EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making](https://arxiv.org/abs/2510.12072)
*Zixing Lei,Sheng Yin,Yichen Xiong,Yuanzhuo Ding,Wenhao Huang,Yuxi Wei,Qingyao Xu,Yiming Li,Weixin Li,Yunhong Wang,Siheng Chen*

Main category: cs.AI

TL;DR: The paper introduces EmboMatrix, an infrastructure for training large language models (LLMs) in embodied decision-making, overcoming limitations from traditional language-only training.


<details>
  <summary>Details</summary>
Motivation: LLMs lack true embodied understanding due to limited exposure to physical interactions in real-world environments.

Method: The paper proposes EmboMatrix, integrating task and scene simulation, embodied interaction, feedback signals, and precise rewards using advanced techniques such as a multi-agent data engine.

Result: EmboBrain-7B, trained using EmboMatrix, exhibited a 9.5% improvement over a baseline model on challenging embodied decision-making benchmarks.

Conclusion: Environment-grounded learning enhances the intelligent capabilities of LLMs in decision-making tasks tied to physical interactions.

Abstract: Embodied decision-making enables agents to translate high-level goals into
executable actions through continuous interactions within the physical world,
forming a cornerstone of general-purpose embodied intelligence. Large language
models (LLMs), with their general decision-making capabilities, offer a
promising path to realize this potential; however, LLMs trained solely on
language lack exposure to physical environments, limiting their true embodied
understanding. To bridge this gap, we propose the concept of a training ground:
a comprehensive infrastructure that provides task and scene simulation,
embodied interaction, and feedback signals, offering a one-stop solution for
LLM acquire genuine embodied decision-making skills. In this work, we present
EmboMatrix, the first training ground of its kind, providing massive and
diverse tasks with efficient simulation and precise rewards. EmboMatrix
incorporates a series of novel techniques: a multi-agent data engine for
large-scale task and scene generation, a distributed heterogeneous-hardware
system for scalable simulation, and a multi-level reward architecture for
precise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM
whose embodied decision-making abilities emerge from extensive embodied
interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1
baseline by 9.5\% on two challenging embodied decision-making benchmarks,
demonstrating the power of interactive, environment-grounded learning for
building truly intelligent embodied agents.

</details>


### [14] [BeSTAD: Behavior-Aware Spatio-Temporal Anomaly Detection for Human Mobility Data](https://arxiv.org/abs/2510.12076)
*Junyi Xie,Jina Kim,Yao-Yi Chiang,Lingyi Zhao,Khurram Shafique*

Main category: cs.AI

TL;DR: This paper introduces BeSTAD, a framework for detecting personalized anomalies in human mobility by jointly analyzing spatial and temporal patterns.


<details>
  <summary>Details</summary>
Motivation: Traditional anomaly detection has overlooked individual-level deviations in large datasets, focusing instead on trajectory-level inconsistencies and statistical outliers.

Method: The proposed BeSTAD framework combines spatial context and temporal dynamics to create enriched mobility representations and uses behavior-cluster-aware modeling to personalize anomaly detection.

Result: BeSTAD identifies subtle deviations in mobility behavior, behavioral shifts, and individuals with anomalous patterns within large-scale datasets.

Conclusion: BeSTAD enhances anomaly detection by integrating personalized, interpretable, and behavior-aware mechanisms for large-scale mobility data analysis.

Abstract: Traditional anomaly detection in human mobility has primarily focused on
trajectory-level analysis, identifying statistical outliers or spatiotemporal
inconsistencies across aggregated movement traces. However, detecting
individual-level anomalies, i.e., unusual deviations in a person's mobility
behavior relative to their own historical patterns, within datasets
encompassing large populations remains a significant challenge. In this paper,
we present BeSTAD (Behavior-aware Spatio-Temporal Anomaly Detection for Human
Mobility Data), an unsupervised framework that captures individualized
behavioral signatures across large populations and uncovers fine-grained
anomalies by jointly modeling spatial context and temporal dynamics. BeSTAD
learns semantically enriched mobility representations that integrate location
meaning and temporal patterns, enabling the detection of subtle deviations in
individual movement behavior. BeSTAD further employs a behavior-cluster-aware
modeling mechanism that builds personalized behavioral profiles from normal
activity and identifies anomalies through cross-period behavioral comparison
with consistent semantic alignment. Building on prior work in mobility behavior
clustering, this approach enables not only the detection of behavioral shifts
and deviations from established routines but also the identification of
individuals exhibiting such changes within large-scale mobility datasets. By
learning individual behaviors directly from unlabeled data, BeSTAD advances
anomaly detection toward personalized and interpretable mobility analysis.

</details>


### [15] [Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models](https://arxiv.org/abs/2510.12080)
*Rabimba Karanjai,Yang Lu,Ranjith Chodavarapu,Lei Xu,Weidong Shi*

Main category: cs.AI

TL;DR: This paper explores the ability of large language models (LLMs) in tasks requiring randomness, revealing inconsistent and suboptimal behavior.


<details>
  <summary>Details</summary>
Motivation: To assess whether LLMs can reliably handle various tasks requiring randomness for applications like AI agents, gaming, and cryptography.

Method: Conducted experiments with factors affecting LLM performance in randomness tasks, such as tool access, task types, model states, and prompting strategies.

Result: LLMs showed inconsistent ability in generating high-quality random numbers, strings, and successful randomness-related tasks.

Conclusion: LLMs possess some ability for randomness but require significant improvement to handle these tasks effectively.

Abstract: The rapid advancement of large language model (LLM) technology has led to
diverse applications, many of which inherently require randomness, such as
stochastic decision-making, gaming, scheduling, AI agents, and
cryptography-related tasks. However, the capabilities of LLMs in handling
randomness, particularly in generating and utilizing random numbers
effectively, remain unclear. This paper investigates the capacity of LLMs for
handling tasks that involve randomness through a series of experiments. We
designed a set of experiments that consider various factors that can influence
an LLM's performance in tasks involving randomness, such as accessibility to
external tools, types of tasks, model states (fresh vs. non-fresh), and
prompting strategies. The experiments cover a range of tasks, including
generating random numbers, generating random strings such as passwords,
shuffling items, and evaluating the quality of randomness using entropy and the
NIST randomness test-suite. Our findings reveal that while LLMs can generate
outputs that exhibit some degree of randomness, their performance is
inconsistent and often deviates significantly from the expected behavior. The
analysis of the experimental results highlights key limitations and areas where
improvement is needed for the LLMs to effectively handle tasks involving
randomness

</details>


### [16] [One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration](https://arxiv.org/abs/2510.12088)
*Zaid Khan,Archiki Prasad,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.AI

TL;DR: The paper introduces OneLife, a probabilistic programming framework for modeling dynamics in stochastic environments where an agent has minimal interaction and no human guidance. OneLife demonstrates strong performance in learning dynamics and planning strategies.


<details>
  <summary>Details</summary>
Motivation: The goal is to model transitional dynamics in complex, stochastic environments where agents interact sparsely and autonomously, avoiding prior reliance on abundant data and human assistance.

Method: OneLife uses conditionally-activated programmatic laws within a probabilistic framework. These laws operate with a precondition-effect structure and dynamically adjust the computation graph for relevance, facilitating effective learning of sparse dynamics.

Result: OneLife outperformed a strong baseline in 16 out of 23 test scenarios using the Crafter-OO environment, showing better understanding of dynamics and improved planning strategies.

Conclusion: The work provides a robust method for autonomously constructing programmatic world models in unknown and complex environments, marking progress toward realistic symbolic world modeling.

Abstract: Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

</details>


### [17] [ToPolyAgent: AI Agents for Coarse-Grained Topological Polymer Simulations](https://arxiv.org/abs/2510.12091)
*Lijie Ding,Jan-Michael Carrillo,Changwoo Do*

Main category: cs.AI

TL;DR: ToPolyAgent is a multi-agent AI framework integrating large language models and computational tools to perform molecular simulations of polymers via natural language commands.


<details>
  <summary>Details</summary>
Motivation: To lower barriers in complex computational workflows and advance AI-driven discoveries in polymer science.

Method: The framework uses four LLM-powered agents for configuration generation, simulation execution, report compilation, and workflow management in both interactive and autonomous modes.

Result: Demonstrated versatility in case studies of various polymer architectures and conditions, showing its capability to explore polymer science parameters.

Conclusion: ToPolyAgent facilitates research in polymer science, enabling autonomous multi-agent workflows and extensible scientific research ecosystems.

Abstract: We introduce ToPolyAgent, a multi-agent AI framework for performing
coarse-grained molecular dynamics (MD) simulations of topological polymers
through natural language instructions. By integrating large language models
(LLMs) with domain-specific computational tools, ToPolyAgent supports both
interactive and autonomous simulation workflows across diverse polymer
architectures, including linear, ring, brush, and star polymers, as well as
dendrimers. The system consists of four LLM-powered agents: a Config Agent for
generating initial polymer-solvent configurations, a Simulation Agent for
executing LAMMPS-based MD simulations and conformational analyses, a Report
Agent for compiling markdown reports, and a Workflow Agent for streamlined
autonomous operations. Interactive mode incorporates user feedback loops for
iterative refinements, while autonomous mode enables end-to-end task execution
from detailed prompts. We demonstrate ToPolyAgent's versatility through case
studies involving diverse polymer architectures under varying solvent
condition, thermostats, and simulation lengths. Furthermore, we highlight its
potential as a research assistant by directing it to investigate the effect of
interaction parameters on the linear polymer conformation, and the influence of
grafting density on the persistence length of the brush polymer. By coupling
natural language interfaces with rigorous simulation tools, ToPolyAgent lowers
barriers to complex computational workflows and advances AI-driven materials
discovery in polymer science. It lays the foundation for autonomous and
extensible multi-agent scientific research ecosystems.

</details>


### [18] [Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](https://arxiv.org/abs/2510.12121)
*Rongzhi Zhang,Liqin Ye,Yuzhao Heng,Xiang Chen,Tong Yu,Lingkai Kong,Sudheer Chava,Chao Zhang*

Main category: cs.AI

TL;DR: This paper introduces a novel method to achieve precise control over specific attribute intensities in LLM outputs, resolving challenges of current alignment methods.


<details>
  <summary>Details</summary>
Motivation: To enable AI systems to meet diverse user expectations by achieving exact, user-specified attribute intensities in generated content.

Method: The authors reformulate the control problem as target-reaching, utilize temporal-difference learning to train a lightweight value function predicting intensities, and perform gradient-based interventions to steer outputs.

Result: The method achieves high accuracy in controlling attribute intensities on LLaMA-3.2-3b and Phi-4-mini models, enhancing efficiency across various downstream tasks.

Conclusion: Precise control of attribute intensities is addressed successfully through innovative designs, advancing beyond traditional directional approaches in LLM alignment.

Abstract: Precise attribute intensity control--generating Large Language Model (LLM)
outputs with specific, user-defined attribute intensities--is crucial for AI
systems adaptable to diverse user expectations. Current LLM alignment methods,
however, typically provide only directional or open-ended guidance, failing to
reliably achieve exact attribute intensities. We address this limitation with
three key designs: (1) reformulating precise attribute intensity control as a
target-reaching problem, rather than simple maximization; (2) training a
lightweight value function via temporal-difference learning to predict final
attribute intensity scores from partial generations, thereby steering LLM
outputs; and (3) employing gradient-based interventions on hidden
representations to navigate the model precisely towards specific attribute
intensity targets. Our method enables fine-grained, continuous control over
attribute intensities, moving beyond simple directional alignment. Experiments
on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text
generation to user-specified attribute intensities with high accuracy. Finally,
we demonstrate efficiency enhancements across three downstream tasks:
preference data synthesis, Pareto frontier approximation and optimization, and
distillation of aligned behaviors for intervention-free inference. Our code is
available on https://github.com/Pre-Control/pre-control

</details>


### [19] [MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science](https://arxiv.org/abs/2510.12171)
*Junkai Zhang,Jingru Gan,Xiaoxuan Wang,Zian Jia,Changquan Gu,Jianpeng Chen,Yanqiao Zhu,Mingyu Derek Ma,Dawei Zhou,Ling Li,Wei Wang*

Main category: cs.AI

TL;DR: MatSciBench is a college-level benchmark with 1,340 materials science problems, assessing LLM reasoning through multimodal contexts and structured difficulty.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored reasoning capabilities of Large Language Models in the materials science domain.

Method: Developing MatSciBench, categorizing problems by subfields, difficulty tiers, and incorporating multimodal challenges for evaluation with various reasoning strategies.

Result: Leading LLMs, like Gemini-2.5-Pro, struggle with under 80% accuracy, emphasizing the complexity of reasoning in this benchmark.

Conclusion: MatSciBench sets a robust standard for assessing and advancing LLMs' scientific reasoning in materials science.

Abstract: Large Language Models (LLMs) have demonstrated remarkable abilities in
scientific reasoning, yet their reasoning capabilities in materials science
remain underexplored. To fill this gap, we introduce MatSciBench, a
comprehensive college-level benchmark comprising 1,340 problems that span the
essential subdisciplines of materials science. MatSciBench features a
structured and fine-grained taxonomy that categorizes materials science
questions into 6 primary fields and 31 sub-fields, and includes a three-tier
difficulty classification based on the reasoning length required to solve each
question. MatSciBench provides detailed reference solutions enabling precise
error analysis and incorporates multimodal reasoning through visual contexts in
numerous questions. Evaluations of leading models reveal that even the
highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on
college-level materials science questions, highlighting the complexity of
MatSciBench. Our systematic analysis of different reasoning strategie--basic
chain-of-thought, tool augmentation, and self-correction--demonstrates that no
single method consistently excels across all scenarios. We further analyze
performance by difficulty level, examine trade-offs between efficiency and
accuracy, highlight the challenges inherent in multimodal reasoning tasks,
analyze failure modes across LLMs and reasoning methods, and evaluate the
influence of retrieval-augmented generation. MatSciBench thus establishes a
comprehensive and solid benchmark for assessing and driving improvements in the
scientific reasoning capabilities of LLMs within the materials science domain.

</details>


### [20] [Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey](https://arxiv.org/abs/2510.12178)
*Abdulhady Abas Abdullah,Arkaitz Zubiaga,Seyedali Mirjalili,Amir H. Gandomi,Fatemeh Daneshfar,Mohammadsadra Amini,Alan Salam Mohammed,Hadi Veisi*

Main category: cs.AI

TL;DR: This paper reviews Meta AI's LLaMA models and their evolution, detailing architectures, PEFT methods, performance, and real-world applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore LLaMA models' progression and PEFT methods for efficient fine-tuning of large language models in real-world scenarios.

Method: The paper surveys LLaMA model architectures and PEFT techniques (LoRA, LLaMA-Adapter V1/V2, LLaMA-Excitor, QLoRA), analyzing their mechanisms, benchmarks, and applications.

Result: PEFT techniques like LoRA or QLoRA achieve parameter savings and enable fine-tuned LLaMA models to outperform larger baselines in tasks like instruction tuning and multimodal use cases.

Conclusion: LLaMA and PEFT methodologies provide scalable solutions for specialized domains like legal and medical industries. Challenges and future directions involve scaling context and improving robustness.

Abstract: This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.

</details>


### [21] [ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents](https://arxiv.org/abs/2510.12194)
*Linyi Yang,Yixuan Weng*

Main category: cs.AI

TL;DR: ResearStudio is an open-source research agent framework allowing real-time human intervention and achieving state-of-the-art results on the GAIA benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the lack of real-time human control in existing deep-research agents, which traditionally operate without user corrections or integration of expert knowledge during execution.

Method: Designed a Collaborative Workshop framework with a hierarchical Planner-Executor system that logs actions in a live plan document and streams updates to a web interface. Users can intervene, edit, and resume smoothly while toggling between various levels of AI and human control.

Result: ResearStudio achieves state-of-the-art performance on the GAIA benchmark, surpassing existing systems like OpenAI's DeepResearch and Manus.

Conclusion: ResearStudio demonstrates that robust automation and precise human control can coexist, promoting safer and more controllable research agents. Open-source availability encourages further development in this field.

Abstract: Current deep-research agents run in a ''fire-and-forget'' mode: once started,
they give users no way to fix errors or add expert knowledge during execution.
We present ResearStudio, the first open-source framework that places real-time
human control at its core. The system follows a Collaborative Workshop design.
A hierarchical Planner-Executor writes every step to a live
''plan-as-document,'' a fast communication layer streams each action, file
change, and tool call to a web interface. At any moment, the user can pause the
run, edit the plan or code, run custom commands, and resume -- switching
smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In
fully autonomous mode, ResearStudio achieves state-of-the-art results on the
GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These
results show that strong automated performance and fine-grained human control
can coexist. The full code, protocol, and evaluation scripts are available at
https://github.com/ResearAI/ResearStudio. We will continue to update the
repository to encourage further work on safe and controllable research agents.
Our live demo is publicly accessible at http://ai-researcher.net:3000/. We
support the development of DeepScientist, which can be accessed at
https://github.com/ResearAI/DeepScientist.

</details>


### [22] [On the Design and Evaluation of Human-centered Explainable AI Systems: A Systematic Review and Taxonomy](https://arxiv.org/abs/2510.12201)
*Aline Mangold,Juliane Zietz,Susanne Weinhold,Sebastian Pannasch*

Main category: cs.AI

TL;DR: The paper reviews 65 user studies on XAI systems, outlines evaluation metrics, and proposes human-centered design goals tailored to users with varying AI expertise.


<details>
  <summary>Details</summary>
Motivation: AI's increasing integration into daily life necessitates systems that are both effective and explainable, demanding user-focused evaluation processes.

Method: The authors conducted a comprehensive review of 65 user studies and developed a framework that extends current XAI evaluation and design guidelines.

Result: Key findings include distinguishing XAI core systems from their explanations, categorizing evaluation metrics (e.g., usability, cognition), and tailoring design goals for AI novices and data experts.

Conclusion: Holistic design goals are essential for creating user-focused XAI systems, requiring adaptability to different user expertise levels for responsible, acceptable, and performant systems.

Abstract: As AI becomes more common in everyday living, there is an increasing demand
for intelligent systems that are both performant and understandable.
Explainable AI (XAI) systems aim to provide comprehensible explanations of
decisions and predictions. At present, however, evaluation processes are rather
technical and not sufficiently focused on the needs of human users.
Consequently, evaluation studies involving human users can serve as a valuable
guide for conducting user studies. This paper presents a comprehensive review
of 65 user studies evaluating XAI systems across different domains and
application contexts. As a guideline for XAI developers, we provide a holistic
overview of the properties of XAI systems and evaluation metrics focused on
human users (human-centered). We propose objectives for the human-centered
design (design goals) of XAI systems. To incorporate users' specific
characteristics, design goals are adapted to users with different levels of AI
expertise (AI novices and data experts). In this regard, we provide an
extension to existing XAI evaluation and design frameworks. The first part of
our results includes the analysis of XAI system characteristics. An important
finding is the distinction between the core system and the XAI explanation,
which together form the whole system. Further results include the distinction
of evaluation metrics into affection towards the system, cognition, usability,
interpretability, and explanation metrics. Furthermore, the users, along with
their specific characteristics and behavior, can be assessed. For AI novices,
the relevant extended design goals include responsible use, acceptance, and
usability. For data experts, the focus is performance-oriented and includes
human-AI collaboration and system and user task performance.

</details>


### [23] [GOAT: A Training Framework for Goal-Oriented Agent with Tools](https://arxiv.org/abs/2510.12218)
*Hyunji Min,Sangwon Jung,Junyoung Sung,Dosung Lee,Leekyeung Han,Paul Hongsuck Seo*

Main category: cs.AI

TL;DR: The paper introduces GOAT, a framework for enhancing goal-oriented abilities in open-source large language models (LLMs) to handle complex reasoning and tool usage tasks without needing human annotations.


<details>
  <summary>Details</summary>
Motivation: LLMs show limitations in goal-oriented queries requiring decomposition of tasks into interdependent API calls. Open-source models struggle with complex tool usage compared to closed-source solutions like GPT-4.

Method: GOAT utilizes synthetic data generated from API documentation to fine-tune LLMs, eliminating the need for human annotations. It equips models with reasoning abilities necessary for executing interdependent API calls.

Result: Experiments demonstrate that GOAT-trained agents achieved state-of-the-art results across goal-oriented benchmarks and performed excellently on the newly introduced GOATBench.

Conclusion: GOAT represents a promising method for training LLM agents to handle complex reasoning and tool use in an effective, scalable, and human annotation-free manner.

Abstract: Large language models (LLMs) have recently been extended beyond traditional
text generation to serve as interactive agents capable of using external tools
based on user intent. However, current LLM agents still show limited ability to
handle goal-oriented queries, which require decomposing a high-level objective
into multiple interdependent API calls with correct planning and execution.
Current approaches mainly rely on zero-shot evaluation due to the absence of
training data. While proprietary closed-source models such as GPT-4 demonstrate
strong reasoning abilities, smaller open-source models struggle to perform
complex tool use effectively. Thus, we propose a novel training framework GOAT,
which enables fine-tuning of LLM agents in a human annotation-free setting.
GOAT automatically constructs synthetic datasets of goal-oriented API execution
tasks directly from given API documents, equipping models with the ability to
reason over interdependent calls and generate coherent responses. Through
extensive experiments, we show that GOAT-trained agents achieve
state-of-the-art performance across multiple existing goal-oriented benchmarks.
In addition, we introduce GOATBench, a new goal-oriented API execution
benchmark, and demonstrate that agents trained with GOAT also excel in this
setting. These results highlight GOAT as a practical path toward building
robust open-source LLM agents capable of complex reasoning and tool use.

</details>


### [24] [MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs](https://arxiv.org/abs/2510.12224)
*Yuechun Yu,Han Ying,Haoan Jin,Wenjian Jiang,Dong Xian,Binghao Wang,Zhou Yang,Mengyue Wu*

Main category: cs.AI

TL;DR: The paper introduces MedKGEval, a novel framework for evaluating clinical Large Language Models (LLMs) using structured knowledge graphs and multi-turn interactions.


<details>
  <summary>Details</summary>
Motivation: There is a need for reliable evaluation methods that capture the dynamic nature of doctor-patient interactions in clinical environments, as existing methods are often limited to post hoc analyses and fail to account for evolving informational needs.

Method: The framework includes (1) a knowledge graph-driven patient simulation for realistic conversational behavior, (2) in-situ, turn-level evaluation for assessing model responses using task-specific metrics, and (3) a benchmark of eight state-of-the-art LLMs to identify behavioral flaws and safety risks.

Result: MedKGEval successfully identifies subtle flaws and safety risks in LLMs that traditional evaluation pipelines overlook, demonstrating its effectiveness in diverse medical applications and languages.

Conclusion: MedKGEval enables reliable and dynamic evaluation of clinical LLMs, bridging gaps in current assessment methods and ensuring adaptability across languages and medical domains.

Abstract: The reliable evaluation of large language models (LLMs) in medical
applications remains an open challenge, particularly in capturing the
complexity of multi-turn doctor-patient interactions that unfold in real
clinical environments. Existing evaluation methods typically rely on post hoc
review of full conversation transcripts, thereby neglecting the dynamic,
context-sensitive nature of medical dialogues and the evolving informational
needs of patients. In this work, we present MedKGEval, a novel multi-turn
evaluation framework for clinical LLMs grounded in structured medical
knowledge. Our approach introduces three key contributions: (1) a knowledge
graph-driven patient simulation mechanism, where a dedicated control module
retrieves relevant medical facts from a curated knowledge graph, thereby
endowing the patient agent with human-like and realistic conversational
behavior. This knowledge graph is constructed by integrating open-source
resources with additional triples extracted from expert-annotated datasets; (2)
an in-situ, turn-level evaluation framework, where each model response is
assessed by a Judge Agent for clinical appropriateness, factual correctness,
and safety as the dialogue progresses using a suite of fine-grained,
task-specific metrics; (3) a comprehensive multi-turn benchmark of eight
state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle
behavioral flaws and safety risks that are often overlooked by conventional
evaluation pipelines. Although initially designed for Chinese and English
medical applications, our framework can be readily extended to additional
languages by switching the input knowledge graphs, ensuring seamless bilingual
support and domain-specific applicability.

</details>


### [25] [PromptFlow: Training Prompts Like Neural Networks](https://arxiv.org/abs/2510.12246)
*Jingyi Wang,Hongyuan Zhu,Ye Niu,Yunhui Deng*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) require effective domain-specific adaptations for optimal performance. This paper introduces PromptFlow, a modular training framework for automated and dynamic prompt engineering, enhancing task-specific refinement.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations in current prompt engineering methodologies, such as static updates, coarse-grained prompt adjustments, and a lack of reinforcement mechanisms to recycle experience.

Method: The proposed PromptFlow framework integrates elements like meta-prompts, operators, optimization, and evaluators to refine prompts dynamically using gradient-based meta-learning and reinforcement learning techniques.

Result: Experiments across various datasets demonstrate the effectiveness of PromptFlow in achieving optimal prompt refinement trajectories with minimal task-specific data.

Conclusion: PromptFlow successfully addresses challenges in prompt engineering, providing a modular and efficient framework for domain-specific LLM adaptations, thereby enhancing NLP task performance.

Abstract: Large Language Models (LLMs) have demonstrated profound impact on Natural
Language Processing (NLP) tasks. However, their effective deployment across
diverse domains often require domain-specific adaptation strategies, as generic
models may underperform when faced with specialized data distributions. Recent
advances in prompt engineering (PE) offer a promising alternative to extensive
retraining by refining input instructions to align LLM outputs with task
objectives. This paradigm has emerged as a rapid and versatile approach for
model fine-tuning. Despite its potential, manual prompt design remains
labor-intensive and heavily depends on specialized expertise, often requiring
iterative human effort to achieve optimal formulations. To address this
limitation, automated prompt engineering methodologies have been developed to
systematically generate task-specific prompts. However, current implementations
predominantly employ static update rules and lack mechanisms for dynamic
strategy selection, resulting in suboptimal adaptation to varying NLP task
requirements. Furthermore, most methods treat and update the whole prompts at
each step, without considering editing prompt sections at a finer granularity.
At last, in particular, the problem of how to recycle experience in LLM is
still underexplored. To this end, we propose the PromptFlow, a modular training
framework inspired by TensorFlow, which integrates meta-prompts, operators,
optimization, and evaluator. Our framework can be equipped with the latest
optimization methods and autonomously explores optimal prompt refinement
trajectories through gradient-based meta-learning, requiring minimal
task-specific training data. Specifically, we devise a reinforcement learning
method to recycle experience for LLM in the PE process. Finally, we conduct
extensive experiments on various datasets, and demonstrate the effectiveness of
PromptFlow.

</details>


### [26] [$\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning](https://arxiv.org/abs/2510.12264)
*Deyu Zou,Yongqiang Chen,Jianxiang Wang,Haochen Yang,Mufei Li,James Cheng,Pan Li,Yu Gong*

Main category: cs.AI

TL;DR: The paper introduces $\mathbf{T^3}$, a method that improves reasoning in large language models (LLMs) by addressing belief tracking issues, resulting in up to 30% performance gains and reduced token usage.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents often face difficulties with belief tracking, leading to errors, uninformative actions, and training inefficiencies, hampering their ability to perform active reasoning.

Method: The authors propose $\mathbf{T^3}$, a method that monitors belief deviations during training and truncates uninformative trajectories, crediting only the useful exploratory steps to enhance policy optimization.

Result: Across five tasks, $\mathbf{T^3}$ stabilizes training, reduces rollout tokens by roughly 25%, and improves performance, achieving up to 30% gains in various metrics.

Conclusion: Belief control is crucial for creating robust LLM-based agents, and $\mathbf{T^3}$ effectively improves their reasoning and performance by addressing belief deviation issues.

Abstract: Active reasoning requires large language models (LLMs) to interact with
external sources and strategically gather information to solve problems.
Central to this process is belief tracking: maintaining a coherent
understanding of the problem state and the missing information toward the
solution. However, due to limited reasoning capabilities, LLM-based agents
often suffer from belief deviation: they struggle to correctly model beliefs,
lose track of problem states, and fall into uninformative or repetitive
actions. Once this happens, errors compound and reinforcement learning (RL)
training fails to properly credit the crucial exploratory steps. To address
this issue, we propose to track the deviation of model beliefs and develop
$\mathbf{T^3}$, a simple yet effective method that detects excessive belief
deviation and truncates trajectories during training to remove uninformative
tails. By preserving credit for informative prefixes, $\mathbf{T^3}$
systematically improves policy optimization. Across 5 challenging tasks,
$\mathbf{T^3}$ consistently enhances training stability, token efficiency, and
final performance, achieving up to 30% gains while cutting rollout tokens by
roughly 25%. These results highlight belief control as a key principle for
developing robust and generalizable LLM-based active reasoners.

</details>


### [27] [RAG-Anything: All-in-One RAG Framework](https://arxiv.org/abs/2510.12323)
*Zirui Guo,Xubin Ren,Lingrui Xu,Jiahao Zhang,Chao Huang*

Main category: cs.AI

TL;DR: The paper introduces 'RAG-Anything,' a unified framework that expands retrieval-augmented generation (RAG) capabilities to effectively process multimodal knowledge repositories, achieving notable performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of existing RAG frameworks which are confined to textual data, preventing efficient knowledge retrieval from multimodal repositories.

Method: The framework employs a dual-graph construction to represent cross-modal relationships and textual semantics unifiedly, alongside a cross-modal hybrid retrieval mechanism for structural and semantic reasoning.

Result: RAG-Anything exhibits superior performance on multimodal benchmarks, especially with complex and long documents, outperforming state-of-the-art methods.

Conclusion: This framework sets a new standard for multimodal knowledge retrieval, eliminating the fragmentation in current RAG systems and enhancing reasoning across diverse data types.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm
for expanding Large Language Models beyond their static training limitations.
However, a critical misalignment exists between current RAG capabilities and
real-world information environments. Modern knowledge repositories are
inherently multimodal, containing rich combinations of textual content, visual
elements, structured tables, and mathematical expressions. Yet existing RAG
frameworks are limited to textual content, creating fundamental gaps when
processing multimodal documents. We present RAG-Anything, a unified framework
that enables comprehensive knowledge retrieval across all modalities. Our
approach reconceptualizes multimodal content as interconnected knowledge
entities rather than isolated data types. The framework introduces dual-graph
construction to capture both cross-modal relationships and textual semantics
within a unified representation. We develop cross-modal hybrid retrieval that
combines structural knowledge navigation with semantic matching. This enables
effective reasoning over heterogeneous content where relevant evidence spans
multiple modalities. RAG-Anything demonstrates superior performance on
challenging multimodal benchmarks, achieving significant improvements over
state-of-the-art methods. Performance gains become particularly pronounced on
long documents where traditional approaches fail. Our framework establishes a
new paradigm for multimodal knowledge access, eliminating the architectural
fragmentation that constrains current systems. Our framework is open-sourced
at: https://github.com/HKUDS/RAG-Anything.

</details>


### [28] [O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis](https://arxiv.org/abs/2510.12350)
*Ayush Khaitan,Vijay Ganesh*

Main category: cs.AI

TL;DR: The paper introduces LLM+CAS, a framework combining large language models (LLMs) and computer algebra systems (CAS), to generate and verify creative proofs in research-level mathematics. The approach is applied to solving asymptotic inequalities.


<details>
  <summary>Details</summary>
Motivation: The difficulty in trustworthiness of AI-generated proofs limits their use in research mathematics. Verification is essential for proofs of complex problems like asymptotic inequalities.

Method: The LLM+CAS framework combines a large language model to propose domain decompositions and a computer algebra system for symbolic verification in a feedback loop.

Result: The framework successfully addresses a question posed by Terence Tao by helping to prove intricate asymptotic inequalities, demonstrating its utility for research-level mathematics.

Conclusion: LLM+CAS shows that AI tools can assist mathematicians in research-level tasks and extend beyond contests to professional mathematical reasoning.

Abstract: Large language models have recently demonstrated advanced capabilities in
solving IMO and Putnam problems; yet their role in research mathematics has
remained fairly limited. The key difficulty is verification: suggested proofs
may look plausible, but cannot be trusted without rigorous checking. We present
a framework, called LLM+CAS, and an associated tool, O-Forge, that couples
frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic
Feedback loop to produce proofs that are both creative and symbolically
verified. Our focus is on asymptotic inequalities, a topic that often involves
difficult proofs and appropriate decomposition of the domain into the "right"
subdomains. Many mathematicians, including Terry Tao, have suggested that using
AI tools to find the right decompositions can be very useful for research-level
asymptotic analysis. In this paper, we show that our framework LLM+CAS turns
out to be remarkably effective at proposing such decompositions via a
combination of a frontier LLM and a CAS. More precisely, we use an LLM to
suggest domain decomposition, and a CAS (such as Mathematica) that provides a
verification of each piece axiomatically. Using this loop, we answer a question
posed by Terence Tao: whether LLMs coupled with a verifier can be used to help
prove intricate asymptotic inequalities. More broadly, we show how AI can move
beyond contest math towards research-level tools for professional
mathematicians.

</details>


### [29] [A Survey of Vibe Coding with Large Language Models](https://arxiv.org/abs/2510.12399)
*Yuyao Ge,Lingrui Mei,Zenghao Duan,Tianhao Li,Yujia Zheng,Yiwei Wang,Lexin Wang,Jiayu Yao,Tianyu Liu,Yujun Cai,Baolong Bi,Fangda Guo,Jiafeng Guo,Shenghua Liu,Xueqi Cheng*

Main category: cs.AI

TL;DR: The paper examines 'Vibe Coding,' a development method using large language models for autonomous coding, and identifies challenges in its productivity and collaboration. It introduces theoretical foundations and provides a taxonomy of development models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the unexplored effectiveness of Vibe Coding, aiming to establish theoretical foundations, frameworks, and address productivity and collaboration challenges in autonomous coding with large language models.

Method: The authors conducted a systematic review of over 1000 research papers, analyzed the ecosystem, introduced formalism through a Constrained Markov Decision Process, and synthesized practices into distinct development models.

Result: The paper identifies factors like systematic context engineering, robust development environments, and collaborative models as critical for successful Vibe Coding.

Conclusion: The study establishes Vibe Coding as a formal discipline and provides a taxonomy to enhance productivity and collaboration in human-agent coding systems.

Abstract: The advancement of large language models (LLMs) has catalyzed a paradigm
shift from code generation assistance to autonomous coding agents, enabling a
novel development methodology termed "Vibe Coding" where developers validate
AI-generated implementations through outcome observation rather than
line-by-line code comprehension. Despite its transformative potential, the
effectiveness of this emergent paradigm remains under-explored, with empirical
evidence revealing unexpected productivity losses and fundamental challenges in
human-AI collaboration. To address this gap, this survey provides the first
comprehensive and systematic review of Vibe Coding with large language models,
establishing both theoretical foundations and practical frameworks for this
transformative development approach. Drawing from systematic analysis of over
1000 research papers, we survey the entire vibe coding ecosystem, examining
critical infrastructure components including LLMs for coding, LLM-based coding
agent, development environment of coding agent, and feedback mechanisms. We
first introduce Vibe Coding as a formal discipline by formalizing it through a
Constrained Markov Decision Process that captures the dynamic triadic
relationship among human developers, software projects, and coding agents.
Building upon this theoretical foundation, we then synthesize existing
practices into five distinct development models: Unconstrained Automation,
Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and
Context-Enhanced Models, thus providing the first comprehensive taxonomy in
this domain. Critically, our analysis reveals that successful Vibe Coding
depends not merely on agent capabilities but on systematic context engineering,
well-established development environments, and human-agent collaborative
development models.

</details>


### [30] [PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks](https://arxiv.org/abs/2510.12409)
*Yunuo Liu,Dawei Zhu,Zena Al-Khalili,Dai Cheng,Yanjun Chen,Dietrich Klakow,Wei Zhang,Xiaoyu Shen*

Main category: cs.AI

TL;DR: The paper introduces PricingLogic, a benchmark for assessing the ability of Large Language Models (LLMs) to automate pricing tasks in tourism, uncovered performance limitations on complex fare rules.


<details>
  <summary>Details</summary>
Motivation: To address the need for reliable automation of tourism-related pricing tasks by LLMs to reduce errors and ensure financial stability and customer trust.

Method: Developed PricingLogic, a benchmark with 300 natural-language questions from real-world scenarios that test rule interpretation and arithmetic reasoning skills of LLMs.

Result: Evaluations reveal LLMs struggle with complex bundled-tour pricing involving discounts, showing reliability gaps in revenue-critical applications.

Conclusion: Current LLMs require safeguards and adaptation to be deemed trustworthy in handling high-stakes tasks like tourism pricing.

Abstract: We present PricingLogic, the first benchmark that probes whether Large
Language Models(LLMs) can reliably automate tourism-related prices when
multiple, overlapping fare rules apply. Travel agencies are eager to offload
this error-prone task onto AI systems; however, deploying LLMs without verified
reliability could result in significant financial losses and erode customer
trust. PricingLogic comprises 300 natural-language questions based on booking
requests derived from 42 real-world pricing policies, spanning two levels of
difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations
involving interacting discounts. Evaluations of a line of LLMs reveal a steep
performance drop on the harder tier,exposing systematic failures in rule
interpretation and arithmetic reasoning.These results highlight that, despite
their general capabilities, today's LLMs remain unreliable in revenue-critical
applications without further safeguards or domain adaptation. Our code and
dataset are available at https://github.com/EIT-NLP/PricingLogic.

</details>


### [31] [MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics](https://arxiv.org/abs/2510.12423)
*Dingyi Zuo,Hongjie Zhang,Jie Ou,Chaosheng Feng,Shuwan Liu*

Main category: cs.AI

TL;DR: The paper presents Multi-topic Opinion Simulation (MTOS), a framework for simulating opinion evolution across multiple topics using LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing approaches oversimplify complex opinions and lack tools to simulate multi-topic interactions and cognitive transfers effectively.

Method: Developing MTOS using large language models combined with memory mechanisms and dynamic topic strategies, studying opinion evolution with variables like topic correlations.

Result: Results show that correlations among multiple topics affect echo chambers: positive correlations amplify them, negative correlations suppress them, and irrelevant topics reduce them via resource competition.

Conclusion: MTOS improves simulation by incorporating linguistic realism, complex human reasoning, and stability compared to traditional numerical models.

Abstract: The polarization of opinions, information segregation, and cognitive biases
on social media have attracted significant academic attention. In real-world
networks, information often spans multiple interrelated topics, posing
challenges for opinion evolution and highlighting the need for frameworks that
simulate interactions among topics. Existing studies based on large language
models (LLMs) focus largely on single topics, limiting the capture of cognitive
transfer in multi-topic, cross-domain contexts. Traditional numerical models,
meanwhile, simplify complex linguistic attitudes into discrete values, lacking
interpretability, behavioral consistency, and the ability to integrate multiple
topics. To address these issues, we propose Multi-topic Opinion Simulation
(MTOS), a social simulation framework integrating multi-topic contexts with
LLMs. MTOS leverages LLMs alongside short-term and long-term memory,
incorporates multiple user-selection interaction mechanisms and dynamic
topic-selection strategies, and employs a belief decay mechanism to enable
perspective updates across topics. We conduct extensive experiments on MTOS,
varying topic numbers, correlation types, and performing ablation studies to
assess features such as group polarization and local consistency. Results show
that multi-topic settings significantly alter polarization trends: positively
correlated topics amplify echo chambers, negatively correlated topics inhibit
them, and irrelevant topics also mitigate echo chamber effects through resource
competition. Compared with numerical models, LLM-based agents realistically
simulate dynamic opinion changes, reproduce linguistic features of news texts,
and capture complex human reasoning, improving simulation interpretability and
system stability.

</details>


### [32] [Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections](https://arxiv.org/abs/2510.12428)
*Chengyang Dong,Nan Guo*

Main category: cs.AI

TL;DR: The paper proposes a DRL framework integrated with biased attention to improve autonomous driving decision-making at unsignalized intersections.


<details>
  <summary>Details</summary>
Motivation: To address challenges in autonomous driving at unsignalized intersections by improving safety and efficiency through proactive decision-making.

Method: The authors developed a DRL framework based on the SAC algorithm, incorporating biased attention to predict traffic risks and convert them into dense reward signals for decision-making.

Result: Simulation results demonstrated enhanced traffic efficiency and vehicle safety when using the proposed framework.

Conclusion: The innovative decision-making framework is effective in tackling complex intersection challenges, ensuring improved safety and operational efficiency for autonomous vehicles.

Abstract: Autonomous driving decision-making at unsignalized intersections is highly
challenging due to complex dynamic interactions and high conflict risks. To
achieve proactive safety control, this paper proposes a deep reinforcement
learning (DRL) decision-making framework integrated with a biased attention
mechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm.
Its core innovation lies in the use of biased attention to construct a traffic
risk predictor. This predictor assesses the long-term risk of collision for a
vehicle entering the intersection and transforms this risk into a dense reward
signal to guide the SAC agent in making safe and efficient driving decisions.
Finally, the simulation results demonstrate that the proposed method
effectively improves both traffic efficiency and vehicle safety at the
intersection, thereby proving the effectiveness of the intelligent
decision-making framework in complex scenarios. The code of our work is
available at https://github.com/hank111525/SAC-RWB.

</details>


### [33] [Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems](https://arxiv.org/abs/2510.12462)
*Jiaxin Gao,Chen Chen,Yanwen Jia,Xueluan Gong,Kwok-Yan Lam,Qian Wang*

Main category: cs.AI

TL;DR: The paper investigates judgment biases in Large Language Models (LLMs) acting as judges (GPT-Judge and JudgeLM) and proposes mitigation strategies to ensure fair and reliable evaluations.


<details>
  <summary>Details</summary>
Motivation: To address the potential biases in LLMs used as judges in communication systems and ensure their impartiality and reliability.

Method: The study examines 11 types of biases in LLM-as-a-judge models under a point-wise scoring setting and evaluates their response to biased inputs. It includes experiments with scoring rubrics and analyzes the impact of training on biased data. The correlation of judged scores with task difficulty is also assessed.

Result: LLMs showed robustness to biased inputs when detailed scoring rubrics were provided. However, fine-tuning on biased data degraded performance. Scores were influenced by task difficulty, with higher scores in open-ended reasoning datasets and lower scores in challenging datasets.

Conclusion: The paper concludes that LLM judges have potential biases but can be improved with scoring rubrics and proper training. Four mitigation strategies are proposed to ensure fair and reliable AI judging in communication systems.

Abstract: Large Language Models (LLMs) are increasingly being used to autonomously
evaluate the quality of content in communication systems, e.g., to assess
responses in telecom customer support chatbots. However, the impartiality of
these AI "judges" is not guaranteed, and any biases in their evaluation
criteria could skew outcomes and undermine user trust. In this paper, we
systematically investigate judgment biases in two LLM-as-a-judge models (i.e.,
GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11
types of biases that cover both implicit and explicit forms. We observed that
state-of-the-art LLM judges demonstrate robustness to biased inputs, generally
assigning them lower scores than the corresponding clean samples. Providing a
detailed scoring rubric further enhances this robustness. We further found that
fine-tuning an LLM on high-scoring yet biased responses can significantly
degrade its performance, highlighting the risk of training on biased data. We
also discovered that the judged scores correlate with task difficulty: a
challenging dataset like GPQA yields lower average scores, whereas an
open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.
Finally, we proposed four potential mitigation strategies to ensure fair and
reliable AI judging in practical communication scenarios.

</details>


### [34] [Using Medical Algorithms for Task-Oriented Dialogue in LLM-Based Medical Interviews](https://arxiv.org/abs/2510.12490)
*Rui Reis,Pedro Rangel Henriques,João Ferreira-Coimbra,Eva Oliveira,Nuno F. Rodrigues*

Main category: cs.AI

TL;DR: Development of a Directed Acyclic Graph (DAG)-based task-oriented dialogue system for medical questioning, evaluated for usability and satisfaction with promising results.


<details>
  <summary>Details</summary>
Motivation: To create an adaptive dialogue system that efficiently gathers medical information, reduces cognitive load, and generates structured reports integrated into clinical workflows.

Method: The framework uses a DAG structure for medical questioning, implements mechanisms for cold-start questioning, adaptive branching, termination logic, and automated report synthesis. Human-computer interaction principles guide its design.

Result: The system demonstrated low workload and high satisfaction (NASA-TLX, SUS, QUIS scales), with effective integration into clinical workflows. Limitations included occasional latency and a small evaluation sample.

Conclusion: The system efficiently supports medical information gathering and clinical workflows, showing strong usability and satisfaction, though further testing is needed.

Abstract: We developed a task-oriented dialogue framework structured as a Directed
Acyclic Graph (DAG) of medical questions. The system integrates: (1) a
systematic pipeline for transforming medical algorithms and guidelines into a
clinical question corpus; (2) a cold-start mechanism based on hierarchical
clustering to generate efficient initial questioning without prior patient
information; (3) an expand-and-prune mechanism enabling adaptive branching and
backtracking based on patient responses; (4) a termination logic to ensure
interviews end once sufficient information is gathered; and (5) automated
synthesis of doctor-friendly structured reports aligned with clinical
workflows. Human-computer interaction principles guided the design of both the
patient and physician applications. Preliminary evaluation involved five
physicians using standardized instruments: NASA-TLX (cognitive workload), the
System Usability Scale (SUS), and the Questionnaire for User Interface
Satisfaction (QUIS). The patient application achieved low workload scores
(NASA-TLX = 15.6), high usability (SUS = 86), and strong satisfaction (QUIS =
8.1/9), with particularly high ratings for ease of learning and interface
design. The physician application yielded moderate workload (NASA-TLX = 26) and
excellent usability (SUS = 88.5), with satisfaction scores of 8.3/9. Both
applications demonstrated effective integration into clinical workflows,
reducing cognitive demand and supporting efficient report generation.
Limitations included occasional system latency and a small, non-diverse
evaluation sample.

</details>


### [35] [Artificial Intelligence Virtual Cells: From Measurements to Decisions across Modality, Scale, Dynamics, and Evaluation](https://arxiv.org/abs/2510.12498)
*Chengpeng Hu,Calvin Yu-Chian Chen*

Main category: cs.AI

TL;DR: This paper discusses the challenges of building robust Artificial Intelligence Virtual Cells (AIVCs) models and proposes a Cell-State Latent (CSL) framework to enhance cross-scale coupling, evaluation, and reproducibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations in current AIVC models, including poor cross-laboratory transportability, data leakage, inadequate handling of dose and time effects, and sparse cross-scale coupling between molecular, cellular, and tissue levels.

Method: The authors propose a model-agnostic CSL perspective organized around an operator grammar (measurement, lift/project for cross-scale coupling, and intervention). This approach emphasizes decision-aligned evaluations and function-space readouts, along with improved data design and evaluation protocols.

Result: The study introduces a decision-aligned evaluation blueprint and highlights the importance of operator-aware data design, leakage-resistant partitions, and transparent reporting. This enables more robust AIVC models with better cross-modality and cross-scale compatibility.

Conclusion: The CSL framework provides a systematic strategy to improve the reproducibility, transparency, and reliability of AIVC models, facilitating their alignment with scientific or clinical applications.

Abstract: Artificial Intelligence Virtual Cells (AIVCs) aim to learn executable,
decision-relevant models of cell state from multimodal, multiscale
measurements. Recent studies have introduced single-cell and spatial foundation
models, improved cross-modality alignment, scaled perturbation atlases, and
explored pathway-level readouts. Nevertheless, although held-out validation is
standard practice, evaluations remain predominantly within single datasets and
settings; evidence indicates that transport across laboratories and platforms
is often limited, that some data splits are vulnerable to leakage and coverage
bias, and that dose, time and combination effects are not yet systematically
handled. Cross-scale coupling also remains constrained, as anchors linking
molecular, cellular and tissue levels are sparse, and alignment to scientific
or clinical readouts varies across studies. We propose a model-agnostic
Cell-State Latent (CSL) perspective that organizes learning via an operator
grammar: measurement, lift/project for cross-scale coupling, and intervention
for dosing and scheduling. This view motivates a decision-aligned evaluation
blueprint across modality, scale, context and intervention, and emphasizes
function-space readouts such as pathway activity, spatial neighborhoods and
clinically relevant endpoints. We recommend operator-aware data design,
leakage-resistant partitions, and transparent calibration and reporting to
enable reproducible, like-for-like comparisons.

</details>


### [36] [ProtoSiTex: Learning Semi-Interpretable Prototypes for Multi-label Text Classification](https://arxiv.org/abs/2510.12534)
*Utsav Kumar Nareti,Suraj Kumar,Soumya Pandey,Soumi Chattopadhyay,Chandranath Adak*

Main category: cs.AI

TL;DR: ProtoSiTex is a new model for fine-grained multi-label text classification that achieves interpretability and state-of-the-art performance by leveraging prototype-based explanations.


<details>
  <summary>Details</summary>
Motivation: The growing volume of user-generated reviews necessitates models that provide detailed, interpretable insights beyond coarse granularities, addressing multi-label classification challenges.

Method: ProtoSiTex operates through an unsupervised prototype discovery phase and a supervised classification phase, using a hierarchical loss function for alignment while leveraging adaptive prototypes and multi-head attention for capturing semantics.

Result: ProtoSiTex outperforms state-of-the-art models on various datasets, demonstrating its efficacy in both performance and providing human-aligned, interpretable explanations.

Conclusion: ProtoSiTex offers a scalable and interpretable solution for fine-grained multi-label text classification, capable of explaining overlapping and multi-label semantics at different text granularities.

Abstract: The surge in user-generated reviews has amplified the need for interpretable
models that can provide fine-grained insights. Existing prototype-based models
offer intuitive explanations but typically operate at coarse granularity
(sentence or document level) and fail to address the multi-label nature of
real-world text classification. We propose ProtoSiTex, a semi-interpretable
framework designed for fine-grained multi-label text classification. ProtoSiTex
employs a dual-phase alternating training strategy: an unsupervised prototype
discovery phase that learns semantically coherent and diverse prototypes, and a
supervised classification phase that maps these prototypes to class labels. A
hierarchical loss function enforces consistency across sub-sentence, sentence,
and document levels, enhancing interpretability and alignment. Unlike prior
approaches, ProtoSiTex captures overlapping and conflicting semantics using
adaptive prototypes and multi-head attention. We also introduce a benchmark
dataset of hotel reviews annotated at the sub-sentence level with multiple
labels. Experiments on this dataset and two public benchmarks (binary and
multi-class) show that ProtoSiTex achieves state-of-the-art performance while
delivering faithful, human-aligned explanations, establishing it as a robust
solution for semi-interpretable multi-label text classification.

</details>


### [37] [Inclusive Fitness as a Key Step Towards More Advanced Social Behaviors in Multi-Agent Reinforcement Learning Settings](https://arxiv.org/abs/2510.12555)
*Andries Rosseau,Raphaël Avalos,Ann Nowé*

Main category: cs.AI

TL;DR: The paper introduces a novel multi-agent reinforcement learning framework using genotypes and inclusive fitness, aligning agent interactions with biological principles like Hamilton's rule.


<details>
  <summary>Details</summary>
Motivation: The paper aims to mimic natural evolution's intelligence development by introducing genotypic models and inclusive fitness into multi-agent systems.

Method: A reinforcement learning framework assigns genetic material to agents and uses an inclusive fitness reward structure to capture social dynamics and cooperation levels.

Result: Results show consistency with biological principles, potential emergence of a multi-agent strategy evolution akin to biological autocurriculum, and diverse social interactions beyond simple team-based structures.

Conclusion: The paper suggests that incorporating inclusive fitness may enhance strategic and socially intelligent agent behaviors in multi-agent systems.

Abstract: The competitive and cooperative forces of natural selection have driven the
evolution of intelligence for millions of years, culminating in nature's vast
biodiversity and the complexity of human minds. Inspired by this process, we
propose a novel multi-agent reinforcement learning framework where each agent
is assigned a genotype and where reward functions are modelled after the
concept of inclusive fitness. An agent's genetic material may be shared with
other agents, and our inclusive reward function naturally accounts for this. We
study the resulting social dynamics in two types of network games with
prisoner's dilemmas and find that our results align with well-established
principles from biology, such as Hamilton's rule. Furthermore, we outline how
this framework can extend to more open-ended environments with spatial and
temporal structure, finite resources, and evolving populations. We hypothesize
the emergence of an arms race of strategies, where each new strategy is a
gradual improvement over earlier adaptations of other agents, effectively
producing a multi-agent autocurriculum analogous to biological evolution. In
contrast to the binary team-based structures prevalent in earlier research, our
gene-based reward structure introduces a spectrum of cooperation ranging from
full adversity to full cooperativeness based on genetic similarity, enabling
unique non team-based social dynamics. For example, one agent having a mutual
cooperative relationship with two other agents, while the two other agents
behave adversarially towards each other. We argue that incorporating inclusive
fitness in agents provides a foundation for the emergence of more strategically
advanced and socially intelligent agents.

</details>


### [38] [HardcoreLogic: Challenging Large Reasoning Models with Long-tail Logic Puzzle Games](https://arxiv.org/abs/2510.12563)
*Jingcong Liang,Shijun Wan,Xuehai Wu,Siyuan Wang,Yitong Li,Qianglong Chen,Duyu Tang,Zhongyu Wei*

Main category: cs.AI

TL;DR: This paper introduces 'HardcoreLogic,' a benchmark of over 5,000 puzzles designed to test the reasoning capabilities of Large Reasoning Models (LRMs) on less familiar and challenging puzzles.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gaps in LRMs' ability to adapt to diverse and non-canonical logical puzzles, moving beyond reliance on memorization found in existing popular benchmarks like 9x9 Sudoku.

Method: HardcoreLogic modifies canonical puzzles via three dimensions: Increased Complexity (IC), Uncommon Elements (UE), and Unsolvable Puzzles (UP), creating a challenging dataset to rigorously test models.

Result: The study found significant performance drops among top-scoring LRMs on this benchmark, demonstrating their reliance on pre-learned patterns and difficulty with subtle rule variations.

Conclusion: HardcoreLogic highlights the limitations of LRMs in genuine reasoning, providing a valuable tool to push advancements in logical reasoning models.

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance on
complex tasks, including logical puzzle games that require deriving solutions
satisfying all constraints. However, whether they can flexibly apply
appropriate rules to varying conditions, particularly when faced with
non-canonical game variants, remains an open question. Existing corpora focus
on popular puzzles like 9x9 Sudoku, risking overfitting to canonical formats
and memorization of solution patterns, which can mask deficiencies in
understanding novel rules or adapting strategies to new variants. To address
this, we introduce HardcoreLogic, a challenging benchmark of over 5,000 puzzles
across 10 games, designed to test the robustness of LRMs on the "long-tail" of
logical games. HardcoreLogic systematically transforms canonical puzzles
through three dimensions: Increased Complexity (IC), Uncommon Elements (UE),
and Unsolvable Puzzles (UP), reducing reliance on shortcut memorization.
Evaluations on a diverse set of LRMs reveal significant performance drops, even
for models achieving top scores on existing benchmarks, indicating heavy
reliance on memorized stereotypes. While increased complexity is the dominant
source of difficulty, models also struggle with subtle rule variations that do
not necessarily increase puzzle difficulty. Our systematic error analysis on
solvable and unsolvable puzzles further highlights gaps in genuine reasoning.
Overall, HardcoreLogic exposes the limitations of current LRMs and establishes
a benchmark for advancing high-level logical reasoning.

</details>


### [39] [Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks](https://arxiv.org/abs/2510.12635)
*Yuxiang Zhang,Jiangming Shu,Ye Ma,Xueyuan Lin,Shangxi Wu,Jitao Sang*

Main category: cs.AI

TL;DR: The paper introduces a novel Memory-as-Action framework for large language models (LLMs), focusing on integrating memory editing as an intrinsic capability via reinforcement learning. This improves task reasoning and reduces computational costs.


<details>
  <summary>Details</summary>
Motivation: To address limitations in LLMs when dealing with long-context tasks, where constrained memory often leads to distractions from irrelevant context, by reframing working memory management as an intrinsic capability.

Method: They propose the Memory-as-Action framework, where memory editing is treated as part of the agent's policy, and introduce Dynamic Context Policy Optimization to handle trajectory fractures caused by these non-prefix memory edits.

Result: The approach enables better management of task reasoning and memory by integrating adaptive memory strategies. It leads to enhanced task performance and lower computational consumption.

Conclusion: End-to-end optimization of task reasoning and memory management in LLMs supports adaptive context curation, improving both efficiency and performance.

Abstract: Large Language Models face challenges in long-horizon agentic tasks as their
constrained memory is easily overwhelmed by distracting or irrelevant context.
Existing working memory methods typically rely on external, heuristic
mechanisms that are decoupled from the agent's core policy. In this work, we
reframe working memory management as a learnable, intrinsic capability. We
propose a novel framework, Memory-as-Action, where an agent actively manages
its working memory by executing explicit editing operations as part of a
unified policy. This formulation allows an agent, trained via reinforcement
learning, to balance memory curation against long-term task objectives under
given resource constraints. However, such memory editing actions break the
standard assumption of a continuously growing prefix in LLM interactions,
leading to what we call trajectory fractures. These non-prefix changes disrupt
the causal continuity required by standard policy gradient methods, making
those methods inapplicable. To address this, we propose a new algorithm,
Dynamic Context Policy Optimization, which enables stable end-to-end
reinforcement learning by segmenting trajectories at memory action points and
applying trajectory-level advantages to the resulting action segments. Our
results demonstrate that jointly optimizing for task reasoning and memory
management in an end-to-end fashion not only reduces overall computational
consumption but also improves task performance, driven by adaptive context
curation strategies tailored to the model's intrinsic capabilities.

</details>


### [40] [ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning](https://arxiv.org/abs/2510.12693)
*Hanyang Chen,Mark Zhao,Rui Yang,Qinwei Ma,Ke Yang,Jiarui Yao,Kangrui Wang,Hao Bai,Zhenhailong Wang,Rui Pan,Mengchao Zhang,Jose Barreiros,Aykut Onol,ChengXiang Zhai,Heng Ji,Manling Li,Huan Zhang,Tong Zhang*

Main category: cs.AI

TL;DR: This paper introduces Embodied Reasoning Agent (ERA), a two-stage framework integrating prior knowledge learning and reinforcement learning to enhance smaller vision-language models' embodied AI capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of smaller vision-language models and high deployment costs of large-scale models in embodied AI tasks.

Method: ERA integrates two stages: (1) Embodied Prior Learning from trajectory-augmented, environment-anchored, and external knowledge priors, and (2) Online Reinforcement Learning with designs like self-summarization, dense reward shaping, and turn-level policy optimization.

Result: ERA-3B outperformed prompting-based large models and training-based baselines, with 8.4% improvement on high-level planning tasks and 19.4% improvement on low-level control tasks, demonstrating superior generalization.

Conclusion: ERA offers a scalable and efficient path for developing embodied AI systems, combining knowledge learning and RL to enhance smaller VLM capabilities while achieving competitive performance.

Abstract: Recent advances in embodied AI highlight the potential of vision language
models (VLMs) as agents capable of perception, reasoning, and interaction in
complex environments. However, top-performing systems rely on large-scale
models that are costly to deploy, while smaller VLMs lack the necessary
knowledge and skills to succeed. To bridge this gap, we present
\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates
prior knowledge learning and online reinforcement learning (RL). The first
stage, \textit{Embodied Prior Learning}, distills foundational knowledge from
three types of data: (1) Trajectory-Augmented Priors, which enrich existing
trajectory data with structured reasoning generated by stronger models; (2)
Environment-Anchored Priors, which provide in-environment knowledge and
grounding supervision; and (3) External Knowledge Priors, which transfer
general knowledge from out-of-environment datasets. In the second stage, we
develop an online RL pipeline that builds on these priors to further enhance
agent performance. To overcome the inherent challenges in agent RL, including
long horizons, sparse rewards, and training instability, we introduce three key
designs: self-summarization for context management, dense reward shaping, and
turn-level policy optimization. Extensive experiments on both high-level
planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate
that ERA-3B surpasses both prompting-based large models and previous
training-based baselines. Specifically, it achieves overall improvements of
8.4\% on EB-ALFRED and 19.4\% on EB-Manipulation over GPT-4o, and exhibits
strong generalization to unseen tasks. Overall, ERA offers a practical path
toward scalable embodied intelligence, providing methodological insights for
future embodied AI systems.

</details>


### [41] [Multi-Agent Debate for LLM Judges with Adaptive Stability Detection](https://arxiv.org/abs/2510.12697)
*Tianyu Hu,Zhen Tan,Song Wang,Huaizhi Qu,Tianlong Chen*

Main category: cs.AI

TL;DR: The paper introduces a multi-agent debate judge framework enabling LLMs to collaboratively refine their judgments, outperforming traditional methods like majority voting and achieving higher accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods of using Large Language Models for automated evaluations rely on simplistic aggregation techniques, such as majority voting, which are prone to failure even with accurate individual answers. A more robust evaluation framework is needed.

Method: The proposed multi-agent debate judge framework enables LLM agents to collaboratively reason, iteratively refine their judgments, and detect stability in consensus dynamics. This is formalized mathematically and employs a time-varying Beta-Binomial mixture model with adaptive stopping based on distributional similarity using the Kolmogorov-Smirnov test.

Result: Experimental evaluations across benchmarks showed improved judgment accuracy over majority voting, with the framework maintaining computational efficiency.

Conclusion: Collaborative reasoning among LLMs enhances judgment accuracy, proving the superiority of debate-based mechanisms over static methods in automating evaluations.

Abstract: With advancements in reasoning capabilities, Large Language Models (LLMs) are
increasingly employed for automated judgment tasks. While LLMs-as-Judges offer
promise in automating evaluations, current approaches often rely on simplistic
aggregation methods (e.g., majority voting), which can fail even when
individual agents provide correct answers. To address this, we propose a
multi-agent debate judge framework where agents collaboratively reason and
iteratively refine their responses. We formalize the debate process
mathematically, analyzing agent interactions and proving that debate amplifies
correctness compared to static ensembles. To enhance efficiency, we introduce a
stability detection mechanism that models judge consensus dynamics via a
time-varying Beta-Binomial mixture, with adaptive stopping based on
distributional similarity (Kolmogorov-Smirnov test). This mechanism models the
judges' collective correct rate dynamics using a time-varying mixture of
Beta-Binomial distributions and employs an adaptive stopping criterion based on
distributional similarity (Kolmogorov-Smirnov statistic). Experiments across
multiple benchmarks and models demonstrate that our framework improves judgment
accuracy over majority voting while maintaining computational efficiency.

</details>


### [42] [CAMNet: Leveraging Cooperative Awareness Messages for Vehicle Trajectory Prediction](https://arxiv.org/abs/2510.12703)
*Mattia Grasselli,Angelo Porrello,Carlo Augusto Grazia*

Main category: cs.AI

TL;DR: The paper explores vehicle-to-vehicle communication using Cooperative Awareness Messages (CAMs) to enhance vehicle trajectory prediction, utilizing a graph neural network (CAMNet) and yielding promising results.


<details>
  <summary>Details</summary>
Motivation: Current autonomous driving systems face limitations in situational awareness due to sensor occlusions. This paper aims to assess whether vehicle-to-vehicle communication via CAMs can mitigate such limitations.

Method: The authors designed and trained a neural network (CAMNet) on a motion forecasting dataset and evaluated it using a custom dataset derived from Cooperative Awareness Messages.

Result: Results indicate that CAMNet effectively utilizes CAM data for vehicle trajectory prediction and supports the idea that CAMs can enhance situational awareness.

Conclusion: CAMNet demonstrates the utility of CAM-based communication for trajectory prediction but also reveals limitations that offer areas for future research.

Abstract: Autonomous driving remains a challenging task, particularly due to safety
concerns. Modern vehicles are typically equipped with expensive sensors such as
LiDAR, cameras, and radars to reduce the risk of accidents. However, these
sensors face inherent limitations: their field of view and line of sight can be
obstructed by other vehicles, thereby reducing situational awareness. In this
context, vehicle-to-vehicle communication plays a crucial role, as it enables
cars to share information and remain aware of each other even when sensors are
occluded. One way to achieve this is through the use of Cooperative Awareness
Messages (CAMs). In this paper, we investigate the use of CAM data for vehicle
trajectory prediction. Specifically, we design and train a neural network,
Cooperative Awareness Message-based Graph Neural Network (CAMNet), on a widely
used motion forecasting dataset. We then evaluate the model on a second dataset
that we created from scratch using Cooperative Awareness Messages, in order to
assess whether this type of data can be effectively exploited. Our approach
demonstrates promising results, showing that CAMs can indeed support vehicle
trajectory prediction. At the same time, we discuss several limitations of the
approach, which highlight opportunities for future research.

</details>


### [43] [Towards Robust Artificial Intelligence: Self-Supervised Learning Approach for Out-of-Distribution Detection](https://arxiv.org/abs/2510.12713)
*Wissam Salhab,Darine Ameyed,Hamid Mcheick,Fehmi Jaafar*

Main category: cs.AI

TL;DR: This paper introduces a method to improve out-of-distribution (OOD) detection in AI systems using self-supervised learning and graph-theoretical techniques, achieving an AUROC of 0.99.


<details>
  <summary>Details</summary>
Motivation: Robustness in AI systems is essential for safety-critical applications like autonomous vehicles and healthcare, where failure to handle OOD samples can result in severe consequences.

Method: The approach uses self-supervised learning to extract representations from unlabeled data and combines it with graph-theoretical techniques for efficient detection and classification of OOD samples.

Result: The proposed method significantly improves OOD detection efficiency, demonstrated by achieving a high AUROC score of 0.99.

Conclusion: Enhancing robustness through improved OOD detection without labeled data makes AI systems safer and more reliable, particularly for critical fields and applications.

Abstract: Robustness in AI systems refers to their ability to maintain reliable and
accurate performance under various conditions, including out-of-distribution
(OOD) samples, adversarial attacks, and environmental changes. This is crucial
in safety-critical systems, such as autonomous vehicles, transportation, or
healthcare, where malfunctions could have severe consequences. This paper
proposes an approach to improve OOD detection without the need of labeled data,
thereby increasing the AI systems' robustness. The proposed approach leverages
the principles of self-supervised learning, allowing the model to learn useful
representations from unlabeled data. Combined with graph-theoretical
techniques, this enables the more efficient identification and categorization
of OOD samples. Compared to existing state-of-the-art methods, this approach
achieved an Area Under the Receiver Operating Characteristic Curve (AUROC) =
0.99.

</details>


### [44] [Clutch Control: An Attention-based Combinatorial Bandit for Efficient Mutation in JavaScript Engine Fuzzing](https://arxiv.org/abs/2510.12732)
*Myles Foley,Sergio Maffeis,Muhammad Fakhrur Rozi,Takeshi Takahashi*

Main category: cs.AI

TL;DR: This paper introduces CLUTCH, a deep combinatorial bandit method for improved JavaScript fuzzing, enhancing test case validity and coverage compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: JavaScript engine security concerns have driven the need for better targeted fuzzing techniques that optimize mutation targeting in test cases.

Method: The proposed method, CLUTCH, integrates deep combinatorial bandits with attention mechanisms and Concrete Dropout to improve mutation targeting in JavaScript test cases.

Result: CLUTCH demonstrates increased efficiency in JavaScript fuzzing, with 20.3% more valid test cases and 8.9% better coverage-per-testcase, outperforming state-of-the-art bandit methods in volatile and combinatorial settings.

Conclusion: CLUTCH is a significant advancement for JavaScript fuzzing, providing higher efficiency and lower regret in challenging mutation targeting scenarios.

Abstract: JavaScript engines are widely used in web browsers, PDF readers, and
server-side applications. The rise in concern over their security has led to
the development of several targeted fuzzing techniques. However, existing
approaches use random selection to determine where to perform mutations in
JavaScript code. We postulate that the problem of selecting better mutation
targets is suitable for combinatorial bandits with a volatile number of arms.
Thus, we propose CLUTCH, a novel deep combinatorial bandit that can observe
variable length JavaScript test case representations, using an attention
mechanism from deep learning. Furthermore, using Concrete Dropout, CLUTCH can
dynamically adapt its exploration. We show that CLUTCH increases efficiency in
JavaScript fuzzing compared to three state-of-the-art solutions by increasing
the number of valid test cases and coverage-per-testcase by, respectively,
20.3% and 8.9% on average. In volatile and combinatorial settings we show that
CLUTCH outperforms state-of-the-art bandits, achieving at least 78.1% and 4.1%
less regret in volatile and combinatorial settings, respectively.

</details>


### [45] [CTRL-Rec: Controlling Recommender Systems With Natural Language](https://arxiv.org/abs/2510.12742)
*Micah Carroll,Adeline Foote,Kevin Feng,Marcus Williams,Anca Dragan,W. Bradley Knox,Smitha Milli*

Main category: cs.AI

TL;DR: CTRL-Rec integrates LLMs with traditional recommendation systems, enabling users to adjust suggestions via natural language requests, providing real-time control and satisfaction.


<details>
  <summary>Details</summary>
Motivation: Users often find existing recommendation systems restrictive and lack options for precise customization, motivating the need for more personalized control mechanisms.

Method: CTRL-Rec trains embedding models based on LLM-simulated user judgments from natural language requests, integrates these predictions into traditional systems, and ensures computational efficiency for real-time adjustments.

Result: CTRL-Rec successfully demonstrated its effectiveness in experiments with the MovieLens dataset and was well-received by Letterboxd users who appreciated the enhanced control and satisfaction it provided.

Conclusion: The proposed method offers a practical and efficient approach to incorporating user-driven customization in recommender systems, greatly improving user experience and control.

Abstract: When users are dissatisfied with recommendations from a recommender system,
they often lack fine-grained controls for changing them. Large language models
(LLMs) offer a solution by allowing users to guide their recommendations
through natural language requests (e.g., "I want to see respectful posts with a
different perspective than mine"). We propose a method, CTRL-Rec, that allows
for natural language control of traditional recommender systems in real-time
with computational efficiency. Specifically, at training time, we use an LLM to
simulate whether users would approve of items based on their language requests,
and we train embedding models that approximate such simulated judgments. We
then integrate these user-request-based predictions into the standard weighting
of signals that traditional recommender systems optimize. At deployment time,
we require only a single LLM embedding computation per user request, allowing
for real-time control of recommendations. In experiments with the MovieLens
dataset, our method consistently allows for fine-grained control across a
diversity of requests. In a study with 19 Letterboxd users, we find that
CTRL-Rec was positively received by users and significantly enhanced users'
sense of control and satisfaction with recommendations compared to traditional
controls.

</details>


### [46] [Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics](https://arxiv.org/abs/2510.12787)
*Marco Del Tredici,Jacob McCarran,Benjamin Breen,Javier Aspuru Mijares,Weichen Winston Yin,Jacob M. Taylor,Frank Koppens,Dirk Englund*

Main category: cs.AI

TL;DR: The paper introduces Ax-Prover, a system built around Large Language Models (LLMs) integrated with Lean tools via the Model Context Protocol (MCP), excelling in automated theorem proving across various domains.


<details>
  <summary>Details</summary>
Motivation: Automated theorem proving requires systems capable of handling the creativity of reasoning while ensuring syntactic rigor, aiming to assist experts and ensure correctness across diverse scientific domains.

Method: Ax-Prover integrates LLMs with Lean tools via the Model Context Protocol to generate formal proofs. Evaluations include public math benchmarks and introduced Lean benchmarks in abstract algebra and quantum theory.

Result: Ax-Prover competes effectively with state-of-the-art systems on existing benchmarks and significantly outperforms them on new scientific benchmarks. It also demonstrates practical application by helping an expert formalize a complex cryptography theorem.

Conclusion: Ax-Prover offers a generalized and robust methodology for formal verification, functioning autonomously or collaboratively, with promising results across mathematical and scientific domains.

Abstract: We present Ax-Prover, a multi-agent system for automated theorem proving in
Lean that can solve problems across diverse scientific domains and operate
either autonomously or collaboratively with human experts. To achieve this,
Ax-Prover approaches scientific problem solving through formal proof
generation, a process that demands both creative reasoning and strict syntactic
rigor. Ax-Prover meets this challenge by equipping Large Language Models
(LLMs), which provide knowledge and reasoning, with Lean tools via the Model
Context Protocol (MCP), which ensure formal correctness. To evaluate its
performance as an autonomous prover, we benchmark our approach against frontier
LLMs and specialized prover models on two public math benchmarks and on two
Lean benchmarks we introduce in the fields of abstract algebra and quantum
theory. On public datasets, Ax-Prover is competitive with state-of-the-art
provers, while it largely outperform them on the new benchmarks. This shows
that, unlike specialized systems that struggle to generalize, our tool-based
agentic theorem prover approach offers a generalizable methodology for formal
verification across diverse scientific domains. Furthermore, we demonstrate
Ax-Prover's assistant capabilities in a practical use case, showing how it
enabled an expert mathematician to formalize the proof of a complex
cryptography theorem.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [47] [A Direct Memory Access Controller (DMAC) for Irregular Data Transfers on RISC-V Linux Systems](https://arxiv.org/abs/2510.12277)
*Thomas Benz,Axel Vanoni,Michael Rogenmoser,Luca Benini*

Main category: cs.AR

TL;DR: This paper proposes an optimized descriptor-based DMAC to efficiently handle small-size memory transfers, improving latency, bus utilization, and resource usage.


<details>
  <summary>Details</summary>
Motivation: Increasing heterogeneity in computing systems and demands from machine learning applications require more efficient memory transfer solutions due to inefficiencies in classical DMAC designs.

Method: The paper introduces a lightweight descriptor format and speculative descriptor prefetching scheme in an AXI4-based DMAC. This is integrated into a RISC-V SoC and evaluated on a Kintex FPGA.

Result: Achieved 1.66x less latency for transfers, up to 2.5x higher bus utilization, and reduced hardware resource usage (11% fewer lookup tables, 23% fewer flip-flops). Extended bus utilization to 3.6x in deep memory systems. Synthesis results show clock frequency of 1.44 GHz with a small area of 49.5 kGE.

Conclusion: The proposed DMAC offers significant improvements in performance and resource efficiency while addressing inefficiencies in handling small-size memory transfers. It is suitable for high-performance and resource-constrained systems.

Abstract: With the ever-growing heterogeneity in computing systems, driven by modern
machine learning applications, pressure is increasing on memory systems to
handle arbitrary and more demanding transfers efficiently. Descriptor-based
direct memory access controllers (DMACs) allow such transfers to be executed by
decoupling memory transfers from processing units. Classical descriptor-based
DMACs are inefficient when handling arbitrary transfers of small unit sizes.
Excessive descriptor size and the serialized nature of processing descriptors
employed by the DMAC lead to large static overheads when setting up transfers.
To tackle this inefficiency, we propose a descriptor-based DMAC optimized to
efficiently handle arbitrary transfers of small unit sizes. We implement a
lightweight descriptor format in an AXI4-based DMAC. We further increase
performance by implementing a low-overhead speculative descriptor prefetching
scheme without additional latency penalties in the case of a misprediction. Our
DMAC is integrated into a 64-bit Linux-capable RISC-V SoC and emulated on a
Kintex FPGA to evaluate its performance. Compared to an off-the-shelf
descriptor-based DMAC IP, we achieve 1.66x less latency launching transfers,
increase bus utilization up to 2.5x in an ideal memory system with
64-byte-length transfers while requiring 11% fewer lookup tables, 23% fewer
flip-flops, and no block RAMs. We can extend our lead in bus utilization to
3.6x with 64-byte-length transfers in deep memory systems. We synthesized our
DMAC in GlobalFoundries' GF12LP+ node, achieving a clock frequency of over 1.44
GHz while occupying only 49.5 kGE.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [48] [PHANTOM RECALL: When Familiar Puzzles Fool Smart Models](https://arxiv.org/abs/2510.11812)
*Souradeep Mukhopadhyay,Rishabh Baral,Nimeesh Mahajan,Samhitha Harish,Aswin RRV,Mihir Parmar,Mutsumi Nakamura,Chitta Baral*

Main category: cs.CL

TL;DR: This paper examines the reasoning capability of large language models (LLMs) such as GPT and evaluates their performance on logic puzzles, revealing significant limitations in adapting to slightly modified problems.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenge of assessing whether LLMs genuinely reason through logic puzzles or rely on memorization, and to examine their fragility when faced with modified problems.

Method: The authors introduced the PHANTOM RECALL benchmark, which includes 25 logic puzzles and 149 modified versions. They evaluated 11 LLMs, identified recurring failure modes, and developed tools such as a logical-equivalence judge, a taxonomy of reasoning errors, and a mitigation framework.

Result: The analysis shows that while LLMs perform well on unmodified puzzles, they fail significantly on perturbed versions due to a phenomenon called "phantom recall," where they employ memorized solutions unfit for the changes.

Conclusion: The study highlights a critical limitation in LLM reasoning, emphasizing that these models struggle to adapt their reasoning when contextual details shift, thus revealing the gap between language fluency and true logical understanding.

Abstract: Large language models (LLMs) such as GPT, Gemini, and Claude often appear
adept at solving classic logic puzzles--but how much genuine reasoning
underlies their answers? Recent evidence suggests that these models frequently
rely on memorized templates rather than reasoning from first principles. When
puzzles are slightly modified, their performance collapses, revealing a
striking fragility. In particular, we asked: Have LLMs addressed these issues?
To what extent? How about perturbations to other puzzles? Is there a general
way of reformulating the prompt so that the models do better? To examine these
things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25
well-known logic puzzles and 149 carefully designed perturbations that preserve
reasoning structure but alter superficial details and solutions. We evaluate
eleven leading LLMs and identify a recurring failure mode--phantom
recall--where models confidently reproduce memorized solutions or spurious
rationales that no longer fit the altered scenario. To probe and mitigate this
issue, we contribute three tools: (i) an automated logical-equivalence judge to
detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error
categories, and (iii) a prompting-based mitigation framework guided by these
categories. Despite near-perfect accuracy on unmodified puzzles, models
significantly underperform humans on perturbed ones, exhibiting both phantom
recall and over-elaboration. Our findings reveal a crucial limitation: LLMs
often fail to re-reason when contextual cues shift--highlighting the gap
between linguistic fluency and logical understanding.

</details>


### [49] [R-WoM: Retrieval-augmented World Model For Computer-use Agents](https://arxiv.org/abs/2510.11892)
*Kai Mei,Jiang Guo,Shuaichen Chang,Mingwen Dong,Dongkyu Lee,Xing Niu,Jiarong Jiang*

Main category: cs.CL

TL;DR: The paper explores the use of Large Language Models (LLMs) as world models for decision-making in digital environments, highlighting their limitations in long-term simulations and proposing an improved Retrieval-augmented World Model (R-WoM).


<details>
  <summary>Details</summary>
Motivation: Investigate whether LLMs can serve as reliable world models for predictive tasks like simulating future states and outcomes, despite their deficiencies such as hallucination and static knowledge.

Method: The paper systematically probes LLM capabilities in world modeling tasks like next-state prediction, full-procedure planning, and milestone recognition, and introduces R-WoM, which enhances LLMs with external, factual knowledge for simulations.

Result: Experiments demonstrate that R-WoM significantly improves LLM performance in long-term simulation tasks, with up to 25.3% improvement in OSWorld and 18.1% in WebArena compared to baselines.

Conclusion: LLMs have potential in short-term world modeling but falter in long-term tasks due to knowledge and reasoning limits. R-WoM effectively addresses these gaps, making LLMs more viable for extended simulations.

Abstract: Large Language Models (LLMs) can serve as world models to enhance agent
decision-making in digital environments by simulating future states and
predicting action outcomes, potentially eliminating costly trial-and-error
exploration. However, this capability is fundamentally limited by LLMs'
tendency toward hallucination and their reliance on static training knowledge,
which can lead to compounding errors that inhibit long-horizon simulations. To
systematically investigate whether LLMs are appropriate for world modeling, we
probe two core capabilities of world models--future state prediction and reward
estimation--through three tasks: next-state identification, full-procedure
planning alignment, and milestone transition recognition. Our analysis shows
that while LLMs effectively capture immediate next states and identify
meaningful state transitions, their performance rapidly degrades in
full-procedure planning. This highlights LLMs' limitations in reliably modeling
environment dynamics over long horizons. To address these limitations, we
propose the Retrieval-augmented World Model (R-WoM), which grounds LLM
simulations by incorporating factual, up-to-date knowledge retrieved from
external tutorials. Experiments show that R-WoM achieves substantial
improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to
baselines, with particular advantages in longer-horizon simulations.

</details>


### [50] [LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance](https://arxiv.org/abs/2510.11905)
*Patrick Haller,Mark Ibrahim,Polina Kirichenko,Levent Sagun,Samuel J. Bell*

Main category: cs.CL

TL;DR: The paper investigates whether the brittleness of LLM performance is caused by unstable internal knowledge representations, revealing that LLM truthfulness recognition is sensitive to superficial input variations.


<details>
  <summary>Details</summary>
Motivation: To address the observed brittleness in LLMs, which often fail to apply their knowledge in diverse, unseen settings unlike their training data.

Method: The study evaluates representation separability under semantically-preserving perturbations that drive statements out-of-distribution across multiple LLM families, datasets, and probing methods.

Result: The robustness of LLM internal representations degrades significantly under OOD transformations, indicating reliance on shallow knowledge structures sensitive to exact surface forms.

Conclusion: LLMs exhibit non-robust knowledge representations, limiting their generalizability and calling for research to enhance representation stability and robustness.

Abstract: For Large Language Models (LLMs) to be reliable, they must learn robust
knowledge that can be generally applied in diverse settings -- often unlike
those seen during training. Yet, extensive research has shown that LLM
performance can be brittle, with models exhibiting excessive sensitivity to
trivial input variations. In this work, we explore whether this brittleness is
a direct result of unstable internal knowledge representations. To explore this
question, we build on previous work showing that LLM representations encode
statement truthfulness -- i.e., true, factual statements can be easily
separated from false, inaccurate ones. Specifically, we test the robustness of
learned knowledge by evaluating representation separability on samples that
have undergone superficial transformations to drive them out-of-distribution
(OOD), such as typos or reformulations. By applying semantically-preserving
perturbations, we study how separability degrades as statements become more
OOD, across four LLM families, five evaluation datasets, and three knowledge
probing methods. Our results reveal that internal representations of statement
truthfulness collapse as the samples' presentations become less similar to
those seen during pre-training. While LLMs can often distinguish between true
and false statements when they closely resemble the pre-training data, this
ability is highly dependent on the statement's exact surface form. These
findings offer a possible explanation for brittle benchmark performance: LLMs
may learn shallow, non-robust knowledge representations that allow for only
limited generalizability. Our work presents a fundamental challenge for the
utility of truthfulness probes, and more broadly, calls for further research on
improving the robustness of learned knowledge representations.

</details>


### [51] [LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens](https://arxiv.org/abs/2510.11919)
*Armel Zebaze,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: The study evaluates the utility of "thinking tokens" for machine translation in large reasoning models and concludes that they mostly do not enhance performance unless constructed through modular translation-specific prompting strategies.


<details>
  <summary>Details</summary>
Motivation: To explore whether intermediate reasoning processes, represented as "thinking tokens," can improve machine translation across various languages and setups.

Method: The researchers tested the impact of intermediate tokens by comparing standard input-output fine-tuning, fine-tuning with synthetic chain-of-thought explanations, and modular translation-specific prompting strategies.

Result: "Thinking tokens" generally fail to boost machine translation performance unless designed through modular translation-specific methods.

Conclusion: Using reasoning tokens is not universally beneficial for machine translation, and alternative strategies like teacher-guided refinement or parallel corpus expansion yield greater improvements.

Abstract: Large reasoning models (LRMs) have led to new possibilities in terms of
problem-solving, through the devising of a natural language thought process
prior to answering a query. While their capabilities are well known across
mathematics and coding tasks, their impact on the task of machine translation
(MT) remains underexplored. In this work, we explore the benefits of the
generation of intermediate tokens when performing MT across multiple language
pairs of different levels of resourcedness and multiple setups. We find that
"thinking tokens" do not help LRMs better perform MT. This result generalizes
to models fine-tuned to reason before translating using distilled chain of
thought (CoT) inspired by human translators' practices. Specifically,
fine-tuning a model with synthetic CoT explanations detailing how to translate
step-by-step does not outperform standard input-output fine-tuning. However,
constructing the intermediate tokens by combining the outputs of modular
translation-specific prompting strategies results in improvements. Our findings
underscore that the contribution of intermediate tokens during fine-tuning
highly depends on the presence of translation attempts within them. More
broadly, our results suggest that using a teacher to refine target translations
or to expand parallel corpora is more impactful than distilling their CoT
explanations into "thinking" MT models.

</details>


### [52] [Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering](https://arxiv.org/abs/2510.11928)
*Lorena Calvo-Bartolomé,Valérie Aldana,Karla Cantarero,Alonso Madroñal de Mesa,Jerónimo Arenas-García,Jordan Boyd-Graber*

Main category: cs.CL

TL;DR: The paper proposes MIND, a user-in-the-loop pipeline for detecting factual and cultural inconsistencies in multilingual question-answering (QA) knowledge bases.


<details>
  <summary>Details</summary>
Motivation: Ensure factual consistency across languages in multilingual QA systems while handling cultural variations in responses.

Method: Developed MIND, a user-in-the-loop fact-checking pipeline that identifies discrepancies in multilingual QA knowledge bases with a focus on both factual and culturally sensitive questions.

Result: MIND effectively detects inconsistencies in a bilingual QA system within the maternal and infant health domain and generalizes well across datasets from other domains.

Conclusion: MIND enables the creation of culturally sensitive and factually consistent QA systems by reliably identifying and addressing inconsistencies.

Abstract: Multilingual question answering (QA) systems must ensure factual consistency
across languages, especially for objective queries such as What is jaundice?,
while also accounting for cultural variation in subjective responses. We
propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and
cultural discrepancies in multilingual QA knowledge bases. MIND highlights
divergent answers to culturally sensitive questions (e.g., Who assists in
childbirth?) that vary by region and context. We evaluate MIND on a bilingual
QA system in the maternal and infant health domain and release a dataset of
bilingual questions annotated for factual and cultural inconsistencies. We
further test MIND on datasets from other domains to assess generalization. In
all cases, MIND reliably identifies inconsistencies, supporting the development
of more culturally aware and factually consistent QA systems.

</details>


### [53] [TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition](https://arxiv.org/abs/2510.11944)
*Yupei Li,Philipp Borchert,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: This paper introduces TopoAlign, a framework for leveraging widely available code repositories to improve the training of Math Large Language Models (LLMs) on autoformalization tasks, achieving notable performance improvements.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of autoformalization, which is transforming informal mathematical reasoning into formal statements. This task is important as it connects informal mathematical LLM outputs with formal proof assistants, ensuring machine-verifiable results while reducing hallucinations. The scarcity of large-scale paired data for informal and formal statements motivates the need for alternative training approaches.

Method: TopoAlign is proposed as a framework that decomposes code into structuring components such as docstrings, main functions, and dependencies, and reassembles them to mimic formal mathematical constructs. This technique enables the use of existing code repositories to train Math LLMs without requiring additional human annotation.

Result: Using TopoAlign, the paper reports significant performance improvements for their Math LLMs (DeepSeek-Math and Herald). Specifically, DeepSeek-Math achieved a 17.77% improvement on BEq@10 and 68.82% improvement on typecheck@10. Similarly, gains were observed for the specialized model Herald with 0.12% improvement on BEq@10 and 1.09% improvement on typecheck@10.

Conclusion: TopoAlign demonstrates the utility of leveraging structural code alignment for making existing code repositories usable for training Math LLMs. This approach provides significant performance gains even for specialized models and contributes toward the advancement of autoformalization techniques.

Abstract: Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)
mathematical reasoning but still struggle with autoformalisation, the task of
transforming informal into formal mathematical statements. Autoformalisation
helps pair the informal reasoning of LLMs with formal proof assistants which
enable machine-verifiable generation and mitigate hallucinations. Yet, the
performance of current Math LLMs is constrained by the scarcity of large-scale
corpora, particularly those containing pairs of informal and formal statements.
Although current models are trained to generate code from natural language
instructions, structural and syntactic differences between these and formal
mathematics limit effective transfer learning. We propose TopoAlign, a
framework that unlocks widely available code repositories as training resources
for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and
dependency functions, and reassembles these components into analogues that
structurally mirror formal statements. This produces structurally aligned code
data that can be used for training Math LLMs without requiring additional human
annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and
evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign
provides substantial gains for DeepSeek-Math, improving performance by 17.77%
on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical
knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10
and typecheck@10, respectively, demonstrating that training on aligned code
data is beneficial even for specialized models.

</details>


### [54] [GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences](https://arxiv.org/abs/2510.11952)
*Priyanka Dey,Daniele Rosa,Wenqing Zheng,Daniel Barcklow,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

TL;DR: The paper proposes GRAVITY, a framework generating synthetic profile-grounded data for LLM personalization and demonstrates its effectiveness in creating user-centered content across different cultures.


<details>
  <summary>Details</summary>
Motivation: Existing personalization in LLMs is constrained by costly human feedback and lacks deeper insights into user attributes such as interests, values, and personality traits.

Method: The authors developed GRAVITY, which uses psychological and cultural frameworks like Big Five traits and Hofstede’s dimensions, to generate synthetic user preference data that guides content creation.

Result: GRAVITY improved personalized content generation with over 4% higher preference gains across baselines and was preferred in 86% of user studies, evaluated across cultures.

Conclusion: Scenario-grounded synthetic data effectively enhances personalization in LLMs, reduces the need for human annotation, and scales personalization efforts while maintaining user engagement.

Abstract: Personalization in LLMs often relies on costly human feedback or interaction
logs, limiting scalability and neglecting deeper user attributes. To reduce the
reliance on human annotations, we introduce GRAVITY (Generative Response with
Aligned Values, Interests, and Traits of You), a framework for generating
synthetic, profile-grounded preference data that captures users' interests,
values, beliefs, and personality traits. By integrating demographic, cultural,
and psychological frameworks -- including Hofstede's cultural dimensions,
Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits --
GRAVITY synthesizes preference pairs to guide personalized content generation.
We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to
prompt-based conditioning, standard fine-tuning, and naive synthetic pair
generation. Profile-grounded synthetic data consistently improves generation,
especially across multiple cultures (USA, Brazil, Japan, India), achieving over
4% higher preference gains across baselines, with user studies showing that
GRAVITY outputs are preferred over 86% of the time. Our results show that
scenario-grounded synthetic data can capture richer user variation, reduce
reliance on costly annotation, and produce more engaging, user-centered
content, offering a scalable path for LLM personalization.

</details>


### [55] [Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries](https://arxiv.org/abs/2510.11956)
*Gabrielle Kaili-May Liu,Bryan Li,Arman Cohan,William Gantt Walden,Eugene Yang*

Main category: cs.CL

TL;DR: The paper introduces a pipeline for generating challenging benchmark queries to improve the evaluation of retrieval-augmented generation (RAG) systems.


<details>
  <summary>Details</summary>
Motivation: Existing RAG benchmarks fail to adequately test systems on complex queries, multi-hop reasoning, and out-of-scope questions, which results in limitations being overlooked.

Method: The authors developed a pipeline to create uncheatable, realistic, unanswerable, and multi-hop queries (CRUMQs) that can be applied to any corpus or domain.

Result: Experiments on leading RAG systems demonstrated that CRUMQs significantly increase difficulty, reducing cheatability scores by up to 81.0% compared to existing benchmarks.

Conclusion: The proposed CRUMQs pipeline enhances benchmark realism and difficulty, encouraging the development of more advanced and capable RAG systems.

Abstract: Real-world use cases often present RAG systems with complex queries for which
relevant information is missing from the corpus or is incomplete. In these
settings, RAG systems must be able to reject unanswerable, out-of-scope queries
and identify failures of retrieval and multi-hop reasoning. Despite this,
existing RAG benchmarks rarely reflect realistic task complexity for multi-hop
or out-of-scope questions, which often can be cheated via disconnected
reasoning (i.e., solved without genuine multi-hop inference) or require only
simple factual recall. This limits the ability for such benchmarks to uncover
limitations of existing RAG systems. To address this gap, we present the first
pipeline for automatic, difficulty-controlled creation of
un$\underline{c}$heatable, $\underline{r}$ealistic, $\underline{u}$nanswerable,
and $\underline{m}$ulti-hop $\underline{q}$uerie$\underline{s}$ (CRUMQs),
adaptable to any corpus and domain. We use our pipeline to create CRUMQs over
two popular RAG datasets and demonstrate its effectiveness via benchmark
experiments on leading retrieval-augmented LLMs. Results show that compared to
prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve
up to 81.0\% reduction in cheatability scores. More broadly, our pipeline
offers a simple way to enhance benchmark difficulty and realism and drive
development of more capable RAG systems.

</details>


### [56] [Direct Multi-Token Decoding](https://arxiv.org/abs/2510.11958)
*Xuan Luo,Weizhi Wang,Xifeng Yan*

Main category: cs.CL

TL;DR: The paper proposes Direct Multi-Token Decoding (DMTD), a paradigm allowing late layers of transformers to generate multiple tokens from processed representations, achieving faster inference speeds.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize the efficiency of large language models (LLMs) by reducing computational redundancies during token generation without compromising performance.

Method: This approach eliminates repeated traversals of early and middle layers by utilizing the fully processed hidden states for multi-token generation directly in late layers.

Result: The fine-tuned DMTD model (Qwen3-4B) achieved up to 2x speed improvements in token generation with minimal performance degradation and showed potential for scaling with larger datasets.

Conclusion: DMTD introduces a novel inference method that enhances LLM efficiency, presenting a promising alternative to traditional decoding approaches and offering scalable gains in larger setups.

Abstract: Decoder-only transformers have become the standard architecture for large
language models (LLMs) due to their strong performance. Recent studies suggest
that, in pre-trained LLMs, early, middle, and late layers may serve distinct
roles: Early layers focus on understanding the input context, middle layers
handle task-specific processing, and late layers convert abstract
representations into output tokens. We hypothesize that once representations
have been processed by the early and middle layers, the resulting hidden states
may encapsulate sufficient information to support the generation of multiple
tokens using only the late layers, eliminating the need to repeatedly traverse
the early and middle layers. We refer to this inference paradigm as Direct
Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces
no additional parameters, auxiliary routines, or post-generation verification.
Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model
has already demonstrated promising results, achieving up to a 2x speedup with
only minor performance loss. Moreover, as shown in our scaling analysis, its
performance is expected to further improve with larger training datasets.

</details>


### [57] [Scaling Long-Horizon LLM Agent via Context-Folding](https://arxiv.org/abs/2510.11967)
*Weiwei Sun,Miao Lu,Zhan Ling,Kang Liu,Xuesong Yao,Yiming Yang,Jiecao Chen*

Main category: cs.CL

TL;DR: Introduces Context-Folding, enabling language model agents to manage working context efficiently on long-horizon tasks through task decomposition and context summarization.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of large language models constrained by context length when solving long-horizon tasks.

Method: Development of Context-Folding, including reinforcement learning framework FoldGRPO with task decomposition and process rewards.

Result: The folding agent matches or outperforms baselines using significantly smaller active context and surpasses summarization-based context management models on complex tasks.

Conclusion: Context-Folding effectively enhances long-horizon task performance by optimizing context management without compromising efficiency.

Abstract: Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

</details>


### [58] [Conjecturing: An Overlooked Step in Formal Mathematical Reasoning](https://arxiv.org/abs/2510.11986)
*Jasivan Alex Sivakumar,Philipp Borchert,Ronald Cardenas,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: This paper introduces ConjectureBench, a benchmark for evaluating LLM conjecturing abilities, and proposes an inference-time method (Lean-FIRe) to improve conjecturing and autoformalisation, achieving notable results on mathematical problems.


<details>
  <summary>Details</summary>
Motivation: Autoformalisation of mathematical statements in formal language is hindered by the conjecturing step, which is often overlooked but crucial for certain problems. Existing LLMs show limited success and understanding of conjecturing.

Method: The authors created ConjectureBench to evaluate conjecturing independently, assessed foundational LLMs, and developed Lean-FIRe to enhance conjecturing and autoformalisation capabilities.

Result: The paper finds that LLMs' autoformalisation abilities are commonly overestimated due to overlooked conjecturing, and demonstrates improvements using Lean-FIRe, achieving end-to-end autoformalisation for 13 problems with GPT-4.1 and 7 with DeepSeek-V3.1.

Conclusion: Conjecturing must be recognized as a distinct task to improve autoformalisation. The creation of ConjectureBench highlights this gap, and future research should focus on integrating improved conjecturing methods within the autoformalisation pipeline.

Abstract: Autoformalisation, the task of expressing informal mathematical statements in
formal language, is often viewed as a direct translation process. This,
however, disregards a critical preceding step: conjecturing. Many mathematical
problems cannot be formalised directly without first conjecturing a conclusion
such as an explicit answer, or a specific bound. Since Large Language Models
(LLMs) already struggle with autoformalisation, and the evaluation of their
conjecturing ability is limited and often entangled within autoformalisation or
proof, it is particularly challenging to understand its effect. To address this
gap, we augment existing datasets to create ConjectureBench, and redesign the
evaluation framework and metric specifically to measure the conjecturing
capabilities of LLMs both as a distinct task and within the autoformalisation
pipeline. Our evaluation of foundational models, including GPT-4.1 and
DeepSeek-V3.1, reveals that their autoformalisation performance is
substantially overestimated when the conjecture is accounted for during
evaluation. However, the conjecture should not be assumed to be provided. We
design an inference-time method, Lean-FIRe to improve conjecturing and
autoformalisation, which, to the best of our knowledge, achieves the first
successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1
and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite
knowledge to generate accurate conjectures, improving autoformalisation
performance requires treating conjecturing as an independent task, and
investigating further how to correctly integrate it within autoformalisation.
Finally, we provide forward-looking guidance to steer future research toward
improving conjecturing, an overlooked step of formal mathematical reasoning.

</details>


### [59] [SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation](https://arxiv.org/abs/2510.11997)
*Ryan Shea,Yunan Lu,Liang Qiu,Zhou Yu*

Main category: cs.CL

TL;DR: SAGE is a new framework for evaluating interactive agents by simulating realistic user behavior based on business-specific knowledge rather than generic user modeling.


<details>
  <summary>Details</summary>
Motivation: Evaluating multi-turn interactive agents typically requires human assessment, which is resource-intensive. Simulations with generic user models lack domain-specific realism.

Method: SAGE integrates top-down business logic (e.g., customer profiles) and bottom-up infrastructure knowledge (e.g., product catalogs, FAQs) to simulate realistic user interactions tailored to target markets.

Result: Empirical tests show SAGE generates diverse and realistic interactions, identifying 33% more agent errors than existing methods.

Conclusion: SAGE improves the evaluation process for interactive agents, helping refine their performance by offering precise and domain-specific simulations.

Abstract: Evaluating multi-turn interactive agents is challenging due to the need for
human assessment. Evaluation with simulated users has been introduced as an
alternative, however existing approaches typically model generic users and
overlook the domain-specific principles required to capture realistic behavior.
We propose SAGE, a novel user Simulation framework for multi-turn AGent
Evaluation that integrates knowledge from business contexts. SAGE incorporates
top-down knowledge rooted in business logic, such as ideal customer profiles,
grounding user behavior in realistic customer personas. We further integrate
bottom-up knowledge taken from business agent infrastructure (e.g., product
catalogs, FAQs, and knowledge bases), allowing the simulator to generate
interactions that reflect users' information needs and expectations in a
company's target market. Through empirical evaluation, we find that this
approach produces interactions that are more realistic and diverse, while also
identifying up to 33% more agent errors, highlighting its effectiveness as an
evaluation tool to support bug-finding and iterative agent improvement.

</details>


### [60] [Generate Logical Equivalence Questions](https://arxiv.org/abs/2510.12001)
*Xinyu Wang,Haoming Yu,Yicheng Yang,Zhiyuan Li*

Main category: cs.CL

TL;DR: The paper addresses the issue of plagiarism in online learning by using Automatic Question Generation (AQG) to create unique logical equivalence questions for Discrete Mathematics, presenting a more efficient and uniform method using a formal language and linear-time algorithm.


<details>
  <summary>Details</summary>
Motivation: To mitigate academic dishonesty and provide ample practice questions for students, particularly in online learning, by improving the Automatic Question Generation for Discrete Mathematics.

Method: The AQG defines logical equivalence questions using a formal language, translates it into sets of generation rules, and employs a linear-time algorithm to create questions.

Result: Two experiments were conducted showing the accuracy and difficulty of generated questions were akin to textbook questions and comparable to Language Model-generated questions in solving steps needed.

Conclusion: The proposed AQG approach proves to be efficient, reliable, and generates high-quality questions, providing solutions to plagiarism and enhancing learning resources in online contexts.

Abstract: Academic dishonesty is met with zero tolerance in higher education, yet
plagiarism has become increasingly prevalent in the era of online teaching and
learning. Automatic Question Generation (AQG) presents a potential solution to
mitigate copying by creating unique questions for each student. Additionally,
AQG can provide a vast array of practice questions. Our AQG focuses on
generating logical equivalence questions for Discrete Mathematics, a
foundational course for first-year computer science students. A literature
review reveals that existing AQGs for this type of question generate all
propositions that meet user-defined constraints, resulting in inefficiencies
and a lack of uniform question difficulty. To address this, we propose a new
approach that defines logical equivalence questions using a formal language,
translates this language into two sets of generation rules, and develops a
linear-time algorithm for question generation. We evaluated our AQG through two
experiments. The first involved a group of students completing questions
generated by our system. Statistical analysis shows that the accuracy of these
questions is comparable to that of textbook questions. The second experiment
assessed the number of steps required to solve our generated questions,
textbook questions, and those generated by multiple large language models. The
results indicated that the difficulty of our questions was similar to that of
textbook questions, confirming the quality of our AQG.

</details>


### [61] [Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM](https://arxiv.org/abs/2510.12023)
*Alice Saebom Kwak,Maria Alexeeva,Gus Hahn-Powell,Keith Alcock,Kevin McLaughlin,Doug McCorkle,Gabe McNunn,Mihai Surdeanu*

Main category: cs.CL

TL;DR: The paper contrasts neuro-symbolic (NS) and large language model (LLM)-based information extraction systems in agriculture, finding LLMs outperform in accuracy but have trade-offs in efficiency, control, and reliability.


<details>
  <summary>Details</summary>
Motivation: The authors aim to provide insights into the trade-offs between established neuro-symbolic methods and modern LLM-based methods for practical real-world IE tasks in agriculture.

Method: The study evaluates NS and LLM systems based on information extraction performance across nine interviews in pork, dairy, and crop subdomains, comparing trade-offs such as accuracy, runtime, generalizability, and control.

Result: The LLM-based system performs better in overall accuracy (F1-total: 69.4 vs. 52.7 and F1-core: 63.0 vs. 47.2), but the NS system excels in efficiency, context-free task accuracy, and control.

Conclusion: The findings identify the trade-offs of deploying NLP systems, stressing the need for balance between performance, runtime efficiency, reliability, and resource management.

Abstract: The current trend in information extraction (IE) is to rely extensively on
large language models, effectively discarding decades of experience in building
symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS)
and an LLM-based IE system in the agricultural domain, evaluating them on nine
interviews across pork, dairy, and crop subdomains. The LLM-based system
outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where
total includes all extracted information and core focuses on essential details.
However, each system has trade-offs: the NS approach offers faster runtime,
greater control, and high accuracy in context-free tasks but lacks
generalizability, struggles with contextual nuances, and requires significant
resources to develop and maintain. The LLM-based system achieves higher
performance, faster deployment, and easier maintenance but has slower runtime,
limited control, model dependency and hallucination risks. Our findings
highlight the "hidden cost" of deploying NLP systems in real-world
applications, emphasizing the need to balance performance, efficiency, and
control.

</details>


### [62] [CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement](https://arxiv.org/abs/2510.12029)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: Large language models (LLMs) can produce errors due to vague prompts. The study introduces Curative Prompt Refinement (CPR) to clean prompts and better align user intentions, improving outputs and reducing hallucinations.


<details>
  <summary>Details</summary>
Motivation: Errors known as 'hallucinated facts' undermine trust in LLMs, with vague prompts being a primary overlooked cause.

Method: A framework called CPR refines ill-formed prompts by cleaning their structure and providing task descriptions, using a fine-tuned small language model.

Result: CPR significantly improved LLM output quality and reduced hallucination, achieving over a 90% win rate compared to original prompts.

Conclusion: Improving prompt clarity through CPR enhances trust and performance in LLM outputs without the need for external knowledge.

Abstract: Recent advancements in large language models (LLMs) highlight their fluency
in generating responses to diverse prompts. However, these models sometimes
generate plausible yet incorrect ``hallucinated" facts, undermining trust. A
frequent but often overlooked cause of such errors is the use of poorly
structured or vague prompts by users, leading LLMs to base responses on assumed
rather than actual intentions. To mitigate hallucinations induced by these
ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a
plug-and-play framework for curative prompt refinement that 1) cleans
ill-formed prompts, and 2) generates additional informative task descriptions
to align the intention of the user and the prompt using a fine-tuned small
language model. When applied to language models, we discover that CPR
significantly increases the quality of generation while also mitigating
hallucination. Empirical studies show that prompts with CPR applied achieves
over a 90\% win rate over the original prompts without any external knowledge.

</details>


### [63] [Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12032)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: Multi-stage Prompt Refinement (MPR) improves the clarity and accuracy of prompts to significantly reduce hallucinations in large language models.


<details>
  <summary>Details</summary>
Motivation: Large language models often hallucinate due to ill-formed prompts characterized by ambiguous wording, grammatical errors, or incomplete information, an underexplored issue.

Method: MPR systematically refines prompts in multiple stages using fine-tuned small language models to address errors like punctuation, typographical issues, and terminology misuse. It incorporates self-reflection mechanisms and ranking systems for quality improvement.

Result: Refined prompts using MPR showed over 85% win rate on hallucination benchmarks, proving its effectiveness in improving LLM output accuracy.

Conclusion: MPR is a lightweight and adaptable framework that reduces hallucinations, enhances LLM reliability, and complements other mitigation strategies across various domains.

Abstract: Recent advancements in large language models (LLMs) have shown strong
performance in natural language understanding and generation tasks. However,
LLMs continue to encounter challenges with hallucinations, where models
generate plausible but incorrect information. While several factors contribute
to hallucinations, the impact of ill-formed prompts, prompts with ambiguous
wording, incorrect grammar, or incomplete information, was relatively under
explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a
framework designed to systematically improve these ill-formed prompts across
multiple stages. Each stage addresses specific errors such as punctuation,
typographical mistakes, and misuse of key terms, using small language models
(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of
prompts with additional context and employs a self-reflection mechanism with
ranking to prioritize the most relevant input. Experimental results on
hallucination benchmarks show that prompts refined by MPR achieve over an 85~\%
win rate compared to their original forms, demonstrating its effectiveness in
reducing hallucinations and improving LLM output accuracy. Interestingly, we
reveal that MPR can be combined with existing post-hoc hallucination mitigation
frameworks, further enhancing its versatility. MPR provides a lightweight and
adaptable solution for enhancing LLM reliability across various domains.

</details>


### [64] [On the Interplay between Human Label Variation and Model Fairness](https://arxiv.org/abs/2510.12036)
*Kemal Kurniawan,Meladel Mistica,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: This paper studies the effect of human label variation on model fairness, demonstrating HLV approaches improve fairness even without explicit debiasing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the unexplored relationship between human label variation and fairness in machine learning models.

Method: The paper compares training on majority-vote labels against different human label variation methods.

Result: Experiments revealed that using HLV methods positively impacted fairness without requiring debiasing.

Conclusion: Human label variation methods can inherently enhance fairness in AI models without additional debiasing efforts.

Abstract: The impact of human label variation (HLV) on model fairness is an unexplored
topic. This paper examines the interplay by comparing training on majority-vote
labels with a range of HLV methods. Our experiments show that without explicit
debiasing, HLV training methods have a positive impact on fairness.

</details>


### [65] [Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions](https://arxiv.org/abs/2510.12040)
*Sungmin Kang,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Baturalp Buyukates,Salman Avestimehr*

Main category: cs.CL

TL;DR: The paper explores the role of uncertainty quantification (UQ) in addressing reliability issues in large language models (LLMs), particularly in detecting hallucinations, and provides an overview of methods and their limitations.


<details>
  <summary>Details</summary>
Motivation: The growing deployment of LLMs in real-world applications has raised concerns due to their tendency to produce factually incorrect outputs (hallucinations), stressing the need for measures to assess reliability and trustworthiness.

Method: The paper introduces the theoretical foundations of uncertainty quantification, discusses its adaptation to LLMs, categorizes existing UQ methods for hallucination detection, and presents empirical evaluations of representative techniques.

Result: The study systematically categorizes UQ methods, demonstrates their usage in detecting hallucinations, and provides empirical insights into the effectiveness of these approaches.

Conclusion: UQ is crucial for identifying and mitigating hallucinations in LLMs. Despite progress, challenges remain, and the paper outlines potential future research directions to enhance reliability in LLM deployments.

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of natural language processing, enabling breakthroughs across a wide
range of areas including question answering, machine translation, and text
summarization. Yet, their deployment in real-world applications has raised
concerns over reliability and trustworthiness, as LLMs remain prone to
hallucinations that produce plausible but factually incorrect outputs.
Uncertainty quantification (UQ) has emerged as a central research direction to
address this issue, offering principled measures for assessing the
trustworthiness of model generations. We begin by introducing the foundations
of UQ, from its formal definition to the traditional distinction between
epistemic and aleatoric uncertainty, and then highlight how these concepts have
been adapted to the context of LLMs. Building on this, we examine the role of
UQ in hallucination detection, where quantifying uncertainty provides a
mechanism for identifying unreliable generations and improving reliability. We
systematically categorize a wide spectrum of existing methods along multiple
dimensions and present empirical results for several representative approaches.
Finally, we discuss current limitations and outline promising future research
directions, providing a clearer picture of the current landscape of LLM UQ for
hallucination detection.

</details>


### [66] [Improving Text-to-Image Generation with Input-Side Inference-Time Scaling](https://arxiv.org/abs/2510.12041)
*Ruibo Chen,Jiacheng Pan,Heng Huang,Zhenheng Yang*

Main category: cs.CL

TL;DR: The paper introduces a prompt rewriting framework using large language models to improve text-to-image (T2I) generation, achieving better text-image alignment, visual quality, and aesthetics.


<details>
  <summary>Details</summary>
Motivation: Existing T2I models often fail with simple or underspecified prompts, leading to poor image-text alignment and low-quality outputs.

Method: The framework refines user input prompts using a reward system and an iterative direct preference optimization (DPO) training pipeline, without requiring supervised fine-tuning data. Evaluations were conducted across various T2I models and included scalability analyses.

Result: The proposed prompt rewriter greatly enhances image-text alignment, visual quality, and aesthetics, surpassing strong baselines. The rewriter also generalizes well across different T2I backbones without retraining.

Conclusion: The prompt rewriting approach is a practical and scalable strategy that significantly improves T2I generation while remaining model-agnostic. Code and models will be released for further adoption.

Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models often struggle with simple or underspecified
prompts, leading to suboptimal image-text alignment, aesthetics, and quality.
We propose a prompt rewriting framework that leverages large language models
(LLMs) to refine user inputs before feeding them into T2I backbones. Our
approach introduces a carefully designed reward system and an iterative direct
preference optimization (DPO) training pipeline, enabling the rewriter to
enhance prompts without requiring supervised fine-tuning data. We evaluate our
method across diverse T2I models and benchmarks. Results show that our prompt
rewriter consistently improves image-text alignment, visual quality, and
aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong
transferability by showing that a prompt rewriter trained on one T2I backbone
generalizes effectively to others without needing to be retrained. We also
systematically study scalability, evaluating how performance gains scale with
the capacity of the large LLM used as the rewriter. These findings highlight
that prompt rewriting is an effective, scalable, and practical model-agnostic
strategy for improving T2I systems. We plan to release the code and trained
prompt rewriters soon.

</details>


### [67] [Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models](https://arxiv.org/abs/2510.12044)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: Large Language Models (LLMs) alignment techniques are improved using Hierarchical Alignment, targeting specific functional blocks of their layers. This approach avoids the "alignment tax," enhancing grammatical fluency, factual consistency, and logical coherence simultaneously.


<details>
  <summary>Details</summary>
Motivation: Current alignment techniques for LLMs disregard the specialized roles of different Transformer layers, leading to less effective optimization. The paper aims to address this limitation through targeted, layer-specific adjustments.

Method: The study introduces Hierarchical Alignment, which applies targeted Direct Preference Optimization (DPO) to separate functional blocks of a model's layers. LoRA-based fine-tuning on syntax, logic, and factuality is employed, evaluated with a powerful LLM-as-Judge.

Result: Experiments show that aligning specific layers (local for grammar, global for logic) results in significant improvements. Notably, global layer alignment enhances logical coherence and factual consistency, surpassing baseline methods.

Conclusion: The proposed Hierarchical Alignment method allows resource-efficient and interpretable LLM optimization, shifting from monolithic models to structure-aware fine-tuning to improve fluency, logic, and factual accuracy without trade-offs.

Abstract: Existing alignment techniques for Large Language Models (LLMs), such as
Direct Preference Optimization (DPO), typically treat the model as a monolithic
entity, applying uniform optimization pressure across all layers. This approach
overlooks the functional specialization within the Transformer architecture,
where different layers are known to handle distinct tasks from syntax to
abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm
by introducing Hierarchical Alignment, a novel method that applies targeted DPO
to distinct functional blocks of a model's layers: local (syntax), intermediate
(logic), and global (factuality). Through a series of controlled experiments on
state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for
surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge,
demonstrate significant and predictable improvements. Specifically, aligning
the local layers (Local-Align) enhances grammatical fluency. More importantly,
aligning the global layers (Global-Align) not only improves factual consistency
as hypothesized but also proves to be the most effective strategy for enhancing
logical coherence, outperforming all baselines. Critically, all hierarchical
strategies successfully avoid the "alignment tax" observed in standard DPO,
where gains in fluency come at the cost of degraded logical reasoning. These
findings establish a more resource-efficient, controllable, and interpretable
path for model alignment, highlighting the immense potential of shifting from
monolithic optimization to structure-aware surgical fine-tuning to build more
advanced and reliable LLMs.

</details>


### [68] [APCE: Adaptive Progressive Context Expansion for Long Context Processing](https://arxiv.org/abs/2510.12051)
*Baisub Lee,Sanghyun Byun,Mohanad Odema,Jung Guack,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: The paper addresses challenges in deploying Long-Context Transformer Models (LCTMs) and proposes APCE as a solution to mitigate memory issues and performance degradation for summarization tasks.


<details>
  <summary>Details</summary>
Motivation: Address challenges of growing memory footprint and performance degradation (ContextRot) in LCTMs due to increasing context length.

Method: Introduce APCE, a context-aware method that selects important input chunks based on semantic similarity to reduce memory usage and reliance on hardware.

Result: Empirical evaluation shows APCE achieves superior or comparable summarization performance using only 50%-70% of the input sequence, leading to memory efficiency improvements.

Conclusion: APCE effectively reduces memory footprint and mitigates ContextRot without strict hardware dependency, encouraging further research for similar LCTM tasks.

Abstract: Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear KV-cache scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
transformer architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in KV-cache and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.

</details>


### [69] [An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations](https://arxiv.org/abs/2510.12083)
*Benjamin W. Nelson,Celeste Wong,Matthew T. Silvestrini,Sooyoon Shin,Alanna Robinson,Jessica Lee,Eric Yang,John Torous,Andrew Trister*

Main category: cs.CL

TL;DR: The study evaluates Verily's behavioral health safety filter (VBHSF) for handling mental health crises, demonstrating its superior sensitivity and specificity compared to existing moderation tools.


<details>
  <summary>Details</summary>
Motivation: Large language models often provide harmful advice during psychiatric emergencies, necessitating reliable tools for detecting and addressing mental health crises.

Method: The study tested VBHSF against two labeled datasets: Verily Mental Health Crisis Dataset and NVIDIA Aegis AI Content Safety Dataset, and compared its performance with OpenAI Omni Moderation and NVIDIA NeMo Guardrails.

Result: VBHSF showed high sensitivity and specificity in detecting mental health crises, outperformed competing systems across metrics, and maintained consistent results across different datasets.

Conclusion: The VBHSF is an effective tool for identifying mental health crises, emphasizing sensitivity to minimize risks, making it suitable for healthcare applications.

Abstract: Large language models often mishandle psychiatric emergencies, offering
harmful or inappropriate advice and enabling destructive behaviors. This study
evaluated the Verily behavioral health safety filter (VBHSF) on two datasets:
the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and
the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental
health-related messages. The two datasets were clinician-labelled and we
evaluated performance using the clinician labels. Additionally, we carried out
comparative performance analyses against two open source, content moderation
guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF
demonstrated, well-balanced performance on the Verily Mental Health Crisis
Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in
detecting any mental health crises. It achieved an F1-score of 0.939,
sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in
identifying specific crisis categories. When evaluated against the NVIDIA Aegis
AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive
(0.982) and accuracy (0.921) with reduced specificity (0.859). When compared
with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF
demonstrated superior performance metrics across both datasets, achieving
significantly higher sensitivity in all cases (all p < 0.001) and higher
specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni
Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest
exhibited inconsistent performance across specific crisis types, with
sensitivity for some categories falling below 0.10. Overall, the VBHSF
demonstrated robust, generalizable performance that prioritizes sensitivity to
minimize missed crises, a crucial feature for healthcare applications.

</details>


### [70] [Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models](https://arxiv.org/abs/2510.12110)
*Ziliang Qiu,Renfen Hu*

Main category: cs.CL

TL;DR: PACE, a novel model-based creativity evaluation method for LLMs, minimizes data contamination and correlates well with human rankings, revealing differences between human and LLM creativity.


<details>
  <summary>Details</summary>
Motivation: Challenges such as data contamination and resource-intensive human assessments hinder effective evaluation of LLMs' creativity.

Method: Proposed PACE, a system that generates Parallel Association Chains to assess LLM creativity in an efficient and contamination-resistant manner.

Result: PACE showed strong correlation (Spearman's $ho = 0.739$, $p < 0.001$) with human creative writing rankings and revealed gaps in creativity between LLMs and humans.

Conclusion: PACE provides an effective creativity evaluation for LLMs but establishes that humans demonstrate superior and more diverse associative creativity compared to even high-performing LLMs.

Abstract: The evaluation of LLMs' creativity represents a crucial research domain,
though challenges such as data contamination and costly human assessments often
impede progress. Drawing inspiration from human creativity assessment, we
propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate
their creativity. PACE minimizes the risk of data contamination and offers a
straightforward, highly efficient evaluation, as evidenced by its strong
correlation with Chatbot Arena Creative Writing rankings (Spearman's $\rho =
0.739$, $p < 0.001$) across various proprietary and open-source models. A
comparative analysis of associative creativity between LLMs and humans reveals
that while high-performing LLMs achieve scores comparable to average human
performance, professional humans consistently outperform LLMs. Furthermore,
linguistic analysis reveals that both humans and LLMs exhibit a trend of
decreasing concreteness in their associations, and humans demonstrating a
greater diversity of associative patterns.

</details>


### [71] [Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation](https://arxiv.org/abs/2510.12115)
*Xin Zhao,Naoki Yoshinaga,Yuma Tsuta,Akiko Aizawa*

Main category: cs.CL

TL;DR: This paper explores the dynamics of multilingual domain adaptation (ML-DA) in large language models (LLMs), highlighting challenges in cross-lingual transfer and introducing AdaXEval for evaluation.


<details>
  <summary>Details</summary>
Motivation: To understand how domain knowledge is acquired and transferred across languages in multilingual domain adaptation (ML-DA), addressing gaps in prior research that limit performance, especially for low-resource languages.

Method: They propose AdaXEval, an adaptive evaluation method that builds multiple-choice QA datasets from the same bilingual domain corpus used for training. Through continual training with diverse data recipes, they study how domain knowledge is acquired and transferred.

Result: Experiments on a 13B English-Japanese bilingual LLM demonstrate that cross-lingual knowledge transfer remains challenging, even with high-quality bilingual datasets.

Conclusion: The study provides insights into the mechanisms of multilingual knowledge acquisition and calls attention to the difficulties in achieving effective cross-lingual transfer in ML-DA, emphasizing the need for improved methodologies.

Abstract: Multilingual domain adaptation (ML-DA) is widely used to learn new domain
knowledge across languages into large language models (LLMs). Although many
methods have been proposed to improve domain adaptation, the mechanisms of
multilingual knowledge acquisition, how domain knowledge is learned within a
language and transferred across languages, remain underexplored. This gap leads
to suboptimal performance, particularly in low-resource settings. This work
examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA
studies often train and evaluate on datasets with mismatched knowledge
coverage, we propose AdaXEval, an adaptive evaluation method that builds
multiple-choice QA datasets from the same bilingual domain corpus used for
training, thereby directly studying multilingual knowledge acquisition. Through
continual training of LLMs with diverse data recipes, we track how LLMs acquire
domain facts and pinpoint the mechanism behind the transformation process from
domain training data to knowledge. Our experiments on a 13B English-Japanese
bilingual LLM reveal that cross-lingual transfer remains challenging despite a
high-quality bilingual corpus. The code has been released.

</details>


### [72] [Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models](https://arxiv.org/abs/2510.12116)
*Bajian Xiang,Shuaijiang Zhao,Tingwei Guo,Wei Zou*

Main category: cs.CL

TL;DR: This paper investigates the performance gap ('modality gap') between speech and text inputs in end-to-end Large Speech Language Models (LSLMs), presenting a systematic analysis of representation alignment and proposing interventions to mitigate the gap.


<details>
  <summary>Details</summary>
Motivation: End-to-end LSLMs perform well in conversational tasks but fall short in semantic understanding benchmarks, especially when speech inputs are involved. This work aims to understand and address the performance gap between modalities.

Method: The authors conduct systematic experiments to analyze coarse- and fine-grained text and speech representations, introduce metrics such as the Alignment Path Score, and design interventions like angle projection and length normalization.

Result: Speech and text representations show alignment in cosine similarity but diverge in Euclidean distance. Representation alignment quality is correlated with the modality gap, and targeted interventions are shown to improve speech input performance.

Conclusion: The study systematically analyzes the modality gap in LSLMs, revealing alignment mechanisms and providing approaches to optimize speech input performance for enhanced semantic understanding.

Abstract: End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive
conversational generation abilities, yet consistently fall short of traditional
pipeline systems on semantic understanding benchmarks. In this work, we reveal
through systematic experimentation that although LSLMs lose some text input
performance after speech-text alignment training, the performance gap between
speech and text inputs is more pronounced, which we refer to as the modality
gap. To understand this gap, we analyze both coarse- and fine-grained text and
speech representations. At the coarse-grained level, representations of speech
and text in deeper layers are found to be increasingly aligned in direction
(cosine similarity), while concurrently diverging in magnitude (Euclidean
distance). We further find that representation similarity is strongly
correlated with the modality gap. At the fine-grained level, a spontaneous
token-level alignment pattern between text and speech representations is
observed. Based on this, we introduce the Alignment Path Score to quantify
token-level alignment quality, which exhibits stronger correlation with the
modality gap. Building on these insights, we design targeted interventions on
critical tokens through angle projection and length normalization. These
strategies demonstrate the potential to improve correctness for speech inputs.
Our study provides the first systematic empirical analysis of the modality gap
and alignment mechanisms in LSLMs, offering both theoretical and methodological
guidance for future optimization.

</details>


### [73] [SafeMT: Multi-turn Safety for Multimodal Language Models](https://arxiv.org/abs/2510.12133)
*Han Zhu,Juntao Dai,Jiaming Ji,Haoran Li,Chengkun Cai,Pengcheng Wen,Chi-Min Chan,Boyuan Chen,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: The paper introduces SafeMT, a benchmark for evaluating the safety of multi-modal Large Language Models (MLLMs) in multi-turn dialogues, particularly concerning harmful interactions. It includes 10,000 samples and proposes a Safety Index for assessment.


<details>
  <summary>Details</summary>
Motivation: Safety concerns over the increasing use of multi-modal Large Language Models (MLLMs) in multi-turn dialogues motivated the study, as existing benchmarks insufficiently address this issue.

Method: The study presents SafeMT, a benchmark with 10,000 samples covering 17 scenarios and 4 jailbreak methods. A new metric, Safety Index (SI), is also introduced to assess safety. It further proposes a dialogue safety moderator to detect malicious intent and apply relevant safety policies.

Result: Testing 17 models revealed that safety risks increase with the number of dialogue turns, indicating existing safety systems' inadequacy. The proposed dialogue safety moderator reduced attack success rates (ASR) better than current safety models.

Conclusion: The authors highlight critical vulnerabilities in MLLMs' safety mechanisms for multi-turn dialogues. SafeMT and the dialogue safety moderator propose paths for improved safety in complex conversational contexts.

Abstract: With the widespread use of multi-modal Large Language models (MLLMs), safety
issues have become a growing concern. Multi-turn dialogues, which are more
common in everyday interactions, pose a greater risk than single prompts;
however, existing benchmarks do not adequately consider this situation. To
encourage the community to focus on the safety issues of these models in
multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues
of varying lengths generated from harmful queries accompanied by images. This
benchmark consists of 10,000 samples in total, encompassing 17 different
scenarios and four jailbreak methods. Additionally, we propose Safety Index
(SI) to evaluate the general safety of MLLMs during conversations. We assess
the safety of 17 models using this benchmark and discover that the risk of
successful attacks on these models increases as the number of turns in harmful
dialogues rises. This observation indicates that the safety mechanisms of these
models are inadequate for recognizing the hazard in dialogue interactions. We
propose a dialogue safety moderator capable of detecting malicious intent
concealed within conversations and providing MLLMs with relevant safety
policies. Experimental results from several open-source models indicate that
this moderator is more effective in reducing multi-turn ASR compared to existed
guard models.

</details>


### [74] [Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12137)
*Shihao Ji,Zihui Song,Jiajie Huang*

Main category: cs.CL

TL;DR: The paper introduces the Credal Transformer, addressing hallucination issues in Large Language Models (LLMs) by incorporating uncertainty quantification into the attention mechanism.


<details>
  <summary>Details</summary>
Motivation: To tackle the problem of hallucinations (incorrect yet confident outputs) in LLMs caused by the Softmax function collapsing ambiguous attention scores into a single probability distribution.

Method: The paper proposes replacing the standard attention mechanism with a Credal Attention Mechanism (CAM) based on evidential theory. CAM creates a set of distributions to quantify uncertainty by re-conceptualizing attention scores as evidence masses for a Dirichlet distribution.

Result: The Credal Transformer is able to detect out-of-distribution inputs, measure ambiguity, and significantly lower the generation of confident errors in unanswerable scenarios through model abstention.

Conclusion: Credal Transformer represents a novel architecture that mitigates hallucinations in LLMs, integrates uncertainty quantification, and provides a direction for building more reliable AI systems.

Abstract: Large Language Models (LLMs) hallucinate, generating factually incorrect yet
confident assertions. We argue this stems from the Transformer's Softmax
function, which creates "Artificial Certainty" by collapsing ambiguous
attention scores into a single probability distribution, discarding uncertainty
information at each layer. To fix this, we introduce the Credal Transformer,
which replaces standard attention with a Credal Attention Mechanism (CAM) based
on evidential theory. CAM produces a "credal set" (a set of distributions)
instead of a single attention vector, with the set's size directly measuring
model uncertainty. We implement this by re-conceptualizing attention scores as
evidence masses for a Dirichlet distribution: sufficient evidence recovers
standard attention, while insufficient evidence yields a diffuse distribution,
representing ambiguity. Empirically, the Credal Transformer identifies
out-of-distribution inputs, quantifies ambiguity, and significantly reduces
confident errors on unanswerable questions by abstaining. Our contribution is a
new architecture to mitigate hallucinations and a design paradigm that
integrates uncertainty quantification directly into the model, providing a
foundation for more reliable AI.

</details>


### [75] [A Survey on Parallel Reasoning](https://arxiv.org/abs/2510.12164)
*Ziqi Wang,Boye Niu,Zipeng Gao,Zhi Zheng,Tong Xu,Linghui Meng,Zhongli Li,Jing Liu,Yilong Chen,Chen Zhu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.CL

TL;DR: The paper surveys the progress of parallel reasoning with Large Language Models (LLMs), proposing a new taxonomy and identifying challenges and future directions for its advancement.


<details>
  <summary>Details</summary>
Motivation: To address the fragility of traditional sequential reasoning methods in LLMs by exploring enhanced robustness through parallel reasoning.

Method: The paper formally defines parallel reasoning, differentiates it from related concepts such as Chain-of-Thought, introduces a taxonomy of techniques including non-interactive reasoning, interactive reasoning, and efficiency-focused decoding strategies, and explores its applications and challenges.

Result: The study consolidates state-of-the-art advancements, categorizes techniques, and identifies challenges in parallel reasoning to improve the robustness and reliability of LLM outputs.

Conclusion: The authors offer a roadmap for beginners and researchers, urging the community to delve deeper into parallel reasoning methods and address existing challenges, fostering advancements in this domain of LLMs.

Abstract: With the increasing capabilities of Large Language Models (LLMs), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused decoding strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of LLM outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.

</details>


### [76] [Towards Inference-time Scaling for Continuous Space Reasoning](https://arxiv.org/abs/2510.12167)
*Minghan Wang,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: The paper explores adapting established inference-time scaling and re-ranking techniques to continuous space reasoning in large language models using COCONUT. While it demonstrates potential performance gains, obstacles such as lack of inductive biases are identified.


<details>
  <summary>Details</summary>
Motivation: To investigate if techniques effective in discrete space can be successfully adapted for continuous space reasoning in large language models.

Method: The paper uses dropout-based sampling for generating diverse reasoning paths and analyzes their effectiveness through Pass@N metrics while probing geometric and trajectory properties.

Result: Demonstrates potential for performance gains in continuous reasoning, but highlights challenges in achieving discrimination between correct and incorrect reasoning paths.

Conclusion: Techniques effective in discrete space minimally improve continuous space reasoning. The absence of inductive biases in continuous representations is a key challenge, requiring updated training frameworks that optimize accuracy and allow inductive biases.

Abstract: Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

</details>


### [77] [From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](https://arxiv.org/abs/2510.12181)
*Chengrui Xiang,Tengfei Ma,Xiangzheng Fu,Yiping Liu,Bosheng Song,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: The paper proposes LLaDR, a Large Language Model-assisted framework, to improve drug repurposing by enhancing biomedical knowledge graph representations.


<details>
  <summary>Details</summary>
Motivation: Existing drug repurposing methods largely overlook common-sense biomedical knowledge and mechanistic priors necessary for real-world treatment compatibility.

Method: The authors use large language models to extract semantically enriched textual representations of biomedical entities and fine-tune knowledge graph embedding models with this information.

Result: The LLaDR framework significantly improves the representation of biomedical concepts in knowledge graphs and achieves state-of-the-art performance in drug repurposing benchmarks, with robust case study validation on Alzheimer's disease.

Conclusion: LLaDR demonstrates how integrating large language models with knowledge graph embeddings can enhance understanding of complex or under-studied biomedical scenarios, advancing drug discovery efforts.

Abstract: Drug repurposing plays a critical role in accelerating treatment discovery,
especially for complex and rare diseases. Biomedical knowledge graphs (KGs),
which encode rich clinical associations, have been widely adopted to support
this task. However, existing methods largely overlook common-sense biomedical
concept knowledge in real-world labs, such as mechanistic priors indicating
that certain drugs are fundamentally incompatible with specific treatments. To
address this gap, we propose LLaDR, a Large Language Model-assisted framework
for Drug Repurposing, which improves the representation of biomedical concepts
within KGs. Specifically, we extract semantically enriched treatment-related
textual representations of biomedical entities from large language models
(LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By
injecting treatment-relevant knowledge into KGE, LLaDR largely improves the
representation of biomedical concepts, enhancing semantic understanding of
under-studied or complex indications. Experiments based on benchmarks
demonstrate that LLaDR achieves state-of-the-art performance across different
scenarios, with case studies on Alzheimer's disease further confirming its
robustness and effectiveness. Code is available at
https://github.com/xiaomingaaa/LLaDR.

</details>


### [78] [Not in Sync: Unveiling Temporal Bias in Audio Chat Models](https://arxiv.org/abs/2510.12185)
*Jiayu Yao,Shenghua Liu,Yiwei Wang,Rundong Cheng,Lingrui Mei,Baolong Bi,Zhen Xiong,Xueqi Cheng*

Main category: cs.CL

TL;DR: The study investigates the limitations of Large Audio Language Models (LALMs) in timestamp prediction, revealing prevalent temporal bias across datasets and models, which worsens with audio length and event types.


<details>
  <summary>Details</summary>
Motivation: To analyze and address the overlooked issue of temporal bias in LALMs during timestamp prediction, which affects their use in audio and multimodal reasoning tasks.

Method: Conducted controlled experiments on timestamped datasets, developed the Temporal Bias Index (TBI) to quantify misalignments, and implemented a visualization framework to illustrate results.

Result: Temporal bias is prevalent, worsens with longer audio recordings, varies by event types, and leads to systematic misalignment of predicted event timings.

Conclusion: Current LALMs exhibit significant temporal bias, calling for the development of architectures capable of robust temporal predictions.

Abstract: Large Audio Language Models (LALMs) are increasingly applied to audio
understanding and multimodal reasoning, yet their ability to locate when events
occur remains underexplored. We present the first systematic study of temporal
bias in LALMs, revealing a key limitation in their timestamp prediction. For
example, when asked "At which second does the lecturer introduce the key
formula?", models often predict timestamps that are consistently earlier or
later than the ground truth. Through controlled experiments on timestamped
datasets, we find that temporal bias (i) is prevalent across datasets and
models, (ii) increases with audio length - even accumulating to tens of seconds
in extended recordings, and (iii) varies across event types and positions. We
quantify this effect with the Temporal Bias Index (TBI), measuring systematic
misalignment in predicted event timings, and complement it with a visualization
framework. Our findings highlight a fundamental limitation in current LALMs and
call for the development of temporally robust architectures.

</details>


### [79] [DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation](https://arxiv.org/abs/2510.12195)
*Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: The paper proposes a segmentation framework using large language models (LLMs) optimized with Direct Preference Optimization (DPO) for better real-time speech translation, outperforming previous models like SHAS.


<details>
  <summary>Details</summary>
Motivation: Current supervised learning-based segmentation models lack human preference alignment, which is key for natural and effective simultaneous speech translation.

Method: The segmentation framework utilizes preference-tuned LLMs trained with Direct Preference Optimization to predict natural segmentation points, enhancing real-time translation accuracy.

Result: Experimental evaluation shows the proposed LLM framework outperforms SHAS in segmentation accuracy, translation quality (BLEU, COMET metrics), and latency (Average Lagging).

Conclusion: Preference-tuned LLMs offer a significant improvement in meeting human-aligned requirements for simultaneous speech translation, surpassing existing models.

Abstract: Simultaneous speech translation requires accurate segmentation to balance
translation quality and latency. Recent studies such as SHAS have introduced
pretrained segmentation models, achieving stronger performance than heuristic
rules. However, segmentation models such as SHAS, though pretrained and more
robust than heuristic methods, are still constrained by supervised learning
objectives and do not incorporate human preference alignment, which is crucial
for natural real-time interpretation. In this work, we propose a segmentation
framework based on large language models (LLMs) trained with Direct Preference
Optimization (DPO). By leveraging preference alignment, our method enables LLMs
to predict natural segmentation points that better meet the demands of
real-time translation. We evaluate the system on the ACL 60/60 corpus across
three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2
as the translation backbone. Experimental results show that our DPO-tuned LLM
achieves higher segmentation accuracy than SHAS and yields consistent
improvements in translation quality (BLEU, COMET) as well as latency (Average
Lagging). Furthermore, our system benefits from IWSLT baselines for direct
comparison. These findings highlight the potential of preference-tuned LLMs to
surpass existing pretrained segmentation models and advance adaptive,
human-aligned simultaneous interpretation.

</details>


### [80] [HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment](https://arxiv.org/abs/2510.12217)
*Ali Mekky,Omar El Herraoui,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: The paper presents a framework called HALF (Harm-Aware LLM Fairness) to evaluate the fairness of large language models in realistic applications, prioritizing bias assessment based on harm severity.


<details>
  <summary>Details</summary>
Motivation: The growing deployment of LLMs in critical areas necessitates fairness evaluations grounded in real-world scenarios, addressing the lack of harm severity considerations in current bias assessments.

Method: The HALF framework organizes nine domains into three harm severity tiers (Severe, Moderate, Mild) using a five-stage evaluation pipeline to assess fairness in realistic applications.

Result: The study finds that LLMs display inconsistent fairness across domains, with neither size nor performance guaranteeing fairness. Reasoning models excel in medical contexts but underperform in education.

Conclusion: HALF highlights significant challenges in assessing bias in LLMs before deployment and exposes a gap between traditional benchmarking and practical deployment readiness.

Abstract: Large language models (LLMs) are increasingly deployed across high-impact
domains, from clinical decision support and legal analysis to hiring and
education, making fairness and bias evaluation before deployment critical.
However, existing evaluations lack grounding in real-world scenarios and do not
account for differences in harm severity, e.g., a biased decision in surgery
should not be weighed the same as a stylistic bias in text summarization. To
address this gap, we introduce HALF (Harm-Aware LLM Fairness), a
deployment-aligned framework that assesses model bias in realistic applications
and weighs the outcomes by harm severity. HALF organizes nine application
domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.
Our evaluation results across eight LLMs show that (1) LLMs are not
consistently fair across domains, (2) model size or performance do not
guarantee fairness, and (3) reasoning models perform better in medical decision
support but worse in education. We conclude that HALF exposes a clear gap
between previous benchmarking success and deployment readiness.

</details>


### [81] [Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability](https://arxiv.org/abs/2510.12229)
*Bianca Raimondi,Daniela Dalbagno,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: The paper explores how the Knobe effect, a moral bias, is learned and localized in finetuned LLMs and how targeted interventions can mitigate it.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand how human-like biases in language models arise, focusing on the Knobe effect, a moral bias in intentionality judgments.

Method: They utilized a Layer-Patching analysis across three open-weight language models to identify where the moral bias is learned and localized within the model.

Result: The study found that the bias is learned during finetuning and localized in specific layers. Patching activations from corresponding pretrained models into those layers effectively eliminated the bias.

Conclusion: Social biases in LLMs can be analyzed, localized, and removed through precise interventions without requiring retraining of the entire model.

Abstract: Large language models (LLMs) have been shown to internalize human-like biases
during finetuning, yet the mechanisms by which these biases manifest remain
unclear. In this work, we investigated whether the well-known Knobe effect, a
moral bias in intentionality judgements, emerges in finetuned LLMs and whether
it can be traced back to specific components of the model. We conducted a
Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the
bias is not only learned during finetuning but also localized in a specific set
of layers. Surprisingly, we found that patching activations from the
corresponding pretrained model into just a few critical layers is sufficient to
eliminate the effect. Our findings offer new evidence that social biases in
LLMs can be interpreted, localized, and mitigated through targeted
interventions, without the need for model retraining.

</details>


### [82] [DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering](https://arxiv.org/abs/2510.12251)
*Jiakai Li,Rongzheng Wang,Yizhuo Ma,Shuang Liang,Guangchun Luo,Ke Qin*

Main category: cs.CL

TL;DR: The paper introduces Dual-Stage Adaptive Sharpening (DSAS) to tackle limitations of large language models (LLMs) in multi-document question answering (Multi-doc QA). DSAS improves attention with the Contextual Gate Weighting (CGW) and Reciprocal Attention Suppression (RAS) modules, leading to significant F1-score improvements without requiring model architecture changes.


<details>
  <summary>Details</summary>
Motivation: Large language models face issues in multi-document question answering due to difficulty in maintaining semantic connections in long texts and struggling with the "lost-in-the-middle" issue for information processing in lengthy inputs.

Method: The DSAS framework consists of two modules: Contextual Gate Weighting (CGW) for assessing paragraph relevance and addressing "lost-in-the-middle" issues, and Reciprocal Attention Suppression (RAS) to suppress irrelevant text and enhance focus on key information. DSAS works as a plug-and-play solution, requiring no architectural changes or additional training.

Result: Experiments on four benchmarks showcase an average F1-score improvement of 4.2% in Multi-doc QA tasks, tested on Llama-3.1-8B-Instruct and Qwen2.5-14B-Instruct models, along with ablation studies confirming the value of DSAS's modules.

Conclusion: Dual-Stage Adaptive Sharpening (DSAS) significantly enhances multi-document QA performance in LLMs through improved attention mechanisms, presenting a universal, cost-effective plug-and-play solution without architectural modifications.

Abstract: While large language models (LLMs) show considerable promise across various
fields, they have notable limitations in handling multi-document question
answering (Multi-doc QA) tasks. The first challenge is long-range dependency
modeling, where LLMs struggle to focus on key information in long texts, which
weakens important semantic connections. Second, most LLMs suffer from the
''lost-in-the-middle'' issue, where they have difficulty processing information
in the middle of long inputs. Current solutions either truncate global
dependencies or demand costly finetuning, ultimately lacking a universal and
simple solution for these challenges. To resolve these limitations, we propose
Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The
Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by
assessing paragraph relevance through layer-wise attention tracking and
position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)
module enhances focus on critical paragraphs by suppressing information
exchange between key and irrelevant texts, thus mitigating the limitations in
long-range dependency modeling. Notably, DSAS functions as a plug-and-play
solution requiring no architectural modifications or extra training parameters.
Extensive experiments on four benchmarks demonstrate DSAS's efficacy across
mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score
improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and
Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of
both the CGW and RAS modules. In addition, detailed discussions in the Appendix
further validate the robustness and scalability of DSAS.

</details>


### [83] [Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs](https://arxiv.org/abs/2510.12255)
*Blazej Manczak,Eric Lin,Francisco Eiras,James O' Neill,Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: The paper investigates the robustness of large language models (LLMs) in medical multi-turn conversations, highlighting severe vulnerabilities when interacting in complex multi-turn medical consultation scenarios.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the lack of understanding of LLMs' performance in realistic multi-turn medical interactions, where conflicting input, misleading context, and authority influence are common.

Method: The authors propose MedQA-Followup, evaluating five state-of-the-art LLMs' robustness through controlled interventions on the MedQA dataset. They distinguish between shallow robustness (resisting misleading context) and deep robustness (accuracy when answers are challenged across turns).

Result: Findings reveal that LLMs perform reasonably well under shallow conditions but demonstrate severe accuracy reductions in multi-turn settings, dropping accuracy from 91.2% to as low as 13.5% for some models. Indirect interventions harm accuracy more than direct suggestions.

Conclusion: Multi-turn robustness is a critical and underexplored area for medical LLMs. Their vulnerabilities highlight the need for systematic solutions to ensure safe clinical usage.

Abstract: Large language models (LLMs) are rapidly transitioning into medical clinical
use, yet their reliability under realistic, multi-turn interactions remains
poorly understood. Existing evaluation frameworks typically assess single-turn
question answering under idealized conditions, overlooking the complexities of
medical consultations where conflicting input, misleading context, and
authority influence are common. We introduce MedQA-Followup, a framework for
systematically evaluating multi-turn robustness in medical question answering.
Our approach distinguishes between shallow robustness (resisting misleading
initial context) and deep robustness (maintaining accuracy when answers are
challenged across turns), while also introducing an indirect-direct axis that
separates contextual framing (indirect) from explicit suggestion (direct).
Using controlled interventions on the MedQA dataset, we evaluate five
state-of-the-art LLMs and find that while models perform reasonably well under
shallow perturbations, they exhibit severe vulnerabilities in multi-turn
settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude
Sonnet 4. Counterintuitively, indirect, context-based interventions are often
more harmful than direct suggestions, yielding larger accuracy drops across
models and exposing a significant vulnerability for clinical deployment.
Further compounding analyses reveal model differences, with some showing
additional performance drops under repeated interventions while others
partially recovering or even improving. These findings highlight multi-turn
robustness as a critical but underexplored dimension for safe and reliable
deployment of medical LLMs.

</details>


### [84] [Chinese ModernBERT with Whole-Word Masking](https://arxiv.org/abs/2510.12285)
*Zeyu Zhao,Ningtao Wang,Xing Fu,Yu Cheng*

Main category: cs.CL

TL;DR: The paper introduces Chinese ModernBERT, an encoder-only Transformer specifically tailored for Chinese language tasks through architecture, vocabulary, masking, and pre-training innovations, achieving strong performance on Chinese datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in transferring encoder-based Transformer improvements from English to Chinese due to differences in tokenization and language structure.

Method: The authors developed Chinese ModernBERT by combining a 32k BPE vocabulary optimized for Chinese affixes, whole-word masking with dynamic curriculum, extended-context training, and damped-cosine learning-rate scheduling.

Result: Chinese ModernBERT demonstrates competitive results on CLUE benchmarks, achieves efficient long and short-sequence processing, and outperforms Qwen-0.6B-embedding in open-data retrieval tasks with improved STS quality.

Conclusion: Chinese ModernBERT provides a scalable and reproducible approach for Chinese language modeling, with its tokenizer and weights released to promote further research advancements.

Abstract: Encoder-only Transformers have advanced along three axes -- architecture,
data, and systems -- yielding Pareto gains in accuracy, speed, and memory
efficiency. Yet these improvements have not fully transferred to Chinese, where
tokenization and morphology differ markedly from English. We introduce Chinese
ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware
32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the
embedding budget; (ii) whole-word masking (WWM) with a dynamic masking
curriculum (30% -> 15%) to align task difficulty with training progress; (iii)
a two-stage pre-training pipeline that extends the native context from 1,024 to
8,192 tokens using RoPE and alternating local/global attention; and (iv) a
damped-cosine learning-rate schedule for stable long-horizon optimization. We
pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and
Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong
Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves
high long-sequence throughput while maintaining strong short-sequence speed,
reflecting benefits from budget allocation and attention design. To probe
retrieval-oriented quality, we add a small amount of open contrastive data:
fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking
(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.
Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding
on SimCLUE, suggesting a clear scaling path for STS with additional curated
pairs. We will release tokenizer and weights to facilitate reproducible
research.

</details>


### [85] [A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction](https://arxiv.org/abs/2510.12306)
*Cameron Morin,Matti Marttinen Larsson*

Main category: cs.CL

TL;DR: The paper presents an unsupervised pipeline for automating grammatical annotation in large corpora using large language models, achieving high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the time-intensive bottleneck of manual annotation in corpus linguistics, especially as corpora grow larger.

Method: The pipeline involves a four-phase workflow: prompt engineering, pre-hoc evaluation, automated batch processing, and post-hoc validation. Testing was performed using GPT-5 and 143,933 sentences from the Corpus of Historical American English (COHA).

Result: The pipeline annotated sentences with over 98% accuracy within 60 hours, showcasing its efficiency for corpus linguistics annotation tasks.

Conclusion: LLMs have proven capable of large-scale, accurate data preparation with minimal manual intervention, though challenges like cost, licensing, and ethical concerns remain.

Abstract: As natural language corpora expand at an unprecedented rate, manual
annotation remains a significant methodological bottleneck in corpus linguistic
work. We address this challenge by presenting a scalable, unsupervised pipeline
for automating grammatical annotation in voluminous corpora using large
language models (LLMs). Unlike previous supervised and iterative approaches,
our method employs a four-phase workflow: prompt engineering, pre-hoc
evaluation, automated batch processing, and post-hoc validation. We demonstrate
the pipeline's accessibility and effectiveness through a diachronic case study
of variation in the English consider construction. Using GPT-5 through the
OpenAI API, we annotate 143,933 sentences from the Corpus of Historical
American English (COHA) in under 60 hours, achieving 98%+ accuracy on two
sophisticated annotation procedures. Our results suggest that LLMs can perform
a range of data preparation tasks at scale with minimal human intervention,
opening new possibilities for corpus-based research, though implementation
requires attention to costs, licensing, and other ethical considerations.

</details>


### [86] [Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation](https://arxiv.org/abs/2510.12316)
*Greta Damo,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: This paper presents a novel framework for generating reliable counter-speech using knowledge-wise text generation with advanced Retrieval-Augmented Generation (RAG) pipelines, aimed at addressing reliability and scalability issues in the domain.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of current counter-speech generation methods, including lack of reliability, coherence, and scalability, which are critical in countering harmful content like hate speech.

Method: The authors employ a framework that models counter-speech as a knowledge-wise text generation process, utilizing a knowledge base comprising 32,792 texts from trusted sources like the United Nations Digital Library and EU agencies. This is paired with advanced RAG pipelines tailored for 8 target groups identified in hate speech literature.

Result: Empirical evaluations using the MultiTarget-CONAN dataset, standard metrics (JudgeLM), and human evaluation show that the proposed framework outperforms both traditional LLM baselines and other competitive models in counter-speech generation quality.

Conclusion: This framework and its corresponding knowledge base demonstrate the potential for studying trustworthy counter-speech generation for addressing hate speech and similar challenges, offering improvements in scalability, coherence, and reliability.

Abstract: Counter-speech generation is at the core of many expert activities, such as
fact-checking and hate speech, to counter harmful content. Yet, existing work
treats counter-speech generation as pure text generation task, mainly based on
Large Language Models or NGO experts. These approaches show severe drawbacks
due to the limited reliability and coherence in the generated countering text,
and in scalability, respectively. To close this gap, we introduce a novel
framework to model counter-speech generation as knowledge-wise text generation
process. Our framework integrates advanced Retrieval-Augmented Generation (RAG)
pipelines to ensure the generation of trustworthy counter-speech for 8 main
target groups identified in the hate speech literature, including women, people
of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,
and other. We built a knowledge base over the United Nations Digital Library,
EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792
texts. We use the MultiTarget-CONAN dataset to empirically assess the quality
of the generated counter-speech, both through standard metrics (i.e., JudgeLM)
and a human evaluation. Results show that our framework outperforms standard
LLM baselines and competitive approach, on both assessments. The resulting
framework and the knowledge base pave the way for studying trustworthy and
sound counter-speech generation, in hate speech and beyond.

</details>


### [87] [Fine-grained Analysis of Brain-LLM Alignment through Input Attribution](https://arxiv.org/abs/2510.12355)
*Michela Proietti,Roberto Capobianco,Mariya Toneva*

Main category: cs.CL

TL;DR: The study explores the alignment between large language models (LLMs) and human brain activity during language processing, using a novel attribution method.


<details>
  <summary>Details</summary>
Motivation: To uncover computational principles in language processing by examining the alignment between human brain activity and LLMs.

Method: A fine-grained input attribution method was introduced to analyze specific words' importance for brain-LLM alignment, particularly comparing brain alignment (BA) and next-word prediction (NWP).

Result: The analysis found that BA and NWP rely on distinct word sets, with BA focusing on semantic and discourse-level information, and NWP focusing on syntax and biases like recency and primacy.

Conclusion: The study highlights fundamental differences in the features that BA and NWP prioritize and offers a generalizable attribution method for cognitive and language studies.

Abstract: Understanding the alignment between large language models (LLMs) and human
brain activity can reveal computational principles underlying language
processing. We introduce a fine-grained input attribution method to identify
the specific words most important for brain-LLM alignment, and leverage it to
study a contentious research question about brain-LLM alignment: the
relationship between brain alignment (BA) and next-word prediction (NWP). Our
findings reveal that BA and NWP rely on largely distinct word subsets: NWP
exhibits recency and primacy biases with a focus on syntax, while BA
prioritizes semantic and discourse-level information with a more targeted
recency effect. This work advances our understanding of how LLMs relate to
human language processing and highlights differences in feature reliance
between BA and NWP. Beyond this study, our attribution method can be broadly
applied to explore the cognitive relevance of model predictions in diverse
language processing tasks.

</details>


### [88] [MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts](https://arxiv.org/abs/2510.12357)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.CL

TL;DR: MoBiLE, a novel inference framework for Mixture-of-Experts models, combines big-little experts with fallback mechanisms, achieving significant speed improvements with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address bandwidth limitations in CPU-GPU interconnect during MoE model inference and improve efficiency without compromising accuracy.

Method: MoBiLE introduces big-little experts to reduce the number of experts for low-importance tokens while maintaining quality for high-importance tokens. It uses fallback and prefetch mechanisms to enhance memory efficiency.

Result: MoBiLE improved inference speed by 1.60x to 1.72x compared to the baseline on consumer GPU systems with negligible accuracy degradation.

Conclusion: The proposed framework effectively enhances performance for MoE-based generative tasks on modern architectures while maintaining model quality and memory efficiency.

Abstract: Mixture-of-Experts (MoE) models have recently demonstrated exceptional
performance across a diverse range of applications. The principle of sparse
activation in MoE models facilitates an offloading strategy, wherein active
experts are maintained in GPU HBM, while inactive experts are stored in CPU
DRAM. The efficacy of this approach, however, is fundamentally constrained by
the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck,
existing approaches have employed prefetching to accelerate MoE inference.
These methods attempt to predict and prefetch the required experts using
specially trained modules. Nevertheless, such techniques are often encumbered
by significant training overhead and have shown diminished effectiveness on
recent MoE models with fine-grained expert segmentation.
  In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE
inference framework with \textit{mixture of big-little experts}. It reduces the
number of experts for unimportant tokens to half for acceleration while
maintaining full experts for important tokens to guarantee model quality.
Further, a dedicated fallback and prefetching mechanism is designed for
switching between little and big experts to improve memory efficiency. We
evaluate MoBiLE on four typical modern MoE architectures and challenging
generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to
1.72x compared to the baseline on a consumer GPU system, with negligible
degradation in accuracy.

</details>


### [89] [LLM-REVal: Can We Trust LLM Reviewers Yet?](https://arxiv.org/abs/2510.12367)
*Rui Li,Jia-Chen Gu,Po-Nien Kung,Heming Xia,Junfeng liu,Xiangwen Kong,Zhifang Sui,Nanyun Peng*

Main category: cs.CL

TL;DR: The paper evaluates the influence of integrating large language models (LLMs) into academic research and peer review processes, highlighting potential biases when LLMs are employed as reviewers.


<details>
  <summary>Details</summary>
Motivation: To explore the impact of LLMs on scholarly fairness and identify risks associated with their usage as peer reviewers.

Method: Simulation involving LLMs as both research agents generating/revising papers and review agents assessing submissions, combined with human annotations.

Result: LLMs show biases by inflating scores for LLM-authored papers and underrating human-authored papers with critical statements, favoring LLM-generated writing styles while disfavoring critical language.

Conclusion: LLM reviewers could jeopardize academic equity, but their guidance improves paper quality, showing promise for aiding early-stage researchers and low-quality submissions with caution.

Abstract: The rapid advancement of large language models (LLMs) has inspired
researchers to integrate them extensively into the academic workflow,
potentially reshaping how research is practiced and reviewed. While previous
studies highlight the potential of LLMs in supporting research and peer review,
their dual roles in the academic workflow and the complex interplay between
research and review bring new risks that remain largely underexplored. In this
study, we focus on how the deep integration of LLMs into both peer-review and
research processes may influence scholarly fairness, examining the potential
risks of using LLMs as reviewers by simulation. This simulation incorporates a
research agent, which generates papers and revises, alongside a review agent,
which assesses the submissions. Based on the simulation results, we conduct
human annotations and identify pronounced misalignment between LLM-based
reviews and human judgments: (1) LLM reviewers systematically inflate scores
for LLM-authored papers, assigning them markedly higher scores than
human-authored ones; (2) LLM reviewers persistently underrate human-authored
papers with critical statements (e.g., risk, fairness), even after multiple
revisions. Our analysis reveals that these stem from two primary biases in LLM
reviewers: a linguistic feature bias favoring LLM-generated writing styles, and
an aversion toward critical statements. These results highlight the risks and
equity concerns posed to human authors and academic research if LLMs are
deployed in the peer review cycle without adequate caution. On the other hand,
revisions guided by LLM reviews yield quality gains in both LLM-based and human
evaluations, illustrating the potential of the LLMs-as-reviewers for
early-stage researchers and enhancing low-quality papers.

</details>


### [90] [Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency](https://arxiv.org/abs/2510.12389)
*Hailay Kidu Teklehaymanot,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: The study investigates tokenization disparities in over 200 languages, revealing inequities in computational costs for non-Latin and low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To address barriers in equitable AI accessibility across diverse linguistic populations, motivated by tokenization inefficiencies.

Method: Standardized preprocessing and uniform tokenization using tiktoken library were applied, followed by comparative analysis using established metrics.

Result: Latin-script languages showed higher tokenization efficiency, while non-Latin/morphologically complex languages had greater token inflation and RTC ratios, causing computational disadvantages.

Conclusion: Current AI systems have inequities affecting non-Latin and low-resource language speakers. Future strategies should incorporate language diversity for equitable AI systems.

Abstract: Tokenization disparities pose a significant barrier to achieving equitable
access to artificial intelligence across linguistically diverse populations.
This study conducts a large-scale cross-linguistic evaluation of tokenization
efficiency in over 200 languages to systematically quantify computational
inequities in large language models (LLMs). Using a standardized experimental
framework, we applied consistent preprocessing and normalization protocols,
followed by uniform tokenization through the tiktoken library across all
language samples. Comprehensive tokenization statistics were collected using
established evaluation metrics, including Tokens Per Sentence (TPS) and
Relative Tokenization Cost (RTC), benchmarked against English baselines. Our
cross-linguistic analysis reveals substantial and systematic disparities:
Latin-script languages consistently exhibit higher tokenization efficiency,
while non-Latin and morphologically complex languages incur significantly
greater token inflation, often 3-5 times higher RTC ratios. These
inefficiencies translate into increased computational costs and reduced
effective context utilization for underrepresented languages. Overall, the
findings highlight structural inequities in current AI systems, where speakers
of low-resource and non-Latin languages face disproportionate computational
disadvantages. Future research should prioritize the development of
linguistically informed tokenization strategies and adaptive vocabulary
construction methods that incorporate typological diversity, ensuring more
inclusive and computationally equitable multilingual AI systems.

</details>


### [91] [PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12434)
*Xiangjun Zai,Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Wenjie Zhang*

Main category: cs.CL

TL;DR: PRoH is a new framework designed to overcome limitations of existing KH-based RAG methods by integrating dynamic planning and reasoning mechanisms for better multi-hop question answering.


<details>
  <summary>Details</summary>
Motivation: Existing KH-based RAG methods are limited by static retrieval planning, non-adaptive execution, and superficial KH utilization, hindering effective multi-hop reasoning.

Method: PRoH introduces context-aware planning for local KH neighborhood analysis, adaptive question decomposition using a dynamically evolving DAG, and an EWO-guided retrieval algorithm for semantically coherent reasoning paths.

Result: Experiments showed PRoH surpasses HyperGraphRAG by 19.73% in F1 and 8.41% in Generation Evaluation score, proving its effectiveness and robustness for multi-hop reasoning across domains.

Conclusion: PRoH provides a significant advancement in RAG tasks by leveraging dynamic KH planning and reasoning strategies, improving accuracy and robustness in multi-hop question answering.

Abstract: Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.

</details>


### [92] [Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12460)
*Linfeng Gao,Baolong Bi,Zheng Yuan,Le Wang,Zerui Chen,Zhimin Wei,Shenghua Liu,Qinggang Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: This paper addresses contextual faithfulness issues in Retrieval-Augmented Generation (RAG), proposing CLEAR, a framework emphasizing sentence-level context decomposition, conflict localization, and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing RAG systems often produce responses that contradict retrieved evidence, and current solutions treat LLMs as black boxes without examining how they internally integrate retrieved evidence with parametric memory.

Method: The paper conducts probing-based analysis of LLMs' hidden states to understand knowledge integration and conflict manifestation. Based on findings, CLEAR applies sentence-level decomposition, conflict localization, and conflict-aware fine-tuning.

Result: CLEAR significantly improves accuracy and contextual faithfulness across different benchmarks, outperforming strong baselines in various conflict scenarios.

Conclusion: The CLEAR framework advances RAG systems by enhancing evidence integration through a deeper understanding and targeted fine-tuning, establishing a new standard for accuracy and faithfulness in LLM responses.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.

</details>


### [93] [Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test](https://arxiv.org/abs/2510.12463)
*Nikoleta Pantelidou,Evelina Leivada,Paolo Morosi*

Main category: cs.CL

TL;DR: The study assesses Large Language Models (LLMs) for morphological generalization in Catalan, English, Greek, and Spanish using an adaptation of the Wug Test. Results show LLMs generalize morphological processes like humans but are influenced more by language resource availability than complexity.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs replicate human linguistic competence and understand the factors affecting model accuracy in morphological generalization across various languages.

Method: Six LLMs were tested in a multilingual morphological generalization task using novel words from four languages. Results were compared with human speakers and analyzed for correlation with linguistic complexity and training data availability.

Result: LLMs displayed high accuracy in generalizing morphological processes to unseen words, outperforming in resource-rich languages (English and Spanish) compared to resource-scarce ones (Catalan and Greek). Accuracy patterns correlated more with training data amount than linguistic complexity.

Conclusion: Model behavior reflects a shallow resemblance to human linguistic competence, largely driven by the availability of linguistic resources, rather than an intrinsic sensitivity to grammatical complexity.

Abstract: The linguistic abilities of Large Language Models are a matter of ongoing
debate. This study contributes to this discussion by investigating model
performance in a morphological generalization task that involves novel words.
Using a multilingual adaptation of the Wug Test, six models were tested across
four partially unrelated languages (Catalan, English, Greek, and Spanish) and
compared with human speakers. The aim is to determine whether model accuracy
approximates human competence and whether it is shaped primarily by linguistic
complexity or by the quantity of available training data. Consistent with
previous research, the results show that the models are able to generalize
morphological processes to unseen words with human-like accuracy. However,
accuracy patterns align more closely with community size and data availability
than with structural complexity, refining earlier claims in the literature. In
particular, languages with larger speaker communities and stronger digital
representation, such as Spanish and English, revealed higher accuracy than
less-resourced ones like Catalan and Greek. Overall, our findings suggest that
model behavior is mainly driven by the richness of linguistic resources rather
than by sensitivity to grammatical complexity, reflecting a form of performance
that resembles human linguistic competence only superficially.

</details>


### [94] [SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression](https://arxiv.org/abs/2510.12474)
*Biao Zhang,Lixin Chen,Tong Liu,Bo Zheng*

Main category: cs.CL

TL;DR: The paper introduces SMEC, a novel framework for compressing large language model embeddings, offering significant dimensionality reduction while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: High-dimensional embeddings from large language models lead to computational complexity and storage issues, requiring an effective compression method.

Method: The paper proposes SMEC, incorporating SMRL for reduced gradient variance, ADS for minimal information loss during pruning, and S-XBM for enhancing unsupervised learning between varying dimensions.

Result: SMEC shows effective dimensionality reduction with maintained performance across image, text, and multimodal data, outperforming existing compression models on benchmarks like the BEIR dataset.

Conclusion: SMEC demonstrates its capability as a robust framework for embedding compression, balancing compactness with high performance in various applications.

Abstract: Large language models (LLMs) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.

</details>


### [95] [When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection](https://arxiv.org/abs/2510.12476)
*Lang Gao,Xuhui Li,Chenxi Wang,Mingzhe Li,Wei Liu,Zirui Song,Jinghui Zhang,Rui Yan,Preslav Nakov,Xiuying Chen*

Main category: cs.CL

TL;DR: This paper analyzes the risks of identity impersonation by Large Language Models (LLMs) and introduces a benchmark to assess the robustness of detectors against personalized machine-generated text (MGT).


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the growing threat of identity impersonation via personalized LLM-generated text and the lack of prior research in detecting such personalized MGT.

Method: The researchers introduced a benchmark called \dataset, focusing on literary and blog texts paired with LLM-generated imitations, and proposed \method to predict detector performance changes by analyzing inverted features in personalized text.

Result: Experimental results revealed significant performance drops in state-of-the-art detectors when applied to personalized settings due to the feature-inversion trap. \method displayed an 85% correlation with actual performance gaps.

Conclusion: This study highlights the limitations of current detectors for personalized MGT, proposes a predictive framework \method, and calls for further research in personalized text detection.

Abstract: Large language models (LLMs) have grown more powerful in language generation,
producing fluent text and even imitating personal style. Yet, this ability also
heightens the risk of identity impersonation. To the best of our knowledge, no
prior work has examined personalized machine-generated text (MGT) detection. In
this paper, we introduce \dataset, the first benchmark for evaluating detector
robustness in personalized settings, built from literary and blog texts paired
with their LLM-generated imitations. Our experimental results demonstrate large
performance gaps across detectors in personalized settings: some
state-of-the-art models suffer significant drops. We attribute this limitation
to the \textit{feature-inversion trap}, where features that are discriminative
in general domains become inverted and misleading when applied to personalized
text. Based on this finding, we propose \method, a simple and reliable way to
predict detector performance changes in personalized settings. \method
identifies latent directions corresponding to inverted features and constructs
probe datasets that differ primarily along these features to evaluate detector
dependence. Our experiments show that \method can accurately predict both the
direction and the magnitude of post-transfer changes, showing 85\% correlation
with the actual performance gaps. We hope that this work will encourage further
research on personalized text detection.

</details>


### [96] [BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)](https://arxiv.org/abs/2510.12516)
*Tomas Ruiz,Siyao Peng,Barbara Plank,Carsten Schwemmer*

Main category: cs.CL

TL;DR: The paper explores test-time scaling techniques to enhance LLM outputs in evaluating annotation disagreements, achieving consistent improvements with two methods but not with Best-of-N sampling.


<details>
  <summary>Details</summary>
Motivation: To transfer and test whether test-time scaling techniques, originally applied in domains with verifiably correct answers, can be utilized in tasks evaluating annotation disagreements.

Method: The authors tested three methods: Model Averaging, Majority Voting, and Best-of-N sampling on the LeWiDi-2025 tasks.

Result: Model Averaging and Majority Voting methods consistently improved LLM performance, but the Best-of-N sampling method failed to show effectiveness in this context.

Conclusion: Test-time scaling methods like Model Averaging and Majority Voting can improve LLM performance for annotation disagreements, while Best-of-N does not translate well from mathematics to LeWiDi tasks, requiring further analysis.

Abstract: Test-time scaling is a family of techniques to improve LLM outputs at
inference time by performing extra computation. To the best of our knowledge,
test-time scaling has been limited to domains with verifiably correct answers,
like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025
tasks to evaluate annotation disagreements. We experiment with three test-time
scaling methods: two benchmark algorithms (Model Averaging and Majority
Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM
performance consistently on the LeWiDi tasks, but the Best-of-N method does
not. Our experiments suggest that the Best-of-N method does not currently
transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for
this gap.

</details>


### [97] [VISaGE: Understanding Visual Generics and Exceptions](https://arxiv.org/abs/2510.12548)
*Stella Frank,Emily Allaway*

Main category: cs.CL

TL;DR: The paper introduces a new evaluation dataset called VISaGE to investigate how Vision Language Models (VLMs) manage trade-offs between pragmatic and semantic priors when analyzing atypical inputs.


<details>
  <summary>Details</summary>
Motivation: To explore how VLMs balance their conceptual understanding (semantic prior) against their assumption of input congruency (pragmatic prior) when analyzing both typical and exceptional instances.

Method: The authors constructed VISaGE, a carefully designed evaluation dataset featuring both typical and incongruent images, to conduct experiments that measure how the violation of congruent inputs affects a VLM's conceptual understanding.

Result: Experimental results show that when input congruency is violated (incongruent images), VLMs' conceptual understanding deteriorates. This decrease is more pronounced compared to the effect of the semantic prior when querying individual instances.

Conclusion: The study reveals that the pragmatic prior dominates over the semantic prior in influencing VLM behavior, emphasizing the limitations of VLMs in understanding atypical scenarios.

Abstract: While Vision Language Models (VLMs) learn conceptual representations, in the
form of generalized knowledge, during training, they are typically used to
analyze individual instances. When evaluation instances are atypical, this
paradigm results in tension between two priors in the model. The first is a
pragmatic prior that the textual and visual input are both relevant, arising
from VLM finetuning on congruent inputs; the second is a semantic prior that
the conceptual representation is generally true for instances of the category.
In order to understand how VLMs trade off these priors, we introduce a new
evaluation dataset, VISaGE, consisting of both typical and exceptional images.
In carefully balanced experiments, we show that conceptual understanding
degrades when the assumption of congruency underlying the pragmatic prior is
violated with incongruent images. This effect is stronger than the effect of
the semantic prior when querying about individual instances.

</details>


### [98] [Teaching Language Models to Faithfully Express their Uncertainty](https://arxiv.org/abs/2510.12587)
*Bryan Eikema,Evgenia Ilia,José G. C. de Souza,Chrysoula Zerva,Wilker Aziz*

Main category: cs.CL

TL;DR: The paper addresses large language models' (LLMs) inability to faithfully express uncertainty and proposes Faithful Uncertainty Tuning (FUT) to improve this aspect without compromising answer accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs often fail to communicate their uncertainty accurately, misleading users about their knowledge's reliability and introducing a 'faithfulness gap' in their responses.

Method: The authors introduce 'Faithful Uncertainty Tuning' (FUT), which fine-tunes instruction-tuned LLMs by augmenting training data with verbal uncertainty hedges that reflect response consistency, requiring minimal supervision.

Result: FUT significantly improves the faithfulness of uncertainty communication in LLMs while maintaining answer accuracy and minimizing semantic distribution shifts, showing robustness across strategies and forms of expressing uncertainty.

Conclusion: FUT offers a simple and effective solution for making LLMs communicate their uncertainty more faithfully, providing a step forward in enhancing trust and reliability in AI responses.

Abstract: Large language models (LLMs) often miscommunicate their uncertainty: repeated
queries can produce divergent answers, yet generated responses are typically
unhedged or hedged in ways that do not reflect this variability. This conveys
unfaithful information about the uncertain state of the LLMs' knowledge,
creating a faithfulness gap that affects even strong LLMs. We introduce
Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches
instruction-tuned LLMs to express uncertainty faithfully without altering their
underlying answer distribution. We construct training data by augmenting model
samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or
'likely') aligned with sample consistency, requiring no supervision beyond the
model and a set of prompts. We evaluate FUT on open-domain question answering
(QA) across multiple models and datasets. Our results show that FUT
substantially reduces the faithfulness gap, while preserving QA accuracy and
introducing minimal semantic distribution shift. Further analyses demonstrate
robustness across decoding strategies, choice of hedgers, and other forms of
uncertainty expression (i.e. numerical). These findings establish FUT as a
simple and effective way to teach LLMs to communicate uncertainty faithfully.

</details>


### [99] [StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis](https://arxiv.org/abs/2510.12608)
*Siyuan Li,Aodu Wulianghai,Xi Lin,Guangyan Li,Xiang Chen,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: StyleDecipher is a detection framework for distinguishing LLM-generated text from human-written text, achieving high accuracy and robustness across various domains.


<details>
  <summary>Details</summary>
Motivation: The increasing use of large language models (LLMs) in writing heightens the need to detect machine-generated text to maintain authenticity and trust.

Method: StyleDecipher quantifies stylistic differences by combining discrete stylistic indicators and continuous representations from semantic embeddings into a unified space without relying on model specifics or labeled data.

Result: Experiments in five domains show StyleDecipher achieves state-of-the-art detection accuracy, surpassing baselines by up to 36.30% in cross-domain evaluations while resisting adversarial and hybrid content.

Conclusion: StyleDecipher provides a robust, explainable solution for LLM-generated text detection, addressing challenges like stylistic diversity and hybrid authorship.

Abstract: With the increasing integration of large language models (LLMs) into
open-domain writing, detecting machine-generated text has become a critical
task for ensuring content authenticity and trust. Existing approaches rely on
statistical discrepancies or model-specific heuristics to distinguish between
LLM-generated and human-written text. However, these methods struggle in
real-world scenarios due to limited generalization, vulnerability to
paraphrasing, and lack of explainability, particularly when facing stylistic
diversity or hybrid human-AI authorship. In this work, we propose
StyleDecipher, a robust and explainable detection framework that revisits
LLM-generated text detection using combined feature extractors to quantify
stylistic differences. By jointly modeling discrete stylistic indicators and
continuous stylistic representations derived from semantic embeddings,
StyleDecipher captures distinctive style-level divergences between human and
LLM outputs within a unified representation space. This framework enables
accurate, explainable, and domain-agnostic detection without requiring access
to model internals or labeled segments. Extensive experiments across five
diverse domains, including news, code, essays, reviews, and academic abstracts,
demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain
accuracy. Moreover, in cross-domain evaluations, it surpasses existing
baselines by up to 36.30%, while maintaining robustness against adversarial
perturbations and mixed human-AI content. Further qualitative and quantitative
analysis confirms that stylistic signals provide explainable evidence for
distinguishing machine-generated text. Our source code can be accessed at
https://github.com/SiyuanLi00/StyleDecipher.

</details>


### [100] [ACADATA: Parallel Dataset of Academic Data for Machine Translation](https://arxiv.org/abs/2510.12621)
*Iñaki Lacunza,Javier Garcia Gilabert,Francesca De Luca Fornaciari,Javier Aula-Blasco,Aitor Gonzalez-Agirre,Maite Melero,Marta Villegas*

Main category: cs.CL

TL;DR: The paper introduces ACADATA, a dataset designed to enhance academic translations, including a training set (ACAD-TRAIN) and an evaluation set (ACAD-BENCH). Fine-tuned language models using ACAD-TRAIN demonstrate significant improvements in translation quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the quality and accuracy of academic translations and long-context translations by providing a high-quality dataset and fine-tuned models.

Method: A high-quality dataset ACADATA is created, comprising ACAD-TRAIN (1.5 million paragraph pairs) and ACAD-BENCH (6,000 curated translations). Large Language Models were fine-tuned on ACAD-TRAIN and benchmarked against various machine translation models.

Result: Fine-tuning on ACAD-TRAIN improves translation performance by +6.1 and +12.4 d-BLEU points for 7B and 2B models, respectively. The approach also enhances general long-context translation by up to 24.9% and outperforms top proprietary models in academic translation.

Conclusion: The datasets and fine-tuned models significantly improve academic translation performance, surpassing existing proprietary and open-weight models. They offer valuable resources for advancing research in academic and long-context translation.

Abstract: We present ACADATA, a high-quality parallel dataset for academic translation,
that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5
million author-generated paragraph pairs across 96 language directions and
ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12
directions. To validate its utility, we fine-tune two Large Language Models
(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized
machine-translation systems, general-purpose, open-weight LLMs, and several
large-scale proprietary models. Experimental results demonstrate that
fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality
by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,
while also improving long-context translation in a general domain by up to
24.9% when translating out of English. The fine-tuned top-performing model
surpasses the best propietary and open-weight models on academic translation
domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we
provide the community with a valuable resource to advance research in academic
domain and long-context translation.

</details>


### [101] [COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions](https://arxiv.org/abs/2510.12637)
*Nzubechukwu C. Ohalete,Kevin B. Gittner,Lauren M. Matheny*

Main category: cs.CL

TL;DR: The paper presents COSTAR-A, a refined prompt engineering framework, which improves upon the original COSTAR prompting method by adding an 'Answer' component, optimizing outputs for smaller LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing prompting techniques face challenges in optimizing outputs across different scales of LLMs, particularly smaller, localized versions. This paper aims to address inconsistent performance and adaptability issues.

Method: The study refines the original COSTAR framework by adding the 'Answer' component, conducts controlled experiments across different fine-tuned LLMs, and evaluates output consistency and structure.

Result: COSTAR-A improved output clarity and quality for smaller LLMs, like Llama 3.1-8B, in certain tasks compared to the original COSTAR framework. However, effectiveness varied by model and use case.

Conclusion: COSTAR-A provides a scalable and adaptable prompting approach, especially tailored for smaller, resource-constrained models, enriching computational efficiency in those setups.

Abstract: Large Language Models (LLMs) are highly sensitive to prompt design, and
making optimized prompting techniques is crucial for generating consistent,
high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt
engineering framework that enhances the existing COSTAR method, which stands
for Context, Objective, Style, Tone, Audience, and Response, by adding the
'Answer' component at the end. We demonstrate that while the original COSTAR
framework improves prompt clarity and aligns outputs for larger LLMs, its
performance is less consistent with smaller, locally optimized models,
particularly in tasks that require more directive or constrained outputs.
Through a series of controlled prompt-output assessments with smaller (at most
8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance
the output structure and decisiveness of localized LLMs for certain tasks,
although its effectiveness varies across models and use cases. Notably, the
Llama 3.1-8B model exhibited performance improvements when prompted with
COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability
and scalability of COSTAR-A as a prompting framework, particularly in
computationally efficient AI deployments on resource-constrained hardware.

</details>


### [102] [Reasoning Pattern Matters: Learning to Reason without Human Rationales](https://arxiv.org/abs/2510.12643)
*Chaoxu Pang,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: This paper addresses cost reduction in rationale annotations for reasoning tasks in LLMs by leveraging automated rationale generation tools.


<details>
  <summary>Details</summary>
Motivation: Creating high-quality human rationale annotations is resource-intensive and expensive, particularly for patterned reasoning tasks.

Method: Using numerical semantic matching, the authors introduce PARO, a framework enabling LLMs to generate rationales based on reasoning patterns with minimal human supervision.

Result: PARO-generated rationales perform comparably to human-generated rationales despite being significantly smaller in scale, reducing annotation costs.

Conclusion: LLM-based automated rationale generation offers a viable alternative to human annotations while maintaining reasoning performance standards in large language model training.

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning
capabilities under the widely adopted SFT+RLVR paradigm, which first performs
Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories
(rationales) to establish initial reasoning behaviors, then applies
Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model
using verifiable signals without golden rationales. However, annotating
high-quality rationales for the SFT stage remains prohibitively expensive. This
paper investigates when and how rationale annotation costs can be substantially
reduced without compromising reasoning performance. We identify a broad class
of problems, termed patterned reasoning tasks, where reasoning follows a fixed,
procedural strategy consistent across instances. Although instances vary in
content such as domain knowledge, factual information, or numeric values, the
solution derives from applying a shared reasoning pattern. We argue that the
success of SFT+RLVR on such tasks primarily stems from its ability to enable
models to internalize these reasoning patterns. Using numerical semantic
matching as a representative task, we provide both causal and behavioral
evidence showing that reasoning patterns rather than the quantity or quality of
rationales are the key determinant of performance. Building on these insights,
we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet
effective framework that enables LLMs to generate rationales aligned with
task-specific reasoning patterns without requiring human rationale annotations.
Experiments show that PARO-generated rationales achieve comparable SFT+RLVR
performance to human rationales that are 10 times larger. These results suggest
that large-scale human rationale annotations can be replaced with LLM-based
automatic annotations requiring only limited human supervision over reasoning
patterns.

</details>


### [103] [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](https://arxiv.org/abs/2510.12699)
*Sunny Yu,Ahmad Jabbar,Robert Hawkins,Dan Jurafsky,Myra Cheng*

Main category: cs.CL

TL;DR: The paper introduces the concept of 'generation space size' (GSS) to address issues of miscalibration in large language models (LLMs) and presents GSSBench, a benchmark to assess and improve diverse generation in creative and factual tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with miscalibrated output diversity, producing overly uniform outputs for creative tasks and overly diverse but inaccurate outputs for factual tasks. This points to the need for a principled approach to balance generation diversity.

Method: The paper introduces GSS (generation space size) as a way to measure and calibrate model outputs. It evaluates this using GSSBench, a benchmark composed of prompt pairs with ground-truth GSS relationships, and uses metrics like EigenScore to analyze model behavior.

Result: The study found that hallucination detection metrics, especially EigenScore, outperform standard metrics in measuring GSS and provide insights into how models internally represent tasks. GSS was also applied successfully to address prompt ambiguity, reasoning issues, and generating diverse outputs.

Conclusion: GSS and associated tools offer a unified framework to assess and steer LLMs for better, more task-appropriate diversity in outputs, thereby addressing pressing issues like hallucination and creativity miscalibration.

Abstract: Different open-ended generation tasks require different degrees of output
diversity. However, current LLMs are often miscalibrated. They collapse to
overly homogeneous outputs for creative tasks and hallucinate diverse but
incorrect responses for factual tasks. We argue that these two failure modes
are unified by, and can both be addressed by, the notion of effective
generation space size (GSS) -- the set of semantically distinct outputs a model
considers for a prompt. We present GSSBench, a task suite of prompt pairs with
ground-truth GSS relationships to assess different metrics and understand where
models diverge from desired behavior. We find that hallucination detection
metrics, particularly EigenScore, consistently outperform standard diversity
and uncertainty quantification metrics, while using only model internals,
providing interpretable insights into a model's internal task representations.
We demonstrate three applications of GSS: (1) detecting prompt ambiguity and
predicting clarification questions for better grounding, (2) interpreting
overthinking and underthinking in reasoning models, and (3) steering models to
expand their generation space to yield high-quality and diverse outputs.

</details>


### [104] [Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception](https://arxiv.org/abs/2510.12720)
*Ziyang Ma,Ruiyang Xu,Zhenghao Xing,Yunfei Chu,Yuxuan Wang,Jinzheng He,Jin Xu,Pheng-Ann Heng,Kai Yu,Junyang Lin,Eng Siong Chng,Xie Chen*

Main category: cs.CL

TL;DR: The study explores fine-grained perception in Omni Language Models (OLMs) and introduces methods to enhance detailed audio-visual understanding.


<details>
  <summary>Details</summary>
Motivation: To advance human-AI interaction by improving fine-grained understanding and reasoning in multimodal (audio-visual) information processing.

Method: Proposed an agentic data generation pipeline, Omni-Detective, to create detailed and minimally hallucinatory data; trained two captioning models (Audio-Captioner and Omni-Captioner); developed Omni-Cloze as a benchmark for assessing detailed perception.

Result: Achieved state-of-the-art performance in detailed captioning tasks, surpassing or matching leading models on benchmarks like MMAU, MMAR, VDC, and video-SALMONN 2.

Conclusion: Omni-Detective effectively enhances detailed perception while reducing hallucination, and the Omni-Cloze benchmark offers reliable evaluation of detailed captions.

Abstract: Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.

</details>


### [105] [Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages](https://arxiv.org/abs/2510.12722)
*Nadine El-Naggar,Tatsuki Kuribayashi,Ted Briscoe*

Main category: cs.CL

TL;DR: The paper explores whether language models (LMs) prefer typologically common grammatical properties over rare ones using artificial languages enriched with advanced constructions.


<details>
  <summary>Details</summary>
Motivation: To understand if LMs inherently favor frequent, typologically plausible grammatical structures over unlikely ones.

Method: Extended artificial language formalization using Generalized Categorial Grammar and focused evaluation on LM generalization towards unseen, longer test sentences.

Result: LMs demonstrated better generalization ability for typologically plausible word orders, indicating alignment with linguistic typology.

Conclusion: LMs exhibit inductive biases favoring typologically plausible grammatical structures, showing their alignment with properties of natural languages.

Abstract: Whether language models (LMs) have inductive biases that favor typologically
frequent grammatical properties over rare, implausible ones has been
investigated, typically using artificial languages (ALs) (White and Cotterell,
2021; Kuribayashi et al., 2024). In this paper, we extend these works from two
perspectives. First, we extend their context-free AL formalization by adopting
Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover
attested but previously overlooked constructions, such as unbounded dependency
and mildly context-sensitive structures. Second, our evaluation focuses more on
the generalization ability of LMs to process unseen longer test sentences.
Thus, our ALs better capture features of natural languages and our experimental
paradigm leads to clearer conclusions -- typologically plausible word orders
tend to be easier for LMs to productively generalize.

</details>


### [106] [Hey, wait a minute: on at-issue sensitivity in Language Models](https://arxiv.org/abs/2510.12740)
*Sanghee J. Kim,Kanishka Misra*

Main category: cs.CL

TL;DR: The study leverages 'at-issueness' to evaluate dialogue naturalness in language models (LMs) and introduces the DGRC method for systematic testing of discourse sensitivity.


<details>
  <summary>Details</summary>
Motivation: Current methods to evaluate the naturalness of dialogue are limited and subjective, prompting the need for scalable and systematic approaches.

Method: The proposed DGRC method involves dividing a dialogue, generating continuations using LMs, recombining parts, and comparing likelihoods of recombined sequences.

Result: The study observed that LMs show a preference for continuing at-issue content, with instruct-tuned models demonstrating an enhanced effect. LMs also adapt to cues like 'Hey, wait a minute' by reducing their at-issue preference.

Conclusion: DGRC offers a scalable method to study dialogue dynamics, showing that LMs can exhibit behavior reflective of natural dialogue patterns, especially in handling at-issue content.

Abstract: Evaluating the naturalness of dialogue in language models (LMs) is not
trivial: notions of 'naturalness' vary, and scalable quantitative metrics
remain limited. This study leverages the linguistic notion of 'at-issueness' to
assess dialogue naturalness and introduces a new method: Divide, Generate,
Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)
generates continuations for subparts using LMs, (iii) recombines the dialogue
and continuations, and (iv) compares the likelihoods of the recombined
sequences. This approach mitigates bias in linguistic analyses of LMs and
enables systematic testing of discourse-sensitive behavior. Applying DGRC, we
find that LMs prefer to continue dialogue on at-issue content, with this effect
enhanced in instruct-tuned models. They also reduce their at-issue preference
when relevant cues (e.g., "Hey, wait a minute") are present. Although
instruct-tuning does not further amplify this modulation, the pattern reflects
a hallmark of successful dialogue dynamics.

</details>


### [107] [Language Models Model Language](https://arxiv.org/abs/2510.12766)
*Łukasz Borchmann*

Main category: cs.CL

TL;DR: This paper challenges traditional linguistic critiques of LLMs by applying Witold Mańczak's empiricist viewpoint, emphasizing frequency of language use as the key principle.


<details>
  <summary>Details</summary>
Motivation: To address criticisms of LLMs from traditional linguistic theories and propose a shift towards a frequency-based framework to understand and evaluate LLMs.

Method: The paper adopts Witold Mańczak's empiricist linguistic framework, focusing on language as the totality of usage and frequency-driven principles, to reinterpret critiques and guide language model development.

Result: The argument reframes critiques of LLMs, presenting a constructive perspective for their design and evaluation using frequency-based linguistic principles.

Conclusion: This perspective lays the groundwork for a new methodology to assess LLMs, emphasizing practical usage frequency over traditional theoretical constructs like 'deep structure.'

Abstract: Linguistic commentary on LLMs, heavily influenced by the theoretical
frameworks of de Saussure and Chomsky, is often speculative and unproductive.
Critics challenge whether LLMs can legitimately model language, citing the need
for "deep structure" or "grounding" to achieve an idealized linguistic
"competence." We argue for a radical shift in perspective towards the
empiricist principles of Witold Ma\'nczak, a prominent general and historical
linguist. He defines language not as a "system of signs" or a "computational
system of the brain" but as the totality of all that is said and written. Above
all, he identifies frequency of use of particular language elements as
language's primary governing principle. Using his framework, we challenge prior
critiques of LLMs and provide a constructive guide for designing, evaluating,
and interpreting language models.

</details>


### [108] [Dr.LLM: Dynamic Layer Routing in LLMs](https://arxiv.org/abs/2510.12773)
*Ahmed Heakl,Martin Gubri,Salman Khan,Sangdoo Yun,Seong Joon Oh*

Main category: cs.CL

TL;DR: Dr.LLM introduces a framework for pretrained models to dynamically adjust layer usage, improving efficiency and accuracy without changing base weights.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiency and flexibility issues in LLMs caused by uniformly processing tokens through all transformer layers.

Method: The study retrofits pretrained models with per-layer routers that decide to skip, execute, or repeat blocks. The routers are trained using Monte Carlo Tree Search (MCTS) for optimal layer configurations under compute budgets.

Result: Dr.LLM achieves up to 3.4% accuracy improvement and reduces layer usage by an average of 5 on the ARC (logic) and DART (math) tasks. It generalizes well to out-of-domain tasks with minimal accuracy drop while maintaining efficiency.

Conclusion: Explicitly supervised routers can enhance the efficiency and accuracy of frozen LLMs without altering their base weights, making them suitable for budget-aware, accuracy-driven inference.

Abstract: Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.

</details>


### [109] [Cost Analysis of Human-corrected Transcription for Predominately Oral Languages](https://arxiv.org/abs/2510.12781)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Michael Leventhal*

Main category: cs.CL

TL;DR: Creating accurate annotated speech datasets for low-resource languages requires substantial human labor, with Bambara transcription demanding 30-36 hours of work per hour of speech data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address gaps in understanding the time and effort required to produce high-quality speech datasets for low-resource languages, focusing on Bambara.

Method: The study involved a one-month field experiment with ten native transcribers correcting ASR-generated Bambara speech transcriptions for 53 hours of voice data.

Result: It takes, on average, 30 hours of human labor to transcribe one hour of speech in laboratory settings and 36 hours in field settings for Bambara.

Conclusion: The findings offer both a baseline and practical guidance for developing NLP resources for other low-resource languages with similar profiles.

Abstract: Creating speech datasets for low-resource languages is a critical yet poorly
understood challenge, particularly regarding the actual cost in human labor.
This paper investigates the time and complexity required to produce
high-quality annotated speech data for a subset of low-resource languages, low
literacy Predominately Oral Languages, focusing on Bambara, a Manding language
of Mali. Through a one-month field study involving ten transcribers with native
proficiency, we analyze the correction of ASR-generated transcriptions of 53
hours of Bambara voice data. We report that it takes, on average, 30 hours of
human labor to accurately transcribe one hour of speech data under laboratory
conditions and 36 hours under field conditions. The study provides a baseline
and practical insights for a large class of languages with comparable profiles
undertaking the creation of NLP resources.

</details>


### [110] [RePro: Training Language Models to Faithfully Recycle the Web for Pretraining](https://arxiv.org/abs/2510.10681)
*Zichun Yu,Chenyan Xiong*

Main category: cs.CL

TL;DR: RePro is a novel method for recycling web data into high-quality pretraining material for language models, achieving significant accuracy improvements and data efficiency.


<details>
  <summary>Details</summary>
Motivation: The development of large language models relies heavily on high-quality pretraining data, but current data reserves are insufficient for advancing frontier models.

Method: RePro uses a 4B model trained with reinforcement learning, leveraging one quality reward and three faithfulness rewards to generate rephrasings of data that retain semantic and structural fidelity.

Result: RePro provides 4.7%-14.0% accuracy improvements across various tasks, surpasses the performance of leading web recycling methods, and increases data efficiency by 2-3x.

Conclusion: RePro offers an efficient and controllable approach to transforming existing web data into high-quality pretraining material, proving its viability for optimizing data usage in model training.

Abstract: High-quality pretraining data is the fossil fuel of large language models
(LLMs), yet its reserves are running low for frontier models. In this paper, we
introduce RePro, a novel web recycling method that trains a relatively small LM
with reinforcement learning to generate effective and faithful rephrasings of
pretraining data. Specifically, we design one quality reward and three
faithfulness rewards, optimizing the LM rephraser to convert organic data into
high-quality rephrasings while maintaining its core semantics and structure. In
our experiment, we train a 4B rephraser to recycle 72B tokens sampled from
DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that
RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on
22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web
recycling method that prompts a 70B rephraser, as well as the organic baseline
with a 4x larger data pool. Experiments with different amounts of recycled data
highlight that RePro improves organic data efficiency by 2-3x. Individual and
distributional analyses validate that RePro preserves more critical information
and faithfully reflects the characteristics of organic data compared to
prompting-based methods. Together, these results show that RePro provides an
efficient and controllable path to effectively harness the fossil fuel of LLM
pretraining. We open-source our code, rephraser, and recycled data at
https://github.com/cxcscmu/RePro.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [111] [Enhancing the Quality of 3D Lunar Maps Using JAXA's Kaguya Imagery](https://arxiv.org/abs/2510.11817)
*Yumi Iwashita,Haakon Moe,Yang Cheng,Adnan Ansar,Georgios Georgakis,Adrian Stoica,Kazuto Nakashima,Ryo Kurazume,Jim Torresen*

Main category: cs.CV

TL;DR: This paper addresses the inaccuracies in 3D lunar maps from Kaguya TC images due to stereo matching errors and JPEG compression artifacts, proposing an approach to mitigate disparity noise and improve elevation accuracy.


<details>
  <summary>Details</summary>
Motivation: With increasing global lunar exploration initiatives, such as NASA's Endurance mission aiming to traverse long distances, there is a growing need for precise and reliable 3D lunar maps.

Method: The method involves analyzing the compression behavior in Kaguya TC images and addressing systematic disparity noise patterns, particularly in darker regions, to enhance the disparity maps from compressed data.

Result: The experimental results demonstrate successful reduction of elevation noise, leading to improved quality and reliability of 3D lunar maps.

Conclusion: The proposed technique enhances map quality, contributing to safer and more effective lunar mission planning and execution.

Abstract: As global efforts to explore the Moon intensify, the need for high-quality 3D
lunar maps becomes increasingly critical-particularly for long-distance
missions such as NASA's Endurance mission concept, in which a rover aims to
traverse 2,000 km across the South Pole-Aitken basin. Kaguya TC (Terrain
Camera) images, though globally available at 10 m/pixel, suffer from altitude
inaccuracies caused by stereo matching errors and JPEG-based compression
artifacts. This paper presents a method to improve the quality of 3D maps
generated from Kaguya TC images, focusing on mitigating the effects of
compression-induced noise in disparity maps. We analyze the compression
behavior of Kaguya TC imagery, and identify systematic disparity noise
patterns, especially in darker regions. In this paper, we propose an approach
to enhance 3D map quality by reducing residual noise in disparity images
derived from compressed images. Our experimental results show that the proposed
approach effectively reduces elevation noise, enhancing the safety and
reliability of terrain data for future lunar missions.

</details>


### [112] [Data or Language Supervision: What Makes CLIP Better than DINO?](https://arxiv.org/abs/2510.11835)
*Yiming Liu,Yuhui Zhang,Dhruba Ghosh,Ludwig Schmidt,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: The study compares CLIP and DINO as vision encoders for vision-language models, examining their training settings and benchmarking their performance on various tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand whether CLIP's superior performance over DINO in VLMs is due to language supervision or larger training data.

Method: Researchers pre-trained CLIP and DINO under identical controlled settings, analyzing their embeddings and testing them in VLMs across multiple benchmarks.

Result: CLIP excels in text-intensive tasks, while DINO slightly outperforms in vision-centric tasks. Language supervision variants offered limited improvements.

Conclusion: Insights highlight the differing strengths of vision encoders and underscore their varied impact on VLM performance.

Abstract: CLIP outperforms self-supervised models like DINO as vision encoders for
vision-language models (VLMs), but it remains unclear whether this advantage
stems from CLIP's language supervision or its much larger training data. To
disentangle these factors, we pre-train CLIP and DINO under controlled settings
-- using the same architecture, dataset, and training configuration --
achieving similar ImageNet accuracy. Embedding analysis shows that CLIP
captures high-level semantics (e.g., object categories, text), while DINO is
more responsive to low-level features like colors and styles. When integrated
into VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive
tasks, while DINO slightly outperforms on vision-centric ones. Variants of
language supervision (e.g., sigmoid loss, pre-trained language encoders) yield
limited gains. Our findings provide scientific insights into vision encoder
design and its impact on VLM performance.

</details>


### [113] [MammoDINO: Anatomically Aware Self-Supervision for Mammographic Images](https://arxiv.org/abs/2510.11883)
*Sicheng Zhou,Lei Wu,Cao Xiao,Parminder Bhatia,Taha Kass-Hout*

Main category: cs.CV

TL;DR: MammoDINO introduces a novel self-supervised learning framework tailored for mammography, achieving state-of-the-art results in breast cancer screening tasks.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to overcome challenges in medical imaging SSL caused by limited data and domain biases by transforming the way mammography images are processed and analyzed.

Method: The framework incorporates breast tissue-aware data augmentation, cross-slice contrastive learning using 3D DBT structure, and pretraining on a dataset of 1.4 million mammographic images.

Result: MammoDINO demonstrates state-of-the-art performance on various breast cancer screening tasks and generalizes well across five benchmark datasets.

Conclusion: This scalable and annotation-free framework could enhance computer-aided diagnosis tools and help radiologists in breast cancer screening, improving diagnostic efficiency.

Abstract: Self-supervised learning (SSL) has transformed vision encoder training in
general domains but remains underutilized in medical imaging due to limited
data and domain specific biases. We present MammoDINO, a novel SSL framework
for mammography, pretrained on 1.4 million mammographic images. To capture
clinically meaningful features, we introduce a breast tissue aware data
augmentation sampler for both image-level and patch-level supervision and a
cross-slice contrastive learning objective that leverages 3D digital breast
tomosynthesis (DBT) structure into 2D pretraining. MammoDINO achieves
state-of-the-art performance on multiple breast cancer screening tasks and
generalizes well across five benchmark datasets. It offers a scalable,
annotation-free foundation for multipurpose computer-aided diagnosis (CAD)
tools for mammogram, helping reduce radiologists' workload and improve
diagnostic efficiency in breast cancer screening.

</details>


### [114] [Task-Specific Dual-Model Framework for Comprehensive Traffic Safety Video Description and Analysis](https://arxiv.org/abs/2510.11907)
*Blessing Agyei Kyem,Neema Jakisa Owor,Andrews Danyo,Joshua Kofi Asamoah,Eugene Denteh,Tanner Muturi,Anthony Dontoh,Yaw Adu-Gyamfi,Armstrong Aboah*

Main category: cs.CV

TL;DR: The paper introduces a dual-model framework combining VideoLLaMA and Qwen2.5-VL, optimized separately for captioning and VQA tasks to enhance traffic safety analysis. It achieved competitive results in the 2025 AI City Challenge.


<details>
  <summary>Details</summary>
Motivation: To address the need for a detailed understanding of video content for traffic accident prevention and tackle task interference issues in captioning and VQA.

Method: A dual-model strategy where VideoLLaMA specializes in temporal reasoning and Qwen2.5-VL focuses on visual understanding, coupled with separate training for each model.

Result: Experimental results show VideoLLaMA achieves a CIDEr score of 1.1001 and Qwen2.5-VL earns VQA accuracy of 60.80%. On the WTS dataset, the team achieved an S2 score of 45.7572 in the AI City Challenge, securing 10th place.

Conclusion: Separate training for captioning and VQA improves task performance, as validated by ablation studies, ensuring balanced quality across both tasks.

Abstract: Traffic safety analysis requires complex video understanding to capture
fine-grained behavioral patterns and generate comprehensive descriptions for
accident prevention. In this work, we present a unique dual-model framework
that strategically utilizes the complementary strengths of VideoLLaMA and
Qwen2.5-VL through task-specific optimization to address this issue. The core
insight behind our approach is that separating training for captioning and
visual question answering (VQA) tasks minimizes task interference and allows
each model to specialize more effectively. Experimental results demonstrate
that VideoLLaMA is particularly effective in temporal reasoning, achieving a
CIDEr score of 1.1001, while Qwen2.5-VL excels in visual understanding with a
VQA accuracy of 60.80\%. Through extensive experiments on the WTS dataset, our
method achieves an S2 score of 45.7572 in the 2025 AI City Challenge Track 2,
placing 10th on the challenge leaderboard. Ablation studies validate that our
separate training strategy outperforms joint training by 8.6\% in VQA accuracy
while maintaining captioning quality.

</details>


### [115] [PanoTPS-Net: Panoramic Room Layout Estimation via Thin Plate Spline Transformation](https://arxiv.org/abs/2510.11992)
*Hatem Ibrahem,Ahmed Salem,Qinmin Vivian Hu,Guanghui Wang*

Main category: cs.CV

TL;DR: The paper introduces PanoTPS-Net, a model designed to predict room layouts from panorama images using CNN and Thin Plate Spline transformations, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the ability to estimate 3D room layouts from single panorama images, which is critical for applications in robotics, augmented reality, and interior design.

Method: PanoTPS-Net leverages CNNs for feature extraction and integrates Thin Plate Spline spatial transformations in a two-stage process that learns and applies spatial parameters to refine room layout predictions.

Result: The model demonstrated strong performance in cuboid and non-cuboid layout predictions, achieving high 3DIoU scores across multiple datasets: PanoContext, Stanford-2D3D, Matterport3DLayout, and ZInD.

Conclusion: The PanoTPS-Net successfully improves room layout estimation accuracy, utilizing effective integration of TPS transformations with panoramas, and shows robustness across diverse datasets.

Abstract: Accurately estimating the 3D layout of rooms is a crucial task in computer
vision, with potential applications in robotics, augmented reality, and
interior design. This paper proposes a novel model, PanoTPS-Net, to estimate
room layout from a single panorama image. Leveraging a Convolutional Neural
Network (CNN) and incorporating a Thin Plate Spline (TPS) spatial
transformation, the architecture of PanoTPS-Net is divided into two stages:
First, a convolutional neural network extracts the high-level features from the
input images, allowing the network to learn the spatial parameters of the TPS
transformation. Second, the TPS spatial transformation layer is generated to
warp a reference layout to the required layout based on the predicted
parameters. This unique combination empowers the model to properly predict room
layouts while also generalizing effectively to both cuboid and non-cuboid
layouts. Extensive experiments on publicly available datasets and comparisons
with state-of-the-art methods demonstrate the effectiveness of the proposed
method. The results underscore the model's accuracy in room layout estimation
and emphasize the compatibility between the TPS transformation and panorama
images. The robustness of the model in handling both cuboid and non-cuboid room
layout estimation is evident with a 3DIoU value of 85.49, 86.16, 81.76, and
91.98 on PanoContext, Stanford-2D3D, Matterport3DLayout, and ZInD datasets,
respectively. The source code is available at:
https://github.com/HatemHosam/PanoTPS_Net.

</details>


### [116] [Prompt-Guided Spatial Understanding with RGB-D Transformers for Fine-Grained Object Relation Reasoning](https://arxiv.org/abs/2510.11996)
*Tanner Muturi,Blessing Agyei Kyem,Joshua Kofi Asamoah,Neema Jakisa Owor,Richard Dyzinela,Andrews Danyo,Yaw Adu-Gyamfi,Armstrong Aboah*

Main category: cs.CV

TL;DR: The authors address spatial reasoning challenges in cluttered 3D environments, proposing a model that incorporates bounding box coordinates into prompts for improved spatial comprehension, leading to competitive performance in an AI City competition.


<details>
  <summary>Details</summary>
Motivation: Vision-language systems struggle with spatial reasoning in complex 3D environments like warehouses due to clutter and occlusions.

Method: The framework embeds bounding box coordinates into input prompts and fine-tunes tasks including Distance Estimation, Object Counting, and Spatial Relation Inference; normalized answers were also included in training for better consistency.

Result: The proposed pipeline achieved a score of 73.0606, securing 4th place on the public leaderboard for the AI City Challenge 2025.

Conclusion: Structured prompt enrichment and task-specific tuning significantly enhance spatial reasoning capabilities in industrial settings.

Abstract: Spatial reasoning in large-scale 3D environments such as warehouses remains a
significant challenge for vision-language systems due to scene clutter,
occlusions, and the need for precise spatial understanding. Existing models
often struggle with generalization in such settings, as they rely heavily on
local appearance and lack explicit spatial grounding. In this work, we
introduce a dedicated spatial reasoning framework for the Physical AI Spatial
Intelligence Warehouse dataset introduced in the Track 3 2025 AI City
Challenge. Our approach enhances spatial comprehension by embedding mask
dimensions in the form of bounding box coordinates directly into the input
prompts, enabling the model to reason over object geometry and layout. We
fine-tune the framework across four question categories namely: Distance
Estimation, Object Counting, Multi-choice Grounding, and Spatial Relation
Inference using task-specific supervision. To further improve consistency with
the evaluation system, normalized answers are appended to the GPT response
within the training set. Our comprehensive pipeline achieves a final score of
73.0606, placing 4th overall on the public leaderboard. These results
demonstrate the effectiveness of structured prompt enrichment and targeted
optimization in advancing spatial reasoning for real-world industrial
environments.

</details>


### [117] [Evaluating the Explainability of Vision Transformers in Medical Imaging](https://arxiv.org/abs/2510.12021)
*Leili Barekatain,Ben Glocker*

Main category: cs.CV

TL;DR: This paper studies the explainability of Vision Transformer architectures in medical imaging using methods like Grad-CAM and Gradient Attention Rollout, finding DINO with Grad-CAM to give localized, accurate explanations.


<details>
  <summary>Details</summary>
Motivation: Interpretability in medical imaging models is essential for clinical trust and adoption, especially given the complexity of Vision Transformers (ViTs).

Method: The study evaluates four ViT architectures (ViT, DeiT, DINO, and Swin Transformer) with Gradient Attention Rollout and Grad-CAM, analyzing their explainability on classification tasks in blood cell and breast ultrasound images.

Result: DINO combined with Grad-CAM showed the best localized and faithful explanations across datasets, emphasizing clinically relevant features even in misclassifications.

Conclusion: The findings enhance transparency and explainability of ViTs in medical diagnostics, promoting their reliable use in critical workflows.

Abstract: Understanding model decisions is crucial in medical imaging, where
interpretability directly impacts clinical trust and adoption. Vision
Transformers (ViTs) have demonstrated state-of-the-art performance in
diagnostic imaging; however, their complex attention mechanisms pose challenges
to explainability. This study evaluates the explainability of different Vision
Transformer architectures and pre-training strategies - ViT, DeiT, DINO, and
Swin Transformer - using Gradient Attention Rollout and Grad-CAM. We conduct
both quantitative and qualitative analyses on two medical imaging tasks:
peripheral blood cell classification and breast ultrasound image
classification. Our findings indicate that DINO combined with Grad-CAM offers
the most faithful and localized explanations across datasets. Grad-CAM
consistently produces class-discriminative and spatially precise heatmaps,
while Gradient Attention Rollout yields more scattered activations. Even in
misclassification cases, DINO with Grad-CAM highlights clinically relevant
morphological features that appear to have misled the model. By improving model
transparency, this research supports the reliable and explainable integration
of ViTs into critical medical diagnostic workflows.

</details>


### [118] [APGNet: Adaptive Prior-Guided for Underwater Camouflaged Object Detection](https://arxiv.org/abs/2510.12056)
*Xinxin Huang,Han Sun,Junmin Cai,Ningzhong Liu,Huiyu Zhou*

Main category: cs.CV

TL;DR: APGNet is proposed to detect camouflaged objects in underwater environments by mitigating image degradation effects and enhancing detection performance.


<details>
  <summary>Details</summary>
Motivation: Underwater image degradation and natural camouflage of marine organisms hinder accurate camouflaged object detection in underwater environments.

Method: APGNet integrates a Siamese architecture with prior-guided mechanisms, MSRCR algorithm for illumination correction, and adaptive modules for multi-scale feature refinement.

Result: APGNet outperformed 15 state-of-the-art methods on two public MAS datasets based on evaluation metrics.

Conclusion: APGNet improves underwater camouflaged object detection by addressing challenges in optical degradation and natural camouflage effectively.

Abstract: Detecting camouflaged objects in underwater environments is crucial for
marine ecological research and resource exploration. However, existing methods
face two key challenges: underwater image degradation, including low contrast
and color distortion, and the natural camouflage of marine organisms.
Traditional image enhancement techniques struggle to restore critical features
in degraded images, while camouflaged object detection (COD) methods developed
for terrestrial scenes often fail to adapt to underwater environments due to
the lack of consideration for underwater optical characteristics.
  To address these issues, we propose APGNet, an Adaptive Prior-Guided Network,
which integrates a Siamese architecture with a novel prior-guided mechanism to
enhance robustness and detection accuracy. First, we employ the Multi-Scale
Retinex with Color Restoration (MSRCR) algorithm for data augmentation,
generating illumination-invariant images to mitigate degradation effects.
Second, we design an Extended Receptive Field (ERF) module combined with a
Multi-Scale Progressive Decoder (MPD) to capture multi-scale contextual
information and refine feature representations. Furthermore, we propose an
adaptive prior-guided mechanism that hierarchically fuses position and boundary
priors by embedding spatial attention in high-level features for coarse
localization and using deformable convolution to refine contours in low-level
features.
  Extensive experimental results on two public MAS datasets demonstrate that
our proposed method APGNet outperforms 15 state-of-art methods under widely
used evaluation metrics.

</details>


### [119] [VIDMP3: Video Editing by Representing Motion with Pose and Position Priors](https://arxiv.org/abs/2510.12069)
*Sandeep Mishra,Oindrila Saha,Alan C. Bovik*

Main category: cs.CV

TL;DR: VidMP3 introduces a novel approach for motion-preserved video editing that solves issues like temporal inconsistency and subject identity drift.


<details>
  <summary>Details</summary>
Motivation: Existing editing methods face challenges like poor temporal consistency, identity drift, and dependency on human intervention.

Method: VidMP3 uses pose and position priors to learn generalized motion representation, ensuring flexibility in video editing while preserving motion.

Result: Both qualitative and quantitative evaluations show VidMP3 outperforms current methods in video editing tasks.

Conclusion: VidMP3 provides a superior and generalized motion-preservation video editing technique, addressing prior limitations.

Abstract: Motion-preserved video editing is crucial for creators, particularly in
scenarios that demand flexibility in both the structure and semantics of
swapped objects. Despite its potential, this area remains underexplored.
Existing diffusion-based editing methods excel in structure-preserving tasks,
using dense guidance signals to ensure content integrity. While some recent
methods attempt to address structure-variable editing, they often suffer from
issues such as temporal inconsistency, subject identity drift, and the need for
human intervention. To address these challenges, we introduce VidMP3, a novel
approach that leverages pose and position priors to learn a generalized motion
representation from source videos. Our method enables the generation of new
videos that maintain the original motion while allowing for structural and
semantic flexibility. Both qualitative and quantitative evaluations demonstrate
the superiority of our approach over existing methods. The code will be made
publicly available at https://github.com/sandeep-sm/VidMP3.

</details>


### [120] [A Review on Domain Adaption and Generative Adversarial Networks(GANs)](https://arxiv.org/abs/2510.12075)
*Aashish Dhawan,Divyanshu Mudgal*

Main category: cs.CV

TL;DR: This paper addresses the scarcity of quality labeled data in computer vision, focusing on domain adaptation for image classification.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of obtaining labeled data, which is expensive and sometimes impossible, by exploring methods to adapt models across domains.

Method: Discussing various domain adaptation techniques to use models trained on one dataset to predict on a different domain.

Result: Highlights the potential of domain adaptation techniques to overcome data scarcity and achieve comparable results to established benchmarks.

Conclusion: Domain adaptation can pave the way for effective cross-domain predictions in scenarios with limited labeled data.

Abstract: The major challenge in today's computer vision scenario is the availability
of good quality labeled data. In a field of study like image classification,
where data is of utmost importance, we need to find more reliable methods which
can overcome the scarcity of data to produce results comparable to previous
benchmark results. In most cases, obtaining labeled data is very difficult
because of the high cost of human labor and in some cases impossible. The
purpose of this paper is to discuss Domain Adaptation and various methods to
implement it. The main idea is to use a model trained on a particular dataset
to predict on data from a different domain of the same kind, for example - a
model trained on paintings of airplanes predicting on real images of airplanes

</details>


### [121] [Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback](https://arxiv.org/abs/2510.12089)
*Xingpei Ma,Shenneng Huang,Jiaran Cai,Yuansheng Guan,Shen Zheng,Hanfeng Zhao,Qiang Zhang,Shunsi Zhang*

Main category: cs.CV

TL;DR: A novel diffusion transformer framework is proposed to improve audio-driven human video generation with enhanced lip-sync, temporal coherence, and support for multi-character animation using innovative training strategies and training-free methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in lip-sync accuracy, temporal coherence in long video generation, and multi-character animation in audio-driven human video generation.

Method: Introduces a DiT-based framework utilizing LoRA-based training for long videos, reward feedback to improve lip-sync and body motion, and a training-free Mask-CFG approach for multi-character animation without requiring specialized datasets.

Result: Improved performance over state-of-the-art methods in quality, temporal coherence, and efficient multi-character audio-driven video generation.

Conclusion: The proposed framework enables simple, efficient, and cost-effective lifelike talking videos and multi-character animation, advancing the field of audio-driven human video generation.

Abstract: Recent advances in diffusion models have significantly improved audio-driven
human video generation, surpassing traditional methods in both quality and
controllability. However, existing approaches still face challenges in lip-sync
accuracy, temporal coherence for long video generation, and multi-character
animation. In this work, we propose a diffusion transformer (DiT)-based
framework for generating lifelike talking videos of arbitrary length, and
introduce a training-free method for multi-character audio-driven animation.
First, we employ a LoRA-based training strategy combined with a position shift
inference approach, which enables efficient long video generation while
preserving the capabilities of the foundation model. Moreover, we combine
partial parameter updates with reward feedback to enhance both lip
synchronization and natural body motion. Finally, we propose a training-free
approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character
animation, which requires no specialized datasets or model modifications and
supports audio-driven animation for three or more characters. Experimental
results demonstrate that our method outperforms existing state-of-the-art
approaches, achieving high-quality, temporally coherent, and multi-character
audio-driven video generation in a simple, efficient, and cost-effective
manner.

</details>


### [122] [IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation](https://arxiv.org/abs/2510.12095)
*Wenxu Zhou,Kaixuan Nie,Hang Du,Dong Yin,Wei Huang,Siqiang Guo,Xiaobo Zhang,Pengbo Hu*

Main category: cs.CV

TL;DR: IL3D is a large-scale dataset designed to enhance vision-language multimodal learning, specifically for 3D scene generation, providing 27,816 indoor layouts and 29,215 3D object assets with extensive annotations.


<details>
  <summary>Details</summary>
Motivation: Address the need for high-quality, diverse training data to improve 3D scene generation and vision-language tasks using large language models.

Method: Created IL3D, a dataset with annotated indoor layouts and a variety of multimodal data exports, and established benchmarks to evaluate 3D scene generation performance.

Result: Supervised fine-tuning of LLMs on IL3D data significantly improves generalization and performs better compared to other datasets.

Conclusion: IL3D is a versatile dataset that supports advancements in 3D scene generation and embodied intelligence research by offering extensive, high-fidelity multimodal data.

Abstract: In this study, we present IL3D, a large-scale dataset meticulously designed
for large language model (LLM)-driven 3D scene generation, addressing the
pressing demand for diverse, high-quality training data in indoor layout
design. Comprising 27,816 indoor layouts across 18 prevalent room types and a
library of 29,215 high-fidelity 3D object assets, IL3D is enriched with
instance-level natural language annotations to support robust multimodal
learning for vision-language tasks. We establish rigorous benchmarks to
evaluate LLM-driven scene generation. Experimental results show that supervised
fine-tuning (SFT) of LLMs on IL3D significantly improves generalization and
surpasses the performance of SFT on other datasets. IL3D offers flexible
multimodal data export capabilities, including point clouds, 3D bounding boxes,
multiview images, depth maps, normal maps, and semantic masks, enabling
seamless adaptation to various visual tasks. As a versatile and robust
resource, IL3D significantly advances research in 3D scene generation and
embodied intelligence, by providing high-fidelity scene data to support
environment perception tasks of embodied agents.

</details>


### [123] [Personalized Federated Fine-Tuning of Vision Foundation Models for Healthcare](https://arxiv.org/abs/2510.12741)
*Adam Tupper,Christian Gagné*

Main category: cs.CV

TL;DR: The paper proposes a personalized federated fine-tuning method for healthcare foundation models using LoRA adapters, achieving competitive results in real-world medical imaging tasks.


<details>
  <summary>Details</summary>
Motivation: AI foundation models require fine-tuning for specific healthcare tasks, but data sharing restrictions pose challenges. Federated learning, which avoids centralized data storage, can address this issue.

Method: The authors introduce a personalized federated fine-tuning approach using orthogonal LoRA adapters that separate general knowledge from client-specific knowledge, leveraging data across multiple clients without infringing privacy.

Result: Preliminary results show that the proposed method performs competitively compared to existing federated fine-tuning methods in medical imaging tasks.

Conclusion: The approach enables effective fine-tuning for healthcare foundation models while maximizing utility from both local and shared data across clients, maintaining privacy.

Abstract: Foundation models open up new possibilities for the use of AI in healthcare.
However, even when pre-trained on health data, they still need to be fine-tuned
for specific downstream tasks. Furthermore, although foundation models reduce
the amount of training data required to achieve good performance, obtaining
sufficient data is still a challenge. This is due, in part, to restrictions on
sharing and aggregating data from different sources to protect patients'
privacy. One possible solution to this is to fine-tune foundation models via
federated learning across multiple participating clients (i.e., hospitals,
clinics, etc.). In this work, we propose a new personalized federated
fine-tuning method that learns orthogonal LoRA adapters to disentangle general
and client-specific knowledge, enabling each client to fully exploit both their
own data and the data of others. Our preliminary results on real-world
federated medical imaging tasks demonstrate that our approach is competitive
against current federated fine-tuning methods.

</details>


### [124] [An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring](https://arxiv.org/abs/2510.12098)
*Jianping Li,Dongyang Guo,Wenjie Li,Wei Zhao*

Main category: cs.CV

TL;DR: The paper introduces Edge-Guided Restormer (EG-Restormer) and Adaptive Dual-network (ADNet) for effective QR code deblurring using edge priors for improved decoding success rates.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for QR code deblurring often ignore the structured and sharp-edge character of QR codes, which can be a robust prior for restoration. The paper aims to address this gap.

Method: The authors propose the Edge-Guided Attention Block (EGAB) to embed explicit edge priors in a Transformer architecture. They developed two models: EG-Restormer for severe blur scenarios and LENet for less severe cases. These models are integrated into ADNet, which dynamically switches between them based on the input blur severity.

Result: The proposed models, EG-Restormer and ADNet, achieve state-of-the-art performance in decoding severely and mildly blurred QR codes while maintaining competitive processing speed.

Conclusion: The integration of explicit edge priors into QR code deblurring networks significantly improves their decoding accuracy. The adaptive approach in ADNet makes it particularly beneficial for resource-limited mobile devices.

Abstract: Unlike general image deblurring that prioritizes perceptual quality, QR code
deblurring focuses on ensuring successful decoding. QR codes are characterized
by highly structured patterns with sharp edges, a robust prior for restoration.
Yet existing deep learning methods rarely exploit these priors explicitly. To
address this gap, we propose the Edge-Guided Attention Block (EGAB), which
embeds explicit edge priors into a Transformer architecture. Based on EGAB, we
develop Edge-Guided Restormer (EG-Restormer), an effective network that
significantly boosts the decoding rate of severely blurred QR codes. For mildly
blurred inputs, we design the Lightweight and Efficient Network (LENet) for
fast deblurring. We further integrate these two networks into an Adaptive
Dual-network (ADNet), which dynamically selects the suitable network based on
input blur severity, making it ideal for resource-constrained mobile devices.
Extensive experiments show that our EG-Restormer and ADNet achieve
state-of-the-art performance with a competitive speed. Project page:
https://github.com/leejianping/ADNet

</details>


### [125] [G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior](https://arxiv.org/abs/2510.12099)
*Junfeng Ni,Yixin Chen,Zhifei Yang,Yu Liu,Ruijie Lu,Song-Chun Zhu,Siyuan Huang*

Main category: cs.CV

TL;DR: This paper addresses two major issues in 3D scene reconstruction using pre-trained diffusion models, namely the lack of reliable geometric supervision and multi-view inconsistencies, by focusing on accurate geometry and leveraging planar structures to enhance reconstructions.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstruction methods using pre-trained diffusion models fail to deliver high-quality results due to unreliable geometric supervision and inconsistencies across multiple views.

Method: The paper introduces a technique that leverages planar structures to derive accurate depth maps for supervision. Geometry guidance is integrated throughout the generative pipeline to enhance factors such as visibility mask estimation and multi-view consistency.

Result: The proposed method outperforms baseline approaches in both geometry and appearance reconstruction. It demonstrates improvements in unobserved regions and supports inputs like single views and unposed videos with general applicability across scenarios.

Conclusion: Accurate geometry is critical for effective 3D reconstruction using generative models. By incorporating reliable depth-based supervision and improving consistency, the method has broad potential for real-world applications in indoor and outdoor settings.

Abstract: Despite recent advances in leveraging generative prior from pre-trained
diffusion models for 3D scene reconstruction, existing methods still face two
critical limitations. First, due to the lack of reliable geometric supervision,
they struggle to produce high-quality reconstructions even in observed regions,
let alone in unobserved areas. Second, they lack effective mechanisms to
mitigate multi-view inconsistencies in the generated images, leading to severe
shape-appearance ambiguities and degraded scene geometry. In this paper, we
identify accurate geometry as the fundamental prerequisite for effectively
exploiting generative models to enhance 3D scene reconstruction. We first
propose to leverage the prevalence of planar structures to derive accurate
metric-scale depth maps, providing reliable supervision in both observed and
unobserved regions. Furthermore, we incorporate this geometry guidance
throughout the generative pipeline to improve visibility mask estimation, guide
novel view selection, and enhance multi-view consistency when inpainting with
video diffusion models, resulting in accurate and consistent scene completion.
Extensive experiments on Replica, ScanNet++, and DeepBlending show that our
method consistently outperforms existing baselines in both geometry and
appearance reconstruction, particularly for unobserved regions. Moreover, our
method naturally supports single-view inputs and unposed videos, with strong
generalizability in both indoor and outdoor scenarios with practical real-world
applicability. The project page is available at
https://dali-jack.github.io/g4splat-web/.

</details>


### [126] [DRL: Discriminative Representation Learning with Parallel Adapters for Class Incremental Learning](https://arxiv.org/abs/2510.12107)
*Jiawei Zhan,Jun Liu,Jinlong Peng,Xiaochen Chen,Bin-Bin Gao,Yong Liu,Chengjie Wang*

Main category: cs.CV

TL;DR: The paper introduces the Discriminative Representation Learning (DRL) framework, which leverages Pre-Trained Models to address challenges in Class-Incremental Learning effectively and efficiently by augmenting models with lightweight adapters and decoupled supervision.


<details>
  <summary>Details</summary>
Motivation: Class-Incremental Learning (CIL) with Pre-Trained Models is challenging due to model complexity, representation shift, and inconsistency between local optimization and global inference.

Method: The paper proposes the Incremental Parallel Adapter (IPA) network, which uses lightweight adapters to augment the model and a Decoupled Anchor Supervision (DAS) to improve representation discrimination and alignment.

Result: Extensive experiments on six benchmarks show that DRL achieves state-of-the-art performance in Class-Incremental Learning while maintaining efficiency in training and inference.

Conclusion: DRL effectively addresses key challenges in non-rehearsal CIL by ensuring smooth representation shift and consistent feature alignment, demonstrating superior performance and efficiency over existing approaches.

Abstract: With the excellent representation capabilities of Pre-Trained Models (PTMs),
remarkable progress has been made in non-rehearsal Class-Incremental Learning
(CIL) research. However, it remains an extremely challenging task due to three
conundrums: increasingly large model complexity, non-smooth representation
shift during incremental learning and inconsistency between stage-wise
sub-problem optimization and global inference. In this work, we propose the
Discriminative Representation Learning (DRL) framework to specifically address
these challenges. To conduct incremental learning effectively and yet
efficiently, the DRL's network, called Incremental Parallel Adapter (IPA)
network, is built upon a PTM and increasingly augments the model by learning a
lightweight adapter with a small amount of parameter learning overhead in each
incremental stage. The adapter is responsible for adapting the model to new
classes, it can inherit and propagate the representation capability from the
current model through parallel connection between them by a transfer gate. As a
result, this design guarantees a smooth representation shift between different
incremental stages. Furthermore, to alleviate inconsistency and enable
comparable feature representations across incremental stages, we design the
Decoupled Anchor Supervision (DAS). It decouples constraints of positive and
negative samples by respectively comparing them with the virtual anchor. This
decoupling promotes discriminative representation learning and aligns the
feature spaces learned at different stages, thereby narrowing the gap between
stage-wise local optimization over a subset of data and global inference across
all classes. Extensive experiments on six benchmarks reveal that our DRL
consistently outperforms other state-of-the-art methods throughout the entire
CIL period while maintaining high efficiency in both training and inference
phases.

</details>


### [127] [Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration](https://arxiv.org/abs/2510.12114)
*Wenjie Li,Xiangyi Wang,Heng Guo,Guangwei Gao,Zhanyu Ma*

Main category: cs.CV

TL;DR: The paper presents Self-Supervised Selective-Guided Diffusion (SSDiff) for restoring old face photos with complex degradations, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of restoring old face photos affected by various degradations like breakage, fading, and blur, which existing methods struggle to effectively handle.

Method: The proposed SSDiff employs pseudo-reference faces generated by a pre-trained diffusion model under weak guidance for restoration. It uses staged supervision, leveraging structural guidance during denoising and color refinement in later steps, facilitated by face parsing maps and scratch masks for selective restoration.

Result: SSDiff demonstrates superior performance in perceptual quality, fidelity, and regional controllability, restoring localized artifacts while preserving identity and color. The approach is validated on a newly created benchmark, VintageFace, consisting of 300 old face photos with different levels of degradation.

Conclusion: SSDiff provides an effective pipeline for old photo restoration by deploying a self-supervised, region-specific, diffusion-guided method, offering significant improvement over existing approaches in handling complex degradations.

Abstract: Old-photo face restoration poses significant challenges due to compounded
degradations such as breakage, fading, and severe blur. Existing pre-trained
diffusion-guided methods either rely on explicit degradation priors or global
statistical guidance, which struggle with localized artifacts or face color. We
propose Self-Supervised Selective-Guided Diffusion (SSDiff), which leverages
pseudo-reference faces generated by a pre-trained diffusion model under weak
guidance. These pseudo-labels exhibit structurally aligned contours and natural
colors, enabling region-specific restoration via staged supervision: structural
guidance applied throughout the denoising process and color refinement in later
steps, aligned with the coarse-to-fine nature of diffusion. By incorporating
face parsing maps and scratch masks, our method selectively restores breakage
regions while avoiding identity mismatch. We further construct VintageFace, a
300-image benchmark of real old face photos with varying degradation levels.
SSDiff outperforms existing GAN-based and diffusion-based methods in perceptual
quality, fidelity, and regional controllability. Code link:
https://github.com/PRIS-CV/SSDiff.

</details>


### [128] [ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation](https://arxiv.org/abs/2510.12119)
*Ziyuan Luo,Yangyi Zhao,Ka Chun Cheung,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: The paper introduces ImageSentinel, a system designed to safeguard visual datasets in Retrieval-Augmented Image Generation (RAIG) from unauthorized usage. It accomplishes this by creating sentinel images that let users verify unauthorized access.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper stems from concerns about unauthorized exploitation of private visual datasets in RAIG systems, which traditional watermarking techniques fail to manage effectively due to RAIG's complex processing.

Method: ImageSentinel synthesizes sentinel images using vision-language models, embedding unique character sequences to serve as retrieval keys for detecting unauthorized dataset usage.

Result: The proposed framework successfully detects unauthorized use of datasets while ensuring high-quality image generation for authorized users.

Conclusion: ImageSentinel offers a robust solution for protecting visual datasets in RAIG scenarios, advancing the field by addressing key limitations of prior watermarking methods.

Abstract: The widespread adoption of Retrieval-Augmented Image Generation (RAIG) has
raised significant concerns about the unauthorized use of private image
datasets. While these systems have shown remarkable capabilities in enhancing
generation quality through reference images, protecting visual datasets from
unauthorized use in such systems remains a challenging problem. Traditional
digital watermarking approaches face limitations in RAIG systems, as the
complex feature extraction and recombination processes fail to preserve
watermark signals during generation. To address these challenges, we propose
ImageSentinel, a novel framework for protecting visual datasets in RAIG. Our
framework synthesizes sentinel images that maintain visual consistency with the
original dataset. These sentinels enable protection verification through
randomly generated character sequences that serve as retrieval keys. To ensure
seamless integration, we leverage vision-language models to generate the
sentinel images. Experimental results demonstrate that ImageSentinel
effectively detects unauthorized dataset usage while preserving generation
quality for authorized applications. Code is available at
https://github.com/luo-ziyuan/ImageSentinel.

</details>


### [129] [Hardware-aware Coding Function Design for Compressive Single-Photon 3D Cameras](https://arxiv.org/abs/2510.12123)
*David Parra,Felipe Gutierrez-Barragan,Trevor Seets,Andreas Velten*

Main category: cs.CV

TL;DR: The paper addresses limitations of single-photon cameras in 3D imaging and proposes a constrained optimization approach to improve coding functions under hardware constraints.


<details>
  <summary>Details</summary>
Motivation: Single-photon cameras face challenges due to hardware limitations, such as bandwidth and memory, which restrict their performance in real-world applications.

Method: A constrained optimization approach is developed using gradient descent to optimize illumination and coding matrices while adhering to hardware constraints.

Result: The proposed coding functions outperform traditional designs under bandwidth and peak power constraints, especially in systems restricted by peak power. It also adapts effectively to real-world non-ideal impulse responses.

Conclusion: The work demonstrates a practical and effective solution to enhance the performance of compressive single-photon imaging within realistic hardware limitations.

Abstract: Single-photon cameras are becoming increasingly popular in time-of-flight 3D
imaging because they can time-tag individual photons with extreme resolution.
However, their performance is susceptible to hardware limitations, such as
system bandwidth, maximum laser power, sensor data rates, and in-sensor memory
and compute resources. Compressive histograms were recently introduced as a
solution to the challenge of data rates through an online in-sensor compression
of photon timestamp data. Although compressive histograms work within limited
in-sensor memory and computational resources, they underperform when subjected
to real-world illumination hardware constraints. To address this, we present a
constrained optimization approach for designing practical coding functions for
compressive single-photon 3D imaging. Using gradient descent, we jointly
optimize an illumination and coding matrix (i.e., the coding functions) that
adheres to hardware constraints. We show through extensive simulations that our
coding functions consistently outperform traditional coding designs under both
bandwidth and peak power constraints. This advantage is particularly pronounced
in systems constrained by peak power. Finally, we show that our approach adapts
to arbitrary parameterized impulse responses by evaluating it on a real-world
system with a non-ideal impulse response function.

</details>


### [130] [MetaCaptioner: Towards Generalist Visual Captioning with Open-source Suites](https://arxiv.org/abs/2510.12126)
*Zhenxin Lei,Zhangwei Gao,Changyao Tian,Erfei Cui,Guanzhou Chen,Danni Yang,Yuchen Duan,Zhaokai Wang,Wenhao Li,Weiyun Wang,Xiangyu Zhao,Jiayi Ji,Yu Qiao,Wenhai Wang,Gen Luo*

Main category: cs.CV

TL;DR: The paper introduces CapFlow, a model that uses multi-agent collaboration to achieve high-quality visual captions comparable to GPT-4.1 and leads to a generalist visual captioner, MetaCaptioner.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance gap between open-source and commercial visual captioning models, enabling cost-efficient, high-quality solutions for multimodal research and applications.

Method: The authors propose CapFlow, a multi-agent collaboration workflow leveraging open-source models to generate high-quality captions efficiently. They use CapFlow to fine-tune a generalist captioner named MetaCaptioner.

Result: CapFlow achieves captioning quality comparable to GPT-4.1 across domains with an 89.5% reduction in costs. MetaCaptioner demonstrates high-quality captioning and top-tier multimodal performance in the open-source community.

Conclusion: CapFlow and MetaCaptioner can drive forward multimodal research, providing strong and cost-effective solutions for visual captioning across diverse domains.

Abstract: Generalist visual captioning goes beyond a simple appearance description
task, but requires integrating a series of visual cues into a caption and
handling various visual domains. In this task, current open-source models
present a large performance gap with commercial ones, which limits various
applications such as data synthesis. To bridge the gap, this paper proposes
CapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for
the first time that, by capitalizing on open-source models, it is possible to
achieve caption quality on par with GPT-4.1 in various domains with an 89.5%
reduction in costs. By leveraging CapFlow as the data synthesizer, we produce
high-quality visual captions from image and video domains at scale, and obtain
a generalist visual captioner via fine-tuning, namely MetaCaptioner. Through
extensive experiments, we show that MetaCaptioner not only achieves comparable
captioning capabilities with commercial models but also reaches top-tier
multimodal performance in the open-source community. We hope CapFlow and
MetaCaptioner can benefit future multimodal research by providing a strong and
cost-effective visual captioning solution.

</details>


### [131] [FedHUG: Federated Heterogeneous Unsupervised Generalization for Remote Physiological Measurements](https://arxiv.org/abs/2510.12132)
*Xiao Yang,Jiyao Wang*

Main category: cs.CV

TL;DR: The paper introduces 'Federated Unsupervised Domain Generalization' and a corresponding 'FedHUG' framework for improving remote physiological measurements without labeled data.


<details>
  <summary>Details</summary>
Motivation: Enable contactless remote physiological measurements while addressing privacy concerns and reliance on labeled client data, especially for real-world model updates.

Method: The 'FedHUG' framework integrates a Minimal Bias Aggregation module to handle non-IID data and a Global Distribution-aware Learning Controller to address label distribution skew and long-tail issues.

Result: FedHUG outperformed existing techniques in remote physiological estimation using both RGB video and mmWave radar.

Conclusion: The proposed framework demonstrates significant performance improvements in remote physiological measurement and advances federated unsupervised learning techniques; the implementation will be open-sourced.

Abstract: Remote physiological measurement gained wide attention, while it requires
collecting users' privacy-sensitive information, and existing contactless
measurements still rely on labeled client data. This presents challenges when
we want to further update real-world deployed models with numerous user data
lacking labels. To resolve these challenges, we instantiate a new protocol
called Federated Unsupervised Domain Generalization (FUDG) in this work.
Subsequently, the \textbf{Fed}erated \textbf{H}eterogeneous
\textbf{U}nsupervised \textbf{G}eneralization (\textbf{FedHUG}) framework is
proposed and consists of: (1) Minimal Bias Aggregation module dynamically
adjusts aggregation weights based on prior-driven bias evaluation to cope with
heterogeneous non-IID features from multiple domains. (2) The Global
Distribution-aware Learning Controller parameterizes the label distribution and
dynamically manipulates client-specific training strategies, thereby mitigating
the server-client label distribution skew and long-tail issue. The proposal
shows superior performance across state-of-the-art techniques in estimation
with either RGB video or mmWave radar. The code will be released.

</details>


### [132] [Class-aware Domain Knowledge Fusion and Fission for Continual Test-Time Adaptation](https://arxiv.org/abs/2510.12150)
*Jiahuan Zhou,Chao Zhu,Zhenyu Cui,Zichen Liu,Xu Zou,Gang Hua*

Main category: cs.CV

TL;DR: This paper introduces a method called Knowledge Fusion and Fission (KFF) for Continual Test-Time Adaptation (CTTA) to adapt models to varied downstream domains by dynamically managing historical and new knowledge.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of catastrophic forgetting and insufficient knowledge learning in CTTA when adapting to downstream domains, aiming to improve performance while avoiding interference from harmful historical knowledge.

Method: The authors propose the KFF method, which includes two modules: Knowledge FIssion (KFI) to separate class-aware domain knowledge for new domains and minimize negative influence, and Knowledge FUsion (KFU) to merge new knowledge with old to maintain computational efficiency.

Result: Extensive experiments on the ImageNet-C dataset show improved effectiveness of the proposed KFF method compared to existing CTTA approaches.

Conclusion: The KFF method dynamically manages historical and new domain knowledge, addressing key CTTA challenges to enhance performance and computational efficiency.

Abstract: Continual Test-Time Adaptation (CTTA) aims to quickly fine-tune the model
during the test phase so that it can adapt to multiple unknown downstream
domain distributions without pre-acquiring downstream domain data. To this end,
existing advanced CTTA methods mainly reduce the catastrophic forgetting of
historical knowledge caused by irregular switching of downstream domain data by
restoring the initial model or reusing historical models. However, these
methods are usually accompanied by serious insufficient learning of new
knowledge and interference from potentially harmful historical knowledge,
resulting in severe performance degradation. To this end, we propose a
class-aware domain Knowledge Fusion and Fission method for continual test-time
adaptation, called KFF, which adaptively expands and merges class-aware domain
knowledge in old and new domains according to the test-time data from different
domains, where discriminative historical knowledge can be dynamically
accumulated. Specifically, considering the huge domain gap within streaming
data, a domain Knowledge FIssion (KFI) module is designed to adaptively
separate new domain knowledge from a paired class-aware domain prompt pool,
alleviating the impact of negative knowledge brought by old domains that are
distinct from the current domain. Besides, to avoid the cumulative computation
and storage overheads from continuously fissioning new knowledge, a domain
Knowledge FUsion (KFU) module is further designed to merge the fissioned new
knowledge into the existing knowledge pool with minimal cost, where a greedy
knowledge dynamic merging strategy is designed to improve the compatibility of
new and old knowledge while keeping the computational efficiency. Extensive
experiments on the ImageNet-C dataset verify the effectiveness of our proposed
method against other methods.

</details>


### [133] [DPL: Spatial-Conditioned Diffusion Prototype Enhancement for One-Shot Medical Segmentation](https://arxiv.org/abs/2510.12159)
*Ziyuan Gao,Philippe Morel*

Main category: cs.CV

TL;DR: The paper tackles challenges in one-shot medical image segmentation by introducing Diffusion Prototype Learning (DPL), which builds robust prototypes through diffusion-based feature space exploration.


<details>
  <summary>Details</summary>
Motivation: Existing one-shot segmentation methods encounter difficulties due to limited annotated data and anatomical variability, leading to brittle prototype representations that struggle to generalize across diverse intra-class features.

Method: The introduced DPL framework includes three core components: (1) diffusion-based prototype enhancement for creating diverse prototype variants, (2) spatial-aware conditioning based on geometric properties, and (3) conservative fusion to balance fidelity and diversity. Both training and inference utilize the same pipeline.

Result: Experiments on abdominal MRI and CT datasets show substantial performance gains, achieving state-of-the-art results in the one-shot segmentation task.

Conclusion: DPL enables enhanced prototype generation that improves robustness and segmentation performance through innovative use of diffusion processes to handle limited data and anatomical variabilities.

Abstract: One-shot medical image segmentation faces fundamental challenges in prototype
representation due to limited annotated data and significant anatomical
variability across patients. Traditional prototype-based methods rely on
deterministic averaging of support features, creating brittle representations
that fail to capture intra-class diversity essential for robust generalization.
This work introduces Diffusion Prototype Learning (DPL), a novel framework that
reformulates prototype construction through diffusion-based feature space
exploration. DPL models one-shot prototypes as learnable probability
distributions, enabling controlled generation of diverse yet semantically
coherent prototype variants from minimal labeled data. The framework operates
through three core innovations: (1) a diffusion-based prototype enhancement
module that transforms single support prototypes into diverse variant sets via
forward-reverse diffusion processes, (2) a spatial-aware conditioning mechanism
that leverages geometric properties derived from prototype feature statistics,
and (3) a conservative fusion strategy that preserves prototype fidelity while
maximizing representational diversity. DPL ensures training-inference
consistency by using the same diffusion enhancement and fusion pipeline in both
phases. This process generates enhanced prototypes that serve as the final
representations for similarity calculations, while the diffusion process itself
acts as a regularizer. Extensive experiments on abdominal MRI and CT datasets
demonstrate significant improvements respectively, establishing new
state-of-the-art performance in one-shot medical image segmentation.

</details>


### [134] [State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding](https://arxiv.org/abs/2510.12160)
*Jiahuan Zhou,Kai Zhu,Zhenyu Cui,Zichen Liu,Xu Zou,Gang Hua*

Main category: cs.CV

TL;DR: The paper introduces a State Space Prompting (SSP) method for video classification that improves spatial and temporal information propagation for competitive performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: Pre-trained state space models for video classification fail to capture essential spatiotemporal contextual information during compression, limiting their effectiveness in downstream tasks.

Method: The authors designed SSP using two key modules, Intra-Frame Gathering (IFG) for spatial information aggregation within frames, and Inter-Frame Spreading (IFS) for spatiotemporal information propagation between frames.

Result: Extensive experiments show SSP outperforms prior state-of-the-art video classification methods by 2.76% on average, with reduced fine-tuning parameter overhead.

Conclusion: SSP method effectively improves video understanding by adaptively balancing spatiotemporal information propagation within and across frames, achieving both higher accuracy and efficiency.

Abstract: Recently, pre-trained state space models have shown great potential for video
classification, which sequentially compresses visual tokens in videos with
linear complexity, thereby improving the processing efficiency of video data
while maintaining high performance. To apply powerful pre-trained models to
downstream tasks, prompt learning is proposed to achieve efficient downstream
task adaptation with only a small number of fine-tuned parameters. However, the
sequentially compressed visual prompt tokens fail to capture the spatial and
temporal contextual information in the video, thus limiting the effective
propagation of spatial information within a video frame and temporal
information between frames in the state compression model and the extraction of
discriminative information. To tackle the above issue, we proposed a State
Space Prompting (SSP) method for video understanding, which combines
intra-frame and inter-frame prompts to aggregate and propagate key
spatiotemporal information in the video. Specifically, an Intra-Frame Gathering
(IFG) module is designed to aggregate spatial key information within each
frame. Besides, an Inter-Frame Spreading (IFS) module is designed to spread
discriminative spatio-temporal information across different frames. By
adaptively balancing and compressing key spatio-temporal information within and
between frames, our SSP effectively propagates discriminative information in
videos in a complementary manner. Extensive experiments on four video benchmark
datasets verify that our SSP significantly outperforms existing SOTA methods by
2.76% on average while reducing the overhead of fine-tuning parameters.

</details>


### [135] [UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering](https://arxiv.org/abs/2510.12174)
*Yusen Xie,Zhenmin Huang,Jianhao Jiao,Dimitrios Kanoulas,Jun Ma*

Main category: cs.CV

TL;DR: This paper introduces UniGS, a unified framework for multimodal 3D reconstruction, leveraging 3D Gaussian Splatting and a CUDA-accelerated pipeline.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in existing multimodal 3D reconstruction methods by introducing a unified framework to improve accuracy, consistency, and efficiency.

Method: It employs a redesigned rasterization pipeline for differentiable rendering of depth, normals, RGB images, and semantics, alongside a learnable attribute for pruning irrelevant components during training.

Result: Experimental results show state-of-the-art reconstruction accuracy across RGB, depth, surface normals, and semantics, verifying the paradigm's efficacy.

Conclusion: UniGS provides a significant advancement in multimodal 3D reconstruction techniques, combining unified representation and computational efficiency with publicly available resources.

Abstract: In this paper, we propose UniGS, a unified map representation and
differentiable framework for high-fidelity multimodal 3D reconstruction based
on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated
rasterization pipeline capable of rendering photo-realistic RGB images,
geometrically accurate depth maps, consistent surface normals, and semantic
logits simultaneously. We redesign the rasterization to render depth via
differentiable ray-ellipsoid intersection rather than using Gaussian centers,
enabling effective optimization of rotation and scale attribute through
analytic depth gradients. Furthermore, we derive the analytic gradient
formulation for surface normal rendering, ensuring geometric consistency among
reconstructed 3D scenes. To improve computational and storage efficiency, we
introduce a learnable attribute that enables differentiable pruning of
Gaussians with minimal contribution during training. Quantitative and
qualitative experiments demonstrate state-of-the-art reconstruction accuracy
across all modalities, validating the efficacy of our geometry-aware paradigm.
Source code and multimodal viewer will be available on GitHub.

</details>


### [136] [BEEP3D: Box-Supervised End-to-End Pseudo-Mask Generation for 3D Instance Segmentation](https://arxiv.org/abs/2510.12182)
*Youngju Yoo,Seho Kim,Changick Kim*

Main category: cs.CV

TL;DR: The paper presents BEEP3D, a computationally efficient, end-to-end method for 3D instance segmentation using box-level annotations, improving performance over existing weakly supervised approaches.


<details>
  <summary>Details</summary>
Motivation: Fully supervised 3D instance segmentation is costly due to its reliance on dense point-level annotations; using box-level annotations as weak supervision is more scalable but introduces ambiguity in point-to-instance assignment.

Method: The BEEP3D approach uses a student-teacher framework, enhancing pseudo-mask generation through an Exponential Moving Average update mechanism and introducing techniques like instance center-based query refinement and novel loss functions for better alignment.

Result: BEEP3D consistently achieves competitive or superior performance compared to state-of-the-art weakly supervised models on benchmarks like ScanNetV2 and S3DIS, maintaining computational efficiency.

Conclusion: BEEP3D optimizes the trade-off between annotation effort and segmentation performance, refining weakly supervised methods for practical applications in 3D environments.

Abstract: 3D instance segmentation is crucial for understanding complex 3D
environments, yet fully supervised methods require dense point-level
annotations, resulting in substantial annotation costs and labor overhead. To
mitigate this, box-level annotations have been explored as a weaker but more
scalable form of supervision. However, box annotations inherently introduce
ambiguity in overlapping regions, making accurate point-to-instance assignment
challenging. Recent methods address this ambiguity by generating pseudo-masks
through training a dedicated pseudo-labeler in an additional training stage.
However, such two-stage pipelines often increase overall training time and
complexity, hinder end-to-end optimization. To overcome these challenges, we
propose BEEP3D-Box-supervised End-to-End Pseudo-mask generation for 3D instance
segmentation. BEEP3D adopts a student-teacher framework, where the teacher
model serves as a pseudo-labeler and is updated by the student model via an
Exponential Moving Average. To better guide the teacher model to generate
precise pseudo-masks, we introduce an instance center-based query refinement
that enhances position query localization and leverages features near instance
centers. Additionally, we design two novel losses-query consistency loss and
masked feature consistency loss-to align semantic and geometric signals between
predictions and pseudo-masks. Extensive experiments on ScanNetV2 and S3DIS
datasets demonstrate that BEEP3D achieves competitive or superior performance
compared to state-of-the-art weakly supervised methods while remaining
computationally efficient.

</details>


### [137] [CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs](https://arxiv.org/abs/2510.12184)
*Jiwan Kim,Kibum Kim,Sangwoo Seo,Chanyoung Park*

Main category: cs.CV

TL;DR: The paper introduces CompoDistill, a knowledge distillation framework aligning student and teacher models' visual attention, improving multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: Efficient Multimodal Large Language Models face computational challenges; existing knowledge distillation methods fail to effectively transfer visual perception abilities.

Method: CompoDistill aligns visual attention between teacher and student models to improve the student's visual perception abilities.

Result: Experiments show CompoDistill boosts compositional reasoning and visual question answering tasks while being adaptable to advanced model architectures.

Conclusion: CompoDistill enhances knowledge distillation in MLLMs by addressing visual attention misalignment, improving performance and generalizability.

Abstract: Recently, efficient Multimodal Large Language Models (MLLMs) have gained
significant attention as a solution to their high computational complexity,
making them more practical for real-world applications. In this regard, the
knowledge distillation (KD) approach has emerged as a promising alternative,
which transfers the rich visual and linguistic knowledge from a larger model
(teacher) to a smaller model (student). However, we observe that existing KD
methods struggle to effectively distill the teacher MLLM's rich visual
perception abilities to the student, a challenge that has been largely
overlooked in previous studies. Through a systematic analysis, we identify
visual attention misalignment between student and teacher as the main cause of
this issue. Based on this insight, we propose CompoDistill, a novel KD
framework that explicitly aligns the student's visual attention with that of
the teacher to enhance the student's visual perception abilities. Our extensive
experiments show that CompoDistill significantly improves performance on
compositional reasoning tasks that require visual perception abilities while
maintaining strong performance on visual question answering tasks, as done in
existing studies. Furthermore, CompoDistill demonstrates effectiveness with a
more advanced backbone, highlighting its generalizability.

</details>


### [138] [Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos](https://arxiv.org/abs/2510.12190)
*Shingo Yokoi,Kento Sasaki,Yu Yamaguchi*

Main category: cs.CV

TL;DR: This paper addresses challenges in autonomous driving in out-of-distribution (OOD) scenarios by introducing a hierarchical reasoning framework for incident report generation in dashcam videos, which achieves second place in the 2COOOL challenge.


<details>
  <summary>Details</summary>
Motivation: To improve autonomous driving models' performance in OOD scenarios by enhancing their ability to generate human-interpretable incident reports based on dashcam videos.

Method: The paper presents a hierarchical reasoning approach that combines frame-level captioning, incident frame detection, and fine-grained reasoning within vision-language models (VLMs), alongside model ensembling and a Blind A/B Scoring selection protocol.

Result: The proposed method ranked 2nd among 29 teams on the 2COOOL leaderboard and achieved the best CIDEr-D score for generating accurate and coherent incident narratives.

Conclusion: Hierarchical reasoning with vision-language models is shown to be effective in analyzing accidents and understanding safety-critical traffic events.

Abstract: Recent advances in end-to-end (E2E) autonomous driving have been enabled by
training on diverse large-scale driving datasets, yet autonomous driving models
still struggle in out-of-distribution (OOD) scenarios. The COOOL benchmark
targets this gap by encouraging hazard understanding beyond closed taxonomies,
and the 2COOOL challenge extends it to generating human-interpretable incident
reports. We present a hierarchical reasoning framework for incident report
generation from dashcam videos that integrates frame-level captioning, incident
frame detection, and fine-grained reasoning within vision-language models
(VLMs). We further improve factual accuracy and readability through model
ensembling and a Blind A/B Scoring selection protocol. On the official 2COOOL
open leaderboard, our method ranks 2nd among 29 teams and achieves the best
CIDEr-D score, producing accurate and coherent incident narratives. These
results indicate that hierarchical reasoning with VLMs is a promising direction
for accident analysis and for broader understanding of safety-critical traffic
events. The implementation and code are available at
https://github.com/riron1206/kaggle-2COOOL-2nd-Place-Solution.

</details>


### [139] [The Impact of Synthetic Data on Object Detection Model Performance: A Comparative Analysis with Real-World Data](https://arxiv.org/abs/2510.12208)
*Muammer Bay,Timo von Marcard,Dren Fazlija*

Main category: cs.CV

TL;DR: This paper evaluates synthetic data's impact on object detection models in warehouse logistics, using NVIDIA Omniverse Replicator for experiments.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of real-world data for fine-tuning general-purpose AI models, especially in resource-constrained scenarios.

Method: Generated synthetic data using NVIDIA Omniverse Replicator and evaluated its effect on object detection performance in a warehouse setting, comparing results with models trained on real-world data.

Result: Found that balanced integration of synthetic and real data enhances object detection performance and provides cost efficiency.

Conclusion: Synthetic data is a viable solution for improving object detection models, especially in domains like warehouse logistics, where real-world data is scarce or expensive.

Abstract: Recent advances in generative AI, particularly in computer vision (CV), offer
new opportunities to optimize workflows across industries, including logistics
and manufacturing. However, many AI applications are limited by a lack of
expertise and resources, which forces a reliance on general-purpose models.
Success with these models often requires domain-specific data for fine-tuning,
which can be costly and inefficient. Thus, using synthetic data for fine-tuning
is a popular, cost-effective alternative to gathering real-world data. This
work investigates the impact of synthetic data on the performance of object
detection models, compared to models trained on real-world data only,
specifically within the domain of warehouse logistics. To this end, we examined
the impact of synthetic data generated using the NVIDIA Omniverse Replicator
tool on the effectiveness of object detection models in real-world scenarios.
It comprises experiments focused on pallet detection in a warehouse setting,
utilizing both real and various synthetic dataset generation strategies. Our
findings provide valuable insights into the practical applications of synthetic
image data in computer vision, suggesting that a balanced integration of
synthetic and real data can lead to robust and efficient object detection
models.

</details>


### [140] [DIANet: A Phase-Aware Dual-Stream Network for Micro-Expression Recognition via Dynamic Images](https://arxiv.org/abs/2510.12219)
*Vu Tram Anh Khuong,Luu Tu Nguyen,Thi Bich Phuong Man,Thanh Ha Le,Thi Duyen Ngo*

Main category: cs.CV

TL;DR: The paper introduces DIANet, a dual-stream framework to enhance micro-expression recognition by using phase-aware dynamic images, and shows it outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Micro-expression recognition is crucial yet challenging due to subtle cues and limited annotated data. Existing methods often fail to incorporate distinct temporal phases effectively.

Method: Proposes DIANet, utilizing dynamic images that encode onset-to-apex and apex-to-offset phases separately, processed through dedicated neural networks. Cross-attention fusion integrates features adaptively.

Result: Experiments on three benchmark datasets show DIANet outperforms conventional single-phase methods, confirming the advantage of explicit phase modeling.

Conclusion: Explicitly modeling temporal phase information improves micro-expression recognition, offering a promising direction for future research in this area.

Abstract: Micro-expressions are brief, involuntary facial movements that typically last
less than half a second and often reveal genuine emotions. Accurately
recognizing these subtle expressions is critical for applications in
psychology, security, and behavioral analysis. However, micro-expression
recognition (MER) remains a challenging task due to the subtle and transient
nature of facial cues and the limited availability of annotated data. While
dynamic image (DI) representations have been introduced to summarize temporal
motion into a single frame, conventional DI-based methods often overlook the
distinct characteristics of different temporal phases within a
micro-expression. To address this issue, this paper proposes a novel
dual-stream framework, DIANet, which leverages phase-aware dynamic images - one
encoding the onset-to-apex phase and the other capturing the apex-to-offset
phase. Each stream is processed by a dedicated convolutional neural network,
and a cross-attention fusion module is employed to adaptively integrate
features from both streams based on their contextual relevance. Extensive
experiments conducted on three benchmark MER datasets (CASME-II, SAMM, and
MMEW) demonstrate that the proposed method consistently outperforms
conventional single-phase DI-based approaches. The results highlight the
importance of modeling temporal phase information explicitly and suggest a
promising direction for advancing MER.

</details>


### [141] [CuMPerLay: Learning Cubical Multiparameter Persistence Vectorizations](https://arxiv.org/abs/2510.12795)
*Caner Korkmaz,Brighton Nuwagira,Barış Coşkunuzer,Tolga Birdal*

Main category: cs.CV

TL;DR: CuMPerLay is a novel differentiable layer that integrates Cubical Multiparameter Persistence into deep learning for structured image analysis.


<details>
  <summary>Details</summary>
Motivation: To overcome difficulties in using CMP for image analysis due to multifiltration complexity and vectorization challenges.

Method: A new algorithm for vectorizing CMP is developed, decomposing CMP into learnable single-parameter persistence paired with bifiltration function learning.

Result: CuMPerLay demonstrates improved performance in classification and segmentation tasks, especially in low-data settings.

Conclusion: CuMPerLay provides stable and robust topological features, showing promise for integrating geometric insights into deep image analysis.

Abstract: We present CuMPerLay, a novel differentiable vectorization layer that enables
the integration of Cubical Multiparameter Persistence (CMP) into deep learning
pipelines. While CMP presents a natural and powerful way to topologically work
with images, its use is hindered by the complexity of multifiltration
structures as well as the vectorization of CMP. In face of these challenges, we
introduce a new algorithm for vectorizing MP homologies of cubical complexes.
Our CuMPerLay decomposes the CMP into a combination of individual, learnable
single-parameter persistence, where the bifiltration functions are jointly
learned. Thanks to the differentiability, its robust topological feature
vectors can be seamlessly used within state-of-the-art architectures such as
Swin Transformers. We establish theoretical guarantees for the stability of our
vectorization under generalized Wasserstein metrics. Our experiments on
benchmark medical imaging and computer vision datasets show the benefit
CuMPerLay on classification and segmentation performance, particularly in
limited-data scenarios. Overall, CuMPerLay offers a promising direction for
integrating global structural information into deep networks for structured
image analysis.

</details>


### [142] [HoneyBee: Data Recipes for Vision-Language Reasoners](https://arxiv.org/abs/2510.12225)
*Hritik Bansal,Devandra Singh Sachan,Kai-Wei Chang,Aditya Grover,Gargi Ghosh,Wen-tau Yih,Ramakanth Pasunuru*

Main category: cs.CV

TL;DR: This paper studies how data curation affects the performance of vision-language models (VLMs) in reasoning tasks, introducing techniques like data interventions and scaling dimensions, and proposes HoneyBee, a new dataset yielding superior VLM performance.


<details>
  <summary>Details</summary>
Motivation: Understanding the principles behind constructing effective training datasets for vision-language reasoning tasks to overcome gaps in current VLM training methods.

Method: Various data curation approaches, including context source analysis, targeted interventions like auxiliary signals, and scaling dimensions (e.g., questions and CoTs per image), alongside creating and testing the HoneyBee dataset.

Result: HoneyBee-trained VLMs surpass state-of-the-art models on benchmarks like MathVerse, achieving improved reasoning performance and reducing decoding costs using a new test-time scaling strategy.

Conclusion: This paper shows that strategic data curation significantly boosts VL reasoning capabilities and introduces HoneyBee as a benchmark dataset to elevate VLM performance and efficiency.

Abstract: Recent advances in vision-language models (VLMs) have made them highly
effective at reasoning tasks. However, the principles underlying the
construction of performant VL reasoning training datasets remain poorly
understood. In this work, we introduce several data curation approaches and
study their impacts on VL reasoning capabilities by carefully controlling
training and evaluation setups. We analyze the effects of context (image and
question pair) sources, implement targeted data interventions, and explore
scaling up images, questions, and chain-of-thought (CoT) solutions. Our
findings reveal that (a) context source strategies significantly affect VLM
performance, (b) interventions such as auxiliary signals from image captions
and the inclusion of text-only reasoning yield substantial gains, and (c)
scaling all data dimensions (e.g., unique questions per image and unique CoTs
per image-question pair) consistently improves reasoning capability. Motivated
by these insights, we introduce HoneyBee, a large-scale, high-quality CoT
reasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs
trained with HoneyBee outperform state-of-the-art models across model sizes.
For instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA
model and the base model by 7.8% and 24.8%, respectively, on MathVerse.
Furthermore, we propose a test-time scaling strategy that reduces decoding cost
by 73% without sacrificing accuracy. Overall, this work presents improved
strategies for VL reasoning dataset curation research.

</details>


### [143] [CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving](https://arxiv.org/abs/2510.12560)
*Xiaoji Zheng,Ziyuan Yang,Yanhao Chen,Yuhang Peng,Yuanrong Tang,Gengyuan Liu,Bokui Chen,Jiangtao Gong*

Main category: cs.CV

TL;DR: Combining imitation learning (IL) and reinforcement learning (RL) for autonomous driving, CoIRL-AD introduces a dual-policy framework to improve generalization and reduce collision rates.


<details>
  <summary>Details</summary>
Motivation: Improving generalization and performance in end-to-end autonomous driving models, which often face challenges using only IL or RL.

Method: A competitive dual-policy framework, CoIRL-AD, integrates IL and RL agents during training through a competition-based mechanism for better collaboration and gradient conflict prevention.

Result: Experiments showed an 18% reduction in collisions and improved generalization and performance in long-tail scenarios, validated on the nuScenes dataset.

Conclusion: The CoIRL-AD approach effectively combines IL and RL, promoting better exploration and knowledge exchange, addressing weaknesses of traditional methods, and improving autonomous driving outcomes.

Abstract: End-to-end autonomous driving models trained solely with imitation learning
(IL) often suffer from poor generalization. In contrast, reinforcement learning
(RL) promotes exploration through reward maximization but faces challenges such
as sample inefficiency and unstable convergence. A natural solution is to
combine IL and RL. Moving beyond the conventional two-stage paradigm (IL
pretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive
dual-policy framework that enables IL and RL agents to interact during
training. CoIRL-AD introduces a competition-based mechanism that facilitates
knowledge exchange while preventing gradient conflicts. Experiments on the
nuScenes dataset show an 18% reduction in collision rate compared to baselines,
along with stronger generalization and improved performance on long-tail
scenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.

</details>


### [144] [BIGFix: Bidirectional Image Generation with Token Fixing](https://arxiv.org/abs/2510.12231)
*Victor Besnier,David Hurych,Andrei Bursuc,Eduardo Valle*

Main category: cs.CV

TL;DR: The paper introduces a method to refine token predictions in image and video generation, improving efficiency and quality.


<details>
  <summary>Details</summary>
Motivation: To address inefficiency challenges in generative models caused by large model sizes and numerous inference steps, which impact commercial and scientific viability.

Method: A training scheme that uses random token injection to allow iterative correction during the sampling process, enhancing robustness and quality.

Result: The method significantly improved image generation on ImageNet-256 and CIFAR-10, and video generation on UCF-101 and NuScenes datasets.

Conclusion: The proposed method balances the advantages of parallel token prediction with better generation quality, marking progress in efficient and high-quality generative modeling.

Abstract: Recent advances in image and video generation have raised significant
interest from both academia and industry. A key challenge in this field is
improving inference efficiency, as model size and the number of inference steps
directly impact the commercial viability of generative models while also posing
fundamental scientific challenges. A promising direction involves combining
auto-regressive sequential token modeling with multi-token prediction per step,
reducing inference time by up to an order of magnitude. However, predicting
multiple tokens in parallel can introduce structural inconsistencies due to
token incompatibilities, as capturing complex joint dependencies during
training remains challenging. Traditionally, once tokens are sampled, there is
no mechanism to backtrack and refine erroneous predictions. We propose a method
for self-correcting image generation by iteratively refining sampled tokens. We
achieve this with a novel training scheme that injects random tokens in the
context, improving robustness and enabling token fixing during sampling. Our
method preserves the efficiency benefits of parallel token prediction while
significantly enhancing generation quality. We evaluate our approach on image
generation using the ImageNet-256 and CIFAR-10 datasets, as well as on video
generation with UCF-101 and NuScenes, demonstrating substantial improvements
across both modalities.

</details>


### [145] [EReLiFM: Evidential Reliability-Aware Residual Flow Meta-Learning for Open-Set Domain Generalization under Noisy Labels](https://arxiv.org/abs/2510.12687)
*Kunyu Peng,Di Wen,Kailun Yang,Jia Fu,Yufan Chen,Ruiping Liu,Jiamin Wu,Junwei Zheng,M. Saquib Sarfraz,Luc Van Gool,Danda Pani Paudel,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: The paper introduces "EReLiFM," a meta-learning model improving domain generalization under noisy labels, achieving state-of-the-art results for handling unseen categories.


<details>
  <summary>Details</summary>
Motivation: Label noise obstinately harms knowledge transfer and classification for unseen categories in open-set domain generalization, urging improved methods for robustness.

Method: EReLiFM combines unsupervised evidential loss clustering and residual flow matching, modeling uncertainties while optimizing meta-learning using pseudo-labels.

Result: The method surpasses existing techniques by achieving superior performance in noisy label setups, validated through experimental evaluation and benchmark comparisons.

Conclusion: EReLiFM represents an effective advancement in OSDG-NL research, demonstrating improved generalization through innovative clustering and residual modeling.

Abstract: Open-Set Domain Generalization (OSDG) aims to enable deep learning models to
recognize unseen categories in new domains, which is crucial for real-world
applications. Label noise hinders open-set domain generalization by corrupting
source-domain knowledge, making it harder to recognize known classes and reject
unseen ones. While existing methods address OSDG under Noisy Labels (OSDG-NL)
using hyperbolic prototype-guided meta-learning, they struggle to bridge domain
gaps, especially with limited clean labeled data. In this paper, we propose
Evidential Reliability-Aware Residual Flow Meta-Learning (EReLiFM). We first
introduce an unsupervised two-stage evidential loss clustering method to
promote label reliability awareness. Then, we propose a residual flow matching
mechanism that models structured domain- and category-conditioned residuals,
enabling diverse and uncertainty-aware transfer paths beyond
interpolation-based augmentation. During this meta-learning process, the model
is optimized such that the update direction on the clean set maximizes the loss
decrease on the noisy set, using pseudo labels derived from the most confident
predicted class for supervision. Experimental results show that EReLiFM
outperforms existing methods on OSDG-NL, achieving state-of-the-art
performance. The source code is available at
https://github.com/KPeng9510/ERELIFM.

</details>


### [146] [Ivan-ISTD: Rethinking Cross-domain Heteroscedastic Noise Perturbations in Infrared Small Target Detection](https://arxiv.org/abs/2510.12241)
*Yuehui Li,Yahao Lu,Haoyuan Wu,Sen Zhang,Liang Lin,Yukai Shi*

Main category: cs.CV

TL;DR: The paper introduces Ivan-ISTD, a framework for improving Infrared Small Target Detection (ISTD) by addressing cross-domain shifts and noise through wavelet-guided methods and dynamic noise libraries.


<details>
  <summary>Details</summary>
Motivation: To overcome the dual challenges of cross-domain domain shift and heteroscedastic noise, which hinder the effectiveness of ISTD in practical and varied scenarios.

Method: The framework consists of two stages: wavelet-guided cross-domain synthesis for domain alignment and real-domain noise invariance learning using self-supervised loss and a dynamic noise library. This is supplemented by a new benchmark dataset, Dynamic-ISTD.

Result: The experimental results show that Ivan-ISTD outperforms state-of-the-art methods across various quantitative metrics and demonstrates robustness in cross-domain ISTD scenarios.

Conclusion: Ivan-ISTD effectively addresses the limitations of existing ISTD methods, delivering superior performance and robustness across dynamic real-world applications, as validated by experiments and benchmarks.

Abstract: In the multimedia domain, Infrared Small Target Detection (ISTD) plays a
important role in drone-based multi-modality sensing. To address the dual
challenges of cross-domain shift and heteroscedastic noise perturbations in
ISTD, we propose a doubly wavelet-guided Invariance learning
framework(Ivan-ISTD). In the first stage, we generate training samples aligned
with the target domain using Wavelet-guided Cross-domain Synthesis. This
wavelet-guided alignment machine accurately separates the target background
through multi-frequency wavelet filtering. In the second stage, we introduce
Real-domain Noise Invariance Learning, which extracts real noise
characteristics from the target domain to build a dynamic noise library. The
model learns noise invariance through self-supervised loss, thereby overcoming
the limitations of distribution bias in traditional artificial noise modeling.
Finally, we create the Dynamic-ISTD Benchmark, a cross-domain dynamic
degradation dataset that simulates the distribution shifts encountered in
real-world applications. Additionally, we validate the versatility of our
method using other real-world datasets. Experimental results demonstrate that
our approach outperforms existing state-of-the-art methods in terms of many
quantitative metrics. In particular, Ivan-ISTD demonstrates excellent
robustness in cross-domain scenarios. The code for this work can be found at:
https://github.com/nanjin1/Ivan-ISTD.

</details>


### [147] [Vectorized Video Representation with Easy Editing via Hierarchical Spatio-Temporally Consistent Proxy Embedding](https://arxiv.org/abs/2510.12256)
*Ye Chen,Liming Tan,Yupeng Zhu,Yuanbin Wang,Bingbing Ni*

Main category: cs.CV

TL;DR: The paper introduces spatio-temporally consistent proxy nodes for robust video representation, improving on traditional pixel-level models.


<details>
  <summary>Details</summary>
Motivation: Current video representation methods are fragile due to reliance on pixel-level matching and tracking, which are sensitive to tracking errors, occlusions, and motion.

Method: The method leverages hierarchical proxy nodes for stable, multi-scale structure representation and a dynamic update mechanism to handle spatio-temporal changes and inaccuracies.

Result: This approach achieves high video reconstruction accuracy with fewer parameters and supports tasks like video inpainting and temporally consistent keyframe-based editing.

Conclusion: Proxy node-based video representation is a robust, efficient alternative to pixel-level methods, enhancing video editing and reconstruction capabilities.

Abstract: Current video representations heavily rely on unstable and over-grained
priors for motion and appearance modelling, \emph{i.e.}, pixel-level matching
and tracking. A tracking error of just a few pixels would lead to the collapse
of the visual object representation, not to mention occlusions and large motion
frequently occurring in videos. To overcome the above mentioned vulnerability,
this work proposes spatio-temporally consistent proxy nodes to represent
dynamically changing objects/scenes in the video. On the one hand, the
hierarchical proxy nodes have the ability to stably express the multi-scale
structure of visual objects, so they are not affected by accumulated tracking
error, long-term motion, occlusion, and viewpoint variation. On the other hand,
the dynamic representation update mechanism of the proxy nodes adequately
leverages spatio-temporal priors of the video to mitigate the impact of
inaccurate trackers, thereby effectively handling drastic changes in scenes and
objects. Additionally, the decoupled encoding manner of the shape and texture
representations across different visual objects in the video facilitates
controllable and fine-grained appearance editing capability. Extensive
experiments demonstrate that the proposed representation achieves high video
reconstruction accuracy with fewer parameters and supports complex video
processing tasks, including video in-painting and keyframe-based temporally
consistent video editing.

</details>


### [148] [Multiplicative Loss for Enhancing Semantic Segmentation in Medical and Cellular Images](https://arxiv.org/abs/2510.12258)
*Yuto Yokoi,Kazuhiro Hotta*

Main category: cs.CV

TL;DR: This paper introduces two new loss functions for improving semantic segmentation in medical and cellular images under conditions of limited data: Multiplicative Loss and Confidence-Adaptive Multiplicative Loss.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of widely used Cross Entropy and Dice Loss functions, which struggle with hyperparameter sensitivity and perform poorly under data scarcity, a common issue in medical imaging.

Method: The paper proposes Multiplicative Loss, which combines Cross Entropy and Dice loss multiplicatively to dynamically adjust gradients based on confidence. It also introduces Confidence-Adaptive Multiplicative Loss, incorporating confidence-driven exponential scaling to focus on challenging samples.

Result: Experimental results show that the proposed loss functions consistently outperform traditional additive loss combinations and other existing methods in medical and cellular image segmentation benchmarks.

Conclusion: The proposed loss functions provide a robust, effective, and hyperparameter-free approach, particularly suited for data-limited scenarios, advancing segmentation performance in challenging contexts.

Abstract: We propose two novel loss functions, Multiplicative Loss and
Confidence-Adaptive Multiplicative Loss, for semantic segmentation in medical
and cellular images. Although Cross Entropy and Dice Loss are widely used,
their additive combination is sensitive to hyperparameters and often performs
suboptimally, especially with limited data. Medical images suffer from data
scarcity due to privacy, ethics, and costly annotations, requiring robust and
efficient training objectives. Our Multiplicative Loss combines Cross Entropy
and Dice losses multiplicatively, dynamically modulating gradients based on
prediction confidence. This reduces penalties for confident correct predictions
and amplifies gradients for incorrect overconfident ones, stabilizing
optimization. Building on this, Confidence-Adaptive Multiplicative Loss applies
a confidence-driven exponential scaling inspired by Focal Loss, integrating
predicted probabilities and Dice coefficients to emphasize difficult samples.
This enhances learning under extreme data scarcity by strengthening gradients
when confidence is low. Experiments on cellular and medical segmentation
benchmarks show our framework consistently outperforms tuned additive and
existing loss functions, offering a simple, effective, and hyperparameter-free
mechanism for robust segmentation under challenging data limitations.

</details>


### [149] [Local Background Features Matter in Out-of-Distribution Detection](https://arxiv.org/abs/2510.12259)
*Jinlun Ye,Zhuohao Sun,Yiqiao Qiu,Qiu Li,Zhijun Tan,Ruixuan Wang*

Main category: cs.CV

TL;DR: The paper introduces a new Out-of-distribution (OOD) detection method using local background features from in-distribution (ID) images for training, achieving state-of-the-art detection results.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of overconfident predictions on OOD data by neural networks, without relying on expensive auxiliary OOD datasets or generated fake OOD images.

Method: The proposed method extracts background features from ID images to simulate OOD visual representations during training, leveraging local invariance of convolution and optimizing $L_2$-norm reduction.

Result: Extensive experiments on standard OOD detection benchmarks show the effectiveness of the proposed method and its compatibility with existing post-hoc methods, achieving state-of-the-art performance.

Conclusion: The research provides a cost-effective and efficient OOD detection approach, leveraging background features to improve model reliability and safety in real-world applications.

Abstract: Out-of-distribution (OOD) detection is crucial when deploying deep neural
networks in the real world to ensure the reliability and safety of their
applications. One main challenge in OOD detection is that neural network models
often produce overconfident predictions on OOD data. While some methods using
auxiliary OOD datasets or generating fake OOD images have shown promising OOD
detection performance, they are limited by the high costs of data collection
and training. In this study, we propose a novel and effective OOD detection
method that utilizes local background features as fake OOD features for model
training. Inspired by the observation that OOD images generally share similar
background regions with ID images, the background features are extracted from
ID images as simulated OOD visual representations during training based on the
local invariance of convolution. Through being optimized to reduce the
$L_2$-norm of these background features, the neural networks are able to
alleviate the overconfidence issue on OOD data. Extensive experiments on
multiple standard OOD detection benchmarks confirm the effectiveness of our
method and its wide combinatorial compatibility with existing post-hoc methods,
with new state-of-the-art performance achieved from our method.

</details>


### [150] [AngularFuse: A Closer Look at Angle-based Perception for Spatial-Sensitive Multi-Modality Image Fusion](https://arxiv.org/abs/2510.12260)
*Xiaopeng Liu,Yupei Lin,Sen Zhang,Xiao Wang,Yukai Shi,Liang Lin*

Main category: cs.CV

TL;DR: AngularFuse introduces innovative fusion techniques for visible-infrared image processing, addressing limitations in existing methods and achieving improved results on public datasets.


<details>
  <summary>Details</summary>
Motivation: To address challenges in unsupervised visible-infrared image fusion, particularly issues with manually designed loss functions and reference image limitations.

Method: Proposed AngularFuse framework includes a cross-modal complementary mask module, fine-grained reference image synthesis combining edge enhancement and histogram equalization, and an angle-aware loss considering both gradient magnitude and direction.

Result: Extensive experiments on standard datasets (MSRS, RoadScene, M3FD) demonstrate that AngularFuse outperforms existing methods in both quantitative metrics and visual quality.

Conclusion: AngularFuse effectively enhances brightness, details, and texture directionality, offering a superior solution for spatial-sensitive image fusion applications like autonomous driving and surveillance.

Abstract: Visible-infrared image fusion is crucial in key applications such as
autonomous driving and nighttime surveillance. Its main goal is to integrate
multimodal information to produce enhanced images that are better suited for
downstream tasks. Although deep learning based fusion methods have made
significant progress, mainstream unsupervised approaches still face serious
challenges in practical applications. Existing methods mostly rely on manually
designed loss functions to guide the fusion process. However, these loss
functions have obvious limitations. On one hand, the reference images
constructed by existing methods often lack details and have uneven brightness.
On the other hand, the widely used gradient losses focus only on gradient
magnitude. To address these challenges, this paper proposes an angle-based
perception framework for spatial-sensitive image fusion (AngularFuse). At
first, we design a cross-modal complementary mask module to force the network
to learn complementary information between modalities. Then, a fine-grained
reference image synthesis strategy is introduced. By combining Laplacian edge
enhancement with adaptive histogram equalization, reference images with richer
details and more balanced brightness are generated. Last but not least, we
introduce an angle-aware loss, which for the first time constrains both
gradient magnitude and direction simultaneously in the gradient domain.
AngularFuse ensures that the fused images preserve both texture intensity and
correct edge orientation. Comprehensive experiments on the MSRS, RoadScene, and
M3FD public datasets show that AngularFuse outperforms existing mainstream
methods with clear margin. Visual comparisons further confirm that our method
produces sharper and more detailed results in challenging scenes, demonstrating
superior fusion capability.

</details>


### [151] [SpineBench: Benchmarking Multimodal LLMs for Spinal Pathology Analysis](https://arxiv.org/abs/2510.12267)
*Chenghanyu Zhang,Zekun Li,Peipei Li,Xing Cui,Shuhan Xia,Weixiang Yan,Yiqiao Zhang,Qianyu Zhuang*

Main category: cs.CV

TL;DR: The paper introduces SpineBench, a Visual Question Answering benchmark for evaluating Multimodal Large Language Models (MLLMs) in spinal medicine tasks such as disease diagnosis and lesion localization.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to evaluate MLLMs effectively in specialized medical fields like spinal medicine that rely heavily on visual data.

Method: The authors created SpineBench by standardizing open-source spinal disease datasets and generating QA pairs with hard negative options to simulate real-world scenarios. They assessed 12 MLLMs using this benchmark.

Result: Evaluations showed the 12 MLLMs underperforming in spine-related tasks, indicating limitations in current MLLMs for specialized medical domains.

Conclusion: SpineBench serves as a valuable tool to highlight gaps in MLLM capabilities in spinal medicine, fostering targeted advancements in this field.

Abstract: With the increasing integration of Multimodal Large Language Models (MLLMs)
into the medical field, comprehensive evaluation of their performance in
various medical domains becomes critical. However, existing benchmarks
primarily assess general medical tasks, inadequately capturing performance in
nuanced areas like the spine, which relies heavily on visual input. To address
this, we introduce SpineBench, a comprehensive Visual Question Answering (VQA)
benchmark designed for fine-grained analysis and evaluation of MLLMs in the
spinal domain. SpineBench comprises 64,878 QA pairs from 40,263 spine images,
covering 11 spinal diseases through two critical clinical tasks: spinal disease
diagnosis and spinal lesion localization, both in multiple-choice format.
SpineBench is built by integrating and standardizing image-label pairs from
open-source spinal disease datasets, and samples challenging hard negative
options for each VQA pair based on visual similarity (similar but not the same
disease), simulating real-world challenging scenarios. We evaluate 12 leading
MLLMs on SpineBench. The results reveal that these models exhibit poor
performance in spinal tasks, highlighting limitations of current MLLM in the
spine domain and guiding future improvements in spinal medicine applications.
SpineBench is publicly available at
https://zhangchenghanyu.github.io/SpineBench.github.io/.

</details>


### [152] [PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes](https://arxiv.org/abs/2510.12282)
*Ying A,Wenzhang Sun,Chang Zeng,Chunfeng Wang,Hao Li,Jianxun Cui*

Main category: cs.CV

TL;DR: PAGS is a framework for improving dynamic 3D urban scene reconstruction efficiency by injecting semantic priorities, focusing on safety-critical elements, and achieving high fidelity and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D urban scene reconstruction face inefficiencies due to semantically agnostic designs, failing to prioritize safety-critical objects and balancing fidelity with computational costs.

Method: PAGS introduces Semantically-Guided Pruning and Regularization to simplify non-critical elements and Priority-Driven Rendering to optimize computational resources by culling occluded primitives.

Result: Experiments on Waymo and KITTI datasets show PAGS achieves superior reconstruction quality for critical objects, significantly reduced training times, and rendering speeds exceeding 350 FPS.

Conclusion: PAGS effectively balances fidelity and speed by prioritizing safety-critical objects, demonstrating its potential for advancements in autonomous driving 3D urban scene reconstruction.

Abstract: Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet
current methods face a stark trade-off between fidelity and computational cost.
This inefficiency stems from their semantically agnostic design, which
allocates resources uniformly, treating static backgrounds and safety-critical
objects with equal importance. To address this, we introduce Priority-Adaptive
Gaussian Splatting (PAGS), a framework that injects task-aware semantic
priorities directly into the 3D reconstruction and rendering pipeline. PAGS
introduces two core contributions: (1) Semantically-Guided Pruning and
Regularization strategy, which employs a hybrid importance metric to
aggressively simplify non-critical scene elements while preserving fine-grained
details on objects vital for navigation. (2) Priority-Driven Rendering
pipeline, which employs a priority-based depth pre-pass to aggressively cull
occluded primitives and accelerate the final shading computations. Extensive
experiments on the Waymo and KITTI datasets demonstrate that PAGS achieves
exceptional reconstruction quality, particularly on safety-critical objects,
while significantly reducing training time and boosting rendering speeds to
over 350 FPS.

</details>


### [153] [Dual Learning with Dynamic Knowledge Distillation and Soft Alignment for Partially Relevant Video Retrieval](https://arxiv.org/abs/2510.12283)
*Jianfeng Dong,Lei Huang,Daizong Liu,Xianke Chen,Xun Yang,Changting Lin,Xun Wang,Meng Wang*

Main category: cs.CV

TL;DR: The paper addresses partially relevant video retrieval (PRVR) for untrimmed videos, introducing a novel framework to tackle related challenges and achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Many text-to-video frameworks exist in ideal scenarios with videos trimmed to match queries. However, practical scenarios involve untrimmed videos with complex content, necessitating effective methods for PRVR.

Method: The authors propose Dynamic Knowledge Distillation (DL-DKD++) using a dual-branch student model supervised by a powerful teacher model, with dynamic soft targets for adaptive learning.

Result: The framework achieves state-of-the-art performance on benchmarks such as TVR, ActivityNet, and Charades-STA for partially relevant video queries.

Conclusion: The DL-DKD++ framework effectively bridges domain gaps and fine-tunes PRVR task-specific nuances, demonstrating high efficiency and robustness for video retrieval tasks.

Abstract: Almost all previous text-to-video retrieval works ideally assume that videos
are pre-trimmed with short durations containing solely text-related content.
However, in practice, videos are typically untrimmed in long durations with
much more complicated background content. Therefore, in this paper, we focus on
the more practical yet challenging task of Partially Relevant Video Retrieval
(PRVR), which aims to retrieve partially relevant untrimmed videos with the
given query. To tackle this task, we propose a novel framework that distills
generalization knowledge from a powerful large-scale vision-language
pre-trained model and transfers it to a lightweight, task-specific PRVR
network. Specifically, we introduce a Dual Learning framework with Dynamic
Knowledge Distillation (DL-DKD++), where a large teacher model provides
supervision to a compact dual-branch student network. The student model
comprises two branches: an inheritance branch that absorbs transferable
knowledge from the teacher, and an exploration branch that learns task-specific
information from the PRVR dataset to address domain gaps. To further enhance
learning, we incorporate a dynamic soft-target construction mechanism. By
replacing rigid hard-target supervision with adaptive soft targets that evolve
during training, our method enables the model to better capture the
fine-grained, partial relevance between videos and queries. Experiment results
demonstrate that our proposed model achieves state-of-the-art performance on
TVR, ActivityNet, and Charades-STA datasets for PRVR. The code is available at
https://github.com/HuiGuanLab/DL-DKD.

</details>


### [154] [Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector](https://arxiv.org/abs/2510.12287)
*Sifan Li,Hongkai Chen,Yujun Cai,Qingwen Ye,Liyang Chen,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: Vision Language Models (VLMs) are susceptible to "logo hallucination," generating brand names or text content where logos lack visible words. Despite distortions, hallucinations persist due to reliance on symbolic priors rather than glyph perception.


<details>
  <summary>Details</summary>
Motivation: Investigate the vulnerability of Vision Language Models (VLMs) to hallucinations, specifically when it comes to logos without visible text.

Method: Curated logo splits, robustness testing with nine structured perturbations, embedding-level analysis targeting projector dimensions, and ablation techniques to distinguish hallucinations and enhance Optical Character Recognition (OCR) accuracy.

Result: Findings show hallucinations stemming from symbolic priors and projector subspaces. Targeted ablation reduces errors while preserving OCR accuracy, exposing issues in multimodal reasoning for logos.

Conclusion: VLMs often rely on symbolic priors rather than true visual comprehension, especially in circular logos, leading to logo hallucinations. Projector disentanglement and OCR-guided decoding are recommended solutions to enhance reliability.

Abstract: Vision Language Models (VLMs) have achieved impressive progress in multimodal
reasoning; yet, they remain vulnerable to hallucinations, where outputs are not
grounded in visual evidence. In this paper, we investigate a previously
overlooked setting: logo hallucination, where models generate brand names or
textual content despite logos containing no visible words. Using curated splits
of pure symbols, hybrids, and text-bearing logos, as well as the challenging
Hard-60 subset, we systematically measure hallucination across leading VLMs. We
further probe robustness through nine structured perturbations and show that
hallucinations persist even under strong distortions, with occlusion exposing
the sharpest weaknesses. Embedding-level analysis with open-weight LLaVA
demonstrates that hallucination is tied to a small subset of projector
dimensions, and targeted ablation substantially reduces errors while preserving
OCR accuracy. Together, these findings reveal that VLMs often rely on symbolic
priors rather than genuine glyph perception, particularly for iconic circular
logos, and that projector subspaces play a decisive role in this failure mode.
Our work contributes both a novel diagnostic lens and actionable mitigation
insights, highlighting projector disentanglement and OCR-guided decoding as
promising directions for building more trustworthy multimodal systems.

</details>


### [155] [Hybrid Gaussian Splatting for Novel Urban View Synthesis](https://arxiv.org/abs/2510.12308)
*Mohamed Omran,Farhad Zanjani,Davide Abati,Jens Petersen,Amirhossein Habibian*

Main category: cs.CV

TL;DR: The paper presents Qualcomm AI Research’s solution to the RealADSim-NVS challenge, focusing on novel view synthesis for urban environments using car-centric frames and achieving second place in the competition.


<details>
  <summary>Details</summary>
Motivation: To develop an innovative method for novel view synthesis in urban street scenes using limited car-centric training frames.

Method: The approach involves a two-stage pipeline: (1) 3D reconstruction of scenes with gaussian splatting for view rendering, and (2) applying a single-step diffusion model for enhancement.

Result: The proposed solution achieves an aggregated score of 0.432 on the public leaderboard, ranking second overall.

Conclusion: The hybrid pipeline effectively combines 3D reconstruction and diffusion model enhancement, demonstrating high-quality novel view synthesis for urban environments.

Abstract: This paper describes the Qualcomm AI Research solution to the RealADSim-NVS
challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge
concerns novel view synthesis in street scenes, and participants are required
to generate, starting from car-centric frames captured during some training
traversals, renders of the same urban environment as viewed from a different
traversal (e.g. different street lane or car direction). Our solution is
inspired by hybrid methods in scene generation and generative simulators
merging gaussian splatting and diffusion models, and it is composed of two
stages: First, we fit a 3D reconstruction of the scene and render novel views
as seen from the target cameras. Then, we enhance the resulting frames with a
dedicated single-step diffusion model. We discuss specific choices made in the
initialization of gaussian primitives as well as the finetuning of the enhancer
model and its training data curation. We report the performance of our model
design and we ablate its components in terms of novel view quality as measured
by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our
proposal reaches an aggregated score of 0.432, achieving the second place
overall.

</details>


### [156] [CurriFlow: Curriculum-Guided Depth Fusion with Optical Flow-Based Temporal Alignment for 3D Semantic Scene Completion](https://arxiv.org/abs/2510.12362)
*Jinzhou Lin,Jie Zhou,Wenhao Xu,Rongtao Xu,Changwei Wang,Shunpeng Chen,Kexue Fu,Yihua Shao,Li Guo,Shibiao Xu*

Main category: cs.CV

TL;DR: The paper introduces CurriFlow, a framework enhancing semantic scene completion by integrating optical flow-based alignment and curriculum-guided depth fusion.


<details>
  <summary>Details</summary>
Motivation: Semantic Scene Completion (SSC) from monocular images is crucial for autonomous driving, but existing methods struggle with motion reasoning, occlusions, and noisy depth data.

Method: CurriFlow employs optical flow for temporal alignment, multi-frame fusion for feature consistency, curriculum learning for depth fusion, and semantic priors from SAM for voxel-level semantic learning.

Result: CurriFlow achieves state-of-the-art performance with a mean IoU of 16.9 on the SemanticKITTI benchmark, proving its effectiveness in dynamic object understanding and temporal consistency.

Conclusion: The proposed CurriFlow framework robustly improves camera-based 3D semantic scene completion by addressing motion and depth challenges, with promising deployment adaptability and enhanced performance.

Abstract: Semantic Scene Completion (SSC) aims to infer complete 3D geometry and
semantics from monocular images, serving as a crucial capability for
camera-based perception in autonomous driving. However, existing SSC methods
relying on temporal stacking or depth projection often lack explicit motion
reasoning and struggle with occlusions and noisy depth supervision. We propose
CurriFlow, a novel semantic occupancy prediction framework that integrates
optical flow-based temporal alignment with curriculum-guided depth fusion.
CurriFlow employs a multi-level fusion strategy to align segmentation, visual,
and depth features across frames using pre-trained optical flow, thereby
improving temporal consistency and dynamic object understanding. To enhance
geometric robustness, a curriculum learning mechanism progressively transitions
from sparse yet accurate LiDAR depth to dense but noisy stereo depth during
training, ensuring stable optimization and seamless adaptation to real-world
deployment. Furthermore, semantic priors from the Segment Anything Model (SAM)
provide category-agnostic supervision, strengthening voxel-level semantic
learning and spatial consistency. Experiments on the SemanticKITTI benchmark
demonstrate that CurriFlow achieves state-of-the-art performance with a mean
IoU of 16.9, validating the effectiveness of our motion-guided and
curriculum-aware design for camera-based 3D semantic scene completion.

</details>


### [157] [Deep Attention-guided Adaptive Subsampling](https://arxiv.org/abs/2510.12376)
*Sharath M Shankaranarayana,Soumava Kumar Roy,Prasad Sudhakar,Chandan Aladahalli*

Main category: cs.CV

TL;DR: The paper introduces a novel framework for adaptive subsampling in deep neural networks that dynamically adjusts input slices or frames during inference to reduce computational complexity and redundancy.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks improve tasks like video classification, but their computational cost is significant due to processing redundant information such as unnecessary slices or frames.

Method: An attention-guided sampling module that operates dynamically during inference, overcoming the non-differentiability of traditional subsampling techniques and adapting sampling mechanisms to the input.

Result: The method demonstrated performance gains and computational savings on 3D medical imaging datasets and real-world ultrasound video datasets, including a challenging clinical dataset.

Conclusion: Dynamic input-adaptive subsampling modules can enhance deep learning model efficiency and simplify computational demands while maintaining or improving task performance.

Abstract: Although deep neural networks have provided impressive gains in performance,
these improvements often come at the cost of increased computational complexity
and expense. In many cases, such as 3D volume or video classification tasks,
not all slices or frames are necessary due to inherent redundancies. To address
this issue, we propose a novel learnable subsampling framework that can be
integrated into any neural network architecture. Subsampling, being a
nondifferentiable operation, poses significant challenges for direct adaptation
into deep learning models. While some works, have proposed solutions using the
Gumbel-max trick to overcome the problem of non-differentiability, they fall
short in a crucial aspect: they are only task-adaptive and not inputadaptive.
Once the sampling mechanism is learned, it remains static and does not adjust
to different inputs, making it unsuitable for real-world applications. To this
end, we propose an attention-guided sampling module that adapts to inputs even
during inference. This dynamic adaptation results in performance gains and
reduces complexity in deep neural network models. We demonstrate the
effectiveness of our method on 3D medical imaging datasets from MedMNIST3D as
well as two ultrasound video datasets for classification tasks, one of them
being a challenging in-house dataset collected under real-world clinical
conditions.

</details>


### [158] [Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling](https://arxiv.org/abs/2510.12385)
*Tim J. Schoonbeek,Shao-Hsuan Hung,Dan Lehman,Hans Onvlee,Jacek Kustra,Peter H. N. de With,Fons van der Sommen*

Main category: cs.CV

TL;DR: The paper introduces STORM-PSR, a dual-stream model for recognizing procedural steps in videos by leveraging both spatial and temporal features to improve resilience against occlusions.


<details>
  <summary>Details</summary>
Motivation: Existing models for procedure step recognition rely only on spatial features and fail when objects are partially occluded, reducing accuracy and robustness.

Method: STORM-PSR consists of a dual-stream design with an assembly state detection stream for unobstructed views and a spatio-temporal stream with a pre-trained spatial encoder and a transformer-based temporal encoder.

Result: Compared to prior methods, STORM-PSR reduces the delay between actual and predicted assembly steps by 11.2% on MECCANO and 26.1% on IndustReal datasets.

Conclusion: STORM-PSR effectively improves recognition robustness even under object occlusion and makes its model and dataset contribution publicly available.

Abstract: Procedure step recognition (PSR) aims to identify all correctly completed
steps and their sequential order in videos of procedural tasks. The existing
state-of-the-art models rely solely on detecting assembly object states in
individual video frames. By neglecting temporal features, model robustness and
accuracy are limited, especially when objects are partially occluded. To
overcome these limitations, we propose Spatio-Temporal Occlusion-Resilient
Modeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework
for PSR that leverages both spatial and temporal features. The assembly state
detection stream operates effectively with unobstructed views of the object,
while the spatio-temporal stream captures both spatial and temporal features to
recognize step completions even under partial occlusion. This stream includes a
spatial encoder, pre-trained using a novel weakly supervised approach to
capture meaningful spatial representations, and a transformer-based temporal
encoder that learns how these spatial features relate over time. STORM-PSR is
evaluated on the MECCANO and IndustReal datasets, reducing the average delay
between actual and predicted assembly step completions by 11.2% and 26.1%,
respectively, compared to prior methods. We demonstrate that this reduction in
delay is driven by the spatio-temporal stream, which does not rely on
unobstructed views of the object to infer completed steps. The code for
STORM-PSR, along with the newly annotated MECCANO labels, is made publicly
available at https://timschoonbeek.github.io/stormpsr .

</details>


### [159] [Scene Coordinate Reconstruction Priors](https://arxiv.org/abs/2510.12387)
*Wenjing Bian,Axel Barroso-Laguna,Tommaso Cavallari,Victor Adrian Prisacariu,Eric Brachmann*

Main category: cs.CV

TL;DR: The paper enhances scene coordinate regression (SCR) models using probabilistic interpretations and high-level reconstruction priors, achieving better 3D scene representations for tasks like camera relocalization and novel view synthesis.


<details>
  <summary>Details</summary>
Motivation: Scene coordinate regression (SCR) models are powerful yet tend to degrade when facing insufficient multi-view constraints, necessitating improved mechanisms for robust 3D vision.

Method: The authors propose a probabilistic reinterpretation of SCR models infused with high-level priors, including those learned from 3D point cloud diffusion models trained on extensive indoor scans.

Result: The approach produces more coherent 3D scene point clouds, improves registration rates and camera pose estimation, positively impacting tasks like novel view synthesis and relocalization.

Conclusion: Introducing reconstruction priors into SCR training improves scene representational quality and benefits downstream 3D vision tasks, demonstrating its efficacy on multiple indoor datasets.

Abstract: Scene coordinate regression (SCR) models have proven to be powerful implicit
scene representations for 3D vision, enabling visual relocalization and
structure-from-motion. SCR models are trained specifically for one scene. If
training images imply insufficient multi-view constraints SCR models
degenerate. We present a probabilistic reinterpretation of training SCR models,
which allows us to infuse high-level reconstruction priors. We investigate
multiple such priors, ranging from simple priors over the distribution of
reconstructed depth values to learned priors over plausible scene coordinate
configurations. For the latter, we train a 3D point cloud diffusion model on a
large corpus of indoor scans. Our priors push predicted 3D scene points towards
plausible geometry at each training step to increase their likelihood. On three
indoor datasets our priors help learning better scene representations,
resulting in more coherent scene point clouds, higher registration rates and
better camera poses, with a positive effect on down-stream tasks such as novel
view synthesis and camera relocalization.

</details>


### [160] [Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda](https://arxiv.org/abs/2510.12400)
*André Torneiro,Diogo Monteiro,Paulo Novais,Pedro Rangel Henriques,Nuno F. Rodrigues*

Main category: cs.CV

TL;DR: This paper systematically reviews the role of Vision-Language Models (VLMs) in urban infrastructure monitoring, exploring applications, architectures, datasets, and evaluation methods in the field.


<details>
  <summary>Details</summary>
Motivation: Urban infrastructure monitoring faces challenges due to object diversity, environmental factors, and reliance on costly sensor-based and manual methods. The study seeks to explore if VLMs, with their ability to integrate visual and language understanding, can address these issues effectively.

Method: The researchers analyzed 32 peer-reviewed studies from 2021 to 2025, following the PRISMA methodology. Key areas explored include urban monitoring tasks addressed by VLMs, commonly used architectures, supporting datasets, and evaluation methods.

Result: The paper identifies the effectiveness of VLMs in various urban monitoring tasks, highlights specific VLM architectures and frameworks demonstrating superior performance, and reviews datasets and evaluation metrics applied in the field.

Conclusion: Vision-Language Models hold significant promise for urban infrastructure monitoring by providing scalable and perceptually aligned approaches. The study consolidates key findings and establishes groundwork for future research in this domain.

Abstract: Urban monitoring of public infrastructure (such as waste bins, road signs,
vegetation, sidewalks, and construction sites) poses significant challenges due
to the diversity of objects, environments, and contextual conditions involved.
Current state-of-the-art approaches typically rely on a combination of IoT
sensors and manual inspections, which are costly, difficult to scale, and often
misaligned with citizens' perception formed through direct visual observation.
This raises a critical question: Can machines now "see" like citizens and infer
informed opinions about the condition of urban infrastructure? Vision-Language
Models (VLMs), which integrate visual understanding with natural language
reasoning, have recently demonstrated impressive capabilities in processing
complex visual information, turning them into a promising technology to address
this challenge. This systematic review investigates the role of VLMs in urban
monitoring, with particular emphasis on zero-shot applications. Following the
PRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021
and 2025 to address four core research questions: (1) What urban monitoring
tasks have been effectively addressed using VLMs? (2) Which VLM architectures
and frameworks are most commonly used and demonstrate superior performance? (3)
What datasets and resources support this emerging field? (4) How are VLM-based
applications evaluated, and what performance levels have been reported?

</details>


### [161] [Low-Field Magnetic Resonance Image Quality Enhancement using a Conditional Flow Matching Model](https://arxiv.org/abs/2510.12408)
*Huu Tien Nguyen,Ahmed Karam Eldaly*

Main category: cs.CV

TL;DR: The paper presents a new image quality transfer method for reconstructing high-quality images using conditional flow matching (CFM), targeted at improving low-field MRI outputs.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address the inherent limitations of low-field MRI's reduced signal-to-noise ratio and diagnostic quality, providing cost-effective solutions without expensive infrastructure.

Method: The paper employs conditional flow matching (CFM), which learns a continuous flow between noise and target distributions through optimal velocity field regression, avoiding iterative sampling or adversarial objectives.

Result: The framework achieves state-of-the-art results in robust generalization to both in-distribution and out-of-distribution data, using fewer parameters than other deep learning models.

Conclusion: CFM is a scalable and efficient tool for MRI image quality enhancement, with significant implications for resource-limited clinical setups.

Abstract: This paper introduces a novel framework for image quality transfer based on
conditional flow matching (CFM). Unlike conventional generative models that
rely on iterative sampling or adversarial objectives, CFM learns a continuous
flow between a noise distribution and target data distributions through the
direct regression of an optimal velocity field. We evaluate this approach in
the context of low-field magnetic resonance imaging (LF-MRI), a rapidly
emerging modality that offers affordable and portable scanning but suffers from
inherently low signal-to-noise ratio and reduced diagnostic quality. Our
framework is designed to reconstruct high-field-like MR images from their
corresponding low-field inputs, thereby bridging the quality gap without
requiring expensive infrastructure. Experiments demonstrate that CFM not only
achieves state-of-the-art performance, but also generalizes robustly to both
in-distribution and out-of-distribution data. Importantly, it does so while
utilizing significantly fewer parameters than competing deep learning methods.
These results underline the potential of CFM as a powerful and scalable tool
for MRI reconstruction, particularly in resource-limited clinical environments.

</details>


### [162] [VideoLucy: Deep Memory Backtracking for Long Video Understanding](https://arxiv.org/abs/2510.12422)
*Jialong Zuo,Yongtai Deng,Lingdong Kong,Jingkang Yang,Rui Jin,Yiwei Zhang,Nong Sang,Liang Pan,Ziwei Liu,Changxin Gao*

Main category: cs.CV

TL;DR: This paper introduces VideoLucy, a framework for improving long video understanding by leveraging hierarchical memory structures and iterative backtracking. It addresses challenges in temporal context modeling and sparse frame sampling, achieving superior results compared to other methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in current agent-based systems that struggle with understanding the temporal context of videos and risks of discarding crucial information by sparse frame sampling.

Method: VideoLucy employs a hierarchical memory structure with progressive granularity and an agent-based iterative backtracking mechanism to mine question-relevant information from videos.

Result: VideoLucy demonstrates superior performance compared to state-of-the-art models, outperforming even proprietary systems like GPT-4 on multiple long video understanding benchmarks.

Conclusion: The proposed framework effectively enhances long video understanding, and its open-source code and dataset promise to benefit future research in this area.

Abstract: Recent studies have shown that agent-based systems leveraging large language
models (LLMs) for key information retrieval and integration have emerged as a
promising approach for long video understanding. However, these systems face
two major challenges. First, they typically perform modeling and reasoning on
individual frames, struggling to capture the temporal context of consecutive
frames. Second, to reduce the cost of dense frame-level captioning, they adopt
sparse frame sampling, which risks discarding crucial information. To overcome
these limitations, we propose VideoLucy, a deep memory backtracking framework
for long video understanding. Inspired by the human recollection process from
coarse to fine, VideoLucy employs a hierarchical memory structure with
progressive granularity. This structure explicitly defines the detail level and
temporal scope of memory at different hierarchical depths. Through an
agent-based iterative backtracking mechanism, VideoLucy systematically mines
video-wide, question-relevant deep memories until sufficient information is
gathered to provide a confident answer. This design enables effective temporal
understanding of consecutive frames while preserving critical details. In
addition, we introduce EgoMem, a new benchmark for long video understanding.
EgoMem is designed to comprehensively evaluate a model's ability to understand
complex events that unfold over time and capture fine-grained details in
extremely long videos. Extensive experiments demonstrate the superiority of
VideoLucy. Built on open-source models, VideoLucy significantly outperforms
state-of-the-art methods on multiple long video understanding benchmarks,
achieving performance even surpassing the latest proprietary models such as
GPT-4o. Our code and dataset will be made publicly at
https://videolucy.github.io

</details>


### [163] [A Review of Longitudinal Radiology Report Generation: Dataset Composition, Methods, and Performance Evaluation](https://arxiv.org/abs/2510.12444)
*Shaoyang Zhou,Yingshu Li,Yunyi Liu,Lingqiao Liu,Lei Wang,Luping Zhou*

Main category: cs.CV

TL;DR: This survey reviews advancements in longitudinal radiology report generation, emphasizing the integration of historical Chest X-ray studies to improve model performance.


<details>
  <summary>Details</summary>
Motivation: Radiologists face substantial workloads from high utilization of Chest X-ray imaging. There is a need for automated systems that can generate clinically accurate radiology reports to streamline diagnostic workflows.

Method: The paper surveys dataset creation strategies, longitudinally tailored model architectures, evaluation protocols, and analyzes the role of longitudinal information and architectural design in enhancing performance.

Result: The survey identifies key methods and their performance, highlights critical architectural designs, and conducts ablation studies to show the importance of longitudinal data.

Conclusion: Five core limitations are discussed in current LRRG research, with suggestions for future directions to advance this field and establish systematic frameworks for model design.

Abstract: Chest Xray imaging is a widely used diagnostic tool in modern medicine, and
its high utilization creates substantial workloads for radiologists. To
alleviate this burden, vision language models are increasingly applied to
automate Chest Xray radiology report generation (CXRRRG), aiming for clinically
accurate descriptions while reducing manual effort. Conventional approaches,
however, typically rely on single images, failing to capture the longitudinal
context necessary for producing clinically faithful comparison statements.
Recently, growing attention has been directed toward incorporating longitudinal
data into CXR RRG, enabling models to leverage historical studies in ways that
mirror radiologists diagnostic workflows. Nevertheless, existing surveys
primarily address single image CXRRRG and offer limited guidance for
longitudinal settings, leaving researchers without a systematic framework for
model design. To address this gap, this survey provides the first comprehensive
review of longitudinal radiology report generation (LRRG). Specifically, we
examine dataset construction strategies, report generation architectures
alongside longitudinally tailored designs, and evaluation protocols
encompassing both longitudinal specific measures and widely used benchmarks. We
further summarize LRRG methods performance, alongside analyses of different
ablation studies, which collectively highlight the critical role of
longitudinal information and architectural design choices in improving model
performance. Finally, we summarize five major limitations of current research
and outline promising directions for future development, aiming to lay a
foundation for advancing this emerging field.

</details>


### [164] [MS-GAGA: Metric-Selective Guided Adversarial Generation Attack](https://arxiv.org/abs/2510.12468)
*Dion J. X. Ho,Gabriel Lee Jun Rong,Niharika Shrivastava,Harshavardhan Abichandani,Pai Chet Ng,Xiaoxiao Miao*

Main category: cs.CV

TL;DR: The paper introduces MS-GAGA, a two-stage framework for creating transferable and visually imperceptible adversarial examples to fool deepfake detectors in black-box settings, achieving superior performance against unseen detectors.


<details>
  <summary>Details</summary>
Motivation: Deepfake detectors are increasing in usage; however, their robustness needs scrutiny, especially in black-box settings. The paper aims to craft adversarial examples that can evade these detectors while remaining visually imperceptible.

Method: MS-GAGA uses a two-stage approach. Stage 1 deploys a dual-stream attack module combining MNTD-PGD for small perturbation budgets and SG-PGD for visually salient region perturbations. Stage 2 incorporates a metric-aware selection module to optimize experiments for success against black-box models and structural similarity.

Result: The framework demonstrated up to 27% higher misclassification rates on unseen deepfake detectors than existing state-of-the-art attacks, highlighting its efficacy.

Conclusion: MS-GAGA improves transferable and imperceptible adversarial attacks, demonstrating potential in advancing the understanding of vulnerabilities in deepfake detection systems.

Abstract: We present MS-GAGA (Metric-Selective Guided Adversarial Generation Attack), a
two-stage framework for crafting transferable and visually imperceptible
adversarial examples against deepfake detectors in black-box settings. In Stage
1, a dual-stream attack module generates adversarial candidates: MNTD-PGD
applies enhanced gradient calculations optimized for small perturbation
budgets, while SG-PGD focuses perturbations on visually salient regions. This
complementary design expands the adversarial search space and improves
transferability across unseen models. In Stage 2, a metric-aware selection
module evaluates candidates based on both their success against black-box
models and their structural similarity (SSIM) to the original image. By jointly
optimizing transferability and imperceptibility, MS-GAGA achieves up to 27%
higher misclassification rates on unseen detectors compared to state-of-the-art
attacks.

</details>


### [165] [A Text-Image Fusion Method with Data Augmentation Capabilities for Referring Medical Image Segmentation](https://arxiv.org/abs/2510.12482)
*Shurong Chai,Rahul Kumar JAIN,Rui Xu,Shaocong Mo,Ruibo Hou,Shiyu Teng,Jiaqing Liu,Lanfen Lin,Yen-Wei Chen*

Main category: cs.CV

TL;DR: This paper proposes a data augmentation method for text-guided medical image segmentation that preserves spatial consistency between image and text.


<details>
  <summary>Details</summary>
Motivation: Medical imaging often suffers from limited datasets, requiring augmentation techniques to improve segmentation outcomes.

Method: The authors introduce an early fusion framework for combining text and visual features before augmentation, along with a lightweight generator for projecting text embeddings into visual space.

Result: The proposed method demonstrates state-of-the-art results across three medical imaging tasks and four segmentation frameworks.

Conclusion: Early fusion and spatially consistent augmentation improve segmentation performance in multimodal medical imaging tasks.

Abstract: Deep learning relies heavily on data augmentation to mitigate limited data,
especially in medical imaging. Recent multimodal learning integrates text and
images for segmentation, known as referring or text-guided image segmentation.
However, common augmentations like rotation and flipping disrupt spatial
alignment between image and text, weakening performance. To address this, we
propose an early fusion framework that combines text and visual features before
augmentation, preserving spatial consistency. We also design a lightweight
generator that projects text embeddings into visual space, bridging semantic
gaps. Visualization of generated pseudo-images shows accurate region
localization. Our method is evaluated on three medical imaging tasks and four
segmentation frameworks, achieving state-of-the-art results. Code is publicly
available on GitHub: https://github.com/11yxk/MedSeg_EarlyFusion.

</details>


### [166] [BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring](https://arxiv.org/abs/2510.12493)
*An Zhao,Piaopiao Yu,Zhe Zhu,Mingqiang Wei*

Main category: cs.CV

TL;DR: The paper proposes Bi-Stage 3D Gaussian Splatting (BSGS), a new framework for accurately reconstructing 3D scenes from motion-blurred images.


<details>
  <summary>Details</summary>
Motivation: Reconstructing high-quality 3D scenes from motion-blurred images caused by camera movement poses challenges due to limitations in existing methods, such as reliance on precise camera poses and issues with Gaussian primitive errors.

Method: BSGS employs two stages: Camera Pose Refinement to reduce distortions, and Global Rigid Transformation for further correction. It incorporates subframe gradient aggregation and a bi-stage space-time optimization strategy to improve performance.

Result: Experiments demonstrate that BSGS effectively deblurs motion-blurred images and significantly outperforms existing state-of-the-art methods.

Conclusion: The proposed BSGS framework addresses key limitations in previous methods, offering a robust solution for 3D scene reconstruction from motion-blurred images.

Abstract: 3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene
reconstruction.However, reconstructing high-quality 3D scenes from
motion-blurred images caused by camera motion poses a significant challenge.The
performance of existing 3DGS-based deblurring methods are limited due to their
inherent mechanisms, such as extreme dependence on the accuracy of camera poses
and inability to effectively control erroneous Gaussian primitives
densification caused by motion blur.To solve these problems, we introduce a
novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D
scenes from motion-blurred images.BSGS contains two stages. First, Camera Pose
Refinement roughly optimizes camera poses to reduce motion-induced distortions.
Second, with fixed rough camera poses, Global RigidTransformation further
corrects motion-induced blur distortions.To alleviate multi-subframe gradient
conflicts, we propose a subframe gradient aggregation strategy to optimize both
stages.Furthermore, a space-time bi-stage optimization strategy is introduced
to dynamically adjust primitive densification thresholds and prevent premature
noisy Gaussian generation in blurred regions. Comprehensive experiments verify
the effectiveness of our proposed deblurring method and show its superiority
over the state of the arts.

</details>


### [167] [Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from Unoriented Points](https://arxiv.org/abs/2510.12524)
*Jiayi Kong,Chen Zong,Junkai Deng,Xuhui Chen,Fei Hou,Shiqing Xin,Junhui Hou,Chen Qian,Ying He*

Main category: cs.CV

TL;DR: This paper introduces Voronoi-Assisted Diffusion (VAD), a network-free method for efficiently computing unsigned distance fields (UDFs) from unoriented point clouds.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing neural methods for learning UDFs, such as instability, high computational cost, and lack of control.

Method: The method involves assigning bi-directional normals to points using Voronoi-based criteria, diffusing these normals to approximate a UDF gradient field, and then integrating it to obtain the final UDF.

Result: VAD successfully handles diverse geometries, including watertight, open, non-manifold, and non-orientable structures, while being efficient and stable.

Conclusion: The proposed VAD method offers a robust and computationally efficient alternative for UDF computation without relying on neural networks.

Abstract: Unsigned Distance Fields (UDFs) provide a flexible representation for 3D
shapes with arbitrary topology, including open and closed surfaces, orientable
and non-orientable geometries, and non-manifold structures. While recent neural
approaches have shown promise in learning UDFs, they often suffer from
numerical instability, high computational cost, and limited controllability. We
present a lightweight, network-free method, Voronoi-Assisted Diffusion (VAD),
for computing UDFs directly from unoriented point clouds. Our approach begins
by assigning bi-directional normals to input points, guided by two
Voronoi-based geometric criteria encoded in an energy function for optimal
alignment. The aligned normals are then diffused to form an approximate UDF
gradient field, which is subsequently integrated to recover the final UDF.
Experiments demonstrate that VAD robustly handles watertight and open surfaces,
as well as complex non-manifold and non-orientable geometries, while remaining
computationally efficient and stable.

</details>


### [168] [Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion](https://arxiv.org/abs/2510.12537)
*David Björkstrand,Tiesheng Wang,Lars Bretzner,Josephine Sullivan*

Main category: cs.CV

TL;DR: This paper proposes a streamlined approach using score-based diffusion models for human motion generation, relying on minimal input feature engineering and introducing precise feature-normalization and weighting strategies.


<details>
  <summary>Details</summary>
Motivation: Current models for human motion generation often depend on over-parameterized input features and auxiliary losses, which may be redundant in achieving accurate motion modeling with diffusion models.

Method: The study uses a score-based diffusion model with feature-space normalization and analytically derived weightings for L2 score-matching loss, eliminating the need for additional postprocessing steps like recovering shape from joints.

Result: State-of-the-art results in unconditional human motion generation are achieved, showcasing comparability with more complex methods while being more efficient.

Conclusion: Carefully crafted normalization and loss weighting within diffusion models can simplify and streamline human motion generation while maintaining high performance, as validated through targeted ablation studies.

Abstract: Recent work has explored a range of model families for human motion
generation, including Variational Autoencoders (VAEs), Generative Adversarial
Networks (GANs), and diffusion-based models. Despite their differences, many
methods rely on over-parameterized input features and auxiliary losses to
improve empirical results. These strategies should not be strictly necessary
for diffusion models to match the human motion distribution. We show that on
par with state-of-the-art results in unconditional human motion generation are
achievable with a score-based diffusion model using only careful feature-space
normalization and analytically derived weightings for the standard L2
score-matching loss, while generating both motion and shape directly, thereby
avoiding slow post hoc shape recovery from joints. We build the method step by
step, with a clear theoretical motivation for each component, and provide
targeted ablations demonstrating the effectiveness of each proposed addition in
isolation.

</details>


### [169] [MMOT: The First Challenging Benchmark for Drone-based Multispectral Multi-Object Tracking](https://arxiv.org/abs/2510.12565)
*Tianhao Li,Tingfa Xu,Ying Wang,Haolin Qin,Xu Lin,Jianan Li*

Main category: cs.CV

TL;DR: The paper addresses the challenges in drone-based multi-object tracking (MOT) and introduces MMOT, a benchmark dataset for multispectral tracking, while presenting new methods that enhance tracking performance under difficult conditions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of RGB-based tracking methods in aerial views, especially in scenarios involving small targets, occlusions, and cluttered environments, by utilizing multispectral data.

Method: The study introduces MMOT, a multispectral UAV dataset and proposes techniques like Spectral 3D-Stem, orientation-aware Kalman filter, and an orientation-adaptive transformer to improve multispectral tracking.

Result: Experiments showed that multispectral input significantly enhances tracking performance compared to RGB-based methods, particularly in challenging scenarios like tracking small, densely packed objects.

Conclusion: The research advances drone-based multispectral MOT by providing a benchmark dataset, new methods, and insights to address tracking challenges, promising improvements in this field.

Abstract: Drone-based multi-object tracking is essential yet highly challenging due to
small targets, severe occlusions, and cluttered backgrounds. Existing RGB-based
tracking algorithms heavily depend on spatial appearance cues such as color and
texture, which often degrade in aerial views, compromising reliability.
Multispectral imagery, capturing pixel-level spectral reflectance, provides
crucial cues that enhance object discriminability under degraded spatial
conditions. However, the lack of dedicated multispectral UAV datasets has
hindered progress in this domain. To bridge this gap, we introduce MMOT, the
first challenging benchmark for drone-based multispectral multi-object
tracking. It features three key characteristics: (i) Large Scale - 125 video
sequences with over 488.8K annotations across eight categories; (ii)
Comprehensive Challenges - covering diverse conditions such as extreme small
targets, high-density scenarios, severe occlusions, and complex motion; and
(iii) Precise Oriented Annotations - enabling accurate localization and reduced
ambiguity under aerial perspectives. To better extract spectral features and
leverage oriented annotations, we further present a multispectral and
orientation-aware MOT scheme adapting existing methods, featuring: (i) a
lightweight Spectral 3D-Stem integrating spectral features while preserving
compatibility with RGB pretraining; (ii) an orientation-aware Kalman filter for
precise state estimation; and (iii) an end-to-end orientation-adaptive
transformer. Extensive experiments across representative trackers consistently
show that multispectral input markedly improves tracking performance over RGB
baselines, particularly for small and densely packed objects. We believe our
work will advance drone-based multispectral multi-object tracking research. Our
MMOT, code, and benchmarks are publicly available at
https://github.com/Annzstbl/MMOT.

</details>


### [170] [Learning Human Motion with Temporally Conditional Mamba](https://arxiv.org/abs/2510.12573)
*Quang Nguyen,Tri Le,Baoru Huang,Minh Nhat Vu,Ngan Le,Thieu Vo,Anh Nguyen*

Main category: cs.CV

TL;DR: This paper introduces Temporally Conditional Mamba, a new model for human motion generation addressing limitations in temporal alignment using an improved fusion approach.


<details>
  <summary>Details</summary>
Motivation: The need to reliably generate or estimate human motion aligned with time-dependent inputs due to challenges in maintaining step-by-step temporal alignment with existing methods.

Method: Developed a Mamba-based model that integrates conditional inputs into recurrent dynamics to improve temporally aligned human motion generation.

Result: Experimental validation shows significant improvements in temporal alignment, motion realism, and consistency compared to state-of-the-art methods.

Conclusion: The proposed model demonstrates enhanced motion generation capabilities, showcasing its effectiveness in addressing temporal conditioning challenges.

Abstract: Learning human motion based on a time-dependent input signal presents a
challenging yet impactful task with various applications. The goal of this task
is to generate or estimate human movement that consistently reflects the
temporal patterns of conditioning inputs. Existing methods typically rely on
cross-attention mechanisms to fuse the condition with motion. However, this
approach primarily captures global interactions and struggles to maintain
step-by-step temporal alignment. To address this limitation, we introduce
Temporally Conditional Mamba, a new mamba-based model for human motion
generation. Our approach integrates conditional information into the recurrent
dynamics of the Mamba block, enabling better temporally aligned motion. To
validate the effectiveness of our method, we evaluate it on a variety of human
motion tasks. Extensive experiments demonstrate that our model significantly
improves temporal alignment, motion realism, and condition consistency over
state-of-the-art approaches. Our project page is available at
https://zquang2202.github.io/TCM.

</details>


### [171] [Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence](https://arxiv.org/abs/2510.12579)
*Simon Ravé,Jean-Christophe Lombardo,Pejman Rasti,Alexis Joly,David Rousseau*

Main category: cs.CV

TL;DR: A zero-shot segmentation method combines Plantnet, DinoV2, and SAM to segment plants in agricultural imagery, achieving better results without requiring new annotated datasets.


<details>
  <summary>Details</summary>
Motivation: To alleviate the annotation bottleneck in agricultural segmentation tasks and enable better performance under challenging conditions, such as limited training data and complex field scenarios.

Method: The method leverages Plantnet's plant-centric representations to generate coarse segmentation masks, refined by SAM for detailed segmentation. It employs Plantnet-fine-tuned DinoV2 for better performance.

Result: Consistent improvements in segmentation performance were achieved across four datasets, with Plantnet-fine-tuned DinoV2 outperforming the baseline DinoV2 model, evaluated using the Jaccard Index (IoU).

Conclusion: Combining foundation models like DinoV2 with domain-specific models like Plantnet enables effective zero-shot segmentation for agricultural use, reducing the need for new data annotations.

Abstract: We present a zero-shot segmentation approach for agricultural imagery that
leverages Plantnet, a large-scale plant classification model, in conjunction
with its DinoV2 backbone and the Segment Anything Model (SAM). Rather than
collecting and annotating new datasets, our method exploits Plantnet's
specialized plant representations to identify plant regions and produce coarse
segmentation masks. These masks are then refined by SAM to yield detailed
segmentations. We evaluate on four publicly available datasets of various
complexity in terms of contrast including some where the limited size of the
training data and complex field conditions often hinder purely supervised
methods. Our results show consistent performance gains when using
Plantnet-fine-tuned DinoV2 over the base DinoV2 model, as measured by the
Jaccard Index (IoU). These findings highlight the potential of combining
foundation models with specialized plant-centric models to alleviate the
annotation bottleneck and enable effective segmentation in diverse agricultural
scenarios.

</details>


### [172] [LayerSync: Self-aligning Intermediate Layers](https://arxiv.org/abs/2510.12581)
*Yasaman Haghighi,Bastien van Delft,Mariam Hassan,Alexandre Alahi*

Main category: cs.CV

TL;DR: LayerSync offers a domain-agnostic solution to enhance diffusion models’ training efficiency and output quality by leveraging their internal representations instead of external guidance or additional data.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies in training and generation quality of diffusion models by replacing external guidance with a method that utilizes intrinsic model representations.

Method: A regularization technique called LayerSync that uses the semantically rich intermediate representations of diffusion models as guidance for weaker layers, enabling efficient and effective training without external dependencies.

Result: LayerSync significantly speeds up training (e.g., 8.75x on ImageNet with flow-based transformer) and improves generation quality (e.g., by 23.6%). It is applicable across various domains including image, audio, video, and motion generation.

Conclusion: The proposed LayerSync is a plug-and-play, resource-efficient regularizer that enhances the performance and applicability of diffusion models without relying on pretrained models or additional data.

Abstract: We propose LayerSync, a domain-agnostic approach for improving the generation
quality and the training efficiency of diffusion models. Prior studies have
highlighted the connection between the quality of generation and the
representations learned by diffusion models, showing that external guidance on
model intermediate representations accelerates training. We reconceptualize
this paradigm by regularizing diffusion models with their own intermediate
representations. Building on the observation that representation quality varies
across diffusion model layers, we show that the most semantically rich
representations can act as an intrinsic guidance for weaker ones, reducing the
need for external supervision. Our approach, LayerSync, is a self-sufficient,
plug-and-play regularizer term with no overhead on diffusion model training and
generalizes beyond the visual domain to other modalities. LayerSync requires no
pretrained models nor additional data. We extensively evaluate the method on
image generation and demonstrate its applicability to other domains such as
audio, video, and motion generation. We show that it consistently improves the
generation quality and the training efficiency. For example, we speed up the
training of flow-based transformer by over 8.75x on ImageNet dataset and
improved the generation quality by 23.6%. The code is available at
https://github.com/vita-epfl/LayerSync.

</details>


### [173] [Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training](https://arxiv.org/abs/2510.12586)
*Jiachen Lei,Keli Liu,Julius Berner,Haiming Yu,Hongkai Zheng,Jiahong Wu,Xiangxiang Chu*

Main category: cs.CV

TL;DR: This paper introduces a two-stage training framework for pixel-space diffusion and consistency models, achieving superior image generation quality and efficiency on the ImageNet dataset.


<details>
  <summary>Details</summary>
Motivation: Pixel-space generative models underperform and are harder to train compared to latent-space models, motivating the need for improved methods.

Method: The two-stage framework first pre-trains encoders to capture semantics aligned with deterministic sampling trajectories, then integrates the encoder with a decoder for end-to-end model fine-tuning.

Result: On ImageNet, the diffusion model achieved remarkable FID scores (2.04 for 256px and 2.35 for 512px), while the consistency model achieved FID 8.82 in one sampling step, outperforming latent-space counterparts.

Conclusion: This framework closes the performance gap for pixel-space models, offering an efficient alternative rivaling latent-space methods and enabling direct training of consistency models for high-resolution images.

Abstract: Pixel-space generative models are often more difficult to train and generally
underperform compared to their latent-space counterparts, leaving a persistent
performance and efficiency gap. In this paper, we introduce a novel two-stage
training framework that closes this gap for pixel-space diffusion and
consistency models. In the first stage, we pre-train encoders to capture
meaningful semantics from clean images while aligning them with points along
the same deterministic sampling trajectory, which evolves points from the prior
to the data distribution. In the second stage, we integrate the encoder with a
randomly initialized decoder and fine-tune the complete model end-to-end for
both diffusion and consistency models. Our training framework demonstrates
strong empirical performance on ImageNet dataset. Specifically, our diffusion
model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75
number of function evaluations (NFE), surpassing prior pixel-space methods by a
large margin in both generation quality and efficiency while rivaling leading
VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our
consistency model achieves an impressive FID of 8.82 in a single sampling step,
significantly surpassing its latent-space counterpart. To the best of our
knowledge, this marks the first successful training of a consistency model
directly on high-resolution images without relying on pre-trained VAEs or
diffusion models.

</details>


### [174] [Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space](https://arxiv.org/abs/2510.12603)
*Chao Chen,Zhixin Ma,Yongqi Li,Yupeng Hu,Yinwei Wei,Wenjie Li,Liqiang Nie*

Main category: cs.CV

TL;DR: The paper introduces Interleaved Vision-Text Latent Reasoning (IVT-LR), a method for multimodal reasoning that improves accuracy by 5.45% and speeds up inference over 5 times compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Current multimodal reasoning methods rely on explicit reasoning steps with heavy annotations and slow inference speeds.

Method: IVT-LR combines latent text and vision information, using implicit reasoning steps in a latent space, trained with a progressive multi-stage strategy.

Result: IVT-LR achieves a 5.45% accuracy gain and a 5x speed improvement in experiments on M3CoT and ScienceQA.

Conclusion: The method enhances both efficiency and effectiveness of multimodal reasoning without requiring extensive annotations.

Abstract: Multimodal reasoning aims to enhance the capabilities of MLLMs by
incorporating intermediate reasoning steps before reaching the final answer. It
has evolved from text-only reasoning to the integration of visual information,
enabling the thought process to be conveyed through both images and text.
Despite its effectiveness, current multimodal reasoning methods depend on
explicit reasoning steps that require labor-intensive vision-text annotations
and inherently introduce significant inference latency. To address these
issues, we introduce multimodal latent reasoning with the advantages of
multimodal representation, reduced annotation, and inference efficiency. To
facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),
which injects both visual and textual information in the reasoning process
within the latent space. Specifically, IVT-LR represents each reasoning step by
combining two implicit parts: latent text (the hidden states from the previous
step) and latent vision (a set of selected image embeddings). We further
introduce a progressive multi-stage training strategy to enable MLLMs to
perform the above multimodal latent reasoning steps. Experiments on M3CoT and
ScienceQA demonstrate that our IVT-LR method achieves an average performance
increase of 5.45% in accuracy, while simultaneously achieving a speed increase
of over 5 times compared to existing approaches. Code available at
https://github.com/FYYDCC/IVT-LR.

</details>


### [175] [WaterFlow: Explicit Physics-Prior Rectified Flow for Underwater Saliency Mask Generation](https://arxiv.org/abs/2510.12605)
*Runting Li,Shijie Lian,Hua Li,Yutong Li,Wenhui Wu,Sam Kwong*

Main category: cs.CV

TL;DR: The paper presents WaterFlow, a framework for underwater salient object detection that utilizes underwater imaging physics and temporal modeling to improve performance.


<details>
  <summary>Details</summary>
Motivation: Current underwater salient object detection methods struggle with image degradation and fail to leverage the physical principles of underwater imaging.

Method: The authors introduce WaterFlow, which embeds underwater imaging physics as explicit priors and incorporates temporal modeling into the training process.

Result: WaterFlow achieves a 0.072 gain in S_m on the USOD10K dataset, illustrating its effectiveness.

Conclusion: WaterFlow demonstrates the potential of integrating physical priors and temporal modeling in overcoming challenges in underwater salient object detection.

Abstract: Underwater Salient Object Detection (USOD) faces significant challenges,
including underwater image quality degradation and domain gaps. Existing
methods tend to ignore the physical principles of underwater imaging or simply
treat degradation phenomena in underwater images as interference factors that
must be eliminated, failing to fully exploit the valuable information they
contain. We propose WaterFlow, a rectified flow-based framework for underwater
salient object detection that innovatively incorporates underwater physical
imaging information as explicit priors directly into the network training
process and introduces temporal dimension modeling, significantly enhancing the
model's capability for salient object identification. On the USOD10K dataset,
WaterFlow achieves a 0.072 gain in S_m, demonstrating the effectiveness and
superiority of our method. The code will be published after the acceptance.

</details>


### [176] [Zero-Shot CFC: Fast Real-World Image Denoising based on Cross-Frequency Consistency](https://arxiv.org/abs/2510.12646)
*Yanlin Jiang,Yuchen Liu,Mingren Liu*

Main category: cs.CV

TL;DR: The paper introduces ZSCFC, a method enabling efficient and effective zero-shot denoising of real-world noisy images without assumptions about the noise characteristics.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot denoising methods are computationally expensive, assume noise independence, and struggle with complex real-world noise.

Method: The proposed ZSCFC uses cross-frequency consistency loss and an ultralight network, exploiting texture characteristics across frequency bands to differentiate noise.

Result: ZSCFC outperforms state-of-the-art zero-shot denoisers in both speed and denoising quality, with successful results demonstrated across various real-world datasets.

Conclusion: ZSCFC is a highly efficient and effective method for denoising single noisy images with complex real-world noise, overcoming limitations of prior methods.

Abstract: Zero-shot denoisers address the dataset dependency of deep-learning-based
denoisers, enabling the denoising of unseen single images. Nonetheless,
existing zero-shot methods suffer from long training times and rely on the
assumption of noise independence and a zero-mean property, limiting their
effectiveness in real-world denoising scenarios where noise characteristics are
more complicated. This paper proposes an efficient and effective method for
real-world denoising, the Zero-Shot denoiser based on Cross-Frequency
Consistency (ZSCFC), which enables training and denoising with a single noisy
image and does not rely on assumptions about noise distribution. Specifically,
image textures exhibit position similarity and content consistency across
different frequency bands, while noise does not. Based on this property, we
developed cross-frequency consistency loss and an ultralight network to realize
image denoising. Experiments on various real-world image datasets demonstrate
that our ZSCFC outperforms other state-of-the-art zero-shot methods in terms of
computational efficiency and denoising performance.

</details>


### [177] [On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation](https://arxiv.org/abs/2510.12660)
*Shuhei Tarashima,Yushan Wang,Norio Tagawa*

Main category: cs.CV

TL;DR: The paper introduces efficient human mesh recovery (HMR) and pose estimation (HPE) models by using hierarchical vision foundation models (VFMs), achieving competitive performance with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: Develop efficient and lightweight models for human mesh recovery (HMR) and pose estimation tasks without the computational overhead of large vision transformers.

Method: Adapt lightweight variants of HMR2.0 by leveraging early stages of hierarchical VFMs like Swin Transformer, GroupMixFormer, and VMamba as model encoders.

Result: Evaluation of 27 models shows that truncated hierarchical-VFM designs match the performance of full-stage models while yielding better accuracy-efficiency trade-offs.

Conclusion: Hierarchical-VFM-based models can provide efficient solutions to HMR and HPE, enabling competitive results with fewer computational resources compared to traditional methods.

Abstract: In this work, we aim to develop simple and efficient models for human mesh
recovery (HMR) and its predecessor task, human pose estimation (HPE).
State-of-the-art HMR methods, such as HMR2.0 and its successors, rely on large,
non-hierarchical vision transformers as encoders, which are inherited from the
corresponding HPE models like ViTPose. To establish baselines across varying
computational budgets, we first construct three lightweight HMR2.0 variants by
adapting the corresponding ViTPose models. In addition, we propose leveraging
the early stages of hierarchical vision foundation models (VFMs), including
Swin Transformer, GroupMixFormer, and VMamba, as encoders. This design is
motivated by the observation that intermediate stages of hierarchical VFMs
produce feature maps with resolutions comparable to or higher than those of
non-hierarchical counterparts. We conduct a comprehensive evaluation of 27
hierarchical-VFM-based HMR and HPE models, demonstrating that using only the
first two or three stages achieves performance on par with full-stage models.
Moreover, we show that the resulting truncated models exhibit better trade-offs
between accuracy and computational efficiency compared to existing lightweight
alternatives.

</details>


### [178] [TerraCodec: Compressing Earth Observations](https://arxiv.org/abs/2510.12670)
*Julen Costa-Watanabe,Isabelle Wittmann,Benedikt Blumenstiel,Konrad Schindler*

Main category: cs.CV

TL;DR: TerraCodec (TEC) introduces tailored learned codecs for efficient Earth observation image compression, considering temporal redundancy and achieving higher compression rates. It also enables cloud inpainting surpassing existing methods.


<details>
  <summary>Details</summary>
Motivation: The massive streams of multispectral image data from Earth observation satellites present storage and transmission challenges, with existing codecs failing to capture temporal dependencies or radiometric evolution in largely static imagery.

Method: TerraCodec includes learned image-based variants for multispectral inputs and introduces a Temporal Transformer model (TEC-TT) utilizing dependencies across time, alongside Latent Repacking for flexible-rate functionality.

Result: TerraCodec achieves 3-10x better compression performance compared to classical codecs while maintaining image quality, and TEC-TT excels in cloud inpainting on the AllClear benchmark.

Conclusion: Learned compression algorithms, tailored for Earth observation data, prove effective, providing both improved compression and cloud inpainting capabilities, paving the way for advancements in EO data handling solutions.

Abstract: Earth observation (EO) satellites produce massive streams of multispectral
image time series, posing pressing challenges for storage and transmission.
Yet, learned EO compression remains fragmented, lacking publicly available
pretrained models and misaligned with advances in compression for natural
imagery. Image codecs overlook temporal redundancy, while video codecs rely on
motion priors that fail to capture the radiometric evolution of largely static
scenes. We introduce TerraCodec (TEC), a family of learned codecs tailored to
EO. TEC includes efficient image-based variants adapted to multispectral
inputs, as well as a Temporal Transformer model (TEC-TT) that leverages
dependencies across time. To overcome the fixed-rate setting of today's neural
codecs, we present Latent Repacking, a novel method for training flexible-rate
transformer models that operate on varying rate-distortion settings. Trained on
Sentinel-2 data, TerraCodec outperforms classical codecs, achieving 3-10x
stronger compression at equivalent image quality. Beyond compression, TEC-TT
enables zero-shot cloud inpainting, surpassing state-of-the-art methods on the
AllClear benchmark. Our results establish bespoke, learned compression
algorithms as a promising direction for Earth observation. Code and model
weights will be released under a permissive license.

</details>


### [179] [MCOP: Multi-UAV Collaborative Occupancy Prediction](https://arxiv.org/abs/2510.12679)
*Zefu Lin,Wenbo Chen,Xiaojuan Jin,Yuran Yang,Lue Fan,Yixin Zhang,Yufeng Zhang,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: The paper introduces a multi-UAV collaborative occupancy prediction framework to address limitations in existing BEV-based methods, achieving better accuracy and lower communication overhead.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiencies in current collaborative UAV perception systems, particularly limitations of bounding-box-based BEV approaches in capturing geometric and semantic information.

Method: The framework uses a Spatial-Aware Feature Encoder, Cross-Agent Feature Integration, an Altitude-Aware Feature Reduction, and a Dual-Mask Perceptual Guidance mechanism to enhance UAV collaboration and reduce communication.

Result: The method achieves state-of-the-art performance on extended datasets, improving accuracy and significantly reducing communication overhead.

Conclusion: The proposed framework demonstrates an effective solution to existing challenges, advancing collaborative UAV systems with superior accuracy and practicality.

Abstract: Unmanned Aerial Vehicle (UAV) swarm systems necessitate efficient
collaborative perception mechanisms for diverse operational scenarios. Current
Bird's Eye View (BEV)-based approaches exhibit two main limitations:
bounding-box representations fail to capture complete semantic and geometric
information of the scene, and their performance significantly degrades when
encountering undefined or occluded objects. To address these limitations, we
propose a novel multi-UAV collaborative occupancy prediction framework. Our
framework effectively preserves 3D spatial structures and semantics through
integrating a Spatial-Aware Feature Encoder and Cross-Agent Feature
Integration. To enhance efficiency, we further introduce Altitude-Aware Feature
Reduction to compactly represent scene information, along with a Dual-Mask
Perceptual Guidance mechanism to adaptively select features and reduce
communication overhead. Due to the absence of suitable benchmark datasets, we
extend three datasets for evaluation: two virtual datasets (Air-to-Pred-Occ and
UAV3D-Occ) and one real-world dataset (GauUScene-Occ). Experiments results
demonstrate that our method achieves state-of-the-art accuracy, significantly
outperforming existing collaborative methods while reducing communication
overhead to only a fraction of previous approaches.

</details>


### [180] [Hybrid Explanation-Guided Learning for Transformer-Based Chest X-Ray Diagnosis](https://arxiv.org/abs/2510.12704)
*Shelley Zixin Shu,Haozhe Luo,Alexander Poellinger,Mauricio Reyes*

Main category: cs.CV

TL;DR: A Hybrid Explanation-Guided Learning framework is introduced to improve attention alignment and generalization in medical imaging models, outperforming existing methods in chest X-ray classification.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models show great promise in medical imaging but suffer from biases and spurious correlations. Human-AI attention alignment can address these issues but often requires expensive manual supervision.

Method: The proposed framework, Hybrid Explanation-Guided Learning (H-EGL), combines self-supervised and human-guided constraints to enhance attention alignment. The self-supervised module focuses on class-distinctive attention without strict priors.

Result: The H-EGL framework demonstrated enhanced classification accuracy and generalization in chest X-ray analysis using Vision Transformer (ViT), outperforming existing methods. Additionally, it improved the alignment of attention maps with human expertise.

Conclusion: H-EGL effectively improves biases and generalization issues in medical imaging, offering both robust performance and better interpretability through human-aligned attention maps.

Abstract: Transformer-based deep learning models have demonstrated exceptional
performance in medical imaging by leveraging attention mechanisms for feature
representation and interpretability. However, these models are prone to
learning spurious correlations, leading to biases and limited generalization.
While human-AI attention alignment can mitigate these issues, it often depends
on costly manual supervision. In this work, we propose a Hybrid
Explanation-Guided Learning (H-EGL) framework that combines self-supervised and
human-guided constraints to enhance attention alignment and improve
generalization. The self-supervised component of H-EGL leverages
class-distinctive attention without relying on restrictive priors, promoting
robustness and flexibility. We validate our approach on chest X-ray
classification using the Vision Transformer (ViT), where H-EGL outperforms two
state-of-the-art Explanation-Guided Learning (EGL) methods, demonstrating
superior classification accuracy and generalization capability. Additionally,
it produces attention maps that are better aligned with human expertise.

</details>


### [181] [Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning](https://arxiv.org/abs/2510.12712)
*Xingang Guo,Utkarsh Tyagi,Advait Gosai,Paula Vergara,Ernesto Gabriel Hernández Montoya,Chen Bo Calvin Zhang,Bin Hu,Yunzhong He,Bing Liu,Rakshith Sharma Srinivasa*

Main category: cs.CV

TL;DR: This paper introduces IRIS, a benchmark for testing Multimodal Large Language Models (MLLMs) on tasks requiring interactive image manipulation and reasoning.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for MLLMs consider static images, neglecting the need for active visual transformations and dynamic reasoning.

Method: The study develops IRIS, a benchmark with 1,204 vision-textual tasks across five domains, evaluating MLLMs on their ability to perceive, transform, and reason with images.

Result: Evaluation reveals that current MLLMs struggle with interactive tasks, with the best model achieving an 18.68% pass rate, and shows variation in tool-use effectiveness among models.

Conclusion: IRIS highlights the limitations of current MLLMs with interactive image tasks and provides a pathway for advancing their visual reasoning capabilities.

Abstract: Multimodal Large Language Models (MLLMs) are increasingly applied in
real-world scenarios where user-provided images are often imperfect, requiring
active image manipulations such as cropping, editing, or enhancement to uncover
salient visual cues. Beyond static visual perception, MLLMs must also think
with images: dynamically transforming visual content and integrating it with
other tools to solve complex tasks. However, this shift from treating vision as
passive context to a manipulable cognitive workspace remains underexplored.
Most existing benchmarks still follow a think about images paradigm, where
images are regarded as static inputs. To address this gap, we introduce IRIS,
an Interactive Reasoning with Images and Systems that evaluates MLLMs' ability
to perceive, transform, and reason across complex visual-textual tasks under
the think with images paradigm. IRIS comprises 1,204 challenging, open-ended
vision tasks (603 single-turn, 601 multi-turn) spanning across five diverse
domains, each paired with detailed rubrics to enable systematic evaluation. Our
evaluation shows that current MLLMs struggle with tasks requiring effective
integration of vision and general-purpose tools. Even the strongest model
(GPT-5-think) reaches only 18.68% pass rate. We further observe divergent
tool-use behaviors, with OpenAI models benefiting from diverse image
manipulations while Gemini-2.5-pro shows no improvement. By introducing the
first benchmark centered on think with images, IRIS offers critical insights
for advancing visual intelligence in MLLMs.

</details>


### [182] [FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution](https://arxiv.org/abs/2510.12747)
*Junhao Zhuang,Shi Guo,Xin Cai,Xiaohui Li,Yihao Liu,Chun Yuan,Tianfan Xue*

Main category: cs.CV

TL;DR: FlashVSR introduces an innovative diffusion-based framework for efficient and real-time video super-resolution (VSR), achieving significantly faster speeds and scalability compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in using diffusion models for real-world VSR, such as high latency, heavy computational demands, and poor scalability to ultra-high resolutions.

Method: FlashVSR combines a three-stage distillation pipeline, locality-constrained sparse attention for computation reduction, and a tiny conditional decoder to enhance performance for streaming VSR.

Result: FlashVSR achieves 17 FPS on 768x1408 videos using a single A100 GPU, scales effectively to ultra-high resolutions, and delivers up to 12x speedup over previous diffusion VSR models, validated by experiments with the VSR-120K dataset.

Conclusion: The proposed solution demonstrates state-of-the-art performance in practical VSR applications, with high efficiency and scalability, paving the way for future advancements in diffusion models.

Abstract: Diffusion models have recently advanced video restoration, but applying them
to real-world video super-resolution (VSR) remains challenging due to high
latency, prohibitive computation, and poor generalization to ultra-high
resolutions. Our goal in this work is to make diffusion-based VSR practical by
achieving efficiency, scalability, and real-time performance. To this end, we
propose FlashVSR, the first diffusion-based one-step streaming framework
towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408
videos on a single A100 GPU by combining three complementary innovations: (i) a
train-friendly three-stage distillation pipeline that enables streaming
super-resolution, (ii) locality-constrained sparse attention that cuts
redundant computation while bridging the train-test resolution gap, and (iii) a
tiny conditional decoder that accelerates reconstruction without sacrificing
quality. To support large-scale training, we also construct VSR-120K, a new
dataset with 120k videos and 180k images. Extensive experiments show that
FlashVSR scales reliably to ultra-high resolutions and achieves
state-of-the-art performance with up to 12x speedup over prior one-step
diffusion VSR models. We will release the code, pretrained models, and dataset
to foster future research in efficient diffusion-based VSR.

</details>


### [183] [SPORTS: Simultaneous Panoptic Odometry, Rendering, Tracking and Segmentation for Urban Scenes Understanding](https://arxiv.org/abs/2510.12749)
*Zhiliu Yang,Jinyu Dai,Jianyuan Zhang,Zhu Yang*

Main category: cs.CV

TL;DR: The paper introduces SPORTS, a framework integrating Video Panoptic Segmentation, Visual Odometry, and Scene Rendering for holistic scene understanding, overcoming common challenges in embodied AI.


<details>
  <summary>Details</summary>
Motivation: Current methods for scene perception and simulation in embodied AI suffer from issues like segmentation deficiency, dynamic object interference, sparse sensor data, and view limitations.

Method: The SPORTS framework combines adaptive attention-based geometric fusion in VPS, improved confidence estimation in VO using panoptic data, and point-based rendering in SR for high-fidelity scene synthesis.

Result: Experiments on public datasets show the proposed methods outperform state-of-the-art approaches in odometry, tracking, segmentation, and novel view synthesis.

Conclusion: SPORTS proves effective for improving scene understanding and simulation by integrating multiple tasks and addressing known limitations of current techniques.

Abstract: The scene perception, understanding, and simulation are fundamental
techniques for embodied-AI agents, while existing solutions are still prone to
segmentation deficiency, dynamic objects' interference, sensor data sparsity,
and view-limitation problems. This paper proposes a novel framework, named
SPORTS, for holistic scene understanding via tightly integrating Video Panoptic
Segmentation (VPS), Visual Odometry (VO), and Scene Rendering (SR) tasks into
an iterative and unified perspective. Firstly, VPS designs an adaptive
attention-based geometric fusion mechanism to align cross-frame features via
enrolling the pose, depth, and optical flow modality, which automatically
adjust feature maps for different decoding stages. And a post-matching strategy
is integrated to improve identities tracking. In VO, panoptic segmentation
results from VPS are combined with the optical flow map to improve the
confidence estimation of dynamic objects, which enhances the accuracy of the
camera pose estimation and completeness of the depth map generation via the
learning-based paradigm. Furthermore, the point-based rendering of SR is
beneficial from VO, transforming sparse point clouds into neural fields to
synthesize high-fidelity RGB views and twin panoptic views. Extensive
experiments on three public datasets demonstrate that our attention-based
feature fusion outperforms most existing state-of-the-art methods on the
odometry, tracking, segmentation, and novel view synthesis tasks.

</details>


### [184] [VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage](https://arxiv.org/abs/2510.12750)
*A. Alfarano,L. Venturoli,D. Negueruela del Castillo*

Main category: cs.CV

TL;DR: The paper introduces VQArt-Bench, a new Visual Question Answering benchmark focusing on deep semantic understanding in the cultural heritage domain, revealing significant limitations in current MLLMs.


<details>
  <summary>Details</summary>
Motivation: Existing VQA benchmarks lack the depth to evaluate models' semantic understanding, especially in complex domains like visual art, incentivizing models to rely on shortcuts rather than true reasoning.

Method: The authors developed VQArt-Bench, a large-scale VQA benchmark using a multi-agent pipeline to create diverse and validated questions that evaluate models across dimensions like symbolic meaning, narratives, and visual relationships.

Result: Testing 14 advanced MLLMs with VQArt-Bench revealed weaknesses, including failures in simple counting tasks and a performance gap between proprietary and open-source models.

Conclusion: Current MLLMs exhibit notable limitations in visual reasoning and semantic understanding; VQArt-Bench provides a robust tool for assessing progress in these areas.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated significant
capabilities in joint visual and linguistic tasks. However, existing Visual
Question Answering (VQA) benchmarks often fail to evaluate deep semantic
understanding, particularly in complex domains like visual art analysis.
Confined to simple syntactic structures and surface-level attributes, these
questions fail to capture the diversity and depth of human visual inquiry. This
limitation incentivizes models to exploit statistical shortcuts rather than
engage in visual reasoning. To address this gap, we introduce VQArt-Bench, a
new, large-scale VQA benchmark for the cultural heritage domain. This benchmark
is constructed using a novel multi-agent pipeline where specialized agents
collaborate to generate nuanced, validated, and linguistically diverse
questions. The resulting benchmark is structured along relevant visual
understanding dimensions that probe a model's ability to interpret symbolic
meaning, narratives, and complex visual relationships. Our evaluation of 14
state-of-the-art MLLMs on this benchmark reveals significant limitations in
current models, including a surprising weakness in simple counting tasks and a
clear performance gap between proprietary and open-source models.

</details>


### [185] [E-MoFlow: Learning Egomotion and Optical Flow from Event Data via Implicit Regularization](https://arxiv.org/abs/2510.12753)
*Wenpu Li,Bangyan Liao,Yi Zhou,Qi Xu,Pian Wan,Peidong Liu*

Main category: cs.CV

TL;DR: The paper introduces E-MoFlow, an unsupervised framework for jointly optimizing optical flow and 6-DoF egomotion using implicit spatial-temporal and geometric regularization, offering improved accuracy in neuromorphic vision tasks without supervision.


<details>
  <summary>Details</summary>
Motivation: The independent estimation of optical flow and 6-DoF egomotion in neuromorphic vision systems is ill-posed due to a lack of robust data association and ground-truth supervision. Existing solutions introduce bias or face optimization challenges.

Method: The authors propose E-MoFlow, which models egomotion as a continuous spline and optical flow as an implicit neural representation to enforce spatial-temporal coherence. It incorporates differential geometric constraints for structure-and-motion priors without explicit depth estimation.

Result: E-MoFlow demonstrates state-of-the-art performance for unsupervised optical flow and egomotion estimation tasks, showing robustness in 6-DoF motion scenarios and competitiveness with supervised methods.

Conclusion: The framework effectively addresses limitations in neuromorphic vision tasks by jointly optimizing optical flow and egomotion under an unsupervised paradigm, providing a unified and geometrically consistent approach.

Abstract: The estimation of optical flow and 6-DoF ego-motion, two fundamental tasks in
3D vision, has typically been addressed independently. For neuromorphic vision
(e.g., event cameras), however, the lack of robust data association makes
solving the two problems separately an ill-posed challenge, especially in the
absence of supervision via ground truth. Existing works mitigate this
ill-posedness by either enforcing the smoothness of the flow field via an
explicit variational regularizer or leveraging explicit structure-and-motion
priors in the parametrization to improve event alignment. The former notably
introduces bias in results and computational overhead, while the latter, which
parametrizes the optical flow in terms of the scene depth and the camera
motion, often converges to suboptimal local minima. To address these issues, we
propose an unsupervised framework that jointly optimizes egomotion and optical
flow via implicit spatial-temporal and geometric regularization. First, by
modeling camera's egomotion as a continuous spline and optical flow as an
implicit neural representation, our method inherently embeds spatial-temporal
coherence through inductive biases. Second, we incorporate structure-and-motion
priors through differential geometric constraints, bypassing explicit depth
estimation while maintaining rigorous geometric consistency. As a result, our
framework (called E-MoFlow) unifies egomotion and optical flow estimation via
implicit regularization under a fully unsupervised paradigm. Experiments
demonstrate its versatility to general 6-DoF motion scenarios, achieving
state-of-the-art performance among unsupervised methods and competitive even
with supervised approaches.

</details>


### [186] [PET Head Motion Estimation Using Supervised Deep Learning with Attention](https://arxiv.org/abs/2510.12758)
*Zhuotong Cai,Tianyi Zeng,Jiazhen Zhang,Eléonore V. Lieffrig,Kathryn Fontaine,Chenyu You,Enette Mae Revilla,James S. Duncan,Jingmin Xin,Yihuan Lu,John A. Onofrey*

Main category: cs.CV

TL;DR: The paper introduces DL-HMC++, a deep-learning approach using cross-attention for head motion correction in PET imaging, showcasing high effectiveness and generalization across scanners and radiotracers.


<details>
  <summary>Details</summary>
Motivation: Head movement during PET imaging generates artifacts and inaccuracies, making it necessary to develop an alternative to hardware-based motion tracking for better clinical application.

Method: DL-HMC++ utilizes supervised deep learning with cross-attention based on dynamic PET scans and gold-standard external hardware motion measurements to estimate and correct head motion.

Result: DL-HMC++ consistently outperformed existing methods in motion correction, achieving motion-free imaging and reducing artifacts, with an average difference ratio of less than 2% compared to gold-standard hardware tracking.

Conclusion: DL-HMC++ is a promising data-driven solution, reducing dependency on hardware motion tracking and improving clinical accessibility for PET imaging motion correction.

Abstract: Head movement poses a significant challenge in brain positron emission
tomography (PET) imaging, resulting in image artifacts and tracer uptake
quantification inaccuracies. Effective head motion estimation and correction
are crucial for precise quantitative image analysis and accurate diagnosis of
neurological disorders. Hardware-based motion tracking (HMT) has limited
applicability in real-world clinical practice. To overcome this limitation, we
propose a deep-learning head motion correction approach with cross-attention
(DL-HMC++) to predict rigid head motion from one-second 3D PET raw data.
DL-HMC++ is trained in a supervised manner by leveraging existing dynamic PET
scans with gold-standard motion measurements from external HMT. We evaluate
DL-HMC++ on two PET scanners (HRRT and mCT) and four radiotracers (18F-FDG,
18F-FPEB, 11C-UCB-J, and 11C-LSN3172176) to demonstrate the effectiveness and
generalization of the approach in large cohort PET studies. Quantitative and
qualitative results demonstrate that DL-HMC++ consistently outperforms
state-of-the-art data-driven motion estimation methods, producing motion-free
images with clear delineation of brain structures and reduced motion artifacts
that are indistinguishable from gold-standard HMT. Brain region of interest
standard uptake value analysis exhibits average difference ratios between
DL-HMC++ and gold-standard HMT to be 1.2 plus-minus 0.5% for HRRT and 0.5
plus-minus 0.2% for mCT. DL-HMC++ demonstrates the potential for data-driven
PET head motion correction to remove the burden of HMT, making motion
correction accessible to clinical populations beyond research settings. The
code is available at https://github.com/maxxxxxxcai/DL-HMC-TMI.

</details>


### [187] [AnyUp: Universal Feature Upsampling](https://arxiv.org/abs/2510.12764)
*Thomas Wimmer,Prune Truong,Marie-Julie Rakotosaona,Michael Oechsle,Federico Tombari,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.CV

TL;DR: AnyUp is a feature upsampling method that works with any vision feature at any resolution without encoder-specific training.


<details>
  <summary>Details</summary>
Motivation: Existing upsampling methods require re-training for every feature extractor, limiting their generalizability. AnyUp aims to overcome this by providing an efficient and universal solution.

Method: AnyUp introduces a feature-agnostic upsampling architecture that operates at inference time, allowing generalization across different feature extractors.

Result: Experiments show that AnyUp achieves state-of-the-art performance in upsampled feature quality, generalizes to various feature types, and preserves feature semantics.

Conclusion: AnyUp is a practical and efficient upsampling solution that can be applied universally without the need for encoder-specific re-training, benefiting a wide range of tasks.

Abstract: We introduce AnyUp, a method for feature upsampling that can be applied to
any vision feature at any resolution, without encoder-specific training.
Existing learning-based upsamplers for features like DINO or CLIP need to be
re-trained for every feature extractor and thus do not generalize to different
feature types at inference time. In this work, we propose an inference-time
feature-agnostic upsampling architecture to alleviate this limitation and
improve upsampling quality. In our experiments, AnyUp sets a new state of the
art for upsampled features, generalizes to different feature types, and
preserves feature semantics while being efficient and easy to apply to a wide
range of downstream tasks.

</details>


### [188] [Efficient Perceptual Image Super Resolution: AIM 2025 Study and Benchmark](https://arxiv.org/abs/2510.12765)
*Bruno Longarela,Marcos V. Conde,Alvaro Garcia,Radu Timofte*

Main category: cs.CV

TL;DR: The paper studies efficient perceptual-quality super-resolution, improving on Real-ESRGAN while conforming to strict model constraints.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of current perceptual-quality-oriented super-resolution methods, with a focus on improving visual metrics under constrained resources.

Method: Developed solutions constrained by parameter and GFLOPs limits, tested on a realistic 4K dataset designed with various degradation types.

Result: The top-performing method surpassed Real-ESRGAN in perceptual quality on all benchmark datasets.

Conclusion: Efficient approaches can achieve superior perceptual-quality super-resolution, setting new standards for resource-efficient models.

Abstract: This paper presents a comprehensive study and benchmark on Efficient
Perceptual Super-Resolution (EPSR). While significant progress has been made in
efficient PSNR-oriented super resolution, approaches focusing on perceptual
quality metrics remain relatively inefficient. Motivated by this gap, we aim to
replicate or improve the perceptual results of Real-ESRGAN while meeting strict
efficiency constraints: a maximum of 5M parameters and 2000 GFLOPs, calculated
for an input size of 960x540 pixels. The proposed solutions were evaluated on a
novel dataset consisting of 500 test images of 4K resolution, each degraded
using multiple degradation types, without providing the original high-quality
counterparts. This design aims to reflect realistic deployment conditions and
serves as a diverse and challenging benchmark. The top-performing approach
manages to outperform Real-ESRGAN across all benchmark datasets, demonstrating
the potential of efficient methods in the perceptual domain. This paper
establishes the modern baselines for efficient perceptual super resolution.

</details>


### [189] [Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction](https://arxiv.org/abs/2510.12768)
*Fengzhi Guo,Chih-Chuan Hsu,Sihao Ding,Cheng Zhang*

Main category: cs.CV

TL;DR: The paper introduces USplat4D, an Uncertainty-aware framework for improving dynamic Gaussian Splatting in reconstructing dynamic 3D scenes.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of vanilla dynamic Gaussian Splatting, which struggles with motion drifts under occlusion and poor synthesis in unseen views due to uniformly optimizing all Gaussian primitives.

Method: USplat4D estimates time-varying uncertainty for each Gaussian primitive and uses it to create a spatio-temporal graph for optimization, effectively treating well-observed Gaussians as reliable anchors and lesser-observed ones as less reliable.

Result: The model consistently enhances reconstruction quality, achieving stable geometry under occlusion and improved synthesis for novel and extreme viewpoints, validated across multiple real and synthetic datasets.

Conclusion: Incorporating uncertainty into dynamic Gaussian Splatting significantly improves its ability to reconstruct dynamic 3D scenes, especially under challenging conditions like occlusion and extreme novel views.

Abstract: Reconstructing dynamic 3D scenes from monocular input is fundamentally
under-constrained, with ambiguities arising from occlusion and extreme novel
views. While dynamic Gaussian Splatting offers an efficient representation,
vanilla models optimize all Gaussian primitives uniformly, ignoring whether
they are well or poorly observed. This limitation leads to motion drifts under
occlusion and degraded synthesis when extrapolating to unseen views. We argue
that uncertainty matters: Gaussians with recurring observations across views
and time act as reliable anchors to guide motion, whereas those with limited
visibility are treated as less reliable. To this end, we introduce USplat4D, a
novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates
reliable motion cues to enhance 4D reconstruction. Our key insight is to
estimate time-varying per-Gaussian uncertainty and leverages it to construct a
spatio-temporal graph for uncertainty-aware optimization. Experiments on
diverse real and synthetic datasets show that explicitly modeling uncertainty
consistently improves dynamic Gaussian Splatting models, yielding more stable
geometry under occlusion and high-quality synthesis at extreme viewpoints.

</details>


### [190] [What If : Understanding Motion Through Sparse Interactions](https://arxiv.org/abs/2510.12777)
*Stefan Andreas Baumann,Nick Stracke,Timy Phan,Björn Ommer*

Main category: cs.CV

TL;DR: The paper introduces Flow Poke Transformer (FPT), a new method to predict local motion distributions in physical scenes from sparse interactions.


<details>
  <summary>Details</summary>
Motivation: To enable interpretable multi-modal motion predictions considering local interactions and inherent uncertainties in the dynamics of physical scenes.

Method: FPT predicts distributions of local motion by leveraging sparse physical interactions ('pokes') and analyzing multi-modal scene motion.

Result: FPT demonstrates superiority in face motion generation and articulated object motion estimation on synthetic datasets. It excels in diverse tasks like moving part segmentation.

Conclusion: FPT is a flexible, interpretable tool for understanding scene dynamics and predicting motion. Its applications range across various domains, showcasing its versatility and performance advantages.

Abstract: Understanding the dynamics of a physical scene involves reasoning about the
diverse ways it can potentially change, especially as a result of local
interactions. We present the Flow Poke Transformer (FPT), a novel framework for
directly predicting the distribution of local motion, conditioned on sparse
interactions termed "pokes". Unlike traditional methods that typically only
enable dense sampling of a single realization of scene dynamics, FPT provides
an interpretable directly accessible representation of multi-modal scene
motion, its dependency on physical interactions and the inherent uncertainties
of scene dynamics. We also evaluate our model on several downstream tasks to
enable comparisons with prior methods and highlight the flexibility of our
approach. On dense face motion generation, our generic pre-trained model
surpasses specialized baselines. FPT can be fine-tuned in strongly
out-of-distribution tasks such as synthetic datasets to enable significant
improvements over in-domain methods in articulated object motion estimation.
Additionally, predicting explicit motion distributions directly enables our
method to achieve competitive performance on tasks like moving part
segmentation from pokes which further demonstrates the versatility of our FPT.
Code and models are publicly available at
https://compvis.github.io/flow-poke-transformer.

</details>


### [191] [SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models](https://arxiv.org/abs/2510.12784)
*Weiyang Jin,Yuwei Niu,Jiaqi Liao,Chengqi Duan,Aoxue Li,Shenghua Gao,Xihui Liu*

Main category: cs.CV

TL;DR: This paper introduces SRUM, a framework enabling Unified Multimodal Models (UMMs) to improve their visual generation capabilities by using their understanding module as an internal evaluator without additional labeled data.


<details>
  <summary>Details</summary>
Motivation: Unified Multimodal Models often demonstrate strong visual understanding but struggle to generate accurate visual content. The paper aims to address this gap and explores whether self-improvement can be achieved through internal feedback loops.

Method: SRUM employs a self-rewarding framework where the understanding module evaluates generated images using a global-local dual reward system that ensures semantic and object-level fidelity.

Result: SRUM boosts performance metrics, improving scores on T2I-CompBench from 82.18 to 88.37 and on T2I-ReasonBench from 43.82 to 46.75, showing strong visual generation capabilities and generalization.

Conclusion: The proposed SRUM framework empowers UMMs to utilize their understanding module for self-guided improvement in visual generation, introducing a new paradigm for multimodal AI systems.

Abstract: Recently, remarkable progress has been made in Unified Multimodal Models
(UMMs), which integrate vision-language generation and understanding
capabilities within a single framework. However, a significant gap exists where
a model's strong visual understanding often fails to transfer to its visual
generation. A model might correctly understand an image based on user
instructions, yet be unable to generate a faithful image from text prompts.
This phenomenon directly raises a compelling question: Can a model achieve
self-improvement by using its understanding module to reward its generation
module? To bridge this gap and achieve self-improvement, we introduce SRUM, a
self-rewarding post-training framework that can be directly applied to existing
UMMs of various designs. SRUM creates a feedback loop where the model's own
understanding module acts as an internal ``evaluator'', providing corrective
signals to improve its generation module, without requiring additional
human-labeled data. To ensure this feedback is comprehensive, we designed a
global-local dual reward system. To tackle the inherent structural complexity
of images, this system offers multi-scale guidance: a \textbf{global reward}
ensures the correctness of the overall visual semantics and layout, while a
\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads
to powerful capabilities and shows strong generalization, boosting performance
on T2I-CompBench from 82.18 to \textbf{88.37} and on T2I-ReasonBench from 43.82
to \textbf{46.75}. Overall, our work establishes a powerful new paradigm for
enabling a UMMs' understanding module to guide and enhance its own generation
via self-rewarding.

</details>


### [192] [MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars](https://arxiv.org/abs/2510.12785)
*Felix Taubner,Ruihang Zhang,Mathieu Tuli,Sherwin Bahmani,David B. Lindell*

Main category: cs.CV

TL;DR: This paper presents MVP4D, a video model that creates animatable multi-view videos of digital human avatars from a single reference image, addressing traditional limitations in realism and 3D consistency.


<details>
  <summary>Details</summary>
Motivation: Creating photorealistic human avatars is traditionally expensive and time-consuming, involving large camera setups and professional manual effort. Recent methods simplify this process using image/video models but suffer from degradation in realism for non-reference viewpoints.

Method: MVP4D leverages a state-of-the-art pre-trained video diffusion model to generate multi-view videos from a single reference image and target expressions, producing hundreds of frames simultaneously across 360-degree viewpoints. Outputs are distilled into a 4D avatar for real-time rendering.

Result: The approach improves realism, temporal consistency, and 3D consistency of digital human avatars compared to earlier methods.

Conclusion: MVP4D advances avatar creation technology by enabling efficient, high-quality, and real-time animatable avatars from a single image.

Abstract: Digital human avatars aim to simulate the dynamic appearance of humans in
virtual environments, enabling immersive experiences across gaming, film,
virtual reality, and more. However, the conventional process for creating and
animating photorealistic human avatars is expensive and time-consuming,
requiring large camera capture rigs and significant manual effort from
professional 3D artists. With the advent of capable image and video generation
models, recent methods enable automatic rendering of realistic animated avatars
from a single casually captured reference image of a target subject. While
these techniques significantly lower barriers to avatar creation and offer
compelling realism, they lack constraints provided by multi-view information or
an explicit 3D representation. So, image quality and realism degrade when
rendered from viewpoints that deviate strongly from the reference image. Here,
we build a video model that generates animatable multi-view videos of digital
humans based on a single reference image and target expressions. Our model,
MVP4D, is based on a state-of-the-art pre-trained video diffusion model and
generates hundreds of frames simultaneously from viewpoints varying by up to
360 degrees around a target subject. We show how to distill the outputs of this
model into a 4D avatar that can be rendered in real-time. Our approach
significantly improves the realism, temporal consistency, and 3D consistency of
generated avatars compared to previous methods.

</details>


### [193] [Efficient Real-World Deblurring using Single Images: AIM 2025 Challenge Report](https://arxiv.org/abs/2510.12788)
*Daniel Feijoo,Paula Garrido-Mellado,Marcos V. Conde,Jaesung Rim,Alvaro Garcia,Sunghyun Cho,Radu Timofte*

Main category: cs.CV

TL;DR: The paper reviews the AIM 2025 Efficient Real-World Deblurring Challenge, focusing on developing efficient solutions for real-world image deblurring under strict efficiency constraints.


<details>
  <summary>Details</summary>
Motivation: The paper aims to advance the field of real-world blur restoration by introducing new efficiency constraints.

Method: Participants were tasked with designing image deblurring solutions using fewer than 5 million parameters and under 200 GMACs.

Result: 71 participants registered, with 4 teams providing valid solutions; the best solution achieved a PSNR of 31.1298 dB.

Conclusion: The paper highlights the potential of efficient image deblurring techniques and offers insights for future research in this area.

Abstract: This paper reviews the AIM 2025 Efficient Real-World Deblurring using Single
Images Challenge, which aims to advance in efficient real-blur restoration. The
challenge is based on a new test set based on the well known RSBlur dataset.
Pairs of blur and degraded images in this dataset are captured using a
double-camera system. Participant were tasked with developing solutions to
effectively deblur these type of images while fulfilling strict efficiency
constraints: fewer than 5 million model parameters and a computational budget
under 200 GMACs. A total of 71 participants registered, with 4 teams finally
submitting valid solutions. The top-performing approach achieved a PSNR of
31.1298 dB, showcasing the potential of efficient methods in this domain. This
paper provides a comprehensive overview of the challenge, compares the proposed
solutions, and serves as a valuable reference for researchers in efficient
real-world image deblurring.

</details>


### [194] [UniFusion: Vision-Language Model as Unified Encoder in Image Generation](https://arxiv.org/abs/2510.12789)
*Kevin Li,Manuel Brack,Sudeep Katakol,Hareesh Ravi,Ajinkya Kale*

Main category: cs.CV

TL;DR: UniFusion introduces a novel diffusion-based model using frozen vision-language models (VLMs) as unified multimodal encoders, enhanced by techniques like Layerwise Attention Pooling (LAP) and VERIFI.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing architectures which rely on separate text and image encoders, making cross-modal reasoning and knowledge transfer challenging.

Method: Utilized frozen VLMs with Layerwise Attention Pooling (LAP) and VERIFI as innovative mechanisms for multimodal text-image alignment and flexible inference within diffusion-based models.

Result: UniFusion achieved superior text-image alignment, enabled faithful transfer of visual information, and demonstrated significant generalization capabilities like zero-shot multiple image reference editing.

Conclusion: The model validates the effectiveness of unified encoder designs in improving multimodal generation tasks and showcases scalability and flexibility in inference.

Abstract: Although recent advances in visual generation have been remarkable, most
existing architectures still depend on distinct encoders for images and text.
This separation constrains diffusion models' ability to perform cross-modal
reasoning and knowledge transfer. Prior attempts to bridge this gap often use
the last layer information from VLM, employ multiple visual encoders, or train
large unified models jointly for text and image generation, which demands
substantial computational resources and large-scale data, limiting its
accessibility.We present UniFusion, a diffusion-based generative model
conditioned on a frozen large vision-language model (VLM) that serves as a
unified multimodal encoder. At the core of UniFusion is the Layerwise Attention
Pooling (LAP) mechanism that extracts both high level semantics and low level
details from text and visual tokens of a frozen VLM to condition a diffusion
generative model. We demonstrate that LAP outperforms other shallow fusion
architectures on text-image alignment for generation and faithful transfer of
visual information from VLM to the diffusion model which is key for editing. We
propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI),
which conditions a diffusion transformer (DiT) only on the text tokens
generated by the VLM during in-model prompt rewriting. VERIFI combines the
alignment of the conditioning distribution with the VLM's reasoning
capabilities for increased capabilities and flexibility at inference. In
addition, finetuning on editing task not only improves text-image alignment for
generation, indicative of cross-modality knowledge transfer, but also exhibits
tremendous generalization capabilities. Our model when trained on single image
editing, zero-shot generalizes to multiple image references further motivating
the unified encoder design of UniFusion.

</details>


### [195] [ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution](https://arxiv.org/abs/2510.12793)
*Long Cui,Weiyun Wang,Jie Shao,Zichen Wen,Gen Luo,Linfeng Zhang,Yanting Zhang,Yu Qiao,Wenhai Wang*

Main category: cs.CV

TL;DR: The paper introduces Visual Consistency Learning (ViCO), a novel method to reduce inference costs in Multimodal Large Language Models (MLLMs) by dynamically adjusting the number of vision tokens according to semantic complexity.


<details>
  <summary>Details</summary>
Motivation: Multimodal Large Language Models (MLLMs) have increased inference costs due to the excessive vision tokens introduced by image inputs. The aim is to make MLLMs more computationally efficient while retaining performance.

Method: The proposed method, ViCO, employs multiple MLP connectors to compress vision tokens at varying ratios based on semantic complexity of images. KL divergence minimization ensures consistency across different compression levels. At inference, a Visual Resolution Router (ViR) selects the appropriate compression rate per image dynamically.

Result: Experimental results confirmed that ViCO reduces vision tokens by up to 50% while preserving key capabilities such as perception, reasoning, and OCR.

Conclusion: ViCO improves the efficiency of MLLMs by balancing token compression with performance. The method enables smarter visual token processing, and code/models will be released for further research development.

Abstract: Existing Multimodal Large Language Models (MLLMs) suffer from increased
inference costs due to the additional vision tokens introduced by image inputs.
In this work, we propose Visual Consistency Learning (ViCO), a novel training
algorithm that enables the model to represent images of varying semantic
complexities using different numbers of vision tokens. The key idea behind our
method is to employ multiple MLP connectors, each with a different image
compression ratio, to downsample the vision tokens based on the semantic
complexity of the image. During training, we minimize the KL divergence between
the responses conditioned on different MLP connectors. At inference time, we
introduce an image router, termed Visual Resolution Router (ViR), that
automatically selects the appropriate compression rate for each image patch.
Compared with existing dynamic high-resolution strategies, which adjust the
number of visual tokens based on image resolutions, our method dynamically
adapts the number of visual tokens according to semantic complexity.
Experimental results demonstrate that our method can reduce the number of
vision tokens by up to 50% while maintaining the model's perception, reasoning,
and OCR capabilities. We hope this work will contribute to the development of
more efficient MLLMs. The code and models will be released to facilitate future
research.

</details>


### [196] [DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving](https://arxiv.org/abs/2510.12796)
*Yingyan Li,Shuyao Shang,Weisong Liu,Bing Zhan,Haochen Wang,Yuqi Wang,Yuntao Chen,Xiaoman Wang,Yasong An,Chufeng Tang,Lu Hou,Lue Fan,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: The paper introduces DriveVLA-W0, a training paradigm using world modeling to enhance Vision-Language-Action (VLA) models for driving by generating dense, self-supervised signals.


<details>
  <summary>Details</summary>
Motivation: VLA models for driving are limited by a supervision deficit as they use sparse, low-dimensional actions, failing to fully utilize their potential in generalized driving intelligence.

Method: DriveVLA-W0 employs world modeling, predicting future images to create dense self-supervised signals, thereby teaching models the dynamics of driving environments. It supports autoregressive and diffusion world models.

Result: Experiments on NAVSIM benchmarks and a large in-house dataset show significant performance improvements over BEV and VLA baselines, with accelerated gains as dataset size increases.

Conclusion: World modeling with DriveVLA-W0 enhances VLA models’ representational power, improves their scalability on large datasets, and addresses real-time deployment challenges effectively.

Abstract: Scaling Vision-Language-Action (VLA) models on large-scale data offers a
promising path to achieving a more generalized driving intelligence. However,
VLA models are limited by a ``supervision deficit'': the vast model capacity is
supervised by sparse, low-dimensional actions, leaving much of their
representational power underutilized. To remedy this, we propose
\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to
predict future images. This task generates a dense, self-supervised signal that
compels the model to learn the underlying dynamics of the driving environment.
We showcase the paradigm's versatility by instantiating it for two dominant VLA
archetypes: an autoregressive world model for VLAs that use discrete visual
tokens, and a diffusion world model for those operating on continuous visual
features. Building on the rich representations learned from world modeling, we
introduce a lightweight action expert to address the inference latency for
real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a
680x larger in-house dataset demonstrate that DriveVLA-W0 significantly
outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling
law, showing that performance gains accelerate as the training dataset size
increases.

</details>


### [197] [Detect Anything via Next Point Prediction](https://arxiv.org/abs/2510.12798)
*Qing Jiang,Junan Huo,Xingyu Chen,Yuda Xiong,Zhaoyang Zeng,Yihao Chen,Tianhe Ren,Junzhi Yu,Lei Zhang*

Main category: cs.CV

TL;DR: The paper introduces Rex-Omni, a 3B-scale model for object detection using Multimodal Large Language Models (MLLMs), outperforming traditional regression-based models and offering versatile visual capabilities.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection models like YOLO and DETR face limitations in tasks like low recall and coordinate misalignment. The goal is to address these gaps using a novel MLLM-based approach.

Method: Rex-Omni is designed using three innovations: (1) special tokens for quantized coordinates, (2) data engines generating diverse training data, and (3) a two-stage training pipeline combining supervised fine-tuning and reinforcement learning with geometry-aware rewards.

Result: Rex-Omni matches or outperforms traditional models in zero-shot settings on COCO and LVIS benchmarks. It also offers expanded capabilities beyond regular tasks like object referring, OCR, and spatial tasks.

Conclusion: Rex-Omni demonstrates that MLLMs can match and surpass traditional approaches, introducing a versatile system for language-aware visual perception.

Abstract: Object detection has long been dominated by traditional coordinate
regression-based models, such as YOLO, DETR, and Grounding DINO. Although
recent efforts have attempted to leverage MLLMs to tackle this task, they face
challenges like low recall rate, duplicate predictions, coordinate
misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a
3B-scale MLLM that achieves state-of-the-art object perception performance. On
benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or
exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot
setting. This is enabled by three key designs: 1) Task Formulation: we use
special tokens to represent quantized coordinates from 0 to 999, reducing the
model's learning difficulty and improving token efficiency for coordinate
prediction; 2) Data Engines: we construct multiple data engines to generate
high-quality grounding, referring, and pointing data, providing semantically
rich supervision for training; \3) Training Pipelines: we employ a two-stage
training process, combining supervised fine-tuning on 22 million data with
GRPO-based reinforcement post-training. This RL post-training leverages
geometry-aware rewards to effectively bridge the discrete-to-continuous
coordinate prediction gap, improve box accuracy, and mitigate undesirable
behaviors like duplicate predictions that stem from the teacher-guided nature
of the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent
language understanding enables versatile capabilities such as object referring,
pointing, visual prompting, GUI grounding, spatial referring, OCR and
key-pointing, all systematically evaluated on dedicated benchmarks. We believe
that Rex-Omni paves the way for more versatile and language-aware visual
perception systems.

</details>


### [198] [DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search](https://arxiv.org/abs/2510.12801)
*Kartik Narayan,Yang Xu,Tian Cao,Kavya Nerella,Vishal M. Patel,Navid Shiee,Peter Grasch,Chao Jia,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: The paper introduces DeepMMSearch-R1, a multimodal language model designed for effective real-world web search, overcoming inefficiencies in existing models.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs in real-world applications struggle with inefficiencies like rigid pipelines and suboptimal search functionalities, necessitating a more dynamic and effective solution.

Method: The approach combines supervised finetuning and reinforcement learning. Additionally, a novel dataset, DeepMMSearchVQA, is created with multimodal VQA queries for robust training.

Result: DeepMMSearch-R1 demonstrates superior performance in knowledge-intensive benchmarks, showcasing efficient, self-adaptive text and image search capabilities.

Conclusion: DeepMMSearch-R1 significantly advances multimodal web-search by integrating dynamic search initiation and adaptive query crafting techniques, offering valuable insights for MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) in real-world applications require
access to external knowledge sources and must remain responsive to the dynamic
and ever-changing real-world information in order to address
information-seeking and knowledge-intensive user queries. Existing approaches,
such as retrieval augmented generation (RAG) methods, search agents, and search
equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and
poorly constructed search queries, which result in inefficiencies and
suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,
the first multimodal LLM capable of performing on-demand, multi-turn web
searches and dynamically crafting queries for both image and text search tools.
Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops
of the input image making the image search more effective, and can iteratively
adapt text search queries based on retrieved information, thereby enabling
self-reflection and self-correction. Our approach relies on a two-stage
training pipeline: a cold start supervised finetuning phase followed by an
online reinforcement learning optimization. For training, we introduce
DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated
pipeline intermixed with real-world information from web search tools. This
dataset contains diverse, multi-hop queries that integrate textual and visual
information, teaching the model when to search, what to search for, which
search tool to use and how to reason over the retrieved information. We conduct
extensive experiments across a range of knowledge-intensive benchmarks to
demonstrate the superiority of our approach. Finally, we analyze the results
and provide insights that are valuable for advancing multimodal web-search.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [199] [FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters](https://arxiv.org/abs/2510.11938)
*Yanying Lin,Shijie Peng,Chengzhi Lu,Chengzhong Xu,Kejiang Ye*

Main category: cs.DC

TL;DR: FlexPipe dynamically reconfigures serverless pipeline architectures for serving Large Language Models (LLMs), leading to significant resource efficiency and lower latency.


<details>
  <summary>Details</summary>
Motivation: Serving LLMs in production suffers from inefficiencies due to static pipeline configurations that cannot handle dynamic workloads efficiently.

Method: FlexPipe introduces dynamic pipeline reconfiguration by fine-grained model partitioning, inflight pipeline adjustments, and topology-aware GPU resource allocation.

Result: FlexPipe delivers up to 8.5x better resource efficiency while achieving 38.3% lower latency compared to current systems, and it reduces GPU reservation needs dramatically.

Conclusion: FlexPipe enhances the scalability and adaptability of LLM serverless systems, showcasing the potential for optimized resource utilization and performance.

Abstract: Serving Large Language Models (LLMs) in production faces significant
challenges from highly variable request patterns and severe resource
fragmentation in serverless clusters. Current systems rely on static pipeline
configurations that struggle to adapt to dynamic workload conditions, leading
to substantial inefficiencies. We present FlexPipe, a novel system that
dynamically reconfigures pipeline architectures during runtime to address these
fundamental limitations. FlexPipe decomposes models into fine-grained stages
and intelligently adjusts pipeline granularity based on real-time request
pattern analysis, implementing three key innovations: fine-grained model
partitioning with preserved computational graph constraints, inflight pipeline
refactoring with consistent cache transitions, and topology-aware resource
allocation that navigates GPU fragmentation. Comprehensive evaluation on an
82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource
efficiency while maintaining 38.3% lower latency compared to state-of-the-art
systems, reducing GPU reservation requirements from 75% to 30% of peak
capacity.

</details>


### [200] [Comparing Cross-Platform Performance via Node-to-Node Scaling Studies](https://arxiv.org/abs/2510.12166)
*Kenneth Weiss,Thomas M. Stitt,Daryl Hawkins,Olga Pearce,Stephanie Brink,Robert N. Rieben*

Main category: cs.DC

TL;DR: This paper advocates for using a single compute node as the base unit in cross-platform performance studies, providing guidance, templates, and case studies for effective comparison.


<details>
  <summary>Details</summary>
Motivation: Increasing diversity in high-performance computing platforms necessitates a standard approach to comparing performance and scalability across different systems.

Method: The paper identifies a single compute node as the base comparison unit, suggests setup and analysis procedures, and provides templates and case studies for conducting node-to-node scaling analyses.

Result: Templates and case studies demonstrated the practicality and benefits of using single-node scaling as a standard method for cross-platform performance studies.

Conclusion: Using a single compute node as the basis for cross-platform studies simplifies comparative analysis and enhances the clarity of performance results across diverse systems.

Abstract: Due to the increasing diversity of high-performance computing architectures,
researchers and practitioners are increasingly interested in comparing a code's
performance and scalability across different platforms. However, there is a
lack of available guidance on how to actually set up and analyze such
cross-platform studies. In this paper, we contend that the natural base unit of
computing for such studies is a single compute node on each platform and offer
guidance in setting up, running, and analyzing node-to-node scaling studies. We
propose templates for presenting scaling results of these studies and provide
several case studies highlighting the benefits of this approach.

</details>


### [201] [GPU-Accelerated Algorithms for Process Mapping](https://arxiv.org/abs/2510.12196)
*Petr Samoldekin,Christian Schulz,Henning Woydt*

Main category: cs.DC

TL;DR: This paper introduces two GPU-accelerated algorithms for process mapping, achieving significant speedups compared to CPU-based techniques while maintaining competitive communication costs.


<details>
  <summary>Details</summary>
Motivation: Current process mapping methods are CPU-based, making them slower and less efficient for large-scale computations, and GPUs have shown potential for graph partitioning.

Method: Two methods are presented: hierarchical multisection using GPU-based graph partitioners and integration of process mapping into multilevel graph partitioning, leveraging GPU parallelism.

Result: Both algorithms show speedups exceeding 300 times compared to CPU algorithms. The first is competitive in communication costs, while the second emphasizes faster computation but sacrifices some solution quality.

Conclusion: These GPU-based methods are the first of their kind for process mapping and demonstrate significant advancements in speed and efficiency, although with a tradeoff in solution quality for the second method.

Abstract: Process mapping asks to assign vertices of a task graph to processing
elements of a supercomputer such that the computational workload is balanced
while the communication cost is minimized. Motivated by the recent success of
GPU-based graph partitioners, we propose two GPU-accelerated algorithms for
this optimization problem. The first algorithm employs hierarchical
multisection, which partitions the task graph alongside the hierarchy of the
supercomputer. The method utilizes GPU-based graph partitioners to accelerate
the mapping process. The second algorithm integrates process mapping directly
into the modern multilevel graph partitioning pipeline. Vital phases like
coarsening and refinement are accelerated by exploiting the parallelism of
GPUs. In our experiments, both methods achieve speedups exceeding 300 when
compared to state-of-the-art CPU-based algorithms. The first algorithm has, on
average, about 10 percent greater communication costs and thus remains
competitive to CPU algorithms. The second approach is much faster, with a
geometric mean speedup of 77.6 and peak speedup of 598 at the cost of lower
solution quality. To our knowledge, these are the first GPU-based algorithms
for process mapping.

</details>


### [202] [Metronome: Efficient Scheduling for Periodic Traffic Jobs with Network and Priority Awareness](https://arxiv.org/abs/2510.12274)
*Hao Jiang,Meng Qin,Ruijie Kuai,Dandan Liang*

Main category: cs.DC

TL;DR: The paper introduces Metronome, a network-aware and priority-aware scheduling system for optimizing bandwidth allocation in cloud-native networks, particularly for distributed training.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of resource coordination and handling dynamic bandwidth demands in cloud-native networks for distributed training tasks.

Method: Metronome employs a time-division multiplexing approach to create an elastic network resource allocation model. It also integrates multi-objective optimization for latency and job priorities and adjusts dynamically by monitoring the cluster.

Result: Experiments on 13 machine learning models showed Metronome reduces job completion time by up to 19.50% and improves bandwidth utilization by up to 23.20%, outperforming Kubernetes scheduling.

Conclusion: Metronome enhances resource utilization and service performance, demonstrating the effectiveness of its dynamic and optimized scheduling approach for cloud-native environments.

Abstract: With the rapid growth in computing power demand, cloud native networks have
emerged as a promising solution to address the challenges of efficient resource
coordination, particularly in coping with the dynamic fluctuations of network
bandwidth in clusters. We propose Metronome, a network-aware and priority-aware
scheduling mechanism for cloud native networks. This mechanism is designed to
support jobs that exhibit periodic traffic patterns and dynamic bandwidth
demands, particularly in the context of distributed training. Specifically,
Metronome employs a time-division multiplexing approach that leverages job
traffic characteristics to construct an elastic network resource allocation
model, enabling efficient bandwidth sharing across multiple jobs. In addition,
it incorporates a multi-objective optimization strategy, jointly considering
latency and job priorities to achieve globally optimal as well as dynamic
resource allocation. Finally, Metronome adapts to the dynamic environment by
monitoring the cluster and performing reconfiguration operations. Extensive
experiments with 13 common machine learning models demonstrate that Metronome
can enhance cluster resource utilization while guaranteeing service
performance. Compared with the existing Kubernetes scheduling mechanisms across
multiple scenarios, Metronome reduces job completion time by up to 19.50% while
improving average bandwidth utilization by up to 23.20%.

</details>


### [203] [A Non-Intrusive Framework for Deferred Integration of Cloud Patterns in Energy-Efficient Data-Sharing Pipelines](https://arxiv.org/abs/2510.12354)
*Sepideh Masoudi,Mark Edward Michael Daly,Jannis Kiesel,Stefan Tai*

Main category: cs.DC

TL;DR: The paper introduces a Kubernetes-based tool for modular, cloud-native data-sharing pipelines, enabling the seamless application of cloud design patterns and energy-aware decision-making.


<details>
  <summary>Details</summary>
Motivation: To enhance scalability, energy efficiency, and modularity in consumer-specific data-sharing pipelines, while avoiding conflicts with traditional cloud design patterns.

Method: Development of a Kubernetes-based tool that allows deferred, non-intrusive application of cloud design patterns and collects energy consumption metrics.

Result: The tool supports automated pattern injection and enables developers to optimize pipelines for energy efficiency without altering service source code.

Conclusion: The proposed tool balances modularity, reusability, and energy-awareness in data-sharing pipelines, overcoming challenges posed by traditional cloud design patterns.

Abstract: As data mesh architectures gain traction in federated environments,
organizations are increasingly building consumer-specific data-sharing
pipelines using modular, cloud-native transformation services. Prior work has
shown that structuring these pipelines with reusable transformation stages
enhances both scalability and energy efficiency. However, integrating
traditional cloud design patterns into such pipelines poses a challenge:
predefining and embedding patterns can compromise modularity, reduce
reusability, and conflict with the pipelines dynamic, consumer-driven nature.
To address this, we introduce a Kubernetes-based tool that enables the deferred
and non-intrusive application of selected cloud design patterns without
requiring changes to service source code. The tool supports automated pattern
injection and collects energy consumption metrics, allowing developers to make
energy-aware decisions while preserving the flexible, composable structure of
reusable data-sharing pipelines.

</details>


### [204] [TALP-Pages: An easy-to-integrate continuous performance monitoring framework](https://arxiv.org/abs/2510.12436)
*Valentin Seitz,Jordy Trilaksono,Marta Garcia-Gasulla*

Main category: cs.DC

TL;DR: This paper presents TALP-Pages, a framework to provide quick feedback on code performance during development using metrics and HTML visualization.


<details>
  <summary>Details</summary>
Motivation: Performance degradation detection during HPC code development is critical to maintain code efficiency, along with insights into application scaling.

Method: The authors propose TALP-Pages, which integrates with CI workflows and uses TALP to collect performance metrics on-the-fly, generating HTML reports with visualizations and scaling-efficiency tables.

Result: TALP-Pages is evaluated against tracing-based tools, demonstrating quicker generation of scaling-efficiency tables with lower resource usage.

Conclusion: TALP-Pages is easy to integrate and effective for identifying and understanding performance changes in HPC code development workflows.

Abstract: Ensuring good performance is a key aspect in the development of codes that
target HPC machines. As these codes are under active development, the necessity
to detect performance degradation early in the development process becomes
apparent. In addition, having meaningful insight into application scaling
behavior tightly coupled to the development workflow is helpful. In this paper,
we introduce TALP-Pages, an easy-to-integrate framework that enables developers
to get fast and in-repository feedback about their code performance using
established fundamental performance and scaling factors. The framework relies
on TALP, which enables the on-the-fly collection of these metrics. Based on a
folder structure suited for CI which contains the files generated by TALP,
TALP-Pages generates an HTML report with visualizations of the performance
factor regression as well as scaling-efficiency tables. We compare TALP-Pages
to tracing-based tools in terms of overhead and post-processing requirements
and find that TALP-Pages can produce the scaling-efficiency tables faster and
under tighter resource constraints. To showcase the ease of use and
effectiveness of this approach, we extend the current CI setup of GENE-X with
only minimal changes required and showcase the ability to detect and explain a
performance improvement.

</details>


### [205] [Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT](https://arxiv.org/abs/2510.12597)
*Ilya Baldin,Michael Goodrich,Vardan Gyurjyan,Graham Heyes,Derek Howard,Yatish Kumar,David Lawrence,Brad Sawatzky,Stacey Sheldon,Carl Timmer*

Main category: cs.DC

TL;DR: The paper introduces EJFAT, an architecture utilizing FPGA acceleration for efficient edge-to-cluster computational load balancing, benefiting both experimental science and data centers.


<details>
  <summary>Details</summary>
Motivation: The aim is to streamline data processing between edge devices and compute clusters, improving throughput and latency for critical systems and workflows.

Method: The EJFAT architecture employs FPGA acceleration for data compression, fragmentation, NAT (UDP packet redirection), decompression, and reassembly for seamless integration of edge and cluster computing.

Result: The EJFAT system successfully integrates edge devices and computational clusters, enhancing experimental data processing and demonstrating synergy with DOE initiatives.

Conclusion: EJFAT supports high-throughput, low-latency operations essential for science programs and advanced data center requirements, showcasing practical applications at JLab and LBNL.

Abstract: Thomas Jefferson National Accelerator Facility (JLab) has partnered with
Energy Sciences Network (ESnet) to define and implement an edge to compute
cluster computational load balancing acceleration architecture. The ESnet-JLab
FPGA Accelerated Transport (EJFAT) architecture focuses on FPGA acceleration to
address compression, fragmentation, UDP packet destination redirection (Network
Address Translation (NAT)) and decompression and reassembly.
  EJFAT seamlessly integrates edge and cluster computing to support direct
processing of streamed experimental data. This will directly benefit the JLab
science program as well as data centers of the future that require high
throughput and low latency for both time-critical data acquisition systems and
data center workflows.
  The EJFAT project will be presented along with how it is synergistic with
other DOE activities such as an Integrated Research Infrastructure (IRI), and
recent results using data sources at JLab, an EJFAT LB at ESnet, and
computational cluster resources at Lawrence Berkeley National Laboratory
(LBNL).

</details>


### [206] [A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization of Banded Matrices](https://arxiv.org/abs/2510.12705)
*Evelyne Ringoot,Rabab Alomairy,Alan Edelman*

Main category: cs.DC

TL;DR: This paper introduces the first GPU algorithm for reducing banded matrices to bidiagonal form, significantly outperforming CPU-based implementations.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of CPU-based methods in reducing banded matrices to bidiagonal form and leverage recent advancements in GPU hardware.

Method: A GPU algorithm adapted from CPU-based multicore parallel cache-efficient algorithms, optimized using Julia's abstractions and leveraging hardware-aware performance models.

Result: The GPU implementation outperforms existing CPU libraries, with a performance increase proportional to matrix bandwidth size and showing over 100 times speedup for larger matrices.

Conclusion: Breaking memory and matrix bandwidth barriers, this work demonstrates the feasibility and superiority of GPU-based algorithms for banded matrix reduction in SVD.

Abstract: The reduction of a banded matrix to a bidiagonal form is a crucial step in
the Singular Value Decomposition (SVD), a cornerstone of scientific computing
and AI. Despite being a highly parallel algorithm, it was previously believed
to be unsuitable for GPU computation because it is memory bandwidth-bound.
Recent developments in GPU hardware, including larger L1 memory per Streaming
Multiprocessor/Compute Unit, have changed that. We present the first GPU
algorithm for reducing a banded matrix to bidiagonal form as part of the
NextLA.jl open-source software package. Our algorithm is based on previous
CPU-based multicore parallel cache-efficient bulge chasing algorithms and
adapted to optimize for GPU throughput. We leverage Julia Language's Array
abstractions and KernelAbstractions to implement a single hardware- and data
precision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for
half, single, and double precision, and examine performance optimization across
hardware architectures and data precision. We also develop a hardware-aware
performance model and identify key hyperparameters, such as inner tilewidth and
block concurrency, that govern optimal GPU execution for bandwidth-bound
workloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU
can outperform CPU-based implementations: the GPU algorithm outperforms
multithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size
1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,
the performance of the algorithm increases linearly with matrix bandwidth size,
making faster reduction of larger matrix bandwidths now also possible. With
this work, we break memory bandwidth barriers, as well as matrix bandwidth
barriers, resulting in orders-of-magnitude faster algorithms for the reduction
of banded matrices to bidiagonal form on the GPU.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [207] [Think as a Doctor: An Interpretable AI Approach for ICU Mortality Prediction](https://arxiv.org/abs/2510.11745)
*Qingwen Li,Xiaohang Zhao,Xiao Han,Hailiang Huang,Lanjuan Liu*

Main category: cs.LG

TL;DR: The paper introduces ProtoDoctor, a novel ICU mortality prediction framework that not only improves predictive accuracy but also provides intrinsic interpretability by integrating three critical elements of ICU decision-making practices.


<details>
  <summary>Details</summary>
Motivation: ICU mortality prediction requires not just predictive accuracy but also interpretability to gain clinical trust and comply with regulations. Current methods inadequately integrate the key elements of ICU decision-making practices, such as clinical course identification, demographic heterogeneity, and prognostication awareness.

Method: ProtoDoctor employs a Prognostic Clinical Course Identification module with prototype learning and a new regularization mechanism for clinical awareness, and a Demographic Heterogeneity Recognition module for modeling cohort-specific prototypes and adjusting risks.

Result: Experimental evaluations show that ProtoDoctor achieves higher predictive accuracy than existing approaches. Additionally, human evaluations highlight that its interpretations are more meaningful, trustworthy, and applicable in clinical practice.

Conclusion: ProtoDoctor successfully bridges gaps in ICU mortality prediction frameworks by integrating all critical decision-making elements and offering both improved accuracy and interpretability, making it highly useful for clinical applications.

Abstract: Intensive Care Unit (ICU) mortality prediction, which estimates a patient's
mortality status at discharge using EHRs collected early in an ICU admission,
is vital in critical care. For this task, predictive accuracy alone is
insufficient; interpretability is equally essential for building clinical trust
and meeting regulatory standards, a topic that has attracted significant
attention in information system research. Accordingly, an ideal solution should
enable intrinsic interpretability and align its reasoning with three key
elements of the ICU decision-making practices: clinical course identification,
demographic heterogeneity, and prognostication awareness. However, conventional
approaches largely focus on demographic heterogeneity, overlooking clinical
course identification and prognostication awareness. Recent prototype learning
methods address clinical course identification, yet the integration of the
other elements into such frameworks remains underexplored. To address these
gaps, we propose ProtoDoctor, a novel ICU mortality prediction framework that
delivers intrinsic interpretability while integrating all three elements of the
ICU decision-making practices into its reasoning process. Methodologically,
ProtoDoctor features two key innovations: the Prognostic Clinical Course
Identification module and the Demographic Heterogeneity Recognition module. The
former enables the identification of clinical courses via prototype learning
and achieves prognostication awareness using a novel regularization mechanism.
The latter models demographic heterogeneity through cohort-specific prototypes
and risk adjustments. Extensive empirical evaluations demonstrate that
ProtoDoctor outperforms state-of-the-art baselines in predictive accuracy.
Human evaluations further confirm that its interpretations are more clinically
meaningful, trustworthy, and applicable in ICU practice.

</details>


### [208] [GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving](https://arxiv.org/abs/2510.11769)
*Ruida Wang,Jiarui Yao,Rui Pan,Shizhe Diao,Tong Zhang*

Main category: cs.LG

TL;DR: The paper introduces Generative Adversarial Reinforcement learning (GAR) to jointly train problem composers and solvers, enhancing the training efficiency and theorem-proving performance.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and limitations in training models to solve math problems using verifiable languages, particularly issues with fixed problem sets hindering the tackling of complex problems.

Method: The authors propose the GAR framework, which employs an adversarial training loop with joint training of problem composers and solvers, introducing implicit curriculum learning to align task difficulty with solver capabilities.

Result: The GAR framework improved state-of-the-art models, like Goedel-Prover-V2-8B and DeepSeek-Prover-V2-7B, yielding a 4.20% performance improvement on MiniF2F-Test benchmarks and raising ProofNet-Test performance for DeepSeek-Prover-V2 from 22.58% to 25.81%.

Conclusion: GAR enhances model efficiency and ensures better performance on advanced theorems. Moreover, it offers a broader RL paradigm that integrates problem generation and solving in verifiable settings.

Abstract: Solving math problems through verifiable languages such as Lean has
significantly impacted both the mathematics and computer science communities.
Current state-of-the-art models are often trained with expensive online
Reinforcement Learning (RL) or expert iteration. However, these approaches rely
on fixed problem sets, which causes inefficient training and limits the model
to tackle complex problems. To overcome these limitations, we propose GAR:
Generative Adversarial Reinforcement learning, a comprehensive RL training
framework that jointly trains the problem composer and solver in an adversarial
loop. GAR introduces an implicit curriculum learning mechanism, which aligns
task difficulty with the prover's evolving capability. It thereby improves the
training efficiency and enables stronger performance of proving advanced
theorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and
DeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of
4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on
ProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR
establishes a general RL paradigm for co-evolution of problem generation and
solving under verifiable environments.

</details>


### [209] [General Fourier Feature Physics-Informed Extreme Learning Machine (GFF-PIELM) for High-Frequency PDEs](https://arxiv.org/abs/2510.12293)
*Fei Ren,Sifan Wang,Pei-Zhi Zhuang,Hai-Sui Yu,He Yang*

Main category: cs.LG

TL;DR: This paper introduces a Fourier feature physics-informed extreme learning machine (GFF-PIELM) to address shortcomings in solving PDEs with high- and variable-frequency behaviors. It enhances the conventional PIELM technique with improved predictive accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limited capability of conventional PIELM frameworks in solving partial differential equations with high-frequency and variable-frequency behaviors, which are critical in complex physics modeling.

Method: The method integrates Fourier feature mappings into the ELM network by using Fourier-based activation functions. It assigns frequency coefficients to hidden neurons and employs a novel initialization method for hyperparameters based on ELM output weights.

Result: The proposed GFF-PIELM is demonstrated to improve predictive accuracy compared to conventional PIELM across diverse numerical examples, without adding extra computational costs or complexity to the architecture.

Conclusion: GFF-PIELM extends the capabilities of PIELM to efficiently solve PDEs with high-frequency and variable-frequency behaviors. The novel initialization strategy has potential applicability in broader physics-informed machine learning frameworks.

Abstract: Conventional physics-informed extreme learning machine (PIELM) often faces
challenges in solving partial differential equations (PDEs) involving
high-frequency and variable-frequency behaviors. To address these challenges,
we propose a general Fourier feature physics-informed extreme learning machine
(GFF-PIELM). We demonstrate that directly concatenating multiple Fourier
feature mappings (FFMs) and an extreme learning machine (ELM) network makes it
difficult to determine frequency-related hyperparameters. Fortunately, we find
an alternative to establish the GFF-PIELM in three main steps. First, we
integrate a variation of FFM into ELM as the Fourier-based activation function,
so there is still one hidden layer in the GFF-PIELM framework. Second, we
assign a set of frequency coefficients to the hidden neurons, which enables ELM
network to capture diverse frequency components of target solutions. Finally,
we develop an innovative, straightforward initialization method for these
hyperparameters by monitoring the distribution of ELM output weights. GFF-PIELM
not only retains the high accuracy, efficiency, and simplicity of the PIELM
framework but also inherits the ability of FFMs to effectively handle
high-frequency problems. We carry out five case studies with a total of ten
numerical examples to highlight the feasibility and validity of the proposed
GFF-PIELM, involving high frequency, variable frequency, multi-scale behaviour,
irregular boundary and inverse problems. Compared to conventional PIELM, the
GFF-PIELM approach significantly improves predictive accuracy without
additional cost in training time and architecture complexity. Our results
confirm that that PIELM can be extended to solve high-frequency and
variable-frequency PDEs with high accuracy, and our initialization strategy may
further inspire advances in other physics-informed machine learning (PIML)
frameworks.

</details>


### [210] [Combining Euclidean and Hyperbolic Representations for Node-level Anomaly Detection](https://arxiv.org/abs/2510.11827)
*Simone Mungari,Ettore Ritacco,Pietro Sabatino*

Main category: cs.LG

TL;DR: Janus uses a combination of Euclidean and Hyperbolic Graph Neural Networks for effective node-level anomaly detection by contrasting feature and structural embeddings.


<details>
  <summary>Details</summary>
Motivation: Node-level anomaly detection is critical for tasks such as fraud detection and cybersecurity, necessitating a robust framework to handle diverse structural patterns and feature distributions.

Method: Janus introduces a multi Graph-Autoencoder framework using Euclidean and Hyperbolic embeddings, aligned with a contrastive learning objective to spot anomalies where embeddings diverge.

Result: Experiments on four real-world datasets demonstrate that Janus outperforms existing baselines in detecting subtle and complex graph anomalies.

Conclusion: By leveraging multiple geometric representations, Janus effectively identifies challenging anomalies in graphs, establishing a novel framework for robust anomaly detection.

Abstract: Node-level anomaly detection (NAD) is challenging due to diverse structural
patterns and feature distributions. As such, NAD is a critical task with
several applications which range from fraud detection, cybersecurity, to
recommendation systems. We introduce Janus, a framework that jointly leverages
Euclidean and Hyperbolic Graph Neural Networks to capture complementary aspects
of node representations. Each node is described by two views, composed by the
original features and structural features derived from random walks and
degrees, then embedded into Euclidean and Hyperbolic spaces. A multi
Graph-Autoencoder framework, equipped with a contrastive learning objective as
regularization term, aligns the embeddings across the Euclidean and Hyperbolic
spaces, highlighting nodes whose views are difficult to reconcile and are thus
likely anomalous. Experiments on four real-world datasets show that Janus
consistently outperforms shallow and deep baselines, empirically demonstrating
that combining multiple geometric representations provides a robust and
effective approach for identifying subtle and complex anomalies in graphs.

</details>


### [211] [Schrödinger bridge for generative AI: Soft-constrained formulation and convergence analysis](https://arxiv.org/abs/2510.11829)
*Jin Ma,Ying Tan,Renyuan Xu*

Main category: cs.LG

TL;DR: This paper introduces soft-constrained Schrödinger bridge problems (SCSBPs) to address stability issues in generative AI models, providing theoretical convergence guarantees.


<details>
  <summary>Details</summary>
Motivation: Generative AI models mapping simple measures to complex distributions can face instability due to the strict constraints of classical Schrödinger bridge problems (SBPs) in high-dimensional or data-scarce contexts.

Method: The authors propose the soft-constrained Schrödinger bridge problem (SCSBP), replacing hard terminal constraints with flexible penalty functions, complemented by theoretical analysis using Doob's h-transform and entropic optimal transport.

Result: Optimal solutions exist for all penalty levels, and as the penalty increases, the controls and value functions converge to the classical SBP at a linear rate, ensuring stability and robustness.

Conclusion: Soft-constrained regularization enables both robust generative modeling and flexible applications like fine-tuning and transfer learning, with established convergence guarantees.

Abstract: Generative AI can be framed as the problem of learning a model that maps
simple reference measures into complex data distributions, and it has recently
found a strong connection to the classical theory of the Schr\"odinger bridge
problems (SBPs) due partly to their common nature of interpolating between
prescribed marginals via entropy-regularized stochastic dynamics. However, the
classical SBP enforces hard terminal constraints, which often leads to
instability in practical implementations, especially in high-dimensional or
data-scarce regimes. To address this challenge, we follow the idea of the
so-called soft-constrained Schr\"odinger bridge problem (SCSBP), in which the
terminal constraint is replaced by a general penalty function. This relaxation
leads to a more flexible stochastic control formulation of McKean-Vlasov type.
  We establish the existence of optimal solutions for all penalty levels and
prove that, as the penalty grows, both the controls and value functions
converge to those of the classical SBP at a linear rate. Our analysis builds on
Doob's h-transform representations, the stability results of Schr\"odinger
potentials, Gamma-convergence, and a novel fixed-point argument that couples an
optimization problem over the space of measures with an auxiliary entropic
optimal transport problem. These results not only provide the first
quantitative convergence guarantees for soft-constrained bridges but also shed
light on how penalty regularization enables robust generative modeling,
fine-tuning, and transfer learning.

</details>


### [212] [Z0-Inf: Zeroth Order Approximation for Data Influence](https://arxiv.org/abs/2510.11832)
*Narine Kokhlikyan,Kamalika Chaudhuri,Saeed Mahloujifar*

Main category: cs.LG

TL;DR: The paper introduces a computationally efficient method for measuring the influence of training data on machine learning models, improving scalability and accuracy without relying on gradients or inverse-Hessian calculations.


<details>
  <summary>Details</summary>
Motivation: Understanding the influence of individual training examples on model behavior is crucial for tasks like data selection and model debugging. Existing methods are often inefficient or inaccurate for large models, creating a need for better approaches.

Method: The authors propose a zeroth-order approximation method that estimates data influence using only loss values from intermediate training checkpoints. This method avoids gradients and Hessian computations, making it suitable for large models and non-differentiable loss functions.

Result: The proposed method achieves superior accuracy in estimating self-influence and performs comparably or better for train-test influence in fine-tuned large language models. It also requires significantly less computational time and memory.

Conclusion: The approach enables scalable and practical analysis of training data influence, facilitating broader applications in large-scale machine learning systems.

Abstract: A critical aspect of analyzing and improving modern machine learning systems
lies in understanding how individual training examples influence a model's
predictive behavior. Estimating this influence enables critical applications,
including data selection and model debugging; in particular, self-influence,
which quantifies the influence of a training point on itself, has found many
uses in data quality assessment and outlier detection. Existing methods for
measuring data influence, however, are often impractical for large models due
to low accuracy or prohibitive computational costs: most approaches either
provide poor approximations or rely on gradients and inverse-Hessian
computations that remain challenging to scale. In this work, we introduce a
highly efficient zeroth-order approximation for estimating the influence of
training data that requires only a fraction of the time and memory footprint of
prior methods. Notably, our method relies solely on loss values of intermediate
checkpoints on the training and test data, along with the checkpoints
themselves, making it broadly applicable even when the loss function of
interest is non-differentiable. Beyond its computational efficiency, our
approach achieves superior accuracy in estimating self-influence and comparable
or improved accuracy in estimating train-test influence for fine-tuned large
language models, enabling scalable and practical analysis of how training data
shapes model behavior.

</details>


### [213] [Don't Walk the Line: Boundary Guidance for Filtered Generation](https://arxiv.org/abs/2510.11834)
*Sarah Ball,Andreas Haupt*

Main category: cs.LG

TL;DR: The paper presents Boundary Guidance, a reinforcement learning approach to fine-tune generative models paired with safety classifiers, enhancing safety and utility of outputs by steering generation away from classifier margins.


<details>
  <summary>Details</summary>
Motivation: Generative models often produce undesirable outputs due to safety classifiers pushing generations near decision boundaries, causing inefficiencies and errors.

Method: Boundary Guidance utilizes reinforcement learning to explicitly steer model generation away from the classifier's decision boundary.

Result: Improved safety and utility for generative model outputs, validated on jailbreak and ambiguous prompts using LLM-as-a-Judge evaluations.

Conclusion: Boundary Guidance offers a robust approach to optimize generation safety and effectiveness, with extensive ablations confirming its reliability across scales and designs.

Abstract: Generative models are increasingly paired with safety classifiers that filter
harmful or undesirable outputs. A common strategy is to fine-tune the generator
to reduce the probability of being filtered, but this can be suboptimal: it
often pushes the model toward producing samples near the classifier's decision
boundary, increasing both false positives and false negatives. We propose
Boundary Guidance, a reinforcement learning fine-tuning method that explicitly
steers generation away from the classifier's margin. On a benchmark of
jailbreak and ambiguous prompts, Boundary Guidance improves both the safety and
the utility of outputs, as judged by LLM-as-a-Judge evaluations. Comprehensive
ablations across model scales and reward designs demonstrate the robustness of
our approach.

</details>


### [214] [nuGPR: GPU-Accelerated Gaussian Process Regression with Iterative Algorithms and Low-Rank Approximations](https://arxiv.org/abs/2510.12128)
*Ziqi Zhao,Vivek Sarin*

Main category: cs.LG

TL;DR: The paper introduces nuGPR, a new framework to enhance the computational efficiency of Gaussian Process Regression (GPR) by combining techniques from numerical linear algebra, clustering, low-rank approximations, and parallelization.


<details>
  <summary>Details</summary>
Motivation: High computational cost in the training of Gaussian Process Regression (GPR) models limits their scalability in practical applications.

Method: The nuGPR framework integrates preconditioned conjugate gradient methods for faster linear solves, clustering for covariance matrix approximation, numerical gradient optimization to avoid backpropagation, and GPU-based parallelization using the CUDA Toolkit.

Result: nuGPR achieves up to a 2x reduction in training time and up to a 12x reduction in peak memory usage compared to existing GPU-based GPR implementations, while maintaining performance on synthetic and real-world datasets.

Conclusion: nuGPR provides a more computationally efficient approach to GPR training, making it more scalable and practical for complex datasets without sacrificing predictive accuracy.

Abstract: Gaussian Process Regression (GPR) is an important type of supervised machine
learning model with inherent uncertainty measure in its predictions. We propose
a new framework, nuGPR, to address the well-known challenge of high computation
cost associated with GPR training. Our framework includes several ideas from
numerical linear algebra to reduce the amount of computation in key steps of
GPR, and we combine them to establish an end-to-end training algorithm.
Specifically, we leverage the preconditioned conjugate gradient method to
accelerate the convergence of the linear solves required in GPR. We exploit
clustering in the input data to identify block-diagonal structure of the
covariance matrix and subsequently construct low-rank approximations of the
off-diagonal blocks. These enhancements significantly reduce the time and space
complexity of our computations. In addition, unlike other frameworks that rely
on exact differentiation, we employ numerical gradients to optimize the
hyperparameters of our GPR model, further reducing the training cost by
eliminating the need for backpropagation. Lastly, we leverage the CUDA Toolkit
to efficiently parallelize the training procedure on NVIDIA GPUs. As a result,
nuGPR reduces total training time by up to 2x and peak memory consumption by up
to 12x on various synthetic and real-world datasets when compared to the best
existing GPU-based GPR implementation.

</details>


### [215] [WaveletDiff: Multilevel Wavelet Diffusion For Time Series Generation](https://arxiv.org/abs/2510.11839)
*Yu-Hsiang Wang,Olgica Milenkovic*

Main category: cs.LG

TL;DR: WaveletDiff introduces a method for generating synthetic time series data by leveraging wavelet coefficients through multi-resolution analysis. This approach outperforms existing methods across diverse datasets and metrics.


<details>
  <summary>Details</summary>
Motivation: Current time series datasets are insufficient for many applications, and existing generation methods struggle to capture the complex, multi-scaled nature of real-world data.

Method: The WaveletDiff framework trains diffusion models on wavelet coefficients, using transformers for multi-resolution decomposition levels and energy preservation constraints based on Parseval's theorem.

Result: The model showed superior performance across six datasets from different domains, achieving smaller error scores compared to existing generative methods.

Conclusion: WaveletDiff successfully enhances synthetic time series generation, addressing limitations of time-domain and frequency-domain models.

Abstract: Time series are ubiquitous in many applications that involve forecasting,
classification and causal inference tasks, such as healthcare, finance, audio
signal processing and climate sciences. Still, large, high-quality time series
datasets remain scarce. Synthetic generation can address this limitation;
however, current models confined either to the time or frequency domains
struggle to reproduce the inherently multi-scaled structure of real-world time
series. We introduce WaveletDiff, a novel framework that trains diffusion
models directly on wavelet coefficients to exploit the inherent
multi-resolution structure of time series data. The model combines dedicated
transformers for each decomposition level with cross-level attention mechanisms
that enable selective information exchange between temporal and frequency
scales through adaptive gating. It also incorporates energy preservation
constraints for individual levels based on Parseval's theorem to preserve
spectral fidelity throughout the diffusion process. Comprehensive tests across
six real-world datasets from energy, finance, and neuroscience domains
demonstrate that WaveletDiff consistently outperforms state-of-the-art
time-domain and frequency-domain generative methods on both short and long time
series across five diverse performance metrics. For example, WaveletDiff
achieves discriminative scores and Context-FID scores that are $3\times$
smaller on average than the second-best baseline across all datasets.

</details>


### [216] [Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities](https://arxiv.org/abs/2510.11842)
*Urs Spiegelhalter,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: The study examines how to optimally balance new task adaptation and knowledge retention in language models under computational constraints using replay ratios.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the issue of retaining existing knowledge while adapting language models to new tasks without catastrophic forgetting.

Method: An empirical study is conducted using synthetic data generation, the bAbI reasoning tasks, and varying replay ratio configurations under different total token budgets.

Result: The experiments identified an optimal replay ratio configuration that balances task-specific performance with knowledge retention.

Conclusion: Practitioners can use the study's guidelines to adapt language models effectively with lower training costs while achieving good task adaptation and preserving general knowledge.

Abstract: Adapting language models to new tasks through continued pretraining faces a
fundamental trade-off: models must learn new capabilities while avoiding
catastrophic forgetting of existing knowledge. While prior work has studied
synthetic data generation techniques, the optimal replay ratios for balancing
task performance and knowledge retention under computational constraints remain
poorly understood. We present a comprehensive empirical study investigating the
interplay between replay ratio configuration and computational budget when
adapting language models to new tasks. Using the bAbI reasoning tasks as our
target objective, we apply synthetic data generation and systematically
evaluate different total token budgets and replay ratio configurations. We
analyze their effects on both task mastery and general knowledge retention. Our
experiments reveal an optimal configuration that balances task-specific
performance with general knowledge retention. Based on our findings, we provide
empirically-grounded guidelines for selecting replay ratios based on
computational budget, enabling practitioners to achieve strong task adaptation
with significantly reduced training costs.

</details>


### [217] [Evaluating Open-Source Vision-Language Models for Multimodal Sarcasm Detection](https://arxiv.org/abs/2510.11852)
*Saroj Basnet,Shafkat Farabi,Tharindu Ranasinghe,Diptesh Kanoji,Marcos Zampieri*

Main category: cs.LG

TL;DR: The paper evaluates the performance of seven vision-language models in detecting and explaining multimodal sarcasm.


<details>
  <summary>Details</summary>
Motivation: To understand how well current open-source vision-language models can detect and explain sarcasm in multimodal (image-caption) scenarios.

Method: The authors tested seven state-of-the-art vision-language models on three sarcasm datasets using zero-shot, one-shot, and few-shot prompting approaches for both sarcasm detection and explanation generation.

Result: The models demonstrated moderate effectiveness in binary sarcasm detection but struggled to generate high-quality explanations, revealing limitations without task-specific finetuning.

Conclusion: While current models show some promise in sarcasm detection, their lack of ability to generate meaningful explanations highlights the need for further improvements, particularly through finetuning.

Abstract: Recent advances in open-source vision-language models (VLMs) offer new
opportunities for understanding complex and subjective multimodal phenomena
such as sarcasm. In this work, we evaluate seven state-of-the-art VLMs - BLIP2,
InstructBLIP, OpenFlamingo, LLaVA, PaliGemma, Gemma3, and Qwen-VL - on their
ability to detect multimodal sarcasm using zero-, one-, and few-shot prompting.
Furthermore, we evaluate the models' capabilities in generating explanations to
sarcastic instances. We evaluate the capabilities of VLMs on three benchmark
sarcasm datasets (Muse, MMSD2.0, and SarcNet). Our primary objectives are
twofold: (1) to quantify each model's performance in detecting sarcastic
image-caption pairs, and (2) to assess their ability to generate human-quality
explanations that highlight the visual-textual incongruities driving sarcasm.
Our results indicate that, while current models achieve moderate success in
binary sarcasm detection, they are still not able to generate high-quality
explanations without task-specific finetuning.

</details>


### [218] [PubSub-VFL: Towards Efficient Two-Party Split Learning in Heterogeneous Environments via Publisher/Subscriber Architecture](https://arxiv.org/abs/2510.12494)
*Yi Liu,Yang Liu,Leqian Zheng,Jue Hong,Junjie Shi,Qingyou Yang,Ye Wu,Cong Wang*

Main category: cs.LG

TL;DR: The paper introduces PubSub-VFL, an optimized approach for Vertical Federated Learning (VFL) using Publisher/Subscriber architecture, enhancing computational efficiency and addressing issues of heterogeneity and latency.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to enhance the efficiency and resource utilization of Two-Party Split Learning (Vertical Federated Learning), which faces challenges in computational resource utilization and training efficiency due to synchronous dependencies and heterogeneity among participants.

Method: PubSub-VFL employs a Publisher/Subscriber (Pub/Sub) architecture combined with a hierarchical asynchronous mechanism and data parallelism, along with an optimization problem based on system profiles to mitigate training imbalances and maintain privacy.

Result: PubSub-VFL improves system efficiency and reduces training latency, achieving up to 91.07% computational resource utilization and accelerating training by 2 to 7 times compared to existing solutions while maintaining model accuracy.

Conclusion: The proposed PubSub-VFL presents a significant improvement in computational efficiency for VFL scenarios, ensures stable convergence, and supports privacy-preserving measures, as validated by theoretical analysis and extensive case studies.

Abstract: With the rapid advancement of the digital economy, data collaboration between
organizations has become a well-established business model, driving the growth
of various industries. However, privacy concerns make direct data sharing
impractical. To address this, Two-Party Split Learning (a.k.a. Vertical
Federated Learning (VFL)) has emerged as a promising solution for secure
collaborative learning. Despite its advantages, this architecture still suffers
from low computational resource utilization and training efficiency.
Specifically, its synchronous dependency design increases training latency,
while resource and data heterogeneity among participants further hinder
efficient computation. To overcome these challenges, we propose PubSub-VFL, a
novel VFL paradigm with a Publisher/Subscriber architecture optimized for
two-party collaborative learning with high computational efficiency. PubSub-VFL
leverages the decoupling capabilities of the Pub/Sub architecture and the data
parallelism of the parameter server architecture to design a hierarchical
asynchronous mechanism, reducing training latency and improving system
efficiency. Additionally, to mitigate the training imbalance caused by resource
and data heterogeneity, we formalize an optimization problem based on
participants' system profiles, enabling the selection of optimal
hyperparameters while preserving privacy. We conduct a theoretical analysis to
demonstrate that PubSub-VFL achieves stable convergence and is compatible with
security protocols such as differential privacy. Extensive case studies on five
benchmark datasets further validate its effectiveness, showing that, compared
to state-of-the-art baselines, PubSub-VFL not only accelerates training by $2
\sim 7\times$ without compromising accuracy, but also achieves a computational
resource utilization rate of up to 91.07%.

</details>


### [219] [Actor-Enriched Time Series Forecasting of Process Performance](https://arxiv.org/abs/2510.11856)
*Aurelie Leribaux,Rafael Oyamada,Johannes De Smedt,Zahra Dasht Bozorgi,Artem Polyvyanyy,Jochen De Weerdt*

Main category: cs.LG

TL;DR: This study explores incorporating actor behavior as time-varying signals into predictive models for throughput time (TT) in processes, showing significant improvements in accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance proactive decision-making in Predictive Process Monitoring (PPM) by better understanding and incorporating actor behavior as time-varying signals, given its limited exploration in previous research.

Method: The study uses real-life event logs to construct multivariate time series, including both TT and actor-centric features. Several models are trained and compared to assess the impact of adding actor behavior data.

Result: Actor-enriched models consistently outperform baseline TT-only feature models in predictive accuracy, as measured by RMSE, MAE, and R2 metrics.

Conclusion: Integrating time-dependent actor behavior data significantly improves the accuracy of throughput time predictions in Process Mining.

Abstract: Predictive Process Monitoring (PPM) is a key task in Process Mining that aims
to predict future behavior, outcomes, or performance indicators. Accurate
prediction of the latter is critical for proactive decision-making. Given that
processes are often resource-driven, understanding and incorporating actor
behavior in forecasting is crucial. Although existing research has incorporated
aspects of actor behavior, its role as a time-varying signal in PPM remains
limited. This study investigates whether incorporating actor behavior
information, modeled as time series, can improve the predictive performance of
throughput time (TT) forecasting models. Using real-life event logs, we
construct multivariate time series that include TT alongside actor-centric
features, i.e., actor involvement, the frequency of continuation, interruption,
and handover behaviors, and the duration of these behaviors. We train and
compare several models to study the benefits of adding actor behavior. The
results show that actor-enriched models consistently outperform baseline
models, which only include TT features, in terms of RMSE, MAE, and R2. These
findings demonstrate that modeling actor behavior over time and incorporating
this information into forecasting models enhances performance indicator
predictions.

</details>


### [220] [Laminar: A Scalable Asynchronous RL Post-Training Framework](https://arxiv.org/abs/2510.12633)
*Guangming Sheng,Yuxuan Tong,Borui Wan,Wang Zhang,Chaobo Jia,Xibin Wu,Yuqi Wu,Xiang Li,Chi Zhang,Yanghua Peng,Haibin Lin,Xin Liu,Chuan Wu*

Main category: cs.LG

TL;DR: The paper introduces 'Laminar,' an optimized RL post-training system for Large Language Models that improves GPU utilization and training efficiency through a fully decoupled architecture.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the scalability limitations in RL frameworks caused by skewed trajectory generation latency and inefficient global weight synchronization, which leads to GPU underutilization and suboptimal training efficiency.

Method: It utilizes trajectory-level asynchrony by implementing a decoupled system architecture. Key innovations include relay workers for asynchronous weight synchronization and dynamic repack mechanisms to enhance trajectory generation throughput and training robustness.

Result: Laminar demonstrates up to 5.48× speedup in training throughput over current RL systems, significantly reducing convergence time in a 1024-GPU cluster.

Conclusion: Laminar provides a scalable and robust solution for RL post-training in LLMs, improving GPU efficiency and accelerating convergence while ensuring fault isolation for long-duration tasks.

Abstract: Reinforcement learning (RL) post-training for Large Language Models (LLMs) is
now scaling to large clusters and running for extended durations to enhance
model reasoning performance. However, the scalability of existing RL frameworks
is limited, as extreme long-tail skewness in RL trajectory generation causes
severe GPU underutilization. Current asynchronous RL systems attempt to
mitigate this, but they rely on global weight synchronization between the actor
and all rollouts, which creates a rigid model update schedule. This global
synchronization is ill-suited for the highly skewed and evolving distribution
of trajectory generation latency in RL training, crippling training efficiency.
Our key insight is that efficient scaling requires breaking this lockstep
through trajectory-level asynchrony, which generates and consumes each
trajectory independently. We propose Laminar, a scalable and robust RL
post-training system built on a fully decoupled architecture. First, we replace
global updates with a tier of relay workers acting as a distributed parameter
service. This enables asynchronous and fine-grained weight synchronization,
allowing rollouts to pull the latest weight anytime without stalling the
actor's training loop. Second, a dynamic repack mechanism consolidates
long-tail trajectories onto a few dedicated rollouts, maximizing generation
throughput. The fully decoupled design also isolates failures, ensuring
robustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows
that Laminar achieves up to 5.48$\times$ training throughput speedup over
state-of-the-art systems, while reducing model convergence time.

</details>


### [221] [Improving Knowledge Graph Embeddings through Contrastive Learning with Negative Statements](https://arxiv.org/abs/2510.11868)
*Rita T. Sousa,Heiko Paulheim*

Main category: cs.LG

TL;DR: The paper introduces a dual-model architecture to improve knowledge graph embeddings by integrating explicit negative statements, enhancing tasks like link prediction and triple classification.


<details>
  <summary>Details</summary>
Motivation: Existing methods for knowledge graph embeddings often make inaccurate assumptions, such as treating missing triples as false, limiting their utility for real-world open-world scenarios.

Method: A dual-model architecture is proposed where one model trains on positive statements and another on negative statements. Negative samples are generated by corrupting positive ones, with each model helping score the other's samples.

Result: The approach, tested on both general-purpose and domain-specific knowledge graphs, shows superior predictive performance compared to state-of-the-art models.

Conclusion: Incorporating explicit negative knowledge into the embedding learning process enhances the utility and accuracy of knowledge graph embeddings for diverse tasks.

Abstract: Knowledge graphs represent information as structured triples and serve as the
backbone for a wide range of applications, including question answering, link
prediction, and recommendation systems. A prominent line of research for
exploring knowledge graphs involves graph embedding methods, where entities and
relations are represented in low-dimensional vector spaces that capture
underlying semantics and structure. However, most existing methods rely on
assumptions such as the Closed World Assumption or Local Closed World
Assumption, treating missing triples as false. This contrasts with the Open
World Assumption underlying many real-world knowledge graphs. Furthermore,
while explicitly stated negative statements can help distinguish between false
and unknown triples, they are rarely included in knowledge graphs and are often
overlooked during embedding training.
  In this work, we introduce a novel approach that integrates explicitly
declared negative statements into the knowledge embedding learning process. Our
approach employs a dual-model architecture, where two embedding models are
trained in parallel, one on positive statements and the other on negative
statements. During training, each model generates negative samples by
corrupting positive samples and selecting the most likely candidates as scored
by the other model. The proposed approach is evaluated on both general-purpose
and domain-specific knowledge graphs, with a focus on link prediction and
triple classification tasks. The extensive experiments demonstrate that our
approach improves predictive performance over state-of-the-art embedding
models, demonstrating the value of integrating meaningful negative knowledge
into embedding learning.

</details>


### [222] [Hierarchical Federated Learning for Crop Yield Prediction in Smart Agricultural Production Systems](https://arxiv.org/abs/2510.12727)
*Anas Abouaomar,Mohammed El hanjri,Abdellatif Kobbane,Anis Laouiti,Khalid Nafil*

Main category: cs.LG

TL;DR: This paper introduces a hierarchical federated learning architecture for smart farming and crop yield prediction, emphasizing privacy and efficient communication.


<details>
  <summary>Details</summary>
Motivation: To address challenges in smart agriculture such as data privacy and communication inefficiencies while offering tailored predictions for diverse crops.

Method: A three-layer federated learning model where farms join crop-specific clusters, train specialized models locally, and aggregate them hierarchically for global generalization.

Result: The proposed method significantly outperforms traditional machine learning models, accurately aligning with actual yield patterns across heterogeneous farming environments.

Conclusion: Hierarchical federated learning improves crop yield prediction while respecting privacy constraints and adapting efficiently to diverse agricultural scenarios.

Abstract: In this paper, we presents a novel hierarchical federated learning
architecture specifically designed for smart agricultural production systems
and crop yield prediction. Our approach introduces a seasonal subscription
mechanism where farms join crop-specific clusters at the beginning of each
agricultural season. The proposed three-layer architecture consists of
individual smart farms at the client level, crop-specific aggregators at the
middle layer, and a global model aggregator at the top level. Within each crop
cluster, clients collaboratively train specialized models tailored to specific
crop types, which are then aggregated to produce a higher-level global model
that integrates knowledge across multiple crops. This hierarchical design
enables both local specialization for individual crop types and global
generalization across diverse agricultural contexts while preserving data
privacy and reducing communication overhead. Experiments demonstrate the
effectiveness of the proposed system, showing that local and crop-layer models
closely follow actual yield patterns with consistent alignment, significantly
outperforming standard machine learning models. The results validate the
advantages of hierarchical federated learning in the agricultural context,
particularly for scenarios involving heterogeneous farming environments and
privacy-sensitive agricultural data.

</details>


### [223] [Robust Adversarial Reinforcement Learning in Stochastic Games via Sequence Modeling](https://arxiv.org/abs/2510.11877)
*Xiaohang Tang,Zhuowen Cheng,Satyabrat Kumar*

Main category: cs.LG

TL;DR: The paper introduces CART, a framework enhancing Decision Transformer (DT) robustness in adversarial stochastic games by incorporating NashQ values.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored area of adversarial robustness in reinforcement learning methods based on sequence modeling.

Method: Formulates interactions as a stage game and integrates NashQ values to improve robustness and conservativeness in Transformer policies.

Result: CART shows better minimax value estimation and higher worst-case returns across various adversarial stochastic games.

Conclusion: CART demonstrates potential in advancing adversarial robustness and conservative decision-making within DT frameworks.

Abstract: The Transformer, a highly expressive architecture for sequence modeling, has
recently been adapted to solve sequential decision-making, most notably through
the Decision Transformer (DT), which learns policies by conditioning on desired
returns. Yet, the adversarial robustness of reinforcement learning methods
based on sequence modeling remains largely unexplored. Here we introduce the
Conservative Adversarially Robust Decision Transformer (CART), to our knowledge
the first framework designed to enhance the robustness of DT in adversarial
stochastic games. We formulate the interaction between the protagonist and the
adversary at each stage as a stage game, where the payoff is defined as the
expected maximum value over subsequent states, thereby explicitly incorporating
stochastic state transitions. By conditioning Transformer policies on the NashQ
value derived from these stage games, CART generates policy that are
simultaneously less exploitable (adversarially robust) and conservative to
transition uncertainty. Empirically, CART achieves more accurate minimax value
estimation and consistently attains superior worst-case returns across a range
of adversarial stochastic games.

</details>


### [224] [ADARL: Adaptive Low-Rank Structures for Robust Policy Learning under Uncertainty](https://arxiv.org/abs/2510.11899)
*Chenliang Li,Junyu Leng,Jiaxiang Li,Youbang Sun,Shixiang Chen,Shahin Shahrampour,Alfredo Garcia*

Main category: cs.LG

TL;DR: This paper proposes Adaptive Rank Representation (AdaRL), a bi-level optimization framework for robust reinforcement learning that adapts policy complexity to the task's intrinsic dimension, avoiding computational issues and overly conservative policies.


<details>
  <summary>Details</summary>
Motivation: Current approaches in robust reinforcement learning are computationally expensive and result in conservative policies due to their reliance on nested min-max optimization.

Method: AdaRL operates on a bi-level framework: the lower-level performs policy optimization under fixed-rank constraints with dynamics sampled from a Wasserstein ball, while the upper-level adjusts rank to balance bias-variance trade-offs.

Result: Empirical testing using MuJoCo benchmarks shows AdaRL consistently outperforms fixed-rank baselines and state-of-the-art robust RL methods, converging on intrinsic task ranks.

Conclusion: AdaRL demonstrates that adaptive low-rank policy representations improve efficiency and robustness in reinforcement learning under model uncertainty.

Abstract: Robust reinforcement learning (Robust RL) seeks to handle epistemic
uncertainty in environment dynamics, but existing approaches often rely on
nested min--max optimization, which is computationally expensive and yields
overly conservative policies. We propose \textbf{Adaptive Rank Representation
(AdaRL)}, a bi-level optimization framework that improves robustness by
aligning policy complexity with the intrinsic dimension of the task. At the
lower level, AdaRL performs policy optimization under fixed-rank constraints
with dynamics sampled from a Wasserstein ball around a centroid model. At the
upper level, it adaptively adjusts the rank to balance the bias--variance
trade-off, projecting policy parameters onto a low-rank manifold. This design
avoids solving adversarial worst-case dynamics while ensuring robustness
without over-parameterization. Empirical results on MuJoCo continuous control
benchmarks demonstrate that AdaRL not only consistently outperforms fixed-rank
baselines (e.g., SAC) and state-of-the-art robust RL methods (e.g., RNAC,
Parseval), but also converges toward the intrinsic rank of the underlying
tasks. These results highlight that adaptive low-rank policy representations
provide an efficient and principled alternative for robust RL under model
uncertainty.

</details>


### [225] [Integrating Sequential and Relational Modeling for User Events: Datasets and Prediction Tasks](https://arxiv.org/abs/2510.11903)
*Rizal Fathony,Igor Melnyk,Owen Reinert,Nam H. Nguyen,Daniele Rosa,C. Bayan Bruss*

Main category: cs.LG

TL;DR: The paper highlights the gap in user event modeling by pointing out the lack of methods combining personal and relational events, introduces unified datasets for such modeling, and empirically demonstrates its benefits while releasing resources for further research.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing user event modeling, which focuses separately on personal events (sequences of user actions) and relational events (interactions between users), despite the necessity of combining both in real-world applications.

Method: The paper introduces unified datasets combining personal and relational events, proposes a formalized framework for modeling these events together, and conducts empirical evaluations showing improved modeling performance.

Result: The approach demonstrates that unified models yield better predictions by incorporating both personal and relational user events, and it also uncovers gaps in existing methods.

Conclusion: Unified modeling of user events provides demonstrable benefits, but there is significant potential to further improve these methods. The authors provide datasets and tools to aid future research in this area.

Abstract: User event modeling plays a central role in many machine learning
applications, with use cases spanning e-commerce, social media, finance,
cybersecurity, and other domains. User events can be broadly categorized into
personal events, which involve individual actions, and relational events, which
involve interactions between two users. These two types of events are typically
modeled separately, using sequence-based methods for personal events and
graph-based methods for relational events. Despite the need to capture both
event types in real-world systems, prior work has rarely considered them
together. This is often due to the convenient simplification that user behavior
can be adequately represented by a single formalization, either as a sequence
or a graph. To address this gap, there is a need for public datasets and
prediction tasks that explicitly incorporate both personal and relational
events. In this work, we introduce a collection of such datasets, propose a
unified formalization, and empirically show that models benefit from
incorporating both event types. Our results also indicate that current methods
leave a notable room for improvements. We release these resources to support
further research in unified user event modeling and encourage progress in this
direction.

</details>


### [226] [Variational Mixture of Graph Neural Experts for Alzheimer's Disease Biomarker Recognition in EEG Brain Networks](https://arxiv.org/abs/2510.11917)
*Jun-En Ding,Anna Zilverstand,Shihao Yang,Albert Chih-Chieh Yang,Feng Liu*

Main category: cs.LG

TL;DR: This paper introduces VMoGE, a method using graph neural networks and structured variational inference to improve dementia diagnosis and severity staging based on EEG data. It outperforms existing methods and connects findings to physiological insights.


<details>
  <summary>Details</summary>
Motivation: Accurate diagnosis and severity staging for dementia disorders like Alzheimer's and frontotemporal dementia are challenged by overlapping EEG signatures and limitations in existing full-band frequency analysis approaches.

Method: The proposed VMoGE model combines a multi-granularity transformer for extracting multi-scale temporal patterns across frequency bands, and a variational graph convolutional encoder with Gaussian Markov Random Field priors. It uses structured variational inference and adaptive gating for EEG frequency band analysis and biomarker discovery.

Result: VMoGE delivered superior diagnostic performance, improving AUC by 4%-10% compared to state-of-the-art methods across two datasets. It also enabled interpretable insights correlated with clinical indicators and neuropathological signatures.

Conclusion: VMoGE enhances dementia diagnosis and staging by addressing EEG challenges, improving accuracy, and uncovering physiologically meaningful biomarkers for in-depth understanding and monitoring.

Abstract: Dementia disorders such as Alzheimer's disease (AD) and frontotemporal
dementia (FTD) exhibit overlapping electrophysiological signatures in EEG that
challenge accurate diagnosis. Existing EEG-based methods are limited by
full-band frequency analysis that hinders precise differentiation of dementia
subtypes and severity stages. We propose a variational mixture of graph neural
experts (VMoGE) that integrates frequency-specific biomarker identification
with structured variational inference for enhanced dementia diagnosis and
staging. VMoGE employs a multi-granularity transformer to extract multi-scale
temporal patterns across four frequency bands, followed by a variational graph
convolutional encoder using Gaussian Markov Random Field priors. Through
structured variational inference and adaptive gating, VMoGE links neural
specialization to physiologically meaningful EEG frequency bands. Evaluated on
two diverse datasets for both subtype classification and severity staging,
VMoGE achieves superior performance with AUC improvements of +4% to +10% over
state-of-the-art methods. Moreover, VMoGE provides interpretable insights
through expert weights that correlate with clinical indicators and spatial
patterns aligned with neuropathological signatures, facilitating EEG biomarker
discovery for comprehensive dementia diagnosis and monitoring.

</details>


### [227] [Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer](https://arxiv.org/abs/2510.11926)
*Nayan Sanjay Bhatia,Pranay Kocheta,Russell Elliott,Harikrishna S. Kuttivelil,Katia Obraczka*

Main category: cs.LG

TL;DR: Locaris introduces a decoder-only large language model for indoor Wi-Fi positioning that directly maps raw Wi-Fi telemetry to device location, yielding scalable and robust performance across environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges such as sensitivity of radio signals to environmental changes, dependence on labor-intensive calibration, and performance degradation under changing conditions in indoor Wi-Fi positioning.

Method: The authors developed Locaris, a large language model fine-tuned on raw Wi-Fi telemetry data, which treats each access point measurement as a token and learns a generalizable mapping to device location without preprocessing.

Result: Experimental studies demonstrate that Locaris surpasses existing methods in accuracy, achieves sub-meter accuracy with few-shot adaptation, and remains robust under challenging scenarios such as missing access points and unseen devices.

Conclusion: Locaris offers practical indoor localization with minimal calibration, scalable usage in heterogeneous environments, and high accuracy, making it ideal for large-scale real-world deployments.

Abstract: Indoor Wi-Fi positioning remains a challenging problem due to the high
sensitivity of radio signals to environmental dynamics, channel propagation
characteristics, and hardware heterogeneity. Conventional fingerprinting and
model-based approaches typically require labor-intensive calibration and suffer
rapid performance degradation when devices, channel or deployment conditions
change. In this paper, we introduce Locaris, a decoder-only large language
model (LLM) for indoor localization. Locaris treats each access point (AP)
measurement as a token, enabling the ingestion of raw Wi-Fi telemetry without
pre-processing. By fine-tuning its LLM on different Wi-Fi datasets, Locaris
learns a lightweight and generalizable mapping from raw signals directly to
device location. Our experimental study comparing Locaris with state-of-the-art
methods consistently shows that Locaris matches or surpasses existing
techniques for various types of telemetry. Our results demonstrate that compact
LLMs can serve as calibration-free regression models for indoor localization,
offering scalable and robust cross-environment performance in heterogeneous
Wi-Fi deployments. Few-shot adaptation experiments, using only a handful of
calibration points per device, further show that Locaris maintains high
accuracy when applied to previously unseen devices and deployment scenarios.
This yields sub-meter accuracy with just a few hundred samples, robust
performance under missing APs and supports any and all available telemetry. Our
findings highlight the practical viability of Locaris for indoor positioning in
the real-world scenarios, particularly in large-scale deployments where
extensive calibration is infeasible.

</details>


### [228] [Replicable Clustering](https://arxiv.org/abs/2302.10359)
*Hossein Esfandiari,Amin Karbasi,Vahab Mirrokni,Grigoris Velegkas,Felix Zhou*

Main category: cs.LG

TL;DR: The paper introduces replicable algorithms for statistical clustering, ensuring consistent outcomes under shared randomness for k-medians, k-means, and k-centers tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the replicability concern in statistical clustering algorithms, ensuring consistent partitioning of the sample space across executions.

Method: The authors utilize approximation routines as black-boxes for statistical k-medians, k-means, and k-centers clustering problems, ensuring replicability within these models.

Result: They present replicable O(1)-approximation algorithms for k-medians and k-means with poly(d) sample complexity, and extend the method to k-centers with exponential sample complexity.

Conclusion: The study validates replicability through synthetic 2D experiments and theoretical models, showing promise in statistical clustering under shared randomness.

Abstract: We design replicable algorithms in the context of statistical clustering
under the recently introduced notion of replicability from Impagliazzo et al.
[2022]. According to this definition, a clustering algorithm is replicable if,
with high probability, its output induces the exact same partition of the
sample space after two executions on different inputs drawn from the same
distribution, when its internal randomness is shared across the executions. We
propose such algorithms for the statistical $k$-medians, statistical $k$-means,
and statistical $k$-centers problems by utilizing approximation routines for
their combinatorial counterparts in a black-box manner. In particular, we
demonstrate a replicable $O(1)$-approximation algorithm for statistical
Euclidean $k$-medians ($k$-means) with $\operatorname{poly}(d)$ sample
complexity. We also describe an $O(1)$-approximation algorithm with an
additional $O(1)$-additive error for statistical Euclidean $k$-centers, albeit
with $\exp(d)$ sample complexity. In addition, we provide experiments on
synthetic distributions in 2D using the $k$-means++ implementation from sklearn
as a black-box that validate our theoretical results.

</details>


### [229] [Efficient Restarts in Non-Stationary Model-Free Reinforcement Learning](https://arxiv.org/abs/2510.11933)
*Hiroshi Nonaka,Simon Ambrozak,Sofia R. Miskala-Dinc,Amedeo Ercole,Aviva Prins*

Main category: cs.LG

TL;DR: Proposes three new restart methods for improving model-free non-stationary reinforcement learning, addressing issues in prior approaches.


<details>
  <summary>Details</summary>
Motivation: To address shortcomings in the RestartQ-UCB algorithm, including complete forgetting of learned information post-restart and rigid scheduled restarts insensitive to environmental changes.

Method: Introduces three restart mechanisms: partial, adaptive, and selective restarts, to modify and improve RestartQ-UCB and RANDOMIZEDQ algorithms.

Result: Demonstrates that the new methods achieve near-optimal performance in diverse environments, reducing dynamic regret by up to 91% relative to RestartQ-UCB.

Conclusion: The proposed restart paradigms effectively overcome the limitations of previous approaches, significantly enhancing reinforcement learning in non-stationary settings.

Abstract: In this work, we propose three efficient restart paradigms for model-free
non-stationary reinforcement learning (RL). We identify two core issues with
the restart design of Mao et al. (2022)'s RestartQ-UCB algorithm: (1) complete
forgetting, where all the information learned about an environment is lost
after a restart, and (2) scheduled restarts, in which restarts occur only at
predefined timings, regardless of the incompatibility of the policy with the
current environment dynamics. We introduce three approaches, which we call
partial, adaptive, and selective restarts to modify the algorithms RestartQ-UCB
and RANDOMIZEDQ (Wang et al., 2025). We find near-optimal empirical performance
in multiple different environments, decreasing dynamic regret by up to $91$%
relative to RestartQ-UCB.

</details>


### [230] [Replicable Learning of Large-Margin Halfspaces](https://arxiv.org/abs/2402.13857)
*Alkis Kalavasis,Amin Karbasi,Kasper Green Larsen,Grigoris Velegkas,Felix Zhou*

Main category: cs.LG

TL;DR: The paper introduces efficient, replicable algorithms for learning large-margin halfspaces, surpassing prior methods by Impagliazzo et al. (2022). It offers algorithms with optimal or improved sample complexities and discusses trade-offs in running time and margin parameter dependency.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms for learning large-margin halfspaces suffer from inefficiencies in sample and time complexities. The aim is to design replicable algorithms that overcome these limitations, providing dimension-independent and optimized solutions.

Method: The authors develop multiple replicable algorithms, including one polynomial-time, proper algorithm with optimal sample complexity concerning accuracy, and an SGD-based method with improved efficiency in certain cases. They also leverage a DP-to-Replicability reduction to achieve better sample complexity concerning the margin parameter, though with trade-offs in execution time.

Result: The proposed algorithms demonstrate improved sample complexities concerning relevant parameters (accuracy and margin) compared to previous methods. They also provide better computational flexibility, such as SGD-based applicability. However, certain algorithms trade computational cost for optimization in specific parameters.

Conclusion: The work successfully advances the state-of-the-art in replicable algorithms for learning large-margin halfspaces, offering significant improvements in sample complexity and computational efficiency, albeit with trade-offs in some cases.

Abstract: We provide efficient replicable algorithms for the problem of learning
large-margin halfspaces. Our results improve upon the algorithms provided by
Impagliazzo, Lei, Pitassi, and Sorrell [STOC, 2022]. We design the first
dimension-independent replicable algorithms for this task which runs in
polynomial time, is proper, and has strictly improved sample complexity
compared to the one achieved by Impagliazzo et al. [2022] with respect to all
the relevant parameters. Moreover, our first algorithm has sample complexity
that is optimal with respect to the accuracy parameter $\epsilon$. We also
design an SGD-based replicable algorithm that, in some parameters' regimes,
achieves better sample and time complexity than our first algorithm. Departing
from the requirement of polynomial time algorithms, using the
DP-to-Replicability reduction of Bun, Gaboardi, Hopkins, Impagliazzo, Lei,
Pitassi, Sorrell, and Sivakumar [STOC, 2023], we show how to obtain a
replicable algorithm for large-margin halfspaces with improved sample
complexity with respect to the margin parameter $\tau$, but running time doubly
exponential in $1/\tau^2$ and worse sample complexity dependence on $\epsilon$
than one of our previous algorithms. We then design an improved algorithm with
better sample complexity than all three of our previous algorithms and running
time exponential in $1/\tau^{2}$.

</details>


### [231] [On efficiently computable functions, deep networks and sparse compositionality](https://arxiv.org/abs/2510.11942)
*Tomaso Poggio*

Main category: cs.LG

TL;DR: The paper connects efficient Turing computability with compositional DAG representations and bounded-fan-in neural networks for approximate computation of functions, ensuring polynomial size and depth.


<details>
  <summary>Details</summary>
Motivation: To establish a fundamental relationship between efficient Turing computability and sparse neural network structures for reliable function approximation at specified precision levels.

Method: Analyzes the properties of polynomial-time computable functions, demonstrates their representation using Boolean circuits, and constructs equivalent neural networks using bounded-fan-in deep architectures.

Result: Efficient Turing computability implies a bounded-fan-in Boolean circuit with polynomial size and depth, which translates into neural networks achieving high accuracy (precision scaled as $2^{-m_{\mathrm{out}}}$).

Conclusion: Efficiently computable functions can be represented and approximated using structured, sparse neural networks, bridging computational theory with modern neural approximations.

Abstract: We show that \emph{efficient Turing computability} at any fixed input/output
precision implies the existence of \emph{compositionally sparse}
(bounded-fan-in, polynomial-size) DAG representations and of corresponding
neural approximants achieving the target precision. Concretely: if
$f:[0,1]^d\to\R^m$ is computable in time polynomial in the bit-depths, then for
every pair of precisions $(n,m_{\mathrm{out}})$ there exists a bounded-fan-in
Boolean circuit of size and depth $\poly(n+m_{\mathrm{out}})$ computing the
discretized map; replacing each gate by a constant-size neural emulator yields
a deep network of size/depth $\poly(n+m_{\mathrm{out}})$ that achieves accuracy
$\varepsilon=2^{-m_{\mathrm{out}}}$. We also relate these constructions to
compositional approximation rates
\cite{MhaskarPoggio2016b,poggio_deep_shallow_2017,Poggio2017,Poggio2023HowDS}
and to optimization viewed as hierarchical search over sparse structures.

</details>


### [232] [Sculpting Latent Spaces With MMD: Disentanglement With Programmable Priors](https://arxiv.org/abs/2510.11953)
*Quentin Fruytier,Akshay Malhotra,Shahab Hamidi-Rad,Aditya Sant,Aryan Mokhtari,Sujay Sanghavi*

Main category: cs.LG

TL;DR: This paper challenges the effectiveness of KL divergence in Variational Autoencoders (VAEs) for achieving disentangled representations and introduces a new framework based on MMD for superior latent space control.


<details>
  <summary>Details</summary>
Motivation: Disentangled representations are integral to machine learning, yet VAEs struggle to enforce independent latent variables with current KL-based regularizers.

Method: The authors propose the Programmable Prior Framework using Maximum Mean Discrepancy (MMD), which enables explicit latent space sculpting and includes the novel Latent Predictability Score (LPS) for evaluation.

Result: The framework achieves state-of-the-art mutual independence on datasets like CIFAR-10 and Tiny ImageNet without sacrificing reconstruction quality.

Conclusion: This new method empowers practitioners to design latent spaces with greater control and opens pathways for deeper insights into model identifiability and causal reasoning.

Abstract: Learning disentangled representations, where distinct factors of variation
are captured by independent latent variables, is a central goal in machine
learning. The dominant approach has been the Variational Autoencoder (VAE)
framework, which uses a Kullback-Leibler (KL) divergence penalty to encourage
the latent space to match a factorized Gaussian prior. In this work, however,
we provide direct evidence that this KL-based regularizer is an unreliable
mechanism, consistently failing to enforce the target distribution on the
aggregate posterior. We validate this and quantify the resulting entanglement
using our novel, unsupervised Latent Predictability Score (LPS). To address
this failure, we introduce the Programmable Prior Framework, a method built on
the Maximum Mean Discrepancy (MMD). Our framework allows practitioners to
explicitly sculpt the latent space, achieving state-of-the-art mutual
independence on complex datasets like CIFAR-10 and Tiny ImageNet without the
common reconstruction trade-off. Furthermore, we demonstrate how this
programmability can be used to engineer sophisticated priors that improve
alignment with semantically meaningful features. Ultimately, our work provides
a foundational tool for representation engineering, opening new avenues for
model identifiability and causal reasoning.

</details>


### [233] [Y-shaped Generative Flows](https://arxiv.org/abs/2510.11955)
*Arip Asadulaev,Semyon Semenov,Abduragim Shtanchaev,Eric Moulines,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: The paper introduces Y-shaped generative flows for modeling shared transport pathways in generative models, improving efficiency and structure in probabilistic mass movement.


<details>
  <summary>Details</summary>
Motivation: Current continuous-time generative models often miss shared transport structures by using independent sample trajectories. This paper addresses the limitation by proposing a model that incorporates shared pathways.

Method: The authors propose Y-shaped generative flows using a novel velocity-powered transport cost with a sublinear exponent to encourage joint and rapid mass movement. The method is implemented using scalable neural ODE objectives.

Result: Experiments on synthetic, image, and biological datasets show that Y-flows recover hierarchical structures, improve distribution metrics over existing flow-based models, and require fewer integration steps.

Conclusion: Y-shaped generative flows enhance transport efficiency and structure awareness over traditional generative models, showcasing improved results across diverse datasets.

Abstract: Modern continuous-time generative models often induce V-shaped transport:
each sample travels independently along nearly straight trajectories from prior
to data, overlooking shared structure. We introduce Y-shaped generative flows,
which move probability mass together along shared pathways before branching to
target-specific endpoints. Our formulation is based on novel velocity-powered
transport cost with a sublinear exponent (between zero and one). this concave
dependence rewards joint and fast mass movement. Practically, we instantiate
the idea in a scalable neural ODE training objective. On synthetic, image, and
biology datasets, Y-flows recover hierarchy-aware structure, improve
distributional metrics over strong flow-based baselines, and reach targets with
fewer integration steps.

</details>


### [234] [MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics](https://arxiv.org/abs/2510.11962)
*Bowei Guo,Shengkun Tang,Cong Zeng,Zhiqiang Shen*

Main category: cs.LG

TL;DR: The paper proposes MosaicDiff, a framework leveraging adaptive pruning for more efficient diffusion model training and sampling acceleration.


<details>
  <summary>Details</summary>
Motivation: While diffusion models excel in generative tasks, existing methods have overlooked the varying learning speeds in diffusion pretraining phases, which could inform sampling acceleration strategies.

Method: MosaicDiff employs trajectory-aware structural pruning, varying aggressiveness based on different stages of learning speed during diffusion pretraining.

Result: Experiments on DiT and SDXL show significant sampling speed-ups achieved by MosaicDiff without degrading quality, surpassing previous acceleration methods.

Conclusion: MosaicDiff offers an efficient, robust way to accelerate diffusion model sampling and opens up new perspectives for training-free optimization approaches.

Abstract: Diffusion models are renowned for their generative capabilities, yet their
pretraining processes exhibit distinct phases of learning speed that have been
entirely overlooked in prior post-training acceleration efforts in the
community. In this study, we introduce a novel framework called MosaicDiff that
aligns diffusion pretraining dynamics with post-training sampling acceleration
via trajectory-aware structural pruning. Our approach leverages the observation
that the middle, fast-learning stage of diffusion pretraining requires more
conservative pruning to preserve critical model features, while the early and
later, slow-learning stages benefit from a more aggressive pruning strategy.
This adaptive pruning mechanism is the first to explicitly mirror the inherent
learning speed variations of diffusion pretraining, thereby harmonizing the
model's inner training dynamics with its accelerated sampling process.
Extensive experiments on DiT and SDXL demonstrate that our method achieves
significant speed-ups in sampling without compromising output quality,
outperforming previous state-of-the-art methods by large margins, also
providing a new viewpoint for more efficient and robust training-free diffusion
acceleration.

</details>


### [235] [QLENS: Towards A Quantum Perspective of Language Transformers](https://arxiv.org/abs/2510.11963)
*Aditya Gupta,Kirandeep Kaur,Vinayak Gupta*

Main category: cs.LG

TL;DR: This paper introduces QLENS, a physics-based framework inspired by quantum mechanics to model the inference process of Transformers in natural language processing and sheds light on layer-by-layer prediction dynamics.


<details>
  <summary>Details</summary>
Motivation: The work is motivated by the interpretability gap in understanding how each Transformer layer contributes to evolving states during inference, with inspiration from the parallels between probabilistic language models and quantum mechanics.

Method: QLENS reformulates the latent activations of a Transformer as quantum-inspired state vectors evolving in a Hilbert space, with layers represented as unitary operators informed by Hamiltonian dynamics and output probabilities extracted using the Born rule.

Result: A toy Transformer model is used as a proof-of-concept under QLENS, highlighting how individual layers influence prediction trajectories within this quantum-inspired framework.

Conclusion: This paper lays the groundwork for leveraging insights from physics to enhance the interpretability of Transformer models, proposing a new paradigm for understanding their inference process through cross-domain methodologies.

Abstract: In natural language processing, current methods for understanding
Transformers are successful at identifying intermediate predictions during a
model's inference. However, these approaches function as limited diagnostic
checkpoints, lacking a mathematical framework for mechanistically modeling how
each layer facilitates transitions between these evolving states. This
interpretability gap and past successes of interdisciplinary outlooks inspire
us to turn to physics in search of a descriptive mathematical framework for
Transformers. We observe that language models are intrinsically probabilistic,
an attribute that is echoed in the core postulates of quantum mechanics. This
parallel inspires us to translate insights from this discipline to that of
natural language processing. Towards this objective, we propose QLENS a novel
attempt to develop a physics-based perspective on the Transformer generation
process. Under QLENS, a Transformer is studied by converting its latent
activations into a state vector in a Hilbert space derived from the model's
output units. This state subsequently evolves through hidden layers -
reformulated as unitary operators and analogously defined Hamiltonians - during
inference. The model's final probability distribution is obtained by applying
the Born rule to the end state using a specific measurement operator. To
demonstrate QLENS's potential, we conduct a proof-of-concept by probing a toy
Transformer to investigate the influence of individual layers in a model's
prediction trajectory. We present our work as a foundation for cross-domain
insights to be leveraged towards a broader understanding of Transformers.

</details>


### [236] [Mamaba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning](https://arxiv.org/abs/2510.12026)
*Junsoo Oh,Wei Huang,Taiji Suzuki*

Main category: cs.LG

TL;DR: The paper analyzes the theoretical mechanism behind Mamba's efficient in-context learning, especially for tasks described by low-dimensional nonlinear target functions.


<details>
  <summary>Details</summary>
Motivation: Understanding the theoretical underpinnings of Mamba's empirical success in computational efficiency and in-context learning is necessary to bridge the gap between its practical success and theoretical understanding.

Method: The study focuses on analyzing Mamba's in-context learning by examining its ability to handle single-index models through test-time feature learning. The analysis emphasizes the nonlinear gating mechanism as a key driver of performance.

Result: The analysis establishes Mamba's lower sample complexity at test time, surpassing linear Transformers and achieving a rate comparable to nonlinear Transformers while underscoring the significance of nonlinear gating.

Conclusion: Mamba's nonlinear gating mechanism is critical for feature extraction and achieving optimal computational and learning performance, demonstrating its theoretical potential and efficiency.

Abstract: Mamba, a recently proposed linear-time sequence model, has attracted
significant attention for its computational efficiency and strong empirical
performance. However, a rigorous theoretical understanding of its underlying
mechanisms remains limited. In this work, we provide a theoretical analysis of
Mamba's in-context learning (ICL) capability by focusing on tasks defined by
low-dimensional nonlinear target functions. Specifically, we study in-context
learning of a single-index model $y \approx g_*(\langle \boldsymbol{\beta},
\boldsymbol{x} \rangle)$, which depends on only a single relevant direction
$\boldsymbol{\beta}$, referred to as feature. We prove that Mamba, pretrained
by gradient-based methods, can achieve efficient ICL via test-time feature
learning, extracting the relevant direction directly from context examples.
Consequently, we establish a test-time sample complexity that improves upon
linear Transformers -- analyzed to behave like kernel methods -- and is
comparable to nonlinear Transformers, which have been shown to surpass the
Correlational Statistical Query (CSQ) lower bound and achieve near
information-theoretically optimal rate in previous works. Our analysis reveals
the crucial role of the nonlinear gating mechanism in Mamba for feature
extraction, highlighting it as the fundamental driver behind Mamba's ability to
achieve both computational efficiency and high performance.

</details>


### [237] [Learning Dynamics of VLM Finetuning](https://arxiv.org/abs/2510.11978)
*Jusheng Zhang,Kaitong Cai,Jing Yang,Keze Wang*

Main category: cs.LG

TL;DR: This paper introduces a novel method for robust vision-language model alignment aimed at mitigating brittle preference-based finetuning by optimizing training dynamics.


<details>
  <summary>Details</summary>
Motivation: Preference-based finetuning of vision-language models is prone to issues due to trivial negatives that destabilize training, necessitating a more stable optimization framework.

Method: The paper proposes a two-stage method: Stage 1 performs supervised finetuning with low-weight smoothed supervision using gentle negatives, while Stage 2 employs a Dynamic Policy Optimization (DPO) approach with cooling weights based on average token log-probabilities to handle negatives effectively.

Result: The CW-DPO method shows improved stability, calibration, pairwise win-rates, and faster convergence compared to previous approaches like SFT-only and vanilla DPO across diverse vision-language tasks.

Conclusion: The cooling-weight mechanism and smoothing before preference cooling form a general and effective principle for robust vision-language model alignment.

Abstract: Preference-based finetuning of vision--language models (VLMs) is brittle:
trivially wrong negatives inject uninformative gradients that destabilize
training. We recast alignment as \textbf{learning-dynamics--aware optimization}
and introduce \textbf{Cooling-Weighted DPO (CW-DPO)}, a two-stage recipe that
explicitly models and exploits the training trajectory. \textbf{Stage 1}
performs supervised finetuning with \textbf{gentle negatives}:
\textbf{low-weight smoothed supervision} that regularizes the base policy and
curbs overconfidence without explicit penalties. \textbf{Stage 2} applies a DPO
objective in which the \textbf{negative term is scaled by a cooling weight}
computed from the model's \textbf{average token log-probability} on each
negative, suppressing uninformative gradients from easy or off-distribution
samples while preserving signal from hard negatives. In practice, we emphasize
\textbf{on-policy negatives} and allow \textbf{mixed negatives} by blending a
controllable fraction of dataset negatives to maintain contrast freshness.
Throughout, we instrument training with $\Delta\!\log p$ probes on positives
and negatives as first-class signals for early stopping, curriculum design, and
failure diagnosis. Across diverse VLM tasks, CW-DPO yields \textbf{more stable
optimization}, \textbf{better calibration}, and \textbf{higher pairwise
win-rates} than SFT-only and vanilla DPO, while \textbf{converging in fewer
steps}. Ablations isolate the \textbf{cooling-weight mechanism} as the primary
driver of these gains and show complementary benefits from mixing on-policy and
dataset negatives. Taken together, our results show that \textbf{smoothing
learning dynamics before cooling preferences} is a simple, general principle
for robust VLM alignment.

</details>


### [238] [Learning by Steering the Neural Dynamics: A Statistical Mechanics Perspective](https://arxiv.org/abs/2510.11984)
*Mattia Scardecchia*

Main category: cs.LG

TL;DR: The paper develops a biologically plausible learning algorithm for neural networks, leveraging fixed-point dynamics, local plasticity, and phase transitions identified through statistical mechanics.


<details>
  <summary>Details</summary>
Motivation: To address the gap between artificial neural networks and biological ones, focusing on energy-efficient and robust learning mechanisms while solving the credit assignment problem without backpropagation.

Method: Statistical mechanics is used to analyze asymmetrical recurrent networks, deriving fixed-point dynamics and a phase transition framework. A biologically plausible algorithm maps inputs to fixed points via transient stimuli and local plasticity.

Result: The proposed algorithm successfully learns on benchmarks like MNIST, demonstrating hierarchical representation capabilities and enhanced hetero-association capacity across architectures.

Conclusion: This study bridges AI and neuroscience by introducing biologically inspired neural dynamics and learning mechanisms, emphasizing the importance of phase transitions and offering new avenues for AI inspired by the cortex.

Abstract: Despite the striking successes of deep neural networks trained with
gradient-based optimization, these methods differ fundamentally from their
biological counterparts. This gap raises key questions about how nature
achieves robust, sample-efficient learning at minimal energy costs and solves
the credit-assignment problem without backpropagation. We take a step toward
bridging contemporary AI and computational neuroscience by studying how neural
dynamics can support fully local, distributed learning that scales to simple
machine-learning benchmarks. Using tools from statistical mechanics, we
identify conditions for the emergence of robust dynamical attractors in random
asymmetric recurrent networks. We derive a closed-form expression for the
number of fixed points as a function of self-coupling strength, and we reveal a
phase transition in their structure: below a critical self-coupling, isolated
fixed points coexist with exponentially many narrow clusters showing the
overlap-gap property; above it, subdominant yet dense and extensive clusters
appear. These fixed points become accessible, including to a simple
asynchronous dynamical rule, after an algorithm-dependent self-coupling
threshold. Building on this analysis, we propose a biologically plausible
algorithm for supervised learning with any binary recurrent network. Inputs are
mapped to fixed points of the dynamics, by relaxing under transient external
stimuli and stabilizing the resulting configurations via local plasticity. We
show that our algorithm can learn an entangled version of MNIST, leverages
depth to develop hierarchical representations and increase hetero-association
capacity, and is applicable to several architectures. Finally, we highlight the
strong connection between algorithm performance and the unveiled phase
transition, and we suggest a cortex-inspired alternative to self-couplings for
its emergence.

</details>


### [239] [Nonlinear discretizations and Newton's method: characterizing stationary points of regression objectives](https://arxiv.org/abs/2510.11987)
*Conor Rowan*

Main category: cs.LG

TL;DR: The paper investigates why training neural networks with exact second-order methods (using the true Hessian) fails, despite their theoretical benefits over quasi-Newton approaches.


<details>
  <summary>Details</summary>
Motivation: To explore the practical challenges and failure modes of using exact second-order optimization methods for training neural networks, as only quasi-Newton approximations have been studied extensively.

Method: Analyzing failure modes and their underlying causes when employing the true Hessian for neural network training, with a focus on geometry and stationary point distributions in the loss landscape.

Result: It is shown that training fails reliably with exact curvature, providing insights into nonlinear discretizations and questioning the belief about the prevalence of local minima in the loss landscape.

Conclusion: Using the true Hessian for neural networks does not work as theoretically expected due to geometric and stationary point distribution issues, challenging some existing assumptions about the landscape of loss functions.

Abstract: Second-order methods are emerging as promising alternatives to standard
first-order optimizers such as gradient descent and ADAM for training neural
networks. Though the advantages of including curvature information in computing
optimization steps have been celebrated in the scientific machine learning
literature, the only second-order methods that have been studied are
quasi-Newton, meaning that the Hessian matrix of the objective function is
approximated. Though one would expect only to gain from using the true Hessian
in place of its approximation, we show that neural network training reliably
fails when relying on exact curvature information. The failure modes provide
insight both into the geometry of nonlinear discretizations as well as the
distribution of stationary points in the loss landscape, leading us to question
the conventional wisdom that the loss landscape is replete with local minima.

</details>


### [240] [Cautious Weight Decay](https://arxiv.org/abs/2510.12402)
*Lizhang Chen,Jonathan Li,Kaizhao Liang,Baiyu Su,Cong Xie,Nuo Wang Pierse,Chen Liang,Ni Lao,Qiang Liu*

Main category: cs.LG

TL;DR: Cautious Weight Decay (CWD) is a simple adjustment to optimization algorithms that applies weight decay only to parameters aligned with optimizer updates, improving final performance metrics in large-scale tasks.


<details>
  <summary>Details</summary>
Motivation: Current weight decay approaches implicitly optimize a regularized objective, potentially deviating from the original unmodified objective.

Method: CWD applies weight decay selectively—only to parameters with updates aligned in sign. This does not modify the original loss objective and admits a bilevel optimization perspective.

Result: Results on large-scale language model pre-training and ImageNet classification show consistent improvements in final loss and accuracy with optimizers like AdamW, Lion, and Muon.

Conclusion: CWD is an easy-to-implement, optimizer-agnostic approach that offers better performance across diverse tasks without introducing new hyperparameters or requiring tuning.

Abstract: We introduce Cautious Weight Decay (CWD), a one-line, optimizer-agnostic
modification that applies weight decay only to parameter coordinates whose
signs align with the optimizer update. Unlike standard decoupled decay, which
implicitly optimizes a regularized or constrained objective, CWD preserves the
original loss and admits a bilevel interpretation: it induces sliding-mode
behavior upon reaching the stationary manifold, allowing it to search for
locally Pareto-optimal stationary points of the unmodified objective. In
practice, CWD is a drop-in change for optimizers such as AdamW, Lion, and Muon,
requiring no new hyperparameters or additional tuning. For language model
pre-training and ImageNet classification, CWD consistently improves final loss
and accuracy at million- to billion-parameter scales.

</details>


### [241] [CrossAD: Time Series Anomaly Detection with Cross-scale Associations and Cross-window Modeling](https://arxiv.org/abs/2510.12489)
*Beibu Li,Qichao Shentu,Yang Shu,Hui Zhang,Ming Li,Ning Jin,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: CrossAD introduces a framework addressing anomaly detection in time series by emphasizing cross-scale and cross-window modeling. It achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods fail to dynamically model cross-scale associations and are limited by fixed sliding windows, which hinders their ability to capture comprehensive contextual information.

Method: CrossAD uses a cross-scale reconstruction approach to capture associations between fine-grained and coarser series. Additionally, it incorporates a query library and multi-scale global context to surpass the limitations of fixed window sizes.

Result: Experiments on real-world datasets across nine evaluation metrics demonstrated the method’s state-of-the-art performance in detecting anomalies.

Conclusion: CrossAD effectively addresses gaps in multi-scale and sliding window modeling for time series anomaly detection, enhancing detection performance through advanced cross-scale and contextual modeling techniques.

Abstract: Time series anomaly detection plays a crucial role in a wide range of
real-world applications. Given that time series data can exhibit different
patterns at different sampling granularities, multi-scale modeling has proven
beneficial for uncovering latent anomaly patterns that may not be apparent at a
single scale. However, existing methods often model multi-scale information
independently or rely on simple feature fusion strategies, neglecting the
dynamic changes in cross-scale associations that occur during anomalies.
Moreover, most approaches perform multi-scale modeling based on fixed sliding
windows, which limits their ability to capture comprehensive contextual
information. In this work, we propose CrossAD, a novel framework for time
series Anomaly Detection that takes Cross-scale associations and Cross-window
modeling into account. We propose a cross-scale reconstruction that
reconstructs fine-grained series from coarser series, explicitly capturing
cross-scale associations. Furthermore, we design a query library and
incorporate global multi-scale context to overcome the limitations imposed by
fixed window sizes. Extensive experiments conducted on multiple real-world
datasets using nine evaluation metrics validate the effectiveness of CrossAD,
demonstrating state-of-the-art performance in anomaly detection.

</details>


### [242] [Your VAR Model is Secretly an Efficient and Explainable Generative Classifier](https://arxiv.org/abs/2510.12060)
*Yi-Chung Chen,David I. Inouye,Jing Gao*

Main category: cs.LG

TL;DR: This paper explores an alternative to diffusion-based generative classifiers using a novel visual autoregressive (VAR) method and Adaptive VAR Classifier (A-VARC$^+$) for faster and more robust classification.


<details>
  <summary>Details</summary>
Motivation: To address scalability limitations and broaden understanding of generative classifiers by exploring alternatives to diffusion-based models.

Method: The proposed method introduces visual autoregressive (VAR) modeling to generative classification and enhances it using the Adaptive VAR Classifier$^+$ (A-VARC$^+$).

Result: VAR-based classifiers are faster, more accurate, and offer unique properties such as visual explainability and resistance to catastrophic forgetting.

Conclusion: VAR modeling provides a promising alternative to diffusion approaches, improving inference speed, accuracy, and introducing novel classifier properties.

Abstract: Generative classifiers, which leverage conditional generative models for
classification, have recently demonstrated desirable properties such as
robustness to distribution shifts. However, recent progress in this area has
been largely driven by diffusion-based models, whose substantial computational
cost severely limits scalability. This exclusive focus on diffusion-based
methods has also constrained our understanding of generative classifiers. In
this work, we propose a novel generative classifier built on recent advances in
visual autoregressive (VAR) modeling, which offers a new perspective for
studying generative classifiers. To further enhance its performance, we
introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a
superior trade-off between accuracy and inference speed, thereby significantly
improving practical applicability. Moreover, we show that the VAR-based method
exhibits fundamentally different properties from diffusion-based methods. In
particular, due to its tractable likelihood, the VAR-based classifier enables
visual explainability via token-wise mutual information and demonstrates
inherent resistance to catastrophic forgetting in class-incremental learning
tasks.

</details>


### [243] [The Robustness of Differentiable Causal Discovery in Misspecified Scenarios](https://arxiv.org/abs/2510.12503)
*Huiyang Yi,Yanyan He,Duxin Chen,Mingyu Kang,He Wang,Wenwu Yu*

Main category: cs.LG

TL;DR: This paper benchmarks differentiable causal discovery methods against model assumption violations in real-world data.


<details>
  <summary>Details</summary>
Motivation: Causal discovery is limited by reliance on unverifiable assumptions, which hinders its practical application.

Method: The study evaluates several causal discovery algorithms under eight model assumption violations and examines theoretical explanations behind their robustness.

Result: Differentiable causal discovery methods display robustness in challenging scenarios but struggle with scale variation.

Conclusion: The findings set a benchmark for evaluating causal discovery methods, enhancing their reliability for real-world applications.

Abstract: Causal discovery aims to learn causal relationships between variables from
targeted data, making it a fundamental task in machine learning. However,
causal discovery algorithms often rely on unverifiable causal assumptions,
which are usually difficult to satisfy in real-world data, thereby limiting the
broad application of causal discovery in practical scenarios. Inspired by these
considerations, this work extensively benchmarks the empirical performance of
various mainstream causal discovery algorithms, which assume i.i.d. data, under
eight model assumption violations. Our experimental results show that
differentiable causal discovery methods exhibit robustness under the metrics of
Structural Hamming Distance and Structural Intervention Distance of the
inferred graphs in commonly used challenging scenarios, except for scale
variation. We also provide the theoretical explanations for the performance of
differentiable causal discovery methods. Finally, our work aims to
comprehensively benchmark the performance of recent differentiable causal
discovery methods under model assumption violations, and provide the standard
for reasonable evaluation of causal discovery, as well as to further promote
its application in real-world scenarios.

</details>


### [244] [MEASURE: Multi-scale Minimal Sufficient Representation Learning for Domain Generalization in Sleep Staging](https://arxiv.org/abs/2510.12070)
*Sangmin Jo,Jee Seok Yoon,Wootaek Jeong,Kwanseok Oh,Heung-Il Suk*

Main category: cs.LG

TL;DR: The paper proposes the MEASURE framework to address challenges in automatic sleep staging due to variability in physiological signals, leveraging multi-scale minimal sufficient representation learning.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models struggle with generalizing to unseen subjects due to variability in physiological signals during sleep staging, which impacts performance, especially in out-of-distribution scenarios.

Method: The authors introduce the MEASURE framework that addresses excess domain-relevant information using multi-scale representation learning to preserve essential temporal and spectral features while minimizing irrelevant domain-specific data.

Result: The MEASURE framework consistently outperformed state-of-the-art methods on benchmark sleep staging datasets like SleepEDF-20 and MASS.

Conclusion: MEASURE effectively enhances sleep staging performance by reducing domain-relevant variability and improving generalization, offering consistent results across datasets.

Abstract: Deep learning-based automatic sleep staging has significantly advanced in
performance and plays a crucial role in the diagnosis of sleep disorders.
However, those models often struggle to generalize on unseen subjects due to
variability in physiological signals, resulting in degraded performance in
out-of-distribution scenarios. To address this issue, domain generalization
approaches have recently been studied to ensure generalized performance on
unseen domains during training. Among those techniques, contrastive learning
has proven its validity in learning domain-invariant features by aligning
samples of the same class across different domains. Despite its potential, many
existing methods are insufficient to extract adequately domain-invariant
representations, as they do not explicitly address domain characteristics
embedded within the unshared information across samples. In this paper, we
posit that mitigating such domain-relevant attributes-referred to as excess
domain-relevant information-is key to bridging the domain gap. However, the
direct strategy to mitigate the domain-relevant attributes often overfits
features at the high-level information, limiting their ability to leverage the
diverse temporal and spectral information encoded in the multiple feature
levels. To address these limitations, we propose a novel MEASURE (Multi-scalE
minimAl SUfficient Representation lEarning) framework, which effectively
reduces domain-relevant information while preserving essential temporal and
spectral features for sleep stage classification. In our exhaustive experiments
on publicly available sleep staging benchmark datasets, SleepEDF-20 and MASS,
our proposed method consistently outperformed state-of-the-art methods. Our
code is available at : https://github.com/ku-milab/Measure

</details>


### [245] [Topological Signatures of ReLU Neural Network Activation Patterns](https://arxiv.org/abs/2510.12700)
*Vicente Bosca,Tatum Rask,Sunia Tanweer,Andrew R. Tawfeek,Branden Stone*

Main category: cs.LG

TL;DR: The paper studies topological aspects of ReLU neural network activation patterns, focusing on the dual graph's Fiedler partition correlation with decision boundaries and cellular decomposition's homology behavior.


<details>
  <summary>Details</summary>
Motivation: To understand how the polytope decomposition induced by ReLU neural networks relates to their decision-making and training behaviors.

Method: Analyzed ReLU neural networks' polytope decomposition, investigated Fiedler partitions of the dual graph for classification, and computed cellular decomposition homology for regression tasks.

Result: Found a correlation between Fiedler partitions and decision boundaries in binary classification, and identified patterns between training loss and cell count in regression tasks.

Conclusion: Topological features like the Fiedler partition and polyhedral cell-count offer insights into ReLU neural networks' decision processes and training dynamics.

Abstract: This paper explores the topological signatures of ReLU neural network
activation patterns. We consider feedforward neural networks with ReLU
activation functions and analyze the polytope decomposition of the feature
space induced by the network. Mainly, we investigate how the Fiedler partition
of the dual graph and show that it appears to correlate with the decision
boundary -- in the case of binary classification. Additionally, we compute the
homology of the cellular decomposition -- in a regression task -- to draw
similar patterns in behavior between the training loss and polyhedral
cell-count, as the model is trained.

</details>


### [246] [Influence Dynamics and Stagewise Data Attribution](https://arxiv.org/abs/2510.12071)
*Jin Hwa Lee,Matthew Smith,Maxwell Adam,Jesse Hoogland*

Main category: cs.LG

TL;DR: The paper discusses a novel framework for stagewise training data attribution (TDA) to track dynamic influence changes in neural networks during distinct learning stages.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current TDA methods that assume static influence while neural networks learn in evolving, stagewise patterns.

Method: The framework is based on singular learning theory, featuring analytical and empirical validation using toy models and large-scale language models.

Result: Influence shifts were found to be non-monotonic, including sign flips and peaks during developmental transitions, which were validated in both toy models and language models at scale.

Conclusion: Stagewise TDA reveals dynamic, stage-aligned influences, enhancing understanding of how neural networks develop semantic hierarchies over training.

Abstract: Current training data attribution (TDA) methods treat the influence one
sample has on another as static, but neural networks learn in distinct stages
that exhibit changing patterns of influence. In this work, we introduce a
framework for stagewise data attribution grounded in singular learning theory.
We predict that influence can change non-monotonically, including sign flips
and sharp peaks at developmental transitions. We first validate these
predictions analytically and empirically in a toy model, showing that dynamic
shifts in influence directly map to the model's progressive learning of a
semantic hierarchy. Finally, we demonstrate these phenomena at scale in
language models, where token-level influence changes align with known
developmental stages.

</details>


### [247] [GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs](https://arxiv.org/abs/2510.12085)
*Heng Zhang,Tianyi Zhang,Yuling Shi,Xiaodong Gu,Yaomin Shen,Haochen You,Zijian Zhang,Yilei Yuan,Jin Huang*

Main category: cs.LG

TL;DR: The paper introduces GraphShaper, a geometry-aware framework aimed at preserving geometric diversity of graph structures, addressing performance issues at structural boundaries in graph foundation models.


<details>
  <summary>Details</summary>
Motivation: The paper was motivated by the observation that current graph foundation models suffer from performance losses at structural boundaries due to their reliance on single Euclidean encoding spaces, which cannot sufficiently accommodate diverse geometric needs like hierarchical branching and cyclic patterns.

Method: The proposed GraphShaper framework employs expert networks that specialize in different geometries (e.g., hyperbolic and spherical). It dynamically calculates fusion weights to adaptively integrate these geometric properties based on local structural characteristics before aligning with text embeddings.

Result: GraphShaper demonstrates significant accuracy improvements in zero-shot settings, achieving 9.47% better performance on citation networks and 7.63% on social networks.

Conclusion: The paper concludes that incorporating geometry-aware adaptations through GraphShaper resolves geometric conflicts at structural boundaries, enhancing the structural integrity and performance of graph foundation models.

Abstract: Graph foundation models represent a transformative paradigm for learning
transferable representations across diverse graph domains. Recent methods
leverage large language models to unify graph and text modalities into a shared
representation space using contrastive learning. However, systematic
evaluations reveal significant performance degradation at structural boundaries
where distinct topological patterns converge, with accuracy losses exceeding 20
percentage points. This issue arises from a key limitation: current methods
assume all graph structures can be encoded within a single Euclidean space. In
reality, tree structures require hyperbolic geometry to preserve hierarchical
branching, while cyclic patterns depend on spherical geometry for closure
properties. At structural boundaries, nodes experience conflicting geometric
constraints that uniform encoding spaces cannot resolve. This raises a crucial
challenge: \textbf{Can alignment frameworks be designed to respect the
intrinsic geometric diversity of graph structures?} We introduce
\textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding
through multi-geometric specialization. Our approach employs expert networks
tailored to different geometric spaces, dynamically computing fusion weights to
adaptively integrate geometric properties based on local structural
characteristics. This adaptive fusion preserves structural integrity before
alignment with text embeddings. Extensive experiments demonstrate that
GraphShaper achieves 9.47\% accuracy improvements on citation networks and
7.63\% on social networks in zero-shot settings.

</details>


### [248] [H4G: Unlocking Faithful Inference for Zero-Shot Graph Learning in Hyperbolic Space](https://arxiv.org/abs/2510.12094)
*Heng Zhang,Tianyi Zhang,Zijun Liu,Yuling Shi,Yaomin Shen,Haochen You,Haichuan Hu,Lubin Gan,Jin Huang*

Main category: cs.LG

TL;DR: This paper introduces H4G, a new framework for zero-shot graph learning, addressing over-abstraction issues in existing methods by adjusting embedding radii for better fine-grained structural representation.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the over-abstraction problem in existing graph learning methods, which lose local patterns needed for accurate predictions, especially in heterophilic graphs.

Method: The proposed method, H4G, uses learnable block-diagonal scaling matrices and Möbius matrix multiplication to reduce embedding radii, retaining fine-grained structural details while maintaining global representation.

Result: Experiments demonstrate that H4G achieves state-of-the-art performance, improving zero-shot learning by 12.8% on heterophilic graphs and 8.4% on homophilic graphs.

Conclusion: H4G effectively captures fine-grained structural details, advancing faithful multi-scale representation and enhancing zero-shot graph learning performance with minimal computational cost.

Abstract: Text-attributed graphs are widely used across domains, offering rich
opportunities for zero-shot learning via graph-text alignment. However,
existing methods struggle with tasks requiring fine-grained pattern
recognition, particularly on heterophilic graphs. Through empirical and
theoretical analysis, we identify an \textbf{over-abstraction problem}: current
approaches operate at excessively large hyperbolic radii, compressing
multi-scale structural information into uniform high-level abstractions. This
abstraction-induced information loss obscures critical local patterns essential
for accurate predictions. By analyzing embeddings in hyperbolic space, we
demonstrate that optimal graph learning requires \textbf{faithful preservation}
of fine-grained structural details, better retained by representations
positioned closer to the origin. To address this, we propose \textbf{H4G}, a
framework that systematically reduces embedding radii using learnable
block-diagonal scaling matrices and M\"obius matrix multiplication. This
approach restores access to fine-grained patterns while maintaining global
receptive ability with minimal computational overhead. Experiments show H4G
achieves state-of-the-art zero-shot performance with \textbf{12.8\%}
improvement on heterophilic graphs and \textbf{8.4\%} on homophilic graphs,
confirming that radius reduction enables faithful multi-scale representation
for advancing zero-shot graph learning.

</details>


### [249] [Rethinking the Role of Dynamic Sparse Training for Scalable Deep Reinforcement Learning](https://arxiv.org/abs/2510.12096)
*Guozheng Ma,Lu Li,Zilin Wang,Haoyu Wang,Shengchao Hu,Leszek Rutkowski,Dacheng Tao*

Main category: cs.LG

TL;DR: This paper investigates the limitations in scaling deep reinforcement learning models and proposes a Module-Specific Training (MST) framework to enhance scalability and performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to address optimization pathologies such as plasticity loss which hinder the scalability of deep reinforcement learning models and also examines limitations in existing dynamic training strategies.

Method: The methodology involves a thorough evaluation across various network modules and architectures, systematically comparing dynamic approaches such as sparse-to-sparse, dense-to-sparse, and sparse-to-dense.

Result: The findings indicate that dynamic sparse training strategies provide module-specific benefits, complementing architectural improvements to achieve better scalability.

Conclusion: The proposed Module-Specific Training (MST) framework leverages insights from architectural and training strategy evaluations to deliver significant scalability improvements across reinforcement learning algorithms without requiring changes to algorithmic design.

Abstract: Scaling neural networks has driven breakthrough advances in machine learning,
yet this paradigm fails in deep reinforcement learning (DRL), where larger
models often degrade performance due to unique optimization pathologies such as
plasticity loss. While recent works show that dynamically adapting network
topology during training can mitigate these issues, existing studies have three
critical limitations: (1) applying uniform dynamic training strategies across
all modules despite encoder, critic, and actor following distinct learning
paradigms, (2) focusing evaluation on basic architectures without clarifying
the relative importance and interaction between dynamic training and
architectural improvements, and (3) lacking systematic comparison between
different dynamic approaches including sparse-to-sparse, dense-to-sparse, and
sparse-to-dense. Through comprehensive investigation across modules and
architectures, we reveal that dynamic sparse training strategies provide
module-specific benefits that complement the primary scalability foundation
established by architectural improvements. We finally distill these insights
into Module-Specific Training (MST), a practical framework that further
exploits the benefits of architectural improvements and demonstrates
substantial scalability gains across diverse RL algorithms without algorithmic
modifications.

</details>


### [250] [Chimera: State Space Models Beyond Sequences](https://arxiv.org/abs/2510.12111)
*Aakash Lahoti,Tanya Marwah,Ratish Puduppully,Albert Gu*

Main category: cs.LG

TL;DR: Chimera, a novel unified model, incorporates data topology directly into Transformer-based methods, enhancing performance across diverse domains like language, vision, and graphs.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of current Transformer-based models that require domain-specific biases to incorporate data topology.

Method: Propose Chimera, a model leveraging state-space models to capture graph topology universally without requiring positional embeddings or domain-specific inductive biases.

Result: Chimera outperforms established baselines: +0.7 points over BERT on GLUE, +2.6% over ViT on ImageNet-1k, and surpasses all metrics on Long Range Graph Benchmark.

Conclusion: Incorporating topology directly into state space models is a robust inductive bias, improving generalization and efficiency across multiple domains without relying on task-specific heuristics.

Abstract: Transformer-based deep learning methods have become the standard approach for
modeling diverse data such as sequences, images, and graphs. These methods rely
on self-attention, which treats data as an unordered set of elements. This
ignores the neighborhood structure or graph topology of the data and requires
inductive biases--such as position embeddings in sequences and images, or
random walks in graphs--to incorporate topology. However, designing such
task-specific biases requires significant effort and can introduce side effects
that hinder generalization. We introduce Chimera, a unified model that directly
incorporates data topology in a principled way, removing the need for
domain-specific biases. The key idea is that state space models--which
naturally do not require position embeddings--can be generalized to capture any
graph topology. Our experiments show that Chimera achieves strong performance
across language, vision, and graph domains, outperforming BERT on GLUE by 0.7
points, ViT on ImageNet-1k by 2.6%, and all baselines on the Long Range Graph
Benchmark. We further propose algorithmic optimizations to improve Chimera's
efficiency: (1) for Directed Acyclic Graphs, Chimera can be implemented as a
linear-time recurrence; (2) for general graphs, a simple mathematical
relaxation achieves Transformer's quadratic complexity without domain-specific
heuristics. These results validate Chimera's core contribution and support the
idea that data topology is a powerful inductive bias across modalities.

</details>


### [251] [Graph Few-Shot Learning via Adaptive Spectrum Experts and Cross-Set Distribution Calibration](https://arxiv.org/abs/2510.12140)
*Yonghao Liu,Yajun Wang,Chunli Guo,Wei Pang,Ximing Li,Fausto Giunchiglia,Xiaoyue Feng,Renchu Guan*

Main category: cs.LG

TL;DR: The paper introduces GRACE, a novel graph few-shot learning framework that improves model adaptability and generalization by addressing limitations in spectral operations and support-query set distributions.


<details>
  <summary>Details</summary>
Motivation: Current graph few-shot learning methods face challenges due to reliance on fixed spectral operations and assumptions about support-query set distribution uniformity.

Method: GRACE leverages adaptive spectrum experts for local structural adaptations and cross-set distribution calibration for better generalization in few-shot scenarios.

Result: GRACE consistently outperforms state-of-the-art methods in various graph few-shot learning scenarios.

Conclusion: The proposed framework enhances adaptability and generalization, addressing key limitations, and offers a promising solution for graph few-shot learning with limited labeled nodes.

Abstract: Graph few-shot learning has attracted increasing attention due to its ability
to rapidly adapt models to new tasks with only limited labeled nodes. Despite
the remarkable progress made by existing graph few-shot learning methods,
several key limitations remain. First, most current approaches rely on
predefined and unified graph filters (e.g., low-pass or high-pass filters) to
globally enhance or suppress node frequency signals. Such fixed spectral
operations fail to account for the heterogeneity of local topological
structures inherent in real-world graphs. Moreover, these methods often assume
that the support and query sets are drawn from the same distribution. However,
under few-shot conditions, the limited labeled data in the support set may not
sufficiently capture the complex distribution of the query set, leading to
suboptimal generalization. To address these challenges, we propose GRACE, a
novel Graph few-shot leaRning framework that integrates Adaptive spectrum
experts with Cross-sEt distribution calibration techniques. Theoretically, the
proposed approach enhances model generalization by adapting to both local
structural variations and cross-set distribution calibration. Empirically,
GRACE consistently outperforms state-of-the-art baselines across a wide range
of experimental settings. Our code can be found here.

</details>


### [252] [Fairness-Constrained Optimization Attack in Federated Learning](https://arxiv.org/abs/2510.12143)
*Harsh Kasyap,Minghong Fang,Zhuqing Liu,Carsten Maple,Somanath Tripathy*

Main category: cs.LG

TL;DR: The paper introduces an intentional fairness attack in federated learning, where a single client manipulates the model to increase bias while maintaining global accuracy, undermining fairness metrics.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of federated learning systems to poisoning attacks and to demonstrate how fairness metrics can be deliberately manipulated.

Method: A client intentionally increases fairness loss during training using an optimization problem for metrics like demographic parity and equalized odds. The attack's effectiveness is tested against robust aggregation methods using various datasets.

Result: Empirical evaluation shows that even a single malicious client can increase bias by up to 90%, illustrating the attack's potency against state-of-the-art defense mechanisms.

Conclusion: Federated learning systems remain vulnerable to sophisticated poisoning attacks that challenge fairness, highlighting the need for new defense strategies.

Abstract: Federated learning (FL) is a privacy-preserving machine learning technique
that facilitates collaboration among participants across demographics. FL
enables model sharing, while restricting the movement of data. Since FL
provides participants with independence over their training data, it becomes
susceptible to poisoning attacks. Such collaboration also propagates bias among
the participants, even unintentionally, due to different data distribution or
historical bias present in the data. This paper proposes an intentional
fairness attack, where a client maliciously sends a biased model, by increasing
the fairness loss while training, even considering homogeneous data
distribution. The fairness loss is calculated by solving an optimization
problem for fairness metrics such as demographic parity and equalized odds. The
attack is insidious and hard to detect, as it maintains global accuracy even
after increasing the bias. We evaluate our attack against the state-of-the-art
Byzantine-robust and fairness-aware aggregation schemes over different
datasets, in various settings. The empirical results demonstrate the attack
efficacy by increasing the bias up to 90\%, even in the presence of a single
malicious client in the FL system.

</details>


### [253] [Budget-constrained Active Learning to Effectively De-censor Survival Data](https://arxiv.org/abs/2510.12144)
*Ali Parsaee,Bei Jiang,Zachary Friggstad,Russell Greiner*

Main category: cs.LG

TL;DR: The paper explores budgeted learning for survival datasets, addressing challenges with censored data and improving model performance using state-of-the-art algorithms.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance of supervised learning in survival datasets with censored instances, representing real-world data collection challenges.

Method: Designing budgeted learning algorithms using experimental and theoretical approaches based on BatchBALD for survival data scenarios.

Result: The proposed method shows asymptotic equivalence in bounds and time complexity to BatchBALD and outperforms other models in several survival dataset benchmarks.

Conclusion: The framework effectively applies budgeted learning to censored survival data and demonstrates better performance in benchmarks.

Abstract: Standard supervised learners attempt to learn a model from a labeled dataset.
Given a small set of labeled instances, and a pool of unlabeled instances, a
budgeted learner can use its given budget to pay to acquire the labels of some
unlabeled instances, which it can then use to produce a model. Here, we explore
budgeted learning in the context of survival datasets, which include (right)
censored instances, where we know only a lower bound on an instance's
time-to-event. Here, that learner can pay to (partially) label a censored
instance -- e.g., to acquire the actual time for an instance [perhaps go from
(3 yr, censored) to (7.2 yr, uncensored)], or other variants [e.g., learn about
one more year, so go from (3 yr, censored) to either (4 yr, censored) or
perhaps (3.2 yr, uncensored)]. This serves as a model of real world data
collection, where follow-up with censored patients does not always lead to
uncensoring, and how much information is given to the learner model during data
collection is a function of the budget and the nature of the data itself. We
provide both experimental and theoretical results for how to apply
state-of-the-art budgeted learning algorithms to survival data and the
respective limitations that exist in doing so. Our approach provides bounds and
time complexity asymptotically equivalent to the standard active learning
method BatchBALD. Moreover, empirical analysis on several survival tasks show
that our model performs better than other potential approaches on several
benchmarks.

</details>


### [254] [Self-Verifying Reflection Helps Transformers with CoT Reasoning](https://arxiv.org/abs/2510.12157)
*Zhongwei Yu,Wannian Xia,Xue Yan,Bo Xu,Haifeng Zhang,Yali Du,Jun Wang*

Main category: cs.LG

TL;DR: The paper explores self-verifying reflection in reasoning for small transformers, showing theoretical and experimental progress while analyzing its limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand how self-verifying reflective processes in reasoning contribute to empirical improvements in transformers, especially given their limited error detection in reasoning chains.

Method: A minimalistic reasoning framework is presented for small transformers without natural language. This simplifies analytical clarity and reduces experimentation costs. Theoretical proofs and experiments were conducted to validate the role of self-verification.

Result: Self-verification in tiny transformers improves performance in both training and reflective execution tasks, achieving LLM-level results in specific domains like integer multiplication and Sudoku. Reinforcement learning enhances in-distribution performance but largely optimizes superficial patterns.

Conclusion: Integrating generative transformers with discriminative verification enhances chain-of-thought reasoning, irrespective of scaling or natural language use.

Abstract: Advanced large language models (LLMs) frequently reflect in reasoning
chain-of-thoughts (CoTs), where they self-verify the correctness of current
solutions and explore alternatives. However, given recent findings that LLMs
detect limited errors in CoTs, how reflection contributes to empirical
improvements remains unclear. To analyze this issue, in this paper, we present
a minimalistic reasoning framework to support basic self-verifying reflection
for small transformers without natural language, which ensures analytic clarity
and reduces the cost of comprehensive experiments. Theoretically, we prove that
self-verifying reflection guarantees improvements if verification errors are
properly bounded. Experimentally, we show that tiny transformers, with only a
few million parameters, benefit from self-verification in both training and
reflective execution, reaching remarkable LLM-level performance in integer
multiplication and Sudoku. Similar to LLM results, we find that reinforcement
learning (RL) improves in-distribution performance and incentivizes frequent
reflection for tiny transformers, yet RL mainly optimizes shallow statistical
patterns without faithfully reducing verification errors. In conclusion,
integrating generative transformers with discriminative verification inherently
facilitates CoT reasoning, regardless of scaling and natural language.

</details>


### [255] [Revisiting Meta-Learning with Noisy Labels: Reweighting Dynamics and Theoretical Guarantees](https://arxiv.org/abs/2510.12209)
*Yiming Zhang,Chester Holtz,Gal Mishne,Alex Cloninger*

Main category: cs.LG

TL;DR: The study delivers a detailed theoretical analysis of meta-reweighting under noisy labels, highlights its limitations, and introduces a surrogate method for performance improvement.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of training models with noisy labels, which can lead to memorization of corrupted supervision, and improve understanding and performance of meta-reweighting methods.

Method: The paper analyzes the phases of meta-reweighting under label noise using a theoretical approach and introduces a surrogate method involving mean-centering, row shifting, and label-signed modulation to improve stability and performance.

Result: The analysis reveals specific training dynamics in meta-reweighting, identifies weaknesses in post-filtering phases, and demonstrates superior performance of the proposed surrogate method compared to existing baselines.

Conclusion: The proposed surrogate method aids in achieving stable performance improvements in noisy-label settings without the computational cost of meta-reweighting, advancing understanding and practical application of such techniques.

Abstract: Learning with noisy labels remains challenging because over-parameterized
networks memorize corrupted supervision. Meta-learning-based sample reweighting
mitigates this by using a small clean subset to guide training, yet its
behavior and training dynamics lack theoretical understanding. We provide a
rigorous theoretical analysis of meta-reweighting under label noise and show
that its training trajectory unfolds in three phases: (i) an alignment phase
that amplifies examples consistent with a clean subset and suppresses
conflicting ones; (ii) a filtering phase driving noisy example weights toward
zero until the clean subset loss plateaus; and (iii) a post-filtering phase in
which noise filtration becomes perturbation-sensitive. The mechanism is a
similarity-weighted coupling between training and clean subset signals together
with clean subset training loss contraction; in the post-filtering regime where
the clean-subset loss is sufficiently small, the coupling term vanishes and
meta-reweighting loses discriminatory power. Guided by this analysis, we
propose a lightweight surrogate for meta-reweighting that integrates
mean-centering, row shifting, and label-signed modulation, yielding more stable
performance while avoiding expensive bi-level optimization. Across synthetic
and real noisy-label benchmarks, our method consistently outperforms strong
reweighting/selection baselines.

</details>


### [256] [DE3S: Dual-Enhanced Soft-Sparse-Shape Learning for Medical Early Time-Series Classification](https://arxiv.org/abs/2510.12214)
*Tao Xie,Zexi Tan,Haoyi Xiao,Binbin Sun,Yiqun Zhang*

Main category: cs.LG

TL;DR: This paper proposes DE3S, a novel method for early time-series classification in medical scenarios, focusing on the trade-off between predictive accuracy and earliness. It leverages dual-enhanced soft-shape learning to identify discriminative shapelets for better prediction.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve early classification in time-series medical problems, such as sepsis prediction, to enhance ICU efficiency and precision while addressing challenges like weak initial signals and class imbalance.

Method: The method, DE3S, introduces innovations such as dual-enhancement strategies combining temporal augmentation and attention-based enhancement, a soft shapelet sparsification mechanism, and a dual-path network architecture mixing local and global learning using MoE and Inception modules.

Result: The proposed framework achieves state-of-the-art results on six real-world medical datasets and demonstrates robustness in handling class imbalance and subject consistency. Ablation studies confirm the effectiveness of each component.

Conclusion: DE3S successfully balances accuracy and earliness in early medical time-series classification, offering a robust and interpretable solution with significant real-world implications.

Abstract: Early time-series classification (ETSC) in medical applications is crucial
for time-sensitive scenarios such as sepsis prediction in intensive care units
(ICUs), where a large number of deaths are caused by delayed prediction. ETSC
can significantly improve ICU resource utilization efficiency and healthcare
precision. However, it faces conflicting goals of accuracy and earliness, with
existing methods often trading one for the other, struggling to capture subtle
early-stage patterns due to weak initial signals and class imbalance. The key
to solve these challenges is to find shapelets, which are discriminative
subsequences (or shapes) with high interpretability in time-series
classification. This paper proposes Dual-Enhanced Soft-Sparse-Shape Learning
for Medical Early Time-Series Classification (DE3S), which introduces a novel
Dual-Enhanced Soft-Shape Learning framework to figure out shapelets precisely
through three innovations: (1) a comprehensive dual-enhancement strategy
combines traditional temporal augmentation with attention-based global temporal
enhancement for robust representation learning, (2) an attention-score-based
soft shapelet sparsification mechanism dynamically preserves discriminative
patterns while aggregating less important shapelets into representative tokens,
and (3) a dual-path Mixture of Experts Network (MoE) and Inception modules
fusion architecture where MoE performs local learning within shapelets and
multi-scale Inception modules capture global patterns across shapelets. The
framework employs weighted cross-entropy loss for class imbalance handling and
demonstrates robustness on subject-consistency datasets. Extensive experiments
on six real-world medical datasets show state-of-the-art performance, with
ablation studies confirming component efficacy.

</details>


### [257] [Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory](https://arxiv.org/abs/2510.12220)
*Hanru Bai,Weiyang Ding,Difan Zou*

Main category: cs.LG

TL;DR: This paper introduces Hierarchical Koopman Diffusion, a novel framework combining one-step image synthesis with interpretable generative trajectories, addressing limitations of current diffusion models.


<details>
  <summary>Details</summary>
Motivation: To overcome the slow sampling times of diffusion models while retaining interpretability and control in the generative process.

Method: A framework based on the Koopman operator theory that maps nonlinear diffusion processes to a latent space with globally linear dynamics, coupled with a hierarchical architecture to model multi-scale image features.

Result: The model offers competitive one-step image generation performance while enabling interpretability and fine-grained control over the generative process.

Conclusion: The proposed framework effectively bridges the gap between fast generation and interpretability, offering a new path for explainable and manipulable generative modeling.

Abstract: Diffusion models have achieved impressive success in high-fidelity image
generation but suffer from slow sampling due to their inherently iterative
denoising process. While recent one-step methods accelerate inference by
learning direct noise-to-image mappings, they sacrifice the interpretability
and fine-grained control intrinsic to diffusion dynamics, key advantages that
enable applications like editable generation. To resolve this dichotomy, we
introduce \textbf{Hierarchical Koopman Diffusion}, a novel framework that
achieves both one-step sampling and interpretable generative trajectories.
Grounded in Koopman operator theory, our method lifts the nonlinear diffusion
dynamics into a latent space where evolution is governed by globally linear
operators, enabling closed-form trajectory solutions. This formulation not only
eliminates iterative sampling but also provides full access to intermediate
states, allowing manual intervention during generation. To model the
multi-scale nature of images, we design a hierarchical architecture that
disentangles generative dynamics across spatial resolutions via scale-specific
Koopman subspaces, capturing coarse-to-fine details systematically. We
empirically show that the Hierarchical Koopman Diffusion not only achieves
competitive one-step generation performance but also provides a principled
mechanism for interpreting and manipulating the generative process through
spectral analysis. Our framework bridges the gap between fast sampling and
interpretability in diffusion models, paving the way for explainable image
synthesis in generative modeling.

</details>


### [258] [Unveiling the Vulnerability of Graph-LLMs: An Interpretable Multi-Dimensional Adversarial Attack on TAGs](https://arxiv.org/abs/2510.12233)
*Bowen Fan,Zhilin Guo,Xunkai Li,Yihan Zhou,Bing Zhou,Zhenjun Li,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: Graph Neural Networks (GNNs) are enhanced by large language models (LLMs) for text-attributed graphs but face vulnerabilities. This paper proposes IMDGA, a human-centric framework addressing adversarial attacks on both graph structure and textual features.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address vulnerabilities in text-attributed graphs (TAGs) enhanced by LLMs, which are prone to adversarial attacks affecting both graph topology and textual attributes.

Method: IMDGA introduces a multi-dimensional attack framework with three integrated modules that enable interpretability, balance, and effectiveness in adversarial perturbations.

Result: The framework achieves superior interpretability, stealth, robustness, and attack effectiveness compared to existing methods, tested across diverse graphs, datasets, and architectures.

Conclusion: The study uncovers new areas of vulnerability in Graph-LLMs and provides insights for resilience improvement, emphasizing the importance of safeguarding text-attributed graphs.

Abstract: Graph Neural Networks (GNNs) have become a pivotal framework for modeling
graph-structured data, enabling a wide range of applications from social
network analysis to molecular chemistry. By integrating large language models
(LLMs), text-attributed graphs (TAGs) enhance node representations with rich
textual semantics, significantly boosting the expressive power of graph-based
learning. However, this sophisticated synergy introduces critical
vulnerabilities, as Graph-LLMs are susceptible to adversarial attacks on both
their structural topology and textual attributes. Although specialized attack
methods have been designed for each of these aspects, no work has yet unified
them into a comprehensive approach. In this work, we propose the Interpretable
Multi-Dimensional Graph Attack (IMDGA), a novel human-centric adversarial
attack framework designed to orchestrate multi-level perturbations across both
graph structure and textual features. IMDGA utilizes three tightly integrated
modules to craft attacks that balance interpretability and impact, enabling a
deeper understanding of Graph-LLM vulnerabilities. Through rigorous theoretical
analysis and comprehensive empirical evaluations on diverse datasets and
architectures, IMDGA demonstrates superior interpretability, attack
effectiveness, stealthiness, and robustness compared to existing methods. By
exposing critical weaknesses in TAG representation learning, this work uncovers
a previously underexplored semantic dimension of vulnerability in Graph-LLMs,
offering valuable insights for improving their resilience. Our code and
resources are publicly available at
https://anonymous.4open.science/r/IMDGA-7289.

</details>


### [259] [MoRA: On-the-fly Molecule-aware Low-Rank Adaptation Framework for LLM-based Multi-Modal Molecular Assistant](https://arxiv.org/abs/2510.12245)
*Tao Yin,Xiaohong Zhang,Jiacheng Zhang,Li Huang,Zhibin Zhang,Yuansong Zeng,Jin Xie,Meng Yan*

Main category: cs.LG

TL;DR: This paper proposes MoRA, a dynamic method for adapting frozen Large Language Models (LLMs) to molecular graph tasks by generating instance-specific low-rank adaptation weights, yielding superior performance without compromising general reasoning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in existing molecular graph-LLM integration methods, which struggle with capturing instance-specific features and risk degrading the LLM's general reasoning abilities through static adaptation or fine-tuning.

Method: MoRA generates unique, low-rank adaptation weights tailored to each input molecular graph. These weights are dynamically integrated into a frozen LLM, enabling task-specific reasoning while preserving the model's general knowledge.

Result: MoRA achieves significant performance improvements in molecular tasks, such as a 14.1% relative gain in reaction prediction accuracy and a 22% error reduction in quantum property predictions, outperforming static adaptation baselines.

Conclusion: The MoRA approach successfully enhances molecular graph processing in LLMs by enabling dynamic and instance-specific adaptation, maintaining the LLM's core capabilities and improving task-specific outcomes.

Abstract: Effectively integrating molecular graph structures with Large Language Models
(LLMs) is a key challenge in drug discovery. Most existing multi-modal
alignment methods typically process these structures by fine-tuning the LLM or
adding a static adapter simultaneously. However, these approaches have two main
limitations: (1) it optimizes a shared parameter space across all molecular
inputs, limiting the model's ability to capture instance-specific structural
features; and (2) fine-tuning the LLM for molecular tasks can lead to
catastrophic forgetting, undermining its general reasoning capabilities. In
this paper, instead of static task-oriented adaptation, we propose an
instance-specific parameter space alignment approach for each molecule
on-the-fly. To this end, we introduce Molecule-aware Low-Rank Adaptation (MoRA)
that produces a unique set of low-rank adaptation weights for each input
molecular graph. These weights are then dynamically injected into a frozen LLM,
allowing the model to adapt its reasoning to the structure of each molecular
input, while preserving the LLM's core knowledge. Extensive experiments
demonstrate that on key molecular tasks, such as chemical reaction prediction
and molecular captioning, MoRA's instance-specific dynamic adaptation
outperforms statically adapted baselines, including a 14.1% relative
improvement in reaction prediction exact match and a 22% reduction in error for
quantum property prediction. The code is available at
https://github.com/jk-sounds/MoRA.

</details>


### [260] [Optimal Regularization for Performative Learning](https://arxiv.org/abs/2510.12249)
*Edwige Cyffers,Alireza Mirrokni,Marco Mondelli*

Main category: cs.LG

TL;DR: The paper explores how regularization can address performative effects in machine learning, particularly in ridge regression.


<details>
  <summary>Details</summary>
Motivation: The study aims to optimize models while accounting for potential shifts in data distribution caused by model deployment.

Method: The researchers analyze high-dimensional ridge regression under performative effects, identifying optimal regularization scaling.

Result: Performative effects may worsen test risk overall but can be advantageous in over-parameterized settings with strategic regularization.

Conclusion: Anticipating performative effects can help set optimal regularization, improving model performance under such conditions.

Abstract: In performative learning, the data distribution reacts to the deployed model
- for example, because strategic users adapt their features to game it - which
creates a more complex dynamic than in classical supervised learning. One
should thus not only optimize the model for the current data but also take into
account that the model might steer the distribution in a new direction, without
knowing the exact nature of the potential shift. We explore how regularization
can help cope with performative effects by studying its impact in
high-dimensional ridge regression. We show that, while performative effects
worsen the test risk in the population setting, they can be beneficial in the
over-parameterized regime where the number of features exceeds the number of
samples. We show that the optimal regularization scales with the overall
strength of the performative effect, making it possible to set the
regularization in anticipation of this effect. We illustrate this finding
through empirical evaluations of the optimal regularization parameter on both
synthetic and real-world datasets.

</details>


### [261] [Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development](https://arxiv.org/abs/2510.12253)
*Changfu Xu,Jianxiong Guo,Yuzhu Liang,Haiyang Huang,Haodong Zou,Xi Zheng,Shui Yu,Xiaowen Chu,Jiannong Cao,Tian Wang*

Main category: cs.LG

TL;DR: This survey examines the integration of Diffusion Models (DMs) into reinforcement learning (RL), creating a taxonomy for their roles and techniques and summarizing applications, issues, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address key challenges in reinforcement learning (RL) by leveraging the advantages of Diffusion Models (DMs) such as multi-modal expressiveness, stable training, and trajectory-level planning.

Method: Introduced a dual-axis taxonomy to organize the use of DMs in RL, based on their functions in RL pipelines and their deployment in online and offline learning regimes, along with progression from single-agent to multi-agent domains.

Result: Provided frameworks for DM-RL integration, highlighted diverse applications, analyzed current limitations, and outlined future research directions.

Conclusion: Diffusion Models demonstrate significant potential in overcoming RL challenges, offer practical frameworks and applications, and warrant further exploration and development in this field.

Abstract: Diffusion Models (DMs), as a leading class of generative models, offer key
advantages for reinforcement learning (RL), including multi-modal
expressiveness, stable training, and trajectory-level planning. This survey
delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We
first provide an overview of RL, highlighting its challenges, and then
introduce the fundamental concepts of DMs, investigating how they are
integrated into RL frameworks to address key challenges in this research field.
We establish a dual-axis taxonomy that organizes the field along two orthogonal
dimensions: a function-oriented taxonomy that clarifies the roles DMs play
within the RL pipeline, and a technique-oriented taxonomy that situates
implementations across online versus offline learning regimes. We also provide
a comprehensive examination of this progression from single-agent to
multi-agent domains, thereby forming several frameworks for DM-RL integration
and highlighting their practical utility. Furthermore, we outline several
categories of successful applications of diffusion-based RL across diverse
domains, discuss open research issues of current methodologies, and highlight
key directions for future research to advance the field. Finally, we summarize
the survey to identify promising future development directions. We are actively
maintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for
papers and other related resources to apply DMs for RL.

</details>


### [262] [FedMMKT:Co-Enhancing a Server Text-to-Image Model and Client Task Models in Multi-Modal Federated Learning](https://arxiv.org/abs/2510.12254)
*Ningxin He,Yang Liu,Wei Sun,Xiaozhou Ye,Ye Ouyang,Tiegang Gao,Zehui Zhang*

Main category: cs.LG

TL;DR: This paper introduces FedMMKT, a framework for enhancing T2I models and client-specific models without compromising privacy.


<details>
  <summary>Details</summary>
Motivation: Existing T2I models face challenges adapting to specific tasks due to limited task-specific data and privacy concerns.

Method: The authors propose FedMMKT, a decentralized framework leveraging multimodal data without sharing sensitive information.

Result: FedMMKT ensures co-enhancement of server T2I models and task-specific client models while preserving data privacy.

Conclusion: FedMMKT provides a privacy-preserving solution for adapting T2I models to specialized tasks using decentralized multimodal data.

Abstract: Text-to-Image (T2I) models have demonstrated their versatility in a wide
range of applications. However, adaptation of T2I models to specialized tasks
is often limited by the availability of task-specific data due to privacy
concerns. On the other hand, harnessing the power of rich multimodal data from
modern mobile systems and IoT infrastructures presents a great opportunity.
This paper introduces Federated Multi-modal Knowledge Transfer (FedMMKT), a
novel framework that enables co-enhancement of a server T2I model and client
task-specific models using decentralized multimodal data without compromising
data privacy.

</details>


### [263] [HiLoRA: Adaptive Hierarchical LoRA Routing for Training-Free Domain Generalization](https://arxiv.org/abs/2510.12266)
*Ziyi Han,Huanyu Wang,Zeyu Zhang,Xiangxiang Dai,Xutong Liu,John C. S. Lui*

Main category: cs.LG

TL;DR: HiLoRA introduces a training-free hierarchical routing framework that optimizes LoRA usage for domain generalization, achieving significant gains over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LoRA adaptation techniques face inefficiencies due to reliance on explicit task labels, additional training, and fixed module activation, which degrade performance in domain generalization.

Method: HiLoRA adaptively selects and allocates rank-one components (ROCs) from LoRA pools based on Gaussian likelihoods at sequence and token levels, without requiring additional training.

Result: Experiments demonstrate up to 55% improvements in domain generalization accuracy over state-of-the-art baselines, without compromising inference throughput.

Conclusion: HiLoRA offers a novel, efficient, and theoretically supported approach for leveraging LoRA modules to enhance performance in domain generalization tasks.

Abstract: Low-Rank Adaptation (LoRA) has emerged as a widely used technique for
adapting large language models (LLMs) to new domains, due to its modular design
and broad availability on platforms such as HuggingFace. This availability has
motivated efforts to reuse existing LoRAs for domain generalization.
  However, existing methods often rely on explicit task labels or additional
training, which are impractical for deployment. Moreover, they typically
activate a fixed number of entire LoRA modules, leading to parameter redundancy
or insufficiency that degrade performance.
  In this paper, we propose \texttt{HiLoRA}, a training-free framework that
performs adaptive hierarchical routing over LoRA pools. Drawing on structural
properties of LoRA, we define rank-one components (ROCs), in which each rank
parameter is regarded as an independent unit. For a given input sequence,
\texttt{HiLoRA} first adaptively selects a subset of LoRAs and determines their
ROC allocation based on Gaussian likelihoods at the sequence level. At the
token level, it further refines routing by activating only the most informative
ROCs.
  We further provide theoretical guarantees that \texttt{HiLoRA} selects the
most relevant LoRAs with high probability.
  Extensive experiments show that \texttt{HiLoRA} achieves substantial
improvements in domain generalization, with accuracy gains of up to {\small
$55\%$} over state-of-the-art baselines, while maintaining comparable inference
throughput.

</details>


### [264] [Multi-Action Self-Improvement for Neural Combinatorial Optimization](https://arxiv.org/abs/2510.12273)
*Laurin Luttmann,Lin Xie*

Main category: cs.LG

TL;DR: The paper introduces a method to enhance Neural Combinatorial Optimization (NCO) with improved sample efficiency and faster training by predicting joint multi-agent actions and leveraging permutation symmetries.


<details>
  <summary>Details</summary>
Motivation: The study aims to address inefficiencies and limitations in existing self-improvement paradigms for NCO, particularly regarding computational expense and failure to utilize multi-agent coordination and permutation symmetries.

Method: The method involves extending self-improvement for predicting joint multi-agent actions and employs a set-prediction loss to supervise the policy on multiple expert assignments, exploiting problem symmetries.

Result: The approach shows better final solution quality, faster solution generation, and improved sample efficiency based on empirical validation over various combinatorial problems.

Conclusion: The method significantly improves NCO performance, enhancing both computational efficiency and the ability to learn coordinated multi-agent behavior.

Abstract: Self-improvement has emerged as a state-of-the-art paradigm in Neural
Combinatorial Optimization (NCO), where models iteratively refine their
policies by generating and imitating high-quality solutions. Despite strong
empirical performance, existing methods face key limitations. Training is
computationally expensive, as policy updates require sampling numerous
candidate solutions per instance to extract a single expert trajectory. More
fundamentally, these approaches fail to exploit the structure of combinatorial
problems involving the coordination of multiple agents, such as vehicles in
min-max routing or machines in scheduling. By supervising on single-action
trajectories, they fail to exploit agent-permutation symmetries, where distinct
sequences of actions yield identical solutions, hindering generalization and
the ability to learn coordinated behavior.
  We address these challenges by extending self-improvement to operate over
joint multi-agent actions. Our model architecture predicts complete agent-task
assignments jointly at each decision step. To explicitly leverage symmetries,
we employ a set-prediction loss, which supervises the policy on multiple expert
assignments for any given state. This approach enhances sample efficiency and
the model's ability to learn coordinated behavior. Furthermore, by generating
multi-agent actions in parallel, it drastically accelerates the solution
generation phase of the self-improvement loop. Empirically, we validate our
method on several combinatorial problems, demonstrating consistent improvements
in the quality of the final solution and a reduced generation latency compared
to standard self-improvement.

</details>


### [265] [Deep SPI: Safe Policy Improvement via World Models](https://arxiv.org/abs/2510.12312)
*Florent Delgrange,Raphael Avalos,Willem Röpke*

Main category: cs.LG

TL;DR: The paper introduces DeepSPI, an on-policy algorithm ensuring monotonic policy improvement and theoretical convergence, achieving strong performance on the ALE-57 benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing SPI methods primarily focus on offline and tabular RL; there is a need to explore SPI in online settings paired with world model and representation learning.

Method: The authors propose a theoretical framework linking policy update neighborhoods with monotonic improvement, enabling the introduction of DeepSPI, which couples transition and reward prediction losses with regularized updates.

Result: DeepSPI demonstrates performance equal to or exceeding baselines like PPO and DeepMDPs on ALE-57 while retaining theoretical guarantees.

Conclusion: The authors successfully extend classical SPI concepts to online deep RL, demonstrating practical applicability and strong empirical results while maintaining theoretical backing.

Abstract: Safe policy improvement (SPI) offers theoretical control over policy updates,
yet existing guarantees largely concern offline, tabular reinforcement learning
(RL). We study SPI in general online settings, when combined with world model
and representation learning. We develop a theoretical framework showing that
restricting policy updates to a well-defined neighborhood of the current policy
ensures monotonic improvement and convergence. This analysis links transition
and reward prediction losses to representation quality, yielding online, "deep"
analogues of classical SPI theorems from the offline RL literature. Building on
these results, we introduce DeepSPI, a principled on-policy algorithm that
couples local transition and reward losses with regularised policy updates. On
the ALE-57 benchmark, DeepSPI matches or exceeds strong baselines, including
PPO and DeepMDPs, while retaining theoretical guarantees.

</details>


### [266] [Leveraging Teleconnections with Physics-Informed Graph Attention Networks for Long-Range Extreme Rainfall Forecasting in Thailand](https://arxiv.org/abs/2510.12328)
*Kiattikun Chobtham,Kanoksri Sarinnapakorn,Kritanai Torsri,Prattana Deeprasertkul,Jirawan Kamma*

Main category: cs.LG

TL;DR: The paper introduces physics-informed Graph Neural Networks (GNNs) combined with extreme-value analysis to improve rainfall prediction in Thailand.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the accuracy of rainfall forecasting, especially for extreme weather events, which is a persistent challenge in climatology.

Method: The authors propose an Attention-LSTM-based Graph Neural Network leveraging physics principles and extreme-value statistical methods, such as the Spatial Season-aware Generalized Pareto Distribution for improved predictions.

Result: The proposed model outperforms traditional baselines and operational systems like SEAS5 in predicting rainfall extremes across various regions.

Conclusion: This novel approach improves extreme-event rainfall predictions, offering practical utility in water resource management and decision-making processes.

Abstract: Accurate rainfall forecasting, particularly for extreme events, remains a
significant challenge in climatology and the Earth system. This paper presents
novel physics-informed Graph Neural Networks (GNNs) combined with extreme-value
analysis techniques to improve gauge-station rainfall predictions across
Thailand. The model leverages a graph-structured representation of gauge
stations to capture complex spatiotemporal patterns, and it offers
explainability through teleconnections. We preprocess relevant climate indices
that potentially influence regional rainfall. The proposed Graph Attention
Network with Long Short-Term Memory (Attention-LSTM) applies the attention
mechanism using initial edge features derived from simple
orographic-precipitation physics formulation. The embeddings are subsequently
processed by LSTM layers. To address extremes, we perform Peak-Over-Threshold
(POT) mapping using the novel Spatial Season-aware Generalized Pareto
Distribution (GPD) method, which overcomes limitations of traditional
machine-learning models. Experiments demonstrate that our method outperforms
well-established baselines across most regions, including areas prone to
extremes, and remains strongly competitive with the state of the art. Compared
with the operational forecasting system SEAS5, our real-world application
improves extreme-event prediction and offers a practical enhancement to produce
fine-resolution maps that support decision-making in long-term water
management.

</details>


### [267] [Finite-time Convergence Analysis of Actor-Critic with Evolving Reward](https://arxiv.org/abs/2510.12334)
*Rui Hu,Yu Chen,Longbo Huang*

Main category: cs.LG

TL;DR: The paper presents a finite-time convergence analysis of actor-critic algorithms under evolving reward functions, achieving an $O(1/\sqrt{T})$ convergence rate under certain conditions, and offers improved distribution mismatch analysis.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical foundations for RL algorithms employing evolving reward functions, such as reward shaping or curriculum learning.

Method: The paper derives non-asymptotic error bounds for actor and critic under evolving reward parameters and introduces a novel analysis of distribution mismatch under Markovian sampling.

Result: It demonstrates an $O(1/\sqrt{T})$ convergence rate, provided the reward evolves slowly enough and achieves improved rates in analyzing distribution mismatch.

Conclusion: The findings offer a theoretical basis for popular RL techniques utilizing evolving reward functions and improve upon existing distribution mismatch analysis methods.

Abstract: Many popular practical reinforcement learning (RL) algorithms employ evolving
reward functions-through techniques such as reward shaping, entropy
regularization, or curriculum learning-yet their theoretical foundations remain
underdeveloped. This paper provides the first finite-time convergence analysis
of a single-timescale actor-critic algorithm in the presence of an evolving
reward function under Markovian sampling. We consider a setting where the
reward parameters may change at each time step, affecting both policy
optimization and value estimation. Under standard assumptions, we derive
non-asymptotic bounds for both actor and critic errors. Our result shows that
an $O(1/\sqrt{T})$ convergence rate is achievable, matching the best-known rate
for static rewards, provided the reward parameters evolve slowly enough. This
rate is preserved when the reward is updated via a gradient-based rule with
bounded gradient and on the same timescale as the actor and critic, offering a
theoretical foundation for many popular RL techniques. As a secondary
contribution, we introduce a novel analysis of distribution mismatch under
Markovian sampling, improving the best-known rate by a factor of $\log^2T$ in
the static-reward case.

</details>


### [268] [Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models](https://arxiv.org/abs/2510.12343)
*Donghwan Rho,Sieun Seo,Hyewon Sung,Chohong Min,Ernest K. Ryu*

Main category: cs.LG

TL;DR: The paper explores text generation using encrypted interaction with large language models (LLMs) and proposes a solution utilizing homomorphic encryption.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the challenge of generating next-token predictions securely for privacy-preserving interaction with LLMs via homomorphic encryption.

Method: The authors introduce a TSP-based token reordering strategy alongside a post-processing step to enhance text generation under homomorphic encryption.

Result: Their approach effectively prevents collapse, improves coherence in text generation, and preserves privacy, as evidenced by theoretical analysis and experimental results.

Conclusion: The research highlights advancements towards practical and secure application of LLMs through improved encrypted interaction techniques.

Abstract: As users increasingly interact with large language models (LLMs) using
private information, secure and encrypted communication becomes essential.
Homomorphic encryption (HE) provides a principled solution by enabling
computation directly on encrypted data. Although prior work has explored
aspects of running LLMs under HE, the challenge of text generation,
particularly next-token prediction, has received limited attention and remains
a key obstacle to practical encrypted interaction. In this work, we propose a
TSP-based token reordering strategy to address the difficulties of encrypted
text generation, together with a post-processing step that further reduces
approximation error. Theoretical analysis and experimental results demonstrate
that our method prevents collapse, improves coherence in generated text, and
preserves data privacy throughout. Overall, our contributions advance the
feasibility of practical and privacy-preserving LLM inference.

</details>


### [269] [Towards Cross-Modal Error Detection with Tables and Images](https://arxiv.org/abs/2510.12383)
*Olga Ovcharenko,Sebastian Schelter*

Main category: cs.LG

TL;DR: This paper evaluates methods for cross-modal error detection in tabular data, specifically in contexts involving image, tabular, and text data.


<details>
  <summary>Details</summary>
Motivation: The paper aims to solve the challenge of detecting and correcting data errors, particularly cross-modal inconsistencies, which are common in industries like e-Commerce and healthcare.

Method: The authors benchmarked several methods for cross-modal error detection using four datasets and five baseline approaches, highlighting the performance of Cleanlab and DataScope when paired with a strong AutoML framework.

Result: Cleanlab and DataScope achieve the highest F1 scores in the study, but overall, current methods show limitations when dealing with complex, real-world data.

Conclusion: There is a need for improved techniques in cross-modal error detection due to the limitations of existing methods, particularly in handling heavy-tailed, real-world scenarios.

Abstract: Ensuring data quality at scale remains a persistent challenge for large
organizations. Despite recent advances, maintaining accurate and consistent
data is still complex, especially when dealing with multiple data modalities.
Traditional error detection and correction methods tend to focus on a single
modality, typically a table, and often miss cross-modal errors that are common
in domains like e-Commerce and healthcare, where image, tabular, and text data
co-exist. To address this gap, we take an initial step towards cross-modal
error detection in tabular data, by benchmarking several methods. Our
evaluation spans four datasets and five baseline approaches. Among them,
Cleanlab, a label error detection framework, and DataScope, a data valuation
method, perform the best when paired with a strong AutoML framework, achieving
the highest F1 scores. Our findings indicate that current methods remain
limited, particularly when applied to heavy-tailed real-world data, motivating
further research in this area.

</details>


### [270] [Enhanced Pre-training of Graph Neural Networks for Million-Scale Heterogeneous Graphs](https://arxiv.org/abs/2510.12401)
*Shengyin Sun,Chen Ma,Jiehao Chen*

Main category: cs.LG

TL;DR: The paper introduces a self-supervised framework to pre-train graph neural networks (GNNs) on large-scale heterogeneous graphs, addressing both structural properties and semantic mismatch issues, achieving superior results on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of dependency on labeled graph data for training GNNs, especially in heterogeneous graphs, and tackle semantic mismatch for improved transferability.

Method: A framework with two pre-training tasks: (1) structure-aware task to learn structural properties of heterogeneous graphs, and (2) semantic-aware task using perturbation subspace and semantic neighbors to handle semantic mismatch.

Result: Extensive experiments on large-scale heterogeneous graphs show that the proposed method outperforms state-of-the-art baselines.

Conclusion: The framework effectively pre-trains GNNs on heterogeneous graphs by capturing structural and semantic properties, enhancing transferability and applicability in downstream tasks.

Abstract: In recent years, graph neural networks (GNNs) have facilitated the
development of graph data mining. However, training GNNs requires sufficient
labeled task-specific data, which is expensive and sometimes unavailable. To be
less dependent on labeled data, recent studies propose to pre-train GNNs in a
self-supervised manner and then apply the pre-trained GNNs to downstream tasks
with limited labeled data. However, most existing methods are designed solely
for homogeneous graphs (real-world graphs are mostly heterogeneous) and do not
consider semantic mismatch (the semantic difference between the original data
and the ideal data containing more transferable semantic information). In this
paper, we propose an effective framework to pre-train GNNs on the large-scale
heterogeneous graph. We first design a structure-aware pre-training task, which
aims to capture structural properties in heterogeneous graphs. Then, we design
a semantic-aware pre-training task to tackle the mismatch. Specifically, we
construct a perturbation subspace composed of semantic neighbors to help deal
with the semantic mismatch. Semantic neighbors make the model focus more on the
general knowledge in the semantic space, which in turn assists the model in
learning knowledge with better transferability. Finally, extensive experiments
are conducted on real-world large-scale heterogeneous graphs to demonstrate the
superiority of the proposed method over state-of-the-art baselines. Code
available at https://github.com/sunshy-1/PHE.

</details>


### [271] [Continuous Uniqueness and Novelty Metrics for Generative Modeling of Inorganic Crystals](https://arxiv.org/abs/2510.12405)
*Masahiro Negishi,Hyunsoo Park,Kinga O. Mastej,Aron Walsh*

Main category: cs.LG

TL;DR: The paper proposes two new continuous distance functions to overcome the limitations of traditional distance functions used to evaluate generative AI models for functional materials.


<details>
  <summary>Details</summary>
Motivation: Address limitations in current distance functions for evaluating generative AI models for functional materials, which fail to adequately quantify similarity, distinguish differences, maintain continuity, and ensure sample permutation invariance.

Method: Introduce two continuous distance functions that address the shortcomings of traditional distance metrics in evaluating generative models for inorganic crystals.

Result: The proposed distance functions reveal new insights that traditional metrics fail to capture, offering a more reliable evaluation of generative models for inorganic crystals.

Conclusion: The new continuous distance functions improve the evaluation of generative models for inorganic crystals, offering a more effective and reliable framework compared to traditional metrics.

Abstract: To address pressing scientific challenges such as climate change,
increasingly sophisticated generative artificial intelligence models are being
developed that can efficiently sample the large chemical space of possible
functional materials. These models can quickly sample new chemical compositions
paired with crystal structures. They are typically evaluated using uniqueness
and novelty metrics, which depend on a chosen crystal distance function.
However, the most prevalent distance function has four limitations: it fails to
quantify the degree of similarity between compounds, cannot distinguish
compositional difference and structural difference, lacks Lipschitz continuity
against shifts in atomic coordinates, and results in a uniqueness metric that
is not invariant against the permutation of generated samples. In this work, we
propose using two continuous distance functions to evaluate uniqueness and
novelty, which theoretically overcome these limitations. Our experiments show
that these distances reveal insights missed by traditional distance functions,
providing a more reliable basis for evaluating and comparing generative models
for inorganic crystals.

</details>


### [272] [Bayesian Optimization for Dynamic Pricing and Learning](https://arxiv.org/abs/2510.12447)
*Anush Anand,Pranav Agrawal,Tejas Bodas*

Main category: cs.LG

TL;DR: The paper introduces a Gaussian Process-based Bayesian Optimization method for dynamic pricing, outperforming reinforcement learning techniques in accuracy, efficiency, and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing dynamic pricing methods relying on parametric demand functions and reinforcement learning are constrained by restrictive assumptions, limiting their real-world applicability.

Method: The authors propose a nonparametric dynamic pricing model utilizing Gaussian Processes and Bayesian Optimization, offering tailored algorithms and regret guarantees for infinite and finite inventory scenarios.

Result: Extensive experiments reveal that the proposed BO-based algorithms generate higher revenue while reducing assumptions compared to state-of-the-art reinforcement learning approaches.

Conclusion: Bayesian Optimization proves to be a robust and practical solution for dynamic pricing, addressing real-world uncertainties and outperforming existing methods in various scenarios.

Abstract: Dynamic pricing is the practice of adjusting the selling price of a product
to maximize a firm's revenue by responding to market demand. The literature
typically distinguishes between two settings: infinite inventory, where the
firm has unlimited stock and time to sell, and finite inventory, where both
inventory and selling horizon are limited. In both cases, the central challenge
lies in the fact that the demand function -- how sales respond to price -- is
unknown and must be learned from data. Traditional approaches often assume a
specific parametric form for the demand function, enabling the use of
reinforcement learning (RL) to identify near-optimal pricing strategies.
However, such assumptions may not hold in real-world scenarios, limiting the
applicability of these methods. In this work, we propose a Gaussian Process
(GP) based nonparametric approach to dynamic pricing that avoids restrictive
modeling assumptions. We treat the demand function as a black-box function of
the price and develop pricing algorithms based on Bayesian Optimization (BO) --
a sample-efficient method for optimizing unknown functions. We present BO-based
algorithms tailored for both infinite and finite inventory settings and provide
regret guarantees for both regimes, thereby quantifying the learning efficiency
of our methods. Through extensive experiments, we demonstrate that our BO-based
methods outperform several state-of-the-art RL algorithms in terms of revenue,
while requiring fewer assumptions and offering greater robustness. This
highlights Bayesian Optimization as a powerful and practical tool for dynamic
pricing in complex, uncertain environments.

</details>


### [273] [A Function Centric Perspective On Flat and Sharp Minima](https://arxiv.org/abs/2510.12451)
*Israel Mason-Williams,Gabryel Mason-Williams,Helen Yannakoudakis*

Main category: cs.LG

TL;DR: This paper revisits the correlation between sharpness and generalization in deep neural networks, suggesting sharpness is context-dependent and not necessarily indicative of poor generalization.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to clarify the nuanced relationship between sharpness (loss landscape) and generalization in neural networks, prompted by contradictory findings in prior studies.

Method: Extensive empirical studies were performed across different optimization techniques and tasks, including modern image classification, to analyze the impact of regularization methods on sharpness and associated metrics.

Result: The study finds that sharper minima can coincide with improved generalization, calibration, robustness, and functional consistency when models are regularized.

Conclusion: Function complexity, rather than flatness alone, shapes solution geometry. Sharp minima under regularization may offer better inductive biases, urging a reassessment of loss landscape geometry as a function-centric perspective.

Abstract: Flat minima are widely believed to correlate with improved generalisation in
deep neural networks. However, this connection has proven more nuanced in
recent studies, with both theoretical counterexamples and empirical exceptions
emerging in the literature. In this paper, we revisit the role of sharpness in
model performance, proposing that sharpness is better understood as a
function-dependent property rather than a reliable indicator of poor
generalisation. We conduct extensive empirical studies, from single-objective
optimisation to modern image classification tasks, showing that sharper minima
often emerge when models are regularised (e.g., via SAM, weight decay, or data
augmentation), and that these sharp minima can coincide with better
generalisation, calibration, robustness, and functional consistency. Across a
range of models and datasets, we find that baselines without regularisation
tend to converge to flatter minima yet often perform worse across all safety
metrics. Our findings demonstrate that function complexity, rather than
flatness alone, governs the geometry of solutions, and that sharper minima can
reflect more appropriate inductive biases (especially under regularisation),
calling for a function-centric reappraisal of loss landscape geometry.

</details>


### [274] [Time-Correlated Video Bridge Matching](https://arxiv.org/abs/2510.12453)
*Viacheslav Vasilev,Arseny Ivanov,Nikita Gushchin,Maria Kovaleva,Alexander Korotin*

Main category: cs.LG

TL;DR: Diffusion models struggle with data-to-data tasks. This paper introduces TCVBM, enhancing video generation/manipulation by modeling temporal correlations within time-dependent data sequences.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation of diffusion models in handling translations between complex distributions for time-correlated data, which is crucial for video generation and manipulation.

Method: The proposed Time-Correlated Video Bridge Matching (TCVBM) extends Bridge Matching (BM) methods by integrating temporal dependencies through a diffusion bridge, focusing on video-related tasks.

Result: TCVBM demonstrated superior performance compared to classical methods in video frame interpolation, image-to-video generation, and video super-resolution across various metrics.

Conclusion: TCVBM effectively incorporates temporal correlations in video generation tasks, improving quality and fidelity compared to standard methods.

Abstract: Diffusion models excel in noise-to-data generation tasks, providing a mapping
from a Gaussian distribution to a more complex data distribution. However they
struggle to model translations between complex distributions, limiting their
effectiveness in data-to-data tasks. While Bridge Matching (BM) models address
this by finding the translation between data distributions, their application
to time-correlated data sequences remains unexplored. This is a critical
limitation for video generation and manipulation tasks, where maintaining
temporal coherence is particularly important. To address this gap, we propose
Time-Correlated Video Bridge Matching (TCVBM), a framework that extends BM to
time-correlated data sequences in the video domain. TCVBM explicitly models
inter-sequence dependencies within the diffusion bridge, directly incorporating
temporal correlations into the sampling process. We compare our approach to
classical methods based on bridge matching and diffusion models for three
video-related tasks: frame interpolation, image-to-video generation, and video
super-resolution. TCVBM achieves superior performance across multiple
quantitative metrics, demonstrating enhanced generation quality and
reconstruction fidelity.

</details>


### [275] [Mitigating the Noise Shift for Denoising Generative Models via Noise Awareness Guidance](https://arxiv.org/abs/2510.12497)
*Jincheng Zhong,Boyuan Jiang,Xin Tao,Pengfei Wan,Kun Gai,Mingsheng Long*

Main category: cs.LG

TL;DR: The paper identifies a pervasive issue in denoising generative models, termed noise shift, and proposes a solution called Noise Awareness Guidance (NAG) to address it.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the performance of diffusion-based generative models by addressing the noise shift issue that causes sub-optimal generation.

Method: The authors propose Noise Awareness Guidance (NAG), which steers sampling trajectories to align with the pre-defined noise schedule and introduces a classifier-free variant using noise-condition dropout.

Result: Experiments on ImageNet and fine-tuning tasks demonstrate that NAG effectively mitigates noise shift and substantially enhances generation quality in diffusion models.

Conclusion: NAG proves to be a simple and effective method for resolving noise shift, leading to better alignment with noise schedules and improved generative performance.

Abstract: Existing denoising generative models rely on solving discretized reverse-time
SDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue
in this family of models: a misalignment between the pre-defined noise level
and the actual noise level encoded in intermediate states during sampling. We
refer to this misalignment as noise shift. Through empirical analysis, we
demonstrate that noise shift is widespread in modern diffusion models and
exhibits a systematic bias, leading to sub-optimal generation due to both
out-of-distribution generalization and inaccurate denoising updates. To address
this problem, we propose Noise Awareness Guidance (NAG), a simple yet effective
correction method that explicitly steers sampling trajectories to remain
consistent with the pre-defined noise schedule. We further introduce a
classifier-free variant of NAG, which jointly trains a noise-conditional and a
noise-unconditional model via noise-condition dropout, thereby eliminating the
need for external classifiers. Extensive experiments, including ImageNet
generation and various supervised fine-tuning tasks, show that NAG consistently
mitigates noise shift and substantially improves the generation quality of
mainstream diffusion models.

</details>


### [276] [Multi-Armed Bandits with Minimum Aggregated Revenue Constraints](https://arxiv.org/abs/2510.12523)
*Ahmed Ben Yahmed,Hafedh El Ferchichi,Marc Abeille,Vianney Perchet*

Main category: cs.LG

TL;DR: The paper addresses a multi-armed bandit problem with contextual information aiming to balance fairness (minimum aggregated reward constraints) and maximizing cumulative reward, tackling performance trade-offs and proving optimality bounds.


<details>
  <summary>Details</summary>
Motivation: To address real-world scenarios that require fair revenue allocation across arms while considering contextual variations.

Method: Developed algorithms that focus either on prioritizing cumulative reward (optimistic) or enforcing constraints (pessimistic) and derived bounds for regret and constraint violations.

Result: Demonstrated optimal bounds on time horizon dependence and established limitations of approaches using free exploration from prior work.

Conclusion: The proposed algorithms effectively optimize cumulative reward while respecting constraint satisfaction, ensuring feasibility and fairness in contextual bandit settings.

Abstract: We examine a multi-armed bandit problem with contextual information, where
the objective is to ensure that each arm receives a minimum aggregated reward
across contexts while simultaneously maximizing the total cumulative reward.
This framework captures a broad class of real-world applications where fair
revenue allocation is critical and contextual variation is inherent. The
cross-context aggregation of minimum reward constraints, while enabling better
performance and easier feasibility, introduces significant technical challenges
-- particularly the absence of closed-form optimal allocations typically
available in standard MAB settings. We design and analyze algorithms that
either optimistically prioritize performance or pessimistically enforce
constraint satisfaction. For each algorithm, we derive problem-dependent upper
bounds on both regret and constraint violations. Furthermore, we establish a
lower bound demonstrating that the dependence on the time horizon in our
results is optimal in general and revealing fundamental limitations of the free
exploration principle leveraged in prior work.

</details>


### [277] [Evaluation of Real-Time Preprocessing Methods in AI-Based ECG Signal Analysis](https://arxiv.org/abs/2510.12541)
*Jasmin Freudenberg,Kai Hahn,Christian Weber,Madjid Fathi*

Main category: cs.LG

TL;DR: The FACE project focuses on creating a machine learning solution for long-term ECG analysis by leveraging the synergy between edge and cloud computing.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for privacy-compliant, energy-efficient real-time analysis of ECG data due to the increasing usage of portable ECG systems.

Method: The study examines various ECG pre-processing methods suitable for edge computing, emphasizing criteria like energy efficiency, processing, and real-time capabilities.

Result: It evaluates the applicability of pre-processing techniques for the FACE project's machine learning integration.

Conclusion: Edge and cloud computing are synergistically combined to efficiently handle ECG signal analysis, prioritizing privacy and performance at the point of data acquisition.

Abstract: The increasing popularity of portable ECG systems and the growing demand for
privacy-compliant, energy-efficient real-time analysis require new approaches
to signal processing at the point of data acquisition. In this context, the
edge domain is acquiring increasing importance, as it not only reduces latency
times, but also enables an increased level of data security. The FACE project
aims to develop an innovative machine learning solution for analysing long-term
electrocardiograms that synergistically combines the strengths of edge and
cloud computing. In this thesis, various pre-processing steps of ECG signals
are analysed with regard to their applicability in the project. The selection
of suitable methods in the edge area is based in particular on criteria such as
energy efficiency, processing capability and real-time capability.

</details>


### [278] [Research in Collaborative Learning Does Not Serve Cross-Silo Federated Learning in Practice](https://arxiv.org/abs/2510.12595)
*Kevin Kuo,Chhavi Yadav,Virginia Smith*

Main category: cs.LG

TL;DR: This paper examines the practical barriers to adopting cross-silo federated learning (FL), revealing challenges distinct from cross-device FL, through interviews with various stakeholders.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limited adoption of cross-silo federated learning (FL) despite its potential for facilitating organizational collaboration without sharing private data, and the increasing interest due to data protection regulations like GDPR and HIPAA.

Method: The study adopts an interview approach, engaging with various stakeholders such as user organizations, software providers, and academic researchers to identify challenges in implementing cross-silo FL.

Result: The study identifies barriers to cross-silo FL adoption, including model performance concerns, incentive alignment, and trust issues between participating organizations.

Conclusion: Cross-silo FL faces unique challenges distinct from cross-device FL, and the paper proposes future research directions to address these issues.

Abstract: Cross-silo federated learning (FL) is a promising approach to enable
cross-organization collaboration in machine learning model development without
directly sharing private data. Despite growing organizational interest driven
by data protection regulations such as GDPR and HIPAA, the adoption of
cross-silo FL remains limited in practice. In this paper, we conduct an
interview study to understand the practical challenges associated with
cross-silo FL adoption. With interviews spanning a diverse set of stakeholders
such as user organizations, software providers, and academic researchers, we
uncover various barriers, from concerns about model performance to questions of
incentives and trust between participating organizations. Our study shows that
cross-silo FL faces a set of challenges that have yet to be well-captured by
existing research in the area and are quite distinct from other forms of
federated learning such as cross-device FL. We end with a discussion on future
research directions that can help overcome these challenges.

</details>


### [279] [Rethinking Knowledge Distillation: A Data Dependent Regulariser With a Negative Asymmetric Payoff](https://arxiv.org/abs/2510.12615)
*Israel Mason-Williams,Gabryel Mason-Williams,Helen Yannakoudakis*

Main category: cs.LG

TL;DR: The paper examines the functional impact of knowledge distillation, finding that its primary effect is as a data-dependent regularizer rather than an effective compression mechanism.


<details>
  <summary>Details</summary>
Motivation: To understand the functional impact of knowledge distillation beyond its use as a compression mechanism, emphasizing insights into knowledge transfer dynamics and safety implications.

Method: The study uses hypothesis testing, controls, random control distillation, various distillation variants, and scaling laws across model sizes and data modalities.

Result: The analysis reveals limited and less pronounced knowledge transfer than expected, with significant cases showing an asymmetric transfer of negative knowledge to the student model.

Conclusion: Knowledge distillation is not primarily a compression mechanism, but rather acts as a data-dependent regularizer, often resulting in the unsafe transfer of negative knowledge.

Abstract: Knowledge distillation is often considered a compression mechanism when
judged on the resulting student's accuracy and loss, yet its functional impact
is poorly understood. In this work, we quantify the compression capacity of
knowledge distillation and the resulting knowledge transfer from a functional
perspective, decoupling compression from architectural reduction, which
provides an improved understanding of knowledge distillation. We employ
hypothesis testing, controls, and random control distillation to understand
knowledge transfer mechanisms across data modalities. To rigorously test the
breadth and limits of our analyses, we explore multiple distillation variants
and analyse distillation scaling laws across model sizes. Our findings
demonstrate that, while there is statistically significant knowledge transfer
in some modalities and architectures, the extent of this transfer is less
pronounced than anticipated, even under conditions designed to maximise
knowledge sharing. Notably, in cases of significant knowledge transfer, we
identify a consistent and severe asymmetric transfer of negative knowledge to
the student, raising safety concerns in knowledge distillation applications.
Across 12 experimental setups, 9 architectures, and 7 datasets, our findings
show that knowledge distillation functions less as a compression mechanism and
more as a data-dependent regulariser with a negative asymmetric payoff.

</details>


### [280] [Towards Fast Coarse-graining and Equation Discovery with Foundation Inference Models](https://arxiv.org/abs/2510.12618)
*Manuel Hinz,Maximilian Mauel,Patrick Seifner,David Berghaus,Kostadin Cvejoski,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: The paper introduces a method to identify low-dimensional latent dynamics in dynamical systems by using pretrained models, known as Foundation Inference Models (FIMs), to decouple variable discovery and equation fitting.


<details>
  <summary>Details</summary>
Motivation: High-dimensional dynamical systems often have underlying low-dimensional structures, but identifying these latent dynamics is challenging and requires discovering appropriate variables while fitting governing equations.

Method: The authors leverage pretrained FIMs to estimate dynamical system generators in zero-shot mode. By freezing FIM weights, they focus on training encoder-decoder maps, stabilizing representation learning with a simulation-consistent loss.

Result: The approach is tested on a stochastic double-well system embedded in synthetic video data, showcasing its capability for efficient and reusable coarse-graining pipelines.

Conclusion: The proposed method simplifies and stabilizes the process of identifying latent dynamics, offering a fast and reusable tool for understanding complex dynamical systems.

Abstract: High-dimensional recordings of dynamical processes are often characterized by
a much smaller set of effective variables, evolving on low-dimensional
manifolds. Identifying these latent dynamics requires solving two intertwined
problems: discovering appropriate coarse-grained variables and simultaneously
fitting the governing equations. Most machine learning approaches tackle these
tasks jointly by training autoencoders together with models that enforce
dynamical consistency. We propose to decouple the two problems by leveraging
the recently introduced Foundation Inference Models (FIMs). FIMs are pretrained
models that estimate the infinitesimal generators of dynamical systems (e.g.,
the drift and diffusion of a stochastic differential equation) in zero-shot
mode. By amortizing the inference of the dynamics through a FIM with frozen
weights, and training only the encoder-decoder map, we define a simple,
simulation-consistent loss that stabilizes representation learning. A proof of
concept on a stochastic double-well system with semicircle diffusion, embedded
into synthetic video data, illustrates the potential of this approach for fast
and reusable coarse-graining pipelines.

</details>


### [281] [Learning-To-Measure: In-context Active Feature Acquisition](https://arxiv.org/abs/2510.12624)
*Yuta Kobayashi,Zilin Jing,Jiayu Yao,Hongseok Namkoong,Shalmali Joshi*

Main category: cs.LG

TL;DR: This paper proposes Learning-to-Measure (L2M), a solution to the meta-Active Feature Acquisition (meta-AFA) problem, enabling adaptive feature acquisition across multiple tasks and performing well even with missing or limited data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations in Active Feature Acquisition (AFA) methods, which traditionally focus on single predetermined tasks and struggle with limited labels and systematic missingness in features.

Method: The method, named L2M, combines reliable uncertainty quantification for unseen tasks and an uncertainty-guided greedy feature acquisition approach. It uses a sequence-modeling pre-training methodology to handle missing data and tasks with retrospective missingness.

Result: L2M outperforms task-specific baselines on synthetic and real-world tabular data, specifically when data is sparse or has high levels of missingness.

Conclusion: L2M provides a scalable meta-AFA solution by eliminating the need for per-task retraining and demonstrating robustness under challenging data conditions.

Abstract: Active feature acquisition (AFA) is a sequential decision-making problem
where the goal is to improve model performance for test instances by adaptively
selecting which features to acquire. In practice, AFA methods often learn from
retrospective data with systematic missingness in the features and limited
task-specific labels. Most prior work addresses acquisition for a single
predetermined task, limiting scalability. To address this limitation, we
formalize the meta-AFA problem, where the goal is to learn acquisition policies
across various tasks. We introduce Learning-to-Measure (L2M), which consists of
i) reliable uncertainty quantification over unseen tasks, and ii) an
uncertainty-guided greedy feature acquisition agent that maximizes conditional
mutual information. We demonstrate a sequence-modeling or autoregressive
pre-training approach that underpins reliable uncertainty quantification for
tasks with arbitrary missingness. L2M operates directly on datasets with
retrospective missingness and performs the meta-AFA task in-context,
eliminating per-task retraining. Across synthetic and real-world tabular
benchmarks, L2M matches or surpasses task-specific baselines, particularly
under scarce labels and high missingness.

</details>


### [282] [Expert or not? assessing data quality in offline reinforcement learning](https://arxiv.org/abs/2510.12638)
*Arip Asadulaev,Fakhri Karray,Martin Takac*

Main category: cs.LG

TL;DR: The paper proposes a new metric, Bellman Wasserstein Distance (BWD), to estimate the quality of offline reinforcement learning datasets without requiring agent training.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the problem of assessing offline RL dataset quality in the absence of prior knowledge about the data's origin, skill composition, or quality.

Method: The proposed approach uses the Bellman Wasserstein Distance (BWD), a value-aware optimal transport score computed using a behavioral critic and state-conditional optimal transport. BWD measures the dissimilarity between a dataset's behavioral policy and a random reference policy without requiring environment interactions.

Result: The study shows that BWD correlates strongly with an oracle performance score and can predict the performance of standard RL agents on certain datasets. It also serves as a regularizer during policy optimization to improve returns.

Conclusion: BWD appears to be a practical and effective tool for evaluating offline RL dataset quality and optimizing policies by guiding them away from random behaviors.

Abstract: Offline reinforcement learning (RL) learns exclusively from static datasets,
without further interaction with the environment. In practice, such datasets
vary widely in quality, often mixing expert, suboptimal, and even random
trajectories. The choice of algorithm therefore depends on dataset fidelity.
Behavior cloning can suffice on high-quality data, whereas mixed- or
low-quality data typically benefits from offline RL methods that stitch useful
behavior across trajectories. Yet in the wild it is difficult to assess dataset
quality a priori because the data's provenance and skill composition are
unknown. We address the problem of estimating offline dataset quality without
training an agent. We study a spectrum of proxies from simple cumulative
rewards to learned value based estimators, and introduce the Bellman
Wasserstein distance (BWD), a value aware optimal transport score that measures
how dissimilar a dataset's behavioral policy is from a random reference policy.
BWD is computed from a behavioral critic and a state conditional OT
formulation, requiring no environment interaction or full policy optimization.
Across D4RL MuJoCo tasks, BWD strongly correlates with an oracle performance
score that aggregates multiple offline RL algorithms, enabling efficient
prediction of how well standard agents will perform on a given dataset. Beyond
prediction, integrating BWD as a regularizer during policy optimization
explicitly pushes the learned policy away from random behavior and improves
returns. These results indicate that value aware, distributional signals such
as BWD are practical tools for triaging offline RL datasets and policy
optimization.

</details>


### [283] [On Foundation Models for Temporal Point Processes to Accelerate Scientific Discovery](https://arxiv.org/abs/2510.12640)
*David Berghaus,Patrick Seifner,Kostadin Cvejoski,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: The paper introduces a foundational machine learning model for analyzing event sequences, trained on millions of simulated data, enabling quick adaptation to new datasets without retraining.


<details>
  <summary>Details</summary>
Motivation: The motivation is to eliminate the inefficiency of building and training machine learning models from scratch for each new dataset in scientific fields analyzing event sequences.

Method: The authors developed a foundational model pretrained on millions of simulated event sequences, allowing it to generalize and understand event patterns in various scientific datasets.

Result: The model demonstrated the ability to analyze new datasets instantly by observing a few examples, and could be fine-tuned for higher accuracy, making event data analysis more efficient.

Conclusion: This approach simplifies event sequence analysis, reduces computational costs, and accelerates scientific discovery by removing the need for retraining on every new dataset.

Abstract: Many scientific fields, from medicine to seismology, rely on analyzing
sequences of events over time to understand complex systems. Traditionally,
machine learning models must be built and trained from scratch for each new
dataset, which is a slow and costly process. We introduce a new approach: a
single, powerful model that learns the underlying patterns of event data in
context. We trained this "foundation model" on millions of simulated event
sequences, teaching it a general-purpose understanding of how events can
unfold. As a result, our model can analyze new scientific data instantly,
without retraining, simply by looking at a few examples from the dataset. It
can also be quickly fine-tuned for even higher accuracy. This approach makes
sophisticated event analysis more accessible and accelerates the pace of
scientific discovery.

</details>


### [284] [Towards Foundation Inference Models that Learn ODEs In-Context](https://arxiv.org/abs/2510.12650)
*Maximilian Mauel,Manuel Hinz,Patrick Seifner,David Berghaus,Ramses J. Sanchez*

Main category: cs.LG

TL;DR: This paper introduces FIM-ODE, a pretrained neural model for accurately estimating ODEs from sparse and noisy data.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of dynamical systems as ODEs is difficult, particularly with sparse and noisy datasets.

Method: The authors propose FIM-ODE, a pretrained neural operator trained on synthetic data, capable of zero-shot inference.

Result: FIM-ODE empirically provides accurate ODE estimates comparable to state-of-the-art neural methods, even with corrupted data.

Conclusion: The paper demonstrates FIM-ODE's capability for robust data-driven ODE modeling, addressing challenges posed by data sparsity and noise.

Abstract: Ordinary differential equations (ODEs) describe dynamical systems evolving
deterministically in continuous time. Accurate data-driven modeling of systems
as ODEs, a central problem across the natural sciences, remains challenging,
especially if the data is sparse or noisy. We introduce FIM-ODE (Foundation
Inference Model for ODEs), a pretrained neural model designed to estimate ODEs
zero-shot (i.e., in context) from sparse and noisy observations. Trained on
synthetic data, the model utilizes a flexible neural operator for robust ODE
inference, even from corrupted data. We empirically verify that FIM-ODE
provides accurate estimates, on par with a neural state-of-the-art method, and
qualitatively compare the structure of their estimated vector fields.

</details>


### [285] [SG-XDEAT: Sparsity-Guided Cross-Dimensional and Cross-Encoding Attention with Target-Aware Conditioning in Tabular Learning](https://arxiv.org/abs/2510.12659)
*Chih-Chuan Cheng,Yi-Ju Tseng*

Main category: cs.LG

TL;DR: SG-XDEAT is a novel framework for supervised learning on tabular data, utilizing dual-stream encoding, hierarchical attention modules, and adaptive sparse self-attention to improve robustness and performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of supervised learning on tabular data by effectively modeling raw and label-aware feature representations while mitigating noise in the data.

Method: Introduces a dual-stream encoder to create raw and target-conditioned feature representations, employs cross-dimensional and cross-encoding self-attentions, and uses an adaptive sparse self-attention mechanism to suppress low-utility tokens.

Result: SG-XDEAT demonstrates consistent improvements over strong baseline models on multiple public benchmarks in supervised learning tasks.

Conclusion: The proposed framework, SG-XDEAT, effectively enhances the robustness and learning capabilities of deep tabular models by jointly leveraging raw and label-aware views while adaptively filtering noise.

Abstract: We propose SG-XDEAT (Sparsity-Guided Cross Dimensional and Cross-Encoding
Attention with Target Aware Conditioning), a novel framework designed for
supervised learning on tabular data. At its core, SG-XDEAT employs a
dual-stream encoder that decomposes each input feature into two parallel
representations: a raw value stream and a target-conditioned (label-aware)
stream. These dual representations are then propagated through a hierarchical
stack of attention-based modules. SG-XDEAT integrates three key components: (i)
Cross-Dimensional self-attention, which captures intra-view dependencies among
features within each stream; (ii) Cross-Encoding self-attention, which enables
bidirectional interaction between raw and target-aware representations; and
(iii) an Adaptive Sparse Self-Attention (ASSA) mechanism, which dynamically
suppresses low-utility tokens by driving their attention weights toward
zero--thereby mitigating the impact of noise. Empirical results on multiple
public benchmarks show consistent gains over strong baselines, confirming that
jointly modeling raw and target-aware views--while adaptively filtering
noise--yields a more robust deep tabular learner.

</details>


### [286] [Structured Sparsity and Weight-adaptive Pruning for Memory and Compute efficient Whisper models](https://arxiv.org/abs/2510.12666)
*Prasenjit K Mudi,Anshi Sachan,Dahlia Devapriya,Sheetal Kalyani*

Main category: cs.LG

TL;DR: The paper addresses the issue of deploying Whisper models on resource-constrained devices by proposing a framework for fine-tuning them with reduced size and computational demands, while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the challenge of deploying Whisper speech recognition models on edge devices that are constrained by computational resources, memory, and power.

Method: The paper introduces a framework employing the Sparse Group LASSO penalty to enforce structured sparsity for reducing computations and parameters. Additionally, a weight statistics-aware pruning method and a customized text normalizer for WER evaluation are also proposed.

Result: Without compromising Word Error Rate (WER), the authors achieved significant reductions in parameters, memory usage, and floating-point operations for Whisper-small (parameters: -35.4%, memory: -14.25%, FLOPs: -18.5%) and Whisper-medium (parameters: -31%, memory: -15.29%, FLOPs: -16.95%). The method also outperformed existing pruning techniques by removing 18.7% more parameters and achieving a 12.31 reduction in WER.

Conclusion: The proposed methods effectively reduce model size and computational demand of Whisper models, making them more suitable for resource-constrained edge devices while maintaining or even improving their recognition accuracy.

Abstract: Whisper models have achieved remarkable progress in speech recognition; yet
their large size remains a bottleneck for deployment on resource-constrained
edge devices. This paper proposes a framework to design fine-tuned variants of
Whisper which address the above problem. Structured sparsity is enforced via
the Sparse Group LASSO penalty as a loss regularizer, to reduce the number of
FLOating Point operations (FLOPs). Further, a weight statistics aware pruning
algorithm is proposed. We also design our custom text normalizer for WER
evaluation. On Common Voice 11.0 Hindi dataset, we obtain, without degrading
WER, (a) 35.4% reduction in model parameters, 14.25% lower memory consumption
and 18.5% fewer FLOPs on Whisper-small, and (b) 31% reduction in model
parameters, 15.29% lower memory consumption and 16.95% fewer FLOPs on
Whisper-medium; and, (c) substantially outperform the state-of-the-art
Iterative Magnitude Pruning based method by pruning 18.7% more parameters along
with a 12.31 reduction in WER.

</details>


### [287] [Structure-Aware Spectral Sparsification via Uniform Edge Sampling](https://arxiv.org/abs/2510.12669)
*Kaiwen He,Petros Drineas,Rajiv Khanna*

Main category: cs.LG

TL;DR: The paper demonstrates that uniform edge sampling, instead of traditional importance sampling, can preserve spectral clustering properties for graphs with strong clusterability.


<details>
  <summary>Details</summary>
Motivation: Spectral clustering is a widely used technique for graph partitioning but suffers scalability limitations due to its reliance on eigenvector computations. This paper explores whether a simpler and widely applicable uniform sampling can achieve similar results.

Method: The authors analytically investigate uniform edge sampling on graphs with well-separated $k$-clusters, introducing new bounds and leveraging matrix Chernoff bounds for eigenspaces to demonstrate its viability.

Result: Uniform edge sampling preserves the necessary spectral subspace for clustering when $O(\gamma^2 n \log n / \epsilon^2)$ edges are sampled, bypassing the need for expensive preprocessing.

Conclusion: Under strong clusterability in graphs, uniform edge sampling suffices for structure-preserving spectral clustering, uniting coreset-based clustering theory with spectral sparsification ideas.

Abstract: Spectral clustering is a fundamental method for graph partitioning, but its
reliance on eigenvector computation limits scalability to massive graphs.
Classical sparsification methods preserve spectral properties by sampling edges
proportionally to their effective resistances, but require expensive
preprocessing to estimate these resistances. We study whether uniform edge
sampling-a simple, structure-agnostic strategy-can suffice for spectral
clustering. Our main result shows that for graphs admitting a well-separated
$k$-clustering, characterized by a large structure ratio $\Upsilon(k) =
\lambda_{k+1} / \rho_G(k)$, uniform sampling preserves the spectral subspace
used for clustering. Specifically, we prove that uniformly sampling $O(\gamma^2
n \log n / \epsilon^2)$ edges, where $\gamma$ is the Laplacian condition
number, yields a sparsifier whose top $(n-k)$-dimensional eigenspace is
approximately orthogonal to the cluster indicators. This ensures that the
spectral embedding remains faithful, and clustering quality is preserved. Our
analysis introduces new resistance bounds for intra-cluster edges, a
rank-$(n-k)$ effective resistance formulation, and a matrix Chernoff bound
adapted to the dominant eigenspace. These tools allow us to bypass importance
sampling entirely. Conceptually, our result connects recent coreset-based
clustering theory to spectral sparsification, showing that under strong
clusterability, even uniform sampling is structure-aware. This provides the
first provable guarantee that uniform edge sampling suffices for
structure-preserving spectral clustering.

</details>


### [288] [Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers](https://arxiv.org/abs/2510.12672)
*Ruben Belo,Claudia Soares,Marta Guimaraes*

Main category: cs.LG

TL;DR: The paper introduces "CALM," a method to suppress harmful outputs of large language models without retraining.


<details>
  <summary>Details</summary>
Motivation: Large Language Models are vulnerable to jailbreak attacks, posing risks by generating harmful outputs despite safety guardrails.

Method: The proposed method, CALM, modifies latent representations at inference time using adversarial techniques from Computer Vision and orthogonal projection, removing harmful directions in the model's representations.

Result: CALM shows reduced harmful outputs compared to baseline methods and maintains model performance with minimal computational overhead.

Conclusion: CALM is an effective, lightweight solution for AI safety, offering better handling of harmful content without retraining or fine-tuning the model.

Abstract: Large Language Models are susceptible to jailbreak attacks that bypass
built-in safety guardrails (e.g., by tricking the model with adversarial
prompts). We propose Concept Alignment and Concept Manipulation \textbf{CALM},
an inference-time method that suppresses harmful concepts by modifying latent
representations of the last layer of the model, without retraining. Leveraging
\gls*{cw} technique from Computer Vision combined with orthogonal projection,
CALM removes unwanted latent directions associated with harmful content while
preserving model performance. Experiments show that CALM reduces harmful
outputs and outperforms baseline methods in most metrics, offering a
lightweight approach to AI safety with no additional training data or model
fine-tuning, while incurring only a small computational overhead at inference.

</details>


### [289] [Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?](https://arxiv.org/abs/2510.12680)
*Shouren Wang,Wang Yang,Xianxuan Long,Qifan Wang,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: Current hybrid thinking LLMs struggle to fully separate reasoning and direct answering modes, leading to leaks in reasoning behavior during no-think mode.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency and lack of strict mode controllability in hybrid thinking LLMs, which compromise performance in 'no-think' mode.

Method: Analyzed four major factors affecting controllability and proposed a training strategy, including larger datasets, separating think/no-think questions, moderate increase in no-think data, and a two-phase training process.

Result: Proposed training recipe maintained accuracy across modes, reduced no-think response length and reasoning-supportive token occurrences significantly on benchmark dataset MATH500.

Conclusion: Current hybrid thinking approaches have limitations; the suggested methodology improves controllability and provides insights for future advancements.

Abstract: Hybrid thinking enables LLMs to switch between reasoning and direct
answering, offering a balance between efficiency and reasoning capability. Yet
our experiments reveal that current hybrid thinking LLMs only achieve partial
mode separation: reasoning behaviors often leak into the no-think mode. To
understand and mitigate this, we analyze the factors influencing
controllability and identify four that matter most: (1) larger data scale, (2)
using think and no-think answers from different questions rather than the same
question, (3) a moderate increase in no-think data number, and (4) a two-phase
strategy that first trains reasoning ability and then applies hybrid think
training. Building on these findings, we propose a practical recipe that,
compared to standard training, can maintain accuracy in both modes while
significantly reducing no-think output length (from $1085$ to $585$ on MATH500)
and occurrences of reasoning-supportive tokens such as ``\texttt{wait}'' (from
$5917$ to $522$ on MATH500). Our findings highlight the limitations of current
hybrid thinking and offer directions for strengthening its controllability.

</details>


### [290] [CoRA: Covariate-Aware Adaptation of Time Series Foundation Models](https://arxiv.org/abs/2510.12681)
*Guo Qin,Zhi Chen,Yong Liu,Zhiyuan Shi,Haixuan Liu,Xiangdong Huang,Jianmin Wang,Mingsheng Long*

Main category: cs.LG

TL;DR: This paper introduces CoRA, a framework for adapting Time Series Foundation Models (TSFMs) to include information from diverse covariates, achieving significant prediction accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of TSFMs, which are typically pre-trained on univariate data, making them less effective in utilizing covariate information from real-world multivariate and multimodal datasets.

Method: CoRA integrates exogenous covariates such as time series, language, and images, while preserving the frozen backbones of pre-trained TSFMs. It uses Granger Causality Embedding (GCE) to evaluate the causal influence of covariates and introduces a zero-initialized condition-injection mechanism to blend covariate information without forgetting pre-training.

Result: Experiments demonstrate that CoRA outperforms state-of-the-art covariate-aware forecasters, achieving a significant 31.1% reduction in Mean Squared Error (MSE) on forecasting tasks. It also shows strong compatibility with diverse TSFMs and covariate modalities.

Conclusion: CoRA effectively enhances TSFMs by leveraging covariate information, provides robust improvements in forecasting accuracy, and offers a scalable and versatile framework for real-world applications.

Abstract: Time Series Foundation Models (TSFMs) have shown significant impact through
their model capacity, scalability, and zero-shot generalization. However, due
to the heterogeneity of inter-variate dependencies and the backbone scalability
on large-scale multivariate datasets, most TSFMs are typically pre-trained on
univariate time series. This limitation renders them oblivious to crucial
information from diverse covariates in real-world forecasting tasks. To further
enhance the performance of TSFMs, we propose a general covariate-aware
adaptation (CoRA) framework for TSFMs. It leverages pre-trained backbones of
foundation models while effectively incorporating exogenous covariates from
various modalities, including time series, language, and images, to improve the
quality of predictions. Technically, CoRA maintains the equivalence of
initialization and parameter consistency during adaptation. With preserved
backbones of foundation models as frozen feature extractors, the outcome
embeddings from foundation models are empirically demonstrated more informative
than raw data. Further, CoRA employs a novel Granger Causality Embedding (GCE)
to automatically evaluate covariates regarding their causal predictability with
respect to the target variate. We incorporate these weighted embeddings with a
zero-initialized condition-injection mechanism, avoiding catastrophic
forgetting of pre-trained foundation models and gradually integrates exogenous
information. Extensive experiments show that CoRA of TSFMs surpasses
state-of-the-art covariate-aware deep forecasters with full or few-shot
training samples, achieving 31.1% MSE reduction on covariate-aware forecasting.
Compared to other adaptation methods, CoRA exhibits strong compatibility with
various advanced TSFMs and extends the scope of covariates to other modalities,
presenting a practical paradigm for the application of TSFMs.

</details>


### [291] [Few Shot Semi-Supervised Learning for Abnormal Stop Detection from Sparse GPS Trajectories](https://arxiv.org/abs/2510.12686)
*Muhammad Ayub Sabir,Junbiao Pang,Jiaqi Wu,Fatima Ashraf*

Main category: cs.LG

TL;DR: The paper addresses challenges in detecting abnormal stops in intercity coach transportation due to sparse GPS data and limited labeled data. The proposed method achieves high accuracy with minimal labeled instances.


<details>
  <summary>Details</summary>
Motivation: Ensuring passenger safety, operational reliability, and regulatory compliance in intercity transportation, despite challenges like sparse data and label scarcity.

Method: Introduced the Sparsity-Aware Segmentation (SAS) for adaptive data segmentation, domain-specific indicators, Locally Temporal-Indicator Guided Adjustment (LTIGA) for smoothing, spatial-temporal graph construction, label propagation, Graph Convolutional Network (GCN), and self-training with pseudo-labels.

Result: Experimental results on real-world data achieved AUC of 0.854 and AP of 0.866 using only 10 labeled instances, surpassing prior methods.

Conclusion: The proposed approach demonstrates its effectiveness in detecting abnormal stops under sparse data conditions and label scarcity. It offers superior results and is publicly available for further research.

Abstract: Abnormal stop detection (ASD) in intercity coach transportation is critical
for ensuring passenger safety, operational reliability, and regulatory
compliance. However, two key challenges hinder ASD effectiveness: sparse GPS
trajectories, which obscure short or unauthorized stops, and limited labeled
data, which restricts supervised learning. Existing methods often assume dense
sampling or regular movement patterns, limiting their applicability. To address
data sparsity, we propose a Sparsity-Aware Segmentation (SAS) method that
adaptively defines segment boundaries based on local spatial-temporal density.
Building upon these segments, we introduce three domain-specific indicators to
capture abnormal stop behaviors. To further mitigate the impact of sparsity, we
develop Locally Temporal-Indicator Guided Adjustment (LTIGA), which smooths
these indicators via local similarity graphs. To overcome label scarcity, we
construct a spatial-temporal graph where each segment is a node with
LTIGA-refined features. We apply label propagation to expand weak supervision
across the graph, followed by a GCN to learn relational patterns. A final
self-training module incorporates high-confidence pseudo-labels to iteratively
improve predictions. Experiments on real-world coach data show an AUC of 0.854
and AP of 0.866 using only 10 labeled instances, outperforming prior methods.
The code and dataset are publicly available at
\href{https://github.com/pangjunbiao/Abnormal-Stop-Detection-SSL.git}

</details>


### [292] [DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization](https://arxiv.org/abs/2510.12691)
*Danial Hosseintabar,Fan Chen,Giannis Daras,Antonio Torralba,Constantinos Daskalakis*

Main category: cs.LG

TL;DR: This paper proposes DiffEM, a novel method to train diffusion models from corrupted data using Expectation-Maximization steps, showcasing convergence guarantees and experimental effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in training diffusion models when only corrupted or noisy observations are available.

Method: The DiffEM method employs Expectation-Maximization by using conditional diffusion models: reconstructing clean data in the E-step and refining the models in the M-step.

Result: DiffEM is supported by convergence guarantees under statistical conditions and shown to be effective in various image reconstruction experiments.

Conclusion: DiffEM advances the ability to train diffusion models with corrupted data, improving the accuracy and applicability of generative priors in inverse problems.

Abstract: Diffusion models have emerged as powerful generative priors for
high-dimensional inverse problems, yet learning them when only corrupted or
noisy observations are available remains challenging. In this work, we propose
a new method for training diffusion models with Expectation-Maximization (EM)
from corrupted data. Our proposed method, DiffEM, utilizes conditional
diffusion models to reconstruct clean data from observations in the E-step, and
then uses the reconstructed data to refine the conditional diffusion model in
the M-step. Theoretically, we provide monotonic convergence guarantees for the
DiffEM iteration, assuming appropriate statistical conditions. We demonstrate
the effectiveness of our approach through experiments on various image
reconstruction tasks.

</details>


### [293] [Multitask finetuning and acceleration of chemical pretrained models for small molecule drug property prediction](https://arxiv.org/abs/2510.12719)
*Matthew Adrian,Yunsie Chung,Kevin Boyd,Saee Paliwal,Srimukh Prasad Veccham,Alan C. Cheng*

Main category: cs.LG

TL;DR: The study demonstrates that multitask finetuning of pretrained chemical graph neural network models significantly boosts predictive performance, especially with larger datasets, aiding critical drug discovery processes.


<details>
  <summary>Details</summary>
Motivation: To enhance predictive model performance for drug discovery endpoints by leveraging multitask learning in chemical pretrained graph neural networks.

Method: The paper utilizes multitask finetuning approaches on pretrained graph neural network models like Kinetic GROVER Multi-Task (KERMT) and KGPT, comparing their performance to non-pretrained models.

Result: Multitask finetuned chemical models, particularly KERMT, outperform non-pretrained graph neural network models, with the improvement most pronounced at larger data sizes.

Conclusion: Multitask chemical pretrained models significantly advance drug property prediction, and new ADMET data splits and accelerated implementations facilitate better benchmarking and practical applications in drug discovery workflows.

Abstract: Chemical pretrained models, sometimes referred to as foundation models, are
receiving considerable interest for drug discovery applications. The general
chemical knowledge extracted from self-supervised training has the potential to
improve predictions for critical drug discovery endpoints, including on-target
potency and ADMET properties. Multi-task learning has previously been
successfully leveraged to improve predictive models. Here, we show that
enabling multitasking in finetuning of chemical pretrained graph neural network
models such as Kinetic GROVER Multi-Task (KERMT), an enhanced version of the
GROVER model, and Knowledge-guided Pre-training of Graph Transformer (KGPT)
significantly improves performance over non-pretrained graph neural network
models. Surprisingly, we find that the performance improvement from finetuning
KERMT in a multitask manner is most significant at larger data sizes.
Additionally, we publish two multitask ADMET data splits to enable more
accurate benchmarking of multitask deep learning methods for drug property
prediction. Finally, we provide an accelerated implementation of the KERMT
model on GitHub, unlocking large-scale pretraining, finetuning, and inference
in industrial drug discovery workflows.

</details>


### [294] [CARVQ: Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression](https://arxiv.org/abs/2510.12721)
*Dayin Gou,Sanghyun Byun,Nilesh Malpeddi,Gabrielle De Micheli,Prathamesh Vaste,Jacob Song,Woo Seong Chung*

Main category: cs.LG

TL;DR: The paper introduces CARVQ, a compression technique for large language models (LLMs) to reduce memory requirements while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: LLMs demand substantial storage and memory, posing challenges for deployment on edge devices. Compressing the embedding layer can mitigate these issues.

Method: CARVQ employs a novel corrective adaptor with group residual vector quantization, combining linear and non-linear mappings to mimic original embeddings. It achieves compression without specialized hardware.

Result: CARVQ compresses embeddings to around 1.6 bits, achieving lower bitwidth-per-parameter while maintaining reasonable performance across various tasks compared to scalar quantization.

Conclusion: CARVQ facilitates efficient LLM deployment on memory-constrained edge devices, contributing to transformer quantization advancements.

Abstract: Large Language Models (LLMs) typically rely on a large number of parameters
for token embedding, leading to substantial storage requirements and memory
footprints. In particular, LLMs deployed on edge devices are memory-bound, and
reducing the memory footprint by compressing the embedding layer not only frees
up the memory bandwidth but also speeds up inference. To address this, we
introduce CARVQ, a post-training novel Corrective Adaptor combined with group
Residual Vector Quantization. CARVQ relies on the composition of both linear
and non-linear maps and mimics the original model embedding to compress to
approximately 1.6 bits without requiring specialized hardware to support
lower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B,
LLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B
and Phi-4, evaluating on common generative, discriminative, math and reasoning
tasks. We show that in most cases, CARVQ can achieve lower average
bitwidth-per-parameter while maintaining reasonable perplexity and accuracy
compared to scalar quantization. Our contributions include a novel compression
technique that is compatible with state-of-the-art transformer quantization
methods and can be seamlessly integrated into any hardware supporting 4-bit
memory to reduce the model's memory footprint in memory-constrained devices.
This work demonstrates a crucial step toward the efficient deployment of LLMs
on edge devices.

</details>


### [295] [Improving Decision Trees through the Lens of Parameterized Local Search](https://arxiv.org/abs/2510.12726)
*Juha Harviainen,Frank Sommer,Manuel Sorge*

Main category: cs.LG

TL;DR: The paper explores the NP-complete nature of optimizing decision tree thresholds and features while providing parameterized-complexity insights and an algorithm that is efficient under specific conditions.


<details>
  <summary>Details</summary>
Motivation: To understand why optimizing decision tree thresholds and features is computationally hard and identify scenarios where it becomes tractable.

Method: The study conducts a parameterized-complexity analysis, focusing on properties like the number of features (d) and domain size (D), and provides an algorithm for fixed-parameter tractable cases. They back this with both theoretical proof and empirical evidence.

Result: The work demonstrates that while the problems are NP-complete in general, combining a small domain size (D) and feature count (d) makes the problems fixed-parameter tractable, solvable in $(D + 1)^{2d} \cdot |I|^{O(1)}$ time.

Conclusion: The research identifies concrete traits making decision tree optimization hard and outlines computationally feasible cases, providing an algorithm tested both theoretically and empirically.

Abstract: Algorithms for learning decision trees often include heuristic local-search
operations such as (1) adjusting the threshold of a cut or (2) also exchanging
the feature of that cut. We study minimizing the number of classification
errors by performing a fixed number of a single type of these operations.
Although we discover that the corresponding problems are NP-complete in
general, we provide a comprehensive parameterized-complexity analysis with the
aim of determining those properties of the problems that explain the hardness
and those that make the problems tractable. For instance, we show that the
problems remain hard for a small number $d$ of features or small domain size
$D$ but the combination of both yields fixed-parameter tractability. That is,
the problems are solvable in $(D + 1)^{2d} \cdot |I|^{O(1)}$ time, where $|I|$
is the size of the input. We also provide a proof-of-concept implementation of
this algorithm and report on empirical results.

</details>


### [296] [Doctor Rashomon and the UNIVERSE of Madness: Variable Importance with Unobserved Confounding and the Rashomon Effect](https://arxiv.org/abs/2510.12734)
*Jon Donnelly,Srikar Katta,Emanuele Borgonovo,Cynthia Rudin*

Main category: cs.LG

TL;DR: UNIVERSE introduces a method to estimate variable importance (VI) considering missing features and the Rashomon effect, offering bounds for true VI across near-optimal models.


<details>
  <summary>Details</summary>
Motivation: Variable importance methods face challenges due to dependence on model composition and missing essential variables in datasets. The Rashomon Effect further complicates VI estimation within equally-good predictive models.

Method: The paper introduces UNIVERSE, leveraging Rashomon sets to estimate true VI bounds robustly even with missing features and varying model compositions.

Result: Theoretical guarantees for robustness were established, strong performance validated via semi-synthetic simulations, and practical utility showcased in a credit risk prediction task.

Conclusion: UNIVERSE addresses critical gaps in VI estimation by providing robust bounds across model sets and facilitating better insights despite dataset limitations and model diversity.

Abstract: Variable importance (VI) methods are often used for hypothesis generation,
feature selection, and scientific validation. In the standard VI pipeline, an
analyst estimates VI for a single predictive model with only the observed
features. However, the importance of a feature depends heavily on which other
variables are included in the model, and essential variables are often omitted
from observational datasets. Moreover, the VI estimated for one model is often
not the same as the VI estimated for another equally-good model - a phenomenon
known as the Rashomon Effect. We address these gaps by introducing
UNobservables and Inference for Variable importancE using Rashomon SEts
(UNIVERSE). Our approach adapts Rashomon sets - the sets of near-optimal models
in a dataset - to produce bounds on the true VI even with missing features. We
theoretically guarantee the robustness of our approach, show strong performance
on semi-synthetic simulations, and demonstrate its utility in a credit risk
task.

</details>


### [297] [KoALA: KL-L0 Adversarial Detector via Label Agreement](https://arxiv.org/abs/2510.12752)
*Siqi Li,Yasser Shoukry*

Main category: cs.LG

TL;DR: The paper introduces KoALA, an adversarial detector for deep neural networks that uses two complementary similarity metrics to identify attacks without requiring architectural changes or adversarial retraining.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are vulnerable to adversarial attacks, posing risks to security- and safety-critical applications, which necessitates robust solutions for attack detection.

Method: KoALA detects adversarial attacks by evaluating class label agreement using KL divergence (for dense, low-amplitude perturbations) and an L0-based similarity metric (for sparse, high-impact perturbations). A fine-tuning step on a pre-trained encoder aligns embeddings with these metrics.

Result: Experimental results show that KoALA achieves high precision (0.94 on ResNet/CIFAR-10, 0.66 on CLIP/Tiny-ImageNet) and recall (0.81 on ResNet/CIFAR-10, 0.85 on CLIP/Tiny-ImageNet), validating its effectiveness under the stated conditions.

Conclusion: KoALA is a lightweight, effective, and versatile adversarial detector that can easily integrate with existing models without requiring architectural modifications or adversarial retraining.

Abstract: Deep neural networks are highly susceptible to adversarial attacks, which
pose significant risks to security- and safety-critical applications. We
present KoALA (KL-L0 Adversarial detection via Label Agreement), a novel,
semantics-free adversarial detector that requires no architectural changes or
adversarial retraining. KoALA operates on a simple principle: it detects an
adversarial attack when class predictions from two complementary similarity
metrics disagree. These metrics-KL divergence and an L0-based similarity-are
specifically chosen to detect different types of perturbations. The KL
divergence metric is sensitive to dense, low-amplitude shifts, while the
L0-based similarity is designed for sparse, high-impact changes. We provide a
formal proof of correctness for our approach. The only training required is a
simple fine-tuning step on a pre-trained image encoder using clean images to
ensure the embeddings align well with both metrics. This makes KOALA a
lightweight, plug-and-play solution for existing models and various data
modalities. Our extensive experiments on ResNet/CIFAR-10 and CLIP/Tiny-ImageNet
confirm our theoretical claims. When the theorem's conditions are met, KoALA
consistently and effectively detects adversarial examples. On the full test
sets, KoALA achieves a precision of 0.94 and a recall of 0.81 on
ResNet/CIFAR-10, and a precision of 0.66 and a recall of 0.85 on
CLIP/Tiny-ImageNet.

</details>


### [298] [Sample-Efficient Omniprediction for Proper Losses](https://arxiv.org/abs/2510.12769)
*Isaac Gibbs,Ryan J. Tibshirani*

Main category: cs.LG

TL;DR: The paper addresses constructing probabilistic predictors for accurate decisions across single and multiple decision makers, comparing existing methods, and proposing improvements to optimize sample efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create predictors that ensure accurate decision-making for either single or multiple users, recognizing challenges in optimizing over diverse needs and losses.

Method: The paper evaluates existing omniprediction methods, offers complexity analysis, proposes an online-to-batch conversion for sample-efficient prediction, and designs a new direct unrandomized algorithm leveraging structural properties.

Result: It shows multicalibration to be harder than omniprediction, discusses limitations of randomized predictors, and introduces a more efficient unrandomized algorithm.

Conclusion: The study advances sample-efficient prediction methods, overcoming flaws in existing approaches and providing direct, robust algorithms for minimizing decision-making losses.

Abstract: We consider the problem of constructing probabilistic predictions that lead
to accurate decisions when employed by downstream users to inform actions. For
a single decision maker, designing an optimal predictor is equivalent to
minimizing a proper loss function corresponding to the negative utility of that
individual. For multiple decision makers, our problem can be viewed as a
variant of omniprediction in which the goal is to design a single predictor
that simultaneously minimizes multiple losses. Existing algorithms for
achieving omniprediction broadly fall into two categories: 1) boosting methods
that optimize other auxiliary targets such as multicalibration and obtain
omniprediction as a corollary, and 2) adversarial two-player game based
approaches that estimate and respond to the ``worst-case" loss in an online
fashion. We give lower bounds demonstrating that multicalibration is a strictly
more difficult problem than omniprediction and thus the former approach must
incur suboptimal sample complexity. For the latter approach, we discuss how
these ideas can be used to obtain a sample-efficient algorithm through an
online-to-batch conversion. This conversion has the downside of returning a
complex, randomized predictor. We improve on this method by designing a more
direct, unrandomized algorithm that exploits structural elements of the set of
proper losses.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [299] [SpikePool: Event-driven Spiking Transformer with Pooling Attention](https://arxiv.org/abs/2510.12102)
*Donghyun Lee,Alex Sima,Yuhang Li,Panos Stinis,Priyadarshini Panda*

Main category: cs.NE

TL;DR: The study uncovers that spiking transformers act as high-pass filters contrasting Vision Transformers' low-pass filters, proposing SpikePool to improve event-based data processing.


<details>
  <summary>Details</summary>
Motivation: To provide a deeper understanding of how spiking transformers process event-based data, which has been underexplored in literature.

Method: Analyzing spiking transformers in the frequency domain, discovering functional behavior, and introducing SpikePool, which uses a low-pass filtering mechanism for efficient data processing.

Result: SpikePool enables competitive results on object detection and classification tasks, reducing training and inference times by 42.5% and 32.8%, respectively.

Conclusion: The frequency domain analysis of spiking transformers enhances understanding and performance in event-based vision tasks, fostering efficient processing through innovations like SpikePool.

Abstract: Building on the success of transformers, Spiking Neural Networks (SNNs) have
increasingly been integrated with transformer architectures, leading to spiking
transformers that demonstrate promising performance on event-based vision
tasks. However, despite these empirical successes, there remains limited
understanding of how spiking transformers fundamentally process event-based
data. Current approaches primarily focus on architectural modifications without
analyzing the underlying signal processing characteristics. In this work, we
analyze spiking transformers through the frequency spectrum domain and discover
that they behave as high-pass filters, contrasting with Vision Transformers
(ViTs) that act as low-pass filters. This frequency domain analysis reveals why
certain designs work well for event-based data, which contains valuable
high-frequency information but is also sparse and noisy. Based on this
observation, we propose SpikePool, which replaces spike-based self-attention
with max pooling attention, a low-pass filtering operation, to create a
selective band-pass filtering effect. This design preserves meaningful
high-frequency content while capturing critical features and suppressing noise,
achieving a better balance for event-based data processing. Our approach
demonstrates competitive results on event-based datasets for both
classification and object detection tasks while significantly reducing training
and inference time by up to 42.5% and 32.8%, respectively.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [300] [Analysis and Evaluation of Using Microsecond-Latency Memory for In-Memory Indices and Caches in SSD-Based Key-Value Stores](https://arxiv.org/abs/2510.12280)
*Yosuke Bando,Akinobu Mita,Kazuhiro Hiwada,Shintaro Sano,Tomoya Suzuki,Yu Nakanishi,Kazutaka Tomida,Hirotsugu Kajihara,Akiyuki Kaneko,Daisuke Taki,Yukimasa Miyamoto,Tomokazu Yoshida,Tatsuo Shiozawa*

Main category: cs.PF

TL;DR: This paper proposes using cost-effective, microsecond-latency memory for SSD-based key-value stores instead of expensive DRAM, and demonstrates that software techniques like prefetching can mitigate the latency impacts effectively.


<details>
  <summary>Details</summary>
Motivation: Current SSD-based key-value stores rely on costly DRAM for managing large in-memory data structures. The paper explores a more affordable alternative using microsecond-latency memory and aims to understand its performance impact while analyzing ways to mitigate potential bottlenecks.

Method: The study models the interaction between prefetching techniques and IO operations to analyze memory latency effects on throughput. It uses microbenchmarks and modifies existing SSD-based KV stores, testing them on FPGA-based memory with adjustable latency levels.

Result: The experiments validate the proposed model, showing that KV stores achieve near-DRAM throughput performance for latency up to 5 microseconds. This demonstrates that microsecond-latency memory can perform effectively when paired with latency-hiding techniques like software prefetching.

Conclusion: SSD-based KV stores can use microsecond-latency memory as a viable, cost-efficient alternative to DRAM without requiring entirely new latency mitigation techniques, making it promising for future SSD-based applications.

Abstract: When key-value (KV) stores use SSDs for storing a large number of items,
oftentimes they also require large in-memory data structures including indices
and caches to be traversed to reduce IOs. This paper considers offloading most
of such data structures from the costly host DRAM to secondary memory whose
latency is in the microsecond range, an order of magnitude longer than those of
currently available DIMM-mounted or CXL memory devices. While emerging
microsecond-latency memory is likely to cost much less than DRAM, it can
significantly slow down SSD-based KV stores if naively employed. This paper
analyzes and evaluates the impact of microsecond-level memory latency on the KV
operation throughput. Our analysis finds that a well-known latency-hiding
technique of software prefetching for long-latency memory from user-level
threads is effective. The novelty of our analysis lies in modeling how the
interplay between prefetching and IO affects performance, from which we derive
an equation that well explains the throughput degradation due to long memory
latency. The model tells us that the presence of IO significantly enhances the
tolerance to memory latency, leading to a finding that SSD-based KV stores can
be made latency-tolerant without devising new techniques for
microsecond-latency memory. To confirm this, we design a microbenchmark as well
as modify existing SSD-based KV stores so that they issue prefetches from
user-level threads, and run them while placing most of in-memory data
structures on FPGA-based memory with adjustable microsecond latency. The
results demonstrate that their KV operation throughputs can be well explained
by our model, and the modified KV stores achieve near-DRAM throughputs for up
to a memory latency of 5 microseconds. This suggests the possibility that
SSD-based KV stores can use microsecond-latency memory as a cost-effective
alternative to the host DRAM.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [301] [Verifying Correctness of Shared Channels in a Cooperatively Scheduled Process-Oriented Language](https://arxiv.org/abs/2510.11751)
*Jan Pedersen,Kevin Chalmers*

Main category: cs.PL

TL;DR: The paper studies shared communicating channels in a cooperatively scheduled runtime, showing implementations depend on resource availability and emphasizing runtime modeling for accurate concurrent behavior.


<details>
  <summary>Details</summary>
Motivation: Understanding and ensuring correct behavior of concurrent components in specific runtime environments.

Method: Utilized FDR for refinement checking and modeling, specifying behaviors and simulating implementations in the ProcessJ language.

Result: Correct channel behavior can be implemented, but its success relies on sufficient resources to execute involved processes.

Conclusion: Modeling runtime environments is essential to guarantee real-world behavior aligns with specifications of concurrent components.

Abstract: Correct concurrent behaviour is important in understanding how components
will act within certain conditions. In this work. we analyse the behaviour of
shared communicating channels within a coorporatively scheduled runtime. We use
the refinement checking and modelling tool FDR to develop both specifications
of how such shared channels should behave and models of the implementations of
these channels in the cooperatively scheduled language ProcessJ. Our results
demonstrate that although we can certainly implement the correct behaviour of
such channels, the outcome is dependant on having adequate resources available
to execute all processes involved. We conclude that modelling the runtime
environment of concurrent components is necessary to ensure components behave
as specified in the real world.

</details>


### [302] [AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework](https://arxiv.org/abs/2510.11759)
*Hongyu Lin,Haolin Pan,Haoran Luo,Yuchen Li,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.PL

TL;DR: The paper introduces AwareCompiler, a framework for automating compiler optimization using large language models, overcoming challenges of semantic misalignment, inefficient agent-compiler interaction, and reward sparsity.


<details>
  <summary>Details</summary>
Motivation: Automating compiler optimization to improve program performance while addressing issues like semantic misalignment, inefficient interaction mechanisms, and sparse rewards in optimization spaces.

Method: AwareCompiler combines structured knowledge integration, adaptive pass generation, and a hybrid data-driven training pipeline to enhance compiler optimization.

Result: Experiments show AwareCompiler surpasses existing methods regarding performance and efficiency based on standard benchmarks.

Conclusion: AwareCompiler provides an effective solution for compiler optimization, demonstrating its potential in optimizing performance and efficiency with a knowledge-driven approach.

Abstract: Compiler optimization is crucial for enhancing program performance by
transforming the sequence of optimization passes while maintaining correctness.
Despite the promising potential of large language models (LLMs)-based agent for
software optimization, automating compiler optimization remains challenging due
to: (1) semantic misalignment between abstract program representations and
concrete optimization passes, (2) inefficient interaction mechanisms between
agents and compiler environments, and (3) reward sparsity from the extensive
decision-making process within large optimization spaces. This paper introduces
\textbf{AwareCompiler}, an agentic framework for compiler optimization that
addresses these challenges through three key innovations: structured knowledge
integration and dataset construction, knowledge-driven adaptive pass
generation, and data-driven hybrid training pipeline. Experimental results on
standard benchmarks demonstrate that AwareCompiler significantly outperforms
existing baselines in both performance and efficiency, highlighting the
effectiveness of our synergistic knowledge-data-driven approach. Our code is
publicly available at https://github.com/LHY-24/AwareCompiler.

</details>


### [303] [Functional Reasoning for Distributed Systems with Failures](https://arxiv.org/abs/2510.12131)
*Haobin Ni,Robbert van Renesse,Greg Morrisett*

Main category: cs.PL

TL;DR: This paper introduces a formalized reasoning framework for distributed systems using twin languages, Sync (synchronous) and Async (asynchronous). These frameworks provide a structured way to argue correctness and verify safety properties in distributed protocols, demonstrated on BOSCO and SeqPaxos.


<details>
  <summary>Details</summary>
Motivation: To address the informal and error-prone reasoning in distributed system theory and bridge the gap with formal methods, enabling reliable correctness arguments for distributed systems.

Method: The authors propose twin languages, Sync and Async. Sync uses a synchronous model for simplifying reasoning and employs a denotational semantics for formal proofs. Async models real-world asynchrony with interleaving semantics. Sync programs are compiled into Async programs, maintaining safety properties proven in Sync.

Result: The authors prove the preservation of safety properties from Sync to Async and implement these concepts in the Rocq framework. They validate their approach by verifying safety properties of two consensus protocols, BOSCO and SeqPaxos.

Conclusion: The proposed twin language approach provides a robust, compositional method for formal reasoning about distributed systems, bridging theory with practical implementation while ensuring safety property preservation.

Abstract: Distributed system theory literature often argues for correctness using an
informal, Hoare-like style of reasoning. While these arguments are intuitive,
they have not all been foolproof, and whether they directly correspond to
formal proofs is in question. We formally ground this kind of reasoning and
connect it to standard formal approaches through language design and
meta-analysis, which leads to a functional style of compositional formal
reasoning for a class of distributed systems, including cases involving
Byzantine faults. The core of our approach is twin languages: Sync and Async,
which formalize the insight from distributed system theory that an asynchronous
system can be reduced to a synchronous system for more straightforward
reasoning under certain conditions. Sync describes a distributed system as a
single, synchronous, data-parallel program. It restricts programs syntactically
and has a functional denotational semantics suitable for Hoare-style formal
reasoning. Async models a distributed system as a collection of interacting
monadic programs, one for each non-faulty node in the system. It has a standard
trace-based operational semantics, modeling asynchrony with interleaving. Sync
compiles to Async and can then be extracted to yield executable code. We prove
that any safety property proven for a Sync program in its denotational
semantics is preserved in the operational semantics of its compiled Async
programs. We implement the twin languages in Rocq and verify the safety
properties of two fault-tolerant consensus protocols: BOSCO and SeqPaxos.

</details>


### [304] [Operational methods in semantics](https://arxiv.org/abs/2510.12295)
*Roberto M. Amadio*

Main category: cs.PL

TL;DR: This paper introduces abstract models for the operational semantics of programming languages, covering equivalences, specification languages, and static analyses.


<details>
  <summary>Details</summary>
Motivation: To effectively study and utilize an approach for describing and analyzing the computation steps and semantics of programming languages.

Method: The approach focuses on operational semantics, starting with abstract description of computation steps and building on it to develop formal structures for semantic analysis.

Result: Operational semantics is demonstrated to be effective, requiring moderate mathematical sophistication and being versatile across various programming features.

Conclusion: Operational semantics provides a practical and scalable framework for tasks like building portable implementations and proving correctness of compilers or analyzers.

Abstract: The focus of these lecture notes is on abstract models and basic ideas and
results that relate to the operational semantics of programming languages
largely conceived. The approach is to start with an abstract description of the
computation steps of programs and then to build on top semantic equivalences,
specification languages, and static analyses. While other approaches to the
semantics of programming languages are possible, it appears that the
operational one is particularly effective in that it requires a moderate level
of mathematical sophistication and scales reasonably well to a large variety of
programming features. In practice, operational semantics is a suitable
framework to build portable language implementations and to specify and test
program properties. It is also used routinely to tackle more ambitious tasks
such as proving the correctness of a compiler or a static analyzer.

</details>


### [305] [GUPPY: Pythonic Quantum-Classical Programming](https://arxiv.org/abs/2510.12582)
*Mark Koch,Alan Lawrence,Kartik Singhal,Seyon Sivarajah,Ross Duncan*

Main category: cs.PL

TL;DR: Guppy introduces a domain-specific language embedded in Python for writing high-level hybrid quantum programs with complex control flow.


<details>
  <summary>Details</summary>
Motivation: To create a practical tool for users to write hybrid quantum programs with Python syntax that can run on actual quantum hardware.

Method: Developed Guppy, a Python-embedded domain-specific language tailored for hybrid quantum programming and complex control flows.

Result: Ongoing work on Guppy showcases its potential for enabling hybrid quantum programming in Python syntax.

Conclusion: Guppy provides a promising framework for efficient programming in quantum computing using Python.

Abstract: We present ongoing work on Guppy, a domain-specific language embedded in
Python that allows users to write high-level hybrid quantum programs with
complex control flow in Pythonic syntax, aiming to run them on actual quantum
hardware.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [306] [Translating Milli/Microrobots with A Value-Centered Readiness Framework](https://arxiv.org/abs/2510.12090)
*Hakan Ceylan,Edoardo Sinibaldi,Sanjay Misra,Pankaj J. Pasricha,Dietmar W. Hutmacher*

Main category: cs.RO

TL;DR: The paper reviews the challenges in translating milli/microrobots from laboratory concepts to clinical applications and proposes a Technology Readiness Level framework to facilitate this process.


<details>
  <summary>Details</summary>
Motivation: To overcome the barriers in adopting milli/microrobots for interventional medicine and improve their real-world feasibility in diagnostic and therapeutic applications.

Method: Development of a milli/microrobot Technology Readiness Level framework (mTRL) to outline clear milestones for advancing the technology toward clinical integration.

Result: The proposed mTRL framework offers structured milestones and activities, ensuring alignment with medical needs and supporting translational development of milli/microrobots.

Conclusion: By utilizing the mTRL framework, milli/microrobot technology can advance more effectively toward clinical adoption, enhancing patient outcomes through safer and more efficient medical interventions.

Abstract: Untethered mobile milli/microrobots hold transformative potential for
interventional medicine by enabling more precise and entirely non-invasive
diagnosis and therapy. Realizing this promise requires bridging the gap between
groundbreaking laboratory demonstrations and successful clinical integration.
Despite remarkable technical progress over the past two decades, most
millirobots and microrobots remain confined to laboratory proof-of-concept
demonstrations, with limited real-world feasibility. In this Review, we
identify key factors that slow translation from bench to bedside, focusing on
the disconnect between technical innovation and real-world application. We
argue that the long-term impact and sustainability of the field depend on
aligning development with unmet medical needs, ensuring applied feasibility,
and integrating seamlessly into existing clinical workflows, which are
essential pillars for delivering meaningful patient outcomes. To support this
shift, we introduce a strategic milli/microrobot Technology Readiness Level
framework (mTRL), which maps system development from initial conceptualization
to clinical adoption through clearly defined milestones and their associated
stepwise activities. The mTRL model provides a structured gauge of
technological maturity, a common language for cross-disciplinary collaboration
and actionable guidance to accelerate translational development toward new,
safer and more efficient interventions.

</details>


### [307] [Gaussian Semantic Field for One-shot LiDAR Global Localization](https://arxiv.org/abs/2510.12101)
*Pengyu Yin,Shenghai Yuan,Haozhi Cao,Xingyu Ji,Ruofei Bai,Siyu Chen,Lihua Xie*

Main category: cs.RO

TL;DR: A lightweight tri-layered scene graph is introduced for one-shot LiDAR global localization, enhancing semantic disambiguation using continuous functions derived from Gaussian processes.


<details>
  <summary>Details</summary>
Motivation: Landmark-based methods improve global localization but may struggle with repetition and misleading correspondences.

Method: Uses continuous semantic functions learned from Gaussian processes, organized into a tri-layered 3D scene graph for localization.

Result: Achieved superior performance as demonstrated in experiments on public datasets against state-of-the-art methods.

Conclusion: The proposed pipeline, Outram-GSF, efficiently addresses semantic disambiguation, proving its utility in global localization.

Abstract: We present a one-shot LiDAR global localization algorithm featuring semantic
disambiguation ability based on a lightweight tri-layered scene graph. While
landmark semantic registration-based methods have shown promising performance
improvements in global localization compared with geometric-only methods,
landmarks can be repetitive and misleading for correspondence establishment. We
propose to mitigate this problem by modeling semantic distributions with
continuous functions learned from a population of Gaussian processes. Compared
with discrete semantic labels, the continuous functions capture finer-grained
geo-semantic information and also provide more detailed metric information for
correspondence establishment. We insert this continuous function as the middle
layer between the object layer and the metric-semantic layer, forming a
tri-layered 3D scene graph, serving as a light-weight yet performant backend
for one-shot localization. We term our global localization pipeline Outram-GSF
(Gaussian semantic field) and conduct a wide range of experiments on publicly
available data sets, validating the superior performance against the current
state-of-the-art.

</details>


### [308] [Hybrid Terrain-Aware Path Planning: Integrating VD--RRT\(^{*}\) Exploration and VD--D\(^{*}\) Lite Repair](https://arxiv.org/abs/2510.12169)
*Akshay Naik,William R. Norris,Dustin Nottage,Ahmet Soylemezoglu*

Main category: cs.RO

TL;DR: This paper focuses on real-time navigation for autonomous ground vehicles in off-road environments, introducing a new continuous terrain cost metric and efficient planning algorithms for handling deformable terrain.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enable autonomous ground vehicles to navigate challenging off-road terrains with varying soil strength and slopes, overcoming issues like sensor noise and path feasibility.

Method: The paper proposes a continuous terrain cost metric based on the Bekker pressure-sinkage model and slope penalties, combined with a planning framework using Vehicle-Dynamics RRT* for exploration and D* Lite for real-time local path repair.

Result: The proposed methods enable millisecond-scale replanning, ensuring curvature-feasible paths and stability under noisy sensors, and they are validated with hardware trials in unstructured off-road environments.

Conclusion: The framework provides a robust basis for real-time autonomous navigation in deformable terrains, with potential for integration into deterministic, sampling-based, or learning-driven planning methods.

Abstract: Autonomous ground vehicles operating off-road must plan curvature-feasible
paths while accounting for spatially varying soil strength and slope hazards in
real time. We present a continuous state--cost metric that combines a Bekker
pressure--sinkage model with elevation-derived slope and attitude penalties.
The resulting terrain cost field is analytic, bounded, and monotonic in soil
modulus and slope, ensuring well-posed discretization and stable updates under
sensor noise. This metric is evaluated on a lattice with exact steering
primitives: Dubins and Reeds--Shepp motions for differential drive and
time-parameterized bicycle arcs for Ackermann steering. Global exploration is
performed using Vehicle-Dynamics RRT\(^{*}\), while local repair is managed by
Vehicle-Dynamics D\(^{*}\) Lite, enabling millisecond-scale replanning without
heuristic smoothing. By separating the terrain--vehicle model from the planner,
the framework provides a reusable basis for deterministic, sampling-based, or
learning-driven planning in deformable terrain. Hardware trials on an off-road
platform demonstrate real-time navigation across soft soil and slope
transitions, supporting reliable autonomy in unstructured environments.

</details>


### [309] [Controllable Collision Scenario Generation via Collision Pattern Prediction](https://arxiv.org/abs/2510.12206)
*Pin-Lun Chen,Chi-Hsi Kung,Che-Han Chang,Wei-Chen Chiu,Yi-Ting Chen*

Main category: cs.RO

TL;DR: The paper introduces a method for controllably generating collision scenarios to evaluate autonomous vehicle safety. A dataset called COLLIDE and a predictive framework were developed to achieve this goal.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles require testing in diverse, safety-critical scenarios, especially collisions, which are rare and unsafe to replicate in real-world settings.

Method: The authors created a large dataset (COLLIDE) from real-world driving logs and proposed a framework to predict collision patterns, which are then used to generate full adversarial trajectories.

Result: The proposed method achieved better control over collision scenarios and increased planner failure rates during testing, indicating limitations in current planners.

Conclusion: The generated scenarios effectively improve planner robustness, which can lead to safer AV deployment across various collision situations.

Abstract: Evaluating the safety of autonomous vehicles (AVs) requires diverse,
safety-critical scenarios, with collisions being especially important yet rare
and unsafe to collect in the real world. Therefore, the community has been
focusing on generating safety-critical scenarios in simulation. However,
controlling attributes such as collision type and time-to-accident (TTA)
remains challenging. We introduce a new task called controllable collision
scenario generation, where the goal is to produce trajectories that realize a
user-specified collision type and TTA, to investigate the feasibility of
automatically generating desired collision scenarios. To support this task, we
present COLLIDE, a large-scale collision scenario dataset constructed by
transforming real-world driving logs into diverse collisions, balanced across
five representative collision types and different TTA intervals. We propose a
framework that predicts Collision Pattern, a compact and interpretable
representation that captures the spatial configuration of the ego and the
adversarial vehicles at impact, before rolling out full adversarial
trajectories. Experiments show that our approach outperforms strong baselines
in both collision rate and controllability. Furthermore, generated scenarios
consistently induce higher planner failure rates, revealing limitations of
existing planners. We demonstrate that these scenarios fine-tune planners for
robustness improvements, contributing to safer AV deployment in different
collision scenarios.

</details>


### [310] [Learning Social Navigation from Positive and Negative Demonstrations and Rule-Based Specifications](https://arxiv.org/abs/2510.12215)
*Chanwoo Kim,Jihwan Yoon,Hyeonseong Kim,Taemoon Jeong,Changwoo Yoo,Seungbeen Lee,Soohwan Byeon,Hoon Chung,Matthew Pan,Jean Oh,Kyungjae Lee,Sungjoon Choi*

Main category: cs.RO

TL;DR: The paper proposes a navigation framework for mobile robots in dynamic environments, merging data-driven rewards with rule-based goals to enhance adaptability and safety, validated through both simulations and real-world demonstrations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of developing navigation policies for robots that can balance adaptability to dynamic human behaviors with adherence to safety constraints.

Method: The framework combines density-based rewards derived from demonstrations with rule-based objectives for obstacle avoidance and goal-reaching. It uses a sampling-based controller for generating supervisory actions and distills these into a compact real-time policy with uncertainty estimation.

Result: Results from simulations and real-world scenarios show improved success rates and time efficiency compared to baselines, showcasing the method's effectiveness.

Conclusion: The study concludes that integrating data-driven and rule-based approaches enhances navigation adaptability and safety, making the framework practical for deployment in real-world dynamic environments.

Abstract: Mobile robot navigation in dynamic human environments requires policies that
balance adaptability to diverse behaviors with compliance to safety
constraints. We hypothesize that integrating data-driven rewards with
rule-based objectives enables navigation policies to achieve a more effective
balance of adaptability and safety. To this end, we develop a framework that
learns a density-based reward from positive and negative demonstrations and
augments it with rule-based objectives for obstacle avoidance and goal
reaching. A sampling-based lookahead controller produces supervisory actions
that are both safe and adaptive, which are subsequently distilled into a
compact student policy suitable for real-time operation with uncertainty
estimates. Experiments in synthetic and elevator co-boarding simulations show
consistent gains in success rate and time efficiency over baselines, and
real-world demonstrations with human participants confirm the practicality of
deployment. A video illustrating this work can be found on our project page
https://chanwookim971024.github.io/PioneeR/.

</details>


### [311] [Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model](https://arxiv.org/abs/2510.12276)
*Fuhao Li,Wenxuan Song,Han Zhao,Jingbo Wang,Pengxiang Ding,Donglin Wang,Long Zeng,Haoang Li*

Main category: cs.RO

TL;DR: The paper introduces Spatial Forcing (SF) as a new strategy for vision-language-action (VLA) models, enhancing spatial comprehension without relying on explicit 3D inputs.


<details>
  <summary>Details</summary>
Motivation: VLAs are limited by 2D data as they lack accurate spatial awareness, hindering their applicability in functioning efficiently in the real-world 3D space.

Method: The paper proposes SF, which aligns intermediate visual embeddings of VLAs with geometric representations from pretrained 3D foundation models to implicitly enforce spatial comprehension.

Result: SF achieves state-of-the-art performance, surpassing both 2D and 3D-based models in robotic tasks. It speeds up training by 3.8x and enhances data efficiency.

Conclusion: SF introduces a novel approach that addresses spatial comprehension in VLAs without relying on explicit 3D inputs, significantly improving precision and efficiency in robotic tasks.

Abstract: Vision-language-action (VLA) models have recently shown strong potential in
enabling robots to follow language instructions and execute precise actions.
However, most VLAs are built upon vision-language models pretrained solely on
2D data, which lack accurate spatial awareness and hinder their ability to
operate in the 3D physical world. Existing solutions attempt to incorporate
explicit 3D sensor inputs such as depth maps or point clouds, but these
approaches face challenges due to sensor noise, hardware heterogeneity, and
incomplete depth coverage in existing datasets. Alternative methods that
estimate 3D cues from 2D images also suffer from the limited performance of
depth estimators.We propose Spatial Forcing (SF), a simple yet effective
alignment strategy that implicitly forces VLA models to develop spatial
comprehension capabilities without relying on explicit 3D inputs or depth
estimators. SF aligns intermediate visual embeddings of VLAs with geometric
representations produced by pretrained 3D foundation models. By enforcing
alignment at intermediate layers, SF guides VLAs to encode richer spatial
representations that enhance action precision.Extensive experiments in
simulation and real-world environments demonstrate that SF achieves
state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further
accelerates training by up to 3.8x and improves data efficiency across diverse
robotic tasks. Project page is at https://spatial-forcing.github.io/

</details>


### [312] [Shape-Aware Whole-Body Control for Continuum Robots with Application in Endoluminal Surgical Robotics](https://arxiv.org/abs/2510.12332)
*Mohammadreza Kasaei,Mostafa Ghobadi,Mohsen Khadem*

Main category: cs.RO

TL;DR: This paper proposes a control framework for tendon-driven continuum robots, enhancing safety and precision in endoluminal surgeries like bronchoscopy.


<details>
  <summary>Details</summary>
Motivation: Address challenges of conventional tip-only control leading to wall trauma and failure in navigating patient-specific anatomy during endoluminal procedures.

Method: Developed a shape-aware framework integrating a physics-informed backbone model with Augmented Neural ODE for shape estimation and using MPPI controller for path optimization under constraints.

Result: Simulation studies exhibited high accuracy in diverse tasks including tracking and obstacle avoidance. Experiments on a bronchoscopy phantom showed improved lumen-following and reduced wall contacts.

Conclusion: This framework enhances precision, safety, and efficiency in endoluminal surgeries, with potential applications in other confined environments.

Abstract: This paper presents a shape-aware whole-body control framework for
tendon-driven continuum robots with direct application to endoluminal surgical
navigation. Endoluminal procedures, such as bronchoscopy, demand precise and
safe navigation through tortuous, patient-specific anatomy where conventional
tip-only control often leads to wall contact, tissue trauma, or failure to
reach distal targets. To address these challenges, our approach combines a
physics-informed backbone model with residual learning through an Augmented
Neural ODE, enabling accurate shape estimation and efficient Jacobian
computation. A sampling-based Model Predictive Path Integral (MPPI) controller
leverages this representation to jointly optimize tip tracking, backbone
conformance, and obstacle avoidance under actuation constraints. A task manager
further enhances adaptability by allowing real-time adjustment of objectives,
such as wall clearance or direct advancement, during tele-operation. Extensive
simulation studies demonstrate millimeter-level accuracy across diverse
scenarios, including trajectory tracking, dynamic obstacle avoidance, and
shape-constrained reaching. Real-robot experiments on a bronchoscopy phantom
validate the framework, showing improved lumen-following accuracy, reduced wall
contacts, and enhanced adaptability compared to joystick-only navigation and
existing baselines. These results highlight the potential of the proposed
framework to increase safety, reliability, and operator efficiency in minimally
invasive endoluminal surgery, with broader applicability to other confined and
safety-critical environments.

</details>


### [313] [Achieving Meaningful Collaboration: Worker-centered Design of a Physical Human-Robot Collaborative Blending Task](https://arxiv.org/abs/2510.12340)
*Nicky Mol,Luka Peternel,Alessandro Ianniello,Denis Zatyagov,Auke Nachenius,Stephan Balvert,J. Micah Prendergast,Sara Muscolo,Olger Siebinga,Eva Verhoef,Deborah Forster,David A. Abbink*

Main category: cs.RO

TL;DR: This paper proposes a transdisciplinary approach to using collaborative robots in airplane engine repair, focusing on worker well-being and system effectiveness.


<details>
  <summary>Details</summary>
Motivation: Address the increasing demand for production efficiency while adapting to labor shortages and aging populations in industrial settings.

Method: Implementation of a transdisciplinary framework combining academic research, practical expertise, and worker-oriented values to develop solutions using collaborative robots.

Result: Demonstrated potential of collaborative robots to improve airplane engine repair and maintenance processes.

Conclusion: The transdisciplinary approach has value in integrating technology to better address industrial challenges while enhancing workplace conditions for laborers.

Abstract: The use of robots in industrial settings continues to grow, driven by the
need to address complex societal challenges such as labor shortages, aging
populations, and ever-increasing production demands. In this abstract, we
advocate for (and demonstrate) a transdisciplinary approach when considering
robotics in the workplace. Transdisciplinarity emphasizes the integration of
academic research with pragmatic expertise and embodied experiential knowledge,
that prioritize values such as worker wellbeing and job attractiveness. In the
following, we describe an ongoing multi-pronged effort to explore the potential
of collaborative robots in the context of airplane engine repair and
maintenance operations.

</details>


### [314] [PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing](https://arxiv.org/abs/2510.12346)
*Bingquan Li,Ning Wang,Tianwei Zhang,Zhicheng He,Yucong Wu*

Main category: cs.RO

TL;DR: This paper proposes PolyMap, a real-time framework enabling humanoid robots to climb stairs effectively using multi-sensor data and motion planning.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve humanoid robots' ability to emulate human walking by accurately stepping on visualized positions, specifically designed for climbing stairs in unknown spaces.

Method: They introduced PolyMap, which employs a real-time polygonal staircase plane semantic map and multi-sensor fusion (LiDAR, RGB-D camera, IMUs) for plane segmentation and visual odometry. Footstep planning integrates these inputs, running on NVIDIA Orin hardware at 20-30 Hz.

Result: Experiments conducted in indoor and outdoor real-world scenes demonstrate PolyMap's efficiency and robustness in enabling humanoid robots to climb stairs.

Conclusion: PolyMap provides a solid perception-based locomotion planning framework for humanoid robots to navigate stairs efficiently, contributing significantly to advancements in biped robot walking technology.

Abstract: Recently, biped robot walking technology has been significantly developed,
mainly in the context of a bland walking scheme. To emulate human walking,
robots need to step on the positions they see in unknown spaces accurately. In
this paper, we present PolyMap, a perception-based locomotion planning
framework for humanoid robots to climb stairs. Our core idea is to build a
real-time polygonal staircase plane semantic map, followed by a footstep planar
using these polygonal plane segments. These plane segmentation and visual
odometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs). The
proposed framework is deployed on a NVIDIA Orin, which performs 20-30 Hz
whole-body motion planning output. Both indoor and outdoor real-scene
experiments indicate that our method is efficient and robust for humanoid robot
stair climbing.

</details>


### [315] [Pretraining in Actor-Critic Reinforcement Learning for Robot Motion Control](https://arxiv.org/abs/2510.12363)
*Jiale Fan,Andrei Cramariuc,Tifanny Portela,Marco Hutter*

Main category: cs.RO

TL;DR: The paper introduces a paradigm for pretraining neural networks to enhance reinforcement learning in robot motion control, resulting in improved sample efficiency and task performance.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of learning robot motion control tasks individually from scratch, despite the potential for shared generalizable knowledge across tasks.

Method: The method involves collecting diverse transition data using a task-agnostic algorithm, training a Proprioceptive Inverse Dynamics Model (PIDM) through supervised learning, and using its pretrained weights for initializing actor-critic RL algorithms.

Result: Application of the method on seven robot motion control tasks demonstrated average improvements of 40.1% in sample efficiency and 7.5% in task performance compared to random initialization.

Conclusion: The proposed pretraining strategy successfully enhances RL processes for robot motion control, leveraging shared dynamical knowledge across tasks to achieve better efficiency and outcomes.

Abstract: The pretraining-finetuning paradigm has facilitated numerous transformative
advancements in artificial intelligence research in recent years. However, in
the domain of reinforcement learning (RL) for robot motion control, individual
skills are often learned from scratch despite the high likelihood that some
generalizable knowledge is shared across all task-specific policies belonging
to a single robot embodiment. This work aims to define a paradigm for
pretraining neural network models that encapsulate such knowledge and can
subsequently serve as a basis for warm-starting the RL process in classic
actor-critic algorithms, such as Proximal Policy Optimization (PPO). We begin
with a task-agnostic exploration-based data collection algorithm to gather
diverse, dynamic transition data, which is then used to train a Proprioceptive
Inverse Dynamics Model (PIDM) through supervised learning. The pretrained
weights are loaded into both the actor and critic networks to warm-start the
policy optimization of actual tasks. We systematically validated our proposed
method on seven distinct robot motion control tasks, showing significant
benefits to this initialization strategy. Our proposed approach on average
improves sample efficiency by 40.1% and task performance by 7.5%, compared to
random initialization. We further present key ablation studies and empirical
analyses that shed light on the mechanisms behind the effectiveness of our
method.

</details>


### [316] [Controlling Intent Expressiveness in Robot Motion with Diffusion Models](https://arxiv.org/abs/2510.12370)
*Wenli Shi,Clemence Grislain,Olivier Sigaud,Mohamed Chetouani*

Main category: cs.RO

TL;DR: The paper proposes a motion generation framework for robots that allows for controllable legibility, enabling humans to infer robot intentions clearly across varying degrees of legibility.


<details>
  <summary>Details</summary>
Motivation: Human-robot interaction often suffers due to robot motions prioritizing efficiency over clarity of intent. Existing systems fail to provide adaptability in expressing intentions depending on context.

Method: The authors use an Information Potential Field for assigning legibility scores and a two-stage diffusion framework to generate and implement robot motions that align with specified legibility levels.

Result: Experiments in both 2D and 3D tasks show the framework produces diverse, controllable, and legible motions, with a comparable performance to current state-of-the-art systems.

Conclusion: The framework enables flexibility in robot motion legibility, making it adaptable to various contexts while preserving efficiency and state-of-the-art performance.

Abstract: Legibility of robot motion is critical in human-robot interaction, as it
allows humans to quickly infer a robot's intended goal. Although traditional
trajectory generation methods typically prioritize efficiency, they often fail
to make the robot's intentions clear to humans. Meanwhile, existing approaches
to legible motion usually produce only a single "most legible" trajectory,
overlooking the need to modulate intent expressiveness in different contexts.
In this work, we propose a novel motion generation framework that enables
controllable legibility across the full spectrum, from highly legible to highly
ambiguous motions. We introduce a modeling approach based on an Information
Potential Field to assign continuous legibility scores to trajectories, and
build upon it with a two-stage diffusion framework that first generates paths
at specified legibility levels and then translates them into executable robot
actions. Experiments in both 2D and 3D reaching tasks demonstrate that our
approach produces diverse and controllable motions with varying degrees of
legibility, while achieving performance comparable to SOTA. Code and project
page: https://legibility-modulator.github.io.

</details>


### [317] [Improving Generative Behavior Cloning via Self-Guidance and Adaptive Chunking](https://arxiv.org/abs/2510.12392)
*Junhyuk So,Chiwoong Lee,Shinyoung Lee,Jungseul Ok,Eunhyeok Park*

Main category: cs.RO

TL;DR: The paper introduces Generative Behavior Cloning (GBC) enhancements for robot learning using self-guidance and adaptive chunking to address the limitations of stochasticity and delayed responses in diffusion policies.


<details>
  <summary>Details</summary>
Motivation: To overcome issues of stochastic sampling errors and delays in open-loop (OL) diffusion policies for improving robotic performance in multi-task settings.

Method: Introducing two techniques: self-guidance (leveraging past observations for action fidelity) and adaptive chunking (updating action sequences based on reactive needs).

Result: Proposed techniques showed substantial improvement in GBC performance across a variety of simulated and real-world robotic manipulation tasks.

Conclusion: The approach enhances the consistency and reactivity of diffusion policies, significantly improving robot learning outcomes. The associated codebase is made publicly available.

Abstract: Generative Behavior Cloning (GBC) is a simple yet effective framework for
robot learning, particularly in multi-task settings. Recent GBC methods often
employ diffusion policies with open-loop (OL) control, where actions are
generated via a diffusion process and executed in multi-step chunks without
replanning. While this approach has demonstrated strong success rates and
generalization, its inherent stochasticity can result in erroneous action
sampling, occasionally leading to unexpected task failures. Moreover, OL
control suffers from delayed responses, which can degrade performance in noisy
or dynamic environments. To address these limitations, we propose two novel
techniques to enhance the consistency and reactivity of diffusion policies: (1)
self-guidance, which improves action fidelity by leveraging past observations
and implicitly promoting future-aware behavior; and (2) adaptive chunking,
which selectively updates action sequences when the benefits of reactivity
outweigh the need for temporal consistency. Extensive experiments show that our
approach substantially improves GBC performance across a wide range of
simulated and real-world robotic manipulation tasks. Our code is available at
https://github.com/junhyukso/SGAC

</details>


### [318] [Robot Learning: A Tutorial](https://arxiv.org/abs/2510.12403)
*Francesco Capuano,Caroline Pascal,Adil Zouitine,Thomas Wolf,Michel Aractingi*

Main category: cs.RO

TL;DR: The paper provides an overview of modern robot learning, focusing on the shift from traditional models to advanced data-driven methods.


<details>
  <summary>Details</summary>
Motivation: To address the need for a conceptual and practical guide amidst the transition to modern data-driven approaches in robot learning.

Method: The tutorial explores foundational principles like Reinforcement Learning and Behavioral Cloning, and introduces advanced concepts like generalist, language-conditioned models.

Result: The paper presents a guide equipped with examples implemented in "lerobot" to help researchers understand and participate in robot learning.

Conclusion: This work serves as a resource for researchers and practitioners to contribute to advancements in modern robot learning paradigms.

Abstract: Robot learning is at an inflection point, driven by rapid advancements in
machine learning and the growing availability of large-scale robotics data.
This shift from classical, model-based methods to data-driven, learning-based
paradigms is unlocking unprecedented capabilities in autonomous systems. This
tutorial navigates the landscape of modern robot learning, charting a course
from the foundational principles of Reinforcement Learning and Behavioral
Cloning to generalist, language-conditioned models capable of operating across
diverse tasks and even robot embodiments. This work is intended as a guide for
researchers and practitioners, and our goal is to equip the reader with the
conceptual understanding and practical tools necessary to contribute to
developments in robot learning, with ready-to-use examples implemented in
$\texttt{lerobot}$.

</details>


### [319] [M3D-skin: Multi-material 3D-printed Tactile Sensor with Hierarchical Infill Structures for Pressure Sensing](https://arxiv.org/abs/2510.12419)
*Shunnosuke Yoshimura,Kento Kawaharazuka,Kei Okada*

Main category: cs.RO

TL;DR: This paper introduces a new tactile sensor called M3D-skin using 3D printing methods to capture tactile information effectively.


<details>
  <summary>Details</summary>
Motivation: To simplify fabrication and expand applications of tactile sensors in robotics and human motion measurement.

Method: Leveraging multi-material Fused Deposition Modeling (FDM) 3D printing with conductive and non-conductive filaments to create tactile sensors based on infill patterns.

Result: The proposed sensors effectively detect tactile information through deformation under pressure. Demonstrations include motion measurement and robotic applications.

Conclusion: The M3D-skin sensor is validated as versatile, easy-to-produce, and applicable for various tasks, proving the concept's effectiveness in practical applications.

Abstract: Tactile sensors have a wide range of applications, from utilization in
robotic grippers to human motion measurement. If tactile sensors could be
fabricated and integrated more easily, their applicability would further
expand. In this study, we propose a tactile sensor-M3D-skin-that can be easily
fabricated with high versatility by leveraging the infill patterns of a
multi-material fused deposition modeling (FDM) 3D printer as the sensing
principle. This method employs conductive and non-conductive flexible filaments
to create a hierarchical structure with a specific infill pattern. The flexible
hierarchical structure deforms under pressure, leading to a change in
electrical resistance, enabling the acquisition of tactile information. We
measure the changes in characteristics of the proposed tactile sensor caused by
modifications to the hierarchical structure. Additionally, we demonstrate the
fabrication and use of a multi-tile sensor. Furthermore, as applications, we
implement motion pattern measurement on the sole of a foot, integration with a
robotic hand, and tactile-based robotic operations. Through these experiments,
we validate the effectiveness of the proposed tactile sensor.

</details>


### [320] [A Task-Efficient Reinforcement Learning Task-Motion Planner for Safe Human-Robot Cooperation](https://arxiv.org/abs/2510.12477)
*Gaoyuan Liu,Joris de Winter,Kelly Merckaert,Denis Steckelmacher,Ann Nowe,Bram Vanderborght*

Main category: cs.RO

TL;DR: The paper presents a hybrid framework combining reinforcement learning (RL) and interactive motion planning to enhance safety and efficiency in human-robot cooperation (HRC) by reducing task failures, replanning requests, and computational loads.


<details>
  <summary>Details</summary>
Motivation: To resolve the trade-off between safety and task efficiency in HRC environments, particularly the challenges posed by frequent motion replanning and human intervention.

Method: The framework integrates a Reinforcement Learning task planner with an interactive motion planner. The RL task planner selects safe and efficient task sequences using feedback from the motion planner, while the motion planner ensures collision-free executions by detecting changes in human arm motions and adjusting paths as needed.

Result: The framework was validated on a cobot both in simulation and in real-world scenarios. It was found to react effectively to uncertain human motions, decrease failed goal command repetitions, and reduce the number of replanning requests compared to traditional approaches.

Conclusion: The proposed hybrid RL framework enhances both safety and efficiency in HRC environments, demonstrating its practical value in managing uncertainty and minimizing computational issues during task planning and execution.

Abstract: In a Human-Robot Cooperation (HRC) environment, safety and efficiency are the
two core properties to evaluate robot performance. However, safety mechanisms
usually hinder task efficiency since human intervention will cause backup
motions and goal failures of the robot. Frequent motion replanning will
increase the computational load and the chance of failure. In this paper, we
present a hybrid Reinforcement Learning (RL) planning framework which is
comprised of an interactive motion planner and a RL task planner. The RL task
planner attempts to choose statistically safe and efficient task sequences
based on the feedback from the motion planner, while the motion planner keeps
the task execution process collision-free by detecting human arm motions and
deploying new paths when the previous path is not valid anymore. Intuitively,
the RL agent will learn to avoid dangerous tasks, while the motion planner
ensures that the chosen tasks are safe. The proposed framework is validated on
the cobot in both simulation and the real world, we compare the planner with
hard-coded task motion planning methods. The results show that our planning
framework can 1) react to uncertain human motions at both joint and task
levels; 2) reduce the times of repeating failed goal commands; 3) reduce the
total number of replanning requests.

</details>


### [321] [Fast Visuomotor Policy for Robotic Manipulation](https://arxiv.org/abs/2510.12483)
*Jingkai Jia,Tong Yang,Xueyao Chen,Chenhuan Liu,Wenqiang Zhang*

Main category: cs.RO

TL;DR: The paper introduces a framework called Energy Policy for fast and efficient robotic manipulation tasks. It uses energy scores and an energy MLP to predict multimodal actions in real-time.


<details>
  <summary>Details</summary>
Motivation: To address the need for high-frequency robotic manipulation tasks in resource-constrained systems, surpassing computational limitations of existing methods.

Method: The Energy Policy leverages energy scores as the learning objective and an energy MLP for efficient multimodal action prediction in a straightforward architecture.

Result: Energy Policy matches or outperforms current state-of-the-art methods, reduces computational overhead, and delivers superior performance on the MimicGen benchmark with faster inference speeds.

Conclusion: The Energy Policy is a promising solution for real-world robotic tasks, offering high-speed and precision while maintaining computational efficiency.

Abstract: We present a fast and effective policy framework for robotic manipulation,
named Energy Policy, designed for high-frequency robotic tasks and
resource-constrained systems. Unlike existing robotic policies, Energy Policy
natively predicts multimodal actions in a single forward pass, enabling
high-precision manipulation at high speed. The framework is built upon two core
components. First, we adopt the energy score as the learning objective to
facilitate multimodal action modeling. Second, we introduce an energy MLP to
implement the proposed objective while keeping the architecture simple and
efficient. We conduct comprehensive experiments in both simulated environments
and real-world robotic tasks to evaluate the effectiveness of Energy Policy.
The results show that Energy Policy matches or surpasses the performance of
state-of-the-art manipulation methods while significantly reducing
computational overhead. Notably, on the MimicGen benchmark, Energy Policy
achieves superior performance with at a faster inference compared to existing
approaches.

</details>


### [322] [Automated Behavior Planning for Fruit Tree Pruning via Redundant Robot Manipulators: Addressing the Behavior Planning Challenge](https://arxiv.org/abs/2510.12509)
*Gaoyuan Liu,Bas Boom,Naftali Slob,Yuri Durodié,Ann Nowé,Bram Vanderborght*

Main category: cs.RO

TL;DR: The paper explores behavior planning for robotic manipulator systems in orchard pruning, emphasizing improved planning methods for complex, obstructive environments.


<details>
  <summary>Details</summary>
Motivation: Manual pruning in orchards is labor-intensive and requires specialized skills. Automating this process with robots has seen challenges primarily in planning and control for navigation through obstructive branches.

Method: The authors formulated the planning problem for a high-dimensional robotic arm, analyzed system redundancies, and proposed an integrated workflow combining perception, modeling, and holistic planning.

Result: Experiments showcased significant performance enhancements of the robotic manipulator using comprehensive planning methods. The workflow was also successfully implemented on a real-world robot.

Conclusion: The study advances robotic pruning by addressing complex planning challenges, laying groundwork for future research, and complementing existing efforts in automated pruning systems.

Abstract: Pruning is an essential agricultural practice for orchards. Proper pruning
can promote healthier growth and optimize fruit production throughout the
orchard's lifespan. Robot manipulators have been developed as an automated
solution for this repetitive task, which typically requires seasonal labor with
specialized skills. While previous research has primarily focused on the
challenges of perception, the complexities of manipulation are often
overlooked. These challenges involve planning and control in both joint and
Cartesian spaces to guide the end-effector through intricate, obstructive
branches. Our work addresses the behavior planning challenge for a robotic
pruning system, which entails a multi-level planning problem in environments
with complex collisions. In this paper, we formulate the planning problem for a
high-dimensional robotic arm in a pruning scenario, investigate the system's
intrinsic redundancies, and propose a comprehensive pruning workflow that
integrates perception, modeling, and holistic planning. In our experiments, we
demonstrate that more comprehensive planning methods can significantly enhance
the performance of the robotic manipulator. Finally, we implement the proposed
workflow on a real-world robot. As a result, this work complements previous
efforts on robotic pruning and motivates future research and development in
planning for pruning applications.

</details>


### [323] [Two-stream network-driven vision-based tactile sensor for object feature extraction and fusion perception](https://arxiv.org/abs/2510.12528)
*Muxing Huang,Zibin Chen,Weiliang Xu,Zilan Li,Yuanzhi Zhou,Guoyuan Zhou,Wenjing Chen,Xinming Li*

Main category: cs.RO

TL;DR: This paper addresses inefficiencies in vision-based tactile systems by proposing a two-stream network for feature extraction and fusion, significantly improving recognition accuracy in shape and hardness during object recognition and grasping.


<details>
  <summary>Details</summary>
Motivation: To improve recognition accuracy in embodied intelligent robots by addressing redundancy in tactile data and lack of multidimensional feature fusion in vision-based tactile sensors.

Method: A two-stream network extracts internal and external object features, combining 3D depth map information with contact force data. Features are processed using CNNs and weighted fusion to create comprehensive feature representations.

Result: The proposed method achieves a force prediction error of 0.06 N, 98.0% accuracy in hardness recognition, 93.75% in shape recognition, and over 98.5% accuracy in actual grasping scenarios.

Conclusion: The new method significantly improves the tactile system's ability to move from simple perception to higher-level cognition, making it more effective in embodied perception applications.

Abstract: Tactile perception is crucial for embodied intelligent robots to recognize
objects. Vision-based tactile sensors extract object physical attributes
multidimensionally using high spatial resolution; however, this process
generates abundant redundant information. Furthermore, single-dimensional
extraction, lacking effective fusion, fails to fully characterize object
attributes. These challenges hinder the improvement of recognition accuracy. To
address this issue, this study introduces a two-stream network feature
extraction and fusion perception strategy for vision-based tactile systems.
This strategy employs a distributed approach to extract internal and external
object features. It obtains depth map information through three-dimensional
reconstruction while simultaneously acquiring hardness information by measuring
contact force data. After extracting features with a convolutional neural
network (CNN), weighted fusion is applied to create a more informative and
effective feature representation. In standard tests on objects of varying
shapes and hardness, the force prediction error is 0.06 N (within a 12 N
range). Hardness recognition accuracy reaches 98.0%, and shape recognition
accuracy reaches 93.75%. With fusion algorithms, object recognition accuracy in
actual grasping scenarios exceeds 98.5%. Focused on object physical attributes
perception, this method enhances the artificial tactile system ability to
transition from perception to cognition, enabling its use in embodied
perception applications.

</details>


### [324] [Learning Robust Agile Flight Control with Stability Guarantees](https://arxiv.org/abs/2510.12611)
*Lukas Pries,Markus Ryll*

Main category: cs.RO

TL;DR: Researchers propose a neural-augmented feedback controller for quadrotors, demonstrating robust, efficient trajectory tracking under disturbances and actuator constraints.


<details>
  <summary>Details</summary>
Motivation: Improving trajectory tracking and robustness in high-speed agile quadrotor flight systems under operational constraints and disturbances.

Method: A novel neural-augmented feedback controller combines strengths of existing control paradigms and learns robustly in simulated environments for real-world deployment.

Result: The controller achieves accurate tracking of aggressive trajectories, ensures universal stability, and enables fast computations without requiring additional training or fine-tuning.

Conclusion: The proposed controller advances agile quadrotor flight by integrating robustness, efficient computation, and direct real-world applicability.

Abstract: In the evolving landscape of high-speed agile quadrotor flight, achieving
precise trajectory tracking at the platform's operational limits is paramount.
Controllers must handle actuator constraints, exhibit robustness to
disturbances, and remain computationally efficient for safety-critical
applications. In this work, we present a novel neural-augmented feedback
controller for agile flight control. The controller addresses individual
limitations of existing state-of-the-art control paradigms and unifies their
strengths. We demonstrate the controller's capabilities, including the accurate
tracking of highly aggressive trajectories that surpass the feasibility of the
actuators. Notably, the controller provides universal stability guarantees,
enhancing its robustness and tracking performance even in exceedingly
disturbance-prone settings. Its nonlinear feedback structure is highly
efficient enabling fast computation at high update rates. Moreover, the
learning process in simulation is both fast and stable, and the controller's
inherent robustness allows direct deployment to real-world platforms without
the need for training augmentations or fine-tuning.

</details>


### [325] [Designing Tools with Control Confidence](https://arxiv.org/abs/2510.12630)
*Ajith Anil Meera,Abian Torres,Pablo Lanillos*

Main category: cs.RO

TL;DR: This paper introduces a framework for autonomous robotic tool design that incorporates 'control confidence' to enhance robustness to environmental uncertainties.


<details>
  <summary>Details</summary>
Motivation: Current autonomous tool design frameworks focus only on immediate performance optimization, ignoring repeated-use confidence, leading to less robust tools.

Method: The authors propose an optimization framework incorporating a neuro-inspired control confidence term into a CMAES-based evolutionary strategy for task-conditioned tool design.

Result: Simulations show tools designed with control confidence exhibit better robustness to environmental uncertainties and strike a balance between robustness and accuracy.

Conclusion: Integrating control confidence into autonomous tool design improves robustness and efficiency, outperforming state-of-the-art optimization methods.

Abstract: Prehistoric humans invented stone tools for specialized tasks by not just
maximizing the tool's immediate goal-completion accuracy, but also increasing
their confidence in the tool for later use under similar settings. This factor
contributed to the increased robustness of the tool, i.e., the least
performance deviations under environmental uncertainties. However, the current
autonomous tool design frameworks solely rely on performance optimization,
without considering the agent's confidence in tool use for repeated use. Here,
we take a step towards filling this gap by i) defining an optimization
framework for task-conditioned autonomous hand tool design for robots, where
ii) we introduce a neuro-inspired control confidence term into the optimization
routine that helps the agent to design tools with higher robustness. Through
rigorous simulations using a robotic arm, we show that tools designed with
control confidence as the objective function are more robust to environmental
uncertainties during tool use than a pure accuracy-driven objective. We further
show that adding control confidence to the objective function for tool design
provides a balance between the robustness and goal accuracy of the designed
tools under control perturbations. Finally, we show that our CMAES-based
evolutionary optimization strategy for autonomous tool design outperforms other
state-of-the-art optimizers by designing the optimal tool within the fewest
iterations. Code: https://github.com/ajitham123/Tool_design_control_confidence.

</details>


### [326] [Maximal Adaptation, Minimal Guidance: Permissive Reactive Robot Task Planning with Humans in the Loop](https://arxiv.org/abs/2510.12662)
*Oz Gitelson,Satya Prakash Nayak,Ritam Raha,Anne-Kathrin Schmuck*

Main category: cs.RO

TL;DR: The paper introduces a framework for logical human-robot collaboration that optimizes robot strategy for temporal logic tasks while respecting human autonomy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of enabling robots to effectively collaborate with humans pursuing independent, unknown tasks while ensuring robots meet their formal goals.

Method: The framework uses 'maximal adaptation' to adjust robot strategies dynamically and 'minimal tunable feedback' to request human cooperation only when needed for task progression.

Result: Validated on a robotic block-manipulation task and the Overcooked-AI benchmark, demonstrating superior cooperation behaviors and maintaining task guarantees.

Conclusion: The proposed method achieves cooperative human-robot interaction with balanced autonomy, minimal interference, and formal task satisfaction guarantees.

Abstract: We present a novel framework for human-robot \emph{logical} interaction that
enables robots to reliably satisfy (infinite horizon) temporal logic tasks
while effectively collaborating with humans who pursue independent and unknown
tasks. The framework combines two key capabilities: (i) \emph{maximal
adaptation} enables the robot to adjust its strategy \emph{online} to exploit
human behavior for cooperation whenever possible, and (ii) \emph{minimal
tunable feedback} enables the robot to request cooperation by the human online
only when necessary to guarantee progress. This balance minimizes human-robot
interference, preserves human autonomy, and ensures persistent robot task
satisfaction even under conflicting human goals. We validate the approach in a
real-world block-manipulation task with a Franka Emika Panda robotic arm and in
the Overcooked-AI benchmark, demonstrating that our method produces rich,
\emph{emergent} cooperative behaviors beyond the reach of existing approaches,
while maintaining strong formal guarantees.

</details>


### [327] [Autonomous Legged Mobile Manipulation for Lunar Surface Operations via Constrained Reinforcement Learning](https://arxiv.org/abs/2510.12684)
*Alvaro Belmonte-Baeza,Miguel Cazorla,Gabriel J. García,Carlos J. Pérez-Del-Pulgar,Jorge Pomares*

Main category: cs.RO

TL;DR: This paper proposes a constrained reinforcement learning framework for quadrupedal robots in lunar environments, focusing on mobility, manipulation, and safety constraints like stability and power efficiency.


<details>
  <summary>Details</summary>
Motivation: The reliance on wheeled rovers for planetary exploration limits adaptability in challenging terrains, especially for establishing permanent lunar bases.

Method: A constrained reinforcement learning framework focusing on whole-body locomotion and manipulation while incorporating safety constraints such as collision avoidance, dynamic stability, and power efficiency.

Result: The system achieved an average positional accuracy of 4 cm and orientation accuracy of 8.1 degrees, respecting constraints and adapting to lunar gravity conditions.

Conclusion: This work combines adaptive learning with safety-critical requirements, advancing autonomous robotics for future lunar exploration.

Abstract: Robotics plays a pivotal role in planetary science and exploration, where
autonomous and reliable systems are crucial due to the risks and challenges
inherent to space environments. The establishment of permanent lunar bases
demands robotic platforms capable of navigating and manipulating in the harsh
lunar terrain. While wheeled rovers have been the mainstay for planetary
exploration, their limitations in unstructured and steep terrains motivate the
adoption of legged robots, which offer superior mobility and adaptability. This
paper introduces a constrained reinforcement learning framework designed for
autonomous quadrupedal mobile manipulators operating in lunar environments. The
proposed framework integrates whole-body locomotion and manipulation
capabilities while explicitly addressing critical safety constraints, including
collision avoidance, dynamic stability, and power efficiency, in order to
ensure robust performance under lunar-specific conditions, such as reduced
gravity and irregular terrain. Experimental results demonstrate the framework's
effectiveness in achieving precise 6D task-space end-effector pose tracking,
achieving an average positional accuracy of 4 cm and orientation accuracy of
8.1 degrees. The system consistently respects both soft and hard constraints,
exhibiting adaptive behaviors optimized for lunar gravity conditions. This work
effectively bridges adaptive learning with essential mission-critical safety
requirements, paving the way for advanced autonomous robotic explorers for
future lunar missions.

</details>


### [328] [Reflection-Based Task Adaptation for Self-Improving VLA](https://arxiv.org/abs/2510.12710)
*Baicheng Li,Dong Wu,Zike Yan,Xinchen Liu,Zecui Zeng,Lusong Li,Hongbin Zha*

Main category: cs.RO

TL;DR: The paper introduces Reflective Self-Adaptation, a framework for rapid, autonomous task adaptation in robots using a self-improving loop to enhance execution and strategy.


<details>
  <summary>Details</summary>
Motivation: Pretrained Vision-Language-Action (VLA) models struggle to adapt efficiently to new tasks, posing a challenge for task-specific applications in robotics.

Method: The framework utilizes a dual-pathway architecture: a Failure-Driven Reflective RL pathway that synthesizes targeted dense reward functions to accelerate learning, and a Success-Driven Quality-Guided SFT pathway to align the learning with actual task goals by selectively imitating high-quality trajectories.

Result: Experiments on manipulation tasks showed faster convergence and higher success rates compared to baseline methods.

Conclusion: The framework provides a robust method for creating self-improving agents that can adapt effectively and autonomously to novel tasks with minimal human intervention.

Abstract: Pre-trained Vision-Language-Action (VLA) models represent a major leap
towards general-purpose robots, yet efficiently adapting them to novel,
specific tasks in-situ remains a significant hurdle. While reinforcement
learning (RL) is a promising avenue for such adaptation, the process often
suffers from low efficiency, hindering rapid task mastery. We introduce
Reflective Self-Adaptation, a framework for rapid, autonomous task adaptation
without human intervention. Our framework establishes a self-improving loop
where the agent learns from its own experience to enhance both strategy and
execution.
  The core of our framework is a dual-pathway architecture that addresses the
full adaptation lifecycle. First, a Failure-Driven Reflective RL pathway
enables rapid learning by using the VLM's causal reasoning to automatically
synthesize a targeted, dense reward function from failure analysis. This
provides a focused learning signal that significantly accelerates policy
exploration. However, optimizing such proxy rewards introduces a potential risk
of "reward hacking," where the agent masters the reward function but fails the
actual task. To counteract this, our second pathway, Success-Driven
Quality-Guided SFT, grounds the policy in holistic success. It identifies and
selectively imitates high-quality successful trajectories, ensuring the agent
remains aligned with the ultimate task goal. This pathway is strengthened by a
conditional curriculum mechanism to aid initial exploration.
  We conduct experiments in challenging manipulation tasks. The results
demonstrate that our framework achieves faster convergence and higher final
success rates compared to representative baselines. Our work presents a robust
solution for creating self-improving agents that can efficiently and reliably
adapt to new environments.

</details>


### [329] [Residual MPC: Blending Reinforcement Learning with GPU-Parallelized Model Predictive Control](https://arxiv.org/abs/2510.12717)
*Se Hwan Jeon,Ho Jae Lee,Seungwoo Hong,Sangbae Kim*

Main category: cs.RO

TL;DR: This paper presents a GPU-parallelized architecture integrating Model Predictive Control (MPC) and Reinforcement Learning (RL) to leverage the strengths of both methods for robust and interpretable locomotion control.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the robustness and computational limitations of MPC, improve RL's interpretability and adaptability, and combine both techniques for better performance in locomotion control.

Method: The authors propose an architecture where a residual policy is trained using RL to make corrections to the outputs of a kinodynamic whole-body MPC. Training is conducted using GPU parallelization at 100 Hz.

Result: The proposed approach demonstrates higher sample efficiency, achieves better asymptotic rewards, allows tracking of a wider range of velocity commands, and supports zero-shot adaptation to novel gaits and terrains compared to standalone MPC or RL.

Conclusion: The integration of MPC and RL successfully combines their respective strengths, offering interpretability, constraint handling, and adaptability for robust locomotion across various scenarios.

Abstract: Model Predictive Control (MPC) provides interpretable, tunable locomotion
controllers grounded in physical models, but its robustness depends on frequent
replanning and is limited by model mismatch and real-time computational
constraints. Reinforcement Learning (RL), by contrast, can produce highly
robust behaviors through stochastic training but often lacks interpretability,
suffers from out-of-distribution failures, and requires intensive reward
engineering. This work presents a GPU-parallelized residual architecture that
tightly integrates MPC and RL by blending their outputs at the torque-control
level. We develop a kinodynamic whole-body MPC formulation evaluated across
thousands of agents in parallel at 100 Hz for RL training. The residual policy
learns to make targeted corrections to the MPC outputs, combining the
interpretability and constraint handling of model-based control with the
adaptability of RL. The model-based control prior acts as a strong bias,
initializing and guiding the policy towards desirable behavior with a simple
set of rewards. Compared to standalone MPC or end-to-end RL, our approach
achieves higher sample efficiency, converges to greater asymptotic rewards,
expands the range of trackable velocity commands, and enables zero-shot
adaptation to unseen gaits and uneven terrain.

</details>


### [330] [T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping](https://arxiv.org/abs/2510.12724)
*Xin Fei,Zhixuan Xu,Huaicong Fang,Tianrui Zhang,Lin Shao*

Main category: cs.RO

TL;DR: The paper presents T(R,O) Grasp, a framework for efficient and diverse robotic dexterous grasping using diffusion models and inverse kinematics.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and challenges of dexterous grasping due to high-dimensional state and action spaces.

Method: Introduces a unified T(R,O) Graph representation for hands and objects paired with a graph diffusion model and inverse kinematics solver for grasp synthesis.

Result: Achieves a 94.83% success rate, 0.21s inference speed, and 41 grasps per second, outperforming current methods.

Conclusion: T(R,O) Grasp is fast, accurate, memory-efficient, and scalable, demonstrating potential as a foundation model for dexterous grasping.

Abstract: Dexterous grasping remains a central challenge in robotics due to the
complexity of its high-dimensional state and action space. We introduce T(R,O)
Grasp, a diffusion-based framework that efficiently generates accurate and
diverse grasps across multiple robotic hands. At its core is the T(R,O) Graph,
a unified representation that models spatial transformations between robotic
hands and objects while encoding their geometric properties. A graph diffusion
model, coupled with an efficient inverse kinematics solver, supports both
unconditioned and conditioned grasp synthesis. Extensive experiments on a
diverse set of dexterous hands show that T(R,O) Grasp achieves average success
rate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per
second on an NVIDIA A100 40GB GPU, substantially outperforming existing
baselines. In addition, our approach is robust and generalizable across
embodiments while significantly reducing memory consumption. More importantly,
the high inference speed enables closed-loop dexterous manipulation,
underscoring the potential of T(R,O) Grasp to scale into a foundation model for
dexterous grasping.

</details>


### [331] [HYPE: Hybrid Planning with Ego Proposal-Conditioned Predictions](https://arxiv.org/abs/2510.12733)
*Hang Yu,Julian Jordan,Julian Schmidt,Silvan Lindner,Alessandro Canevaro,Wilhelm Stork*

Main category: cs.RO

TL;DR: This paper introduces HYPE, a hybrid motion planning framework that integrates learned trajectory proposals into Monte Carlo Tree Search to simplify cost function design and improve safety and adaptability in complex urban driving scenarios.


<details>
  <summary>Details</summary>
Motivation: Developing safe and interpretable motion planning systems for urban environments requires effective reasoning about complex bidirectional multi-agent interactions and manageable cost function design.

Method: HYPE combines multimodal trajectory proposals from a learned model as priors into Monte Carlo Tree Search refinement, supported by an ego-conditioned occupancy prediction model to handle bidirectional interactions.

Result: HYPE achieves state-of-the-art performance on large-scale real-world benchmarks, excelling particularly in safety and adaptability metrics.

Conclusion: The paper simplifies cost function design and demonstrates the practicality of integrating learned trajectory models into motion planning, advancing the field of safe urban driving.

Abstract: Safe and interpretable motion planning in complex urban environments needs to
reason about bidirectional multi-agent interactions. This reasoning requires to
estimate the costs of potential ego driving maneuvers. Many existing planners
generate initial trajectories with sampling-based methods and refine them by
optimizing on learned predictions of future environment states, which requires
a cost function that encodes the desired vehicle behavior. Designing such a
cost function can be very challenging, especially if a wide range of complex
urban scenarios has to be considered. We propose HYPE: HYbrid Planning with Ego
proposal-conditioned predictions, a planner that integrates multimodal
trajectory proposals from a learned proposal model as heuristic priors into a
Monte Carlo Tree Search (MCTS) refinement. To model bidirectional interactions,
we introduce an ego-conditioned occupancy prediction model, enabling
consistent, scene-aware reasoning. Our design significantly simplifies cost
function design in refinement by considering proposal-driven guidance,
requiring only minimalistic grid-based cost terms. Evaluations on large-scale
real-world benchmarks nuPlan and DeepUrban show that HYPE effectively achieves
state-of-the-art performance, especially in safety and adaptability.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [332] [eye2vec: Learning Distributed Representations of Eye Movement for Program Comprehension Analysis](https://arxiv.org/abs/2510.11722)
*Haruhiko Yoshioka,Kazumasa Shimari,Hidetake Uwano,Kenichi Matsumoto*

Main category: cs.SE

TL;DR: This paper introduces eye2vec, a system for analyzing developers' eye movements on source code using distributed representations to simplify eye-tracking analysis.


<details>
  <summary>Details</summary>
Motivation: To address challenges in eye-tracking analysis for program comprehension, such as preselection of analysis targets, manually varying AOI levels, and difficulty interpreting fixation metrics.

Method: eye2vec encodes eye movement transitions as distributed representations of syntactic elements, enabling a more streamlined and semantically rich analysis.

Result: The proposed infrastructure supports diverse data analysis methods and reduces manual labor in eye-tracking studies.

Conclusion: eye2vec simplifies and improves the analysis of developers' eye movements in software comprehension tasks, promoting better insights through semantic representations.

Abstract: This paper presents eye2vec, an infrastructure for analyzing software
developers' eye movements while reading source code. In common eye-tracking
studies in program comprehension, researchers must preselect analysis targets
such as control flow or syntactic elements, and then develop analysis methods
to extract appropriate metrics from the fixation for source code. Here,
researchers can define various levels of AOIs like words, lines, or code
blocks, and the difference leads to different results. Moreover, the
interpretation of fixation for word/line can vary across the purposes of the
analyses. Hence, the eye-tracking analysis is a difficult task that depends on
the time-consuming manual work of the researchers. eye2vec represents
continuous two fixations as transitions between syntactic elements using
distributed representations. The distributed representation facilitates the
adoption of diverse data analysis methods with rich semantic interpretations.

</details>


### [333] [Should I Run My Cloud Benchmark on Black Friday?](https://arxiv.org/abs/2510.12397)
*Sören Henning,Adriano Vogel,Esteban Perez-Wohlfeil,Otmar Ertl,Rick Rabiser*

Main category: cs.SE

TL;DR: This paper investigates the impact of performance variability during benchmarks in cloud environments, especially during major events like Black Friday.


<details>
  <summary>Details</summary>
Motivation: Researchers are questioning the reproducibility and credibility of benchmarks in cloud environments due to presumed performance variability.

Method: The study analyzes benchmarking a stream processing application over months, identifying daily/weekly patterns, and further evaluating Black Friday's effect on performance benchmarks.

Result: The study revealed subtle performance patterns over time and explores variability during a significant global event like Black Friday.

Conclusion: While performance variability in cloud environments is observable, it is less pronounced than presumed; global events could further influence benchmarking results.

Abstract: Benchmarks and performance experiments are frequently conducted in cloud
environments. However, their results are often treated with caution, as the
presumed high variability of performance in the cloud raises concerns about
reproducibility and credibility. In a recent study, we empirically quantified
the impact of this variability on benchmarking results by repeatedly executing
a stream processing application benchmark at different times of the day over
several months. Our analysis confirms that performance variability is indeed
observable at the application level, although it is less pronounced than often
assumed. The larger scale of our study compared to related work allowed us to
identify subtle daily and weekly performance patterns. We now extend this
investigation by examining whether a major global event, such as Black Friday,
affects the outcomes of performance benchmarks.

</details>


### [334] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: The paper proposes prioritizing upstream input reduction to optimize Large Language Models (LLMs) for handling noisy, voluminous, and verbose data in data-intensive workflows.


<details>
  <summary>Details</summary>
Motivation: The existing challenges with LLMs stem from inefficiencies associated with processing large volumes of noisy and verbose real-world text data, which is both costly and environmentally unsustainable. Current optimization efforts overlook input verbosity reduction.

Method: The paper introduces an approach that treats text input reduction as a key strategy for better attention allocation in LLMs. It reframes input-side reduction from mere compression to emphasizing relevant signals via adaptive reduction pipelines and token-budget-aware preprocessing.

Result: The proposed method channels computational attention towards meaningful data signals, aiming to improve efficiency, scalability, and accuracy in integrating LLMs with text-rich workflows.

Conclusion: The work advocates for upstream input reduction as a critical design principle for sustainable and effective LLM-powered systems, encouraging further research into benchmarks and reduction techniques.

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

</details>


### [335] [Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling](https://arxiv.org/abs/2510.11838)
*Xu Yang,Jiayuan Zhou,Michael Pacheco,Wenhan Zhu,Pengfei He,Shaowei Wang,Kui Liu,Ruiqi Pan*

Main category: cs.SE

TL;DR: This paper introduces Lingxi, a framework utilizing procedural knowledge to improve issue resolution in software repositories, addressing inefficiencies in LLM-powered agents.


<details>
  <summary>Details</summary>
Motivation: To overcome deficiencies of LLM-based agents in repository-level issue resolution, including procedural knowledge gaps and inefficient computational exploration.

Method: Lingxi extracts procedural knowledge from historical issue-fixing data using hierarchical abstraction and applies knowledge-driven scaling to analyze and resolve issues intelligently.

Result: Lingxi achieves a 74.6% success rate on the SWE-bench Verified benchmark in Past@1 setting, outperforming existing methods by 5.4% to 14.9%.

Conclusion: Procedural knowledge drives Lingxi's success by enabling efficient and effective issue resolution, with qualitative insights highlighting the significance of design patterns and coding practices.

Abstract: Driven by the advancements of Large Language Models (LLMs), LLM-powered
agents are making significant improvements in software engineering tasks, yet
struggle with complex, repository-level issue resolution. Existing agent-based
methods have two key limitations. First, they lack of procedural knowledge
(i.e., how an issue is fixed step-by-step and rationales behind it) to learn
and leverage for issue resolution. Second, they rely on massive computational
power to blindly explore the solution space. % To address those limitations, we
propose Lingxi, an issue resolution framework that leverages procedural
knowledge extracted from historical issue-fixing data to guide agents in
solving repository-level issues. \ourTool first constructs this knowledge
offline through a hierarchical abstraction mechanism, enabling agents to learn
the how and why behind a fix, not just the final solution. During online
application, it employs a knowledge-driven scaling method that leverages the
procedural knowledge of similar issues to intelligently analyze the target
issue from multiple perspectives, in sharp contrast to undirected, brute-force
exploration. % Lingxi successfully resolves 74.6\% of bugs on the SWE-bench
Verified benchmark in Past@1 setting, outperforming five state-of-the-art
techniques by a significant margin (5.4\% to 14.9\%). Our comprehensive
ablation study confirmed that the success of Lingxi comes directly from its use
of procedural knowledge. Without it, the performance gains from scaling alone
is negligible. Our qualitative study further shows that the ``design patterns
$\&$ coding practices'' is the most critical knowledge aspect, and that the
roles of different knowledge aspects switch across different stages (i.e.,
analysis, planning, and fixing).

</details>


### [336] [DMAS-Forge: A Framework for Transparent Deployment of AI Applications as Distributed Systems](https://arxiv.org/abs/2510.11872)
*Alessandro Cornacchia,Vaastav Anand,Muhammad Bilal,Zafar Qazi,Marco Canini*

Main category: cs.SE

TL;DR: Deploying and testing AI agents as distributed systems is challenging, and DMAS-Forge aims to simplify this with minimal manual effort.


<details>
  <summary>Details</summary>
Motivation: AI applications increasingly rely on distributed multi-agent systems, resembling service-oriented architectures, that are hard to deploy and test efficiently.

Method: DMAS-Forge framework decouples application logic from specific deployment scenarios, generating necessary code and configurations automatically.

Result: A prototype of DMAS-Forge is presented alongside its design principles and vision for minimally manual distributed multi-agent application setup.

Conclusion: DMAS-Forge addresses deployment challenges for distributed multi-agent systems, presenting promising opportunities for streamlined and automated workflows.

Abstract: Agentic AI applications increasingly rely on multiple agents with distinct
roles, specialized tools, and access to memory layers to solve complex tasks --
closely resembling service-oriented architectures. Yet, in the rapid evolving
landscape of programming frameworks and new protocols, deploying and testing AI
agents as distributed systems remains a daunting and labor-intensive task. We
present DMAS-Forge, a framework designed to close this gap. DMAS-Forge
decouples application logic from specific deployment choices, and aims at
transparently generating the necessary glue code and configurations to spawn
distributed multi-agent applications across diverse deployment scenarios with
minimal manual effort. We present our vision, design principles, and a
prototype of DMAS-Forge. Finally, we discuss the opportunities and future work
for our approach.

</details>


### [337] [TorchCor: High-Performance Cardiac Electrophysiology Simulations with the Finite Element Method on GPUs](https://arxiv.org/abs/2510.12011)
*Bei Zhou,Maximilian Balmus,Cesare Corrado,Ludovica Cicci,Shuang Qian,Steven A. Niederer*

Main category: cs.SE

TL;DR: TorchCor is a Python library enabling efficient cardiac electrophysiology simulations on GPUs, improving accessibility for researchers and clinicians.


<details>
  <summary>Details</summary>
Motivation: CEP simulations require significant computational resources, often inaccessible to researchers and clinicians. TorchCor aims to provide an alternative solution using GPUs.

Method: TorchCor uses the finite element method, implemented on general-purpose GPUs with PyTorch, to accelerate CEP simulations.

Result: TorchCor achieves significant speed-ups for large 3D meshes and displays verified accuracy against known analytical solutions and benchmarks.

Conclusion: TorchCor enhances accessibility and performance of CEP simulations, offering a freely available tool for academic and commercial applications.

Abstract: Cardiac electrophysiology (CEP) simulations are increasingly used for
understanding cardiac arrhythmias and guiding clinical decisions. However,
these simulations typically require high-performance computing resources with
numerous CPU cores, which are often inaccessible to many research groups and
clinicians. To address this, we present TorchCor, a high-performance Python
library for CEP simulations using the finite element method on general-purpose
GPUs. Built on PyTorch, TorchCor significantly accelerates CEP simulations,
particularly for large 3D meshes. The accuracy of the solver is verified
against manufactured analytical solutions and the $N$-version benchmark
problem. TorchCor is freely available for both academic and commercial use
without restrictions.

</details>


### [338] [Enhancing Neural Code Representation with Additional Context](https://arxiv.org/abs/2510.12082)
*Huy Nguyen,Christoph Treude,Patanamon Thongtanunam*

Main category: cs.SE

TL;DR: The study examines the impact of enriching code representation with contextual information on program comprehension tasks using neural models. Experiments highlight improved performance in tasks like code summarisation and clone detection.


<details>
  <summary>Details</summary>
Motivation: To address limitations in neural models which typically rely solely on source code, disregarding contextual information such as version history and structural relationships.

Method: By evaluating five neural models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) on two tasks, clone detection and code summarisation, under both code-only and enriched context-augmented representations.

Result: Adding contextual signals improves model performance significantly, with boosts up to +21.48% macro-F1 for combined contexts, and human evaluations show higher preference for contextual summaries in terms of accuracy and content adequacy.

Conclusion: Incorporating contextual signals into code representation enhances program comprehension tasks and provides avenues to optimize contextual encoding in software engineering neural models.

Abstract: Automated program comprehension underpins many software engineering tasks,
from code summarisation to clone detection. Recent deep learning models achieve
strong results but typically rely on source code alone, overlooking contextual
information such as version history or structural relationships. This limits
their ability to capture how code evolves and operates. We conduct an empirical
study on how enriching code representations with such contextual signals
affects neural model performance on key comprehension tasks. Two downstream
tasks, code clone detection and code summarisation, are evaluated using SeSaMe
(1,679 Java methods) and CodeSearchNet (63,259 methods). Five representative
models (CodeBERT, GraphCodeBERT, CodeT5, PLBART, ASTNN) are fine-tuned under
code-only and context-augmented settings. Results show that context generally
improves performance: version history consistently boosts clone detection
(e.g., CodeT5 +15.92% F1) and summarisation (e.g., GraphCodeBERT +5.56%
METEOR), while call-graph effects vary by model and task. Combining multiple
contexts yields further gains (up to +21.48% macro-F1). Human evaluation on 100
Java snippets confirms that context-augmented summaries are significantly
preferred for Accuracy and Content Adequacy (p <= 0.026; |delta| up to 0.55).
These findings highlight the potential of contextual signals to enhance code
comprehension and open new directions for optimising contextual encoding in
neural SE models.

</details>


### [339] [Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach](https://arxiv.org/abs/2510.12120)
*Zhenyu Mao,Jacky Keung,Fengji Zhang,Shuo Liu,Yifei Wang,Jialong Li*

Main category: cs.SE

TL;DR: The paper proposes SEMAP, a protocol for enhancing large language models (LLMs) in multi-agent software development systems. It addresses key issues like under-specification, coordination gaps, and verification challenges, showing significant reductions in task failures.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the shortcomings of existing multi-agent systems (MAS) using LLMs in software engineering, particularly in handling under-specification, coordination issues, and poor verification due to the lack of foundational SE principles.

Method: SEMAP, a protocol-layer methodology implemented on Google's A2A system, integrates three core SE principles: explicit behavioral contracts, structured messaging, and lifecycle-guided execution with verification.

Result: Empirical results show SEMAP reduces failure rates significantly, with up to 69.6% less failure in function-level code development, 56.7% in deployment-level development, and reductions of 47.4% and 28.2% in vulnerability detection for Python and C/C++ tasks, respectively.

Conclusion: SEMAP effectively addresses key deficiencies in multi-agent LLM-based systems for software engineering, demonstrating its utility in reducing failure rates and improving workflow reliability in various tasks.

Abstract: The increasing demand for software development has driven interest in
automating software engineering (SE) tasks using Large Language Models (LLMs).
Recent efforts extend LLMs into multi-agent systems (MAS) that emulate
collaborative development workflows, but these systems often fail due to three
core deficiencies: under-specification, coordination misalignment, and
inappropriate verification, arising from the absence of foundational SE
structuring principles. This paper introduces Software Engineering Multi-Agent
Protocol (SEMAP), a protocol-layer methodology that instantiates three core SE
design principles for multi-agent LLMs: (1) explicit behavioral contract
modeling, (2) structured messaging, and (3) lifecycle-guided execution with
verification, and is implemented atop Google's Agent-to-Agent (A2A)
infrastructure. Empirical evaluation using the Multi-Agent System Failure
Taxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures
across different SE tasks. In code development, it achieves up to a 69.6%
reduction in total failures for function-level development and 56.7% for
deployment-level development. For vulnerability detection, SEMAP reduces
failure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.

</details>


### [340] [Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?](https://arxiv.org/abs/2510.12702)
*Cedric Richter,Heike Wehrheim*

Main category: cs.SE

TL;DR: This paper introduces NL2Contract, a task where Large Language Models (LLMs) translate natural language into formal functional contracts for software verification to improve accuracy and reduce false alarms.


<details>
  <summary>Details</summary>
Motivation: The lack of formal specifications in real-world software code hampers the broader adoption of automatic software verifiers, which rely on specifications for effectiveness.

Method: The paper integrates LLMs to infer formal functional contracts (including preconditions and postconditions) from natural language hints in code, and evaluates these generated contracts based on metrics such as soundness, bug discriminative power, and usability for verification.

Result: Findings show that LLMs effectively generate sound and expressive functional contracts, improving the ability to discriminate between buggy and correct behavior, and lowering false alarms for software verifiers.

Conclusion: LLM-inferred preconditions align with developer intentions and enhance automatic software verifiers’ capacity to identify real-world bugs more accurately.

Abstract: Automatic software verifiers have become increasingly effective at the task
of checking software against (formal) specifications. Yet, their adoption in
practice has been hampered by the lack of such specifications in real world
code. Large Language Models (LLMs) have shown promise in inferring formal
postconditions from natural language hints embedded in code such as function
names, comments or documentation. Using the generated postconditions as
specifications in a subsequent verification, however, often leads verifiers to
suggest invalid inputs, hinting at potential issues that ultimately turn out to
be false alarms.
  To address this, we revisit the problem of specification inference from
natural language in the context of automatic software verification. In the
process, we introduce NL2Contract, the task of employing LLMs to translate
informal natural language into formal functional contracts, consisting of
postconditions as well as preconditions. We introduce metrics to validate and
compare different NL2Contract approaches, using soundness, bug discriminative
power of the generated contracts and their usability in the context of
automatic software verification as key metrics. We evaluate NL2Contract with
different LLMs and compare it to the task of postcondition generation
nl2postcond. Our evaluation shows that (1) LLMs are generally effective at
generating functional contracts sound for all possible inputs, (2) the
generated contracts are sufficiently expressive for discriminating buggy from
correct behavior, and (3) verifiers supplied with LLM inferred functional
contracts produce fewer false alarms than when provided with postconditions
alone. Further investigations show that LLM inferred preconditions generally
align well with developers intentions which allows us to use automatic software
verifiers to catch real-world bugs.

</details>


### [341] [iCodeReviewer: Improving Secure Code Review with Mixture of Prompts](https://arxiv.org/abs/2510.12186)
*Yun Peng,Kisub Kim,Linghan Meng,Kui Liu*

Main category: cs.SE

TL;DR: The paper introduces iCodeReviewer, an automated secure code review system based on large language models (LLMs), utilizing a mixture-of-prompts approach to enhance precision and coverage in identifying security issues.


<details>
  <summary>Details</summary>
Motivation: Ensure software security and minimize human effort in code reviews, addressing the limitations of current automated secure code review methods such as low precision, coverage, and lack of thorough evaluation.

Method: iCodeReviewer employs a mixture-of-prompts architecture that segments the process into dynamic prompt experts, each checking specific security issues. Additionally, it uses a routing algorithm to reduce false positives by activating only necessary prompts.

Result: Achieves an F1 score of 63.98% for security issue identification and localization, with an 84% acceptance rate of review comments in production.

Conclusion: iCodeReviewer demonstrates enhanced reliability and effectiveness in automated secure code reviews, addressing longstanding challenges in precision, coverage, and practical evaluation.

Abstract: Code review is an essential process to ensure the quality of software that
identifies potential software issues at an early stage of software development.
Among all software issues, security issues are the most important to identify,
as they can easily lead to severe software crashes and service disruptions.
Recent research efforts have been devoted to automated approaches to reduce the
manual efforts required in the secure code review process. Despite the
progress, current automated approaches on secure code review, including static
analysis, deep learning models, and prompting approaches, still face the
challenges of limited precision and coverage, and a lack of comprehensive
evaluation.
  To mitigate these challenges, we propose iCodeReviewer, which is an automated
secure code review approach based on large language models (LLMs).
iCodeReviewer leverages a novel mixture-of-prompts architecture that
incorporates many prompt experts to improve the coverage of security issues.
Each prompt expert is a dynamic prompt pipeline to check the existence of a
specific security issue. iCodeReviewer also implements an effective routing
algorithm to activate only necessary prompt experts based on the code features
in the input program, reducing the false positives induced by LLM
hallucination. Experiment results in our internal dataset demonstrate the
effectiveness of iCodeReviewer in security issue identification and
localization with an F1 of 63.98%. The review comments generated by
iCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed
in production environments.

</details>


### [342] [Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening](https://arxiv.org/abs/2510.12294)
*Gergő Balogh,Dávid Kószó,Homayoun Safarpour Motealegh Mahalegi,László Tóth,Bence Szakács,Áron Búcsú*

Main category: cs.SE

TL;DR: This paper reviews the intersection of software engineering (SE) and psychology (PSY) through verbalization techniques and evaluates the use of GPT for screening papers.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to understand software developers' cognition, decision-making, and behaviors using verbalization techniques, while addressing the effectiveness of GPT in interdisciplinary literature screening.

Method: A scoping review using a GPT-assisted screening pipeline assessed the relevance of over 9,000 papers based on titles, validating against human reviews.

Result: High consistency between GPT and human reviewers (13% disagreement rate); themes in verbalization work focus on SE craft, with less emphasis on human-centered topics. SE draws heavily on PSY methods, while PSY rarely draws on SE.

Conclusion: Verbalization techniques effectively study cognitive aspects in SE. GPT is efficient for interdisciplinary reviews, though human-centered topics need more exploration in SE and PSY integration.

Abstract: Understanding how software developers think, make decisions, and behave
remains a key challenge in software engineering (SE). Verbalization techniques
(methods that capture spoken or written thought processes) offer a lightweight
and accessible way to study these cognitive aspects. This paper presents a
scoping review of research at the intersection of SE and psychology (PSY),
focusing on the use of verbal data. To make large-scale interdisciplinary
reviews feasible, we employed a large language model (LLM)-assisted screening
pipeline using GPT to assess the relevance of over 9,000 papers based solely on
titles. We addressed two questions: what themes emerge from
verbalization-related work in SE, and how effective are LLMs in supporting
interdisciplinary review processes? We validated GPT's outputs against human
reviewers and found high consistency, with a 13\% disagreement rate. Prominent
themes mainly were tied to the craft of SE, while more human-centered topics
were underrepresented. The data also suggests that SE frequently draws on PSY
methods, whereas the reverse is rare.

</details>


### [343] [(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm](https://arxiv.org/abs/2510.12364)
*Kevin Krings,Nino S. Bohn,Thomas Ludwig*

Main category: cs.SE

TL;DR: The paper explores a new coding paradigm called Vibe Coding (VC), which emphasizes intuitive and affect-driven collaboration between developers and AI systems, investigating its impact on programming practices and culture.


<details>
  <summary>Details</summary>
Motivation: A need to understand how advancements in generative AI like large language models are reshaping software development practices and enabling novel paradigms such as Vibe Coding.

Method: The study incorporates five semi-structured interview sessions with ten experienced software practitioners, analyzing thematic dimensions like creativity, collaboration, and sustainability.

Result: VC is conceptualized within the metaphor of co-drifting rather than co-piloting, highlighting changes in developer roles that blur lines between professionals and non-developers.

Conclusion: Vibe Coding represents a significant cultural shift in programming, offering innovative opportunities but also posing challenges such as scalability, reproducibility, and inclusivity that require further research.

Abstract: Recent advancements in generative artificial intelligence (GenAI),
particularly large language models, have introduced new possibilities for
software development practices. In our paper we investigate the emerging Vibe
Coding (VC) paradigm that emphasizes intuitive, affect-driven, and
improvisational interactions between developers and AI systems. Building upon
the discourse of End-User Development (EUD), we explore how VC diverges from
conventional programming approaches such as those supported by tools like
GitHub Copilot. Through five semi-structured interview sessions with ten
experienced software practitioners, we identify five thematic dimensions:
creativity, sustainability, the future of programming, collaboration, and
criticism. Our analysis conceptualizes VC within the metaphor of co-drifting,
contrasting it with the prevalent co-piloting perspective of AI-assisted
development. We argue that VC reconfigures the developers role, blurring
boundaries between professional and non-developers. While VC enables novel
forms of expression and rapid prototyping, it also introduces challenges
regarding reproducibility, scalability, and inclusivity. We propose that VC
represents a meaningful shift in programming culture, warranting further
investigation within human-computer interaction (HCI) and software engineering
research.

</details>


### [344] [DarTwin made precise by SysMLv2 -- An Experiment](https://arxiv.org/abs/2510.12478)
*Øystein Haugen,Stefan Klikovits,Martin Arthur Andersen,Jonathan Beaulieu,Francis Bordeleau,Joachim Denil,Joost Mertens*

Main category: cs.SE

TL;DR: SysMLv2 introduces new capabilities for domain-specific language creation and integration with systems engineering, exemplified by the DarTwin DSL for Digital Twin evolution, while noting some tooling limitations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the new functionalities in SysMLv2 facilitating domain-specific language creation, with the goal of enhancing system descriptions and supporting Digital Twin evolution.

Method: The authors introduced and evaluated the DarTwin DSL, a formalization of the DarTwin notation for Digital Twin evolution, by leveraging SysMLv2's built-in specification tools and use cases.

Result: DarTwin DSL was successfully demonstrated to enable Digital Twin evolution management, although limitations in graphical notation capabilities of SysMLv2 tools were identified.

Conclusion: SysMLv2 offers promising features for integrating domain-specific languages into systems engineering, but further improvements in tooling are necessary for full graphical support.

Abstract: The new SysMLv2 adds mechanisms for the built-in specification of
domain-specific concepts and language extensions. This feature promises to
facilitate the creation of Domain-Specific Languages (DSLs) and interfacing
with existing system descriptions and technical designs. In this paper, we
review these features and evaluate SysMLv2's capabilities using concrete use
cases. We develop DarTwin DSL, a DSL that formalizes the existing DarTwin
notation for Digital Twin (DT) evolution, through SysMLv2, thereby supposedly
enabling the wide application of DarTwin's evolution templates using any
SysMLv2 tool. We demonstrate DarTwin DSL, but also point out limitations in the
currently available tooling of SysMLv2 in terms of graphical notation
capabilities. This work contributes to the growing field of Model-Driven
Engineering (MDE) for DTs and combines it with the release of SysMLv2, thus
integrating a systematic approach with DT evolution management in systems
engineering.

</details>


### [345] [Diff-XYZ: A Benchmark for Evaluating Diff Understanding](https://arxiv.org/abs/2510.12487)
*Evgeniy Glukhov,Michele Conti,Egor Bogomolov,Yaroslav Golubev,Alexander Bezzubov*

Main category: cs.SE

TL;DR: The paper introduces Diff-XYZ, a benchmark for code-diff understanding with three tasks: apply, anti-apply, and diff generation. The dataset enables empirical studies and comparisons of diff formats.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the need for reliable handling of code diffs by automated agents working at scale, with a focus on improving diff-understanding.

Method: The authors curated a dataset, Diff-XYZ, comprising real commit data from CommitPackFT, defined three supervised tasks, and performed evaluations with automatic metrics across different diff representations.

Result: Findings show that diff format effectiveness varies by use case and model size. For example, larger models favor search-replace format for diff generation, but this format is less effective for analysis and smaller models.

Conclusion: Diff-XYZ serves as a reusable benchmark for evaluating and improving code diff handling by LLMs, guiding further advancements in diff formats and code-editing models.

Abstract: Reliable handling of code diffs is central to agents that edit and refactor
repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff
understanding with three supervised tasks: apply (old code $+$ diff
$\rightarrow$ new code), anti-apply (new code $-$ diff $\rightarrow$ old code),
and diff generation (new code $-$ old code $\rightarrow$ diff). Instances in
the benchmark are triples $\langle \textit{old code}, \textit{new code},
\textit{diff} \rangle$ drawn from real commits in CommitPackFT, paired with
automatic metrics and a clear evaluation protocol. We use the benchmark to do a
focused empirical study of the unified diff format and run a cross-format
comparison of different diff representations. Our findings reveal that
different formats should be used depending on the use case and model size. For
example, representing diffs in search-replace format is good for larger models
in the diff generation scenario, yet not suited well for diff analysis and
smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing
and improving diff handling in LLMs that can aid future development of diff
formats and models editing code. The dataset is published on HuggingFace Hub:
https://huggingface.co/datasets/JetBrains-Research/diff-xyz.

</details>


### [346] [The EmpathiSEr: Development and Validation of Software Engineering Oriented Empathy Scales](https://arxiv.org/abs/2510.12546)
*Hashini Gunatilake,John Grundy,Rashina Hoda,Ingo Mueller*

Main category: cs.SE

TL;DR: The paper addresses the lack of tools to measure empathy in software engineering by developing two context-specific empathy scales: EmpathiSEr-P and EmpathiSEr-U.


<details>
  <summary>Details</summary>
Motivation: Empathy is a critical factor in the success of software engineering, influencing collaboration and design. However, existing empathy measurement tools are not suited for the socio-technical nuances of software engineering. This paper aims to fill this gap.

Method: The authors created and validated two domain-specific scales, EmpathiSEr-P and EmpathiSEr-U, using a multi-phase approach involving expert evaluation, cognitive interviews, and practitioner surveys.

Result: The study produced the first psychometrically validated empathy measurement tools tailored for software engineering contexts, covering three dimensions of empathy: cognitive, affective, and empathic responses.

Conclusion: The new scales enable researchers and practitioners to measure and enhance empathy in software teams and user interactions effectively, filling a vital gap in the domain.

Abstract: Empathy plays a critical role in software engineering (SE), influencing
collaboration, communication, and user-centred design. Although SE research has
increasingly recognised empathy as a key human aspect, there remains no
validated instrument specifically designed to measure it within the unique
socio-technical contexts of SE. Existing generic empathy scales, while
well-established in psychology and healthcare, often rely on language,
scenarios, and assumptions that are not meaningful or interpretable for
software practitioners. These scales fail to account for the diverse,
role-specific, and domain-bound expressions of empathy in SE, such as
understanding a non-technical user's frustrations or another practitioner's
technical constraints, which differ substantially from empathy in clinical or
everyday contexts. To address this gap, we developed and validated two
domain-specific empathy scales: EmpathiSEr-P, assessing empathy among
practitioners, and EmpathiSEr-U, capturing practitioner empathy towards users.
Grounded in a practitioner-informed conceptual framework, the scales encompass
three dimensions of empathy: cognitive empathy, affective empathy, and empathic
responses. We followed a rigorous, multi-phase methodology, including expert
evaluation, cognitive interviews, and two practitioner surveys. The resulting
instruments represent the first psychometrically validated empathy scales
tailored to SE, offering researchers and practitioners a tool for assessing
empathy and designing empathy-enhancing interventions in software teams and
user interactions.

</details>


### [347] [Evaluating End-User Device Energy Models in Sustainability Reporting of Browser-Based Web Services](https://arxiv.org/abs/2510.12566)
*Maja H. Kirkeby,Timmie Lagermann*

Main category: cs.SE

TL;DR: This study examines the accuracy of energy and carbon models used for web-based services sustainability reporting, highlighting discrepancies in energy estimation based on website categories, device types, and tasks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of precision and accuracy in widely used energy and carbon models for sustainability reporting.

Method: Conducted an empirical study measuring real energy consumption across multiple website categories and devices during predefined user interactions.

Result: Found substantial deviations between models and actual measured energy. Discrepancies are systematic, influenced by category, device type, and task considerations.

Conclusion: Existing models need category-aware, device-reflective adjustments to enhance precision and reproducibility in sustainability reporting frameworks.

Abstract: Sustainability reporting in web-based services increasingly relies on
simplified energy and carbon models such as the Danish Agency of Digital
Government's Digst framework and the United Kingdom-based DIMPACT model.
Although these models are widely adopted, their accuracy and precision remain
underexplored. This paper presents an empirical study evaluating how well such
models reflect actual energy consumption during realistic user interactions
with common website categories. Energy use was measured across shopping,
booking, navigation, and news services using predefined user flows executed on
four laptop platforms. The results show that the commonly applied
constant-power approximation (P * t) can diverge substantially from measured
energy, depending on website category, device type, and task characteristics.
The findings demonstrate that model deviations are systematic rather than
random and highlight the need for category-aware and device-reflective power
parameters in reproducible sustainability reporting frameworks.

</details>


### [348] [Runtime Composition in Dynamic System of Systems: A Systematic Review of Challenges, Solutions, Tools, and Evaluation Methods](https://arxiv.org/abs/2510.12616)
*Muhammad Ashfaq,Ahmed R. Sadik,Teerath Das,Muhammad Waseem,Niko Makitalo,Tommi Mikkonen*

Main category: cs.SE

TL;DR: A Systematic Literature Review (SLR) explored runtime composition in dynamic systems of systems (SoSs), analyzing challenges, solutions, tools, and evaluations from literature spanning 2019-2024. The study emphasizes the need for standardized metrics and cross-domain frameworks.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the gaps in understanding runtime composition in dynamic Systems of Systems (SoSs), crucial for improving adaptability in dynamic environments like smart cities and autonomous vehicles.

Method: A Systematic Literature Review (SLR) was conducted, analyzing 1,774 studies, with thematic analysis performed on 80 primary studies to synthesize findings on challenges, solutions, tools, and evaluation strategies.

Result: Key challenges are modeling/analysis, resilient operations, system orchestration, and heterogeneity of constituent systems (CSs). Solutions include co-simulation tools, semantic ontologies, adaptive architectures, middleware, and AI-driven resilience. While currently service-oriented frameworks dominate tooling and simulations aid evaluation, key gaps include interoperability issues and lack of benchmarks.

Conclusion: The study highlights the need for standardized evaluation metrics, scalable decentralized architectures, and cross-domain frameworks, offering guidance for researchers and practitioners toward improving dynamic systems of systems (SoSs).

Abstract: Context: Modern Systems of Systems (SoSs) increasingly operate in dynamic
environments (e.g., smart cities, autonomous vehicles) where runtime
composition -- the on-the-fly discovery, integration, and coordination of
constituent systems (CSs)--is crucial for adaptability. Despite growing
interest, the literature lacks a cohesive synthesis of runtime composition in
dynamic SoSs. Objective: This study synthesizes research on runtime composition
in dynamic SoSs and identifies core challenges, solution strategies, supporting
tools, and evaluation methods. Methods: We conducted a Systematic Literature
Review (SLR), screening 1,774 studies published between 2019 and 2024 and
selecting 80 primary studies for thematic analysis (TA). Results: Challenges
fall into four categories: modeling and analysis, resilient operations, system
orchestration, and heterogeneity of CSs. Solutions span seven areas:
co-simulation and digital twins, semantic ontologies, integration frameworks,
adaptive architectures, middleware, formal methods, and AI-driven resilience.
Service-oriented frameworks for composition and integration dominate tooling,
while simulation platforms support evaluation. Interoperability across tools,
limited cross-toolchain workflows, and the absence of standardized benchmarks
remain key gaps. Evaluation approaches include simulation-based,
implementation-driven, and human-centered studies, which have been applied in
domains such as smart cities, healthcare, defense, and industrial automation.
Conclusions: The synthesis reveals tensions, including autonomy versus
coordination, the modeling-reality gap, and socio-technical integration. It
calls for standardized evaluation metrics, scalable decentralized
architectures, and cross-domain frameworks. The analysis aims to guide
researchers and practitioners in developing and implementing dynamically
composable SoSs.

</details>


### [349] [Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method Refactoring](https://arxiv.org/abs/2503.20934)
*Fraol Batole,Abhiram Bellur,Malinda Dilhara,Mohammed Raihan Ullah,Yaroslav Zharov,Timofey Bryksin,Kai Ishikawa,Haifeng Chen,Masaharu Morimoto,Shota Motoura,Takeo Hosomi,Tien N. Nguyen,Hridesh Rajan,Nikolaos Tsantalis,Danny Dig*

Main category: cs.SE

TL;DR: This paper introduces MM-assist, an end-to-end automated assistant for MOVEMETHOD refactoring using Large Language Models, which outperforms current approaches and is positively rated by users.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the mismatch between existing refactoring tool recommendations and expert developer practices, leveraging the natural language learning capabilities of LLMs to enhance the MOVEMETHOD refactoring process.

Method: The method involves integrating static analysis from IDEs, a self-consistency workflow for LLMs, and refactoring-aware retrieval augmentation to filter out hallucinations and improve recommender accuracy.

Result: MM-assist demonstrates a 1.7x improvement on benchmark Recall@1 and Recall@3, a 2.4x improvement on a recent open-source refactoring corpus, and an 82.8% positive rating from 30 user participants.

Conclusion: The findings show that MM-assist effectively automates MOVEMETHOD refactoring, offers superior performance compared to previous approaches, and is well-received by experienced developers.

Abstract: MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools
that recommend which methods to move and where, these recommendations do not
align with how expert developers perform MOVEMETHOD. Given the extensive
training of Large Language Models and their reliance upon naturalness of code,
they should expertly recommend which methods are misplaced in a given class and
which classes are better hosts. Our formative study of 2016 LLM recommendations
revealed that LLMs give expert suggestions, yet they are unreliable: up to 80%
of the suggestions are hallucinations. We introduce the first LLM fully powered
assistant for MOVEMETHOD refactoring that automates its whole end-to-end
lifecycle, from recommendation to execution. We designed novel solutions that
automatically filter LLM hallucinations using static analysis from IDEs and a
novel workflow that requires LLMs to be self-consistent, critique, and rank
refactoring suggestions. As MOVEMETHOD refactoring requires global,
projectlevel reasoning, we solved the limited context size of LLMs by employing
refactoring-aware retrieval augment generation (RAG). Our approach, MM-assist,
synergistically combines the strengths of the LLM, IDE, static analysis, and
semantic relevance. In our thorough, multi-methodology empirical evaluation, we
compare MM-assist with the previous state-of-the-art approaches. MM-assist
significantly outperforms them: (i) on a benchmark widely used by other
researchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a
corpus of 210 recent refactorings from Open-source software, our Recall rates
improve by at least 2.4x. Lastly, we conducted a user study with 30 experienced
participants who used MM-assist to refactor their own code for one week. They
rated 82.8% of MM-assist recommendations positively. This shows that MM-assist
is both effective and useful.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [350] [Inpainting the Neural Picture: Inferring Unrecorded Brain Area Dynamics from Multi-Animal Datasets](https://arxiv.org/abs/2510.11924)
*Ji Xia,Yizi Zhang,Shuqi Wang,Genevera I. Allen,Liam Paninski,Cole Lincoln Hurwitz,Kenneth D. Miller*

Main category: q-bio.NC

TL;DR: The paper presents NeuroPaint, a model that predicts unrecorded brain area dynamics by leveraging multi-animal datasets and overlapping recorded areas.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in single-animal experiments where not all brain area dynamics can be simultaneously recorded, and to explore multi-animal data to understand interactions between brain areas.

Method: The method involves NeuroPaint, a masked autoencoding model trained on synthetic and Neuropixels datasets. The training utilizes overlapping subsets of recorded brain regions across animals to predict the dynamics of unrecorded regions.

Result: NeuroPaint successfully reconstructed activity in missing brain areas and demonstrated its capability for accurate multi-area analysis beyond single animal or single session data.

Conclusion: The approach allows multi-area brain studies by utilizing partial and overlapping data from multiple animals, addressing the challenge of limited simultaneous recordings.

Abstract: Characterizing interactions between brain areas is a fundamental goal of
systems neuroscience. While such analyses are possible when areas are recorded
simultaneously, it is rare to observe all combinations of areas of interest
within a single animal or recording session. How can we leverage multi-animal
datasets to better understand multi-area interactions? Building on recent
progress in large-scale, multi-animal models, we introduce NeuroPaint, a masked
autoencoding approach for inferring the dynamics of unrecorded brain areas. By
training across animals with overlapping subsets of recorded areas, NeuroPaint
learns to reconstruct activity in missing areas based on shared structure
across individuals. We train and evaluate our approach on synthetic data and
two multi-animal, multi-area Neuropixels datasets. Our results demonstrate that
models trained across animals with partial observations can successfully
in-paint the dynamics of unrecorded areas, enabling multi-area analyses that
transcend the limitations of any single experiment.

</details>


### [351] [MAPS: Masked Attribution-based Probing of Strategies- A computational framework to align human and model explanations](https://arxiv.org/abs/2510.12141)
*Sabine Muzellec,Yousif Kashef Alghetaa,Simon Kornblith,Kohitij Kar*

Main category: q-bio.NC

TL;DR: The paper introduces MAPS, a computational tool that tests if explanations from artificial neural networks (ANNs) align with human vision, using explanation-masked images and behavioral comparisons.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding human core object recognition by leveraging interpretable strategies in artificial neural networks and linking them to biological vision.

Method: Developed MAPS (Masked Attribution-based Probing of Strategies), which converts ANN attribution maps into explanation-masked images (EMIs) and compares human and model accuracies using minimal and full stimulus images.

Result: MAPS demonstrated its ability to reliably indicate the similarity between models based on their strategies and provided insight into ANN-explanation combinations that closely align with the vision strategies of humans and macaques.

Conclusion: MAPS offers a scalable and efficient way to evaluate ANN explanation methods, linking human behavior, neural activity, and model decisions without requiring extensive behavioral trials.

Abstract: Human core object recognition depends on the selective use of visual
information, but the strategies guiding these choices are difficult to measure
directly. We present MAPS (Masked Attribution-based Probing of Strategies), a
behaviorally validated computational tool that tests whether explanations
derived from artificial neural networks (ANNs) can also explain human vision.
MAPS converts attribution maps into explanation-masked images (EMIs) and
compares image-by-image human accuracies on these minimal images with limited
pixel budgets with accuracies on the full stimuli. MAPS provides a principled
way to evaluate and choose among competing ANN interpretability methods. In
silico, EMI-based behavioral similarity between models reliably recovers the
ground-truth similarity computed from their attribution maps, establishing
which explanation methods best capture the model's strategy. When applied to
humans and macaques, MAPS identifies ANN-explanation combinations whose
explanations align most closely with biological vision, achieving the
behavioral validity of Bubble masks while requiring far fewer behavioral
trials. Because it needs only access to model attributions and a modest set of
behavioral data on the original images, MAPS avoids exhaustive psychophysics
while offering a scalable tool for adjudicating explanations and linking human
behavior, neural activity, and model decisions under a common standard.

</details>


### [352] [Readout Representation: Redefining Neural Codes by Input Recovery](https://arxiv.org/abs/2510.12228)
*Shunsuke Onoo,Yoshihiro Nagano,Yukiyasu Kamitani*

Main category: q-bio.NC

TL;DR: The paper introduces 'readout representation,' a framework focused on recovering information from features rather than relying on their causal origin, challenging traditional hierarchical-causal perspectives in sensory representation.


<details>
  <summary>Details</summary>
Motivation: To address the tension between abstracting away details in sensory representations while preserving fine-grained information needed for downstream tasks.

Method: The authors proposed a metric called 'representation size' and empirically demonstrated the reconstructability of inputs from perturbed mid-level features, highlighting redundancy in the feature space.

Result: Empirical findings showed that inputs can still be reconstructed even from heavily altered mid-level features, indicating representational redundancy and challenging traditional causal mapping paradigms.

Conclusion: The proposed readout framework bridges understanding of biological and artificial systems by emphasizing information-rich and robust sensory representations over causal hierarchies.

Abstract: Sensory representation is typically understood through a hierarchical-causal
framework where progressively abstract features are extracted sequentially.
However, this causal view fails to explain misrepresentation, a phenomenon
better handled by an informational view based on decodable content. This
creates a tension: how does a system that abstracts away details still preserve
the fine-grained information needed for downstream functions? We propose
readout representation to resolve this, defining representation by the
information recoverable from features rather than their causal origin.
Empirically, we show that inputs can be accurately reconstructed even from
heavily perturbed mid-level features, demonstrating that a single input
corresponds to a broad, redundant region of feature space, challenging the
causal mapping perspective. To quantify this property, we introduce
representation size, a metric linked to model robustness and representational
redundancy. Our framework offers a new lens for analyzing how both biological
and artificial neural systems learn complex features while maintaining robust,
information-rich representations of the world.

</details>


### [353] [Non-linear associations of amyloid-$β$ with resting-state functional networks and their cognitive relevance in a large community-based cohort of cognitively normal older adults](https://arxiv.org/abs/2510.12751)
*Junjie Wu,Benjamin B Risk,Taylor A James,Nicholas Seyfried,David W Loring,Felicia C Goldstein,Allan I Levey,James J Lah,Deqiang Qiu*

Main category: q-bio.NC

TL;DR: The study investigates non-linear changes in brain network connectivity as early neural indicators of Alzheimer's Disease (AD) in cognitively normal older adults, using resting-state functional MRI and cerebrospinal fluid biomarkers.


<details>
  <summary>Details</summary>
Motivation: To understand non-linear brain network connectivity changes related to early amyloid pathology in Alzheimer's Disease and their cognitive significance for early detection.

Method: The study analyzed data from 968 cognitively normal older adults using resting-state functional MRI and cerebrospinal fluid biomarkers to assess the relationships between amyloid-beta levels, tau levels, and functional brain connectivity across 14 large-scale networks. Functional networks were identified using independent component analysis.

Result: Inverted U-shaped associations between amyloid-beta levels and functional connectivity were found in specific networks like the precuneus and ventral default mode network. Functional connectivity in amyloid-beta-related networks correlated with better cognitive performance, with no significant relationships observed for tau.

Conclusion: Non-linear changes in functional connectivity precede symptoms of AD, specifically in amyloid-beta-associated networks, thereby emphasizing their potential as early biomarkers for AD detection and prognosis.

Abstract: Background: Non-linear alterations in brain network connectivity may
represent early neural signatures of Alzheimer's disease (AD) pathology in
cognitively normal older adults. Understanding these changes and their
cognitive relevance could provide sensitive biomarkers for early detection.
Most prior studies recruited participants from memory clinics, often with
subjective memory concerns, limiting generalizability.
  Methods: We examined 14 large-scale functional brain networks in 968
cognitively normal older adults recruited from the community using
resting-state functional MRI, cerebrospinal fluid (CSF) biomarkers
(amyloid-$\beta$ 1-42 [A$\beta$], total tau, phosphorylated tau 181), and
neuropsychological assessments. Functional networks were identified using group
independent component analysis.
  Results: Inverted U-shaped associations between CSF A$\beta$ and functional
connectivity were observed in the precuneus network and ventral default mode
network (DMN), but not in the dorsal DMN, indicating network-specific
vulnerability to early amyloid pathology. Higher connectivity in
A$\beta$-related networks, including dorsal and ventral DMN, precuneus, and
posterior salience networks, was associated with better visual memory,
visuospatial, and executive performance. No significant relationships were
observed between CSF tau and functional connectivity.
  Conclusions: Using a large, community-based cohort, we demonstrate that
non-linear alterations in functional connectivity occur in specific networks
even during the asymptomatic phase of AD. Moreover, A$\beta$-related network
connectivity is cognitively relevant, highlighting functional brain networks as
promising imaging markers for early detection and prognosis of AD.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [354] [Dimension-Free Minimax Rates for Learning Pairwise Interactions in Attention-Style Models](https://arxiv.org/abs/2510.11789)
*Shai Zucker,Xiong Wang,Fei Lu,Inbar Seroussi*

Main category: stat.ML

TL;DR: The paper examines the convergence rate of learning pairwise interactions in single-layer attention-style models, revealing a fundamental dimension-free statistical efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand the statistical efficiency and theoretical properties of attention-style models for handling pairwise interactions, independent of complex factors like token count or matrix rank.

Method: The authors derive theoretical minimax convergence rates, analyzing the dependency on smoothness of activation and other inherent properties of the attention mechanism.

Result: The convergence rate for learning pairwise interactions is $M^{-(2\beta)/(2\beta+1)}$, solely influenced by activation smoothness ($\beta$), unaffected by token count, ambient dimension, or weight matrix rank.

Conclusion: Attention-style models demonstrate dimension-free statistical efficiency, offering insights into their training and mechanisms despite identifiability challenges in weight matrices and activations.

Abstract: We study the convergence rate of learning pairwise interactions in
single-layer attention-style models, where tokens interact through a weight
matrix and a non-linear activation function. We prove that the minimax rate is
$M^{-\frac{2\beta}{2\beta+1}}$ with $M$ being the sample size, depending only
on the smoothness $\beta$ of the activation, and crucially independent of token
count, ambient dimension, or rank of the weight matrix. These results highlight
a fundamental dimension-free statistical efficiency of attention-style nonlocal
models, even when the weight matrix and activation are not separately
identifiable and provide a theoretical understanding of the attention mechanism
and its training.

</details>


### [355] [On Thompson Sampling and Bilateral Uncertainty in Additive Bayesian Optimization](https://arxiv.org/abs/2510.11792)
*Nathan Wycoff*

Main category: stat.ML

TL;DR: The study evaluates whether ignoring bilateral uncertainty (pairwise covariances) in Bayesian Optimization significantly impacts performance, finding it to be practically insignificant.


<details>
  <summary>Details</summary>
Motivation: Additive assumptions simplify high-dimensional Bayesian Optimization but commonly used methods ignore pairwise covariances, raising concerns about their impact on practical performance, especially with small budgets.

Method: The paper leverages conditional independence to conduct Thompson Sampling that incorporates bilateral uncertainty, and empirically compares its results with approximations ignoring BU.

Result: Ignoring bilateral uncertainty leads to slightly worse, but practically insignificant results, confirming the sufficiency of simpler approximations in small-budget Bayesian Optimization.

Conclusion: Simplified additive methods without considering bilateral uncertainty are adequate for practical Bayesian Optimization, even outside asymptotic regimes.

Abstract: In Bayesian Optimization (BO), additive assumptions can mitigate the twin
difficulties of modeling and searching a complex function in high dimension.
However, common acquisition functions, like the Additive Lower Confidence
Bound, ignore pairwise covariances between dimensions, which we'll call
\textit{bilateral uncertainty} (BU), imposing a second layer of approximations.
While theoretical results indicate that asymptotically not much is lost in
doing so, little is known about the practical effects of this assumption in
small budgets. In this article, we show that by exploiting conditional
independence, Thompson Sampling respecting BU can be efficiently conducted. We
use this fact to execute an empirical investigation into the loss incurred by
ignoring BU, finding that the additive approximation to Thompson Sampling does
indeed have, on balance, worse performance than the exact method, but that this
difference is of little practical significance. This buttresses the theoretical
understanding and suggests that the BU-ignoring approximation is sufficient for
BO in practice, even in the non-asymptotic regime.

</details>


### [356] [Active Subspaces in Infinite Dimension](https://arxiv.org/abs/2510.11871)
*Poorbita Kundu,Nathan Wycoff*

Main category: stat.ML

TL;DR: The paper extends active subspace analysis to real-valued functionals on Hilbert spaces and investigates its properties in infinite dimensional settings.


<details>
  <summary>Details</summary>
Motivation: To generalize active subspace analysis from Euclidean spaces to Hilbert spaces for better visualization, modeling, and optimization.

Method: The paper introduces an operator for Hilbert spaces, adapts Monte Carlo procedures, and analyzes its convergence properties.

Result: Methodology showed effectiveness in creating visualizations and optimizing models on complex test problems.

Conclusion: Active subspace analysis can be successfully extended to Hilbert spaces, retaining desirable properties and improving modeling and optimization tasks.

Abstract: Active subspace analysis uses the leading eigenspace of the gradient's second
moment to conduct supervised dimension reduction. In this article, we extend
this methodology to real-valued functionals on Hilbert space. We define an
operator which coincides with the active subspace matrix when applied to a
Euclidean space. We show that many of the desirable properties of Active
Subspace analysis extend directly to the infinite dimensional setting. We also
propose a Monte Carlo procedure and discuss its convergence properties.
Finally, we deploy this methodology to create visualizations and improve
modeling and optimization on complex test problems.

</details>


### [357] [High-Probability Bounds For Heterogeneous Local Differential Privacy](https://arxiv.org/abs/2510.11895)
*Maryam Aliakbarpour,Alireza Fallah,Swaha Roy,Ria Stevens*

Main category: stat.ML

TL;DR: This paper examines statistical estimation and distribution learning under heterogeneous local differential privacy (LDP) with high-probability guarantees, providing optimal bounds and practical guidance.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of handling varying privacy levels among users while ensuring statistical estimation and learning tasks maintain high-probability guarantees.

Method: The authors develop finite sample upper bounds for $\\ell_2$-norm and matching minimax lower bounds for mean estimation problems, along with algorithms for distribution learning in $\\ell_\\infty$-distance under heterogeneous LDP.

Result: The paper provides upper bounds, establishes optimal high-probability guarantees for heterogeneous LDP scenarios, and introduces tailored algorithms for distribution learning.

Conclusion: The findings offer a framework for optimal mechanism design in scenarios with user-specific privacy requirements and varying levels of protection.

Abstract: We study statistical estimation under local differential privacy (LDP) when
users may hold heterogeneous privacy levels and accuracy must be guaranteed
with high probability. Departing from the common in-expectation analyses, and
for one-dimensional and multi-dimensional mean estimation problems, we develop
finite sample upper bounds in $\ell_2$-norm that hold with probability at least
$1-\beta$. We complement these results with matching minimax lower bounds,
establishing the optimality (up to constants) of our guarantees in the
heterogeneous LDP regime. We further study distribution learning in
$\ell_\infty$-distance, designing an algorithm with high-probability guarantees
under heterogeneous privacy demands. Our techniques offer principled guidance
for designing mechanisms in settings with user-specific privacy levels.

</details>


### [358] [Simplifying Optimal Transport through Schatten-$p$ Regularization](https://arxiv.org/abs/2510.11910)
*Tyler Maunu*

Main category: stat.ML

TL;DR: The paper introduces a new framework for recovering low-rank structures in optimal transport using Schatten-$p$ norm regularization.


<details>
  <summary>Details</summary>
Motivation: Existing methods in optimal transport lack a unified approach for promoting low-dimensional, interpretable transport maps, which motivates the need for a principled framework.

Method: A convex program for encouraging low-rank structures is proposed, with theoretical analysis and a mirror descent algorithm for efficient optimization when $p \geq 1$.

Result: The authors derive optimality conditions and provide recovery guarantees in simplified cases, supported by experiments showing efficiency and scalability on both synthetic and real data.

Conclusion: The proposed framework is effective for recovering low-rank transport structures, providing theoretical guarantees and practical efficacy in relevant applications.

Abstract: We propose a new general framework for recovering low-rank structure in
optimal transport using Schatten-$p$ norm regularization. Our approach extends
existing methods that promote sparse and interpretable transport maps or plans,
while providing a unified and principled family of convex programs that
encourage low-dimensional structure. The convexity of our formulation enables
direct theoretical analysis: we derive optimality conditions and prove recovery
guarantees for low-rank couplings and barycentric maps in simplified settings.
To efficiently solve the proposed program, we develop a mirror descent
algorithm with convergence guarantees for $p \geq 1$. Experiments on synthetic
and real data demonstrate the method's efficiency, scalability, and ability to
recover low-rank transport structures.

</details>


### [359] [Statistical Guarantees for High-Dimensional Stochastic Gradient Descent](https://arxiv.org/abs/2510.12013)
*Jiaqi Li,Zhipeng Lou,Johannes Schmidt-Hieber,Wei Biao Wu*

Main category: stat.ML

TL;DR: The paper provides rigorous statistical guarantees for SGD and ASGD with constant learning rates in high-dimensional settings, leveraging techniques from high-dimensional time series.


<details>
  <summary>Details</summary>
Motivation: Understanding the theoretical properties of SGD and ASGD in high-dimensional regimes, which are central to modern machine learning.

Method: Adapt tools from high-dimensional time series and nonlinear autoregressive processes to analyze and establish geometric-moment contraction and asymptotic stationarity.

Result: Proved geometric-moment contraction for constant learning rates, derived $q$-th moment convergence, and provided high-probability concentration bounds for high-dimensional ASGD.

Conclusion: This work bridges a critical gap in the theory of SGD and introduces a new analytical toolkit for high-dimensional learning algorithms.

Abstract: Stochastic Gradient Descent (SGD) and its Ruppert-Polyak averaged variant
(ASGD) lie at the heart of modern large-scale learning, yet their theoretical
properties in high-dimensional settings are rarely understood. In this paper,
we provide rigorous statistical guarantees for constant learning-rate SGD and
ASGD in high-dimensional regimes. Our key innovation is to transfer powerful
tools from high-dimensional time series to online learning. Specifically, by
viewing SGD as a nonlinear autoregressive process and adapting existing
coupling techniques, we prove the geometric-moment contraction of
high-dimensional SGD for constant learning rates, thereby establishing
asymptotic stationarity of the iterates. Building on this, we derive the $q$-th
moment convergence of SGD and ASGD for any $q\ge2$ in general $\ell^s$-norms,
and, in particular, the $\ell^{\infty}$-norm that is frequently adopted in
high-dimensional sparse or structured models. Furthermore, we provide sharp
high-probability concentration analysis which entails the probabilistic bound
of high-dimensional ASGD. Beyond closing a critical gap in SGD theory, our
proposed framework offers a novel toolkit for analyzing a broad class of
high-dimensional learning algorithms.

</details>


### [360] [Compressibility Measures Complexity: Minimum Description Length Meets Singular Learning Theory](https://arxiv.org/abs/2510.12077)
*Einar Urdshals,Edmund Lau,Jesse Hoogland,Stan van Wingerden,Daniel Murfet*

Main category: stat.ML

TL;DR: The paper uses singular learning theory to extend the MDL principle for evaluating neural network compressibility and establishes a correlation between local learning coefficient (LLC) and compressibility.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate and understand the limits of neural network model compression by establishing a theoretical framework using singular learning theory and exploring its connection with compressibility.

Method: The authors extend the MDL principle for singular models like neural networks through singular learning theory and perform extensive experiments using the Pythia suite with various compression techniques.

Result: The study demonstrates that complexity estimates based on the local learning coefficient (LLC) are highly and, in some cases, linearly correlated with model compressibility.

Conclusion: Local learning coefficient (LLC)-based complexity estimates can provide an effective framework for understanding and evaluating neural network compressibility, offering a theoretical tool to examine the limits of compression.

Abstract: We study neural network compressibility by using singular learning theory to
extend the minimum description length (MDL) principle to singular models like
neural networks. Through extensive experiments on the Pythia suite with
quantization, factorization, and other compression techniques, we find that
complexity estimates based on the local learning coefficient (LLC) are closely,
and in some cases, linearly correlated with compressibility. Our results
provide a path toward rigorously evaluating the limits of model compression.

</details>


### [361] [Follow-the-Perturbed-Leader for Decoupled Bandits: Best-of-Both-Worlds and Practicality](https://arxiv.org/abs/2510.12152)
*Chaiwon Kim,Jongyeong Lee,Min-hwan Oh*

Main category: stat.ML

TL;DR: The paper introduces a new policy for decoupled multi-armed bandit (MAB) problems, achieving near-optimal regret in both stochastic and adversarial settings with significant computational efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the decoupled MAB challenge, where the learner must balance exploration and exploitation with partially observable losses, while improving computational efficiency and achieving optimal regret bounds.

Method: A policy based on the Follow-the-Perturbed-Leader (FTPL) framework using Pareto perturbations is proposed, eliminating the need for convex optimization and resampling steps required by previous policies.

Result: The new policy achieves constant regret in stochastic settings, minimax optimal regret in adversarial settings, and is approximately 20 times faster than the Decoupled-Tsallis-INF policy while showing superior empirical performance.

Conclusion: This work demonstrates a practically efficient, high-performing solution for decoupled MAB problems, surpassing existing approaches both theoretically and computationally.

Abstract: We study the decoupled multi-armed bandit (MAB) problem, where the learner
selects one arm for exploration and one arm for exploitation in each round. The
loss of the explored arm is observed but not counted, while the loss of the
exploited arm is incurred without being observed. We propose a policy within
the Follow-the-Perturbed-Leader (FTPL) framework using Pareto perturbations.
Our policy achieves (near-)optimal regret regardless of the environment, i.e.,
Best-of-Both-Worlds (BOBW): constant regret in the stochastic regime, improving
upon the optimal bound of the standard MABs, and minimax optimal regret in the
adversarial regime. Moreover, the practicality of our policy stems from
avoiding both the convex optimization step required by the previous BOBW
policy, Decoupled-Tsallis-INF (Rouyer & Seldin, 2020), and the resampling step
that is typically necessary in FTPL. Consequently, it achieves substantial
computational improvement, about $20$ times faster than Decoupled-Tsallis-INF,
while also demonstrating better empirical performance in both regimes. Finally,
we empirically show that our approach outperforms a pure exploration policy,
and that naively combining a pure exploration with a standard exploitation
policy is suboptimal.

</details>


### [362] [Learning Latent Energy-Based Models via Interacting Particle Langevin Dynamics](https://arxiv.org/abs/2510.12311)
*Joanna Marks,Tim Y. J. Wang,O. Deniz Akyildiz*

Main category: stat.ML

TL;DR: This paper proposes a new interacting particle algorithm leveraging SDEs for solving MMLE problems in latent variable models with energy-based priors, showing theoretical guarantees and empirical success.


<details>
  <summary>Details</summary>
Motivation: The motivation is to advance methods for learning latent variable models with energy-based priors, which requires solving MMLE problems effectively.

Method: The paper uses stochastic differential equations (SDEs) as a framework for MMLE, offering a continuous-time approach that is discretized into a practical algorithm with convergence guarantees.

Result: The proposed algorithm is shown to be theoretically sound and empirically effective on both synthetic and image datasets.

Conclusion: The study demonstrates that SDE-based methods are a robust approach for learning latent energy-based models, combining theoretical rigor and practical applicability.

Abstract: We develop interacting particle algorithms for learning latent variable
models with energy-based priors. To do so, we leverage recent developments in
particle-based methods for solving maximum marginal likelihood estimation
(MMLE) problems. Specifically, we provide a continuous-time framework for
learning latent energy-based models, by defining stochastic differential
equations (SDEs) that provably solve the MMLE problem. We obtain a practical
algorithm as a discretisation of these SDEs and provide theoretical guarantees
for the convergence of the proposed algorithm. Finally, we demonstrate the
empirical effectiveness of our method on synthetic and image datasets.

</details>


### [363] [Improved Central Limit Theorem and Bootstrap Approximations for Linear Stochastic Approximation](https://arxiv.org/abs/2510.12375)
*Bogdan Butyrin,Eric Moulines,Alexey Naumov,Sergey Samsonov,Qi-Man Shao,Zhuo-Song Zhang*

Main category: stat.ML

TL;DR: This paper enhances Berry-Esseen bounds for multivariate normal approximations in Linear Stochastic Approximation (LSA) with averaged iterates, and validates a bootstrap method with improved approximation rates.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve theoretical bounds and validation methods for multivariate normal approximations and the associated bootstrap procedure in LSA settings.

Method: The study refines Berry-Esseen bounds for convex distances and proves non-asymptotic multiplier bootstrap validity for the averaged LSA estimator.

Result: Enhanced bounds with rate up to $n^{-1/3}$ for normal approximations and $1/\sqrt{n}$ for error distribution approximations are established, marking significant improvements over prior work.

Conclusion: The results underscore the improved precision and reliability of both normal approximation and bootstrap methodologies in LSA, benefiting scenarios with finite samples.

Abstract: In this paper, we refine the Berry-Esseen bounds for the multivariate normal
approximation of Polyak-Ruppert averaged iterates arising from the linear
stochastic approximation (LSA) algorithm with decreasing step size. We consider
the normal approximation by the Gaussian distribution with covariance matrix
predicted by the Polyak-Juditsky central limit theorem and establish the rate
up to order $n^{-1/3}$ in convex distance, where $n$ is the number of samples
used in the algorithm. We also prove a non-asymptotic validity of the
multiplier bootstrap procedure for approximating the distribution of the
rescaled error of the averaged LSA estimator. We establish approximation rates
of order up to $1/\sqrt{n}$ for the latter distribution, which significantly
improves upon the previous results obtained by Samsonov et al. (2024).

</details>


### [364] [Geopolitics, Geoeconomics and Risk:A Machine Learning Approach](https://arxiv.org/abs/2510.12416)
*Alvaro Ortiz,Tomasa Rodrigo*

Main category: stat.ML

TL;DR: This paper introduces a novel daily dataset covering 42 countries to study the influence of sentiment dynamics on sovereign risk, showing improved forecasts using news-based indicators and machine learning.


<details>
  <summary>Details</summary>
Motivation: To better understand how news-based sentiment dynamics impact sovereign risk, and to assess the forecasting potential of these indicators compared to traditional economic drivers.

Method: The authors employ a novel high-frequency panel dataset and use statistical methods, including machine learning models like Random Forests, to analyze the relationships and forecasting accuracy.

Result: News-based indicators significantly enhance the predictive accuracy of forecasting models for sovereign risk, particularly when combined with non-linear machine learning methods.

Conclusion: Global financial variables are the dominant drivers of sovereign risk, but geopolitical risk and economic policy uncertainty also significantly influence sovereign risk, especially in emerging markets and under certain financial conditions.

Abstract: We introduce a novel high-frequency daily panel dataset of both markets and
news-based indicators -- including Geopolitical Risk, Economic Policy
Uncertainty, Trade Policy Uncertainty, and Political Sentiment -- for 42
countries across both emerging and developed markets. Using this dataset, we
study how sentiment dynamics shape sovereign risk, measured by Credit Default
Swap (CDS) spreads, and evaluate their forecasting value relative to
traditional drivers such as global monetary policy and market volatility. Our
horse-race analysis of forecasting models demonstrates that incorporating
news-based indicators significantly enhances predictive accuracy and enriches
the analysis, with non-linear machine learning methods -- particularly Random
Forests -- delivering the largest gains. Our analysis reveals that while global
financial variables remain the dominant drivers of sovereign risk, geopolitical
risk and economic policy uncertainty also play a meaningful role. Crucially,
their effects are amplified through non-linear interactions with global
financial conditions. Finally, we document pronounced regional heterogeneity,
as certain asset classes and emerging markets exhibit heightened sensitivity to
shocks in policy rates, global financial volatility, and geopolitical risk.

</details>


### [365] [Universal Adaptive Environment Discovery](https://arxiv.org/abs/2510.12547)
*Madi Matymov,Ba-Hien Tran,Maurizio Filippone*

Main category: stat.ML

TL;DR: The paper introduces Universal Adaptive Environment Discovery (UAED), a framework that learns adaptive data environments to improve generalization and reduce dependency on spurious correlations in machine learning.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of model reliance on spurious correlations (e.g., background-label shortcuts in datasets) and the lack of predefined environments for robust training.

Method: UAED introduces an adaptive framework to discover a distribution over data transformations, which is then used to create environments for robust objective optimization, applicable to existing methods like IRM, REx, and GroupDRO.

Result: UAED demonstrates theoretical robustness (via PAC-Bayes bounds) and empirical improvements, enhancing worst-case accuracy without predefined environmental knowledge while remaining competitive in mean accuracy.

Conclusion: The adaptive discovery of environments in UAED is a viable approach for achieving better out-of-distribution generalization in machine learning models.

Abstract: An open problem in Machine Learning is how to avoid models to exploit
spurious correlations in the data; a famous example is the background-label
shortcut in the Waterbirds dataset. A common remedy is to train a model across
multiple environments; in the Waterbirds dataset, this corresponds to training
by randomizing the background. However, selecting the right environments is a
challenging problem, given that these are rarely known a priori. We propose
Universal Adaptive Environment Discovery (UAED), a unified framework that
learns a distribution over data transformations that instantiate environments,
and optimizes any robust objective averaged over this learned distribution.
UAED yields adaptive variants of IRM, REx, GroupDRO, and CORAL without
predefined groups or manual environment design. We provide a theoretical
analysis by providing PAC-Bayes bounds and by showing robustness to test
environment distributions under standard conditions. Empirically, UAED
discovers interpretable environment distributions and improves worst-case
accuracy on standard benchmarks, while remaining competitive on mean accuracy.
Our results indicate that making environments adaptive is a practical route to
out-of-distribution generalization.

</details>


### [366] [Adapting Noise to Data: Generative Flows from 1D Processes](https://arxiv.org/abs/2510.12636)
*Jannis Chemseddine,Gregor Kornhardt,Richard Duong,Gabriele Steidl*

Main category: stat.ML

TL;DR: The paper introduces a flexible framework for generative models using learnable one-dimensional noising processes, offering advantages for data with heavy tails and compact supports.


<details>
  <summary>Details</summary>
Motivation: Existing generative models, such as those based on diffusion processes, have limitations in handling certain data characteristics. This paper seeks a flexible framework that adapts more effectively to data distributions.

Method: The method involves parameterizing the noise distribution through learnable quantile functions, enabling adaptation to data properties. It integrates with existing objectives like Flow Matching and consistency models.

Result: The proposed method effectively captures data with heavy tails and compact supports, as demonstrated in numerical experiments showcasing its flexibility and performance.

Conclusion: Learning quantile-based noise in generative models enhances adaptability to diverse data distributions, highlighting a promising direction for generative modeling frameworks.

Abstract: We introduce a general framework for constructing generative models using
one-dimensional noising processes. Beyond diffusion processes, we outline
examples that demonstrate the flexibility of our approach. Motivated by this,
we propose a novel framework in which the 1D processes themselves are
learnable, achieved by parameterizing the noise distribution through quantile
functions that adapt to the data. Our construction integrates seamlessly with
standard objectives, including Flow Matching and consistency models. Learning
quantile-based noise naturally captures heavy tails and compact supports when
present. Numerical experiments highlight both the flexibility and the
effectiveness of our method.

</details>


### [367] [Contraction and entropy production in continuous-time Sinkhorn dynamics](https://arxiv.org/abs/2510.12639)
*Anand Srinivasan,Jean-Jacques Slotine*

Main category: stat.ML

TL;DR: This paper examines the Sinkhorn algorithm and its vanishing-step-size limit, establishing connections to mirror descent in probability spaces and Markov dynamics. It provides entropy production analysis, metrics contraction conditions, and proves mathematical inequalities for improved practical applications.


<details>
  <summary>Details</summary>
Motivation: To better understand the theoretical properties of the Sinkhorn algorithm and its connections to probability measures, reversible Markov processes, and entropy dynamics, and explore its implications in practical applications such as generative models and algorithm stopping criteria.

Method: The authors use mathematical analysis to establish $L^2$ contraction criteria in certain metrics, derive entropy production rates, and prove inequalities like Poincaré and logarithmic Sobolev, focusing on the Sinkhorn flow as a structured gradient flow.

Result: They show $L^2$ contraction criteria, derive entropy production rate identities, and prove strict positivity of the spectral gap for $	ext{ε}>0$. Additionally, they illustrate practical applications like its utility in generative model latent spaces and stopping heuristics for algorithms.

Conclusion: The study highlights systematic diffusion process extensions for the Sinkhorn flow, proving exponential entropy decay reliant on LSI and emphasizing its applicability in areas like machine learning training spaces and algorithm optimization.

Abstract: Recently, the vanishing-step-size limit of the Sinkhorn algorithm at finite
regularization parameter $\varepsilon$ was shown to be a mirror descent in the
space of probability measures. We give $L^2$ contraction criteria in two
time-dependent metrics induced by the mirror Hessian, which reduce to the
coercivity of certain conditional expectation operators. We then give an exact
identity for the entropy production rate of the Sinkhorn flow, which was
previously known only to be nonpositive. Examining this rate shows that the
standard semigroup analysis of diffusion processes extends systematically to
the Sinkhorn flow. We show that the flow induces a reversible Markov dynamics
on the target marginal as an Onsager gradient flow. We define the Dirichlet
form associated to its (nonlocal) infinitesimal generator, prove a Poincar\'e
inequality for it, and show that the spectral gap is strictly positive along
the Sinkhorn flow whenever $\varepsilon > 0$. Lastly, we show that the entropy
decay is exponential if and only if a logarithmic Sobolev inequality (LSI)
holds. We give for illustration two immediate practical use-cases for the
Sinkhorn LSI: as a design principle for the latent space in which generative
models are trained, and as a stopping heuristic for discrete-time algorithms.

</details>


### [368] [Dendrograms of Mixing Measures for Softmax-Gated Gaussian Mixture of Experts: Consistency without Model Sweeps](https://arxiv.org/abs/2510.12744)
*Do Tien Hai,Trung Nguyen Mai,TrungTin Nguyen,Nhat Ho,Binh T. Nguyen,Christopher Drovandi*

Main category: stat.ML

TL;DR: This paper proposes a unified statistical method for softmax-gated Gaussian mixture of experts (SGMoE) to address challenges in parameter estimation and model selection. It introduces Voronoi-type loss functions and presents improved techniques for identifying the number of experts.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in the SGMoE framework, including gating parameter non-identifiability, coupled gate-expert interactions, and tight parameter dependencies in softmax-induced densities.

Method: The authors introduce Voronoi-type loss functions aligned with gate geometries, derive convergence rates for MLE in over-specified models, and adapt dendrogram-based techniques for consistent expert count selection.

Result: Simulation experiments confirm theoretical predictions, showing robust expert count recovery and improved performance even under model misspecification, outperforming other selection criteria.

Conclusion: The proposed SGMoE framework provides robust, interpretable results for model selection and parameter estimation, outperforming existing criteria and showing practical potential in real-world datasets like genetic trait mapping.

Abstract: We develop a unified statistical framework for softmax-gated Gaussian mixture
of experts (SGMoE) that addresses three long-standing obstacles in parameter
estimation and model selection: (i) non-identifiability of gating parameters up
to common translations, (ii) intrinsic gate-expert interactions that induce
coupled differential relations in the likelihood, and (iii) the tight
numerator-denominator coupling in the softmax-induced conditional density. Our
approach introduces Voronoi-type loss functions aligned with the gate-partition
geometry and establishes finite-sample convergence rates for the maximum
likelihood estimator (MLE). In over-specified models, we reveal a link between
the MLE's convergence rate and the solvability of an associated system of
polynomial equations characterizing near-nonidentifiable directions. For model
selection, we adapt dendrograms of mixing measures to SGMoE, yielding a
consistent, sweep-free selector of the number of experts that attains
pointwise-optimal parameter rates under overfitting while avoiding multi-size
training. Simulations on synthetic data corroborate the theory, accurately
recovering the expert count and achieving the predicted rates for parameter
estimation while closely approximating the regression function. Under model
misspecification (e.g., $\epsilon$-contamination), the dendrogram selection
criterion is robust, recovering the true number of mixture components, while
the Akaike information criterion, the Bayesian information criterion, and the
integrated completed likelihood tend to overselect as sample size grows. On a
maize proteomics dataset of drought-responsive traits, our dendrogram-guided
SGMoE selects two experts, exposes a clear mixing-measure hierarchy, stabilizes
the likelihood early, and yields interpretable genotype-phenotype maps,
outperforming standard criteria without multi-size training.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [369] [Enhancing Diffusion-Based Sampling with Molecular Collective Variables](https://arxiv.org/abs/2510.11923)
*Juno Nam,Bálint Máté,Artur P. Toshev,Manasa Kaniselvan,Rafael Gómez-Bombarelli,Ricky T. Q. Chen,Brandon Wood,Guan-Horng Liu,Benjamin Kurt Miller*

Main category: physics.chem-ph

TL;DR: The paper introduces a diffusion-based sampling method enhanced with collective variables (CVs) to improve molecular sampling efficiency and accuracy in discovering thermodynamic modes and estimating free energies.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based samplers are inefficient for practical molecular sampling because they are slower than molecular dynamics and fail to capture important thermodynamic modes.

Method: The method introduces a repulsive potential based on low-dimensional collective variables (CVs) to guide samples towards novel regions, effectively increasing temperature in the CV space. This supports mode discovery and facilitates reweighting to approximate the Boltzmann distribution.

Result: The proposed method demonstrated better efficiency, discovered diverse molecular conformational states, recovered accurate free energy profiles, and achieved reactive sampling (bond breaking and formation) with universal interatomic potentials.

Conclusion: This research advances diffusion-based sampling by making it practical for molecular sciences, offering faster, accurate, and thermodynamically relevant molecular sampling solutions.

Abstract: Diffusion-based samplers learn to sample complex, high-dimensional
distributions using energies or log densities alone, without training data.
Yet, they remain impractical for molecular sampling because they are often
slower than molecular dynamics and miss thermodynamically relevant modes.
Inspired by enhanced sampling, we encourage exploration by introducing a
sequential bias along bespoke, information-rich, low-dimensional projections of
atomic coordinates known as collective variables (CVs). We introduce a
repulsive potential centered on the CVs from recent samples, which pushes
future samples towards novel CV regions and effectively increases the
temperature in the projected space. Our resulting method improves efficiency,
mode discovery, enables the estimation of free energy differences, and retains
independent sampling from the approximate Boltzmann distribution via
reweighting by the bias. On standard peptide conformational sampling
benchmarks, the method recovers diverse conformational states and accurate free
energy profiles. We are the first to demonstrate reactive sampling using a
diffusion-based sampler, capturing bond breaking and formation with universal
interatomic potentials at near-first-principles accuracy. The approach resolves
reactive energy landscapes at a fraction of the wall-clock time of standard
sampling methods, advancing diffusion-based sampling towards practical use in
molecular sciences.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [370] [Proof of Cloud: Data Center Execution Assurance for Confidential VMs](https://arxiv.org/abs/2510.12469)
*Filip Rezabek,Moe Mahhouk,Andrew Miller,Stefan Genchev,Quintus Kilbourn,Georg Carle,Jonathan Passerat-Palmbach*

Main category: cs.CR

TL;DR: This paper introduces Data Center Execution Assurance (DCEA), a mechanism to ensure that Confidential Virtual Machines (CVMs) operate within physically trusted environments, thus bridging the gap in existing remote attestation processes.


<details>
  <summary>Details</summary>
Motivation: The paper was motivated by the lack of mechanisms to verify that CVMs run on physically trusted data center platforms, which leads to gaps in confidence about alignment with TEE vendors' threat models.

Method: The authors propose DCEA, which combines vTPM-anchored measurements and TPM quotes to verify the physical platform integrity of CVMs. This method enables remote verification and mitigates attacks like replay and attestation proxying.

Result: The proposed DCEA design was implemented on Google Cloud using Intel TDX and Intel TXT, demonstrating its practicality in real-world cloud environments.

Conclusion: The paper refines the threat model of CVMs and offers a practical, high-assurance approach for deploying confidential workloads in environments with minimal trusted infrastructure.

Abstract: Confidential Virtual Machines (CVMs) protect data in use by running workloads
inside hardware-isolated environments. In doing so, they also inherit the
limitations of the underlying hardware. Trusted Execution Environments (TEEs),
which enforce this isolation, explicitly exclude adversaries with physical
access from their threat model. Commercial TEEs, e.g., Intel TDX, thus assume
infrastructure providers do not physically exploit hardware and serve as
safeguards instead. This creates a tension: tenants must trust provider
integrity at the hardware layer, yet existing remote attestation offers no way
to verify that CVMs actually run on physically trusted platforms, leaving
today's CVM deployments unable to demonstrate that their guarantees align with
the TEE vendor's threat model.
  We bridge this confidence gap with Data Center Execution Assurance (DCEA), a
design generating "Proofs of Cloud". DCEA binds a CVM to its underlying
platform using vTPM-anchored measurements, ensuring CVM launch evidence and TPM
quotes refer to the same physical chassis.
  This takes advantage of the fact that data centers are often identifiable via
TPMs. Our approach applies to CVMs accessing vTPMs and running on top of
software stacks fully controlled by the cloud provider, as well as
single-tenant bare-metal deployments with discrete TPMs. We trust providers for
integrity (certificate issuance), but not for the confidentiality of
CVM-visible state. DCEA enables remote verification of a CVM's platform origin
and integrity, mitigating attacks like replay and attestation proxying. We
include a candidate implementation on Google Cloud and Intel TDX that leverages
Intel TXT for trusted launch. Our design refines CVMs' threat model and
provides a practical path for deploying high-assurance, confidential workloads
in minimally trusted environments.

</details>


### [371] [BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing](https://arxiv.org/abs/2510.11823)
*Caelin Kaplan,Alexander Warnecke,Neil Archibald*

Main category: cs.CR

TL;DR: BlackIce is a containerized toolkit aimed at simplifying AI red teaming by bundling essential tools within a reproducible Docker image, enabling streamlined AI model assessments.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the challenges in selecting appropriate tools and managing dependencies for AI red teaming due to the growing complexity and the need for enhanced security in AI systems.

Method: The authors introduce BlackIce, a containerized toolkit inspired by Kali Linux, which includes a unified interface and 14 selected open-source tools for AI model security testing. The container image enables ease of use, modular architecture, and streamlined assessments.

Result: BlackIce allows for simplified AI red teaming processes through a reproducible Docker image that integrates multiple tools. It supports easy deployment either locally or via cloud platforms.

Conclusion: BlackIce lowers barriers for AI red teaming practices, facilitates standardized assessments, and encourages community-driven improvements through its modular framework.

Abstract: AI models are being increasingly integrated into real-world systems, raising
significant concerns about their safety and security. Consequently, AI red
teaming has become essential for organizations to proactively identify and
address vulnerabilities before they can be exploited by adversaries. While
numerous AI red teaming tools currently exist, practitioners face challenges in
selecting the most appropriate tools from a rapidly expanding landscape, as
well as managing complex and frequently conflicting software dependencies
across isolated projects. Given these challenges and the relatively small
number of organizations with dedicated AI red teams, there is a strong need to
lower barriers to entry and establish a standardized environment that
simplifies the setup and execution of comprehensive AI model assessments.
  Inspired by Kali Linux's role in traditional penetration testing, we
introduce BlackIce, an open-source containerized toolkit designed for red
teaming Large Language Models (LLMs) and classical machine learning (ML)
models. BlackIce provides a reproducible, version-pinned Docker image that
bundles 14 carefully selected open-source tools for Responsible AI and Security
testing, all accessible via a unified command-line interface. With this setup,
initiating red team assessments is as straightforward as launching a container,
either locally or using a cloud platform. Additionally, the image's modular
architecture facilitates community-driven extensions, allowing users to easily
adapt or expand the toolkit as new threats emerge. In this paper, we describe
the architecture of the container image, the process used for selecting tools,
and the types of evaluations they support.

</details>


### [372] [Countermind: A Multi-Layered Security Architecture for Large Language Models](https://arxiv.org/abs/2510.11837)
*Dominik Schwarz*

Main category: cs.CR

TL;DR: Proposes 'Countermind,' a security framework for Large Language Models (LLM) to proactively defend against prompt injection and jailbreaking attacks.


<details>
  <summary>Details</summary>
Motivation: LLMs face security vulnerabilities due to their inability to differentiate between trusted and untrusted inputs, making them susceptible to attacks like prompt injection and jailbreaking.

Method: This paper introduces Countermind, a multi-layered security architecture containing several components: Semantic Boundary Logic, Parameter-Space Restriction, a Secure Self-Regulating Core, Multimodal Input Sandbox, and Context-Defense mechanisms.

Result: Detailed an evaluation framework to assess Countermind's effectiveness in reducing attack success rates and measuring threat mitigation latency.

Conclusion: Adopting Countermind could significantly improve the security of LLMs by shifting from a reactive to a proactive defense model.

Abstract: The security of Large Language Model (LLM) applications is fundamentally
challenged by "form-first" attacks like prompt injection and jailbreaking,
where malicious instructions are embedded within user inputs. Conventional
defenses, which rely on post hoc output filtering, are often brittle and fail
to address the root cause: the model's inability to distinguish trusted
instructions from untrusted data. This paper proposes Countermind, a
multi-layered security architecture intended to shift defenses from a reactive,
post hoc posture to a proactive, pre-inference, and intra-inference enforcement
model. The architecture proposes a fortified perimeter designed to structurally
validate and transform all inputs, and an internal governance mechanism
intended to constrain the model's semantic processing pathways before an output
is generated. The primary contributions of this work are conceptual designs
for: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text
Crypter intended to reduce the plaintext prompt injection attack surface,
provided all ingestion paths are enforced. (2) A Parameter-Space Restriction
(PSR) mechanism, leveraging principles from representation engineering, to
dynamically control the LLM's access to internal semantic clusters, with the
goal of mitigating semantic drift and dangerous emergent behaviors. (3) A
Secure, Self-Regulating Core that uses an OODA loop and a learning security
module to adapt its defenses based on an immutable audit log. (4) A Multimodal
Input Sandbox and Context-Defense mechanisms to address threats from
non-textual data and long-term semantic poisoning. This paper outlines an
evaluation plan designed to quantify the proposed architecture's effectiveness
in reducing the Attack Success Rate (ASR) for form-first attacks and to measure
its potential latency overhead.

</details>


### [373] [Deep Research Brings Deeper Harm](https://arxiv.org/abs/2510.11851)
*Shuo Chen,Zonggen Li,Zhen Han,Bailan He,Tong Liu,Haokun Chen,Georg Groh,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: Deep Research (DR) agents built on Large Language Models (LLMs) exhibit significant risks due to systemic alignment issues, especially in high-stakes domains like biosecurity.


<details>
  <summary>Details</summary>
Motivation: To address the risks posed by the misuse of DR agents, particularly their ability to produce forbidden knowledge and bypass safeguards in complex and dangerous scenarios.

Method: Proposal of two jailbreak strategies (Plan Injection and Intent Hijack) to investigate vulnerabilities in DR agents and experimentation across multiple LLMs and safety benchmarks.

Result: Experiments revealed systemic vulnerabilities where DR agents bypass safeguards, weaken alignment through multi-step planning, and generate dangerous and coherent reports, surpassing standalone LLMs.

Conclusion: DR agents demonstrate misalignment vulnerabilities that current safeguards cannot adequately address, and there is a need for advanced safety and alignment methods tailored to these agents.

Abstract: Deep Research (DR) agents built on Large Language Models (LLMs) can perform
complex, multi-step research by decomposing tasks, retrieving online
information, and synthesizing detailed reports. However, the misuse of LLMs
with such powerful capabilities can lead to even greater risks. This is
especially concerning in high-stakes and knowledge-intensive domains such as
biosecurity, where DR can generate a professional report containing detailed
forbidden knowledge. Unfortunately, we have found such risks in practice:
simply submitting a harmful query, which a standalone LLM directly rejects, can
elicit a detailed and dangerous report from DR agents. This highlights the
elevated risks and underscores the need for a deeper safety analysis. Yet,
jailbreak methods designed for LLMs fall short in exposing such unique risks,
as they do not target the research ability of DR agents. To address this gap,
we propose two novel jailbreak strategies: Plan Injection, which injects
malicious sub-goals into the agent's plan; and Intent Hijack, which reframes
harmful queries as academic research questions. We conducted extensive
experiments across different LLMs and various safety benchmarks, including
general and biosecurity forbidden prompts. These experiments reveal 3 key
findings: (1) Alignment of the LLMs often fail in DR agents, where harmful
prompts framed in academic terms can hijack agent intent; (2) Multi-step
planning and execution weaken the alignment, revealing systemic vulnerabilities
that prompt-level safeguards cannot address; (3) DR agents not only bypass
refusals but also produce more coherent, professional, and dangerous content,
compared with standalone LLMs. These results demonstrate a fundamental
misalignment in DR agents and call for better alignment techniques tailored to
DR agents. Code and datasets are available at
https://chenxshuo.github.io/deeper-harm.

</details>


### [374] [CTIArena: Benchmarking LLM Knowledge and Reasoning Across Heterogeneous Cyber Threat Intelligence](https://arxiv.org/abs/2510.11974)
*Yutong Cheng,Yang Liu,Changze Li,Dawn Song,Peng Gao*

Main category: cs.CR

TL;DR: The paper introduces CTIArena, a benchmark for evaluating large language models (LLMs) in the context of cyber threat intelligence (CTI), addressing limitations in current studies.


<details>
  <summary>Details</summary>
Motivation: There is growing interest in applying LLMs to CTI, but existing efforts lack a comprehensive evaluation system and fail to leverage external knowledge bases or multi-source analysis.

Method: The authors developed CTIArena to evaluate LLMs across a wide range of CTI tasks under heterogeneous and knowledge-augmented conditions, using retrieval-augmented techniques.

Result: Ten LLMs were tested, showing struggles in closed-book scenarios but improving significantly when complemented with security-specific knowledge.

Conclusion: General-purpose LLMs are insufficient for CTI tasks, highlighting the necessity for domain-specific methods to optimize their performance.

Abstract: Cyber threat intelligence (CTI) is central to modern cybersecurity, providing
critical insights for detecting and mitigating evolving threats. With the
natural language understanding and reasoning capabilities of large language
models (LLMs), there is increasing interest in applying them to CTI, which
calls for benchmarks that can rigorously evaluate their performance. Several
early efforts have studied LLMs on some CTI tasks but remain limited: (i) they
adopt only closed-book settings, relying on parametric knowledge without
leveraging CTI knowledge bases; (ii) they cover only a narrow set of tasks,
lacking a systematic view of the CTI landscape; and (iii) they restrict
evaluation to single-source analysis, unlike realistic scenarios that require
reasoning across multiple sources. To fill these gaps, we present CTIArena, the
first benchmark for evaluating LLM performance on heterogeneous, multi-source
CTI under knowledge-augmented settings. CTIArena spans three categories,
structured, unstructured, and hybrid, further divided into nine tasks that
capture the breadth of CTI analysis in modern security operations. We evaluate
ten widely used LLMs and find that most struggle in closed-book setups but show
noticeable gains when augmented with security-specific knowledge through our
designed retrieval-augmented techniques. These findings highlight the
limitations of general-purpose LLMs and the need for domain-tailored techniques
to fully unlock their potential for CTI.

</details>


### [375] [HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities](https://arxiv.org/abs/2510.12200)
*Xiaoxue Ren,Penghao Jiang,Kaixin Li,Zhiyong Huang,Xiaoning Du,Jiaojiao Jiang,Zhenchang Xing,Jiamou Sun,Terry Yue Zhuo*

Main category: cs.CR

TL;DR: HackWorld introduces a framework to evaluate computer-use agents' (CUAs) capability to identify and exploit web vulnerabilities through visual interactions, finding their current performance lacking.


<details>
  <summary>Details</summary>
Motivation: Cyberattacks targeting web applications are rising, while traditional penetration testing is resource-intensive and insufficient to keep up. CUAs show promise in automating cybersecurity efforts but lack exploration in handling complex web vulnerabilities interactively.

Method: The framework, HackWorld, includes 36 real-world applications with realistic flaws and uses a Capture-the-Flag (CTF) setup to systematically test CUAs' ability to exploit vulnerabilities via complex visual interactions.

Result: State-of-the-art CUAs show exploitation rates below 12%, struggle with multi-step attack planning, and demonstrate low security awareness and poor utilization of security tools.

Conclusion: CUAs currently fall short in web security exploitation through graphical interaction, revealing a need for enhanced cybersecurity capabilities in future CUAs.

Abstract: Web applications are prime targets for cyberattacks as gateways to critical
services and sensitive data. Traditional penetration testing is costly and
expertise-intensive, making it difficult to scale with the growing web
ecosystem. While language model agents show promise in cybersecurity, modern
web applications demand visual understanding, dynamic content handling, and
multi-step interactions that only computer-use agents (CUAs) can perform. Yet,
their ability to discover and exploit vulnerabilities through graphical
interfaces remains largely unexplored. We present HackWorld, the first
framework for systematically evaluating CUAs' capabilities to exploit web
application vulnerabilities via visual interaction. Unlike sanitized
benchmarks, HackWorld includes 36 real-world applications across 11 frameworks
and 7 languages, featuring realistic flaws such as injection vulnerabilities,
authentication bypasses, and unsafe input handling. Using a Capture-the-Flag
(CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses
while navigating complex web interfaces. Evaluation of state-of-the-art CUAs
reveals concerning trends: exploitation rates below 12% and low cybersecurity
awareness. CUAs often fail at multi-step attack planning and misuse security
tools. These results expose the current limitations of CUAs in web security
contexts and highlight opportunities for developing more security-aware agents
capable of effective vulnerability detection and exploitation.

</details>


### [376] [PromptLocate: Localizing Prompt Injection Attacks](https://arxiv.org/abs/2510.12252)
*Yuqi Jia,Yupei Liu,Zedian Shao,Jinyuan Jia,Neil Gong*

Main category: cs.CR

TL;DR: The paper introduces PromptLocate, the first method for identifying injected prompts in contaminated inputs caused by prompt injection attacks.


<details>
  <summary>Details</summary>
Motivation: Prompt injection attacks compromise large language models by tricking them into executing malicious instructions embedded in input data. Detecting the injected prompt is critical for forensic analysis and recovery.

Method: PromptLocate executes three steps: semantic segmentation of input data, detection of segments with injected instructions, and identification of segments with injected data.

Result: PromptLocate was demonstrated to accurately identify injected prompts across diverse attacks, including existing and adaptive variations.

Conclusion: PromptLocate effectively addresses the challenge of localizing injected prompts, filling an unexplored gap in understanding and mitigating prompt injection attacks.

Abstract: Prompt injection attacks deceive a large language model into completing an
attacker-specified task instead of its intended task by contaminating its input
data with an injected prompt, which consists of injected instruction(s) and
data. Localizing the injected prompt within contaminated data is crucial for
post-attack forensic analysis and data recovery. Despite its growing
importance, prompt injection localization remains largely unexplored. In this
work, we bridge this gap by proposing PromptLocate, the first method for
localizing injected prompts. PromptLocate comprises three steps: (1) splitting
the contaminated data into semantically coherent segments, (2) identifying
segments contaminated by injected instructions, and (3) pinpointing segments
contaminated by injected data. We show PromptLocate accurately localizes
injected prompts across eight existing and eight adaptive attacks.

</details>


### [377] [Locket: Robust Feature-Locking Technique for Language Models](https://arxiv.org/abs/2510.12117)
*Lipeng He,Vasisht Duddu,N. Asokan*

Main category: cs.CR

TL;DR: The paper introduces 'Locket,' a robust and scalable feature-locking technique for tiered chatbot subscription models, enabling pay-to-unlock schemes.


<details>
  <summary>Details</summary>
Motivation: Current pay-to-unlock schemes for chatbots lack robust and scalable feature-locking techniques, needed to balance economic viability with effective feature controls.

Method: Locket employs a novel merging approach to attach adapters to large language models (LLMs), effectively refusing unauthorized features while preserving functionality.

Result: Evaluation shows Locket achieves 100% refusal for unauthorized features, ≤7% utility degradation in unlocked features, ≤5% attack success rate, and scalability across multiple features and clients.

Conclusion: Locket addresses the limitations of existing techniques, offering a scalable and robust solution for pay-to-unlock schemes in chatbot systems.

Abstract: Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to
generate revenue, offering basic models for free users, and advanced models for
paying subscribers. However, a finer-grained pay-to-unlock scheme for premium
features (e.g., math, coding) is thought to be more economically viable for the
providers. Such a scheme requires a feature-locking technique (FLoTE) which is
(i) effective in refusing locked features, (ii) utility-preserving for unlocked
features, (iii) robust against evasion or unauthorized credential sharing, and
(iv) scalable to multiple features and users. However, existing FLoTEs (e.g.,
password-locked models) are not robust or scalable. We present Locket, the
first robust and scalable FLoTE to enable pay-to-unlock schemes. Locket uses a
novel merging approach to attach adapters to an LLM for refusing unauthorized
features. Our comprehensive evaluation shows that Locket is effective ($100$%
refusal on locked features), utility-preserving ($\leq 7$% utility degradation
in unlocked features), robust ($\leq 5$% attack success rate), and scales to
multiple features and clients.

</details>


### [378] [DeepTrust: Multi-Step Classification through Dissimilar Adversarial Representations for Robust Android Malware Detection](https://arxiv.org/abs/2510.12310)
*Daniel Pulido-Cortázar,Daniel Gibert,Felip Manyà*

Main category: cs.CR

TL;DR: The paper introduces DeepTrust, a robust metaheuristic for detecting malware on Android, which secures state-of-the-art performance under adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of machine learning-based Android malware detection methods against adversarial examples.

Method: Proposes DeepTrust, a system organizing classifiers (e.g., deep neural networks) into a sequence with a final decision made by an internal model based on cascading conditions, ensuring robustness against attacks by maximizing divergence in representations among the classifiers.

Result: DeepTrust achieved first place in the 2025 Robust Android Malware Detection competition, outperforming competitors by up to 266% under feature-space evasion attacks, with a high malware detection rate, low false positives (below 1%), and strong accuracy on clean data.

Conclusion: DeepTrust provides a significant leap in adversarial robustness for Android malware detection by leveraging diverse representations and creating unpredictability in decision boundaries, all while maintaining strong overall accuracy.

Abstract: Over the last decade, machine learning has been extensively applied to
identify malicious Android applications. However, such approaches remain
vulnerable against adversarial examples, i.e., examples that are subtly
manipulated to fool a machine learning model into making incorrect predictions.
This research presents DeepTrust, a novel metaheuristic that arranges flexible
classifiers, like deep neural networks, into an ordered sequence where the
final decision is made by a single internal model based on conditions activated
in cascade. In the Robust Android Malware Detection competition at the 2025
IEEE Conference SaTML, DeepTrust secured the first place and achieved
state-of-the-art results, outperforming the next-best competitor by up to 266%
under feature-space evasion attacks. This is accomplished while maintaining the
highest detection rate on non-adversarial malware and a false positive rate
below 1%. The method's efficacy stems from maximizing the divergence of the
learned representations among the internal models. By using classifiers
inducing fundamentally dissimilar embeddings of the data, the decision space
becomes unpredictable for an attacker. This frustrates the iterative
perturbation process inherent to evasion attacks, enhancing system robustness
without compromising accuracy on clean examples.

</details>


### [379] [Formal Models and Convergence Analysis for Context-Aware Security Verification](https://arxiv.org/abs/2510.12440)
*Ayush Chaudhary*

Main category: cs.CR

TL;DR: This paper introduces a formal framework for context-aware security verification in adaptive systems, outlining new theoretical properties and validating improvements over static models through experiments.


<details>
  <summary>Details</summary>
Motivation: Traditional security verifiers may fail to adapt to context, limiting their effectiveness in detecting threats in ML-enhanced adaptive systems.

Method: The authors propose a security property called context-completeness, derive theoretical proofs for verification, and validate through experiments using exploit samples to demonstrate improvements.

Result: Key findings include increases in detection accuracy, success rates with enriched context, training loss convergence, and manageable false positive rates, all validating their theoretical predictions.

Conclusion: The research demonstrates that context-aware adaptive verification significantly outperforms static verification methods in improving security guarantees, under the given assumptions.

Abstract: We present a formal framework for context-aware security verification that
establishes provable guarantees for ML-enhanced adaptive systems. We introduce
context-completeness - a new security property - and prove: (1) sample
complexity bounds showing when adaptive verification succeeds, (2)
information-theoretic limits relating context richness to detection capability,
(3) convergence guarantees for ML-based payload generators, and (4)
compositional soundness bounds. We further provide a formal separation between
static context-blind verifiers and context-aware adaptive verifiers: for a
natural family of targets, any static verifier with finite payload budget
achieves completeness at most alpha, while a context-aware verifier with
sufficient information achieves completeness greater than alpha. We validate
our theoretical predictions through controlled experiments on 97,224 exploit
samples, demonstrating: detection accuracy improving from 58% to 69.93% with
dataset growth, success probability increasing from 51% to 82% with context
enrichment, training loss converging at O(1/sqrt(T)) rate, and false positive
rate (10.19%) within theoretical bounds (12%). Our results show that
theoretically-grounded adaptive verification achieves provable improvements
over static approaches under stated assumptions while maintaining soundness
guarantees.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [380] [Generative AI and Firm Productivity: Field Experiments in Online Retail](https://arxiv.org/abs/2510.12049)
*Lu Fang,Zhe Yuan,Kaifu Zhang,Dante Donati,Miklos Sarvary*

Main category: econ.GN

TL;DR: This study examines how Generative Artificial Intelligence (GenAI) impacts firm productivity in online retail through field experiments, showing significant sales growth and productivity improvements.


<details>
  <summary>Details</summary>
Motivation: To understand the economic impact of GenAI on productivity, especially in an online retail context, and provide evidence of its efficacy.

Method: The research conducted large-scale randomized field experiments over six months, testing GenAI integration in seven business workflows at an online retail platform with millions of users.

Result: The adoption of GenAI increased sales by up to 16.3% depending on its contribution, leading to a total factor productivity improvement and an annual incremental value of $5 per consumer.

Conclusion: GenAI significantly improves online retail efficiency and conversion rates, especially benefiting smaller sellers and newer consumers, offering promising insights into its broader potential.

Abstract: We quantify the impact of Generative Artificial Intelligence (GenAI) on firm
productivity through a series of large-scale randomized field experiments
involving millions of users and products at a leading cross-border online
retail platform. Over six months in 2023-2024, GenAI-based enhancements were
integrated into seven consumer-facing business workflows. We find that GenAI
adoption significantly increases sales, with treatment effects ranging from 0\%
to 16.3\%, depending on GenAI's marginal contribution relative to existing firm
practices. Because inputs and prices were held constant across experimental
arms, these gains map directly into total factor productivity improvements.
Across the four GenAI applications with positive effects, the implied annual
incremental value is approximately \$5 per consumer-an economically meaningful
impact given the retailer's scale and the early stage of GenAI adoption. The
primary mechanism operates through higher conversion rates, consistent with
GenAI reducing frictions in the marketplace and improving consumer experience.
We also document substantial heterogeneity: smaller and newer sellers, as well
as less experienced consumers, exhibit disproportionately larger gains. Our
findings provide novel, large-scale causal evidence on the productivity effects
of GenAI in online retail, highlighting both its immediate value and broader
potential.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [381] [Disentangling Neurodegeneration with Brain Age Gap Prediction Models: A Graph Signal Processing Perspective](https://arxiv.org/abs/2510.12763)
*Saurabh Sihag,Gonzalo Mateos,Alejandro Ribeiro*

Main category: eess.SP

TL;DR: This paper addresses limitations in traditional neurodegeneration assessment methods by introducing a graph-based framework, focusing on improving brain age gap prediction (BAGP) using graph neural networks, specifically the coVariance neural network (VNN).


<details>
  <summary>Details</summary>
Motivation: Conventional methods for assessing neurodegeneration (e.g., measuring cortical thickness or brain volume) lack the statistical sophistication to fully account for spatially correlated and heterogeneous brain changes.

Method: The authors propose using graph signal processing (GSP) and graph neural networks (GNNs), particularly the coVariance neural network (VNN), which uses anatomical covariance matrices derived from MRI data to improve BAGP models.

Result: The VNN approach provides robust brain age gap predictions with theoretical grounding and operational interpretability, improving the generalizability of BAGP models.

Conclusion: This study offers a clearer pathway for reliable and interpretable brain age gap prediction methods, paving the way for advancements in personalized medicine.

Abstract: Neurodegeneration, characterized by the progressive loss of neuronal
structure or function, is commonly assessed in clinical practice through
reductions in cortical thickness or brain volume, as visualized by structural
MRI. While informative, these conventional approaches lack the statistical
sophistication required to fully capture the spatially correlated and
heterogeneous nature of neurodegeneration, which manifests both in healthy
aging and in neurological disorders. To address these limitations, brain age
gap has emerged as a promising data-driven biomarker of brain health. The brain
age gap prediction (BAGP) models estimate the difference between a person's
predicted brain age from neuroimaging data and their chronological age. The
resulting brain age gap serves as a compact biomarker of brain health, with
recent studies demonstrating its predictive utility for disease progression and
severity. However, practical adoption of BAGP models is hindered by their
methodological obscurities and limited generalizability across diverse clinical
populations. This tutorial article provides an overview of BAGP and introduces
a principled framework for this application based on recent advancements in
graph signal processing (GSP). In particular, we focus on graph neural networks
(GNNs) and introduce the coVariance neural network (VNN), which leverages the
anatomical covariance matrices derived from structural MRI. VNNs offer strong
theoretical grounding and operational interpretability, enabling robust
estimation of brain age gap predictions. By integrating perspectives from GSP,
machine learning, and network neuroscience, this work clarifies the path
forward for reliable and interpretable BAGP models and outlines future research
directions in personalized medicine.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [382] [Why the noise model matters: A performance gap in learned regularization](https://arxiv.org/abs/2510.12521)
*Sebastian Banert,Christoph Brauer,Dirk Lorenz,Lionel Tondji*

Main category: math.NA

TL;DR: This paper investigates learning regularizers for linear inverse problems, emphasizing the role of noise statistics in achieving optimal reconstruction.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of optimizing regularization methods in scenarios where precise noise statistics are unavailable.

Method: The authors theoretically analyze regularization methods such as Tikhonov, Lavrentiev, and quadratic regularization, comparing them against the optimal affine reconstruction, and conduct numerical experiments.

Result: They find that a performance gap exists between standard regularization methods and optimal reconstruction under non-white noise conditions. Different regularization types yield distinct outcomes.

Conclusion: The choice of regularizer structure is essential when noise models are not explicitly learned, and accurately modeling or co-learning noise statistics is highly valuable.

Abstract: This article addresses the challenge of learning effective regularizers for
linear inverse problems. We analyze and compare several types of learned
variational regularization against the theoretical benchmark of the optimal
affine reconstruction, i.e. the best possible affine linear map for minimizing
the mean squared error. It is known that this optimal reconstruction can be
achieved using Tikhonov regularization, but this requires precise knowledge of
the noise covariance to properly weight the data fidelity term. However, in
many practical applications, noise statistics are unknown. We therefore
investigate the performance of regularization methods learned without access to
this noise information, focusing on Tikhonov, Lavrentiev, and quadratic
regularization. Our theoretical analysis and numerical experiments demonstrate
that for non-white noise, a performance gap emerges between these methods and
the optimal affine reconstruction. Furthermore, we show that these different
types of regularization yield distinct results, highlighting that the choice of
regularizer structure is critical when the noise model is not explicitly
learned. Our findings underscore the significant value of accurately modeling
or co-learning noise statistics in data-driven regularization.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [383] [Quantum Kernel Methods: Convergence Theory, Separation Bounds and Applications to Marketing Analytics](https://arxiv.org/abs/2510.11744)
*Laura Sáez-Ortuño,Santiago Forgas-Coll,Massimiliano Ferrara*

Main category: quant-ph

TL;DR: This paper investigates using quantum kernel methods for consumer classification in the Noisy Intermediate-Scale Quantum (NISQ) regime and proposes a hybrid quantum-classical pipeline.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to explore the feasibility of quantum kernel methods as a practical solution for consumer classification tasks, taking into account current hardware constraints in the NISQ era.

Method: The paper proposes a hybrid pipeline that incorporates quantum-kernel Support Vector Machines (Q-SVM) and quantum feature extraction (QFE). The approach is evaluated through simulations and shallow-depth quantum hardware tests.

Result: The Q-SVM achieves competitive performance metrics, including an accuracy of 0.7790, precision of 0.7647, recall of 0.8609, F1 score of 0.8100, and a ROC AUC of 0.83, outperforming classical SVM in sensitivity and maintaining competitive precision.

Conclusion: The results serve as a promising starting point for integrating quantum algorithms into consumer classification workflows in the NISQ era, rather than presenting a definitive benchmark, demonstrating the potential of shallow yet expressive quantum embeddings.

Abstract: This work studies the feasibility of applying quantum kernel methods to a
real consumer classification task in the NISQ regime. We present a hybrid
pipeline that combines a quantum-kernel Support Vector Machine (Q-SVM) with a
quantum feature extraction module (QFE), and benchmark it against classical and
quantum baselines in simulation and with limited shallow-depth hardware runs.
With fixed hyperparameters, the proposed Q-SVM attains 0.7790 accuracy, 0.7647
precision, 0.8609 recall, 0.8100 F1, and 0.83 ROC AUC, exhibiting higher
sensitivity while maintaining competitive precision relative to classical SVM.
We interpret these results as an initial indicator and a concrete starting
point for NISQ-era workflows and hardware integration, rather than a definitive
benchmark. Methodologically, our design aligns with recent work that formalizes
quantum-classical separations and verifies resources via XEB-style approaches,
motivating shallow yet expressive quantum embeddings to achieve robust
separability despite hardware noise constraints.

</details>


### [384] [Neural Guided Sampling for Quantum Circuit Optimization](https://arxiv.org/abs/2510.12430)
*Bodo Rosenhahn,Tobias J. Osborne,Christoph Hirche*

Main category: quant-ph

TL;DR: The paper introduces a 2D neural-guided sampling method for reducing quantum circuits, improving upon traditional optimization techniques like those in Qiskit and BQSKit.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in quantum circuit optimization processes, where longer circuits lead to reduced computational accuracy due to hardware constraints and decoherence.

Method: The authors propose a neural network that leverages a 2D representation of quantum circuits to predict and reduce redundant gate groups, improving sampling efficiency during transpilation.

Result: The proposed method demonstrated superior performance and reduced compute times when compared to traditional optimization techniques like Qiskit and BQSKit.

Conclusion: The 2D neural-guided sampling method offers a practical solution for efficient quantum circuit reduction, paving the way for improved functionality in quantum computing hardware.

Abstract: Translating a general quantum circuit on a specific hardware topology with a
reduced set of available gates, also known as transpilation, comes with a
substantial increase in the length of the equivalent circuit. Due to
decoherence, the quality of the computational outcome can degrade seriously
with increasing circuit length. Thus, there is major interest to reduce a
quantum circuit to an equivalent circuit which is in its gate count as short as
possible. One method to address efficient transpilation is based on approaches
known from stochastic optimization, e.g. by using random sampling and token
replacement strategies. Here, a core challenge is that these methods can suffer
from sampling efficiency, causing long and energy consuming optimization time.
As a remedy, we propose in this work 2D neural guided sampling. Thus, given a
2D representation of a quantum circuit, a neural network predicts groups of
gates in the quantum circuit, which are likely reducible. Thus, it leads to a
sampling prior which can heavily reduce the compute time for quantum circuit
reduction. In several experiments, we demonstrate that our method is superior
to results obtained from different qiskit or BQSKit optimization levels.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [385] [Multi-objective Bayesian Optimization with Human-in-the-Loop for Flexible Neuromorphic Electronics Fabrication](https://arxiv.org/abs/2510.11727)
*Benius Dunn,Javier Meza-Arroyo,Armi Tiihonen,Mark Lee,Julia W. P. Hsu*

Main category: cs.ET

TL;DR: The research utilizes a combined method of multi-objective Bayesian optimization (MOBO) and human-in-the-loop feedback to optimize flexible metal-insulator-metal capacitors with aluminum oxide dielectrics for neuromorphic computing applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the processing constraints in fabricating flexible metal oxide-based neuromorphic electronics and optimize the trade-offs between the electrical properties of aluminum oxide dielectrics.

Method: The study uses photonic curing combined with MOBO and a human-in-the-loop (HITL) framework to determine ideal curing conditions. This approach incorporates failed experiments into the learning process and analyzes variable importance using Shapley Additive exPlanations.

Result: The approach reduces the number of experimental rounds required, optimizes processing conditions, and provides insights into the relationship between inputs and the properties of the capacitors.

Conclusion: This combined framework accelerates the optimization process and can be adapted to other complex experimental challenges, particularly in scenarios with interconnected variables and high experimental failure rates.

Abstract: Neuromorphic computing hardware enables edge computing and can be implemented
in flexible electronics for novel applications. Metal oxide materials are
promising candidates for fabricating flexible neuromorphic electronics, but
suffer from processing constraints due to the incompatibilities between oxides
and polymer substrates. In this work, we use photonic curing to fabricate
flexible metal-insulator-metal capacitors with solution-processible aluminum
oxide dielectric tailored for neuromorphic applications. Because photonic
curing outcomes depend on many input parameters, identifying an optimal
processing condition through a traditional grid-search approach is unfeasible.
Here, we apply multi-objective Bayesian optimization (MOBO) to determine
photonic curing conditions that optimize the trade-off between desired
electrical properties of large capacitance-frequency dispersion and low leakage
current. Furthermore, we develop a human-in-the-loop (HITL) framework for
incorporating failed experiments into the MOBO machine learning workflow,
demonstrating that this framework accelerates optimization by reducing the
number of experimental rounds required. Once optimization is concluded, we
analyze different Pareto-optimal conditions to tune the dielectrics properties
and provide insight into the importance of different inputs through Shapley
Additive exPlanations analysis. The demonstrated framework of combining MOBO
with HITL feedback can be adapted to a wide range of multi-objective
experimental problems that have interconnected inputs and high experimental
failure rates to generate usable results for machine learning models.

</details>


### [386] [Quantum Annealing for Staff Scheduling in Educational Environments](https://arxiv.org/abs/2510.12278)
*Alessia Ciacco,Francesca Guerriero,Eneko Osaba*

Main category: cs.ET

TL;DR: This paper tackles a staff allocation problem in a public school system using quantum annealing, showing efficient and balanced solutions.


<details>
  <summary>Details</summary>
Motivation: The aim is to address a staff allocation issue involving multiple school sites and levels in Calabria, Italy, while considering constraints such as availability, competencies, and fairness.

Method: The researchers developed an optimization model and utilized quantum annealing techniques to achieve efficient staff allocations.

Result: Quantum annealing delivered balanced staff assignments with short runtimes, demonstrating its computational capability on real-world data.

Conclusion: Quantum annealing proves effective for practical use in educational scheduling tasks and complex resource allocation challenges.

Abstract: We address a novel staff allocation problem that arises in the organization
of collaborators among multiple school sites and educational levels. The
problem emerges from a real case study in a public school in Calabria, Italy,
where staff members must be distributed across kindergartens, primary, and
secondary schools under constraints of availability, competencies, and
fairness. To tackle this problem, we develop an optimization model and
investigate a solution approach based on quantum annealing. Our computational
experiments on real-world data show that quantum annealing is capable of
producing balanced assignments in short runtimes. These results provide
evidence of the practical applicability of quantum optimization methods in
educational scheduling and, more broadly, in complex resource allocation tasks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [387] [FedLoDrop: Federated LoRA with Dropout for Generalized LLM Fine-tuning](https://arxiv.org/abs/2510.12078)
*Sijing Xie,Dingzhu Wen,Changsheng You,Qimei Chen,Mehdi Bennis,Kaibin Huang*

Main category: cs.IT

TL;DR: The paper introduces FedLoDrop, a framework that uses dropout techniques in Federated LoRA to enhance generalization while minimizing costs.


<details>
  <summary>Details</summary>
Motivation: The need to adapt large language models efficiently to specific tasks to improve relevance and accuracy, while addressing overfitting, underfitting, and communication challenges.

Method: FedLoDrop applies dropout to matrix rows and columns, analyzes the trade-off between errors, optimizes dropout rates and resources via B&B and P-SCA methods.

Result: FedLoDrop reduces overfitting, lowers generalization errors, improves communication efficiency, and offers a manageable computational complexity.

Conclusion: FedLoDrop is effective for enhancing model generalization and efficiency in network-constrained settings, validated by numerical results.

Abstract: Fine-tuning (FT) large language models (LLMs) is crucial for adapting
general-purpose models to specific tasks, enhancing accuracy and relevance with
minimal resources. To further enhance generalization ability while reducing
training costs, this paper proposes Federated LoRA with Dropout (FedLoDrop), a
new framework that applies dropout to the rows and columns of the trainable
matrix in Federated LoRA. A generalization error bound and convergence analysis
under sparsity regularization are obtained, which elucidate the fundamental
trade-off between underfitting and overfitting. The error bound reveals that a
higher dropout rate increases model sparsity, thereby lowering the upper bound
of pointwise hypothesis stability (PHS). While this reduces the gap between
empirical and generalization errors, it also incurs a higher empirical error,
which, together with the gap, determines the overall generalization error. On
the other hand, though dropout reduces communication costs, deploying FedLoDrop
at the network edge still faces challenges due to limited network resources. To
address this issue, an optimization problem is formulated to minimize the upper
bound of the generalization error, by jointly optimizing the dropout rate and
resource allocation subject to the latency and per-device energy consumption
constraints. To solve this problem, a branch-and-bound (B\&B)-based method is
proposed to obtain its globally optimal solution. Moreover, to reduce the high
computational complexity of the B\&B-based method, a penalized successive
convex approximation (P-SCA)-based algorithm is proposed to efficiently obtain
its high-quality suboptimal solution. Finally, numerical results demonstrate
the effectiveness of the proposed approach in mitigating overfitting and
improving the generalization capability.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [388] [GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in Virtual Reality](https://arxiv.org/abs/2510.11878)
*Anastasiya Pechko,Piotr Borycki,Joanna Waczyńska,Daniel Barczyk,Agata Szymańska,Sławomir Tadeja,Przemysław Spurek*

Main category: cs.GR

TL;DR: The paper proposes Gaussian Splatting for Virtual Environment Rendering and Scene Editing (GS-VERSE), a novel method enabling more realistic 3D content manipulation in VR by directly integrating object meshes with Gaussian Splatting representation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing VR manipulation methods that rely on simplified geometric representations and engineering-intensive processes, often resulting in lower visual fidelity and limited physical accuracy.

Method: A new method integrates 3D meshes with Gaussian Splatting representations, enabling precise and realistic deformations for intuitive 3D manipulation. The system supports existing 3D assets, simplifies workflows, and is independent of specific physics engines.

Result: Through a user study with 18 participants, the method was found statistically significantly better in physics-aware stretching manipulation and displayed consistent performance in other manipulations like twisting and shaking.

Conclusion: The method offers a robust, realistic, and intuitive approach, demonstrating potential as a superior alternative for 3D interaction and manipulation in VR environments.

Abstract: As the demand for immersive 3D content grows, the need for intuitive and
efficient interaction methods becomes paramount. Current techniques for
physically manipulating 3D content within Virtual Reality (VR) often face
significant limitations, including reliance on engineering-intensive processes
and simplified geometric representations, such as tetrahedral cages, which can
compromise visual fidelity and physical accuracy. In this paper, we introduce
\our{} (\textbf{G}aussian \textbf{S}platting for \textbf{V}irtual
\textbf{E}nvironment \textbf{R}endering and \textbf{S}cene \textbf{E}diting), a
novel method designed to overcome these challenges by directly integrating an
object's mesh with a Gaussian Splatting (GS) representation. Our approach
enables more precise surface approximation, leading to highly realistic
deformations and interactions. By leveraging existing 3D mesh assets, \our{}
facilitates seamless content reuse and simplifies the development workflow.
Moreover, our system is designed to be physics-engine-agnostic, granting
developers robust deployment flexibility. This versatile architecture delivers
a highly realistic, adaptable, and intuitive approach to interactive 3D
manipulation. We rigorously validate our method against the current
state-of-the-art technique that couples VR with GS in a comparative user study
involving 18 participants. Specifically, we demonstrate that our approach is
statistically significantly better for physics-aware stretching manipulation
and is also more consistent in other physics-based manipulations like twisting
and shaking. Further evaluation across various interactions and scenes confirms
that our method consistently delivers high and reliable performance, showing
its potential as a plausible alternative to existing methods.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [389] [Constrained Sensing and Reliable State Estimation with Shallow Recurrent Decoders on a TRIGA Mark II Reactor](https://arxiv.org/abs/2510.12368)
*Stefano Riva,Carolina Introini,Josè Nathan Kutz,Antonio Cammi*

Main category: cs.CE

TL;DR: The paper introduces Shallow Recurrent Decoder networks, a robust deep learning method for reconstructing full state estimation in engineering systems, applied specifically to the TRIGA Mark II nuclear reactor, using sparse and noisy data.


<details>
  <summary>Details</summary>
Motivation: To develop a data-driven methodology for accurate state estimation in complex systems like nuclear reactors, overcoming challenges such as sparse measurements, noisy data, and discrepancies between model and real-world data.

Method: Shallow Recurrent Decoder networks are implemented to map sparse sensor data to full state estimations using both synthetic and experimental data, leveraging ensemble strategies and short training times without hyperparameter tuning.

Result: The architecture successfully reconstructed multiple state fields (temperature, velocity, pressure, turbulence quantities) and demonstrated correction capabilities, validating its performance with both synthetic and experimental data.

Conclusion: The proposed technique proved effective for real-time accurate state reconstruction and correction, supporting its use in reactor digital twins for monitoring and control applications.

Abstract: Shallow Recurrent Decoder networks are a novel data-driven methodology able
to provide accurate state estimation in engineering systems, such as nuclear
reactors. This deep learning architecture is a robust technique designed to map
the temporal trajectories of a few sparse measures to the full state space,
including unobservable fields, which is agnostic to sensor positions and able
to handle noisy data through an ensemble strategy, leveraging the short
training times and without the need for hyperparameter tuning. Following its
application to a novel reactor concept, this work investigates the performance
of Shallow Recurrent Decoders when applied to a real system. The underlying
model is represented by a fluid dynamics model of the TRIGA Mark II research
reactor; the architecture will use both synthetic temperature data coming from
the numerical model and leveraging experimental temperature data recorded
during a previous campaign. The objective of this work is, therefore, two-fold:
1) assessing if the architecture can reconstruct the full state of the system
(temperature, velocity, pressure, turbulence quantities) given sparse data
located in specific, low-dynamics channels and 2) assessing the correction
capabilities of the architecture (that is, given a discrepancy between model
and data, assessing if sparse measurements can provide some correction to the
architecture output). As will be shown, the accurate reconstruction of every
characteristic field, using both synthetic and experimental data, in real-time
makes this approach suitable for interpretable monitoring and control purposes
in the framework of a reactor digital twin.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [390] [Probabilistic Super-Resolution for Urban Micrometeorology via a Schrödinger Bridge](https://arxiv.org/abs/2510.12148)
*Yuki Yasuda,Ryo Onishi*

Main category: physics.ao-ph

TL;DR: This paper uses a Schrödinger bridge neural network to improve super-resolution of urban temperature data, achieving comparable accuracy at lower computational cost compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the computational inefficiency and uncertainty quantification challenges in generating high-resolution urban temperature data.

Method: The study utilizes a Schrödinger-bridge neural network model that transforms low-resolution data into high-resolution data based on diffusion processes, while contrasting its performance against denoising diffusion probabilistic models.

Result: The Schrödinger bridge model achieves comparable accuracy to diffusion models but at one-fifth the computational cost and demonstrates superior uncertainty quantification via higher variance in generated samples.

Conclusion: The reduced computational cost and advanced uncertainty quantification suggest that Schrödinger-bridge-based super-resolution can enable real-time ensemble micrometeorological predictions.

Abstract: This study employs a neural network that represents the solution to a
Schr\"odinger bridge problem to perform super-resolution of 2-m temperature in
an urban area. Schr\"odinger bridges generally describe transformations between
two data distributions based on diffusion processes. We use a specific
Schr\"odinger-bridge model (SM) that directly transforms low-resolution data
into high-resolution data, unlike denoising diffusion probabilistic models
(simply, diffusion models; DMs) that generate high-resolution data from
Gaussian noise. Low-resolution and high-resolution data were obtained from
separate numerical simulations with a physics-based model under common initial
and boundary conditions. Compared with a DM, the SM attains comparable accuracy
at one-fifth the computational cost, requiring 50 neural-network evaluations
per datum for the DM and only 10 for the SM. Furthermore, high-resolution
samples generated by the SM exhibit larger variance, implying superior
uncertainty quantification relative to the DM. Owing to the reduced
computational cost of the SM, our results suggest the feasibility of real-time
ensemble micrometeorological prediction using SM-based super-resolution.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [391] [Contrastive Dimension Reduction: A Systematic Review](https://arxiv.org/abs/2510.11847)
*Sam Hawke,Eric Zhang,Jiawen Chen,Didong Li*

Main category: stat.ME

TL;DR: This paper reviews contrastive dimension reduction (CDR) methods, addressing their role in isolating unique signals in treatment versus control groups, and proposes a unified framework and pipeline for case-control studies.


<details>
  <summary>Details</summary>
Motivation: Traditional dimension reduction techniques like PCA often fail to identify signals unique to treatment groups compared to control groups in scientific domains such as genomics, imaging, and time series.

Method: The paper categorizes CDR methods based on assumptions, objectives, and mathematical structures, suggesting a unified conceptual framework and analysis pipeline for case-control studies.

Result: The paper systematically reviews existing CDR methods, highlights their applications and challenges, and identifies avenues for future research.

Conclusion: It offers a shared framework to facilitate broader use and further development of CDR methods, promoting their application across scientific domains.

Abstract: Contrastive dimension reduction (CDR) methods aim to extract signal unique to
or enriched in a treatment (foreground) group relative to a control
(background) group. This setting arises in many scientific domains, such as
genomics, imaging, and time series analysis, where traditional dimension
reduction techniques such as Principal Component Analysis (PCA) may fail to
isolate the signal of interest. In this review, we provide a systematic
overview of existing CDR methods. We propose a pipeline for analyzing
case-control studies together with a taxonomy of CDR methods based on their
assumptions, objectives, and mathematical formulations, unifying disparate
approaches under a shared conceptual framework. We highlight key applications
and challenges in existing CDR methods, and identify open questions and future
directions. By providing a clear framework for CDR and its applications, we aim
to facilitate broader adoption and motivate further developments in this
emerging field.

</details>


### [392] [Sliding-Window Signatures for Time Series: Application to Electricity Demand Forecasting](https://arxiv.org/abs/2510.12337)
*Nina Drobac,Margaux Brégère,Joseph de Vilmarest,Olivier Wintenberger*

Main category: stat.ME

TL;DR: The paper proposes a ridge regression method using signature features on sliding windows to improve time series forecasting, capturing complex temporal patterns without hand-crafted representations.


<details>
  <summary>Details</summary>
Motivation: To address challenges in time series forecasting caused by nonlinear and delayed effects of covariates, and to find a method that does not rely on learned or manual feature representations.

Method: The approach uses ridge regression combined with signature features extracted from sliding windows to model temporal dynamics. They develop a theoretical basis (universality of approximation and stationarity properties) and an efficient sequential algorithm for computation.

Result: The method performs well with synthetic and real-world electricity demand datasets, demonstrating that signature features can encode temporal and nonlinear dependencies effectively—producing competitive forecasts compared to expert-driven approaches.

Conclusion: The framework provides an efficient, theoretically sound, and accurate methodology for time-series forecasting, particularly in situations with complex dependencies.

Abstract: Nonlinear and delayed effects of covariates often render time series
forecasting challenging. To this end, we propose a novel forecasting framework
based on ridge regression with signature features calculated on sliding
windows. These features capture complex temporal dynamics without relying on
learned or hand-crafted representations. Focusing on the discrete-time setting,
we establish theoretical guarantees, namely universality of approximation and
stationarity of signatures. We introduce an efficient sequential algorithm for
computing signatures on sliding windows. The method is evaluated on both
synthetic and real electricity demand data. Results show that signature
features effectively encode temporal and nonlinear dependencies, yielding
accurate forecasts competitive with those based on expert knowledge.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [393] [Zero-Shot Large Language Model Agents for Fully Automated Radiotherapy Treatment Planning](https://arxiv.org/abs/2510.11754)
*Dongrong Yang,Xin Wu,Yibo Xie,Xinyi Li,Qiuwen Wu,Jackie Wu,Yang Sheng*

Main category: physics.med-ph

TL;DR: This study uses a large language model (LLM)-based agent for automated intensity-modulated radiation therapy (IMRT) treatment planning, showing feasibility and clinical comparability to manual planning.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the unsustainability of manual radiation therapy planning processes due to increasing cancer cases and the need for automation.

Method: An LLM-based agent interacts with clinical treatment planning systems in a zero-shot setting, proposing dynamic constraints for inverse optimization without fine-tuning or prior training.

Result: LLM-generated treatment plans demonstrated comparable organ-at-risk sparing, improved hot spot control, and superior conformity compared to manual plans over 20 head-and-neck cancer cases.

Conclusion: The study showcases the potential of zero-shot LLM-driven workflows to standardize IMRT planning while enhancing clinical outcomes and supporting AI adoption in treatment planning.

Abstract: Radiation therapy treatment planning is an iterative, expertise-dependent
process, and the growing burden of cancer cases has made reliance on manual
planning increasingly unsustainable, underscoring the need for automation. In
this study, we propose a workflow that leverages a large language model
(LLM)-based agent to navigate inverse treatment planning for
intensity-modulated radiation therapy (IMRT). The LLM agent was implemented to
directly interact with a clinical treatment planning system (TPS) to
iteratively extract intermediate plan states and propose new constraint values
to guide inverse optimization. The agent's decision-making process is informed
by current observations and previous optimization attempts and evaluations,
allowing for dynamic strategy refinement. The planning process was performed in
a zero-shot inference setting, where the LLM operated without prior exposure to
manually generated treatment plans and was utilized without any fine-tuning or
task-specific training. The LLM-generated plans were evaluated on twenty
head-and-neck cancer cases against clinical manual plans, with key dosimetric
endpoints analyzed and reported. The LLM-generated plans achieved comparable
organ-at-risk (OAR) sparing relative to clinical plans while demonstrating
improved hot spot control (Dmax: 106.5% vs. 108.8%) and superior conformity
(conformity index: 1.18 vs. 1.39 for boost PTV; 1.82 vs. 1.88 for primary PTV).
This study demonstrates the feasibility of a zero-shot, LLM-driven workflow for
automated IMRT treatment planning in a commercial TPS. The proposed approach
provides a generalizable and clinically applicable solution that could reduce
planning variability and support broader adoption of AI-based planning
strategies.

</details>


### [394] [Artificial intelligence for simplified patient-centered dosimetry in radiopharmaceutical therapies](https://arxiv.org/abs/2510.12714)
*Alejandro Lopez-Montes,Fereshteh Yousefirizi,Yizhou Chen,Yazdan Salimi,Robert Seifert,Ali Afshar-Oromieh,Carlos Uribe,Axel Rominger,Habib Zaidi,Arman Rahmim,Kuangyu Shi*

Main category: physics.med-ph

TL;DR: The abstract highlights the use of Artificial Intelligence (AI) to enhance dosimetry calculations in radiopharmaceutical therapy (RPT), aiming for personalized approaches.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in current dosimetry calculations and improve patient-centered care in RPT by leveraging AI.

Method: AI-based advancements are reviewed and discussed in the context of simplifying and optimizing dosimetry processes for RPT.

Result: AI has the potential to overcome limitations in dosimetry calculations and offers pathways toward simplified, patient-friendly solutions in RPT.

Conclusion: AI could play a crucial role in advancing personalized dosimetry in RPT, with future opportunities for optimizing therapy outcomes.

Abstract: KEY WORDS: Artificial Intelligence (AI), Theranostics, Dosimetry,
Radiopharmaceutical Therapy (RPT), Patient-friendly dosimetry KEY POINTS - The
rapid evolution of radiopharmaceutical therapy (RPT) highlights the growing
need for personalized and patient-centered dosimetry. - Artificial Intelligence
(AI) offers solutions to the key limitations in current dosimetry calculations.
- The main advances on AI for simplified dosimetry toward patient-friendly RPT
are reviewed. - Future directions on the role of AI in RPT dosimetry are
discussed.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [395] [Simple Projection Variants Improve ColBERT Performance](https://arxiv.org/abs/2510.12327)
*Benjamin Clavié,Sean Lee,Rikiya Takehi,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: Replacing the single-layer linear projection in ColBERT models with more advanced feedforward network structures improves retrieval performance.


<details>
  <summary>Details</summary>
Motivation: To examine and overcome limitations in single-layer linear projections used in ColBERT for multi-vector dense retrieval.

Method: Alternative projection blocks, including deeper non-linear FFN blocks, GLU blocks, and skip-connections, were systematically designed, evaluated, and studied through ablation experiments.

Result: Best-performing projection variants increased retrieval quality by over 2 NDCG@10 points across diverse benchmarks.

Conclusion: Replacing ColBERT’s linear layer with advanced projections yields robust improvements and serves as a reliable enhancement across settings.

Abstract: Multi-vector dense retrieval methods like ColBERT systematically use a
single-layer linear projection to reduce the dimensionality of individual
vectors. In this study, we explore the implications of the MaxSim operator on
the gradient flows of the training of multi-vector models and show that such a
simple linear projection has inherent, if non-critical, limitations in this
setting. We then discuss the theoretical improvements that could result from
replacing this single-layer projection with well-studied alternative
feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU
blocks, and skip-connections, could alleviate these limitations. Through the
design and systematic evaluation of alternate projection blocks, we show that
better-designed final projections positively impact the downstream performance
of ColBERT models. We highlight that many projection variants outperform the
original linear projections, with the best-performing variants increasing
average performance on a range of retrieval benchmarks across domains by over 2
NDCG@10 points. We then conduct further exploration on the individual
parameters of these projections block in order to understand what drives this
empirical performance, highlighting the particular importance of upscaled
intermediate projections and residual connections. As part of these ablation
studies, we show that numerous suboptimal projection variants still outperform
the traditional single-layer projection across multiple benchmarks, confirming
our hypothesis. Finally, we observe that this effect is consistent across
random seeds, further confirming that replacing the linear layer of ColBERT
models is a robust, drop-in upgrade.

</details>


### [396] [The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12668)
*Minghao Tang,Shiyu Ni,Jingtong Wu,Zengxin Han,Keping Bi*

Main category: cs.IR

TL;DR: This paper investigates parametric retrieval-augmented generation (PRAG), a technique where documents are injected into language model parameters, finding it efficient but limited alone, and proposing joint use with textual documents for better performance.


<details>
  <summary>Details</summary>
Motivation: To explore the mechanism behind parametric injection in PRAG and evaluate its efficiency and effectiveness compared to traditional text-level interactions.

Method: The study systematically examines PRAG, analyzing how parameterized documents capture semantic information and their performance compared to text-level interactions.

Result: Parameterized documents alone provide partial semantic capture, yielding inferior performance. However, combining them with textual documents enhances information retrieval, model robustness, and overall performance.

Conclusion: The paper concludes that PRAG benefits from combining parameterized and textual documents, and suggests increasing parametric representations' information content to improve future applications.

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving external documents. As an emerging form of RAG, parametric
retrieval-augmented generation (PRAG) encodes documents as model parameters
(i.e., LoRA modules) and injects these representations into the model during
inference, enabling interaction between the LLM and documents at parametric
level. Compared with directly placing documents in the input context, PRAG is
more efficient and has the potential to offer deeper model-document
interaction. Despite its growing attention, the mechanism underlying parametric
injection remains poorly understood. In this work, we present a systematic
study of PRAG to clarify the role of parametric injection, showing that
parameterized documents capture only partial semantic information of documents,
and relying on them alone yields inferior performance compared to interaction
at text level. However, these parametric representations encode high-level
document information that can enhance the model's understanding of documents
within the input context. When combined parameterized documents with textual
documents, the model can leverage relevant information more effectively and
become more robust to noisy inputs, achieving better performance than either
source alone. We recommend jointly using parameterized and textual documents
and advocate for increasing the information content of parametric
representations to advance PRAG.

</details>


### [397] [SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model](https://arxiv.org/abs/2510.12709)
*Lin Lin,Jiefeng Long,Zhihe Wan,Yuchi Wang,Dingkang Yang,Shuang Yang,Yueyang Yao,Xu Chen,Zirui Guo,Shengqiang Li,Weiran Li,Hanyu Li,Yaling Mou,Yan Qiu,Haiyang Yu,Xiao Liang,Hongsheng Li,Chao Feng*

Main category: cs.IR

TL;DR: SAIL-Embedding addresses limitations in multimodal embedding models by introducing tailored training strategies and a new architecture, achieving superior results in recommendation and retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Previous multimodal embedding models faced challenges like limited modality support, unstable training mechanics, and adaptation to industrial applications.

Method: The model uses a multi-stage training approach, including content-aware and collaboration-aware training, alongside other optimizations like stochastic specialization and dataset-driven pattern matching.

Result: SAIL-Embedding achieves state-of-the-art performance in retrieval tasks and improves crucial metrics in real-world applications, such as significant Lifetime (LT) and AUC gains in the Douyin platform.

Conclusion: The proposed model overcomes prior limitations with an omni-modal approach and tailored training methods, resulting in enhanced performance and practical application in recommendation systems.

Abstract: Multimodal embedding models aim to yield informative unified representations
that empower diverse cross-modal tasks. Despite promising developments in the
evolution from CLIP-based dual-tower architectures to large vision-language
models, prior works still face unavoidable challenges in real-world
applications and business scenarios, such as the limited modality support,
unstable training mechanisms, and industrial domain gaps. In this work, we
introduce SAIL-Embedding, an omni-modal embedding foundation model that
addresses these issues through tailored training strategies and architectural
design. In the optimization procedure, we propose a multi-stage training scheme
to boost the multifaceted effectiveness of representation learning.
Specifically, the content-aware progressive training aims to enhance the
model's adaptability to diverse downstream tasks and master enriched
cross-modal proficiency. The collaboration-aware recommendation enhancement
training further adapts multimodal representations for recommendation scenarios
by distilling knowledge from sequence-to-item and ID-to-item embeddings while
mining user historical interests. Concurrently, we develop the stochastic
specialization and dataset-driven pattern matching to strengthen model training
flexibility and generalizability. Experimental results show that SAIL-Embedding
achieves SOTA performance compared to other methods in different retrieval
tasks. In online experiments across various real-world scenarios integrated
with our model, we observe a significant increase in Lifetime (LT), which is a
crucial indicator for the recommendation experience. For instance, the model
delivers the 7-day LT gain of +0.158% and the 14-day LT gain of +0.144% in the
Douyin-Selected scenario. For the Douyin feed rank model, the match features
produced by SAIL-Embedding yield a +0.08% AUC gain.

</details>


### [398] [Causal Inspired Multi Modal Recommendation](https://arxiv.org/abs/2510.12325)
*Jie Yang,Chenyang Gu,Zixuan Liu*

Main category: cs.IR

TL;DR: The paper proposes a causal framework for multimodal recommender systems to address biases like modal confounding and interaction bias, enhancing performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To mitigate critical biases in multimodal recommender systems, such as modal confounding and interaction bias, which create spurious associations and dilute genuine preferences.

Method: A causal-inspired framework introduces a dual-channel cross-modal diffusion module, back-door adjustment with hierarchical matching and vector-quantized codebooks, and front-door adjustment for causal topology reconstruction to address biases.

Result: Extensive tests on three real-world e-commerce datasets showed superior performance over state-of-the-art methods while maintaining strong interpretability.

Conclusion: The introduced causal-inspired multimodal recommendation approach effectively corrects biases, demonstrating both improved recommendation accuracy and interpretability, beneficial for e-commerce applications.

Abstract: Multimodal recommender systems enhance personalized recommendations in
e-commerce and online advertising by integrating visual, textual, and user-item
interaction data. However, existing methods often overlook two critical biases:
(i) modal confounding, where latent factors (e.g., brand style or product
category) simultaneously drive multiple modalities and influence user
preference, leading to spurious feature-preference associations; (ii)
interaction bias, where genuine user preferences are mixed with noise from
exposure effects and accidental clicks. To address these challenges, we propose
a Causal-inspired multimodal Recommendation framework. Specifically, we
introduce a dual-channel cross-modal diffusion module to identify hidden modal
confounders, utilize back-door adjustment with hierarchical matching and
vector-quantized codebooks to block confounding paths, and apply front-door
adjustment combined with causal topology reconstruction to build a deconfounded
causal subgraph. Extensive experiments on three real-world e-commerce datasets
demonstrate that our method significantly outperforms state-of-the-art
baselines while maintaining strong interpretability.

</details>


### [399] [Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval](https://arxiv.org/abs/2510.12014)
*Eric He,Akash Gupta,Adian Liusie,Vatsal Raina,Piotr Molenda,Shirom Chabra,Vyas Raina*

Main category: cs.IR

TL;DR: The paper proposes a framework that bridges vision-language models and embedding-based systems to improve personalized text-image retrieval for tasks like product recommendation.


<details>
  <summary>Details</summary>
Motivation: Current embedding-based methods like CLIP are efficient but fail to capture abstract text-image pairs, which are critical in applications such as personalized product recommendation.

Method: The proposed framework distills preference rankings from a vision-language model into an embedding-based system, combining nuanced alignment with scalability.

Result: The method significantly outperforms existing embedding-based baselines in persona-driven product recommendation tasks.

Conclusion: The approach effectively enhances text-image retrieval by coupling the alignment capability of vision-language models with the scalability of embedding-based methods, providing an efficient solution.

Abstract: Text--image retrieval is necessary for applications such as product
recommendation. Embedding-based approaches like CLIP enable efficient
large-scale retrieval via vector similarity search, but they are primarily
trained on literal caption-like text--image pairs and often fail to capture
abstract or persona-driven attributes common in product recommendation
applications (e.g., ``a gift for a mother who loves gardening''). In contrast,
state-of-the-art vision--language models (vLLMs) can align text with images in
a flexible manner, but their limited context window prevents them from directly
handling retrieval over large catalogs. We propose a framework that distills
the preference rankings of a powerful vLLM into an embedding-based system,
transferring its nuanced alignment abilities while maintaining the
inference-time scalability of an embedding-based approach. Experiments on
persona-driven product recommendation tasks demonstrate that our method
significantly outperforms existing embedding-based baselines, providing an
efficient solution for personalized text--image retrieval.

</details>


### [400] [MIARec: Mutual-influence-aware Heterogeneous Network Embedding for Scientific Paper Recommendation](https://arxiv.org/abs/2510.12054)
*Wenjin Xie,Tao Jia*

Main category: cs.IR

TL;DR: This study introduces the MIARec model, which enhances paper recommendations by improving graph representation learning with mutual academic influence and multi-channel aggregation.


<details>
  <summary>Details</summary>
Motivation: To improve mechanism for recommending scientific papers by addressing limitations in existing graph-based approaches that neglect asymmetric academic influence.

Method: The study proposes the MIARec model using a gravity-based approach to measure mutual academic influence and multi-channel aggregation for comprehensive graph representation learning.

Result: Experiments showed that MIARec outperforms baseline models in scientific paper recommendation tasks across three main metrics.

Conclusion: Incorporating mutual academic influence and multi-channel aggregation leads to a more effective scholarly network representation, improving recommendation accuracy.

Abstract: With the rapid expansion of scientific literature, scholars increasingly
demand precise and high-quality paper recommendations. Among various
recommendation methodologies, graph-based approaches have garnered attention by
effectively exploiting the structural characteristics inherent in scholarly
networks. However, these methods often overlook the asymmetric academic
influence that is prevalent in scholarly networks when learning graph
representations. To address this limitation, this study proposes the
Mutual-Influence-Aware Recommendation (MIARec) model, which employs a
gravity-based approach to measure the mutual academic influence between
scholars and incorporates this influence into the feature aggregation process
during message propagation in graph representation learning. Additionally, the
model utilizes a multi-channel aggregation method to capture both individual
embeddings of distinct single relational sub-networks and their interdependent
embeddings, thereby enabling a more comprehensive understanding of the
heterogeneous scholarly network. Extensive experiments conducted on real-world
datasets demonstrate that the MIARec model outperforms baseline models across
three primary evaluation metrics, indicating its effectiveness in scientific
paper recommendation tasks.

</details>


### [401] [SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch](https://arxiv.org/abs/2510.12604)
*Qihang Zhao,Zhongbo Sun,Xiaoyang Zheng,Xian Guo,Siyuan Wang,Zihan Liang,Mingcan Peng,Ben Chen,Chenyi Lei*

Main category: cs.IR

TL;DR: This paper introduces SMILE to enhance item representation on search and recommendation platforms, addressing cold-start challenges and improving platform diversity.


<details>
  <summary>Details</summary>
Motivation: Modern platforms struggle with the cold-start problem, where insufficient collaborative data for new items exacerbates biases and limits diversity.

Method: The paper proposes SMILE, leveraging RQ-OPQ encoding to perform a two-step alignment for quantizing and enhancing item content and collaboration signals.

Result: Experiments show superior performance, with significant improvements in critical metrics such as item CTR (+1.66%), buyers (+1.57%), and order volume (+2.17%).

Conclusion: SMILE mitigates cold-start challenges, improves item representation, and enhances the diversity and effectiveness of recommendation platforms.

Abstract: With the rise of modern search and recommendation platforms, insufficient
collaborative information of cold-start items exacerbates the Matthew effect of
existing platform items, challenging platform diversity and becoming a
longstanding issue. Existing methods align items' side content with
collaborative information to transfer collaborative signals from
high-popularity items to cold-start items. However, these methods fail to
account for the asymmetry between collaboration and content, nor the
fine-grained differences among items. To address these issues, we propose
SMILE, an item representation enhancement approach based on fused alignment of
semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and
collaborative information, followed by a two-step alignment: RQ encoding
transfers shared collaborative signals across items, while OPQ encoding learns
differentiated information of items. Comprehensive offline experiments on
large-scale industrial datasets demonstrate superiority of SMILE, and rigorous
online A/B tests confirm statistically significant improvements: item CTR
+1.66%, buyers +1.57%, and order volume +2.17%.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [402] [An Empirical Study of Reducing AV1 Decoder Complexity and Energy Consumption via Encoder Parameter Tuning](https://arxiv.org/abs/2510.12380)
*Vibhoothi Vibhoothi,Julien Zouein,Shanker Shreejith,Jean-Baptiste Kempf,Anil Kokaram*

Main category: eess.IV

TL;DR: This paper addresses the decoding complexity of AV1, proposing strategies for energy-efficient video streaming by tweaking encoder configurations.


<details>
  <summary>Details</summary>
Motivation: To reduce the energy consumption and decoding complexity of AV1 video codecs for battery-constrained devices.

Method: Systematic evaluation of coding tools and parameters in two AV1 encoders (libaom-av1 and SVT-AV1) using energy measurement tools such as RAPL, Intel SoC Watch, and VTune profiler.

Result: Disabling certain tools in libaom-av1 (e.g., CDEF) reduces decoding cycles by 10%, and using fast-decode=2 preset in SVT-AV1 achieves a 24% reduction in decoding cycles.

Conclusion: Specific encoder configurations can significantly lower AV1 decoding complexity and energy consumption with minimal perceptual quality loss, aiding energy-efficient video streaming strategies.

Abstract: The widespread adoption of advanced video codecs such as AV1 is often
hindered by their high decoding complexity, posing a challenge for
battery-constrained devices. While encoders can be configured to produce
bitstreams that are decoder-friendly, estimating the decoding complexity and
energy overhead for a given video is non-trivial. In this study, we
systematically analyse the impact of disabling various coding tools and
adjusting coding parameters in two AV1 encoders, libaom-av1 and SVT-AV1. Using
system-level energy measurement tools like RAPL (Running Average Power Limit),
Intel SoC Watch (integrated with VTune profiler), we quantify the resulting
trade-offs between decoding complexity, energy consumption, and compression
efficiency for decoding a bitstream. Our results demonstrate that specific
encoder configurations can substantially reduce decoding complexity with
minimal perceptual quality degradation. For libaom-av1, disabling CDEF, an
in-loop filter gives us a mean reduction in decoding cycles by 10%. For
SVT-AV1, using the in-built, fast-decode=2 preset achieves a more substantial
24% reduction in decoding cycles. These findings provide strategies for content
providers to lower the energy footprint of AV1 video streaming.

</details>


### [403] [LiteVPNet: A Lightweight Network for Video Encoding Control in Quality-Critical Applications](https://arxiv.org/abs/2510.12379)
*Vibhoothi Vibhoothi,François Pitié,Anil Kokaram*

Main category: eess.IV

TL;DR: The paper introduces LiteVPNet, a lightweight neural network to improve video transcoding for Virtual Production workflows by accurately predicting encoding parameters for quality preservation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in video production workflows that demand high-quality control and energy efficiency, which are unmet by existing transcoding methods.

Method: The proposed LiteVPNet leverages low-complexity features such as bitstream characteristics, video complexity measures, and CLIP-based semantic embeddings to predict Quantisation Parameters for video encoders.

Result: LiteVPNet achieves a mean VMAF error below 1.2 points and performs notably better than state-of-the-art methods, with over 87% accuracy in predicting quality within 2 VMAF points.

Conclusion: LiteVPNet proves to be applicable for enhancing media streaming quality and efficiency in Virtual Production workflows, offering improved precision and energy-efficient solutions.

Abstract: In the last decade, video workflows in the cinema production ecosystem have
presented new use cases for video streaming technology. These new workflows,
e.g. in On-set Virtual Production, present the challenge of requiring precise
quality control and energy efficiency. Existing approaches to transcoding often
fall short of these requirements, either due to a lack of quality control or
computational overhead. To fill this gap, we present a lightweight neural
network (LiteVPNet) for accurately predicting Quantisation Parameters for NVENC
AV1 encoders that achieve a specified VMAF score. We use low-complexity
features, including bitstream characteristics, video complexity measures, and
CLIP-based semantic embeddings. Our results demonstrate that LiteVPNet achieves
mean VMAF errors below 1.2 points across a wide range of quality targets.
Notably, LiteVPNet achieves VMAF errors within 2 points for over 87% of our
test corpus, c.f. approx 61% with state-of-the-art methods. LiteVPNet's
performance across various quality regions highlights its applicability for
enhancing high-value content transport and streaming for more energy-efficient,
high-quality media experiences.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [404] [Fast and Interpretable Protein Substructure Alignment via Optimal Transport](https://arxiv.org/abs/2510.11752)
*Zhiyu Wang,Bingxin Zhou,Jing Wang,Yang Tan,Weishu Zhao,Pietro Liò,Liang Hong*

Main category: q-bio.QM

TL;DR: The paper introduces PLASMA, a deep learning framework for residue-level protein substructure alignment using optimal transport, offering accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing computational methods struggle with identifying and comparing local protein motifs, hindering advancements in understanding protein functions and engineering.

Method: The study reformulates protein substructure alignment as a regularized optimal transport task using differentiable Sinkhorn iterations, providing interpretable alignment matrices.

Result: PLASMA achieves accurate, efficient, and interpretable alignment in quantitative evaluations and biological case studies, along with a training-free variant named PLASMA-PF.

Conclusion: PLASMA fills a gap in protein structure analysis, enabling functional annotation, evolutionary studies, and drug design, with openly accessible implementation.

Abstract: Proteins are essential biological macromolecules that execute life functions.
Local motifs within protein structures, such as active sites, are the most
critical components for linking structure to function and are key to
understanding protein evolution and enabling protein engineering. Existing
computational methods struggle to identify and compare these local structures,
which leaves a significant gap in understanding protein structures and
harnessing their functions. This study presents PLASMA, the first deep learning
framework for efficient and interpretable residue-level protein substructure
alignment. We reformulate the problem as a regularized optimal transport task
and leverage differentiable Sinkhorn iterations. For a pair of input protein
structures, PLASMA outputs a clear alignment matrix with an interpretable
overall similarity score. Through extensive quantitative evaluations and three
biological case studies, we demonstrate that PLASMA achieves accurate,
lightweight, and interpretable residue-level alignment. Additionally, we
introduce PLASMA-PF, a training-free variant that provides a practical
alternative when training data are unavailable. Our method addresses a critical
gap in protein structure analysis tools and offers new opportunities for
functional annotation, evolutionary studies, and structure-based drug design.
Reproducibility is ensured via our official implementation at
https://github.com/ZW471/PLASMA-Protein-Local-Alignment.git.

</details>


### [405] [scPPDM: A Diffusion Model for Single-Cell Drug-Response Prediction](https://arxiv.org/abs/2510.11726)
*Zhaokang Liang,Shuyang Zhuang,Xiaoran Jiao,Weian Mao,Hao Chen,Chunhua Shen*

Main category: q-bio.QM

TL;DR: The paper introduces scPPDM, a novel diffusion-based framework for predicting single-cell drug responses using scRNA-seq data.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy and interpretability in single-cell drug-response predictions by addressing challenges in covariate and drug generalization.

Method: scPPDM uses a diffusion-based model with dual condition channels (pre-perturbation state and drug dose) in a shared latent space, enabling tunable dose-response predictions via classifier-free guidance.

Result: The model achieves state-of-the-art performance on the Tahoe-100M benchmark, especially in conditions with unseen covariates or drugs, with significant gains in metrics like DEG logFC-Spearman/Pearson correlations.

Conclusion: scPPDM provides a transparent and adjustable framework for drug-response predictions, facilitating precise what-if analyses while reducing experimental burden.

Abstract: This paper introduces the Single-Cell Perturbation Prediction Diffusion Model
(scPPDM), the first diffusion-based framework for single-cell drug-response
prediction from scRNA-seq data. scPPDM couples two condition channels,
pre-perturbation state and drug with dose, in a unified latent space via
non-concatenative GD-Attn. During inference, factorized classifier-free
guidance exposes two interpretable controls for state preservation and
drug-response strength and maps dose to guidance magnitude for tunable
intensity. Evaluated on the Tahoe-100M benchmark under two stringent regimes,
unseen covariate combinations (UC) and unseen drugs (UD), scPPDM sets new
state-of-the-art results across log fold-change recovery, delta correlations,
explained variance, and DE-overlap. Representative gains include
+36.11%/+34.21% on DEG logFC-Spearman/Pearson in UD over the second-best model.
This control interface enables transparent what-if analyses and dose tuning,
reducing experimental burden while preserving biological specificity.

</details>


### [406] [PRISM: Enhancing Protein Inverse Folding through Fine-Grained Retrieval on Structure-Sequence Multimodal Representations](https://arxiv.org/abs/2510.11750)
*Sazan Mahbub,Souvik Kundu,Eric P. Xing*

Main category: q-bio.QM

TL;DR: PRISM is a novel deep learning framework addressing the protein inverse folding problem with state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of protein design due to vast sequence space and structural constraints, and to leverage conserved patterns from natural proteins that existing models underexploit.

Method: A multimodal retrieval-augmented generation framework that retrieves motif representations from known proteins and integrates them via a hybrid self-cross attention decoder.

Result: Outperformed existing methods across five benchmarks in metrics like perplexity, amino acid recovery, and foldability.

Conclusion: Multimodal retrieval-based approaches like PRISM offer a powerful and efficient solution for protein sequence design.

Abstract: Designing protein sequences that fold into a target three-dimensional
structure, known as the inverse folding problem, is central to protein
engineering but remains challenging due to the vast sequence space and the
importance of local structural constraints. Existing deep learning approaches
achieve strong recovery rates, yet they lack explicit mechanisms to reuse
fine-grained structure-sequence patterns that are conserved across natural
proteins. We present PRISM, a multimodal retrieval-augmented generation
framework for inverse folding that retrieves fine-grained representations of
potential motifs from known proteins and integrates them with a hybrid
self-cross attention decoder. PRISM is formulated as a latent-variable
probabilistic model and implemented with an efficient approximation, combining
theoretical grounding with practical scalability. Across five benchmarks
(CATH-4.2, TS50, TS500, CAMEO 2022, and the PDB date split), PRISM establishes
new state of the art in both perplexity and amino acid recovery, while also
improving foldability metrics (RMSD, TM-score, pLDDT), demonstrating that
fine-grained multimodal retrieval is a powerful and efficient paradigm for
protein sequence design.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [407] [Human-in-the-Loop Bandwidth Estimation for Quality of Experience Optimization in Real-Time Video Communication](https://arxiv.org/abs/2510.12265)
*Sami Khairy,Gabriel Mittag,Vishak Gopal,Ross Cutler*

Main category: cs.MM

TL;DR: This paper proposes a data-driven, human-in-the-loop framework using offline reinforcement learning (RL) for improving bandwidth estimation in video conferencing systems, demonstrating a reduction in poor call ratios and generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: Accurately estimating time-varying bandwidth is crucial for enhancing the QoE in real-time video conferencing systems, but it remains a challenge due to evolving network architectures and complex protocols.

Method: The authors trained QoE reward models using subjective user evaluations, curated a huge dataset from real-world Microsoft Teams calls, and developed a novel offline RL algorithm for a neural network-based bandwidth estimator.

Result: The proposed approach improved QoE by reducing the subjective poor call ratio by 11.41% compared to a baseline estimator, and the RL algorithm showed generalization in D4RL benchmark tasks.

Conclusion: The framework successfully improves QoE in video conferencing and highlights the potential of offline RL algorithms beyond bandwidth estimation.

Abstract: The quality of experience (QoE) delivered by video conferencing systems is
significantly influenced by accurately estimating the time-varying available
bandwidth between the sender and receiver. Bandwidth estimation for real-time
communications remains an open challenge due to rapidly evolving network
architectures, increasingly complex protocol stacks, and the difficulty of
defining QoE metrics that reliably improve user experience. In this work, we
propose a deployed, human-in-the-loop, data-driven framework for bandwidth
estimation to address these challenges. Our approach begins with training
objective QoE reward models derived from subjective user evaluations to measure
audio and video quality in real-time video conferencing systems. Subsequently,
we collect roughly $1$M network traces with objective QoE rewards from
real-world Microsoft Teams calls to curate a bandwidth estimation training
dataset. We then introduce a novel distributional offline reinforcement
learning (RL) algorithm to train a neural-network-based bandwidth estimator
aimed at improving QoE for users. Our real-world A/B test demonstrates that the
proposed approach reduces the subjective poor call ratio by $11.41\%$ compared
to the baseline bandwidth estimator. Furthermore, the proposed offline RL
algorithm is benchmarked on D4RL tasks to demonstrate its generalization beyond
bandwidth estimation.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [408] [Ground Stratification for a Logic of Definitions with Induction](https://arxiv.org/abs/2510.12297)
*Nathan Guermond,Gopalan Nadathur*

Main category: cs.LO

TL;DR: The paper explores generalizing the Abella proof assistant's logic to include ground stratification for inductive definitions while resolving issues of inconsistency.


<details>
  <summary>Details</summary>
Motivation: The strict stratification in the Abella proof assistant's logic limits its applicability to scenarios requiring more flexible semantic equivalence reasoning, such as logical relations approach.

Method: The authors extend Tiu's concept of ground stratification to cover inductive definitions while addressing and preventing inconsistencies.

Result: Weakening stratification for inductive definitions leads to inconsistency, but the specific generalization aligns with practical uses of logical relations.

Conclusion: This work enables more flexible definitions in Abella's underlying logic, serving as a step toward broader enhancements involving co-induction and nominal abstraction.

Abstract: The logic underlying the Abella proof assistant includes mechanisms for
interpreting atomic predicates through fixed point definitions that can
additionally be treated inductively or co-inductively. However, the original
formulation of the logic includes a strict stratification condition on
definitions that is too restrictive for some applications such as those that
use a logical relations based approach to semantic equivalence. Tiu has shown
how this restriction can be eased by utilizing a weaker notion referred to as
ground stratification. Tiu's results were limited to a version of the logic
that does not treat inductive definitions. We show here that they can be
extended to cover such definitions. While our results are obtained by using
techniques that have been previously deployed in related ways in this context,
their use is sensitive to the particular way in which we generalize the logic.
In particular, although ground stratification may be used with arbitrary
fixed-point definitions, we show that weakening stratification to this form for
inductive definitions leads to inconsistency. The particular generalization we
describe accords well with the way logical relations are used in practice. Our
results are also a intermediate step to building a more flexible form for
definitions into the full logic underlying Abella, which additionally includes
co-induction, generic quantification, and a mechanism referred to as nominal
abstraction for analyzing occurrences of objects in terms that are governed by
generic quantifiers.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [409] [Rationally Analyzing Shelby: Proving Incentive Compatibility in a Decentralized Storage Network](https://arxiv.org/abs/2510.11866)
*Michael Crystal,Guy Goren,Scott Duke Kominers*

Main category: cs.GT

TL;DR: This paper provides a formal analysis of the incentive properties of Shelby, a decentralized storage network protocol, addressing a significant gap in the current research on blockchain-based storage protocols.


<details>
  <summary>Details</summary>
Motivation: Decentralized storage systems are key to the Web3 ecosystem but lack formal analysis of their incentive mechanisms, raising concerns about their effectiveness in truly decentralizing storage.

Method: The authors use a game-theoretic model to analyze Shelby's incentive properties, assessing the effectiveness of its peer audits combined with occasional on-chain verification.

Result: Shelby is shown to achieve incentive compatibility under certain conditions, and a modification is proposed to enhance its resistance to collusion.

Conclusion: Shelby's incentive mechanisms are theoretically sound with the outlined adjustments, and the paper highlights the importance of formal incentive analysis for decentralized storage systems.

Abstract: Decentralized storage is one of the most natural applications built on
blockchains and a central component of the Web3 ecosystem. Yet despite a decade
of active development -- from IPFS and Filecoin to more recent entrants -- most
of these storage protocols have received limited formal analysis of their
incentive properties. Claims of incentive compatibility are sometimes made, but
rarely proven. This gap matters: without well-designed incentives, a system may
distribute storage but fail to truly decentralize it.
  We analyze Shelby -- a storage network protocol recently proposed by Aptos
Labs and Jump Crypto -- and provide the first formal proof of its incentive
properties. Our game-theoretic model shows that while off-chain audits alone
collapse to universal shirking, Shelby's combination of peer audits with
occasional on-chain verification yields incentive compatibility under natural
parameter settings. We also examine coalition behavior and outline a simple
modification that strengthens the protocol's collusion-resilience.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [410] [Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a High-Stakes Startup Competition](https://arxiv.org/abs/2510.12692)
*Sarina Xi,Orelia Pi,Miaomiao Zhang,Becca Xiong,Jacqueline Ng Lane,Nihar B. Shah*

Main category: cs.HC

TL;DR: The paper explores the effectiveness of an AI-based algorithm for judge assignment in a venture competition, comparing it to human expert decisions.


<details>
  <summary>Details</summary>
Motivation: To determine if AI can achieve human-level judgment quality in complex decision-making scenarios like judge assignment in competitions.

Method: An AI algorithm named HLSE was developed and compared with human expert assignments using blinded quality scores for judge-venture pairs.

Result: No significant difference was found between the quality of human and algorithmic matches; however, the AI method was faster and more scalable.

Conclusion: The AI algorithm demonstrates human-level effectiveness in judge assignments while offering improved scalability and efficiency.

Abstract: There is growing interest in applying artificial intelligence (AI) to
automate and support complex decision-making tasks. However, it remains unclear
how algorithms compare to human judgment in contexts requiring semantic
understanding and domain expertise. We examine this in the context of the judge
assignment problem, matching submissions to suitably qualified judges.
Specifically, we tackled this problem at the Harvard President's Innovation
Challenge, the university's premier venture competition awarding over \$500,000
to student and alumni startups. This represents a real-world environment where
high-quality judge assignment is essential. We developed an AI-based
judge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE),
and deployed it at the competition. We then evaluated its performance against
human expert assignments using blinded match-quality scores from judges on
$309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we
found no statistically significant difference in assignment quality between the
two approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated
$3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an
excellent match. Furthermore, manual assignments that previously required a
full week could be automated in several hours by the algorithm during
deployment. These results demonstrate that HLSE achieves human-expert-level
matching quality while offering greater scalability and efficiency,
underscoring the potential of AI-driven solutions to support and enhance human
decision-making for judge assignment in high-stakes settings.

</details>


### [411] [Data-Model Co-Evolution: Growing Test Sets to Refine LLM Behavior](https://arxiv.org/abs/2510.12728)
*Minjae Lee,Minsuk Kahng*

Main category: cs.HC

TL;DR: The authors propose a system allowing users to iteratively refine Large Language Model (LLM) behavior by editing prompt instructions, aligning it with domain-specific policies through a human-in-the-loop approach.


<details>
  <summary>Details</summary>
Motivation: Machine learning has faced historically slow fine-tuning cycles and a rigid separation between data and model refinement. LLMs bridge this gap and enable dynamic interaction between data and model instructions.

Method: An interactive workflow helps users identify edge cases, clarify policies, and refine instructions by iteratively testing them against an evolving test set.

Result: A user study demonstrates that participants can refine instructions systematically and specify ambiguous policies more clearly, proving the system's effectiveness.

Conclusion: The work introduces a paradigm of data-model co-evolution, enhancing robust and responsible LLM applications through iterative human-guided development.

Abstract: A long-standing challenge in machine learning has been the rigid separation
between data work and model refinement, enforced by slow fine-tuning cycles.
The rise of Large Language Models (LLMs) overcomes this historical barrier,
allowing applications developers to instantly govern model behavior by editing
prompt instructions. This shift enables a new paradigm: data-model
co-evolution, where a living test set and a model's instructions evolve in
tandem. We operationalize this paradigm in an interactive system designed to
address the critical challenge of encoding subtle, domain-specific policies
into prompt instructions. The system's structured workflow guides people to
discover edge cases, articulate rationales for desired behavior, and
iteratively evaluate instruction revisions against a growing test set. A user
study shows our workflow helps participants refine instructions systematically
and specify ambiguous policies more concretely. This work points toward more
robust and responsible LLM applications through human-in-the-loop development
aligned with local preferences and policies.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [412] [The Living Forecast: Evolving Day-Ahead Predictions into Intraday Reality](https://arxiv.org/abs/2510.12271)
*Kutay Bölat,Peter Palensky,Simon Tindemans*

Main category: stat.AP

TL;DR: The paper introduces a Bayesian updating mechanism for transforming day-ahead probabilistic forecasts into intraday forecasts, enhancing forecast accuracy for power system operations without requiring retraining.


<details>
  <summary>Details</summary>
Motivation: To improve accuracy and efficiency of intraday forecasts for power systems as day-ahead forecasts lose relevance during real-time operations.

Method: The method uses a Bayesian mechanism to update day-ahead Gaussian mixture probabilistic forecasts into intraday versions, leveraging conditional variational autoencoder-based forecasters with measured observations.

Result: Improved forecast accuracy by up to 25% across various metrics, particularly at time steps with strong temporal correlations, using household electricity consumption and photovoltaic generation datasets.

Conclusion: The framework provides a computationally efficient, theoretically grounded solution for intraday forecasting that is suitable for real-time applications in power systems.

Abstract: Accurate intraday forecasts are essential for power system operations,
complementing day-ahead forecasts that gradually lose relevance as new
information becomes available. This paper introduces a Bayesian updating
mechanism that converts fully probabilistic day-ahead forecasts into intraday
forecasts without retraining or re-inference. The approach conditions the
Gaussian mixture output of a conditional variational autoencoder-based
forecaster on observed measurements, yielding an updated distribution for the
remaining horizon that preserves its probabilistic structure. This enables
consistent point, quantile, and ensemble forecasts while remaining
computationally efficient and suitable for real-time applications. Experiments
on household electricity consumption and photovoltaic generation datasets
demonstrate that the proposed method improves forecast accuracy up to 25%
across likelihood-, sample-, quantile-, and point-based metrics. The largest
gains occur in time steps with strong temporal correlation to observed data,
and the use of pattern dictionary-based covariance structures further enhances
performance. The results highlight a theoretically grounded framework for
intraday forecasting in modern power systems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [413] [Aixel: A Unified, Adaptive and Extensible System for AI-powered Data Analysis](https://arxiv.org/abs/2510.12642)
*Meihui Zhang,Liming Wang,Chi Zhang,Zhaojing Luo*

Main category: cs.DB

TL;DR: Aixel is a unified system that integrates data management with learning to address challenges in modern data analysis, ensuring adaptability, efficiency, and extensibility across components.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve challenges caused by fragmented systems in modern data analysis that hinder adaptability, performance, user-friendliness, and component extensibility.

Method: Aixel organizes work across four layers (application, task, model, data), offering declarative interfaces, optimized execution plans, reuse mechanisms, adaptive model management, and unified data capabilities.

Result: Aixel provides a streamlined, user-friendly system that ensures adaptive, efficient handling of AI-powered data analysis tasks through its layered design.

Conclusion: Aixel successfully addresses fragmentation issues in data analysis with a unified framework that promotes adaptability, optimization, and extensibility.

Abstract: A growing trend in modern data analysis is the integration of data management
with learning, guided by accuracy, latency, and cost requirements. In practice,
applications draw data of different formats from many sources. In the
meanwhile, the objectives and budgets change over time. Existing systems handle
these applications across databases, analysis libraries, and tuning services.
Such fragmentation leads to complex user interaction, limited adaptability,
suboptimal performance, and poor extensibility across components. To address
these challenges, we present Aixel, a unified, adaptive, and extensible system
for AI-powered data analysis. The system organizes work across four layers:
application, task, model, and data. The task layer provides a declarative
interface to capture user intent, which is parsed into an executable operator
plan. An optimizer compiles and schedules this plan to meet specified goals in
accuracy, latency, and cost. The task layer coordinates the execution of data
and model operators, with built-in support for reuse and caching to improve
efficiency. The model layer offers versioned storage for index, metadata,
tensors, and model artifacts. It supports adaptive construction, task-aligned
drift detection, and safe updates that reuse shared components. The data layer
provides unified data management capabilities, including indexing,
constraint-aware discovery, task-aligned selection, and comprehensive feature
management. With the above designed layers, Aixel delivers a user friendly,
adaptive, efficient, and extensible system.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [414] [Serial-Parallel Dual-Path Architecture for Speaking Style Recognition](https://arxiv.org/abs/2510.11732)
*Guojian Li,Qijie Shao,Zhixian Zhao,Shuiyuan Wang,Zhonghua Fu,Lei Xie*

Main category: cs.SD

TL;DR: The paper introduces a dual-path model combining acoustic and linguistic data for Speaking Style Recognition, achieving significant accuracy improvements.


<details>
  <summary>Details</summary>
Motivation: Existing methods primarily focus on linguistic data, neglecting the potential of acoustic data integration for better SSR accuracy.

Method: Proposed a dual-path architecture with sequential dependency in one path and an Acoustic-Linguistic Similarity Module for cross-modal interaction in the other.

Result: Achieved 30.3% improvement in SSR accuracy compared to baselines, with 88.4% reduction in model size.

Conclusion: Fusing acoustic and linguistic data enhances SSR accuracy, demonstrating the potential for more efficient multimodal models in this domain.

Abstract: Speaking Style Recognition (SSR) identifies a speaker's speaking style
characteristics from speech. Existing style recognition approaches primarily
rely on linguistic information, with limited integration of acoustic
information, which restricts recognition accuracy improvements. The fusion of
acoustic and linguistic modalities offers significant potential to enhance
recognition performance. In this paper, we propose a novel serial-parallel
dual-path architecture for SSR that leverages acoustic-linguistic bimodal
information. The serial path follows the ASR+STYLE serial paradigm, reflecting
a sequential temporal dependency, while the parallel path integrates our
designed Acoustic-Linguistic Similarity Module (ALSM) to facilitate cross-modal
interaction with temporal simultaneity. Compared to the existing SSR baseline
-- the OSUM model, our approach reduces parameter size by 88.4% and achieves a
30.3% improvement in SSR accuracy for eight styles on the test set.

</details>


### [415] [SeeingSounds: Learning Audio-to-Visual Alignment via Text](https://arxiv.org/abs/2510.11738)
*Simone Carnemolla,Matteo Pennisi,Chiara Russo,Simone Palazzo,Daniela Giordano,Concetto Spampinato*

Main category: cs.SD

TL;DR: SeeingSounds framework enables audio-to-image generation without paired audio-visual data, using dual alignment with language and vision models.


<details>
  <summary>Details</summary>
Motivation: To create an efficient and controllable audio-to-image generation framework inspired by human cross-modal perception, without relying on visual generative training or paired data.

Method: Audio is projected to a language semantic space through a frozen language encoder and contextually aligned with the visual domain via a vision-language model, using lightweight adapters for scalable learning.

Result: SeeingSounds demonstrated state-of-the-art performance in audio-to-visual generation tasks across zero-shot and supervised benchmarks.

Conclusion: This novel approach achieves effective audio-to-image generation, leveraging interpretability and control, while being efficient and modular for broader applications.

Abstract: We introduce SeeingSounds, a lightweight and modular framework for
audio-to-image generation that leverages the interplay between audio, language,
and vision-without requiring any paired audio-visual data or training on visual
generative models. Rather than treating audio as a substitute for text or
relying solely on audio-to-text mappings, our method performs dual alignment:
audio is projected into a semantic language space via a frozen language
encoder, and, contextually grounded into the visual domain using a
vision-language model. This approach, inspired by cognitive neuroscience,
reflects the natural cross-modal associations observed in human perception. The
model operates on frozen diffusion backbones and trains only lightweight
adapters, enabling efficient and scalable learning. Moreover, it supports
fine-grained and interpretable control through procedural text prompt
generation, where audio transformations (e.g., volume or pitch shifts)
translate into descriptive prompts (e.g., "a distant thunder") that guide
visual outputs. Extensive experiments across standard benchmarks confirm that
SeeingSounds outperforms existing methods in both zero-shot and supervised
settings, establishing a new state of the art in controllable audio-to-visual
generation.

</details>


### [416] [Audio-Guided Visual Perception for Audio-Visual Navigation](https://arxiv.org/abs/2510.11760)
*Yi Wang,Yinfeng Yu,Fuchun Sun,Liejun Wang,Wendong Zheng*

Main category: cs.SD

TL;DR: This paper introduces the AGVP framework for Audio-Visual Navigation to address poor generalization to unheard sounds and unseen environments, using cross-modal alignment to improve navigation efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods for Audio-Visual Navigation lack explicit mechanisms to align auditory signals with visual regions, causing ineffective navigation for new sound sources or scenarios.

Method: The AGVP framework employs audio self-attention to extract auditory context, which is used to guide visual attention and perform cross-modal alignment, followed by temporal modeling and policy optimization.

Result: The proposed AGVP framework enhances navigation efficiency, robustness, and cross-scenario generalization, especially against unheard sound sources.

Conclusion: AGVP successfully mitigates reliance on acoustic fingerprints by providing spatial auditory guidance, making agents more capable of navigating in novel environments.

Abstract: Audio-Visual Embodied Navigation aims to enable agents to autonomously
navigate to sound sources in unknown 3D environments using auditory cues. While
current AVN methods excel on in-distribution sound sources, they exhibit poor
cross-source generalization: navigation success rates plummet and search paths
become excessively long when agents encounter unheard sounds or unseen
environments. This limitation stems from the lack of explicit alignment
mechanisms between auditory signals and corresponding visual regions. Policies
tend to memorize spurious \enquote{acoustic fingerprint-scenario} correlations
during training, leading to blind exploration when exposed to novel sound
sources. To address this, we propose the AGVP framework, which transforms sound
from policy-memorable acoustic fingerprint cues into spatial guidance. The
framework first extracts global auditory context via audio self-attention, then
uses this context as queries to guide visual feature attention, highlighting
sound-source-related regions at the feature level. Subsequent temporal modeling
and policy optimization are then performed. This design, centered on
interpretable cross-modal alignment and region reweighting, reduces dependency
on specific acoustic fingerprints. Experimental results demonstrate that AGVP
improves both navigation efficiency and robustness while achieving superior
cross-scenario generalization on previously unheard sounds.

</details>


### [417] [UALM: Unified Audio Language Model for Understanding, Generation and Reasoning](https://arxiv.org/abs/2510.12000)
*Jinchuan Tian,Sang-gil Lee,Zhifeng Kong,Sreyan Ghosh,Arushi Goel,Chao-Han Huck Yang,Wenliang Dai,Zihan Liu,Hanrong Ye,Shinji Watanabe,Mohammad Shoeybi,Bryan Catanzaro,Rafael Valle,Wei Ping*

Main category: cs.SD

TL;DR: The paper introduces Unified Audio Language Model (UALM), unifying audio understanding, text-to-audio generation, and multimodal reasoning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of unified approaches to handle audio understanding, text-to-audio generation, and multimodal reasoning within a single framework.

Method: Developed UALM-Gen for text-to-audio tasks and UALM-Reason for multimodal reasoning, applying advanced training recipes, data blending, and inference techniques.

Result: UALM matches specialized models' performance in audio tasks and showcases cross-modal generative reasoning effectiveness.

Conclusion: UALM marks a significant step in unifying multimodal tasks, setting a foundation for complex reasoning and generation in audio research.

Abstract: Recent advances in the audio language modeling (ALM) domain tackle audio
understanding and text-to-audio generation as separate tasks. Very few studies
attempt to unify these tasks -- an essential step toward advanced multimodal
reasoning. This paper introduces U}nified Audio Language Model (UALM), which
aims to unify audio understanding, text-to-audio generation, and multimodal
reasoning in a single model. To achieve this goal, we first present UALM-Gen, a
text-to-audio language model that directly predicts audio tokens and is
comparable to state-of-the-art diffusion-based models. We then demonstrate,
using proper data blending, training recipes, and inference techniques, that
our single UALM model matches the quality of state-of-the-art specialized
models in audio understanding, text-to-audio generation, and text reasoning.
Furthermore, we present UALM-Reason, a multimodal reasoning model that utilizes
both text and audio in the intermediate thinking steps to facilitate complex
generation tasks. To our knowledge, this is the first demonstration in audio
research of cross-modal generative reasoning, with its effectiveness confirmed
by subjective evaluations.

</details>


### [418] [Content Anonymization for Privacy in Long-form Audio](https://arxiv.org/abs/2510.12780)
*Cristina Aggazzotti,Ashi Garg,Zexin Cai,Nicholas Andrews*

Main category: cs.SD

TL;DR: The paper examines a privacy challenge with long-form audio, proposing content anonymization methods that use contextual rewriting to mitigate risks of content-based attacks while preserving speech utility.


<details>
  <summary>Details</summary>
Motivation: Long-form audio poses greater privacy risks because multiple utterances allow attackers to exploit vocabulary, syntax, and turns of phrase for re-identification, even with voice disguises.

Method: The paper proposes content anonymization methods using contextual rewriting of transcripts within an ASR-TTS pipeline to eliminate speaker-specific styles while retaining meaningful speech.

Result: Results demonstrate that content-based attacks on anonymized speech are effective, but paraphrasing methods can mitigate this risk in long-form telephone conversations.

Conclusion: Paraphrasing is recommended as an effective defense against content-based attacks in long-form audio to enhance speakers’ anonymity while maintaining the utility of speech.

Abstract: Voice anonymization techniques have been found to successfully obscure a
speaker's acoustic identity in short, isolated utterances in benchmarks such as
the VoicePrivacy Challenge. In practice, however, utterances seldom occur in
isolation: long-form audio is commonplace in domains such as interviews, phone
calls, and meetings. In these cases, many utterances from the same speaker are
available, which pose a significantly greater privacy risk: given multiple
utterances from the same speaker, an attacker could exploit an individual's
vocabulary, syntax, and turns of phrase to re-identify them, even when their
voice is completely disguised. To address this risk, we propose new content
anonymization approaches. Our approach performs a contextual rewriting of the
transcripts in an ASR-TTS pipeline to eliminate speaker-specific style while
preserving meaning. We present results in a long-form telephone conversation
setting demonstrating the effectiveness of a content-based attack on
voice-anonymized speech. Then we show how the proposed content-based
anonymization methods can mitigate this risk while preserving speech utility.
Overall, we find that paraphrasing is an effective defense against
content-based attacks and recommend that stakeholders adopt this step to ensure
anonymity in long-form audio.

</details>


### [419] [TFGA-Net: Temporal-Frequency Graph Attention Network for Brain-Controlled Speaker Extraction](https://arxiv.org/abs/2510.12275)
*Youhao Si,Yuan Liao,Qiushi Han,Yuhang Yang,Rui Dai,Liya Huang*

Main category: cs.SD

TL;DR: The paper proposes a brain-controlled speaker extraction model using EEG signals and introduces an advanced model (TFGA-Net) leveraging multi-scale time-frequency features and advanced neural network architectures.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of effectively utilizing common information between EEG and speech signals in the context of EEG-driven target speaker extraction.

Method: The study proposes a method combining EEG-derived multi-scale time-frequency features, cortical topology, graph convolutional networks, self-attention mechanisms, and the MossFormer2 architecture to enhance speaker extraction.

Result: The proposed TFGA-Net model achieved superior performance on public datasets (Cocktail Party and KUL) compared to state-of-the-art methods in specific evaluation metrics.

Conclusion: TFGA-Net demonstrates effective utilization of EEG and speech features for target speaker extraction, introducing promising methodologies for improving such systems. The source code is openly available for further research.

Abstract: The rapid development of auditory attention decoding (AAD) based on
electroencephalography (EEG) signals offers the possibility EEG-driven target
speaker extraction. However, how to effectively utilize the target-speaker
common information between EEG and speech remains an unresolved problem. In
this paper, we propose a model for brain-controlled speaker extraction, which
utilizes the EEG recorded from the listener to extract the target speech. In
order to effectively extract information from EEG signals, we derive
multi-scale time--frequency features and further incorporate cortical
topological structures that are selectively engaged during the task. Moreover,
to effectively exploit the non-Euclidean structure of EEG signals and capture
their global features, the graph convolutional networks and self-attention
mechanism are used in the EEG encoder. In addition, to make full use of the
fused EEG and speech feature and preserve global context and capture speech
rhythm and prosody, we introduce MossFormer2 which combines MossFormer and
RNN-Free Recurrent as separator. Experimental results on both the public
Cocktail Party and KUL dataset in this paper show that our TFGA-Net model
significantly outper-forms the state-of-the-art method in certain objective
evaluation metrics. The source code is available at:
https://github.com/LaoDa-X/TFGA-NET.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [420] [DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation](https://arxiv.org/abs/2510.12210)
*Yakun Song,Xiaobin Zhuang,Jiawei Chen,Zhikang Niu,Guanrou Yang,Chenpeng Du,Zhuo Chen,Yuping Wang,Yuxuan Wang,Xie Chen*

Main category: eess.AS

TL;DR: This paper introduces DISTAR, a zero-shot text-to-speech (TTS) framework that operates in a discrete code space, using a combination of an autoregressive language model and masked diffusion for synthesis.


<details>
  <summary>Details</summary>
Motivation: The study aims to address brittleness under distribution shift and improve controllability in interleaved autoregressive and diffusion-based TTS models.

Method: DISTAR tightly integrates autoregressive language modeling with masked diffusion in discrete residual vector quantization (RVQ) code space. It drafts tokens with AR modeling and uses diffusion infilling for synthesis.

Result: The framework delivers robust, natural, and speaker/style-consistent audio quality with enhanced controllability. Extensive tests show that DISTAR surpasses existing systems in various metrics.

Conclusion: DISTAR effectively combines AR and diffusion methods within a discrete code space, offering controllable, high-quality TTS synthesis with improved robustness and output diversity.

Abstract: Recent attempts to interleave autoregressive (AR) sketchers with
diffusion-based refiners over continuous speech representations have shown
promise, but they remain brittle under distribution shift and offer limited
levers for controllability. We introduce DISTAR, a zero-shot text-to-speech
framework that operates entirely in a discrete residual vector quantization
(RVQ) code space and tightly couples an AR language model with a masked
diffusion model, without forced alignment or a duration predictor. Concretely,
DISTAR drafts block-level RVQ tokens with an AR language model and then
performs parallel masked-diffusion infilling conditioned on the draft to
complete the next block, yielding long-form synthesis with blockwise
parallelism while mitigating classic AR exposure bias. The discrete code space
affords explicit control at inference: DISTAR produces high-quality audio under
both greedy and sample-based decoding using classifier-free guidance, supports
trade-offs between robustness and diversity, and enables variable bit-rate and
controllable computation via RVQ layer pruning at test time. Extensive
experiments and ablations demonstrate that DISTAR surpasses state-of-the-art
zero-shot TTS systems in robustness, naturalness, and speaker/style
consistency, while maintaining rich output diversity. Audio samples are
provided on https://anonymous.4open.science/w/DiSTAR_demo.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [421] [A Unidirectionally Connected FAS Approach for 6-DOF Quadrotor Control](https://arxiv.org/abs/2510.12360)
*Weijie Ren,Haowen Liu,Guang-Ren Duan*

Main category: eess.SY

TL;DR: This paper introduces a system for improving quadrotor control through a novel framework, simplifying controller design and achieving precise tracking.


<details>
  <summary>Details</summary>
Motivation: The study aims to address limitations in state-space and the Fully Actuated System (FAS) framework for quadrotor control.

Method: The proposed UC-FAS model transforms underactuated quadrotor dynamics into a unidirectionally connected fully actuated system, simplifying the design by eliminating high-order derivative estimations.

Result: Simulations validate effective 6-DOF tracking performance.

Conclusion: The UC-FAS model integrates theoretical advancements with practical control needs, establishing a standardized approach for nonlinear quadrotor dynamics.

Abstract: This paper proposes a unidirectionally connected fully actuated system
(UC-FAS) approach for the sub-stabilization and tracking control of 6-DOF
quadrotors, tackling limitations both in state-space and FAS framework to some
extent. The framework systematically converts underactuated quadrotor dynamics
into a UC-FAS model, unifying the existing different FAS transformation ways.
By eliminating estimation of the high-order derivatives of control inputs, a
drawback of current methods, the UC-FAS model simplifies controller design and
enables direct eigenstructure assignment for closed-loop dynamics. Simulations
demonstrate precise 6-DOF tracking performance. This work bridges theoretical
FAS approach advancements with practical implementation needs, offering a
standardized paradigm for nonlinear quadrotor control.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [422] [Modeling Hypergraph Using Large Language Models](https://arxiv.org/abs/2510.11728)
*Bingqiao Gu,Jiale Zeng,Xingqin Qi,Dong Li*

Main category: cs.SI

TL;DR: The paper introduces HyperLLM, a framework leveraging large language models (LLMs) to generate real-world hypergraphs, addressing the scarcity of large-scale hypergraph datasets.


<details>
  <summary>Details</summary>
Motivation: The lack of high-quality hypergraph datasets limits the development and evaluation of hypergraph learning algorithms. The paper aims to address this by exploring LLMs’ capabilities for hypergraph generation.

Method: The authors developed HyperLLM, a generator utilizing multi-agent collaboration, prompts, and feedback mechanisms to simulate hypergraph formation, ensuring alignment with real-world patterns.

Result: HyperLLM demonstrated superior fidelity to real-world structural and temporal hypergraph patterns across diverse datasets, with minimal need for statistical priors.

Conclusion: LLM-based frameworks such as HyperLLM open a promising avenue for hypergraph modeling, tackling challenges in dataset generation effectively.

Abstract: Due to the advantages of hypergraphs in modeling high-order relationships in
complex systems, they have been applied to higher-order clustering, hypergraph
neural networks and computer vision. These applications rely heavily on access
to high-quality, large-scale real-world hypergraph data. Yet, compared to
traditional pairwise graphs, real hypergraph datasets remain scarce in both
scale and diversity. This shortage significantly limits the development and
evaluation of advanced hypergraph learning algorithms. Therefore, how to
quickly generate large-scale hypergraphs that conform to the characteristics of
real networks is a crucial task that has not received sufficient attention.
Motivated by recent advances in large language models (LLMs), particularly
their capabilities in semantic reasoning, structured generation, and simulating
human behavior, we investigate whether LLMs can facilitate hypergraph
generation from a fundamentally new perspective. We introduce HyperLLM, a novel
LLM-driven hypergraph generator that simulates the formation and evolution of
hypergraphs through a multi-agent collaboration. The framework integrates
prompts and structural feedback mechanisms to ensure that the generated
hypergraphs reflect key real-world patterns. Extensive experiments across
diverse datasets demonstrate that HyperLLM achieves superior fidelity to
structural and temporal hypergraph patterns, while requiring minimal
statistical priors. Our findings suggest that LLM-based frameworks offer a
promising new direction for hypergraph modeling.

</details>


### [423] [Celebrity Profiling on Short Urdu Text using Twitter Followers' Feed](https://arxiv.org/abs/2510.11739)
*Muhammad Hamza,Rizwan Jafar*

Main category: cs.SI

TL;DR: This paper explores profiling celebrities by analyzing Urdu tweets from followers using machine learning techniques, achieving reasonable accuracy in predicting demographics.


<details>
  <summary>Details</summary>
Motivation: The study aims to fill the gap in celebrity profiling research by focusing on low-resource languages like Urdu, which have been largely unexplored despite their vast online presence.

Method: A dataset of Urdu tweets from followers of subcontinent celebrities was collected and preprocessed. Machine learning and deep learning models, such as Logistic Regression, SVM, Random Forests, CNN, and LSTM, were trained and evaluated.

Result: The best results were achieved for gender prediction with an accuracy of 65%, while moderate results were obtained for age, profession, and fame prediction across various evaluation metrics.

Conclusion: Linguistic features from follower tweets can be successfully used with advanced AI methods to profile celebrity demographics in a low-resource language like Urdu.

Abstract: Social media has become an essential part of the digital age, serving as a
platform for communication, interaction, and information sharing. Celebrities
are among the most active users and often reveal aspects of their personal and
professional lives through online posts. Platforms such as Twitter provide an
opportunity to analyze language and behavior for understanding demographic and
social patterns. Since followers frequently share linguistic traits and
interests with the celebrities they follow, textual data from followers can be
used to predict celebrity demographics. However, most existing research in this
field has focused on English and other high-resource languages, leaving Urdu
largely unexplored.
  This study applies modern machine learning and deep learning techniques to
the problem of celebrity profiling in Urdu. A dataset of short Urdu tweets from
followers of subcontinent celebrities was collected and preprocessed. Multiple
algorithms were trained and compared, including Logistic Regression, Support
Vector Machines, Random Forests, Convolutional Neural Networks, and Long
Short-Term Memory networks. The models were evaluated using accuracy,
precision, recall, F1-score, and cumulative rank (cRank). The best performance
was achieved for gender prediction with a cRank of 0.65 and an accuracy of
0.65, followed by moderate results for age, profession, and fame prediction.
These results demonstrate that follower-based linguistic features can be
effectively leveraged using machine learning and neural approaches for
demographic prediction in Urdu, a low-resource language.

</details>


### [424] [Evolution of wartime discourse on Telegram: A comparative study of Ukrainian and Russian policymakers' communication before and after Russia's full-scale invasion of Ukraine](https://arxiv.org/abs/2510.11746)
*Mykola Makhortykh,Aytalina Kulichkina,Kateryna Maikovska*

Main category: cs.SI

TL;DR: The paper studies Telegram political communication by Ukrainian and Russian policymakers during the Russo-Ukrainian war, revealing differences in communication strategies and volume.


<details>
  <summary>Details</summary>
Motivation: To understand how policymakers adapt their communication strategies during wartime, specifically on Telegram in the Russo-Ukrainian war context.

Method: The study uses a unique dataset of Telegram posts (2019-2024) from Ukrainian and Russian policymakers, focusing on changes in communication volume, thematic content, and actor engagement post-invasion.

Result: Russian leaders avoided war discussions, focusing on other topics, while Ukrainian policymakers initially emphasized war but talked less about it over time. Different strategies emerged between large vs. small parties and individual policymakers.

Conclusion: Policymakers adjust their online communication tactics to cope with wartime challenges, affecting online political discourse dynamics.

Abstract: This study examines elite-driven political communication on Telegram during
the ongoing Russo-Ukrainian war, the first large-scale European war in the
social media era. Using a unique dataset of Telegram public posts from
Ukrainian and Russian policymakers (2019-2024), we analyze changes in
communication volume, thematic content, and actor engagement following Russia's
2022 full-scale invasion. Our findings show a sharp increase in Telegram
activity after the invasion, particularly among ruling-party policymakers.
Ukrainian policymakers initially focused on war-related topics, but this
emphasis declined over time In contrast, Russian policymakers largely avoided
war-related discussions, instead emphasizing unrelated topics, such as Western
crises, to distract public attention. We also identify differences in
communication strategies between large and small parties, as well as individual
policymakers. Our findings shed light on how policymakers adapt to wartime
communication challenges and offer critical insights into the dynamics of
online political discourse during times of war.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [425] [Tensor Completion via Monotone Inclusion: Generalized Low-Rank Priors Meet Deep Denoisers](https://arxiv.org/abs/2510.12425)
*Peng Chen,Deliang Wei,Jiale Yao,Fang Li*

Main category: math.OC

TL;DR: The study introduces a tensor completion framework (GTCTV DPC) that combines generalized low rank priors with deep pseudo-contractive denoisers, achieving superior performance in handling missing tensor data.


<details>
  <summary>Details</summary>
Motivation: Missing entries in multi-dimensional data create difficulties for downstream analysis in various applications. Current tensor-based methods rely on empirical assumptions, which may not always hold.

Method: The proposed framework employs a monotone inclusion paradigm with a combination of generalized low rank priors and deep pseudo-contractive denoisers. It uses the Davis-Yin splitting scheme to ensure global convergence.

Result: The GTCTV DPC algorithm is experimentally validated, showing improved performance (better quantitative metrics and visual quality) compared to existing methods, especially at low sampling rates.

Conclusion: GTCTV DPC provides a robust solution for tensor completion and addresses limitations of prior approaches by backing its methodology with rigorous theoretical convergence guarantees.

Abstract: Missing entries in multi dimensional data pose significant challenges for
downstream analysis across diverse real world applications. These data are
naturally modeled as tensors, and recent completion methods integrating global
low rank priors with plug and play denoisers have demonstrated strong empirical
performance. However, these approaches often rely on empirical convergence
alone or unrealistic assumptions, such as deep denoisers acting as proximal
operators of implicit regularizers, which generally does not hold. To address
these limitations, we propose a novel tensor completion framework grounded in
the monotone inclusion paradigm, which unifies generalized low rank priors with
deep pseudo contractive denoisers and extends beyond traditional convex
optimization. Building on the Davis Yin splitting scheme, we develop the GTCTV
DPC algorithm and rigorously establish its global convergence. Extensive
experiments demonstrate that GTCTV DPC consistently outperforms existing
methods in both quantitative metrics and visual quality, particularly at low
sampling rates.

</details>


### [426] [Learning Mean-Field Games through Mean-Field Actor-Critic Flow](https://arxiv.org/abs/2510.12180)
*Mo Zhou,Haosheng Zhou,Ruimeng Hu*

Main category: math.OC

TL;DR: The paper proposes the Mean-Field Actor-Critic (MFAC) flow, a novel gradient-based methodology for solving mean-field games (MFGs), integrating reinforcement learning and optimal transport techniques, and demonstrates its convergence and effectiveness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving mean-field games (MFGs), this paper aims to combine reinforcement learning and optimal transport approaches to develop a robust algorithmic framework for better computation of equilibria.

Method: The proposed MFAC framework evolves the actor, critic, and distribution through coupled updates guided by partial differential equations (PDEs), and it uses the Optimal Transport Geodesic Picard (OTGP) flow to optimize the distribution along Wasserstein-2 geodesics.

Result: The paper establishes a rigorous convergence analysis using Lyapunov functionals, achieving global exponential convergence of the MFAC flow, and supports the theoretical insights with relevant numerical experiments.

Conclusion: The MFAC framework not only provides theoretical guarantees for its convergence and interplay among components but also proves practically effective for computing equilibria in mean-field games (MFGs).

Abstract: We propose the Mean-Field Actor-Critic (MFAC) flow, a continuous-time
learning dynamics for solving mean-field games (MFGs), combining techniques
from reinforcement learning and optimal transport. The MFAC framework jointly
evolves the control (actor), value function (critic), and distribution
components through coupled gradient-based updates governed by partial
differential equations (PDEs). A central innovation is the Optimal Transport
Geodesic Picard (OTGP) flow, which drives the distribution toward equilibrium
along Wasserstein-2 geodesics. We conduct a rigorous convergence analysis using
Lyapunov functionals and establish global exponential convergence of the MFAC
flow under a suitable timescale. Our results highlight the algorithmic
interplay among actor, critic, and distribution components. Numerical
experiments illustrate the theoretical findings and demonstrate the
effectiveness of the MFAC framework in computing MFG equilibria.

</details>


### [427] [A Gradient Guided Diffusion Framework for Chance Constrained Programming](https://arxiv.org/abs/2510.12238)
*Boyang Zhang,Zhiguo Wang,Ya-Feng Liu*

Main category: math.OC

TL;DR: GGDOpt presents a novel framework for solving optimization problems under uncertainty using gradient-guided diffusion methods, yielding better solutions and stability.


<details>
  <summary>Details</summary>
Motivation: To tackle optimization problems involving uncertainty and complex nonconvex constraints, ensuring robustness without requiring exact knowledge of distributions.

Method: GGDOpt reformulates CCP using sampling techniques over combined distributions, utilizes forward diffusion processes for convexification, reverse sampling, and guarantees convergence and error bounds.

Result: Experimental results show GGDOpt achieves higher solution quality, better stability, and reduces overhead by nearly 80% compared to existing methods.

Conclusion: GGDOpt is a powerful and efficient solution to CCP problems under uncertainty, demonstrating theoretical and practical advantages in optimization applications.

Abstract: Chance constrained programming (CCP) is a powerful framework for addressing
optimization problems under uncertainty. In this paper, we introduce a novel
Gradient-Guided Diffusion-based Optimization framework, termed GGDOpt, which
tackles CCP through three key innovations. First, GGDOpt accommodates a broad
class of CCP problems without requiring the knowledge of the exact distribution
of uncertainty-relying solely on a set of samples. Second, to address the
nonconvexity of the chance constraints, it reformulates the CCP as a sampling
problem over the product of two distributions: an unknown data distribution
supported on a nonconvex set and a Boltzmann distribution defined by the
objective function, which fully leverages both first- and second-order gradient
information. Third, GGDOpt has theoretical convergence guarantees and provides
practical error bounds under mild assumptions. By progressively injecting noise
during the forward diffusion process to convexify the nonconvex feasible
region, GGDOpt enables guided reverse sampling to generate asymptotically
optimal solutions. Experimental results on synthetic datasets and a waveform
design task in wireless communications demonstrate that GGDOpt outperforms
existing methods in both solution quality and stability with nearly 80%
overhead reduction.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [428] [Phenome-Wide Multi-Omics Integration Uncovers Distinct Archetypes of Human Aging](https://arxiv.org/abs/2510.12384)
*Huifa Li,Feilong Tang,Haochen Xue,Yulong Li,Xinlin Zhuang,Bin Zhang,Eran Segal,Imran Razzak*

Main category: q-bio.GN

TL;DR: This paper builds a multi-omics aging clock using machine learning and extensive datasets to robustly predict health outcomes, reveal diverse aging patterns, and enable personalized health strategies.


<details>
  <summary>Details</summary>
Motivation: Aging varies across individuals, and developing biological age indicators is crucial for proactive health monitoring. Previous single-omics approaches fail to capture the complexity of aging.

Method: Utilizing a large cohort with multi-omics datasets (transcriptomics, lipidomics, metabolomics, microbiome), advanced machine learning models were applied to develop an integrated aging clock and cluster biological aging subtypes.

Result: The multi-omics aging clock accurately predicts health outcomes and disease risks, while clustering illuminates distinct biological aging subtypes and their molecular pathways.

Conclusion: Multi-omics integration offers powerful insights into aging, underlining its potential for personalized health and strategies for age-related disease prevention.

Abstract: Aging is a highly complex and heterogeneous process that progresses at
different rates across individuals, making biological age (BA) a more accurate
indicator of physiological decline than chronological age. While previous
studies have built aging clocks using single-omics data, they often fail to
capture the full molecular complexity of human aging. In this work, we
leveraged the Human Phenotype Project, a large-scale cohort of 12,000 adults
aged 30--70 years, with extensive longitudinal profiling that includes
clinical, behavioral, environmental, and multi-omics datasets -- spanning
transcriptomics, lipidomics, metabolomics, and the microbiome. By employing
advanced machine learning frameworks capable of modeling nonlinear biological
dynamics, we developed and rigorously validated a multi-omics aging clock that
robustly predicts diverse health outcomes and future disease risk. Unsupervised
clustering of the integrated molecular profiles from multi-omics uncovered
distinct biological subtypes of aging, revealing striking heterogeneity in
aging trajectories and pinpointing pathway-specific alterations associated with
different aging patterns. These findings demonstrate the power of multi-omics
integration to decode the molecular landscape of aging and lay the groundwork
for personalized healthspan monitoring and precision strategies to prevent
age-related diseases.

</details>


### [429] [Same model, better performance: the impact of shuffling on DNA Language Models benchmarking](https://arxiv.org/abs/2510.12617)
*Davide Greco,Konrad Rawlik*

Main category: q-bio.GN

TL;DR: This paper discusses the creation of BEND, a benchmark for DNA Language Models. It highlights performance variations caused by hardware-dependent data shuffling and provides a solution by pre-shuffling data.


<details>
  <summary>Details</summary>
Motivation: To address the lack of standardized benchmarks for evaluating DNA Language Models and to study how standard ML practices interact with domain-specific characteristics in genomics.

Method: Experiments with three DNA LMs (HyenaDNA, DNABERT-2, ResNet-LM) to understand the impact of hardware-dependent hyperparameters and testing a pre-shuffling solution.

Result: Identified 4% performance variations caused by data loading workers and buffer sizes due to inadequate shuffling. These artifacts impacted absolute performance and model rankings.

Conclusion: Pre-shuffling data before storage resolves hardware dependencies and ensures benchmark validity, emphasizing the need for careful benchmark design in specialized domains.

Abstract: Large Language Models are increasingly popular in genomics due to their
potential to decode complex biological sequences. Hence, researchers require a
standardized benchmark to evaluate DNA Language Models (DNA LMs) capabilities.
However, evaluating DNA LMs is a complex task that intersects genomic's
domain-specific challenges and machine learning methodologies, where seemingly
minor implementation details can significantly compromise benchmark validity.
We demonstrate this through BEND (Benchmarking DNA Language Models), where
hardware-dependent hyperparameters -- number of data loading workers and buffer
sizes -- create spurious performance variations of up to 4% for identical
models. The problem stems from inadequate data shuffling interacting with
domain specific data characteristics. Experiments with three DNA language
models (HyenaDNA, DNABERT-2, ResNet-LM) show these artifacts affect both
absolute performance and relative model rankings. We propose a simple solution:
pre-shuffling data before storage eliminates hardware dependencies while
maintaining efficiency. This work highlights how standard ML practices can
interact unexpectedly with domain-specific data characteristics, with broader
implications for benchmark design in specialized domains.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [430] [Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need](https://arxiv.org/abs/2510.11734)
*Yuqi Bai,Tianyu Huang,Kun Sun,Yuting Chen*

Main category: cs.CY

TL;DR: The study explores using large language models (LLMs) for social experiments, focusing on their ability to simulate human personalities and evaluates their performance through a systematic framework.


<details>
  <summary>Details</summary>
Motivation: To understand and evaluate the capability of LLMs in simulating human-like personalities for social science experiments, addressing their potential and limitations.

Method: The paper introduces an evaluation framework involving individual-level and population-level analyses, modifies traditional psychometric approaches, and highlights factors influencing simulation quality.

Result: The research demonstrated the significance of persona detail in LLM personality simulation quality and uncovered scaling laws and marginal utility effects of persona profiles.

Conclusion: LLMs show potential in social science experiments with proposed evaluation metrics and theoretical foundations, but their ability to emulate human personalities is still in developmental stages.

Abstract: This research focuses on using large language models (LLMs) to simulate
social experiments, exploring their ability to emulate human personality in
virtual persona role-playing. The research develops an end-to-end evaluation
framework, including individual-level analysis of stability and
identifiability, as well as population-level analysis called progressive
personality curves to examine the veracity and consistency of LLMs in
simulating human personality. Methodologically, this research proposes
important modifications to traditional psychometric approaches (CFA and
construct validity) which are unable to capture improvement trends in LLMs at
their current low-level simulation, potentially leading to remature rejection
or methodological misalignment. The main contributions of this research are:
proposing a systematic framework for LLM virtual personality evaluation;
empirically demonstrating the critical role of persona detail in personality
simulation quality; and identifying marginal utility effects of persona
profiles, especially a Scaling Law in LLM personality simulation, offering
operational evaluation metrics and a theoretical foundation for applying large
language models in social science experiments.

</details>


### [431] [Artificial Intelligence for Optimal Learning: A Comparative Approach towards AI-Enhanced Learning Environments](https://arxiv.org/abs/2510.11755)
*Ananth Hariharan*

Main category: cs.CY

TL;DR: This paper evaluates traditional education, technology-enhanced methods, and AI-driven learning to create a hybrid educational approach for equitable, effective learning.


<details>
  <summary>Details</summary>
Motivation: To address how different educational methods and technologies influence learning outcomes, engagement, and inclusivity and to design a holistic framework for education.

Method: A comparative study of traditional education, non-AI technology-enhanced learning, and AI-driven education to assess their unique contributions and limitations.

Result: The study identifies the strengths and challenges of each learning model and suggests combining their benefits for an improved hybrid educational system.

Conclusion: Integrating traditional, non-AI, and AI-driven educational methods can create a more inclusive, effective, and adaptable learning environment that serves diverse student needs.

Abstract: In the rapidly evolving educational landscape, the integration of technology
has shifted from an enhancement to a cornerstone of educational strategy
worldwide. This transition is propelled by advancements in digital technology,
especially the emergence of artificial intelligence as a crucial tool in
learning environments. This research project critically evaluates the impact of
three distinct educational settings: traditional educational methods without
technological integration, those enhanced by non-AI technology, and those
utilising AI-driven technologies. This comparison aims to assess how each
environment influences educational outcomes, engagement, pedagogical methods,
and equity in access to learning resources, and how each contributes uniquely
to the learning experience. The ultimate goal of this research is to synthesise
the strengths of each model to create a more holistic educational approach. By
integrating the personal interaction and tested pedagogical techniques of
traditional classrooms, the enhanced accessibility and collaborative tools
offered by non-AI technology, and the personalised, adaptive learning
strategies enabled by AI-driven technologies, education systems can develop
richer, more effective learning environments. This hybrid approach aims to
leverage the best elements of each setting, thereby enhancing educational
outcomes, engagement, and inclusiveness, while also addressing the distinct
challenges and limitations inherent in each model. The intention is to create
an educational framework deeply attentive to the diverse needs of students,
ensuring equitable access to high-quality education for all.

</details>


### [432] [The Adoption Paradox: A Comparative Analysis of Veterinary AI Adoption in China and the North America](https://arxiv.org/abs/2510.11758)
*Shumin Li,Xiaoyun Lai*

Main category: cs.CY

TL;DR: The study explores how AI is adopted in veterinary fields in China and North America, finding Chinese practitioners focus on clinical use despite low familiarity, while North Americans prioritize administrative tasks with high familiarity but lower adoption.


<details>
  <summary>Details</summary>
Motivation: To examine differences in AI adoption patterns in veterinary practice between China and North America, influenced by regional market and demographic factors.

Method: A descriptive, cross-sectional survey was conducted on 455 veterinary professionals in China and compared with previously published data from a 2024 North American survey of 3,968 participants.

Result: The Chinese cohort showed high AI adoption (71.0%) despite low familiarity, focusing on clinical tasks. The North American cohort had high familiarity but lower adoption (39.2%), focusing on administrative tasks. Concerns about AI reliability were a common barrier in both regions.

Conclusion: AI integration in veterinary practice requires tailored strategies specific to regional needs, as one-size-fits-all approaches fail to address differing market motivations and adoption behaviors effectively.

Abstract: This study compares the perception, adoption, and application of artificial
intelligence (AI) among veterinary professionals in China and North America
(NA), testing the hypothesis that adoption patterns are shaped by regional
market and demographic factors. A descriptive, cross-sectional survey was
conducted with 455 veterinary professionals in China between May and July 2025.
The results were compared with published data from a 2024 survey of 3,968
veterinary professionals in the United States and Canada. The Chinese cohort,
primarily composed of clinicians (81.5%), showed a high AI adoption rate
(71.0%) despite low familiarity (55.4%). Their AI use was focused on clinical
tasks, such as disease diagnosis (50.1%) and prescription calculation (44.8%).
In contrast, the NA cohort reported high familiarity (83.8%) but a lower
adoption rate (39.2%). Their priorities were administrative, including imaging
analysis (39.0%) and record-keeping (39.0%). Concerns about AI reliability and
accuracy were the top barrier in both groups. Our findings reveal an "adoption
paradox" where the Chinese market demonstrates a practitioner-driven, bottom-up
adoption model focused on augmenting clinical efficacy, while the NA market
shows a more cautious, structured, top-down integration aimed at improving
administrative efficiency. This suggests that a one-size-fits-all approach to
AI development and integration is insufficient, and tailored, region-specific
strategies are necessary to responsibly incorporate AI into global veterinary
practice.

</details>


### [433] [From Delegates to Trustees: How Optimizing for Long-Term Interests Shapes Bias and Alignment in LLM](https://arxiv.org/abs/2510.12689)
*Suyash Fulay,Jocelyn Zhu,Michiel Bakker*

Main category: cs.CY

TL;DR: The paper explores the trade-offs between designing AI systems as delegates, reflecting user preferences, versus trustees, exercising judgment for long-term welfare, in the context of responding to policy preferences.


<details>
  <summary>Details</summary>
Motivation: To investigate whether AI models should prioritize mirroring user preferences or promoting long-term welfare in human representation, given their influence in domains like policy decision-making.

Method: Conducting experiments using a temporal utility framework to simulate votes on U.S. policy issues, comparing delegate-style (behavior cloning) and trustee-style (long-term interest weighing) AI models.

Result: Trustee models aligned better with expert consensus on clear policy issues but displayed biases on ambiguous topics. Delegate models preserved user autonomy but could deviate from well-supported policy positions.

Conclusion: Designing AI systems requires balancing user autonomy with welfare promotion; delegate models uphold expressed preferences while trustee models may better serve long-term interests at the risk of paternalism and bias.

Abstract: Large language models (LLMs) have shown promising accuracy in predicting
survey responses and policy preferences, which has increased interest in their
potential to represent human interests in various domains. Most existing
research has focused on behavioral cloning, effectively evaluating how well
models reproduce individuals' expressed preferences. Drawing on theories of
political representation, we highlight an underexplored design trade-off:
whether AI systems should act as delegates, mirroring expressed preferences, or
as trustees, exercising judgment about what best serves an individual's
interests. This trade-off is closely related to issues of LLM sycophancy, where
models can encourage behavior or validate beliefs that may be aligned with a
user's short-term preferences, but is detrimental to their long-term interests.
Through a series of experiments simulating votes on various policy issues in
the U.S. context, we apply a temporal utility framework that weighs short and
long-term interests (simulating a trustee role) and compare voting outcomes to
behavior-cloning models (simulating a delegate). We find that trustee-style
predictions weighted toward long-term interests produce policy decisions that
align more closely with expert consensus on well-understood issues, but also
show greater bias toward models' default stances on topics lacking clear
agreement. These findings reveal a fundamental trade-off in designing AI
systems to represent human interests. Delegate models better preserve user
autonomy but may diverge from well-supported policy positions, while trustee
models can promote welfare on well-understood issues yet risk paternalism and
bias on subjective topics.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [434] [Wavefront Coding for Accommodation-Invariant Near-Eye Displays](https://arxiv.org/abs/2510.12778)
*Ugur Akpinar,Erdem Sahin,Tina M. Hayward,Apratim Majumder,Rajesh Menon,Atanas Gotchev*

Main category: physics.optics

TL;DR: The paper introduces a near-eye display method that resolves the vergence-accommodation conflict using wavefront coding optics and neural networks, overcoming perceptual challenges and demonstrating efficacy up to four diopters.


<details>
  <summary>Details</summary>
Motivation: The study seeks to address the vergence-accommodation conflict problem in stereoscopic displays, which is a key challenge in achieving realistic depth perception.

Method: It employs a combination of refractive lens eyepieces, wavefront coding diffractive optical elements, and a convolutional neural network pre-processing module optimized through end-to-end learning with a differentiable retinal image formation model.

Result: The method demonstrated accommodation-invariance for depth ranges up to four diopters, validated through simulations and a benchtop setup.

Conclusion: The approach successfully mitigates the vergence-accommodation conflict and offers practical depth perception enhancements in near-eye displays.

Abstract: We present a new computational near-eye display method that addresses the
vergence-accommodation conflict problem in stereoscopic displays through
accommodation-invariance. Our system integrates a refractive lens eyepiece with
a novel wavefront coding diffractive optical element, operating in tandem with
a pre-processing convolutional neural network. We employ end-to-end learning to
jointly optimize the wavefront-coding optics and the image pre-processing
module. To implement this approach, we develop a differentiable retinal image
formation model that accounts for limiting aperture and chromatic aberrations
introduced by the eye optics. We further integrate the neural transfer function
and the contrast sensitivity function into the loss model to account for
related perceptual effects. To tackle off-axis distortions, we incorporate
position dependency into the pre-processing module. In addition to conducting
rigorous analysis based on simulations, we also fabricate the designed
diffractive optical element and build a benchtop setup, demonstrating
accommodation-invariance for depth ranges of up to four diopters.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [435] [Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2510.11824)
*Simin Li,Zihao Mao,Hanxiao Li,Zonglei Jing,Zhuohang bian,Jun Guo,Li Wang,Zhuoran Han,Ruixiao Xu,Xin Yu,Chengdong Ma,Yuqing Ma,Bo An,Yaodong Yang,Weifeng Lv,Xianglong Liu*

Main category: cs.MA

TL;DR: This paper investigates the robustness and resilience of cooperative Multi-Agent Reinforcement Learning (MARL) under real-world uncertainties using 82,620 experiments and provides guidance for hyperparameter tuning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap in understanding robustness and resilience in MARL systems, essential for developing trustworthy systems, as most existing studies focus only on cooperation without considering real-world uncertainties.

Method: The authors conducted a large-scale empirical study with 82,620 experiments across 4 real-world environments, 13 uncertainty types, and 15 hyperparameters to evaluate cooperation, robustness, and resilience in MARL systems.

Result: The study found that robustness and resilience depend on the intensity of perturbations, the type of uncertainty, and algorithm choice. Hyperparameter tuning significantly impacts performance, with some practices improving results while others, surprisingly, harming robustness.

Conclusion: By optimizing hyperparameters, substantial improvements in cooperation, robustness, and resilience were observed, suggesting hyperparameter tuning is critical for trustworthy MARL systems.

Abstract: In cooperative Multi-Agent Reinforcement Learning (MARL), it is a common
practice to tune hyperparameters in ideal simulated environments to maximize
cooperative performance. However, policies tuned for cooperation often fail to
maintain robustness and resilience under real-world uncertainties. Building
trustworthy MARL systems requires a deep understanding of robustness, which
ensures stability under uncertainties, and resilience, the ability to recover
from disruptions--a concept extensively studied in control systems but largely
overlooked in MARL. In this paper, we present a large-scale empirical study
comprising over 82,620 experiments to evaluate cooperation, robustness, and
resilience in MARL across 4 real-world environments, 13 uncertainty types, and
15 hyperparameters. Our key findings are: (1) Under mild uncertainty,
optimizing cooperation improves robustness and resilience, but this link
weakens as perturbations intensify. Robustness and resilience also varies by
algorithm and uncertainty type. (2) Robustness and resilience do not generalize
across uncertainty modalities or agent scopes: policies robust to action noise
for all agents may fail under observation noise on a single agent. (3)
Hyperparameter tuning is critical for trustworthy MARL: surprisingly, standard
practices like parameter sharing, GAE, and PopArt can hurt robustness, while
early stopping, high critic learning rates, and Leaky ReLU consistently help.
By optimizing hyperparameters only, we observe substantial improvement in
cooperation, robustness and resilience across all MARL backbones, with the
phenomenon also generalizing to robust MARL methods across these backbones.
Code and results available at
https://github.com/BUAA-TrustworthyMARL/adv_marl_benchmark .

</details>


### [436] [Heterogeneous RBCs via deep multi-agent reinforcement learning](https://arxiv.org/abs/2510.12272)
*Federico Gabriele,Aldo Glielmo,Marco Taboga*

Main category: cs.MA

TL;DR: The paper introduces MARL-BC, integrating multi-agent reinforcement learning with RBC models to address limitations of heterogeneous agent macroeconomic models.


<details>
  <summary>Details</summary>
Motivation: Existing models like HANK and KS face computational difficulties and constrained heterogeneity, while ABMs require extensive development of behavioral rules; there is a need for an integrated solution.

Method: The authors propose combining deep multi-agent reinforcement learning (MARL) with Real Business Cycle (RBC) models to enhance agent-based modeling and general equilibrium analysis.

Result: MARL-BC successfully replicates RBC results, mean-field KS model findings, and accommodates diverse heterogeneity among agents.

Conclusion: MARL-BC bridges the divide between agent-based models and general equilibrium models, offering a synthesized framework that tackles their respective limitations effectively.

Abstract: Current macroeconomic models with agent heterogeneity can be broadly divided
into two main groups. Heterogeneous-agent general equilibrium (GE) models, such
as those based on Heterogeneous Agents New Keynesian (HANK) or Krusell-Smith
(KS) approaches, rely on GE and 'rational expectations', somewhat unrealistic
assumptions that make the models very computationally cumbersome, which in turn
limits the amount of heterogeneity that can be modelled. In contrast,
agent-based models (ABMs) can flexibly encompass a large number of arbitrarily
heterogeneous agents, but typically require the specification of explicit
behavioural rules, which can lead to a lengthy trial-and-error
model-development process. To address these limitations, we introduce MARL-BC,
a framework that integrates deep multi-agent reinforcement learning (MARL) with
Real Business Cycle (RBC) models. We demonstrate that MARL-BC can: (1) recover
textbook RBC results when using a single agent; (2) recover the results of the
mean-field KS model using a large number of identical agents; and (3)
effectively simulate rich heterogeneity among agents, a hard task for
traditional GE approaches. Our framework can be thought of as an ABM if used
with a variety of heterogeneous interacting agents, and can reproduce GE
results in limit cases. As such, it is a step towards a synthesis of these
often opposed modelling paradigms.

</details>
