{"id": "2507.02004", "pdf": "https://arxiv.org/pdf/2507.02004", "abs": "https://arxiv.org/abs/2507.02004", "authors": ["Ruofan Jin", "Zaixi Zhang", "Mengdi Wang", "Le Cong"], "title": "STELLA: Self-Evolving LLM Agent for Biomedical Research", "categories": ["cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "The rapid growth of biomedical data, tools, and literature has created a\nfragmented research landscape that outpaces human expertise. While AI agents\noffer a solution, they typically rely on static, manually curated toolsets,\nlimiting their ability to adapt and scale. Here, we introduce STELLA, a\nself-evolving AI agent designed to overcome these limitations. STELLA employs a\nmulti-agent architecture that autonomously improves its own capabilities\nthrough two core mechanisms: an evolving Template Library for reasoning\nstrategies and a dynamic Tool Ocean that expands as a Tool Creation Agent\nautomatically discovers and integrates new bioinformatics tools. This allows\nSTELLA to learn from experience. We demonstrate that STELLA achieves\nstate-of-the-art accuracy on a suite of biomedical benchmarks, scoring\napproximately 26\\% on Humanity's Last Exam: Biomedicine, 54\\% on LAB-Bench:\nDBQA, and 63\\% on LAB-Bench: LitQA, outperforming leading models by up to 6\npercentage points. More importantly, we show that its performance\nsystematically improves with experience; for instance, its accuracy on the\nHumanity's Last Exam benchmark almost doubles with increased trials. STELLA\nrepresents a significant advance towards AI Agent systems that can learn and\ngrow, dynamically scaling their expertise to accelerate the pace of biomedical\ndiscovery.", "AI": {"tldr": "STELLA is a self-evolving AI agent that autonomously improves its capabilities to address the fragmented biomedical research landscape. It outperforms leading models in benchmarks and improves with experience.", "motivation": "The fragmentation in biomedical data, tools, and literature has outpaced human expertise, necessitating a scalable, adaptive AI solution.", "method": "STELLA employs a multi-agent architecture with a Template Library for reasoning strategies and a Tool Ocean for dynamic tool integration via a Tool Creation Agent.", "result": "STELLA achieved state-of-the-art accuracy in biomedical benchmarks, consistently improving its performance with experience.", "conclusion": "STELLA marks an advancement in AI systems able to independently grow and dynamically accelerate biomedical discovery."}}
{"id": "2507.02073", "pdf": "https://arxiv.org/pdf/2507.02073", "abs": "https://arxiv.org/abs/2507.02073", "authors": ["Nikita Bhedasgaonkar", "Rushikesh K. Joshi"], "title": "HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection", "categories": ["cs.AI", "cs.LG"], "comment": "11 pages, 5 tables, 2 figures", "summary": "In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting\nRules), a lightweight rule-based feature selection method that combines\nParameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to\neliminate redundant features and retain relevant ones. This method is a hybrid\nof non-iterative and iterative filtering approaches for dimensionality\nreduction. It is a greedy method, which works by backward elimination,\neliminating possibly multiple features at every step. The rules contribute to\nvoting for features, and a decision to keep or discard is made by majority\nvoting. The rules make use of correlation thresholds between every pair of\nfeatures, and between features and the target. We provide the results from the\napplication of HCVR to the SPAMBASE dataset. The results showed improvement\nperformance as compared to traditional non-iterative (CFS, mRMR and MI) and\niterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was\nassessed based on the performance of different classifiers after applying\nfiltering.", "AI": {"tldr": "This paper introduces HCVR, a rule-based feature selection method combining correlations for efficient dimensionality reduction.", "motivation": "To improve feature selection processes by eliminating redundancy and retaining relevant features while combining advantages of both non-iterative and iterative filtering methods.", "method": "HCVR uses a hybrid approach with P2P and P2T correlations, applying backward elimination via correlation-aware voting rules.", "result": "HCVR demonstrated improved performance on the SPAMBASE dataset compared to traditional non-iterative and iterative techniques.", "conclusion": "HCVR is effective in dimensionality reduction, enhancing classifier performance compared to existing methods."}}
{"id": "2507.02076", "pdf": "https://arxiv.org/pdf/2507.02076", "abs": "https://arxiv.org/abs/2507.02076", "authors": ["Mohammad Ali Alomrani", "Yingxue Zhang", "Derek Li", "Qianyi Sun", "Soumyasundar Pal", "Zhanguang Zhang", "Yaochen Hu", "Rohan Deepak Ajwani", "Antonios Valkanas", "Raika Karimi", "Peng Cheng", "Yunzhou Wang", "Pengyi Liao", "Hanrui Huang", "Bin Wang", "Jianye Hao", "Mark Coates"], "title": "Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have rapidly progressed into general-purpose\nagents capable of solving a broad spectrum of tasks. However, current models\nremain inefficient at reasoning: they apply fixed inference-time compute\nregardless of task complexity, often overthinking simple problems while\nunderthinking hard ones. This survey presents a comprehensive review of\nefficient test-time compute (TTC) strategies, which aim to improve the\ncomputational efficiency of LLM reasoning. We introduce a two-tiered taxonomy\nthat distinguishes between L1-controllability, methods that operate under fixed\ncompute budgets, and L2-adaptiveness, methods that dynamically scale inference\nbased on input difficulty or model confidence. We benchmark leading proprietary\nLLMs across diverse datasets, highlighting critical trade-offs between\nreasoning performance and token usage. Compared to prior surveys on efficient\nreasoning, our review emphasizes the practical control, adaptability, and\nscalability of TTC methods. Finally, we discuss emerging trends such as hybrid\nthinking models and identify key challenges for future work towards making LLMs\nmore computationally efficient, robust, and responsive to user constraints.", "AI": {"tldr": "The paper reviews efficient test-time compute (TTC) strategies to improve large language models (LLMs) reasoning efficiency by balancing computational effort with task complexity.", "motivation": "Current LLMs apply a fixed amount of computational effort at reasoning regardless of task complexity, leading to inefficiencies in both simple and difficult tasks.", "method": "The paper introduces a taxonomy to categorize TTC strategies into L1-controllability (fixed compute budgets) and L2-adaptiveness (dynamic scaling based on input complexity or model confidence).", "result": "Benchmarking of proprietary LLMs demonstrates trade-offs between reasoning performance and token usage using diverse datasets.", "conclusion": "Emerging trends like hybrid models and challenges for improving computational efficiency in LLMs are discussed to achieve better adaptability, scalability, and robustness."}}
{"id": "2507.02083", "pdf": "https://arxiv.org/pdf/2507.02083", "abs": "https://arxiv.org/abs/2507.02083", "authors": ["Haonan Duan", "Stephen Zhewen Lu", "Caitlin Fiona Harrigan", "Nishkrit Desai", "Jiarui Lu", "Micha\u0142 Koziarski", "Leonardo Cotta", "Chris J. Maddison"], "title": "Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab", "categories": ["cs.AI"], "comment": null, "summary": "Designing experiments and result interpretations are core scientific\ncompetencies, particularly in biology, where researchers perturb complex\nsystems to uncover the underlying systems. Recent efforts to evaluate the\nscientific capabilities of large language models (LLMs) fail to test these\ncompetencies because wet-lab experimentation is prohibitively expensive: in\nexpertise, time and equipment. We introduce SciGym, a first-in-class benchmark\nthat assesses LLMs' iterative experiment design and analysis abilities in\nopen-ended scientific discovery tasks. SciGym overcomes the challenge of\nwet-lab costs by running a dry lab of biological systems. These models, encoded\nin Systems Biology Markup Language, are efficient for generating simulated\ndata, making them ideal testbeds for experimentation on realistically complex\nsystems. We evaluated six frontier LLMs on 137 small systems, and released a\ntotal of 350 systems. Our evaluation shows that while more capable models\ndemonstrated superior performance, all models' performance declined\nsignificantly as system complexity increased, suggesting substantial room for\nimprovement in the scientific capabilities of LLM agents.", "AI": {"tldr": "This paper introduces SciGym, a benchmark to assess large language models (LLMs) in scientific tasks like experiment design and analysis using a simulated 'dry lab' for biological systems.", "motivation": "To address the limitations of evaluating LLMs in scientific reasoning due to the high cost and complexity of wet-lab biological experiments.", "method": "They developed SciGym, a framework that uses simulated biological systems encoded in Systems Biology Markup Language for cost-effective experimentation.", "result": "They evaluated six cutting-edge LLMs on 137 systems and released a dataset of 350 systems. Performance declined with increased system complexity.", "conclusion": "There is significant room for improvement in the scientific reasoning capabilities of existing LLMs, particularly for handling complex systems."}}
{"id": "2507.02235", "pdf": "https://arxiv.org/pdf/2507.02235", "abs": "https://arxiv.org/abs/2507.02235", "authors": ["Meng Xu", "Frank Neumann", "Aneta Neumann", "Yew Soon Ong"], "title": "Quality Diversity Genetic Programming for Learning Scheduling Heuristics", "categories": ["cs.NE"], "comment": "9 pages, 5 figures", "summary": "Real-world optimization often demands diverse, high-quality solutions.\nQuality-Diversity (QD) optimization is a multifaceted approach in evolutionary\nalgorithms that aims to generate a set of solutions that are both\nhigh-performing and diverse. QD algorithms have been successfully applied\nacross various domains, providing robust solutions by exploring diverse\nbehavioral niches. However, their application has primarily focused on static\nproblems, with limited exploration in the context of dynamic combinatorial\noptimization problems. Furthermore, the theoretical understanding of QD\nalgorithms remains underdeveloped, particularly when applied to learning\nheuristics instead of directly learning solutions in complex and dynamic\ncombinatorial optimization domains, which introduces additional challenges.\nThis paper introduces a novel QD framework for dynamic scheduling problems. We\npropose a map-building strategy that visualizes the solution space by linking\nheuristic genotypes to their behaviors, enabling their representation on a QD\nmap. This map facilitates the discovery and maintenance of diverse scheduling\nheuristics. Additionally, we conduct experiments on both fixed and dynamically\nchanging training instances to demonstrate how the map evolves and how the\ndistribution of solutions unfolds over time. We also discuss potential future\nresearch directions that could enhance the learning process and broaden the\napplicability of QD algorithms to dynamic combinatorial optimization\nchallenges.", "AI": {"tldr": "The paper introduces a novel Quality-Diversity (QD) approach for solving dynamic scheduling problems, emphasizing diverse and high-performing heuristic solutions instead of directly optimizing solutions.", "motivation": "To address the limitations of existing QD algorithms primarily designed for static problems and to tackle their theoretical underdevelopment in learning heuristics for dynamic combinatorial optimization.", "method": "The authors propose a map-building strategy connecting heuristic genotypes to behaviors, enabling visualization and dynamic exploration of solution spaces on a QD map. Experiments were conducted on fixed and dynamic training instances to evaluate the map's evolution.", "result": "The approach effectively discovers and maintains diverse scheduling heuristics, demonstrating adaptability to changing contexts and showcasing the evolving solution distribution over time.", "conclusion": "The proposed QD framework is a step forward in applying QD optimization to dynamic combinatorial problems, broadening its scope and opening doors to further research in the domain."}}
{"id": "2507.02067", "pdf": "https://arxiv.org/pdf/2507.02067", "abs": "https://arxiv.org/abs/2507.02067", "authors": ["Nikolaos Papanikolaou", "Doha Touhafi", "Jurgen Vandendriessche", "Danial Karimi", "Sohail Fatimi", "Gianluca Cornetta", "Abdellah Touhafi"], "title": "Advanced Printed Sensors for Environmental Applications: A Path Towards Sustainable Monitoring Solutions", "categories": ["cs.AR"], "comment": null, "summary": "Printed sensors represent a transformative advancement in sensor technology,\nutilizing innovative printing techniques to create flexible, cost-effective,\nand highly customizable sensing devices. Their versatility allows integration\ninto numerous applications across diverse fields such as monitoring a wide\nrange of environmental factors e.g. air and water quality, soil conditions, and\natmospheric changes among others. These sensors demonstrate high sensitivity\nand accuracy in detecting pollutants, temperature variations, humidity levels,\nand other critical parameters essential for environmental assessment and\nprotection.", "AI": {"tldr": "Printed sensors use innovative printing techniques to create adaptable environmental monitoring tools.", "motivation": "The paper aims to address the need for flexible, cost-effective, and customizable sensors for environmental monitoring.", "method": "Develop printed sensors using advanced printing technologies for detecting environmental parameters.", "result": "Printed sensors showed high sensitivity and accuracy in measuring pollutants, temperature, humidity, and other critical factors.", "conclusion": "Printed sensors provide a versatile, efficient solution for comprehensive environmental assessment and protection."}}
{"id": "2507.02088", "pdf": "https://arxiv.org/pdf/2507.02088", "abs": "https://arxiv.org/abs/2507.02088", "authors": ["Tian Lan", "Xiangdong Su", "Xu Liu", "Ruirui Wang", "Ke Chang", "Jiang Li", "Guanglai Gao"], "title": "McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": "24 pages, 9 figures", "summary": "As large language models (LLMs) are increasingly applied to various NLP\ntasks, their inherent biases are gradually disclosed. Therefore, measuring\nbiases in LLMs is crucial to mitigate its ethical risks. However, most existing\nbias evaluation datasets focus on English and North American culture, and their\nbias categories are not fully applicable to other cultures. The datasets\ngrounded in the Chinese language and culture are scarce. More importantly,\nthese datasets usually only support single evaluation tasks and cannot evaluate\nthe bias from multiple aspects in LLMs. To address these issues, we present a\nMulti-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias\nevaluation instances, covering 12 single bias categories, 82 subcategories and\nintroducing 5 evaluation tasks, providing extensive category coverage, content\ndiversity, and measuring comprehensiveness. Additionally, we evaluate several\npopular LLMs from different series and with parameter sizes. In general, all\nthese LLMs demonstrated varying degrees of bias. We conduct an in-depth\nanalysis of results, offering novel insights into bias in LLMs.", "AI": {"tldr": "This paper introduces McBE, a comprehensive Chinese Bias Evaluation Benchmark for assessing biases in large language models (LLMs), overcoming limitations of existing datasets focused on English and North American culture.", "motivation": "To address the lack of comprehensive bias evaluation benchmarks for LLMs in Chinese language and culture, while improving on existing datasets that only support single tasks and focus narrowly on Western settings.", "method": "The authors developed McBE, a benchmark with 4,077 bias evaluation instances, covering 12 bias categories, 82 subcategories, and 5 evaluation tasks to measure biases from multiple aspects in Chinese-based LLMs.", "result": "Several popular LLMs were evaluated using McBE, revealing varying degrees of bias across the board. The analysis provided insights into the prevalence and types of biases.", "conclusion": "McBE offers a culturally and linguistically relevant tool to evaluate biases in LLMs, advancing understanding and mitigation of bias while filling a key gap in research for Chinese language contexts."}}
{"id": "2507.02124", "pdf": "https://arxiv.org/pdf/2507.02124", "abs": "https://arxiv.org/abs/2507.02124", "authors": ["Fumikazu Konishi"], "title": "SAKURAONE: Empowering Transparent and Open AI Platforms through Private-Sector HPC Investment in Japan", "categories": ["cs.DC", "cs.NI", "C.5.5; B.8.2"], "comment": "13 pages, 2 Figures, 10 tables", "summary": "SAKURAONE is a managed high performance computing (HPC) cluster developed and\noperated by the SAKURA Internet Research Center. It reinforces the ``KOKARYOKU\nPHY'' configuration of bare-metal GPU servers and is designed as a cluster\ncomputing resource optimized for advanced workloads, including large language\nmodel (LLM) training.\n  In the ISC 2025 edition of the TOP500 list, SAKURAONE was ranked\n\\textbf{49th} in the world based on its High Performance Linpack (HPL) score,\ndemonstrating its global competitiveness. In particular, it is the \\textbf{only\nsystem within the top 100} that employs a fully open networking stack based on\n\\textbf{800~GbE (Gigabit Ethernet)} and the \\textbf{SONiC (Software for Open\nNetworking in the Cloud)} operating system, highlighting the viability of open\nand vendor-neutral technologies in large-scale HPC infrastructure.\n  SAKURAONE achieved a sustained performance of 33.95~PFLOP/s on the HPL\nbenchmark (Rmax), and 396.295~TFLOP/s on the High Performance Conjugate\nGradient (HPCG) benchmark. For the HPL-MxP benchmark, which targets\nlow-precision workloads representative of AI applications, SAKURAONE delivered\nan impressive 339.86~PFLOP/s using FP8 precision.\n  The system comprises 100 compute nodes, each equipped with eight NVIDIA H100\nGPUs. It is supported by an all-flash Lustre storage subsystem with a total\nphysical capacity of 2~petabytes, providing high-throughput and low-latency\ndata access. Internode communication is enabled by a full-bisection bandwidth\ninterconnect based on a Rail-Optimized topology, where the Leaf and Spine\nlayers are interconnected via 800~GbE links. This topology, in combination with\nRoCEv2 (RDMA over Converged Ethernet version 2), enables high-speed, lossless\ndata transfers and mitigates communication bottlenecks in large-scale parallel\nworkloads.", "AI": {"tldr": "SAKURAONE, a managed HPC cluster by SAKURA Internet Research Center, ranks 49th globally in the ISC 2025 TOP500 list and highlights open networking technologies in HPC, achieving advanced benchmark results.", "motivation": "The paper seeks to showcase SAKURAONE's innovation in HPC systems by incorporating vendor-neutral, high-performance components, emphasizing open networking technologies for scalability and effectiveness in demanding workloads such as LLM training.", "method": "SAKURAONE employs 100 compute nodes with NVIDIA H100 GPUs and an all-flash Lustre storage system. The Rail-Optimized topology with 800 GbE links and SONiC OS supports open, high-performance interconnects for data-intensive computing.", "result": "The system sustained 33.95 PFLOP/s in HPL benchmarks, 396.295 TFLOP/s in HPCG benchmarks, and 339.86 PFLOP/s in HPL-MxP benchmarks using FP8 precision. It ranks as the 49th fastest HPC system globally and the only top-100 system using open network technology.", "conclusion": "SAKURAONE demonstrates the feasibility and competitiveness of open and vendor-neutral technologies in large-scale HPC infrastructure while also addressing complex computing tasks efficiently for AI and HPC workloads."}}
{"id": "2507.02226", "pdf": "https://arxiv.org/pdf/2507.02226", "abs": "https://arxiv.org/abs/2507.02226", "authors": ["Mohammad Akyash", "Kimia Azar", "Hadi Kamali"], "title": "DecoRTL: A Run-time Decoding Framework for RTL Code Generation with LLMs", "categories": ["cs.PL", "cs.AR", "cs.LG"], "comment": "Accepted to the International Conference on Computer-Aided Design\n  (ICCAD 2025)", "summary": "As one of their many applications, large language models (LLMs) have recently\nshown promise in automating register transfer level (RTL) code generation.\nHowever, conventional LLM decoding strategies, originally designed for natural\nlanguage, often fail to meet the structural and semantic demands of RTL,\nleading to hallucinated, repetitive, or invalid code outputs. In this paper, we\nfirst investigate the root causes of these decoding failures through an\nempirical analysis of token-level entropy during RTL generation. Our findings\nreveal that LLMs exhibit low confidence in regions of structural ambiguity or\nsemantic complexity, showing that standard decoding strategies fail to\ndifferentiate between regions requiring determinism (syntax-critical regions)\nand those that benefit from creative exploratory variability (design-critical\nregions). Then, to overcome this, we introduce DecoRTL, a novel run-time\ndecoding strategy, that is both syntax-aware and contrastive for RTL code\ngeneration. DecoRTL integrates two complementary components: (i)\nself-consistency sampling, which generates multiple candidates and re-ranks\nthem based on token-level agreement to promote correctness while maintaining\ndiversity; and (ii) syntax-aware temperature adaptation, which classifies\ntokens by their syntactical and functional roles and adjusts the sampling\ntemperature accordingly, enforcing low temperature for syntax-critical tokens\nand higher temperature for exploratory ones. Our approach operates entirely at\ninference time without requiring any additional model fine-tuning. Through\nevaluations on multiple open-source LLMs using the VerilogEval benchmark, we\ndemonstrate significant improvements in syntactic validity, functional\ncorrectness, and output diversity, while the execution overhead (performance\noverhead) is imperceptible.", "AI": {"tldr": "This paper addresses failures in standard large language model (LLM) decoding strategies for generating register transfer level (RTL) code, introducing DecoRTL, a novel inference-time approach to optimize correctness and diversity without model fine-tuning.", "motivation": "Standard LLM decoding strategies struggle to generate valid RTL code due to structural ambiguities and semantic complexities, leading to unusable outputs.", "method": "The authors propose DecoRTL, which employs self-consistency sampling for candidate refinement and syntax-aware temperature adaptation to balance determinism and exploratory creativity during RTL code generation.", "result": "DecoRTL improves syntactic validity, functional correctness, and output diversity in RTL generation across multiple LLMs, while imposing negligible inference-time overhead.", "conclusion": "The paper establishes DecoRTL as an effective inference-time decoding strategy for enhancing RTL code generation, addressing key limitations in conventional methods without requiring additional model training."}}
{"id": "2507.01966", "pdf": "https://arxiv.org/pdf/2507.01966", "abs": "https://arxiv.org/abs/2507.01966", "authors": ["Guobin Shen", "Dongcheng Zhao", "Yiting Dong", "Qian Zhang", "Yi Zeng"], "title": "Alignment between Brains and AI: Evidence for Convergent Evolution across Modalities, Scales and Training Trajectories", "categories": ["q-bio.NC"], "comment": null, "summary": "Artificial and biological systems may evolve similar computational solutions\ndespite fundamental differences in architecture and learning mechanisms -- a\nform of convergent evolution. We demonstrate this phenomenon through\nlarge-scale analysis of alignment between human brain activity and internal\nrepresentations of over 600 AI models spanning language and vision domains,\nfrom 1.33M to 72B parameters. Analyzing 60 million alignment measurements\nreveals that higher-performing models spontaneously develop stronger brain\nalignment without explicit neural constraints, with language models showing\nmarkedly stronger correlation (r=0.89, p<7.5e-13) than vision models (r=0.53,\np<2.0e-44). Crucially, longitudinal analysis demonstrates that brain alignment\nconsistently precedes performance improvements during training, suggesting that\ndeveloping brain-like representations may be a necessary stepping stone toward\nhigher capabilities. We find systematic patterns: language models exhibit\nstrongest alignment with limbic and integrative regions, while vision models\nshow progressive alignment with visual cortices; deeper processing layers\nconverge across modalities; and as representational scale increases, alignment\nsystematically shifts from primary sensory to higher-order associative regions.\nThese findings provide compelling evidence that optimization for task\nperformance naturally drives AI systems toward brain-like computational\nstrategies, offering both fundamental insights into principles of intelligent\ninformation processing and practical guidance for developing more capable AI\nsystems.", "AI": {"tldr": "The study finds that higher-performing AI models naturally align more closely with human brain patterns, suggesting brain-like representations may enhance AI capabilities.", "motivation": "To investigate whether artificial and biological systems evolve similar computational strategies despite different architectures and to understand the relationship between AI task performance and brain-like alignments.", "method": "The researchers examined over 600 AI models (spanning language and vision domains) using large-scale analysis of 60 million brain-activity and AI alignment measurements. They also conducted longitudinal analysis observing alignment and performance changes during training.", "result": "Higher-performing AI models showed greater brain alignment, with stronger correlations in language models (r=0.89) than vision models (r=0.53). Patterns emerged linking alignment with specific brain regions and layers of processing, showing a progression as model scales increased.", "conclusion": "Optimizing task performance in AI systems naturally leads to brain-like computational strategies, offering insights into intelligent processing and providing a framework for advancing AI capabilities."}}
{"id": "2507.02068", "pdf": "https://arxiv.org/pdf/2507.02068", "abs": "https://arxiv.org/abs/2507.02068", "authors": ["Brian Bell", "Teresa Thomas", "Sang Won Lee", "Chris Brown"], "title": "How do Software Engineering Candidates Prepare for Technical Interviews?", "categories": ["cs.SE"], "comment": null, "summary": "To obtain employment, aspiring software engineers must complete technical\ninterviews -- a hiring process which involves candidates writing code while\ncommunicating to an audience. However, the complexities of tech interviews are\ndifficult to prepare for and seldom faced in computing curricula. To this end,\nwe seek to understand how candidates prepare for technical interviews,\ninvestigating the effects of preparation methods and the role of education. We\ndistributed a survey to candidates (n = 131) actively preparing for technical\ninterviews. Our results suggest candidates rarely train in authentic settings\nand courses fail to support preparation efforts -- leading to stress and\nunpreparedness. Based on our findings, we provide implications for stakeholders\nto enhance tech interview preparation for candidates pursuing software\nengineering roles.", "AI": {"tldr": "The study examines how software engineering candidates prepare for technical interviews, finding that current educational approaches are insufficient, causing stress and unpreparedness.", "motivation": "Technical interviews are a significant hurdle for aspiring software engineers due to the lack of focused preparation within computing curricula.", "method": "A survey was distributed to 131 candidates who were actively preparing for technical interviews, analyzing preparation methods and educational influences.", "result": "Candidates typically do not train in authentic interview settings, and courses fail to adequately support their preparation, resulting in stress and feelings of unpreparedness.", "conclusion": "Enhancing technical interview preparation efforts within educational contexts can better support candidates and reduce stress during this crucial process."}}
{"id": "2507.02016", "pdf": "https://arxiv.org/pdf/2507.02016", "abs": "https://arxiv.org/abs/2507.02016", "authors": ["Cong Wang", "Roberto Calandra", "Verena Kl\u00f6s"], "title": "Effective Explanations for Belief-Desire-Intention Robots: When and What to Explain", "categories": ["cs.RO", "cs.AI"], "comment": "Paper accepted at IEEE RO-MAN 2025; 6 pages", "summary": "When robots perform complex and context-dependent tasks in our daily lives,\ndeviations from expectations can confuse users. Explanations of the robot's\nreasoning process can help users to understand the robot intentions. However,\nwhen to provide explanations and what they contain are important to avoid user\nannoyance. We have investigated user preferences for explanation demand and\ncontent for a robot that helps with daily cleaning tasks in a kitchen. Our\nresults show that users want explanations in surprising situations and prefer\nconcise explanations that clearly state the intention behind the confusing\naction and the contextual factors that were relevant to this decision. Based on\nthese findings, we propose two algorithms to identify surprising actions and to\nconstruct effective explanations for Belief-Desire-Intention (BDI) robots. Our\nalgorithms can be easily integrated in the BDI reasoning process and pave the\nway for better human-robot interaction with context- and user-specific\nexplanations.", "AI": {"tldr": "The paper explores when and how robots should provide explanations for their actions to avoid user confusion and annoyance, proposing algorithms for better human-robot interaction.", "motivation": "Improve human-robot interaction by addressing confusion caused by unexpected robot actions during complex tasks.", "method": "Investigated user preferences for explanation timing and content; developed algorithms for identifying surprising actions and making effective explanations integrated with BDI robots.", "result": "Users prefer concise explanations in surprising situations; proposed algorithms function with Belief-Desire-Intention systems for tailored explanations.", "conclusion": "Effective explanations tailored to user preferences enhance understanding in human-robot interaction and can be implemented easily in BDI robot frameworks."}}
{"id": "2507.02084", "pdf": "https://arxiv.org/pdf/2507.02084", "abs": "https://arxiv.org/abs/2507.02084", "authors": ["Yining Feng", "Ivan Selesnick"], "title": "Adaptive Iterative Soft-Thresholding Algorithm with the Median Absolute Deviation", "categories": ["stat.ML", "cs.LG", "eess.SP"], "comment": null, "summary": "The adaptive Iterative Soft-Thresholding Algorithm (ISTA) has been a popular\nalgorithm for finding a desirable solution to the LASSO problem without\nexplicitly tuning the regularization parameter $\\lambda$. Despite that the\nadaptive ISTA is a successful practical algorithm, few theoretical results\nexist. In this paper, we present the theoretical analysis on the adaptive ISTA\nwith the thresholding strategy of estimating noise level by median absolute\ndeviation. We show properties of the fixed points of the algorithm, including\nscale equivariance, non-uniqueness, and local stability, prove the local linear\nconvergence guarantee, and show its global convergence behavior.", "AI": {"tldr": "This paper examines the adaptive ISTA for solving the LASSO problem, focusing on theoretical properties such as fixed-point properties, local linear convergence, and global behavior.", "motivation": "Despite the practical success of adaptive ISTA in solving LASSO problems without explicitly tuning the regularization parameter, its theoretical understanding is limited.", "method": "The authors analyze adaptive ISTA using the median absolute deviation thresholding strategy, exploring fixed-point properties, local linear convergence, and global convergence.", "result": "The paper proves properties of fixed points (e.g., scale equivariance, non-uniqueness, local stability), guarantees local linear convergence, and discusses its global behavior.", "conclusion": "The analysis provides theoretical insights into adaptive ISTA's functionality, addressing gaps in its understanding and offering guarantees for its performance."}}
{"id": "2507.02074", "pdf": "https://arxiv.org/pdf/2507.02074", "abs": "https://arxiv.org/abs/2507.02074", "authors": ["Sanjeda Akter", "Ibne Farabi Shihab", "Anuj Sharma"], "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Crash detection from video feeds is a critical problem in intelligent\ntransportation systems. Recent developments in large language models (LLMs) and\nvision-language models (VLMs) have transformed how we process, reason about,\nand summarize multimodal information. This paper surveys recent methods\nleveraging LLMs for crash detection from video data. We present a structured\ntaxonomy of fusion strategies, summarize key datasets, analyze model\narchitectures, compare performance benchmarks, and discuss ongoing challenges\nand opportunities. Our review provides a foundation for future research in this\nfast-growing intersection of video understanding and foundation models.", "AI": {"tldr": "This paper surveys the use of large language models (LLMs) and vision-language models (VLMs) for crash detection in video feeds, presenting a taxonomy, performance comparison, and future directions.", "motivation": "The need for effective crash detection methods in intelligent transportation systems, alongside developments in vision-language and large language models, motivates this research.", "method": "The authors conduct a survey of recent methods, offering a taxonomy, dataset summaries, model architecture analysis, and benchmark comparisons.", "result": "The paper categorizes various fusion strategies, summarizes datasets, analyzes model designs, and highlights performance benchmarks in crash detection.", "conclusion": "This review lays a foundation for advancing research at the intersection of video understanding and foundation models, identifying challenges and opportunities in the field."}}
{"id": "2507.01975", "pdf": "https://arxiv.org/pdf/2507.01975", "abs": "https://arxiv.org/abs/2507.01975", "authors": ["Mengtao Yan", "Qi Wang", "Haining Wang", "Ruizhi Chengze", "Yi Zhang", "Hongsheng Liu", "Zidong Wang", "Fan Yu", "Qi Qi", "Hao Sun"], "title": "Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows", "categories": ["cs.LG", "cs.AI", "physics.flu-dyn"], "comment": "19 pages, 12 figures, accepted at KDD 2025 (ACM SIGKDD Conference on\n  Knowledge Discovery and Data Mining)", "summary": "Simulation of fluid flows is crucial for modeling physical phenomena like\nmeteorology, aerodynamics, and biomedicine. Classical numerical solvers often\nrequire fine spatiotemporal grids to satisfy stability, consistency, and\nconvergence conditions, leading to substantial computational costs. Although\nmachine learning has demonstrated better efficiency, they typically suffer from\nissues of interpretability, generalizability, and data dependency. Hence, we\npropose a learnable and differentiable finite volume solver, called LDSolver,\ndesigned for efficient and accurate simulation of fluid flows on spatiotemporal\ncoarse grids. LDSolver comprises two key components: (1) a differentiable\nfinite volume solver, and (2) an learnable module providing equivalent\napproximation for fluxes (derivatives and interpolations), and temporal error\ncorrection on coarse grids. Even with limited training data (e.g., only a few\ntrajectories), our model could accelerate the simulation while maintaining a\nhigh accuracy with superior generalizability. Experiments on different flow\nsystems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver\nachieves state-of-the-art performance, surpassing baseline models with notable\nmargins.", "AI": {"tldr": "The paper introduces LDSolver, a novel machine learning-based differentiable finite volume solver designed for efficient and accurate fluid flow simulations on coarse grids, achieving state-of-the-art performance.", "motivation": "The paper addresses the limitations of classical numerical solvers for fluid simulations, which demand high computational resources, and machine learning approaches that struggle with interpretability, generalizability, and data dependency.", "method": "The authors developed LDSolver, featuring a differentiable finite volume solver and a learnable module that approximates fluxes and corrects temporal errors on coarse grids for improved simulation performance.", "result": "LDSolver demonstrated superior generalizability and high accuracy even with limited training data, as validated through experiments on various fluid flow systems, outperforming other baseline methods.", "conclusion": "LDSolver offers an efficient, accurate, and scalable solution to fluid flow simulations on coarse grids, addressing key challenges such as computational cost and machine learning limitations."}}
{"id": "2507.02103", "pdf": "https://arxiv.org/pdf/2507.02103", "abs": "https://arxiv.org/abs/2507.02103", "authors": ["Daniel Durstewitz", "Bruno Averbeck", "Georgia Koppe"], "title": "What Neuroscience Can Teach AI About Learning in Continuously Changing Environments", "categories": ["cs.AI", "q-bio.NC", "I.2; I.6; A.1"], "comment": "Submitted as a Perspective article (10 pages, 5 figures)", "summary": "Modern AI models, such as large language models, are usually trained once on\na huge corpus of data, potentially fine-tuned for a specific task, and then\ndeployed with fixed parameters. Their training is costly, slow, and gradual,\nrequiring billions of repetitions. In stark contrast, animals continuously\nadapt to the ever-changing contingencies in their environments. This is\nparticularly important for social species, where behavioral policies and reward\noutcomes may frequently change in interaction with peers. The underlying\ncomputational processes are often marked by rapid shifts in an animal's\nbehaviour and rather sudden transitions in neuronal population activity. Such\ncomputational capacities are of growing importance for AI systems operating in\nthe real world, like those guiding robots or autonomous vehicles, or for\nagentic AI interacting with humans online. Can AI learn from neuroscience? This\nPerspective explores this question, integrating the literature on continual and\nin-context learning in AI with the neuroscience of learning on behavioral tasks\nwith shifting rules, reward probabilities, or outcomes. We will outline an\nagenda for how specifically insights from neuroscience may inform current\ndevelopments in AI in this area, and - vice versa - what neuroscience may learn\nfrom AI, contributing to the evolving field of NeuroAI.", "AI": {"tldr": "This paper reviews the possibility of AI benefiting from neuroscience insights to enhance adaptability and flexibility in learning environments with shifting rules or outcomes.", "motivation": "AI systems typically lack the continuous adaptability seen in animals' brains, especially in dynamic and social contexts. The paper aims to explore how neuroscience can inform AI to overcome this limitation.", "method": "The paper integrates the literature on continual and in-context learning in AI with neuroscience findings about adaptive behavioral and neuronal mechanisms.", "result": "The paper identifies parallels between adaptive capabilities in animals and potential advancements in AI systems to operate in real-world settings.", "conclusion": "The paper highlights an agenda for mutual learning between neuroscience and AI, fostering the NeuroAI field to advance both domains effectively."}}
{"id": "2507.02331", "pdf": "https://arxiv.org/pdf/2507.02331", "abs": "https://arxiv.org/abs/2507.02331", "authors": ["Ana Nikolikj", "Mario Andr\u00e9s Mu\u00f1oz", "Eva Tuba", "Tome Eftimov"], "title": "Tracing the Interactions of Modular CMA-ES Configurations Across Problem Landscapes", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "This paper leverages the recently introduced concept of algorithm footprints\nto investigate the interplay between algorithm configurations and problem\ncharacteristics. Performance footprints are calculated for six modular variants\nof the CMA-ES algorithm (modCMA), evaluated on 24 benchmark problems from the\nBBOB suite, across two-dimensional settings: 5-dimensional and 30-dimensional.\nThese footprints provide insights into why different configurations of the same\nalgorithm exhibit varying performance and identify the problem features\ninfluencing these outcomes. Our analysis uncovers shared behavioral patterns\nacross configurations due to common interactions with problem properties, as\nwell as distinct behaviors on the same problem driven by differing problem\nfeatures. The results demonstrate the effectiveness of algorithm footprints in\nenhancing interpretability and guiding configuration choices.", "AI": {"tldr": "The paper examines algorithm footprints to analyze the relationship between algorithm configurations and problem properties using variants of CMA-ES on benchmark problems.", "motivation": "Understand how algorithm configurations interact with problem features and improve interpretability in performance outcomes.", "method": "Performance footprints are generated for six CMA-ES variants applied to benchmark problems, across dimensional settings of 5D and 30D.", "result": "Shared and distinct behavioral patterns across configurations are identified, driven by interactions and features of problems.", "conclusion": "Algorithm footprints are effective for understanding configurations and guiding choices for algorithm design."}}
{"id": "2507.02164", "pdf": "https://arxiv.org/pdf/2507.02164", "abs": "https://arxiv.org/abs/2507.02164", "authors": ["Ruibai Tang", "Chengbin Quan"], "title": "Hardware-Accelerated Algorithm for Complex Function Roots Density Graph Plotting", "categories": ["cs.AR"], "comment": null, "summary": "Solving and visualizing the potential roots of complex functions is essential\nin both theoretical and applied domains, yet often computationally intensive.\nWe present a hardware-accelerated algorithm for complex function roots density\ngraph plotting by approximating functions with polynomials and solving their\nroots using single-shift QR iteration. By leveraging the Hessenberg structure\nof companion matrices and optimizing QR decomposition with Givens rotations, we\ndesign a pipelined FPGA architecture capable of processing a large amount of\npolynomials with high throughput. Our implementation achieves up to 65x higher\nenergy efficiency than CPU-based approaches, and while it trails modern GPUs in\nperformance due to differences in fabrication technique.", "AI": {"tldr": "The paper introduces a hardware-accelerated algorithm and FPGA architecture to plot the density graph of complex function roots, achieving significant energy efficiency.", "motivation": "To address the computational inefficiency of solving and visualizing complex function roots in theoretical and applied contexts.", "method": "An algorithm is designed to approximate functions with polynomials, solve roots using single-shift QR iteration, and optimize QR decomposition via Givens rotations. A pipelined FPGA architecture is employed for hardware acceleration.", "result": "The FPGA implementation shows up to 65x greater energy efficiency compared to CPUs, though it is outperformed by GPUs due to fabrication differences.", "conclusion": "The proposed FPGA-based design is a highly energy-efficient solution with potential for large-scale polynomial processing, despite limitations in direct performance comparisons with GPUs."}}
{"id": "2507.02145", "pdf": "https://arxiv.org/pdf/2507.02145", "abs": "https://arxiv.org/abs/2507.02145", "authors": ["Keyan Jin", "Yapeng Wang", "Leonel Santos", "Tao Fang", "Xu Yang", "Sio Kei Im", "Hugo Gon\u00e7alo Oliveira"], "title": "Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Dialogue summarization is a challenging task with significant practical value\nin customer service, meeting analysis, and conversational AI. Although large\nlanguage models (LLMs) have achieved substantial progress in summarization\ntasks, the performance of step-by-step reasoning architectures-specifically\nLong Chain-of-Thought (CoT) implementations such as OpenAI-o1 and\nDeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent\nabstraction and conciseness. In this work, we present the first comprehensive\nand systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning\nLLMs across three major paradigms-generic, role-oriented, and query-oriented\ndialogue summarization. Our study spans diverse languages, domains, and summary\nlengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and\nadvanced evaluation protocols that include both LLM-based automatic metrics and\nhuman-inspired criteria. Contrary to trends in other reasoning-intensive tasks,\nour findings show that explicit stepwise reasoning does not consistently\nimprove dialogue summarization quality. Instead, reasoning LLMs are often prone\nto verbosity, factual inconsistencies, and less concise summaries compared to\ntheir non-reasoning counterparts. Through scenario-specific analyses and\ndetailed case studies, we further identify when and why explicit reasoning may\nfail to benefit-or even hinder-summarization in complex dialogue contexts. Our\nwork provides new insights into the limitations of current reasoning LLMs and\nhighlights the need for targeted modeling and evaluation strategies for\nreal-world dialogue summarization.", "AI": {"tldr": "This paper evaluates reasoning large language models (LLMs) and non-reasoning LLMs for dialogue summarization, finding that reasoning LLMs do not consistently outperform and often produce verbose and inconsistent results.", "motivation": "To investigate whether reasoning architectures like Long Chain-of-Thought (CoT) enhance dialogue summarization, a complex task requiring both abstraction and conciseness.", "method": "Conducted a systematic evaluation of reasoning and non-reasoning LLMs across generic, role-oriented, and query-oriented paradigms using benchmark datasets and advanced evaluation metrics.", "result": "Reasoning LLMs struggle with verbosity, factual inconsistencies, and less concise summaries compared to non-reasoning LLMs, contrary to their performance in other reasoning tasks.", "conclusion": "Reasoning LLMs exhibit limitations in dialogue summarization, and there is a need for specialized modeling and evaluation strategies tailored to this context."}}
{"id": "2507.02158", "pdf": "https://arxiv.org/pdf/2507.02158", "abs": "https://arxiv.org/abs/2507.02158", "authors": ["Jacob Roberts", "Blair Archibald", "Phil Trinder"], "title": "Signalling Health for Improved Kubernetes Microservice Availability", "categories": ["cs.DC"], "comment": "10 pages, 7 figures", "summary": "Microservices are often deployed and managed by a container orchestrator that\ncan detect and fix failures to maintain the service availability critical in\nmany applications. In Poll-based Container Monitoring (PCM), the orchestrator\nperiodically checks container health. While a common approach, PCM requires\ncareful tuning, may degrade service availability, and can be slow to detect\ncontainer health changes. An alternative is Signal-based Container Monitoring\n(SCM), where the container signals the orchestrator when its status changes. We\npresent the design, implementation, and evaluation of an SCM approach for\nKubernetes and empirically show that it has benefits over PCM, as predicted by\na new mathematical model. We compare the service availability of SCM and PCM\nover six experiments using the SockShop benchmark. SCM does not require that\npolling intervals are tuned, and yet detects container failure 86\\% faster than\nPCM and container readiness in a comparable time with limited resource\noverheads. We find PCM can erroneously detect failures, and this reduces\nservice availability by 4\\%. We propose that orchestrators offer SCM features\nfor faster failure detection than PCM without erroneous detections or careful\ntuning.", "AI": {"tldr": "The paper introduces Signal-based Container Monitoring (SCM) for Kubernetes and shows it outperforms traditional Poll-based Container Monitoring (PCM) methods in detecting container status changes faster and with fewer errors.", "motivation": "To improve the detection and handling of container health changes in orchestration environments, addressing limitations of the traditional Poll-based Container Monitoring (PCM) approach.", "method": "Designed, implemented, and evaluated a Signal-based Container Monitoring (SCM) system for Kubernetes. Conducted experiments using SockShop benchmark and utilized a mathematical model to compare SCM against PCM in terms of performance, speed, and accuracy.", "result": "SCM detects container failures 86% faster than PCM, identifies container readiness in comparable time, and exhibits lower erroneous failure detections, improving service availability by 4%. It also avoids the need for tuning polling intervals.", "conclusion": "SCM offers faster and more accurate container health monitoring over PCM without requiring complex tuning or introducing significant resource overheads. It is recommended that orchestrators adopt SCM for enhanced service availability."}}
{"id": "2507.02107", "pdf": "https://arxiv.org/pdf/2507.02107", "abs": "https://arxiv.org/abs/2507.02107", "authors": ["Ben Limpanukorn", "Yanjun Wang", "Zach Patterson", "Pranav Garg", "Murali Krishna Ramanathan", "Xiaofei Ma", "Anoop Deoras", "Miryung Kim"], "title": "Structural Code Search using Natural Language Queries", "categories": ["cs.SE", "cs.PL"], "comment": null, "summary": "Searching code is a common task that developers perform to understand APIs,\nlearn common code patterns, and navigate code. Currently, developers most\ncommonly search using keywords and regular expressions that are easy to use and\nwidely available. Beyond keywords and regular expressions, structural code\nsearch tools allow developers to search for code based on its syntactic\nstructure. This has numerous applications ranging from bug finding to\nsystematically refactoring code. However, these structural code search tools\noperate on queries expressed in domain-specific languages (DSL) that can be\ndifficult to learn and write. We propose to allow developers to use natural\nlanguage to search for code structurally. Expressing queries in natural\nlanguage provides an intuitive way to search for code and lowers the barrier to\nentry.\n  In this work, we develop a novel general approach that combines the reasoning\ncapabilities of an LLM to interpret natural language search queries with the\npower of structural search tools to efficiently and accurately retrieve\nrelevant code. We then instantiate this approach for two structural code search\nDSLs: Semgrep and GQL. In our evaluation, we construct a new benchmark for\nstructural code search consisting of 400 queries over 10 Java projects. We show\nthat our approach for structural code search based on translating NL queries to\nDSL queries using an LLM is effective and robust, achieving a high precision\nand recall ranging from 55% - 70%. Further, our approach significantly\noutperforms baselines based on semantic code search and LLM retrievals by up to\n57% and 14% on F1 scores.", "AI": {"tldr": "The paper proposes using natural language to search for code structurally, combining large language models (LLMs) with existing structural search tools like Semgrep and GQL, achieving better precision and recall over traditional methods.", "motivation": "Structural code search tools allow developers to query code based on syntax but are hindered by the complexity of domain-specific languages (DSLs). This work seeks to lower the entry barrier by enabling natural language queries.", "method": "A system is developed that translates natural language queries into DSL queries using the reasoning power of LLMs and integrates these with structural search tools. The approach is evaluated using a newly constructed benchmark of 400 queries across 10 Java projects.", "result": "The proposed approach achieves high precision and recall between 55% - 70% and outperforms baselines like semantic search and LLM retrievals by up to 57% and 14% in F1 scores.", "conclusion": "Translating natural language queries to DSL with LLMs is an effective way to make structural code search more accessible and robust, offering significant performance improvements."}}
{"id": "2507.01967", "pdf": "https://arxiv.org/pdf/2507.01967", "abs": "https://arxiv.org/abs/2507.01967", "authors": ["Llewellin RG Jegels"], "title": "Ghost in the Machine: Examining the Philosophical Implications of Recursive Algorithms in Artificial Intelligence Systems", "categories": ["q-bio.NC"], "comment": "27 pages", "summary": "This paper investigates whether contemporary AI architectures employing deep\nrecursion, meta-learning, and self-referential mechanisms provide evidence of\nmachine consciousness. Integrating philosophical history, cognitive science,\nand AI engineering, it situates recursive algorithms within a lineage spanning\nCartesian dualism, Husserlian intentionality, Integrated Information Theory,\nthe Global Workspace model, and enactivist perspectives. The argument proceeds\nthrough textual analysis, comparative architecture review, and synthesis of\nneuroscience findings on integration and prediction. Methodologically, the\nstudy combines conceptual analysis, case studies, and normative risk assessment\ninformed by phenomenology and embodied cognition. Technical examples, including\ntransformer self-attention, meta-cognitive agents, and neuromorphic chips,\nillustrate how functional self-modeling can arise without subjective\nexperience. By distinguishing functional from phenomenal consciousness, the\npaper argues that symbol grounding, embodiment, and affective qualia remain\nunresolved barriers to attributing sentience to current AI. Ethical analysis\nexplores risks of premature anthropomorphism versus neglect of future sentient\nsystems; legal implications include personhood, liability, authorship, and\nlabor impacts. Future directions include quantum architectures, embodied\nrobotics, unsupervised world modeling, and empirical tests for non-biological\nphenomenality. The study reframes the \"hard problem\" as a graded and\nincreasingly testable phenomenon, rather than a metaphysical impasse. It\nconcludes that recursive self-referential design enhances capability but does\nnot entail consciousness or justify moral status. Keywords: Recursive\nalgorithms; self-reference; machine consciousness; AI ethics; AI consciousness", "AI": {"tldr": "The paper assesses the possibility of machine consciousness in current AI systems utilizing recursion, meta-learning, and self-referential mechanisms, concluding that they lack subjective experience.", "motivation": "The study aims to clarify whether advanced AI architectures demonstrate machine consciousness, addressing unresolved challenges like symbol grounding and affective qualia.", "method": "Integrating philosophical, cognitive science, and engineering perspectives, the study uses conceptual analysis, textual review, case studies, and neuroscientific synthesis.", "result": "The paper finds that while AI systems can achieve functional self-modeling, they do not exhibit phenomenal consciousness, posing ethical and legal challenges but suggesting future research directions.", "conclusion": "Recursive self-referential designs improve AI capabilities but fail to equate to consciousness, leaving moral status and other related debates unresolved."}}
{"id": "2507.02029", "pdf": "https://arxiv.org/pdf/2507.02029", "abs": "https://arxiv.org/abs/2507.02029", "authors": ["BAAI RoboBrain Team", "Mingyu Cao", "Huajie Tan", "Yuheng Ji", "Minglan Lin", "Zhiyu Li", "Zhou Cao", "Pengwei Wang", "Enshen Zhou", "Yi Han", "Yingbo Tang", "Xiangqi Xu", "Wei Guo", "Yaoxu Lyu", "Yijie Xu", "Jiayu Shi", "Cheng Chi", "Mengdi Zhao", "Xiaoshuai Hao", "Shanyu Rong", "Zhengliang Cai", "Bolun Zhang", "Shuyi Zhang", "Huaihai Lyu", "Mengfei Du", "Lingfeng Zhang", "Xi Feng", "Xiaodan Liu", "Yance Jiao", "Chenrui He", "Mengsi Lyu", "Zhuo Chen", "Yulong Ao", "Xue Sun", "Zheqi He", "Jingshu Zheng", "Xi Yang", "Donghai Shi", "Kunchang Xie", "Bochao Zhang", "Shaokai Nie", "Chunlei Men", "Yonghua Lin", "Zhongyuan Wang", "Tiejun Huang", "Shanghang Zhang"], "title": "RoboBrain 2.0 Technical Report", "categories": ["cs.RO"], "comment": null, "summary": "We introduce RoboBrain 2.0, our latest generation of embodied vision-language\nfoundation models, designed to unify perception, reasoning, and planning for\ncomplex embodied tasks in physical environments. It comes in two variants: a\nlightweight 7B model and a full-scale 32B model, featuring a heterogeneous\narchitecture with a vision encoder and a language model. Despite its compact\nsize, RoboBrain 2.0 achieves strong performance across a wide spectrum of\nembodied reasoning tasks. On both spatial and temporal benchmarks, the 32B\nvariant achieves leading results, surpassing prior open-source and proprietary\nmodels. In particular, it supports key real-world embodied AI capabilities,\nincluding spatial understanding (e.g., affordance prediction, spatial\nreferring, trajectory forecasting) and temporal decision-making (e.g.,\nclosed-loop interaction, multi-agent long-horizon planning, and scene graph\nupdating). This report details the model architecture, data construction,\nmulti-stage training strategies, infrastructure and practical applications. We\nhope RoboBrain 2.0 advances embodied AI research and serves as a practical step\ntoward building generalist embodied agents. The code, checkpoint and benchmark\nare available at https://superrobobrain.github.io.", "AI": {"tldr": "The paper introduces RoboBrain 2.0, a new embodied vision-language foundation model with 7B and 32B variants, achieving strong results in spatial and temporal embodied AI tasks.", "motivation": "To unify perception, reasoning, and planning for complex tasks in physical environments with an advanced embodied AI model.", "method": "Developed two model variants, a 7B and a 32B architecture, combining a vision encoder and a language model, trained on custom datasets using multi-stage strategies.", "result": "The 32B variant surpasses prior state-of-the-art models in spatial understanding and temporal decision-making benchmarks.", "conclusion": "RoboBrain 2.0 represents progress towards generalist embodied agents, advancing embodied AI research and offering practical applications with open-access resources."}}
{"id": "2507.02215", "pdf": "https://arxiv.org/pdf/2507.02215", "abs": "https://arxiv.org/abs/2507.02215", "authors": ["Ben Adcock", "Bernhard Hientzsch", "Akil Narayan", "Yiming Xu"], "title": "Hybrid least squares for learning functions from highly noisy data", "categories": ["stat.ML", "cs.LG", "cs.NA", "math.NA"], "comment": "30 pages", "summary": "Motivated by the need for efficient estimation of conditional expectations,\nwe consider a least-squares function approximation problem with heavily\npolluted data. Existing methods that are powerful in the small noise regime are\nsuboptimal when large noise is present. We propose a hybrid approach that\ncombines Christoffel sampling with certain types of optimal experimental design\nto address this issue. We show that the proposed algorithm enjoys appropriate\noptimality properties for both sample point generation and noise mollification,\nleading to improved computational efficiency and sample complexity compared to\nexisting methods. We also extend the algorithm to convex-constrained settings\nwith similar theoretical guarantees. When the target function is defined as the\nexpectation of a random field, we extend our approach to leverage adaptive\nrandom subspaces and establish results on the approximation capacity of the\nadaptive procedure. Our theoretical findings are supported by numerical studies\non both synthetic data and on a more challenging stochastic simulation problem\nin computational finance.", "AI": {"tldr": "This paper proposes a new hybrid algorithm combining Christoffel sampling and optimal experimental design for least-squares function approximation under heavy noise, yielding improved efficiency and theoretical guarantees.", "motivation": "Efficiently estimating conditional expectations under heavily polluted data remains challenging, as current methods excel in small noise scenarios but falter under larger noise.", "method": "A hybrid approach is developed by integrating Christoffel sampling with optimal experimental design. The algorithm is extended to convex-constrained settings and adaptive random subspaces.", "result": "The proposed method improves computational efficiency and sample complexity, while theoretical findings are validated using numerical studies in synthetic and computational finance datasets.", "conclusion": "The hybrid approach addresses heavy noise limitations, ensures theoretical guarantees, and demonstrates practical utility across different applications."}}
{"id": "2507.02148", "pdf": "https://arxiv.org/pdf/2507.02148", "abs": "https://arxiv.org/abs/2507.02148", "authors": ["Zijie Cai", "Christopher Metzler"], "title": "Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Monocular depth estimation has recently advanced to provide not only relative\nbut also metric depth predictions. However, its reliability in underwater\nenvironments remains limited due to light attenuation and scattering, color\ndistortion, turbidity, and the lack of high-quality metric ground-truth data.\nIn this paper, we present a comprehensive benchmark of zero-shot and fine-tuned\nmonocular metric depth estimation models on real-world underwater datasets with\nmetric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of\nstate-of-the-art models across a range of underwater conditions with different\nranges. Our results show that large-scale models trained on terrestrial (real\nor synthetic) data, while effective in in-air settings, perform poorly\nunderwater due to significant domain shifts. To address this, we fine-tune\nDepth Anything V2 with a ViT-S backbone encoder on a synthetic underwater\nvariant of the Hypersim dataset, which we generated using a physically based\nunderwater image formation model. We demonstrate our fine-tuned model\nconsistently improves performance across all benchmarks and outperforms\nbaselines trained only on the clean in-air Hypersim dataset. Our study provides\na detailed evaluation and visualization for monocular metric depth estimation\nin underwater scenes, highlighting the importance of domain adaptation and\nscale-aware supervision for achieving robust and generalizable metric depth\npredictions in challenging underwater environments for future research.", "AI": {"tldr": "This paper evaluates depth estimation models in underwater environments, highlighting challenges posed by domain shifts and proposing fine-tuning techniques using synthetic underwater data to improve performance.", "motivation": "Monocular depth estimation struggles with underwater environments due to domain-specific issues such as light distortion, turbidity, and lack of high-quality ground-truth data.", "method": "The authors benchmark existing models on underwater datasets and fine-tune the Depth Anything V2 model using a synthetic underwater dataset generated with a physically based underwater image formation model.", "result": "Fine-tuned models demonstrated consistent performance improvements across underwater benchmarks, outperforming baseline models trained on terrestrial data.", "conclusion": "Domain adaptation and scale-aware supervision are critical for developing robust underwater metric depth estimation models for future research advancement."}}
{"id": "2507.01982", "pdf": "https://arxiv.org/pdf/2507.01982", "abs": "https://arxiv.org/abs/2507.01982", "authors": ["Siqing Long", "Xiangzhi Huang", "Jiemin Xie", "Ming Cai"], "title": "DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": "39 pages, 14 figures", "summary": "Accurate traffic demand forecasting enables transportation management\ndepartments to allocate resources more effectively, thereby improving their\nutilization efficiency. However, complex spatiotemporal relationships in\ntraffic systems continue to limit the performance of demand forecasting models.\nTo improve the accuracy of spatiotemporal traffic demand prediction, we propose\na new graph convolutional network structure called DKGCM. Specifically, we\nfirst consider the spatial flow distribution of different traffic nodes and\npropose a novel temporal similarity-based clustering graph convolution method,\nDK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering\nto group traffic nodes and more effectively capture spatial dependencies. On\nthe temporal scale, we integrate the Fast Fourier Transform (FFT) within the\nbidirectional Mamba deep learning framework to capture temporal dependencies in\ntraffic demand. To further optimize model training, we incorporate the GRPO\nreinforcement learning strategy to enhance the loss function feedback\nmechanism. Extensive experiments demonstrate that our model outperforms several\nadvanced methods and achieves strong results on three public datasets.", "AI": {"tldr": "A novel graph convolutional network structure (DKGCM) is introduced for accurate traffic demand forecasting, leveraging techniques like Dynamic Time Warping, clustering, and reinforcement learning.", "motivation": "The motivation is to address the performance limitations of traffic demand forecasting models caused by complex spatiotemporal relationships in traffic systems, enabling better resource allocation.", "method": "The study introduces DK-GCN, combining Dynamic Time Warping and K-means clustering for spatial dependencies and integrates FFT with a bidirectional Mamba framework for temporal dependencies. GRPO reinforcement learning is used for optimizing training.", "result": "Experiments show the proposed DKGCM model outperforms advanced methods, achieving strong results on three public datasets.", "conclusion": "The DKGCM model improves accuracy in spatiotemporal traffic demand prediction, demonstrating effectiveness through robust evaluations."}}
{"id": "2507.02152", "pdf": "https://arxiv.org/pdf/2507.02152", "abs": "https://arxiv.org/abs/2507.02152", "authors": ["Disa Sariola", "Patrick Button", "Aron Culotta", "Nicholas Mattei"], "title": "The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies", "categories": ["cs.AI"], "comment": null, "summary": "Artificial intelligence systems, especially those using machine learning, are\nbeing deployed in domains from hiring to loan issuance in order to automate\nthese complex decisions. Judging both the effectiveness and fairness of these\nAI systems, and their human decision making counterpart, is a complex and\nimportant topic studied across both computational and social sciences. Within\nmachine learning, a common way to address bias in downstream classifiers is to\nresample the training data to offset disparities. For example, if hiring rates\nvary by some protected class, then one may equalize the rate within the\ntraining set to alleviate bias in the resulting classifier. While simple and\nseemingly effective, these methods have typically only been evaluated using\ndata obtained through convenience samples, introducing selection bias and label\nbias into metrics. Within the social sciences, psychology, public health, and\nmedicine, audit studies, in which fictitious ``testers'' (e.g., resumes,\nemails, patient actors) are sent to subjects (e.g., job openings, businesses,\ndoctors) in randomized control trials, provide high quality data that support\nrigorous estimates of discrimination. In this paper, we investigate how data\nfrom audit studies can be used to improve our ability to both train and\nevaluate automated hiring algorithms. We find that such data reveals cases\nwhere the common fairness intervention method of equalizing base rates across\nclasses appears to achieve parity using traditional measures, but in fact has\nroughly 10% disparity when measured appropriately. We additionally introduce\ninterventions based on individual treatment effect estimation methods that\nfurther reduce algorithmic discrimination using this data.", "AI": {"tldr": "The paper explores the use of high-quality data from audit studies for better training and evaluation of automated hiring systems, revealing limitations in conventional fairness methods and proposing improved interventions.", "motivation": "AI-driven hiring systems often face issues of bias and fairness, and the paper seeks to address these challenges by leveraging high-quality data from audit studies.", "method": "The authors analyze data gathered from audit studies and evaluate fairness interventions, such as resampling training data, while proposing improved methods to measure algorithmic discrimination.", "result": "The study finds that traditional fairness interventions fail to eliminate discrimination, revealing approximately 10% disparity when measured with audit study data.", "conclusion": "Audit study data can enhance fairness evaluations and offer ways to reduce discrimination in automated hiring algorithms, improving their reliability and trustworthiness."}}
{"id": "2507.02337", "pdf": "https://arxiv.org/pdf/2507.02337", "abs": "https://arxiv.org/abs/2507.02337", "authors": ["Gjorgjina Cenikj", "Ga\u0161per Petelin", "Tome Eftimov"], "title": "ClustOpt: A Clustering-based Approach for Representing and Visualizing the Search Dynamics of Numerical Metaheuristic Optimization Algorithms", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Understanding the behavior of numerical metaheuristic optimization algorithms\nis critical for advancing their development and application. Traditional\nvisualization techniques, such as convergence plots, trajectory mapping, and\nfitness landscape analysis, often fall short in illustrating the structural\ndynamics of the search process, especially in high-dimensional or complex\nsolution spaces. To address this, we propose a novel representation and\nvisualization methodology that clusters solution candidates explored by the\nalgorithm and tracks the evolution of cluster memberships across iterations,\noffering a dynamic and interpretable view of the search process. Additionally,\nwe introduce two metrics - algorithm stability and algorithm similarity- to\nquantify the consistency of search trajectories across runs of an individual\nalgorithm and the similarity between different algorithms, respectively. We\napply this methodology to a set of ten numerical metaheuristic algorithms,\nrevealing insights into their stability and comparative behaviors, thereby\nproviding a deeper understanding of their search dynamics.", "AI": {"tldr": "This paper introduces a novel methodology for visualizing and analyzing numerical metaheuristic optimization algorithms, using clustering and metrics that reveal algorithm stability and similarity.", "motivation": "Existing visualization techniques for numerical metaheuristic optimization algorithms are insufficient to capture search dynamics in high-dimensional or complex solution spaces.", "method": "The authors propose clustering solution candidates from the algorithms, tracking cluster memberships dynamically, and introducing metrics for algorithm stability and similarity.", "result": "The methodology was applied to ten numerical metaheuristic algorithms, offering insights into their stability and comparative behaviors.", "conclusion": "The approach enhances the understanding of search dynamics, providing interpretable insights for advancing algorithm development and evaluation."}}
{"id": "2507.02456", "pdf": "https://arxiv.org/pdf/2507.02456", "abs": "https://arxiv.org/abs/2507.02456", "authors": ["Wenzhe Guo", "Joyjit Kundu", "Uras Tos", "Weijiang Kong", "Giuliano Sisto", "Timon Evenblij", "Manu Perumkunnil"], "title": "System-performance and cost modeling of Large Language Model training and inference", "categories": ["cs.AR"], "comment": null, "summary": "Large language models (LLMs), based on transformer architectures, have\nrevolutionized numerous domains within artificial intelligence, science, and\nengineering due to their exceptional scalability and adaptability. However, the\nexponential growth in LLM size and complexity has outpaced advancements in\ncompute capacity, memory bandwidth, network performance, and cost efficiency,\nposing significant challenges to their scalability on distributed systems. To\naddress these limitations, alternative model architectures, optimization\nstrategies, communication-aware network topologies, and novel system design\napproaches have been proposed in literature. This paper introduces a\nperformance-cost modeling methodology for LLM training and inference that\nintegrates state-of-the-art compute techniques with memory optimizations, and\nlatest communication techniques. Building on an analytical performance model,\nour approach incorporates recent innovations such as the flash attention\ntechnique and mixture of experts models to address the memory bandwidth and\ncompute bottlenecks. It also considers the impact of different network\ntopologies and topology-specific communication algorithms with 5D parallellism.\nThe framework also integrates a chiplet cost model. The proposed modeling\nmethodology provides valuable insights to guide future compute system design\nand facilitates hardware-software co-development, in particular due to its\nability to analyze performance-cost trade-offs for various system architectural\nconfigurations.", "AI": {"tldr": "The paper presents a performance-cost modeling methodology to optimize training and inference of large language models (LLMs) by addressing memory and compute bottlenecks, utilizing state-of-the-art techniques like flash attention and 5D parallelism.", "motivation": "The exponential growth in LLM size and complexity has outpaced advancements in computing resources, creating challenges in scalability and cost efficiency.", "method": "An analytical performance model combining state-of-the-art compute techniques, memory optimizations, and advanced communication techniques such as flash attention, mixture of experts, and 5D parallelism, along with an integrated chiplet cost model.", "result": "The methodology provides valuable analysis of performance-cost trade-offs for system architectures and considers novel computational advancements and network topologies.", "conclusion": "The proposed methodology facilitates better design of future compute systems and hardware-software co-development, addressing key bottlenecks in LLM scalability."}}
{"id": "2507.02199", "pdf": "https://arxiv.org/pdf/2507.02199", "abs": "https://arxiv.org/abs/2507.02199", "authors": ["Wenquan Lu", "Yuechuan Yang", "Kyle Lee", "Yanshu Li", "Enqi Liu"], "title": "Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Chain-of-thought (CoT) reasoning has enabled transformer-based language\nmodels to excel at complex mathematics and multi-step planning. However, in\nstandard decoder-only architectures, these reasoning steps are externalized in\nnatural language, improving interpretability at the cost of efficiency. To\ncapture reasoning that is not easily represented in words, many works have\nexplored recurrent architectures that aim to internalize reasoning in latent\nspace, potentially supporting latent CoT. In this paper, we investigate whether\nsuch reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer\nthat reuses layers at inference time without increasing parameter count. We\nexamine the model's internal behavior on arithmetic tasks using a suite of\nprobing techniques including the Logit Lens and Coda Lens. Our findings reveal\nlimited evidence of interpretable latent CoT by tracking rank trajectories of\nfinal and intermediate result tokens. Furthermore, we uncover significant\nprobing inconsistencies across recurrent blocks, where the interpretability of\nhidden states depends heavily on both the layer index and the decoding method.\nFinally, we empirically show that increasing recurrence depth yields only\nmarginal gains and falls well short of models that explicitly externalize\nreasoning steps. The code is available at\nhttps://github.com/wenquanlu/huginn-latent-cot.", "AI": {"tldr": "This paper evaluates whether depth-recurrent Transformers like Huginn-3.5B demonstrate Chain-of-Thought (CoT) reasoning internally without explicit natural language output. Findings reveal limited latent CoT reasoning, inconsistencies across recurrent blocks, and minimal performance gains from deeper recurrence.", "motivation": "To explore the potential of depth-recurrent Transformer architectures in capturing latent Chain-of-Thought (CoT) reasoning, which could bypass the efficiency trade-offs of externalizing reasoning in natural language.", "method": "The study analyzes Huginn-3.5B, a depth-recurrent Transformer, using techniques such as Logit Lens and Coda Lens to probe its internal behavior and reasoning capabilities on arithmetic tasks.", "result": "The results show limited latent CoT reasoning, uncover significant inconsistencies across recurrent blocks in interpretability, and demonstrate that deeper recurrence depth provides marginal performance improvements compared to explicit reasoning externalization.", "conclusion": "Depth-recurrent Transformers like Huginn-3.5B offer limited advantages for internalized reasoning tasks when compared to standard architectures that externalize reasoning, suggesting inefficiency and inconsistency in their current design."}}
{"id": "2507.02233", "pdf": "https://arxiv.org/pdf/2507.02233", "abs": "https://arxiv.org/abs/2507.02233", "authors": ["Bruce Fang", "Danyi Gao"], "title": "Domain-Adversarial Transfer Learning for Fault Root Cause Identification in Cloud Computing Systems", "categories": ["cs.DC"], "comment": null, "summary": "This paper addresses the challenge of fault root cause identification in\ncloud computing environments. The difficulty arises from complex system\nstructures, dense service coupling, and limited fault information. To solve\nthis problem, an intelligent identification algorithm based on transfer\nlearning is proposed. The method introduces a shared feature extraction module\nand a domain adversarial mechanism to enable effective knowledge transfer from\nthe source domain to the target domain. This improves the model's\ndiscriminative ability and generalization performance in the target domain. The\nmodel incorporates a pseudo-label selection strategy. When labeled samples are\nlacking in the target domain, high-confidence predictions are used in training.\nThis enhances the model's ability to recognize minority classes. To evaluate\nthe stability and adaptability of the method in real-world scenarios,\nexperiments are designed under three conditions: label scarcity, class\nimbalance, and heterogeneous node environments. Experimental results show that\nthe proposed method outperforms existing mainstream approaches in several key\nmetrics, including accuracy, F1-Score, and AUC. The model demonstrates stronger\ndiscriminative power and robustness. Notably, under extreme class imbalance and\nsignificant structural differences in the target domain, the model still\nmaintains high performance. This validates the effectiveness and practical\nvalue of the proposed mechanisms in complex cloud computing systems.", "AI": {"tldr": "The paper proposes a transfer learning-based algorithm for fault root cause identification in cloud computing, addressing challenges like complex system structures and limited fault information. It outperforms existing methods under various realistic conditions.", "motivation": "Fault root cause identification in cloud computing is challenging due to complex structures, dense service coupling, and scarce fault information. Existing methods struggle in such scenarios.", "method": "The paper introduces a method based on transfer learning with shared feature extraction modules and domain adversarial mechanisms. It uses pseudo-label selection to enhance performance under limited labeled data.", "result": "Experimental results demonstrate superior performance of the proposed method compared to mainstream approaches, excelling in accuracy, F1-Score, and AUC, even under extreme imbalance and structural differences.", "conclusion": "The proposed method is effective and robust for fault root cause identification in complex cloud computing environments, showing practical value for real-world applications."}}
{"id": "2507.02064", "pdf": "https://arxiv.org/pdf/2507.02064", "abs": "https://arxiv.org/abs/2507.02064", "authors": ["Zhaoze Wang", "Genela Morris", "Dori Derdikman", "Pratik Chaudhari", "Vijay Balasubramanian"], "title": "REMI: Reconstructing Episodic Memory During Intrinsic Path Planning", "categories": ["q-bio.NC"], "comment": null, "summary": "Grid cells in the medial entorhinal cortex (MEC) are believed to path\nintegrate speed and direction signals to activate at triangular grids of\nlocations in an environment, thus implementing a population code for position.\nIn parallel, place cells in the hippocampus (HC) fire at spatially confined\nlocations, with selectivity tuned not only to allocentric position but also to\nenvironmental contexts, such as sensory cues. Although grid and place cells\nboth encode spatial information and support memory for multiple locations, why\nanimals maintain two such representations remains unclear. Noting that place\nrepresentations seem to have other functional roles in intrinsically motivated\ntasks such as recalling locations from sensory cues, we propose that animals\nmaintain grid and place representations together to support planning.\nSpecifically, we posit that place cells auto-associate not only sensory\ninformation relayed from the MEC but also grid cell patterns, enabling recall\nof goal location grid patterns from sensory and motivational cues, permitting\nsubsequent planning with only grid representations. We extend a previous\ntheoretical framework for grid-cell-based planning and show that local\ntransition rules can generalize to long-distance path forecasting. We further\nshow that a planning network can sequentially update grid cell states toward\nthe goal. During this process, intermediate grid activity can trigger place\ncell pattern completion, reconstructing experiences along the planned path. We\ndemonstrate all these effects using a single-layer RNN that simultaneously\nmodels the HC-MEC loop and the planning subnetwork. We show that such recurrent\nmechanisms for grid cell-based planning, with goal recall driven by the place\nsystem, make several characteristic, testable predictions.", "AI": {"tldr": "This paper explores how both grid cells in the medial entorhinal cortex (MEC) and place cells in the hippocampus (HC) contribute to spatial navigation and planning, proposing a unified framework.", "motivation": "Although grid and place cells both support spatial memory and information processing, it remains unclear why animals use two distinct spatial representations.", "method": "The authors developed a single-layer recurrent neural network (RNN) model to simulate the HC-MEC loop and planning network. This model incorporates interactions between grid and place cells for goal-directed planning.", "result": "The study demonstrates that local transition rules can generalize to long-distance planning and that sequential updates in the grid cell network can assist in path forecasting. Place cell activity complements this process by reconstructing experiences along the selected path.", "conclusion": "Grid and place cells synergistically support planning by leveraging their distinct yet interrelated roles in spatial representation. The proposed mechanism also makes specific, testable predictions regarding their functional interactions."}}
{"id": "2507.02110", "pdf": "https://arxiv.org/pdf/2507.02110", "abs": "https://arxiv.org/abs/2507.02110", "authors": ["Md Nahidul Islam Opu", "Fatima Islam Mouri", "Rick Kazman", "Yuanfang Cai", "Shaiful Chowdhury"], "title": "Can Internal Software Metrics Predict App Popularity at Launch? Yeas! and Nays!", "categories": ["cs.SE"], "comment": null, "summary": "Predicting mobile app popularity before release can provide developers with a\nstrategic advantage in a competitive marketplace, yet it remains a challenging\nproblem. This study explores whether internal software metrics, measurable from\nsource code before deployment, can predict an app's popularity, defined by user\nratings (calculated from user reviews) and DownloadsPerYear (yearly downloads).\nUsing a dataset of 446 open-source Android apps from F-Droid, we extract a wide\narray of features, including system-, class-, and method-level code metrics,\ncode smells, and app metadata. Additional information, such as user reviews,\ndownload counts, and uses-permission, was collected from the Google Play Store.\nWe evaluate regression and classification models across three feature sets: a\nminimal Size-only baseline, a domain-informed Handpicked set, and a Voting set\nderived via feature selection algorithms. Regression models perform poorly due\nto skewed data, with low $R^2$ scores. However, when reframed as binary\nclassification (Popular vs. Unpopular), results improve significantly. The best\nmodel, a Multilayer Perceptron using the Voting set, achieves F1-scores of\n0.72. These results suggest that internal code metrics, although limited in\ntheir explanatory power, can serve as useful indicators of app popularity. This\nchallenges earlier findings that dismissed internal metrics as predictors of\nsoftware quality.", "AI": {"tldr": "The study investigates if internal software metrics (from source code) can predict mobile app popularity, attaining improved results with classification models.", "motivation": "The paper aims to help developers strategize by effectively predicting app popularity using pre-release internal metrics, overcoming challenges posed by competitive markets.", "method": "A dataset of 446 open-source Android apps from F-Droid was analyzed using various code and metadata metrics. Regression models and binary classification models (e.g., Multilayer Perceptron) evaluated different feature sets.", "result": "Regression models yielded poor performance due to skewed data. Binary classification significantly improved results (F1-score of 0.72 with Multilayer Perceptron using Voting set features).", "conclusion": "Internal code metrics can moderately predict app popularity, contradicting earlier studies that dismissed these metrics as irrelevant for software quality indicators."}}
{"id": "2507.02171", "pdf": "https://arxiv.org/pdf/2507.02171", "abs": "https://arxiv.org/abs/2507.02171", "authors": ["Miroslav Cibula", "Krist\u00edna Malinovsk\u00e1", "Matthias Kerzel"], "title": "Towards Bio-Inspired Robotic Trajectory Planning via Self-Supervised RNN", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "12 pages, 4 figures, 2 tables. To be published in 2025 International\n  Conference on Artificial Neural Networks (ICANN) proceedings. This research\n  was funded by the Horizon Europe project TERAIS, GA no. 101079338, and in\n  part by the Slovak Grant Agency for Science (VEGA), project 1/0373/23", "summary": "Trajectory planning in robotics is understood as generating a sequence of\njoint configurations that will lead a robotic agent, or its manipulator, from\nan initial state to the desired final state, thus completing a manipulation\ntask while considering constraints like robot kinematics and the environment.\nTypically, this is achieved via sampling-based planners, which are\ncomputationally intensive. Recent advances demonstrate that trajectory planning\ncan also be performed by supervised sequence learning of trajectories, often\nrequiring only a single or fixed number of passes through a neural\narchitecture, thus ensuring a bounded computation time. Such fully supervised\napproaches, however, perform imitation learning; they do not learn based on\nwhether the trajectories can successfully reach a goal, but try to reproduce\nobserved trajectories. In our work, we build on this approach and propose a\ncognitively inspired self-supervised learning scheme based on a recurrent\narchitecture for building a trajectory model. We evaluate the feasibility of\nthe proposed method on a task of kinematic planning for a robotic arm. The\nresults suggest that the model is able to learn to generate trajectories only\nusing given paired forward and inverse kinematics models, and indicate that\nthis novel method could facilitate planning for more complex manipulation tasks\nrequiring adaptive solutions.", "AI": {"tldr": "This paper introduces a cognitively inspired self-supervised learning model for robotic trajectory planning, simplifying planning for complex tasks.", "motivation": "Traditional trajectory planning relies heavily on computationally intensive sampling-based methods or supervised sequence learning techniques, which have limitations like imitation without goal validation.", "method": "The authors propose a novel self-supervised learning approach utilizing a recurrent neural network to model trajectories based on paired forward and inverse kinematics.", "result": "The proposed model successfully learns to generate effective trajectories for robotic arms using paired kinematic models.", "conclusion": "The method shows promise in simplifying trajectory planning for complex robotic tasks, demonstrating adaptability and effectiveness."}}
{"id": "2507.02248", "pdf": "https://arxiv.org/pdf/2507.02248", "abs": "https://arxiv.org/abs/2507.02248", "authors": ["Dali Liu", "Haolei Weng"], "title": "Transfer Learning for Matrix Completion", "categories": ["stat.ML", "cs.LG", "15A83", "I.2.6; G.3"], "comment": "37 pages, 1 figure", "summary": "In this paper, we explore the knowledge transfer under the setting of matrix\ncompletion, which aims to enhance the estimation of a low-rank target matrix\nwith auxiliary data available. We propose a transfer learning procedure given\nprior information on which source datasets are favorable. We study its\nconvergence rates and prove its minimax optimality. Our analysis reveals that\nwith the source matrices close enough to the target matrix, out method\noutperforms the traditional method using the single target data. In particular,\nwe leverage the advanced sharp concentration inequalities introduced in\n\\cite{brailovskaya2024universality} to eliminate a logarithmic factor in the\nconvergence rate, which is crucial for proving the minimax optimality. When the\nrelevance of source datasets is unknown, we develop an efficient detection\nprocedure to identify informative sources and establish its selection\nconsistency. Simulations and real data analysis are conducted to support the\nvalidity of our methodology.", "AI": {"tldr": "The paper introduces a transfer learning approach for matrix completion to improve low-rank matrix estimation using auxiliary data, proving its theoretical effectiveness and practical utility.", "motivation": "To improve the accuracy of estimating low-rank matrices by effectively utilizing relevant auxiliary data in matrix completion.", "method": "The paper proposes a transfer learning procedure leveraging favorable source datasets and addresses model convergence rates, using advanced sharp concentration inequalities. Additionally, a detection procedure identifies relevant source datasets when prior information is absent.", "result": "The proposed method outperforms traditional single target methods when source matrices are sufficiently related to the target, achieving minimax optimality in convergence rates and selection consistency for source datasets.", "conclusion": "The methodology demonstrates theoretical robustness and practical feasibility, supported by simulations and real data analysis, establishing its superiority in matrix completion tasks with relevant auxiliary data."}}
{"id": "2507.02200", "pdf": "https://arxiv.org/pdf/2507.02200", "abs": "https://arxiv.org/abs/2507.02200", "authors": ["Xiao Wang", "Jingtao Jiang", "Qiang Chen", "Lan Chen", "Lin Zhu", "Yaowei Wang", "Yonghong Tian", "Jin Tang"], "title": "ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "A Strong Baseline for Reasoning based Event Stream Scene Text\n  Recognition", "summary": "Event stream based scene text recognition is a newly arising research topic\nin recent years which performs better than the widely used RGB cameras in\nextremely challenging scenarios, especially the low illumination, fast motion.\nExisting works either adopt end-to-end encoder-decoder framework or large\nlanguage models for enhanced recognition, however, they are still limited by\nthe challenges of insufficient interpretability and weak contextual logical\nreasoning. In this work, we propose a novel chain-of-thought reasoning based\nevent stream scene text recognition framework, termed ESTR-CoT. Specifically,\nwe first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input\nevent stream into tokens and utilize a Llama tokenizer to encode the given\ngeneration prompt. A Q-former is used to align the vision token to the\npre-trained large language model Vicuna-7B and output both the answer and\nchain-of-thought (CoT) reasoning process simultaneously. Our framework can be\noptimized using supervised fine-tuning in an end-to-end manner. In addition, we\nalso propose a large-scale CoT dataset to train our framework via a three stage\nprocessing (i.e., generation, polish, and expert verification). This dataset\nprovides a solid data foundation for the development of subsequent\nreasoning-based large models. Extensive experiments on three event stream STR\nbenchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the\neffectiveness and interpretability of our proposed framework. The source code\nand pre-trained models will be released on\nhttps://github.com/Event-AHU/ESTR-CoT.", "AI": {"tldr": "This paper introduces ESTR-CoT, a novel chain-of-thought reasoning framework for event stream-based scene text recognition, addressing interpretability and contextual reasoning challenges.", "motivation": "To improve event stream-based scene text recognition by addressing challenges of insufficient interpretability and weak contextual reasoning in existing models.", "method": "The authors propose ESTR-CoT, leveraging EVA-CLIP for vision encoding, a Llama tokenizer for generation prompts, and a Q-former to align tokens with a pre-trained Vicuna-7B model. They fine-tune the framework end-to-end using a supervised learning approach and introduce a three-stage process to create a large-scale CoT dataset.", "result": "The authors validate ESTR-CoT on three event stream STR benchmarks (EventSTR, WordArt*, and IC15*), showing its effectiveness and improved interpretability. They also develop a CoT dataset for future research.", "conclusion": "ESTR-CoT effectively improves interpretability and contextual reasoning in event stream-based scene text recognition, making it a promising framework for challenging scenarios like low illumination and fast motion."}}
{"id": "2507.01984", "pdf": "https://arxiv.org/pdf/2507.01984", "abs": "https://arxiv.org/abs/2507.01984", "authors": ["Gautam Kishore Shahi"], "title": "Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features", "categories": ["cs.LG", "cs.CL", "cs.SI"], "comment": null, "summary": "Amid a tidal wave of misinformation flooding social media during elections\nand crises, extensive research has been conducted on misinformation detection,\nprimarily focusing on text-based or image-based approaches. However, only a few\nstudies have explored multimodal feature combinations, such as integrating text\nand images for building a classification model to detect misinformation. This\nstudy investigates the effectiveness of different multimodal feature\ncombinations, incorporating text, images, and social features using an early\nfusion approach for the classification model. This study analyzed 1,529 tweets\ncontaining both text and images during the COVID-19 pandemic and election\nperiods collected from Twitter (now X). A data enrichment process was applied\nto extract additional social features, as well as visual features, through\ntechniques such as object detection and optical character recognition (OCR).\nThe results show that combining unsupervised and supervised machine learning\nmodels improves classification performance by 15% compared to unimodal models\nand by 5% compared to bimodal models. Additionally, the study analyzes the\npropagation patterns of misinformation based on the characteristics of\nmisinformation tweets and the users who disseminate them.", "AI": {"tldr": "The paper studies the effectiveness of multimodal approaches, incorporating text, image, and social features for misinformation detection, showing a 15% performance increase over unimodal models.", "motivation": "To address the limited exploration of multimodal feature combinations in misinformation detection, especially during critical periods such as elections and crises.", "method": "The researchers applied an early fusion approach to analyze 1,529 multimodal tweets during the COVID-19 pandemic and election periods, utilizing techniques like object detection, OCR, and data enrichment for additional insights.", "result": "Combining supervised and unsupervised machine learning models led to a 15% performance boost over unimodal methods and a 5% improvement over bimodal models.", "conclusion": "Multimodal approaches leveraging text, images, and social features enhance misinformation detection significantly while providing deeper insights into misinformation propagation patterns."}}
{"id": "2507.02173", "pdf": "https://arxiv.org/pdf/2507.02173", "abs": "https://arxiv.org/abs/2507.02173", "authors": ["Berkan Dokmeci", "Qingyang Wu", "Ben Athiwaratkun", "Ce Zhang", "Shuaiwen Leon Song", "James Zou"], "title": "Data Diversification Methods In Alignment Enhance Math Performance In LLMs", "categories": ["cs.AI"], "comment": null, "summary": "While recent advances in preference learning have enhanced alignment in human\nfeedback, mathematical reasoning remains a persistent challenge. We investigate\nhow data diversification strategies in preference optimization can improve the\nmathematical reasoning abilities of large language models (LLMs). We evaluate\nthree common data generation methods: temperature sampling, Chain-of-Thought\nprompting, and Monte Carlo Tree Search (MCTS), and introduce\nDiversified-ThinkSolve (DTS), a novel structured approach that systematically\ndecomposes problems into diverse reasoning paths. Our results show that with\nstrategically diversified preference data, models can substantially improve\nmathematical reasoning performance, with the best approach yielding gains of\n7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong\nperformance, DTS incurs only a marginal computational overhead (1.03x) compared\nto the baseline, while MCTS is nearly five times more costly with lower\nreturns. These findings demonstrate that structured exploration of diverse\nproblem-solving methods creates more effective preference data for mathematical\nalignment than traditional approaches.", "AI": {"tldr": "This paper explores how diversified data strategies improve large language models (LLMs) in mathematical reasoning and introduces a novel method, Diversified-ThinkSolve (DTS), which outperforms traditional approaches.", "motivation": "Mathematical reasoning in large language models remains challenging despite advances in preference learning. The paper strives to address this gap by leveraging diversified data strategies to enhance alignment and reasoning capabilities.", "method": "The authors evaluate three common data generation methods (temperature sampling, Chain-of-Thought prompting, and Monte Carlo Tree Search) and propose Diversified-ThinkSolve (DTS), which systematically decomposes problems into diverse reasoning paths.", "result": "The proposed DTS method yields an improvement of 7.1% on GSM8K and 4.2% on MATH datasets compared to the base model while maintaining minimal computational cost (1.03x baseline). It significantly outperforms MCTS, which is computationally costly.", "conclusion": "Strategically diversified preference data, as implemented in DTS, offers a more effective way to improve mathematical reasoning in LLMs with minimal overhead, outperforming traditional methods like MCTS."}}
{"id": "2507.02372", "pdf": "https://arxiv.org/pdf/2507.02372", "abs": "https://arxiv.org/abs/2507.02372", "authors": ["Han Huang", "Tianyu Wang", "Chaoda Peng", "Tongli He", "Zhifeng Hao"], "title": "An Experimental Approach for Running-Time Estimation of Multi-objective Evolutionary Algorithms in Numerical Optimization", "categories": ["cs.NE"], "comment": null, "summary": "Multi-objective evolutionary algorithms (MOEAs) have become essential tools\nfor solving multi-objective optimization problems (MOPs), making their running\ntime analysis crucial for assessing algorithmic efficiency and guiding\npractical applications. While significant theoretical advances have been\nachieved for combinatorial optimization, existing studies for numerical\noptimization primarily rely on algorithmic or problem simplifications, limiting\ntheir applicability to real-world scenarios. To address this gap, we propose an\nexperimental approach for estimating upper bounds on the running time of MOEAs\nin numerical optimization without simplification assumptions. Our approach\nemploys an average gain model that characterizes algorithmic progress through\nthe Inverted Generational Distance metric. To handle the stochastic nature of\nMOEAs, we use statistical methods to estimate the probabilistic distribution of\ngains. Recognizing that gain distributions in numerical optimization exhibit\nirregular patterns with varying densities across different regions, we\nintroduce an adaptive sampling method that dynamically adjusts sampling density\nto ensure accurate surface fitting for running time estimation. We conduct\ncomprehensive experiments on five representative MOEAs (NSGA-II, MOEA/D,\nAR-MOEA, AGEMOEA-II, and PREA) using the ZDT and DTLZ benchmark suites. The\nresults demonstrate the effectiveness of our approach in estimating upper\nbounds on the running time without requiring algorithmic or problem\nsimplifications. Additionally, we provide a web-based implementation to\nfacilitate broader adoption of our methodology. This work provides a practical\ncomplement to theoretical research on MOEAs in numerical optimization.", "AI": {"tldr": "The paper presents a novel experimental method to estimate upper bounds on the running time of multi-objective evolutionary algorithms for numerical optimization without relying on simplifications.", "motivation": "Existing running time analyses for MOEAs in numerical optimization often rely on oversimplifications, limiting their usefulness in real-world applications.", "method": "An average gain model is used alongside statistical methods and adaptive sampling to accurately characterize algorithmic progress and estimate running time bounds, supported by experiments on benchmark problems.", "result": "The proposed approach successfully estimates running time upper bounds for various MOEAs (e.g., NSGA-II, MOEA/D) on benchmark suites like ZDT and DTLZ without simplifications.", "conclusion": "This experimentally-based methodology complements theoretical research and provides an effective tool for assessing MOEAs in numerical optimization tasks."}}
{"id": "2507.02598", "pdf": "https://arxiv.org/pdf/2507.02598", "abs": "https://arxiv.org/abs/2507.02598", "authors": ["Chenhao Xue", "Kezhi Li", "Jiaxing Zhang", "Yi Ren", "Zhengyuan Shi", "Chen Zhang", "Yibo Lin", "Lining Zhang", "Qiang Xu", "Guangyu Sun"], "title": "AC-Refiner: Efficient Arithmetic Circuit Optimization Using Conditional Diffusion Models", "categories": ["cs.AR", "cs.AI"], "comment": "8 pages, 12 figures", "summary": "Arithmetic circuits, such as adders and multipliers, are fundamental\ncomponents of digital systems, directly impacting the performance, power\nefficiency, and area footprint. However, optimizing these circuits remains\nchallenging due to the vast design space and complex physical constraints.\nWhile recent deep learning-based approaches have shown promise, they struggle\nto consistently explore high-potential design variants, limiting their\noptimization efficiency. To address this challenge, we propose AC-Refiner, a\nnovel arithmetic circuit optimization framework leveraging conditional\ndiffusion models. Our key insight is to reframe arithmetic circuit synthesis as\na conditional image generation task. By carefully conditioning the denoising\ndiffusion process on target quality-of-results (QoRs), AC-Refiner consistently\nproduces high-quality circuit designs. Furthermore, the explored designs are\nused to fine-tune the diffusion model, which focuses the exploration near the\nPareto frontier. Experimental results demonstrate that AC-Refiner generates\ndesigns with superior Pareto optimality, outperforming state-of-the-art\nbaselines. The performance gain is further validated by integrating AC-Refiner\ninto practical applications.", "AI": {"tldr": "The paper introduces AC-Refiner, a novel framework using conditional diffusion models to optimize arithmetic circuits for improved quality, surpassing existing methods.", "motivation": "Optimizing arithmetic circuits is challenging due to their vast design space and complex constraints, which directly impact digital system performance.", "method": "The authors use conditional diffusion models to treat arithmetic circuit synthesis as a conditional image generation task, fine-tuning the model periodically for better designs near the Pareto frontier.", "result": "AC-Refiner outperforms state-of-the-art baselines by generating superior Pareto-optimal arithmetic circuit designs.", "conclusion": "AC-Refiner provides a highly efficient framework for generating high-quality arithmetic circuit designs, demonstrating practical and scalable improvements."}}
{"id": "2507.02221", "pdf": "https://arxiv.org/pdf/2507.02221", "abs": "https://arxiv.org/abs/2507.02221", "authors": ["Steven Song", "Anirudh Subramanyam", "Zhenyu Zhang", "Aarti Venkat", "Robert L. Grossman"], "title": "GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons", "categories": ["cs.CL"], "comment": "11 pages, 1 figure, 7 tables", "summary": "Motivation: The Genomic Data Commons (GDC) provides access to high quality,\nharmonized cancer genomics data through a unified curation and analysis\nplatform centered around patient cohorts. While GDC users can interactively\ncreate complex cohorts through the graphical Cohort Builder, users (especially\nnew ones) may struggle to find specific cohort descriptors across hundreds of\npossible fields and properties. However, users may be better able to describe\ntheir desired cohort in free-text natural language.\n  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for\ncurating cohorts from the GDC. GDC Cohort Copilot automatically generates the\nGDC cohort filter corresponding to a user-input natural language description of\ntheir desired cohort, before exporting the cohort back to the GDC for further\nanalysis. An interactive user interface allows users to further refine the\ngenerated cohort. We develop and evaluate multiple large language models (LLMs)\nfor GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC\nCohort LLM achieves better results than GPT-4o prompting in generating GDC\ncohorts.\n  Availability and implementation: The standalone docker image for GDC Cohort\nCopilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.\nSource code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC\nCohort LLM weights are available at https://huggingface.co/uc-ctds.", "AI": {"tldr": "Researchers introduced GDC Cohort Copilot, a tool that uses large language models (LLMs) to enable free-text input for creating cancer patient cohorts within the GDC platform. The tool outperforms GPT-4o and includes an interactive feature for refining cohorts.", "motivation": "Many users find it difficult to navigate the hundreds of fields and properties in the GDC's Cohort Builder interface to describe their desired cohorts, especially if they are new. Allowing natural language input could simplify the process.", "method": "The tool leverages large language models (LLMs) to convert user-provided, free-text cohort descriptions into the corresponding GDC cohort filters. An interactive interface also allows for additional cohort refinements.", "result": "The locally-served, open-source GDC Cohort Copilot, using a custom-trained LLM, outperformed GPT-4o in generating accurate GDC cohorts based on user descriptions. It simplifies the experience for users.", "conclusion": "GDC Cohort Copilot provides an innovative, open-source solution for creating GDC cohorts through natural language descriptions. The tool improves accessibility and usability in cancer genomics research."}}
{"id": "2507.02295", "pdf": "https://arxiv.org/pdf/2507.02295", "abs": "https://arxiv.org/abs/2507.02295", "authors": ["Roopkatha Banerjee", "Prince Modi", "Jinal Vyas", "Chunduru Sri Abhijit", "Tejus Chandrashekar", "Harsha Varun Marisetty", "Manik Gupta", "Yogesh Simmhan"], "title": "Flotilla: A scalable, modular and resilient federated learning framework for heterogeneous resources", "categories": ["cs.DC"], "comment": null, "summary": "With the recent improvements in mobile and edge computing and rising concerns\nof data privacy, Federated Learning(FL) has rapidly gained popularity as a\nprivacy-preserving, distributed machine learning methodology. Several FL\nframeworks have been built for testing novel FL strategies. However, most focus\non validating the learning aspects of FL through pseudo-distributed simulation\nbut not for deploying on real edge hardware in a distributed manner to\nmeaningfully evaluate the federated aspects from a systems perspective. Current\nframeworks are also inherently not designed to support asynchronous\naggregation, which is gaining popularity, and have limited resilience to client\nand server failures. We introduce Flotilla, a scalable and lightweight FL\nframework. It adopts a ``user-first'' modular design to help rapidly compose\nvarious synchronous and asynchronous FL strategies while being agnostic to the\nDNN architecture. It uses stateless clients and a server design that separates\nout the session state, which are periodically or incrementally checkpointed. We\ndemonstrate the modularity of Flotilla by evaluating five different FL\nstrategies for training five DNN models. We also evaluate the client and\nserver-side fault tolerance on 200+ clients, and showcase its ability to\nrapidly failover within seconds. Finally, we show that Flotilla's resource\nusage on Raspberry Pis and Nvidia Jetson edge accelerators are comparable to or\nbetter than three state-of-the-art FL frameworks, Flower, OpenFL and FedML. It\nalso scales significantly better compared to Flower for 1000+ clients. This\npositions Flotilla as a competitive candidate to build novel FL strategies on,\ncompare them uniformly, rapidly deploy them, and perform systems research and\noptimizations.", "AI": {"tldr": "This paper introduces Flotilla, a modular and scalable Federated Learning framework, designed for real-world edge deployments with support for synchronous and asynchronous FL strategies, fault tolerance, and efficient resource usage.", "motivation": "To address the limitations of existing FL frameworks, which focus on pseudo-distributed simulation rather than real-world edge deployment, lack support for asynchronous aggregation, and have limited fault tolerance.", "method": "The authors designed Flotilla, a framework with modular architecture, stateless clients, a decoupled session state for servers, and periodic/incremental checkpointing. It supports various FL strategies and is hardware-agnostic.", "result": "Flotilla was tested with five FL strategies and five DNN models, showing fault tolerance on systems with 200+ clients, rapid failover capabilities, and resource efficiency on edge devices. It also scaled efficiently for 1000+ clients compared to competitors.", "conclusion": "Flotilla is a robust and efficient FL framework that supports a wide range of strategies, scales well, and is suitable for real-world deployments, making it a strong candidate for FL systems research and deployment."}}
{"id": "2507.02264", "pdf": "https://arxiv.org/pdf/2507.02264", "abs": "https://arxiv.org/abs/2507.02264", "authors": ["Jacob J. Morra", "Kaitlyn E. Fouke", "Kexin Hang", "Zichen He", "Owen Traubert", "Timothy W. Dunn", "Eva A. Naumann"], "title": "NLP4Neuro: Sequence-to-sequence learning for neural population decoding", "categories": ["q-bio.NC", "cs.LG"], "comment": "17 pages, 6 figures", "summary": "Delineating how animal behavior arises from neural activity is a foundational\ngoal of neuroscience. However, as the computations underlying behavior unfold\nin networks of thousands of individual neurons across the entire brain, this\npresents challenges for investigating neural roles and computational mechanisms\nin large, densely wired mammalian brains during behavior. Transformers, the\nbackbones of modern large language models (LLMs), have become powerful tools\nfor neural decoding from smaller neural populations. These modern LLMs have\nbenefited from extensive pre-training, and their sequence-to-sequence learning\nhas been shown to generalize to novel tasks and data modalities, which may also\nconfer advantages for neural decoding from larger, brain-wide activity\nrecordings. Here, we present a systematic evaluation of off-the-shelf LLMs to\ndecode behavior from brain-wide populations, termed NLP4Neuro, which we used to\ntest LLMs on simultaneous calcium imaging and behavior recordings in larval\nzebrafish exposed to visual motion stimuli. Through NLP4Neuro, we found that\nLLMs become better at neural decoding when they use pre-trained weights learned\nfrom textual natural language data. Moreover, we found that a recent\nmixture-of-experts LLM, DeepSeek Coder-7b, significantly improved behavioral\ndecoding accuracy, predicted tail movements over long timescales, and provided\nanatomically consistent highly interpretable readouts of neuron salience.\nNLP4Neuro demonstrates that LLMs are highly capable of informing brain-wide\nneural circuit dissection.", "AI": {"tldr": "This paper evaluates the use of pre-trained large language models (LLMs) for decoding animal behavior from neural activity across the brain, finding them effective, especially when using models like DeepSeek Coder-7b.", "motivation": "The study aims to tackle the challenge of understanding how neural activity across densely connected, large mammalian brains leads to specific behaviors, by leveraging the capabilities of LLMs that excel in sequence-to-sequence learning and generalization.", "method": "The authors systematically evaluated off-the-shelf pre-trained LLMs, termed NLP4Neuro, utilizing them to decode behavior from brain-wide neural activity recorded through calcium imaging in larval zebrafish exposed to visual stimuli.", "result": "They found that pre-trained LLMs, especially DeepSeek Coder-7b, enhanced decoding accuracy of behavior, predicted long-timescale tail movements, and offered interpretable neuron salience maps with anatomical consistency.", "conclusion": "LLMs, especially advanced models like DeepSeek Coder-7b, show promising capabilities in decoding complex neural activity and offer a novel approach for brain-wide neural circuit analysis."}}
{"id": "2507.02118", "pdf": "https://arxiv.org/pdf/2507.02118", "abs": "https://arxiv.org/abs/2507.02118", "authors": ["Cristina Martinez Montes", "Daniela Grassi", "Nicole Novielli", "Birgit Penzenstadle"], "title": "A Multimodal Approach Combining Biometrics and Self-Report Instruments for Monitoring Stress in Programming: Methodological Insights", "categories": ["cs.SE"], "comment": null, "summary": "The study of well-being, stress and other human factors has traditionally\nrelied on self-report instruments to assess key variables. However, concerns\nabout potential biases in these instruments, even when thoroughly validated and\nstandardised, have driven growing interest in alternatives in combining these\nmeasures with more objective methods, such as physiological measures.\n  We aimed to (i) compare psychometric stress measures and biometric indicators\nand (ii) identify stress-related patterns in biometric data during software\nengineering tasks.\n  We conducted an experiment where participants completed a pre-survey, then\nprogrammed two tasks wearing biometric sensors, answered brief post-surveys for\neach, and finally went through a short exit interview.\n  Our results showed diverse outcomes; we found no stress in the psychometric\ninstruments. Participants in the interviews reported a mix of feeling no stress\nand experiencing time pressure. Finally, the biometrics showed a significant\ndifference only in EDA phasic peaks.\n  We conclude that our chosen way of inducing stress by imposing a stricter\ntime limit was insufficient. We offer methodological insights for future\nstudies working with stress, biometrics, and psychometric instruments.", "AI": {"tldr": "This paper explores the relationship between self-reported stress measures and biometric indicators, finding limited biometric stress evidence during software engineering tasks.", "motivation": "The motivation stems from concerns about biases in self-report instruments for measuring human factors like stress and well-being, prompting interest in objective alternatives such as biometrics.", "method": "Participants were surveyed, completed programming tasks with biometric sensors, answered post-task surveys, and participated in exit interviews to compare psychometric and biometric stress measures.", "result": "The psychometric tools found no stress, interviews noted mixed feelings, and biometric data revealed significant differences only in EDA phasic peaks.", "conclusion": "The stress induction method was insufficient, and the study provides insights for improving methodologies in stress research using psychometric and biometric approaches."}}
{"id": "2507.02190", "pdf": "https://arxiv.org/pdf/2507.02190", "abs": "https://arxiv.org/abs/2507.02190", "authors": ["Max Argus", "Jelena Bratulic", "Houman Masnavi", "Maxim Velikanov", "Nick Heppert", "Abhinav Valada", "Thomas Brox"], "title": "cVLA: Towards Efficient Camera-Space VLAs", "categories": ["cs.RO", "cs.LG"], "comment": "20 pages, 10 figures", "summary": "Vision-Language-Action (VLA) models offer a compelling framework for tackling\ncomplex robotic manipulation tasks, but they are often expensive to train. In\nthis paper, we propose a novel VLA approach that leverages the competitive\nperformance of Vision Language Models (VLMs) on 2D images to directly infer\nrobot end-effector poses in image frame coordinates. Unlike prior VLA models\nthat output low-level controls, our model predicts trajectory waypoints, making\nit both more efficient to train and robot embodiment agnostic. Despite its\nlightweight design, our next-token prediction architecture effectively learns\nmeaningful and executable robot trajectories. We further explore the\nunderutilized potential of incorporating depth images, inference-time\ntechniques such as decoding strategies, and demonstration-conditioned action\ngeneration. Our model is trained on a simulated dataset and exhibits strong\nsim-to-real transfer capabilities. We evaluate our approach using a combination\nof simulated and real data, demonstrating its effectiveness on a real robotic\nsystem.", "AI": {"tldr": "The paper proposes a Vision-Language-Action (VLA) model that leverages Vision Language Models (VLMs) for robotic task prediction, emphasizing efficiency and strong sim-to-real transfer abilities.", "motivation": "To address the high costs and inefficiencies in training VLA models for complex robotic manipulation tasks by leveraging VLMs.", "method": "Developed a next-token prediction architecture with trajectory waypoint predictions, utilizing depth images and demonstration-conditioned action generation tested in simulated environments.", "result": "The proposed model exhibits strong performance on simulated to real-world evaluation, demonstrating accurate and executable robotic trajectories.", "conclusion": "The approach efficiently predicts meaningful robotic actions and establishes an advance in sim-to-real transfer for robotic systems, balancing performance and lightweight architecture."}}
{"id": "2507.02275", "pdf": "https://arxiv.org/pdf/2507.02275", "abs": "https://arxiv.org/abs/2507.02275", "authors": ["Jikai Jin", "Lester Mackey", "Vasilis Syrgkanis"], "title": "It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation", "categories": ["stat.ML", "cs.LG", "econ.EM", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Structure-agnostic causal inference studies how well one can estimate a\ntreatment effect given black-box machine learning estimates of nuisance\nfunctions (like the impact of confounders on treatment and outcomes). Here, we\nfind that the answer depends in a surprising way on the distribution of the\ntreatment noise. Focusing on the partially linear model of\n\\citet{robinson1988root}, we first show that the widely adopted double machine\nlearning (DML) estimator is minimax rate-optimal for Gaussian treatment noise,\nresolving an open problem of \\citet{mackey2018orthogonal}. Meanwhile, for\nindependent non-Gaussian treatment noise, we show that DML is always suboptimal\nby constructing new practical procedures with higher-order robustness to\nnuisance errors. These \\emph{ACE} procedures use structure-agnostic cumulant\nestimators to achieve $r$-th order insensitivity to nuisance errors whenever\nthe $(r+1)$-st treatment cumulant is non-zero. We complement these core results\nwith novel minimax guarantees for binary treatments in the partially linear\nmodel. Finally, using synthetic demand estimation experiments, we demonstrate\nthe practical benefits of our higher-order robust estimators.", "AI": {"tldr": "The paper examines the estimation of treatment effects using black-box machine learning under varying treatment noise distributions, showing improvements in robustness and proposing novel techniques.", "motivation": "To understand and improve causal inference methods when treatment noise distributions vary, addressing limitations with traditional double machine learning (DML) for non-Gaussian noise.", "method": "The paper utilizes partially linear models, compares DML with newly designed ACE procedures, and provides robustness results based on cumulants. Minimax guarantees are also proposed for binary treatments.", "result": "DML is shown to be optimal for Gaussian noise but suboptimal for non-Gaussian noise. ACE procedures achieve higher-order robustness under specific conditions, and practical performance is demonstrated in synthetic experiments.", "conclusion": "Treatment noise distribution plays a critical role in causal inference. ACE procedures provide improvements over DML for non-Gaussian noise, showcasing their practical utility with extended robustness."}}
{"id": "2507.02205", "pdf": "https://arxiv.org/pdf/2507.02205", "abs": "https://arxiv.org/abs/2507.02205", "authors": ["Elena Ryumina", "Maxim Markitantov", "Alexandr Axyonov", "Dmitry Ryumin", "Mikhail Dolgushin", "Alexey Karpov"], "title": "Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach", "categories": ["cs.CV"], "comment": "8", "summary": "Compound Expression Recognition (CER), a subfield of affective computing,\naims to detect complex emotional states formed by combinations of basic\nemotions. In this work, we present a novel zero-shot multimodal approach for\nCER that combines six heterogeneous modalities into a single pipeline: static\nand dynamic facial expressions, scene and label matching, scene context, audio,\nand text. Unlike previous approaches relying on task-specific training data,\nour approach uses zero-shot components, including Contrastive Language-Image\nPretraining (CLIP)-based label matching and Qwen-VL for semantic scene\nunderstanding. We further introduce a Multi-Head Probability Fusion (MHPF)\nmodule that dynamically weights modality-specific predictions, followed by a\nCompound Expressions (CE) transformation module that uses Pair-Wise Probability\nAggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods\nto produce interpretable compound emotion outputs. Evaluated under multi-corpus\ntraining, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%\non Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via\nzero-shot testing, which is comparable to the results of supervised approaches\ntrained on target data. This demonstrates the effectiveness of the proposed\napproach for capturing CE without domain adaptation. The source code is\npublicly available.", "AI": {"tldr": "The paper proposes a zero-shot multimodal approach for Compound Expression Recognition (CER) using six modalities, achieving performance near supervised methods.", "motivation": "To address the need for effective recognition of complex emotional states (Compound Expression Recognition) without requiring task-specific data or domain adaptation.", "method": "They use a zero-shot multimodal pipeline incorporating six modalities (facial expressions, scene context, audio, and text) and introduce modules like Multi-Head Probability Fusion (MHPF) and Compound Expressions (CE) transformation for dynamic weighting and interpretation.", "result": "The approach achieves F1 scores of 46.95% on AffWild2, 49.02% on AFEW, and 34.85% on C-EXPR-DB, comparable to supervised approaches.", "conclusion": "The zero-shot multimodal approach is effective for recognizing compound emotions and showcases performance comparable to supervised methods, making the source code publicly available for further advancements."}}
{"id": "2507.01998", "pdf": "https://arxiv.org/pdf/2507.01998", "abs": "https://arxiv.org/abs/2507.01998", "authors": ["Hexiang Bai", "Deyu Li", "Jiye Liang", "Yanhui Zhai"], "title": "Positive region preserved random sampling: an efficient feature selection method for massive data", "categories": ["cs.LG"], "comment": null, "summary": "Selecting relevant features is an important and necessary step for\nintelligent machines to maximize their chances of success. However, intelligent\nmachines generally have no enough computing resources when faced with huge\nvolume of data. This paper develops a new method based on sampling techniques\nand rough set theory to address the challenge of feature selection for massive\ndata. To this end, this paper proposes using the ratio of discernible object\npairs to all object pairs that should be distinguished to measure the\ndiscriminatory ability of a feature set. Based on this measure, a new feature\nselection method is proposed. This method constructs positive region preserved\nsamples from massive data to find a feature subset with high discriminatory\nability. Compared with other methods, the proposed method has two advantages.\nFirst, it is able to select a feature subset that can preserve the\ndiscriminatory ability of all the features of the target massive data set\nwithin an acceptable time on a personal computer. Second, the lower boundary of\nthe probability of the object pairs that can be discerned using the feature\nsubset selected in all object pairs that should be distinguished can be\nestimated before finding reducts. Furthermore, 11 data sets of different sizes\nwere used to validate the proposed method. The results show that approximate\nreducts can be found in a very short period of time, and the discriminatory\nability of the final reduct is larger than the estimated lower boundary.\nExperiments on four large-scale data sets also showed that an approximate\nreduct with high discriminatory ability can be obtained in reasonable time on a\npersonal computer.", "AI": {"tldr": "This paper introduces a new feature selection method using sampling techniques and rough set theory to efficiently handle massive datasets on limited computing resources.", "motivation": "Feature selection is vital for intelligent machines optimizing success, but handling massive data with limited computing resources remains challenging.", "method": "A new discriminatory measure is utilized for feature sets, coupled with constructing positive region preserved samples and finding a feature subset based on high discriminatory ability.", "result": "Experiments showed that approximate reducts with high discriminatory ability were efficiently obtained on large datasets using personal computers.", "conclusion": "The proposed method balances computational efficiency and discriminatory ability, making it practical for feature selection in massive datasets."}}
{"id": "2507.02197", "pdf": "https://arxiv.org/pdf/2507.02197", "abs": "https://arxiv.org/abs/2507.02197", "authors": ["Amogh Mannekote", "Adam Davies", "Guohao Li", "Kristy Elizabeth Boyer", "ChengXiang Zhai", "Bonnie J Dorr", "Francesco Pinto"], "title": "Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust", "categories": ["cs.AI"], "comment": null, "summary": "As LLMs are increasingly studied as role-playing agents to generate synthetic\ndata for human behavioral research, ensuring that their outputs remain coherent\nwith their assigned roles has become a critical concern. In this paper, we\ninvestigate how consistently LLM-based role-playing agents' stated beliefs\nabout the behavior of the people they are asked to role-play (\"what they say\")\ncorrespond to their actual behavior during role-play (\"how they act\").\nSpecifically, we establish an evaluation framework to rigorously measure how\nwell beliefs obtained by prompting the model can predict simulation outcomes in\nadvance. Using an augmented version of the GenAgents persona bank and the Trust\nGame (a standard economic game used to quantify players' trust and\nreciprocity), we introduce a belief-behavior consistency metric to\nsystematically investigate how it is affected by factors such as: (1) the types\nof beliefs we elicit from LLMs, like expected outcomes of simulations versus\ntask-relevant attributes of individual characters LLMs are asked to simulate;\n(2) when and how we present LLMs with relevant information about Trust Game;\nand (3) how far into the future we ask the model to forecast its actions. We\nalso explore how feasible it is to impose a researcher's own theoretical priors\nin the event that the originally elicited beliefs are misaligned with research\nobjectives. Our results reveal systematic inconsistencies between LLMs' stated\n(or imposed) beliefs and the outcomes of their role-playing simulation, at both\nan individual- and population-level. Specifically, we find that, even when\nmodels appear to encode plausible beliefs, they may fail to apply them in a\nconsistent way. These findings highlight the need to identify how and when\nLLMs' stated beliefs align with their simulated behavior, allowing researchers\nto use LLM-based agents appropriately in behavioral studies.", "AI": {"tldr": "The paper examines inconsistencies between what large language models (LLMs) say (stated beliefs) and how they behave (simulation actions) during role-playing tasks, using a new evaluation framework.", "motivation": "As LLMs are being increasingly used as role-playing agents in synthetic data generation for behavioral studies, ensuring role-consistent outputs is essential to establish reliable research outcomes.", "method": "The study establishes a framework to measure belief-behavior consistency in LLM role-playing. It uses the GenAgents persona bank, the Trust Game, and introduces a metric to analyze variables like types of elicited beliefs, timing of information presentation, and action forecasting over time.", "result": "The researchers found systematic inconsistencies between LLMs\u2019 stated beliefs and their role-playing actions. Even when beliefs appear plausible, models often fail to implement them consistently.", "conclusion": "Aligning beliefs with simulated behavior in LLMs requires careful investigation. Researchers must identify conditions under which belief-behavior consistency is reliable to ensure valid applications in behavioral modeling."}}
{"id": "2507.02381", "pdf": "https://arxiv.org/pdf/2507.02381", "abs": "https://arxiv.org/abs/2507.02381", "authors": ["Min Huang", "Pengxiang Chen", "Han Huang", "Tongli He", "Yushan Zhang", "Zhifeng Hao"], "title": "Running-time Analysis of ($\u03bc+\u03bb$) Evolutionary Combinatorial Optimization Based on Multiple-gain Estimation", "categories": ["cs.NE"], "comment": null, "summary": "The running-time analysis of evolutionary combinatorial optimization is a\nfundamental topic in evolutionary computation. However, theoretical results\nregarding the $(\\mu+\\lambda)$ evolutionary algorithm (EA) for combinatorial\noptimization problems remain relatively scarce compared to those for simple\npseudo-Boolean problems. This paper proposes a multiple-gain model to analyze\nthe running time of EAs for combinatorial optimization problems. The proposed\nmodel is an improved version of the average gain model, which is a\nfitness-difference drift approach under the sigma-algebra condition to estimate\nthe running time of evolutionary numerical optimization. The improvement yields\na framework for estimating the expected first hitting time of a stochastic\nprocess in both average-case and worst-case scenarios. It also introduces novel\nrunning-time results of evolutionary combinatorial optimization, including two\ntighter time complexity upper bounds than the known results in the case of\n($\\mu+\\lambda$) EA for the knapsack problem with favorably correlated weights,\na closed-form expression of time complexity upper bound in the case of\n($\\mu+\\lambda$) EA for general $k$-MAX-SAT problems and a tighter time\ncomplexity upper bounds than the known results in the case of ($\\mu+\\lambda$)\nEA for the traveling salesperson problem. Experimental results indicate that\nthe practical running time aligns with the theoretical results, verifying that\nthe multiple-gain model is an effective tool for running-time analysis of\n($\\mu+\\lambda$) EA for combinatorial optimization problems.", "AI": {"tldr": "This paper improves running-time analysis for $(\\mu+\\lambda)$ evolutionary algorithms (EA) in combinatorial optimization using a multiple-gain model, achieving tighter time complexity bounds.", "motivation": "Theoretical understanding of $(\\mu+\\lambda)$ EAs in combinatorial optimization is limited compared to simpler problems like pseudo-Boolean problems.", "method": "A multiple-gain model was developed, building on the average gain model with enhancements to estimate expected first hitting time in stochastic processes.", "result": "Tighter or novel time complexity upper bounds were derived for $(\\mu+\\lambda)$ EA in problems like knapsack, $k$-MAX-SAT, and traveling salesperson. Experimental results supported theoretical predictions.", "conclusion": "The multiple-gain model is a robust approach for analyzing $(\\mu+\\lambda)$ EA runtimes in combinatorial optimization, contributing tighter bounds and practical validation."}}
{"id": "2507.02654", "pdf": "https://arxiv.org/pdf/2507.02654", "abs": "https://arxiv.org/abs/2507.02654", "authors": ["Rui Xie", "Asad Ul Haq", "Yunhua Fang", "Linsen Ma", "Sanchari Sen", "Swagath Venkataramani", "Liu Liu", "Tong Zhang"], "title": "Breaking the HBM Bit Cost Barrier: Domain-Specific ECC for AI Inference Infrastructure", "categories": ["cs.AR"], "comment": null, "summary": "High-Bandwidth Memory (HBM) delivers exceptional bandwidth and energy\nefficiency for AI workloads, but its high cost per bit, driven in part by\nstringent on-die reliability requirements, poses a growing barrier to scalable\ndeployment. This work explores a system-level approach to cost reduction by\neliminating on-die ECC and shifting all fault management to the memory\ncontroller. We introduce a domain-specific ECC framework combining\nlarge-codeword Reed--Solomon~(RS) correction with lightweight fine-grained CRC\ndetection, differential parity updates to mitigate write amplification, and\ntunable protection based on data importance. Our evaluation using LLM inference\nworkloads shows that, even under raw HBM bit error rates up to $10^{-3}$, the\nsystem retains over 78\\% of throughput and 97\\% of model accuracy compared with\nsystems equipped with ideal error-free HBM. By treating reliability as a\ntunable system parameter rather than a fixed hardware constraint, our design\nopens a new path toward low-cost, high-performance HBM deployment in AI\ninfrastructure.", "AI": {"tldr": "The paper proposes a system-level approach to lower the cost of High-Bandwidth Memory (HBM) by removing on-die ECC and managing reliability through the memory controller.", "motivation": "HBM is valuable for AI workloads due to its bandwidth and efficiency, but its cost remains a barrier due to reliability requirements.", "method": "The paper uses a domain-specific ECC framework including Reed--Solomon correction, CRC detection, differential parity updates, and tunable data protection.", "result": "The system maintains over 78% of throughput and 97% accuracy under high error rates in HBM when evaluated with AI inference workloads.", "conclusion": "By shifting reliability management to the system level, the approach makes HBM deployment more cost-efficient while maintaining performance and accuracy."}}
{"id": "2507.02259", "pdf": "https://arxiv.org/pdf/2507.02259", "abs": "https://arxiv.org/abs/2507.02259", "authors": ["Hongli Yu", "Tinghong Chen", "Jiangtao Feng", "Jiangjie Chen", "Weinan Dai", "Qiying Yu", "Ya-Qin Zhang", "Wei-Ying Ma", "Jingjing Liu", "Mingxuan Wang", "Hao Zhou"], "title": "MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://memagent-sialab.github.io/", "summary": "Despite improvements by length extrapolation, efficient attention and memory\nmodules, handling infinitely long documents with linear complexity without\nperformance degradation during extrapolation remains the ultimate challenge in\nlong-text processing. We directly optimize for long-text tasks in an end-to-end\nfashion and introduce a novel agent workflow, MemAgent, which reads text in\nsegments and updates the memory using an overwrite strategy. We extend the DAPO\nalgorithm to facilitate training via independent-context multi-conversation\ngeneration. MemAgent has demonstrated superb long-context capabilities, being\nable to extrapolate from an 8K context trained on 32K text to a 3.5M QA task\nwith performance loss < 5% and achieves 95%+ in 512K RULER test.", "AI": {"tldr": "The paper introduces MemAgent, a workflow optimized for handling ultra-long text efficiently, achieving impressive extension from training on short contexts to extrapolating millions of tokens.", "motivation": "Handling ultra-long documents efficiently without degradation during extrapolation is a critical challenge in long-text processing.", "method": "The paper introduces MemAgent, which reads text segments and updates memory using an overwrite strategy, combined with an extended DAPO algorithm for training with independent multi-conversation generation.", "result": "MemAgent successfully extrapolated from an 8K training context to a 3.5M QA task with <5% performance loss and achieved over 95% accuracy in a 512K RULER test.", "conclusion": "MemAgent proves to be an effective approach for ultra-long text processing, addressing challenges in extrapolation with minimal performance degradation."}}
{"id": "2507.02404", "pdf": "https://arxiv.org/pdf/2507.02404", "abs": "https://arxiv.org/abs/2507.02404", "authors": ["Maxime Martinasso", "Mark Klein", "Thomas C. Schulthess"], "title": "Alps, a versatile research infrastructure", "categories": ["cs.DC"], "comment": "10 pages, 6 figures, Cray User Group(CUG) 2025 Best Paper Award", "summary": "The Swiss National Supercomputing Centre (CSCS) has a long-standing tradition\nof delivering top-tier high-performance computing systems, exemplified by the\nPiz Daint supercomputer. However, the increasing diversity of scientific needs\nhas exposed limitations in traditional vertically integrated HPC architectures,\nwhich often lack flexibility and composability. To address these challenges,\nCSCS developed Alps, a next-generation HPC infrastructure designed with a\ntransformative principle: resources operate as independent endpoints within a\nhigh-speed network. This architecture enables the creation of independent\ntenant-specific and platform-specific services, tailored to diverse scientific\nrequirements.\n  Alps incorporates heterogeneous hardware, including CPUs and GPUs,\ninterconnected by a high-performance Slingshot network, and offers a modular\nstorage system. A key innovation is the versatile software-defined cluster\n(vCluster) technology, which bridges cloud and HPC paradigms. By abstracting\ninfrastructure, service management, and user environments into distinct layers,\nvClusters allow for customized platforms that support diverse workloads.\nCurrent platforms on Alps serve various scientific domains, including numerical\nweather prediction, and AI research.", "AI": {"tldr": "CSCS introduces Alps, a next-generation HPC infrastructure aimed at flexible and composable computing for diverse scientific needs.", "motivation": "The paper addresses the limitations of traditional HPC architectures, which fail to meet the increasingly diverse requirements of scientific computing.", "method": "CSCS designed Alps with modularity and adaptability, employing a network-centric model where resources operate independently, and integrating vCluster technology to combine cloud and HPC paradigms.", "result": "Alps supports diverse scientific workloads such as numerical weather prediction and AI research, demonstrating the adaptability of the system.", "conclusion": "Alps provides a significant leap in flexibility and composability for high-performance computing by leveraging vCluster technology and heterogeneous hardware."}}
{"id": "2507.02304", "pdf": "https://arxiv.org/pdf/2507.02304", "abs": "https://arxiv.org/abs/2507.02304", "authors": ["Kai Chen", "Zhong-qi K. Tian", "Yifei Chen", "Songting Li", "Douglas Zhou"], "title": "Nonlinear Network Reconstruction by Pairwise Time-delayed Transfer Entropy", "categories": ["q-bio.NC"], "comment": "27 pages, 11 figures", "summary": "Analyzing network structural connectivity is crucial for understanding\ndynamics and functions of complex networks across disciplines. In many\nnetworks, structural connectivity is not observable, which requires to be\ninferred via causal inference methods. Among them, transfer entropy (TE) is one\nof the most broadly applied causality measure due to its model-free property.\nHowever, TE often faces the curse of dimensionality in high-dimensional\nprobability estimation, and the relation between the inferred causal\nconnectivity and the underlying structural connectivity remains poorly\nunderstood. Here we address these issues by proposing a pairwise time-delayed\ntransfer entropy (PTD-TE) method. We theoretically establish a quadratic\nrelationship between PTD-TE values and node coupling strengths, and demonstrate\nits immunity to dimensionality issues and broad applicability. Tests on\nbiological neuronal networks, nonlinear physical systems, and\nelectrophysiological data show PTD-TE achieves consistent, high-performance\nreconstructions. Compared to a bunch of existing approaches for network\nconnectivity reconstruction, PTD-TE outperforms these methods across various\nnetwork systems in accuracy and robustness against noise. Our framework\nprovides a scalable, model-agnostic tool for structural connectivity inference\nin nonlinear real-world networks.", "AI": {"tldr": "This study introduces a pairwise time-delayed transfer entropy (PTD-TE) method to reconstruct network structural connectivity more accurately and robustly, addressing limitations of traditional transfer entropy methods.", "motivation": "Existing methods like transfer entropy face challenges such as high-dimensional probability estimation and weak understanding of causal and structural connectivity relationships.", "method": "The study proposes PTD-TE, an analytical framework that establishes a quadratic relationship between PTD-TE values and coupling strengths, improving on dimensionality issues of standard TE.", "result": "Performance tests show PTD-TE consistently reconstructs networks accurately and maintains robustness against noise across biological, physical, and electrophysiological systems.", "conclusion": "PTD-TE offers a scalable and versatile tool for inferring structural connectivity in nonlinear real-world networks, outperforming traditional approaches."}}
{"id": "2507.02137", "pdf": "https://arxiv.org/pdf/2507.02137", "abs": "https://arxiv.org/abs/2507.02137", "authors": ["Martin Obaidi", "Marc Herrmann", "Jil Kl\u00fcnder", "Kurt Schneider"], "title": "Towards Trustworthy Sentiment Analysis in Software Engineering: Dataset Characteristics and Tool Selection", "categories": ["cs.SE"], "comment": "This paper has been accepted at the RETRAI workshop of the 33rd IEEE\n  International Requirements Engineering Conference (REW 2025)", "summary": "Software development relies heavily on text-based communication, making\nsentiment analysis a valuable tool for understanding team dynamics and\nsupporting trustworthy AI-driven analytics in requirements engineering.\nHowever, existing sentiment analysis tools often perform inconsistently across\ndatasets from different platforms, due to variations in communication style and\ncontent.\n  In this study, we analyze linguistic and statistical features of 10 developer\ncommunication datasets from five platforms and evaluate the performance of 14\nsentiment analysis tools. Based on these results, we propose a mapping approach\nand questionnaire that recommends suitable sentiment analysis tools for new\ndatasets, using their characteristic features as input.\n  Our results show that dataset characteristics can be leveraged to improve\ntool selection, as platforms differ substantially in both linguistic and\nstatistical properties. While transformer-based models such as SetFit and\nRoBERTa consistently achieve strong results, tool effectiveness remains\ncontext-dependent. Our approach supports researchers and practitioners in\nselecting trustworthy tools for sentiment analysis in software engineering,\nwhile highlighting the need for ongoing evaluation as communication contexts\nevolve.", "AI": {"tldr": "The paper examines sentiment analysis tools in software engineering and presents a method to recommend suitable tools based on dataset characteristics.", "motivation": "Understand and improve sentiment analysis in the context of software development communication to ensure trustworthy analytics.", "method": "Analyzed linguistic and statistical features of 10 developer communication datasets across five platforms; evaluated 14 sentiment analysis tools; proposed a mapping approach and questionnaire to recommend tools.", "result": "Transformer models like SetFit and RoBERTa achieve strong results but effectiveness varies with context. Proposed method enhances tool selection based on dataset features.", "conclusion": "Dataset characteristics significantly impact sentiment analysis tool performance. A systematic selection approach helps researchers and practitioners use trustworthy tools effectively."}}
{"id": "2507.02198", "pdf": "https://arxiv.org/pdf/2507.02198", "abs": "https://arxiv.org/abs/2507.02198", "authors": ["Surya Pratap Singh", "Tsimafei Lazouski", "Maani Ghaffari"], "title": "GPS-DRIFT: Marine Surface Robot Localization using IMU-GPS Fusion and Invariant Filtering", "categories": ["cs.RO"], "comment": "6 pages", "summary": "This paper presents an extension of the DRIFT invariant state estimation\nframework, enabling robust fusion of GPS and IMU data for accurate pose and\nheading estimation. Originally developed for testing and usage on a marine\nautonomous surface vehicle (ASV), this approach can also be utilized on other\nmobile systems. Building upon the original proprioceptive only DRIFT algorithm,\nwe develop a symmetry-preserving sensor fusion pipeline utilizing the invariant\nextended Kalman filter (InEKF) to integrate global position updates from GPS\ndirectly into the correction step. Crucially, we introduce a novel heading\ncorrection mechanism that leverages GPS course-over-ground information in\nconjunction with IMU orientation, overcoming the inherent unobservability of\nyaw in dead-reckoning. The system was deployed and validated on a customized\nBlue Robotics BlueBoat, but the methodological focus is on the algorithmic\napproach to fusing exteroceptive and proprioceptive sensors for drift-free\nlocalization and reliable orientation estimation. This work provides an open\nsource solution for accurate yaw observation and localization in challenging or\nGPS-degraded conditions, and lays the groundwork for future experimental and\ncomparative studies.", "AI": {"tldr": "This paper extends the DRIFT framework for GPS and IMU sensor fusion enabling accurate pose and heading estimation using the Invariant Extended Kalman Filter (InEKF).", "motivation": "There is a need for robust and drift-free localization in mobile systems, particularly in applications like autonomous surface vehicles, where GPS and IMU data fusion is crucial for accurate navigation.", "method": "The authors develop a symmetry-preserving sensor fusion pipeline using the Invariant Extended Kalman Filter (InEKF) to fuse GPS position updates and IMU data, introducing a novel heading correction mechanism leveraging GPS course-over-ground information.", "result": "The proposed system was deployed successfully on the Blue Robotics BlueBoat, demonstrating accurate yaw observation and drift-free localization, even in GPS-degraded conditions.", "conclusion": "The study provides an open-source algorithmic solution for robust sensor fusion, enabling accurate localization and heading estimation for mobile systems, and sets the stage for future experimental enhancements."}}
{"id": "2507.02377", "pdf": "https://arxiv.org/pdf/2507.02377", "abs": "https://arxiv.org/abs/2507.02377", "authors": ["Thang D. Bui", "Michalis K. Titsias"], "title": "Sparse Gaussian Processes: Structured Approximations and Power-EP Revisited", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Inducing-point-based sparse variational Gaussian processes have become the\nstandard workhorse for scaling up GP models. Recent advances show that these\nmethods can be improved by introducing a diagonal scaling matrix to the\nconditional posterior density given the inducing points. This paper first\nconsiders an extension that employs a block-diagonal structure for the scaling\nmatrix, provably tightening the variational lower bound. We then revisit the\nunifying framework of sparse GPs based on Power Expectation Propagation (PEP)\nand show that it can leverage and benefit from the new structured approximate\nposteriors. Through extensive regression experiments, we show that the proposed\nblock-diagonal approximation consistently performs similarly to or better than\nexisting diagonal approximations while maintaining comparable computational\ncosts. Furthermore, the new PEP framework with structured posteriors provides\ncompetitive performance across various power hyperparameter settings, offering\npractitioners flexible alternatives to standard variational approaches.", "AI": {"tldr": "This paper introduces a block-diagonal extension to sparse variational Gaussian Processes (GPs) and applies it in a revised Power Expectation Propagation (PEP) framework, achieving improved results with similar costs.", "motivation": "To improve sparse variational Gaussian Processes by enhancing their posterior approximation and to offer flexible alternatives to existing methods.", "method": "Introduced a block-diagonal scaling matrix for the posterior density and incorporated it into a unified PEP framework, supported by both theoretical analysis and extensive regression experiments.", "result": "The block-diagonal approximation outperformed or matched diagonal approximations in accuracy while maintaining similar computational efficiency. The PEP framework with structured posteriors performed competitively across different settings.", "conclusion": "The proposed block-diagonal method and its integration with PEP provide improved performance and flexible alternatives, making them valuable additions to current sparse GP approaches."}}
{"id": "2507.02212", "pdf": "https://arxiv.org/pdf/2507.02212", "abs": "https://arxiv.org/abs/2507.02212", "authors": ["Takuro Kawada", "Shunsuke Kitada", "Sota Nemoto", "Hitoshi Iyatomi"], "title": "SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "21 pages, 15 figures, 4 tables. Project Page:\n  https://iyatomilab.github.io/SciGA/", "summary": "Graphical Abstracts (GAs) play a crucial role in visually conveying the key\nfindings of scientific papers. While recent research has increasingly\nincorporated visual materials such as Figure 1 as de facto GAs, their potential\nto enhance scientific communication remains largely unexplored. Moreover,\ndesigning effective GAs requires advanced visualization skills, creating a\nbarrier to their widespread adoption. To tackle these challenges, we introduce\nSciGA-145k, a large-scale dataset comprising approximately 145,000 scientific\npapers and 1.14 million figures, explicitly designed for supporting GA\nselection and recommendation as well as facilitating research in automated GA\ngeneration. As a preliminary step toward GA design support, we define two\ntasks: 1) Intra-GA recommendation, which identifies figures within a given\npaper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,\nwhich retrieves GAs from other papers to inspire the creation of new GAs. We\nprovide reasonable baseline models for these tasks. Furthermore, we propose\nConfidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation\nmetric that offers a fine-grained analysis of model behavior. CAR addresses\nlimitations in traditional ranking-based metrics by considering cases where\nmultiple figures within a paper, beyond the explicitly labeled GA, may also\nserve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a\nfoundation for advancing visual scientific communication while contributing to\nthe development of AI for Science.", "AI": {"tldr": "The paper introduces SciGA-145k, a dataset of 145,000 scientific papers aimed at enhancing Graphical Abstracts (GAs) selection and recommendation, accompanied by novel tasks and metrics.", "motivation": "To address the challenges of designing effective Graphical Abstracts (GAs), which require advanced visualization skills and have untapped scientific communication potential.", "method": "The authors created a dataset (SciGA-145k) with 1.14 million figures, proposed two GA recommendation tasks (Intra-GA and Inter-GA), and developed a new evaluation metric (CAR).", "result": "Baseline models for GA recommendation tasks and the CAR metric were introduced, facilitating a deeper understanding of GA selection and retrieval.", "conclusion": "SciGA-145k offers a unified platform and methodological framework to enhance visual scientific communication and AI applications in science."}}
{"id": "2507.01999", "pdf": "https://arxiv.org/pdf/2507.01999", "abs": "https://arxiv.org/abs/2507.01999", "authors": ["Bappaditya Dey", "Daniel Sorensen", "Minjin Hwang", "Sandip Halder"], "title": "Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series", "categories": ["cs.LG", "I.2.0; J.6"], "comment": "13 pages, 7 figures, submitted to IEEE Transactions on Semiconductor\n  Manufacturing", "summary": "Semiconductor manufacturing is an extremely complex process, characterized by\nthousands of interdependent parameters collected across diverse tools and\nprocess steps. Multi-variate time-series (MTS) analysis has emerged as a\ncritical methodology for enabling real-time monitoring, fault detection, and\npredictive maintenance in such environments. However, anomaly prediction in\nsemiconductor fabrication presents several critical challenges, including high\ndata dimensionality, severe class imbalance due to the rarity of true faults,\nnoisy and missing measurements, and non-stationary behavior of production\nsystems. Furthermore, the complex interdependencies between variables and the\ndelayed emergence of faults across downstream stages complicate both anomaly\ndetection and root-cause-analysis. This paper presents a novel and generic\napproach for anomaly detection in MTS data using machine learning. The proposed\nmethodology consists of three main steps: a) converting MTS data into\nimage-based representations using the Continuous Wavelet Transform, b)\ndeveloping a multi-class image classifier by fine-tuning a pretrained VGG-16\narchitecture on custom CWT image datasets, and c) constructing a Siamese\nnetwork composed of two identical sub-networks, each utilizing the fine-tuned\nVGG-16 as a backbone. The network takes pairs of CWT images as input -one\nserving as a reference or anchor (representing a known-good signal), and the\nother as a query (representing an unknown signal). The model then compares the\nembeddings of both inputs to determine whether they belong to the same class at\na given time step. Our approach demonstrates high accuracy in identifying\nanomalies on a real FAB process time-series dataset, offering a promising\nsolution for offline anomaly detection in process and tool trace data.\nMoreover, the approach is flexible and can be applied in both supervised and\nsemi-supervised settings.", "AI": {"tldr": "This paper proposes a machine learning methodology to address challenges in semiconductor manufacturing by transforming multi-variate time-series (MTS) data into image-based representations for anomaly detection.", "motivation": "The paper aims to address key challenges in anomaly prediction within semiconductor manufacturing, such as high data dimensionality, class imbalance, and the non-stationary behavior of production systems.", "method": "The methodology involves converting MTS data into Continuous Wavelet Transform (CWT) images, fine-tuning a pretrained VGG-16 model for image classification, and using a Siamese network to compare embeddings of image pairs for anomaly detection.", "result": "The proposed method achieves high accuracy in identifying anomalies in real FAB process time-series datasets and demonstrates flexibility for application in supervised and semi-supervised settings.", "conclusion": "The presented approach provides a generic and promising solution for anomaly detection in semiconductor fabrication, overcoming issues like complexity and delayed fault emergence."}}
{"id": "2507.02211", "pdf": "https://arxiv.org/pdf/2507.02211", "abs": "https://arxiv.org/abs/2507.02211", "authors": ["Gustavo C. Mangold", "Heitor C. M. Fernandes", "Mendeli H. Vainstein"], "title": "Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning", "categories": ["cs.AI", "cs.NE", "physics.comp-ph"], "comment": null, "summary": "Recent studies in the spatial prisoner's dilemma games with reinforcement\nlearning have shown that static agents can learn to cooperate through a diverse\nsort of mechanisms, including noise injection, different types of learning\nalgorithms and neighbours' payoff knowledge.In this work, using an independent\nmulti-agent Q-learning algorithm, we study the effects of dilution and mobility\nin the spatial version of the prisoner's dilemma. Within this setting,\ndifferent possible actions for the algorithm are defined, connecting with\nprevious results on the classical, non-reinforcement learning spatial\nprisoner's dilemma, showcasing the versatility of the algorithm in modeling\ndifferent game-theoretical scenarios and the benchmarking potential of this\napproach.As a result, a range of effects is observed, including evidence that\ngames with fixed update rules can be qualitatively equivalent to those with\nlearned ones, as well as the emergence of a symbiotic mutualistic effect\nbetween populations that forms when multiple actions are defined.", "AI": {"tldr": "The paper explores the impact of dilution and mobility in spatial prisoner's dilemma games using multi-agent Q-learning.", "motivation": "To understand cooperation mechanisms in spatial prisoner's dilemma games by integrating reinforcement learning techniques like Q-learning.", "method": "Utilization of independent multi-agent Q-learning algorithm to model and analyze diluted and mobile agents in spatial prisoner's dilemma scenarios.", "result": "Observational evidence reveals that fixed and learned update rules can lead to qualitative equivalence, and symbiotic effects emerge when defining multiple actions.", "conclusion": "Multi-agent Q-learning provides a versatile tool for examining complex game-theoretical phenomena and enhances understanding of cooperation in spatial dilemmas."}}
{"id": "2507.02302", "pdf": "https://arxiv.org/pdf/2507.02302", "abs": "https://arxiv.org/abs/2507.02302", "authors": ["Dohoon Kim", "Donghun Kang", "Taesup Moon"], "title": "DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "22 pages, 5 figures, ACL 2025 Main", "summary": "Domain-Adaptive Pre-training (DAP) has recently gained attention for its\neffectiveness in fine-tuning pre-trained models. Building on this, continual\nDAP has been explored to develop pre-trained models capable of incrementally\nincorporating different domain datasets. However, existing continual DAP\nmethods face several limitations: (1) high computational cost and GPU memory\nusage during training; (2) sensitivity to incremental data order; and (3)\nproviding a single, generalized model for all end tasks, which contradicts the\nessence of DAP. In this paper, we propose DoMIX, a novel approach that\naddresses these challenges by leveraging LoRA modules, a representative\nparameter-efficient fine-tuning (PEFT) method. Our approach enables efficient\nand parallel domain-adaptive pre-training that is robust to domain order and\neffectively utilizes accumulated knowledge to provide tailored pre-trained\nmodels for specific tasks. We also demonstrate that our method can be extended\nbeyond the DAP setting to standard LLM fine-tuning scenarios. Code is available\nat https://github.com/dohoonkim-ai/DoMIX.", "AI": {"tldr": "The paper introduces DoMIX, a parameter-efficient method to perform efficient and parallel domain-adaptive pre-training, addressing challenges in continual domain-adaptive pre-training.", "motivation": "The motivation stems from the limitations of existing continual DAP methods, including high computational cost, sensitivity to domain order, and the inability to provide models tailored to specific tasks.", "method": "The authors propose a novel approach leveraging LoRA modules, which are efficient parameter-efficient fine-tuning (PEFT) methods, to achieve efficient DAP that is robust to domain order and supports task-specific pre-trained models.", "result": "DoMIX demonstrated effectiveness in addressing the limitations of existing continual DAP and showed adaptability to standard large language model (LLM) fine-tuning scenarios.", "conclusion": "DoMIX is a step forward in the domain-adaptive pre-training space, providing a flexible and resource-efficient method for pre-training models specialized for specific tasks, with code available for public use."}}
{"id": "2507.02620", "pdf": "https://arxiv.org/pdf/2507.02620", "abs": "https://arxiv.org/abs/2507.02620", "authors": ["Xing Liu", "Lizhuo Luo", "Ming Tang", "Chao Huang"], "title": "FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference", "categories": ["cs.DC", "cs.AI"], "comment": "16 pages, and the last 3 are appendix", "summary": "Distributed inference serves as a promising approach to enabling the\ninference of large language models (LLMs) at the network edge. It distributes\nthe inference process to multiple devices to ensure that the LLMs can fit into\nthe device memory. Recent pipeline-based approaches have the potential to\nparallelize communication and computation, which helps reduce inference\nlatency. However, the benefit diminishes when the inference request at the\nnetwork edge is sparse, where pipeline is typically at low utilization. To\nenable efficient distributed LLM inference at the edge, we propose\n\\textbf{FlowSpec}, a pipeline-parallel tree-based speculative decoding\nframework. FlowSpec incorporates three key mechanisms to improve decoding\nefficiency: 1) score-based step-wise verification prioritizes more important\ndraft tokens to bring earlier accpeted tokens; 2) efficient draft management to\nprune invalid tokens while maintaining correct causal relationship during\nverification; 3) dynamic draft expansion strategies to supply high-quality\nspeculative inputs. These techniques work in concert to enhance both pipeline\nutilization and speculative efficiency. We evaluate FlowSpec on a real-world\ntestbed with other baselines. Experimental results demonstrate that our\nproposed framework significantly improves inference speed across diverse models\nand configurations, achieving speedup ratios 1.36$\\times$-1.77$\\times$ compared\nto baselines. Our code is publicly available at\n\\href{https://github.com/Leosang-lx/FlowSpec#}{https://github.com/Leosang-lx/FlowSpec\\#}", "AI": {"tldr": "The paper introduces FlowSpec, a tree-based speculative decoding framework for distributed inference of large language models (LLMs) at the edge, achieving 1.36\u00d7-1.77\u00d7 speedup over baselines.", "motivation": "Current pipeline-based approaches for distributed inference at the edge struggle with low utilization when requests are sparse, leading to inefficiencies.", "method": "FlowSpec leverages pipeline-parallel speculative decoding with mechanisms like score-based step-wise verification, efficient draft management, and dynamic draft expansion strategies to enhance decoding efficiency.", "result": "Experimental evaluation demonstrated that FlowSpec outpaces existing baselines in inference speed across diverse setups, with speedups ranging from 1.36\u00d7 to 1.77\u00d7.", "conclusion": "FlowSpec represents a significant advancement in efficient distributed LLM inference, improving latency and throughput through innovative speculative decoding techniques."}}
{"id": "2507.02450", "pdf": "https://arxiv.org/pdf/2507.02450", "abs": "https://arxiv.org/abs/2507.02450", "authors": ["Kai Chen", "Mingzhang Wang", "Songting Li", "Douglas Zhou"], "title": "Network structural change point detection and reconstruction for balanced neuronal networks", "categories": ["q-bio.NC"], "comment": "22 pages, 5 figures", "summary": "Understanding brain dynamics and functions critically depends on knowledge of\nthe network connectivity among neurons. However, the complexity of brain\nstructural connectivity, coupled with continuous modifications driven by\nsynaptic plasticity, makes its direct experimental measurement particularly\nchallenging. Conventional connectivity inference methods based on neuronal\nrecordings often assumes a static underlying structural connectivity and\nrequires stable statistical features of neural activities, making them\nunsuitable for reconstructing structural connectivity that undergoes changes.\nTo fulfill the needs of reconstructing networks undergoing potential structural\nchanges, we propose a unified network reconstruction framework that combines\nconnectivity-induced change point detection (CPD) with pairwise time-delayed\ncorrelation coefficient (TDCC) method. For general neuronal networks in\nbalanced regimes, we develop a theoretical analysis for discriminating changes\nin structural connectivity based on the fluctuation of neuronal voltage time\nseries. We then demonstrate a pairwise TDCC method to reconstruct the network\nusing spike train recordings segmented at the detected change points. We show\nthe effectiveness of our CPD-TDCC network reconstruction using large-scale\nnetwork simulations with multiple neuronal models. Crucially, our method\naccommodates networks with changes in both network topologies and synaptic\ncoupling strengths while retaining accuracy even with sparsely sampled\nsubnetwork data, achieving a critical advancement for practical applications in\nreal experimental situations. Our CPD-TDCC framework addresses the critical gap\nin network reconstruction by accounting connectivity-induced changes points,\npotentially offering a valuable tool for studying structure and dynamics in the\ncortical brain.", "AI": {"tldr": "The paper proposes a novel method for reconstructing dynamic neuronal networks undergoing structural changes using a CPD-TDCC framework, addressing prior limitations tied to static connectivity assumptions.", "motivation": "Conventional methods for inferring brain network connectivity struggle with dynamic changes and modifications in structural connectivity, limiting their applicability to real-world neuronal networks.", "method": "The authors introduce a unified framework that combines connectivity-induced change point detection (CPD) with time-delayed correlation coefficient (TDCC) analysis to identify structural changes and reconstruct the network using segmented neuronal spike train recordings.", "result": "Simulation experiments with various neuronal models showed the framework accurately reconstructed networks undergoing changes in topology and synaptic coupling strength, even with sparsely sampled subnetwork data.", "conclusion": "The CPD-TDCC framework represents an innovative approach to address dynamic changes in brain connectivity, making it suitable for practical experimental applications in neurobiology."}}
{"id": "2507.02182", "pdf": "https://arxiv.org/pdf/2507.02182", "abs": "https://arxiv.org/abs/2507.02182", "authors": ["Fangjian Lei", "Jiawen Liu", "Shayan Noei", "Ying Zou", "Derek Truong", "William Alexander"], "title": "Enhancing COBOL Code Explanations: A Multi-Agents Approach Using Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Common Business Oriented Language (COBOL) is a programming language used to\ndevelop business applications that are widely adopted by financial, business,\nand government agencies. Due to its age, complexity, and declining number of\nCOBOL developers, maintaining COBOL codebases is becoming increasingly\nchallenging. In particular, the lack of documentation makes it difficult for\nnew developers to effectively understand and maintain COBOL systems. Existing\nresearch utilizes large language models (LLMs) to explain the functionality of\ncode snippets. However, COBOL presents unique challenges due to its\narchitectural and syntactical differences, which often cause its code to exceed\nthe token window size of LLMs. In this work, we propose a multi-agent approach\nthat leverages two LLM-based agents working collaboratively to generate\nexplanations for functions, files, and the overall project. These agents\nincorporate together by utilizing contextual information from the codebase into\nthe code explanation prompts. We evaluate the effectiveness of our approach\nusing 14 open-source, real-world COBOL projects. Our results indicate that our\napproach performs significantly better than the baseline in function code\nexplanation, with improvements of 12.67%, 18.59%, and 0.62% in terms of METEOR,\nchrF, and SentenceBERT scores, respectively. At the file level, our approach\neffectively explains both short and long COBOL files that exceed the token\nwindow size of LLMs and surpass the baseline by 4.21%, 10.72%, and 14.68% in\nexplaining the purpose, functionality, and clarity of the generated\nexplanation. At the project level, our approach generates explanations that\nconvey the functionality and purpose of 82% of the selected projects.", "AI": {"tldr": "The paper addresses the difficulties of explaining COBOL code using LLMs due to its syntactical and architectural challenges, proposing a multi-agent LLM approach to improve code explanation at function, file, and project levels.", "motivation": "The motivation stems from the challenges in maintaining COBOL systems due to a declining number of COBOL developers, the language's complexity, and the lack of documentation, making it hard for new developers to understand the code.", "method": "A multi-agent approach is proposed where two LLM-based agents collaborate to provide explanations for COBOL code, leveraging contextual information to overcome token window size limitations.", "result": "The approach outperforms baselines, showing significant improvements in explaining COBOL code, with metrics like METEOR, chrF, and SentenceBERT improving by 12.67%, 18.59%, and 0.62% at the function level and notable enhancements in file and project-level explanations.", "conclusion": "The study concludes that the multi-agent method is effective in addressing the unique challenges of COBOL code by significantly improving explanation quality for functions, files, and projects."}}
{"id": "2507.02245", "pdf": "https://arxiv.org/pdf/2507.02245", "abs": "https://arxiv.org/abs/2507.02245", "authors": ["Minghao Ning", "Yufeng Yang", "Keqi Shu", "Shucheng Huang", "Jiaming Zhong", "Maryam Salehi", "Mahdi Rahmani", "Yukun Lu", "Chen Sun", "Aladdin Saleh", "Ehsan Hashemi", "Amir Khajepour"], "title": "CoInfra: A Large-Scale Cooperative Infrastructure Perception System and Dataset in Adverse Weather", "categories": ["cs.RO"], "comment": "This paper has been submitted to the IEEE Transactions on Robotics\n  for review", "summary": "We present CoInfra, a large-scale cooperative infrastructure perception\nsystem and dataset designed to advance robust multi-agent perception under\nreal-world and adverse weather conditions. The CoInfra system includes 14 fully\nsynchronized sensor nodes, each equipped with dual RGB cameras and a LiDAR,\ndeployed across a shared region and operating continuously to capture all\ntraffic participants in real-time. A robust, delay-aware synchronization\nprotocol and a scalable system architecture that supports real-time data\nfusion, OTA management, and remote monitoring are provided in this paper. On\nthe other hand, the dataset was collected in different weather scenarios,\nincluding sunny, rainy, freezing rain, and heavy snow and includes 195k LiDAR\nframes and 390k camera images from 8 infrastructure nodes that are globally\ntime-aligned and spatially calibrated. Furthermore, comprehensive 3D bounding\nbox annotations for five object classes (i.e., car, bus, truck, person, and\nbicycle) are provided in both global and individual node frames, along with\nhigh-definition maps for contextual understanding. Baseline experiments\ndemonstrate the trade-offs between early and late fusion strategies, the\nsignificant benefits of HD map integration are discussed. By openly releasing\nour dataset, codebase, and system documentation at\nhttps://github.com/NingMingHao/CoInfra, we aim to enable reproducible research\nand drive progress in infrastructure-supported autonomous driving, particularly\nin challenging, real-world settings.", "AI": {"tldr": "The paper introduces CoInfra, a multi-agent perception system and dataset for research in autonomous driving under various weather conditions. It includes 14 synchronized sensor nodes, real-world weather data, and 3D annotations for traffic participants.", "motivation": "To address the challenges of robust multi-agent perception in autonomous driving, especially under adverse weather conditions, through a cooperative infrastructure-based approach.", "method": "The proposed CoInfra system uses 14 sensor nodes with cameras and LiDARs for data capture. It employs synchronization protocols and scalable architectures to ensure accurate real-time perception. The accompanying dataset was collected under diverse weather scenarios and includes aligned data with annotated 3D bounding boxes.", "result": "Key dataset characteristics include 195k LiDAR frames, 390k camera images, high-definition maps, and extensive annotations. Experiments showcase the trade-offs between fusion strategies and highlight the benefits of integrating HD maps.", "conclusion": "By releasing the dataset, codebase, and system documentation publicly, the paper aims to drive progress in autonomous driving research through reproducibility and a focus on challenging environments."}}
{"id": "2507.01980", "pdf": "https://arxiv.org/pdf/2507.01980", "abs": "https://arxiv.org/abs/2507.01980", "authors": ["Linh Nguyen", "Marcel Boersma", "Erman Acar"], "title": "Detecting Fraud in Financial Networks: A Semi-Supervised GNN Approach with Granger-Causal Explanations", "categories": ["q-fin.ST", "cs.LG", "stat.ML"], "comment": null, "summary": "Fraudulent activity in the financial industry costs billions annually.\nDetecting fraud, therefore, is an essential yet technically challenging task\nthat requires carefully analyzing large volumes of data. While machine learning\n(ML) approaches seem like a viable solution, applying them successfully is not\nso easy due to two main challenges: (1) the sparsely labeled data, which makes\nthe training of such approaches challenging (with inherent labeling costs), and\n(2) lack of explainability for the flagged items posed by the opacity of ML\nmodels, that is often required by business regulations. This article proposes\nSAGE-FIN, a semi-supervised graph neural network (GNN) based approach with\nGranger causal explanations for Financial Interaction Networks. SAGE-FIN learns\nto flag fraudulent items based on weakly labeled (or unlabelled) data points.\nTo adhere to regulatory requirements, the flagged items are explained by\nhighlighting related items in the network using Granger causality. We\nempirically validate the favorable performance of SAGE-FIN on a real-world\ndataset, Bipartite Edge-And-Node Attributed financial network (Elliptic++),\nwith Granger-causal explanations for the identified fraudulent items without\nany prior assumption on the network structure.", "AI": {"tldr": "Fraud detection in finance faces challenges like sparse data and lack of explainability in machine learning models. SAGE-FIN uses a semi-supervised graph neural network with Granger causal explanations to tackle these issues effectively.", "motivation": "Fraud detection is crucial in the financial sector, costing billions annually. The paper aims to address challenges in sparsely labeled datasets and regulatory need for explainability in ML models.", "method": "The paper introduces SAGE-FIN, a semi-supervised graph neural network combined with Granger causal explanations, designed to evaluate fraudulent activities in financial interaction networks.", "result": "SAGE-FIN demonstrates favorable performance on the real-world financial dataset, Elliptic++, detecting fraud and providing causal explanations without prior assumptions on network structure.", "conclusion": "SAGE-FIN offers a promising solution to detect and explain financial fraud by leveraging semi-supervised learning and causality analysis, meeting regulatory requirements and overcoming technical limitations."}}
{"id": "2507.02217", "pdf": "https://arxiv.org/pdf/2507.02217", "abs": "https://arxiv.org/abs/2507.02217", "authors": ["Brandon Trabucco", "Qasim Wani", "Benjamin Pikus", "Vasu Sharma"], "title": "Understanding Trade offs When Conditioning Synthetic Data", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Learning robust object detectors from only a handful of images is a critical\nchallenge in industrial vision systems, where collecting high quality training\ndata can take months. Synthetic data has emerged as a key solution for data\nefficient visual inspection and pick and place robotics. Current pipelines rely\non 3D engines such as Blender or Unreal, which offer fine control but still\nrequire weeks to render a small dataset, and the resulting images often suffer\nfrom a large gap between simulation and reality. Diffusion models promise a\nstep change because they can generate high quality images in minutes, yet\nprecise control, especially in low data regimes, remains difficult. Although\nmany adapters now extend diffusion beyond plain text prompts, the effect of\ndifferent conditioning schemes on synthetic data quality is poorly understood.\nWe study eighty diverse visual concepts drawn from four standard object\ndetection benchmarks and compare two conditioning strategies: prompt based and\nlayout based. When the set of conditioning cues is narrow, prompt conditioning\nyields higher quality synthetic data; as diversity grows, layout conditioning\nbecomes superior. When layout cues match the full training distribution,\nsynthetic data raises mean average precision by an average of thirty four\npercent and by as much as one hundred seventy seven percent compared with using\nreal data alone.", "AI": {"tldr": "The paper explores using diffusion models for generating synthetic data for training robust object detectors in industrial vision systems, comparing two conditioning methods: prompt-based and layout-based.", "motivation": "The need to efficiently train object detectors with limited high-quality images and existing challenges in generating synthetic data via traditional 3D rendering pipelines motivates exploring diffusion models for better synthetic data generation.", "method": "The authors evaluate synthetic data generation via diffusion models with different conditioning schemes\u2014prompt-based and layout-based\u2014on object detection benchmarks.", "result": "Prompt conditioning performs better under narrow conditioning cues, while layout conditioning outperforms prompt conditioning as data diversity grows. Synthetic data improves detection metrics significantly, raising mean average precision by 34% on average and up to 177% in certain cases over real data usage.", "conclusion": "Diffusion model-based synthetic data generation, tailored appropriately with effective conditioning strategies, offers a compelling solution for boosting object detection performance in data-limited regimes."}}
{"id": "2507.02001", "pdf": "https://arxiv.org/pdf/2507.02001", "abs": "https://arxiv.org/abs/2507.02001", "authors": ["Anurag Arnab", "Ahmet Iscen", "Mathilde Caron", "Alireza Fathi", "Cordelia Schmid"], "title": "Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames", "categories": ["cs.LG"], "comment": null, "summary": "Despite recent advances in Vision-Language Models (VLMs), long-video\nunderstanding remains a challenging problem. Although state-of-the-art\nlong-context VLMs can process around 1000 input frames, they still struggle to\neffectively leverage this sequence length, and succumb to irrelevant\ndistractors within the context window. We present Temporal Chain of Thought, an\ninference strategy for video question-answering that curates the model's input\ncontext. We use the VLM itself to iteratively identify and extract the most\nrelevant frames from the video, which are then used for answering. We\ndemonstrate how leveraging more computation at inference-time to select the\nmost relevant context leads to improvements in accuracy, in agreement with\nrecent work on inference-time scaling of LLMs. Moreover, we achieve\nstate-of-the-art results on 4 diverse video question-answering datasets,\nshowing consistent improvements with 3 different VLMs. In particular, our\nmethod shines on longer videos which would not otherwise fit within the model's\ncontext window: On longer videos of more than 1 hour on LVBench, our approach\nusing a context window of 32K outperforms the same VLM using standard inference\nwith a 700K context window by 2.8 points.", "AI": {"tldr": "The paper introduces Temporal Chain of Thought, an inference strategy for video question-answering, which improves model accuracy by selecting the most relevant frames from long videos.", "motivation": "Long-video understanding in Vision-Language Models remains challenging due to ineffective use of long-context sequences and distractions from irrelevant frames.", "method": "Temporal Chain of Thought identifies and extracts the most relevant frames iteratively using the VLM itself, optimizing context selection during inference.", "result": "The method achieves state-of-the-art results across 4 video QA datasets, especially excelling on longer videos with improved performance using 32K context windows.", "conclusion": "Leveraging enhanced computation and targeted context selection at inference leads to improved video QA accuracy, particularly for long videos, and demonstrates scalability of VLMs at inference-time."}}
{"id": "2507.02253", "pdf": "https://arxiv.org/pdf/2507.02253", "abs": "https://arxiv.org/abs/2507.02253", "authors": ["Jungkoo Kang"], "title": "Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation", "categories": ["cs.AI"], "comment": "20 pages, 7 figures", "summary": "Progress in enhancing large language model (LLM) planning and reasoning\ncapabilities is significantly hampered by the bottleneck of scalable, reliable\ndata generation and evaluation. To overcome this, I introduce NL2FLOW, a fully\nautomated system for parametrically generating planning problems - expressed in\nnatural language, a structured intermediate representation, and formal PDDL -\nand rigorously evaluating the quality of generated plans. I demonstrate\nNL2FLOW's capabilities by generating a dataset of 2296 problems in the\nautomated workflow generation domain and evaluating multiple open-sourced,\ninstruct-tuned LLMs. My results reveal that the highest performing models\nachieved 86% success in generating valid plans and 69% in generating optimal\nplans, specifically for problems with feasible solutions. Regression analysis\nshows that the influence of problem characteristics on plan generation is\ncontingent on both model and prompt design. Notably, I observed that the\nhighest success rate for translating natural language into a JSON\nrepresentation of a plan was lower than the highest rate of generating a valid\nplan directly. This suggests that unnecessarily decomposing the reasoning task\n- introducing intermediate translation steps - may actually degrade\nperformance, implying a benefit to models capable of reasoning directly from\nnatural language to action. As I scale LLM reasoning to increasingly complex\nproblems, the bottlenecks and sources of error within these systems will\ninevitably shift. Therefore, a dynamic understanding of these limitations - and\nthe tools to systematically reveal them - will be crucial for unlocking the\nfull potential of LLMs as intelligent problem solvers.", "AI": {"tldr": "The paper introduces NL2FLOW, an automated system for generating and evaluating planning problems to improve large language model (LLM) reasoning capabilities, showcasing its applications with datasets and model performance comparisons.", "motivation": "To address the bottleneck in scalable, reliable data generation and evaluation for enhancing LLM planning and reasoning capabilities.", "method": "NL2FLOW automatically generates planning problems expressed in natural language, structured representations, and formal PDDL, along with methods for rigorous evaluation of LLMs.", "result": "NL2FLOW generated a dataset of 2296 planning problems, tested instruct-tuned LLMs, achieving 86% success for valid plans and revealing performance impacts from task characteristics and prompt design.", "conclusion": "Decomposing reasoning tasks unnecessarily may hurt performance, encouraging direct translation from natural language to actions, while dynamic evaluation tools are essential for scaling LLMs to complex problems."}}
{"id": "2507.02510", "pdf": "https://arxiv.org/pdf/2507.02510", "abs": "https://arxiv.org/abs/2507.02510", "authors": ["Ahmed G. Habashi", "Ahmed M. Azab", "Seif Eldawlatly", "Gamal M. Aly"], "title": "TFOC-Net: A Short-time Fourier Transform-based Deep Learning Approach for Enhancing Cross-Subject Motor Imagery Classification", "categories": ["cs.LG", "cs.HC", "cs.NE"], "comment": null, "summary": "Cross-subject motor imagery (CS-MI) classification in brain-computer\ninterfaces (BCIs) is a challenging task due to the significant variability in\nElectroencephalography (EEG) patterns across different individuals. This\nvariability often results in lower classification accuracy compared to\nsubject-specific models, presenting a major barrier to developing\ncalibration-free BCIs suitable for real-world applications. In this paper, we\nintroduce a novel approach that significantly enhances cross-subject MI\nclassification performance through optimized preprocessing and deep learning\ntechniques. Our approach involves direct classification of Short-Time Fourier\nTransform (STFT)-transformed EEG data, optimized STFT parameters, and a\nbalanced batching strategy during training of a Convolutional Neural Network\n(CNN). This approach is uniquely validated across four different datasets,\nincluding three widely-used benchmark datasets leading to substantial\nimprovements in cross-subject classification, achieving 67.60% on the BCI\nCompetition IV Dataset 1 (IV-1), 65.96% on Dataset 2A (IV-2A), and 80.22% on\nDataset 2B (IV-2B), outperforming state-of-the-art techniques. Additionally, we\nsystematically investigate the classification performance using MI windows\nranging from the full 4-second window to 1-second windows. These results\nestablish a new benchmark for generalizable, calibration-free MI classification\nin addition to contributing a robust open-access dataset to advance research in\nthis domain.", "AI": {"tldr": "This paper focuses on improving cross-subject motor imagery (CS-MI) classification for brain-computer interfaces (BCIs) using optimized preprocessing, STFT-transformed EEG data, and a CNN, achieving notable accuracy improvements over existing methods.", "motivation": "To address the challenge of significant variability in EEG patterns across individuals, which hampers accuracy in calibration-free BCIs for real-world applications.", "method": "The authors utilize an optimized preprocessing pipeline that includes Short-Time Fourier Transform (STFT) with parameter tuning and a balanced batching training strategy for a convolutional neural network (CNN).", "result": "Their method achieved classification accuracy of 67.60%, 65.96%, and 80.22% on three benchmark BCI datasets, significantly outperforming state-of-the-art techniques.", "conclusion": "This study sets a new benchmark for generalizable, calibration-free motor imagery classification, offering a pathway for developing BCIs suitable for real-world use while contributing an open-access dataset for further research."}}
{"id": "2507.02660", "pdf": "https://arxiv.org/pdf/2507.02660", "abs": "https://arxiv.org/abs/2507.02660", "authors": ["Deepak Narayan Gadde", "Keerthan Kopparam Radhakrishna", "Vaisakh Naduvodi Viswambharan", "Aman Kumar", "Djones Lettnin", "Wolfgang Kunz", "Sebastian Simon"], "title": "Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification", "categories": ["cs.AI", "cs.AR"], "comment": "To appear at the 38th SBC/SBMicro/IEEE Symposium on Integrated\n  Circuits and Systems Design (SBCCI), August 25-29, 2025, Manaus, BRAZIL", "summary": "Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is\ntheir development process. Hardware design verification entails a methodical\nand disciplined approach to the planning, development, execution, and sign-off\nof functionally correct hardware designs. This tedious process requires\nsignificant effort and time to ensure a bug-free tape-out. The field of Natural\nLanguage Processing has undergone a significant transformation with the advent\nof Large Language Models (LLMs). These powerful models, often referred to as\nGenerative AI (GenAI), have revolutionized how machines understand and generate\nhuman language, enabling unprecedented advancements in a wide array of\napplications, including hardware design verification. This paper presents an\nagentic AI-based approach to hardware design verification, which empowers AI\nagents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage\nin a more dynamic, iterative, and self-reflective process, ultimately\nperforming end-to-end hardware design and verification. This methodology is\nevaluated on five open-source designs, achieving over 95% coverage with reduced\nverification time while demonstrating superior performance, adaptability, and\nconfigurability.", "AI": {"tldr": "The paper introduces an AI-driven approach using Large Language Models (LLMs) for hardware design verification, achieving 95% coverage on open-source designs with reduced time.", "motivation": "The paper aims to address the increasingly complex process of hardware design verification which demands significant time and effort.", "method": "An agentic AI-based methodology is proposed, combining AI agents with Human-in-the-Loop (HITL) interaction for iterative and reflective design and verification processes.", "result": "The approach demonstrated over 95% coverage and reduced verification time on five open-source hardware designs.", "conclusion": "The agentic AI-based methodology proves to be effective, showcasing enhanced performance, adaptability, and configurability in hardware design verification."}}
{"id": "2507.02357", "pdf": "https://arxiv.org/pdf/2507.02357", "abs": "https://arxiv.org/abs/2507.02357", "authors": ["Christian Jaumann", "Annemarie Friedrich", "Rainer Lienhart"], "title": "Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models", "categories": ["cs.CL"], "comment": "Accepted at 5th Workshop on Scholarly Document Processing @ ACL 2025", "summary": "This paper describes our system for the SciVQA 2025 Shared Task on Scientific\nVisual Question Answering. Our system employs an ensemble of two Multimodal\nLarge Language Models and various few-shot example retrieval strategies. The\nmodel and few-shot setting are selected based on the figure and question type.\nWe also select answers based on the models' confidence levels. On the blind\ntest data, our system ranks third out of seven with an average F1 score of\n85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.", "AI": {"tldr": "The paper presents a system employing multimodal LLMs and retrieval strategies for the SciVQA 2025 task, achieving third place with an 85.12 average F1 score.", "motivation": "To develop an effective system for answering scientific visual questions in the SciVQA 2025 Shared Task.", "method": "The system employs an ensemble of two Multimodal Large Language Models and uses figure-specific and question-type-specific few-shot example retrieval strategies, with answer selection based on model confidence.", "result": "The system ranked third out of seven in the shared task, achieving an average F1 score of 85.12 across evaluation metrics.", "conclusion": "The approach demonstrated competitive performance and the developed system/code is made publicly available for further use."}}
{"id": "2507.02376", "pdf": "https://arxiv.org/pdf/2507.02376", "abs": "https://arxiv.org/abs/2507.02376", "authors": ["Chung-ju Huang", "Ziqi Zhang", "Yinggui Wang", "Binghui Wang", "Tao Wei", "Leye Wang"], "title": "VeFIA: An Efficient Inference Auditing Framework for Vertical Federated Collaborative Software", "categories": ["cs.SE", "cs.AI", "cs.DC"], "comment": null, "summary": "Vertical Federated Learning (VFL) is a distributed AI software deployment\nmechanism for cross-silo collaboration without accessing participants' data.\nHowever, existing VFL work lacks a mechanism to audit the execution correctness\nof the inference software of the data party. To address this problem, we design\na Vertical Federated Inference Auditing (VeFIA) framework. VeFIA helps the task\nparty to audit whether the data party's inference software is executed as\nexpected during large-scale inference without leaking the data privacy of the\ndata party or introducing additional latency to the inference system. The core\nof VeFIA is that the task party can use the inference results from a framework\nwith Trusted Execution Environments (TEE) and the coordinator to validate the\ncorrectness of the data party's computation results. VeFIA guarantees that, as\nlong as the abnormal inference exceeds 5.4%, the task party can detect\nexecution anomalies in the inference software with a probability of 99.99%,\nwithout incurring any additional online inference latency. VeFIA's random\nsampling validation achieves 100% positive predictive value, negative\npredictive value, and true positive rate in detecting abnormal inference. To\nthe best of our knowledge, this is the first paper to discuss the correctness\nof inference software execution in VFL.", "AI": {"tldr": "The paper presents the VeFIA framework to audit the correctness of inference software in Vertical Federated Learning without compromising data privacy or increasing latency.", "motivation": "Existing Vertical Federated Learning implementations lack mechanisms to verify proper execution of data party inference software.", "method": "The VeFIA framework integrates Trusted Execution Environments (TEE) and a coordinator to validate computational correctness while preserving privacy and maintaining latency.", "result": "VeFIA detects inference abnormalities above 5.4% with 99.99% accuracy, achieving perfect predictive metrics and true positive rates.", "conclusion": "VeFIA successfully audits inference correctness in VFL, addressing a crucial gap in ensuring software accountability without privacy or performance trade-offs."}}
{"id": "2507.02614", "pdf": "https://arxiv.org/pdf/2507.02614", "abs": "https://arxiv.org/abs/2507.02614", "authors": ["Diogo Sousa"], "title": "A Dopamine-Serotonin Theory of Consciousness", "categories": ["q-bio.NC", "q-bio.OT"], "comment": "37 pages, 16 figures, 3 tables. Includes extensive theoretical\n  modeling, data analysis of 70,290 patient-nights, and supplementary sections\n  on developmental trajectories and future therapeutic protocols. All analysis\n  code is available at: https://github.com/DiogoSousa509/dopa-5ht-consciousness", "summary": "This work presents a comprehensive theory of consciousness grounded in\nmathematical formalism and supported by clinical data analysis. The framework\ndeveloped herein demonstrates that consciousness exists as a continuous,\nnon-monotonic function across a high-dimensional neurochemical space, with\ndopamine serving as the primary intensity regulator and serotonin (5-HT2A) as\nthe complexity modulator. This work offers mechanistic explanations for the\nfull spectrum of conscious states, from deep sleep and psychosis to the\nultimate collapse in neural death. The theory explains paradoxical phenomena\nsuch as prefrontal cortex hypoactivity during seizures, the evolutionary\npersistence of psychosis-prone individuals, and why controlled administration\nof classical 5-HT2A agonists shows a comparatively low incidence of serious\nmedical events (< 0.01 % in modern clinical trials), while dopaminergic excess\nproves rapidly lethal. The framework is tested using 70,290 sleep nights from\n242 Parkinson's disease patients, using disease severity (UPDRS) as a proxy for\nsystem integrity and medication (LEDD) as a proxy for dopaminergic input. The\nanalysis reveals a significant LEDD x UPDRS interaction (beta=-1.7, p<.0001),\nconfirming the model's prediction of state-dependent, non-linear dynamics.", "AI": {"tldr": "A mathematical theory of consciousness explains its neurochemical basis, focusing on dopamine and serotonin roles, validated by clinical data.", "motivation": "There is a lack of comprehensive theoretical explanations for diverse conscious states across neurochemical dynamics.", "method": "The study proposes a formalized neurochemical model and tests it using clinical data from Parkinson's disease patients, analyzing interactions between disease severity and dopaminergic medication.", "result": "The analysis confirms the hypothesis, showing a significant interaction between dopamine input and disease severity, supporting state-dependent consciousness dynamics.", "conclusion": "The proposed framework successfully accounts for varying conscious states and explains several paradoxical phenomena in neuroscience and pharmacology."}}
{"id": "2507.02318", "pdf": "https://arxiv.org/pdf/2507.02318", "abs": "https://arxiv.org/abs/2507.02318", "authors": ["Chen Yang", "Ziqi Wang", "Yanjie Jiang", "Lin Yang", "Yuteng Zheng", "Jianyi Zhou", "Junjie Chen"], "title": "Precisely Detecting Python Type Errors via LLM-based Unit Test Generation", "categories": ["cs.SE"], "comment": null, "summary": "Type errors in Python often lead to runtime failures, posing significant\nchallenges to software reliability and developer productivity. Existing static\nanalysis tools aim to detect such errors without execution but frequently\nsuffer from high false positive rates. Recently, unit test generation\ntechniques offer great promise in achieving high test coverage, but they often\nstruggle to produce bug-revealing tests without tailored guidance. To address\nthese limitations, we present RTED, a novel type-aware test generation\ntechnique for automatically detecting Python type errors. Specifically, RTED\ncombines step-by-step type constraint analysis with reflective validation to\nguide the test generation process and effectively suppress false positives. We\nevaluated RTED on two widely-used benchmarks, BugsInPy and TypeBugs.\nExperimental results show that RTED can detect 22-29 more benchmarked type\nerrors than four state-of-the-art techniques. RTED is also capable of producing\nfewer false positives, achieving an improvement of 173.9%-245.9% in precision.\nFurthermore, RTED successfully discovered 12 previously unknown type errors\nfrom six real-world open-source Python projects.", "AI": {"tldr": "RTED presents a novel approach to detect Python type errors using type-aware test generation, outperforming existing techniques in accuracy and precision.", "motivation": "Type errors in Python lead to runtime failures affecting software reliability and productivity; existing tools either produce high false positives or struggle to generate effective test cases.", "method": "RTED utilizes step-by-step type constraint analysis coupled with reflective validation to guide test case generation, effectively suppressing false positives.", "result": "RTED outperformed four state-of-the-art techniques, detecting up to 29 more type errors and reducing false positives by up to 245.9%; it also identified previously unknown errors in real-world projects.", "conclusion": "RTED significantly enhances Python type error detection, improving both coverage and precision, and could aid developers in boosting software reliability."}}
{"id": "2507.02313", "pdf": "https://arxiv.org/pdf/2507.02313", "abs": "https://arxiv.org/abs/2507.02313", "authors": ["Zengjie Zhang", "Giannis Badakis", "Michalis Galanis", "Adem Bavar\u015fi", "Edwin van Hassel", "Mohsen Alirezaei", "Sofie Haesaert"], "title": "A Vehicle-in-the-Loop Simulator with AI-Powered Digital Twins for Testing Automated Driving Controllers", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "Simulators are useful tools for testing automated driving controllers.\nVehicle-in-the-loop (ViL) tests and digital twins (DTs) are widely used\nsimulation technologies to facilitate the smooth deployment of controllers to\nphysical vehicles. However, conventional ViL tests rely on full-size vehicles,\nrequiring large space and high expenses. Also, physical-model-based DT suffers\nfrom the reality gap caused by modeling imprecision. This paper develops a\ncomprehensive and practical simulator for testing automated driving controllers\nenhanced by scaled physical cars and AI-powered DT models. The scaled cars\nallow for saving space and expenses of simulation tests. The AI-powered DT\nmodels ensure superior simulation fidelity. Moreover, the simulator integrates\nwell with off-the-shelf software and control algorithms, making it easy to\nextend. We use a filtered control benchmark with formal safety guarantees to\nshowcase the capability of the simulator in validating automated driving\ncontrollers. Experimental studies are performed to showcase the efficacy of the\nsimulator, implying its great potential in validating control solutions for\nautonomous vehicles and intelligent traffic.", "AI": {"tldr": "A practical simulator was developed featuring scaled physical cars and AI-enhanced digital twin models to save costs, increase fidelity, and test automated driving controllers effectively.", "motivation": "The demand for efficient and cost-effective testing mechanisms for automated driving controllers, overcoming limitations like high expenses or reality gaps in conventional simulation techniques.", "method": "The study introduced scaled physical cars and AI-based digital twins, integrating these elements with software and control algorithms, along with formal safety benchmarks.", "result": "The developed simulator demonstrated high efficacy in experimental tests, validating its capabilities in testing autonomous vehicle controllers and intelligent traffic solutions.", "conclusion": "The simulator presents a feasible, scalable tool for reliable testing of autonomous driving systems, paving the way for enhanced deployment and validation efficiency."}}
{"id": "2507.02014", "pdf": "https://arxiv.org/pdf/2507.02014", "abs": "https://arxiv.org/abs/2507.02014", "authors": ["Anoushka Harit", "Zhongtian Sun", "Suncica Hadzidedic"], "title": "ManifoldMind: Dynamic Hyperbolic Reasoning for Trustworthy Recommendations", "categories": ["cs.IR", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "We introduce ManifoldMind, a probabilistic geometric recommender system for\nexploratory reasoning over semantic hierarchies in hyperbolic space. Unlike\nprior methods with fixed curvature and rigid embeddings, ManifoldMind\nrepresents users, items, and tags as adaptive-curvature probabilistic spheres,\nenabling personalised uncertainty modeling and geometry-aware semantic\nexploration. A curvature-aware semantic kernel supports soft, multi-hop\ninference, allowing the model to explore diverse conceptual paths instead of\noverfitting to shallow or direct interactions. Experiments on four public\nbenchmarks show superior NDCG, calibration, and diversity compared to strong\nbaselines. ManifoldMind produces explicit reasoning traces, enabling\ntransparent, trustworthy, and exploration-driven recommendations in sparse or\nabstract domains.", "AI": {"tldr": "ManifoldMind is a geometric recommender system leveraging hyperbolic space for reasoning over semantic hierarchies. It uses adaptive-curvature probabilistic spheres for personalized modeling and enables diverse semantic exploration.", "motivation": "The paper aims to improve reasoning and recommendation in semantic hierarchies by introducing personalized uncertainty modeling and geometry-aware exploration of abstract or sparse domains.", "method": "ManifoldMind uses adaptive-curvature probabilistic spheres to represent users, items, and tags, paired with a curvature-aware semantic kernel for soft multi-hop inference and conceptual path exploration.", "result": "Experiments on four benchmarks demonstrate superior recommendation performance in terms of NDCG, calibration, and diversity compared to existing methods.", "conclusion": "ManifoldMind enhances exploratory reasoning and recommendation transparency through explicit reasoning traces, making it suitable for complex semantic challenges."}}
{"id": "2507.02222", "pdf": "https://arxiv.org/pdf/2507.02222", "abs": "https://arxiv.org/abs/2507.02222", "authors": ["Tian Gao", "Zhiyuan Zhang", "Kaijie Yin", "Xu-Cheng Zhong", "Hui Kong"], "title": "High-Fidelity Differential-information Driven Binary Vision Transformer", "categories": ["cs.CV"], "comment": null, "summary": "The binarization of vision transformers (ViTs) offers a promising approach to\naddressing the trade-off between high computational/storage demands and the\nconstraints of edge-device deployment. However, existing binary ViT methods\noften suffer from severe performance degradation or rely heavily on\nfull-precision modules. To address these issues, we propose DIDB-ViT, a novel\nbinary ViT that is highly informative while maintaining the original ViT\narchitecture and computational efficiency. Specifically, we design an\ninformative attention module incorporating differential information to mitigate\ninformation loss caused by binarization and enhance high-frequency retention.\nTo preserve the fidelity of the similarity calculations between binary Q and K\ntensors, we apply frequency decomposition using the discrete Haar wavelet and\nintegrate similarities across different frequencies. Additionally, we introduce\nan improved RPReLU activation function to restructure the activation\ndistribution, expanding the model's representational capacity. Experimental\nresults demonstrate that our DIDB-ViT significantly outperforms\nstate-of-the-art network quantization methods in multiple ViT architectures,\nachieving superior image classification and segmentation performance.", "AI": {"tldr": "The authors introduce DIDB-ViT, a binary vision transformer architecture that minimizes performance loss and computational demands for edge devices while improving image classification and segmentation.", "motivation": "To solve the challenges of performance degradation and reliance on full-precision modules in binary vision transformers while advancing edge-device deployment.", "method": "The approach involves an innovative attention module with differential information, frequency decomposition via discrete Haar wavelets, and an improved activation function (RPReLU).", "result": "DIDB-ViT outperforms existing network quantization methods, achieving superior results in image classification and segmentation across diverse ViT architectures.", "conclusion": "The proposed DIDB-ViT successfully balances performance, computational efficiency, and fidelity, making it a strong candidate for edge-device applications."}}
{"id": "2507.02006", "pdf": "https://arxiv.org/pdf/2507.02006", "abs": "https://arxiv.org/abs/2507.02006", "authors": ["Shakya Jayakody", "Youpeng Zhao", "Jun Wang"], "title": "AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design", "categories": ["cs.LG"], "comment": "36th IEEE International Conference on Application-Specific Systems,\n  Architectures and Processors. (Accepted)", "summary": "Graph convolutional networks (GCNs) are fundamental in various scientific\napplications, ranging from biomedical protein-protein interactions (PPI) to\nlarge-scale recommendation systems. An essential component for modeling graph\nstructures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As\nthe size of graph data continues to scale up, SpGEMMs are often conducted in an\nout-of-core fashion due to limited GPU memory space in resource-constrained\nsystems. Albeit recent efforts that aim to alleviate the memory constraints of\nout-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory\nlayout, or performing the computation in sparse format, current systems suffer\nfrom both high I/O latency and GPU under-utilization issues.\n  In this paper, we first identify the problems of existing systems, where\nsparse format data alignment and memory allocation are the main performance\nbottlenecks, and propose AIRES, a novel algorithm-system co-design solution to\naccelerate out-of-core SpGEMM computation for GCNs. Specifically, from the\nalgorithm angle, AIRES proposes to alleviate the data alignment issues on the\nblock level for matrices in sparse formats and develops a tiling algorithm to\nfacilitate row block-wise alignment. On the system level, AIRES employs a\nthree-phase dynamic scheduling that features a dual-way data transfer strategy\nutilizing a tiered memory system: integrating GPU memory, GPU Direct Storage\n(GDS), and host memory to reduce I/O latency and improve throughput.\nEvaluations show that AIRES significantly outperforms the state-of-the-art\nmethods, achieving up to 1.8x lower latency in real-world graph processing\nbenchmarks.", "AI": {"tldr": "This paper presents AIRES, an algorithm-system co-designed solution for faster out-of-core sparse matrix-matrix multiplication (SpGEMM) in graph convolutional networks (GCNs), addressing issues in data alignment and memory bottlenecks.", "motivation": "Graph convolutional networks demand efficient handling of sparse matrix operations due to increasing graph data size, but traditional methods suffer from memory limitation and GPU under-utilization.", "method": "AIRES introduces row block-wise alignment for sparse format optimization, along with a three-phase dynamic scheduling system that combines GPU memory, GPU Direct Storage, and host memory for improved throughput.", "result": "AIRES achieves up to 1.8x lower latency compared to state-of-the-art methods in graph processing benchmarks.", "conclusion": "AIRES effectively accelerates out-of-core SpGEMM for GCNs by addressing critical bottlenecks like data alignment issues and memory allocation challenges."}}
{"id": "2507.02319", "pdf": "https://arxiv.org/pdf/2507.02319", "abs": "https://arxiv.org/abs/2507.02319", "authors": ["Paolo Liberatore"], "title": "Iterated belief revision: from postulates to abilities", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "The belief revision field is opulent in new proposals and indigent in\nanalyses of existing approaches. Much work hinge on postulates, employed as\nsyntactic characterizations: some revision mechanism is equivalent to some\nproperties. Postulates constraint specific revision instances: certain\nrevisions update certain beliefs in a certain way. As an example, if the\nrevision is consistent with the current beliefs, it is incorporated with no\nother change. A postulate like this tells what revisions must do and neglect\nwhat they can do. Can they reach a certain state of beliefs? Can they reach all\npossible states of beliefs? Can they reach all possible states of beliefs from\nno previous belief? Can they reach a dogmatic state of beliefs, where\neverything not believed is impossible? Can they make two conditions equally\nbelieved? An application where every possible state of beliefs is sensible\nrequires each state of beliefs to be reachable. An application where conditions\nmay be equally believed requires such a belief state to be reachable. An\napplication where beliefs may become dogmatic requires a way to make them\ndogmatic. Such doxastic states need to be reached in a way or another. Not in\nspecific way, as dictated by a typical belief revision postulate. This is an\nability, not a constraint: the ability of being plastic, equating, dogmatic.\nAmnesic, correcting, believer, damascan, learnable are other abilities. Each\nrevision mechanism owns some of these abilities and lacks the others:\nlexicographic, natural, restrained, very radical, full meet, radical, severe,\nmoderate severe, deep severe, plain severe and deep severe revisions, each of\nthese revisions is proved to possess certain abilities.", "AI": {"tldr": "The paper focuses on the limitations of current belief revision postulates and emphasizes the need for describing the abilities of revision mechanisms rather than their constraints.", "motivation": "To analyze how belief revision mechanisms can achieve various states of belief, and to highlight the shortcomings of current postulate-based approaches that focus more on constraints than abilities.", "method": "The paper examines different belief revision mechanisms\u2014including lexicographic and radical\u2014contrasting them based on their abilities (plasticity, dogmatism, etc.) rather than constraints dictated by postulates.", "result": "It is demonstrated that existing belief revision mechanisms possess distinct abilities and lack others, offering insights into their suitability for various applications.", "conclusion": "Belief revision mechanisms should be assessed based on their abilities to reach diverse doxastic states rather than constrained by specific postulates. This approach can better support applications requiring flexible belief states."}}
{"id": "2507.02364", "pdf": "https://arxiv.org/pdf/2507.02364", "abs": "https://arxiv.org/abs/2507.02364", "authors": ["Pilsung Kang"], "title": "QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers", "categories": ["cs.CL", "quant-ph"], "comment": null, "summary": "Parameterized quantum circuits (PQCs) have recently emerged as promising\ncomponents for enhancing the expressibility of neural architectures. In this\nwork, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the\nfeedforward network (FFN) modules of a compact BERT variant are replaced by\nPQC-based layers. This design is motivated by the dominant parameter\ncontribution of FFNs, which account for approximately two-thirds of the\nparameters within standard Transformer encoder blocks. While prior studies have\nprimarily integrated PQCs into self-attention modules, our work focuses on the\nFFN and systematically investigates the trade-offs between PQC depth,\nexpressibility, and trainability. Our final PQC architecture incorporates a\nresidual connection, both $R_Y$ and $R_Z$ rotations, and an alternating\nentanglement strategy to ensure stable training and high expressibility. Our\nexperiments, conducted on a classical simulator, on the SST-2 and DBpedia\nbenchmarks demonstrate two key findings. First, a carefully configured\nQFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its\nclassical counterpart in a full-data setting while reducing FFN-specific\nparameters by over 99%. Second, our model exhibits a consistent and competitive\nedge in few-shot learning scenarios, confirming its potential for superior data\nefficiency. These results, supported by an ablation study on a non-optimized\nPQC that failed to learn, confirm that PQCs can serve as powerful and\nparameter-efficient alternatives to classical FFNs when co-designed with\nfoundational deep learning principles.", "AI": {"tldr": "This paper introduces QFFN-BERT, a hybrid quantum-classical transformer where the feedforward network (FFN) modules in BERT are replaced with parameterized quantum circuits (PQCs).", "motivation": "The motivation is to reduce the parameter count in Transformer encoder blocks, as FFNs contribute to two-thirds of the parameters, while potentially achieving superior performance by enhancing the expressibility of neural architectures.", "method": "The authors replace the FFN modules with PQC-based layers, including a residual connection, $R_Y$ and $R_Z$ rotations, and an alternating entanglement strategy for stable training and expressibility. They test this on a classical simulator using SST-2 and DBpedia benchmarks.", "result": "The proposed QFFN-BERT achieves up to 102.0% of baseline accuracy while reducing FFN parameters by over 99%. It also demonstrates competitive performance in few-shot learning and confirms PQC\u2019s potential for data-efficient learning.", "conclusion": "QFFN-BERT highlights that PQCs can act as parameter-efficient and effective alternatives to classical FFNs when co-designed with foundational deep learning principles."}}
{"id": "2507.02443", "pdf": "https://arxiv.org/pdf/2507.02443", "abs": "https://arxiv.org/abs/2507.02443", "authors": ["Sandro Costa Magalh\u00e3es", "Marco Almeida", "Filipe Neves dos Santos", "Ant\u00f3nio Paulo Moreira", "Jorge Dias"], "title": "Red grape detection with accelerated artificial neural networks in the FPGA's programmable logic", "categories": ["cs.CV", "cs.AI", "cs.DC", "cs.LG", "cs.RO"], "comment": "Submitted to ROBOT'2025", "summary": "Robots usually slow down for canning to detect objects while moving.\nAdditionally, the robot's camera is configured with a low framerate to track\nthe velocity of the detection algorithms. This would be constrained while\nexecuting tasks and exploring, making robots increase the task execution time.\nAMD has developed the Vitis-AI framework to deploy detection algorithms into\nFPGAs. However, this tool does not fully use the FPGAs' PL. In this work, we\nuse the FINN architecture to deploy three ANNs, MobileNet v1 with 4-bit\nquantisation, CNV with 2-bit quantisation, and CNV with 1-bit quantisation\n(BNN), inside an FPGA's PL. The models were trained on the RG2C dataset. This\nis a self-acquired dataset released in open access. MobileNet v1 performed\nbetter, reaching a success rate of 98 % and an inference speed of 6611 FPS. In\nthis work, we proved that we can use FPGAs to speed up ANNs and make them\nsuitable for attention mechanisms.", "AI": {"tldr": "The paper focuses on using FPGAs to speed up neural network-based object detection in robots, achieving high inference speeds and accuracy.", "motivation": "Robots typically slow down for object detection due to limitations in camera framerate and detection algorithm speed, which impacts task efficiency. The paper aims to address this constraint.", "method": "The authors deployed three quantized ANNs (MobileNet v1, CNV with 2-bit, and CNV with 1-bit) inside an FPGA's programmable logic using the FINN architecture, and trained them using the RG2C dataset.", "result": "MobileNet v1 achieved the best performance with a 98% success rate and 6611 FPS inference speed. The study demonstrated significant acceleration of ANNs using FPGAs.", "conclusion": "This research confirms the effectiveness of FPGAs in enhancing inference speed and accuracy of neural networks, making them suitable for robotic attention mechanisms."}}
{"id": "2507.02328", "pdf": "https://arxiv.org/pdf/2507.02328", "abs": "https://arxiv.org/abs/2507.02328", "authors": ["Gabriel O. Flores-Aquino", "Octavio Gutierrez-Frias", "Juan Irving Vasquez"], "title": "Path Planning using a One-shot-sampling Skeleton Map", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Path planning algorithms aim to compute a collision-free path, and many works\nfocus on finding the optimal distance path. However, for some applications, a\nmore suitable approach is to balance response time, safety of the paths, and\npath length. In this context, a skeleton map is a useful tool in graph-based\nschemes, as it provides an intrinsic representation of free configuration\nspace. However, skeletonization algorithms are very resource-intensive, being\nprimarily oriented towards image processing tasks. We propose an efficient\npath-planning methodology that finds safe paths within an acceptable processing\ntime. This methodology leverages a Deep Denoising Auto-Encoder (DDAE) based on\nU-Net architecture to compute a skeletonized version of the navigation map,\nwhich we refer to as SkelUnet. The SkelUnet network facilitates exploration of\nthe entire workspace through one-shot sampling (OSS), as opposed to the\niterative process used by exact algorithms or the probabilistic sampling\nprocess. SkelUnet is trained and tested on a dataset consisting of 12,500\nbi-dimensional dungeon maps. The motion planning methodology is evaluated in a\nsimulation environment for an Unmanned Aerial Vehicle (UAV) using 250\npreviously unseen maps, and assessed with various navigation metrics to\nquantify the navigability of the computed paths. The results demonstrate that\nusing SkelUnet to construct a roadmap offers significant advantages, such as\nconnecting all regions of free workspace, providing safer paths, and reducing\nprocessing times. These characteristics make this method particularly suitable\nfor mobile service robots in structured environments.", "AI": {"tldr": "The paper presents a path-planning method using a U-Net-based autoencoder called SkelUnet to efficiently compute safe and navigable routes, especially for UAVs, achieving improved safety and reduced processing times.", "motivation": "Standard path-planning algorithms focus on finding optimal distance paths, but they may not balance response time, safety, and length. There is a need for methods that address these trade-offs, especially in structured robotics environments.", "method": "A Deep Denoising Auto-Encoder (DDAE) based on U-Net architecture, named SkelUnet, is used for skeletonizing navigation maps. This neural network enables one-shot sampling for path planning. The method is evaluated using 12,500 maps for training and 250 unseen maps for testing, specifically for UAV simulation environments.", "result": "The proposed SkelUnet approach demonstrates significant improvements in creating roadmaps that connect all free workspace regions, enhance path safety, and reduce resource-intensive processing times.", "conclusion": "The SkelUnet-based methodology is effective for path planning in structured environments, offering key advantages in processing speed and safety, and is well-suited for mobile service robots."}}
{"id": "2507.02032", "pdf": "https://arxiv.org/pdf/2507.02032", "abs": "https://arxiv.org/abs/2507.02032", "authors": ["Aishik Ghosh", "Maximilian Griese", "Ulrich Haisch", "Tae Hyoun Park"], "title": "Neural simulation-based inference of the Higgs trilinear self-coupling via off-shell Higgs production", "categories": ["hep-ph", "hep-ex", "physics.data-an", "stat.ML"], "comment": "24 pages, 14 figures, 2 tables", "summary": "One of the forthcoming major challenges in particle physics is the\nexperimental determination of the Higgs trilinear self-coupling. While efforts\nhave largely focused on on-shell double- and single-Higgs production in\nproton-proton collisions, off-shell Higgs production has also been proposed as\na valuable complementary probe. In this article, we design a hybrid neural\nsimulation-based inference (NSBI) approach to construct a likelihood of the\nHiggs signal incorporating modifications from the Standard Model effective\nfield theory (SMEFT), relevant background processes, and quantum interference\neffects. It leverages the training efficiency of matrix-element-enhanced\ntechniques, which are vital for robust SMEFT applications, while also\nincorporating the practical advantages of classification-based methods for\neffective background estimates. We demonstrate that our NSBI approach achieves\nsensitivity close to the theoretical optimum and provide expected constraints\nfor the high-luminosity upgrade of the Large Hadron Collider. While we\nprimarily concentrate on the Higgs trilinear self-coupling, we also consider\nconstraints on other SMEFT operators that affect off-shell Higgs production.", "AI": {"tldr": "The study develops a hybrid neural approach combining matrix-element-enhanced and classification-based techniques for analyzing off-shell Higgs production. It aims to provide sensitive constraints to support precision Higgs measurements.", "motivation": "To address the challenge of experimentally determining the Higgs trilinear self-coupling and to investigate off-shell Higgs production as a complementary probe to on-shell measurements.", "method": "A hybrid neural simulation-based inference (NSBI) method was designed, combining matrix-element-enhanced techniques for robustness with classification-based methods for efficient background estimation.", "result": "The proposed NSBI approach achieved sensitivity close to theoretical limits and provided constraints for the high-luminosity upgrade of the Large Hadron Collider.", "conclusion": "The study highlights the effectiveness of the NSBI approach for analyzing Higgs behaviors, with implications for precise measurements of SMEFT operators and Higgs trilinear self-coupling."}}
{"id": "2507.02250", "pdf": "https://arxiv.org/pdf/2507.02250", "abs": "https://arxiv.org/abs/2507.02250", "authors": ["Jiangxia Chen", "Tongyuan Huang", "Ke Song"], "title": "FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model", "categories": ["cs.CV"], "comment": null, "summary": "3D semantic occupancy prediction plays a pivotal role in autonomous driving.\nHowever, inherent limitations of fewframe images and redundancy in 3D space\ncompromise prediction accuracy for occluded and distant scenes. Existing\nmethods enhance performance by fusing historical frame data, which need\nadditional data and significant computational resources. To address these\nissues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement\noccupancy network with flow matching selective state space model for few-frame\n3D occupancy prediction. Firstly, to generate missing features, we designed a\nfeature refinement module based on a flow matching model, which is called Flow\nMatching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and\nPlane Selective SSM (PS3M), we selectively filter TPV features to reduce the\nimpact of air voxels on non-air voxels, thereby enhancing the overall\nefficiency of the model and prediction capability for distant scenes. Finally,\nwe design the Mask Training (MT) method to enhance the robustness of FMOcc and\naddress the issue of sensor data loss. Experimental results on the\nOcc3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing\nstate-of-theart methods. Our FMOcc with two frame input achieves notable scores\nof 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on\nOpenOcc with 5.4 G inference memory and 330ms inference time.", "AI": {"tldr": "This paper introduces FMOcc, a novel method for 3D semantic occupancy prediction in few-frame scenarios, aiming to improve prediction accuracy and efficiency.", "motivation": "Enhance the accuracy and efficiency of 3D semantic occupancy predictions in autonomous driving, especially for occluded and distant scenes, while addressing inherent limitations in few-frame image data and 3D space redundancies.", "method": "The paper proposes FMOcc, which uses a flow-matching selective state-space model (FMSSM) for feature refinement, along with TPV SSM and Plane Selective SSM (PS3M) layers to selectively filter occupancy data and improve model efficiency. Additionally, a Mask Training (MT) method is introduced to enhance robustness against sensor data loss.", "result": "FMOcc outperforms current state-of-the-art methods with notable improvements in metrics such as 43.1% RayIoU and 39.8% mIoU on the Occ3D-nuScenes dataset. It also showcases efficient memory usage and reasonable inference times.", "conclusion": "The proposed FMOcc significantly enhances prediction capabilities for 3D semantic occupancy while ensuring efficiency and robustness, making it highly suitable for autonomous driving applications."}}
{"id": "2507.02085", "pdf": "https://arxiv.org/pdf/2507.02085", "abs": "https://arxiv.org/abs/2507.02085", "authors": ["Wanjia Zhao", "Jiaqi Han", "Siyi Gu", "Mingjian Jiang", "James Zou", "Stefano Ermon"], "title": "GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Geometric diffusion models have shown remarkable success in molecular\ndynamics and structure generation. However, efficiently fine-tuning them for\ndownstream tasks with varying geometric controls remains underexplored. In this\nwork, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables\nflexible and parameter-efficient fine-tuning for controlled generative tasks\nwithout modifying the original model architecture. GeoAda introduces a\nstructured adapter design: control signals are first encoded through coupling\noperators, then processed by a trainable copy of selected pretrained model\nlayers, and finally projected back via decoupling operators followed by an\nequivariant zero-initialized convolution. By fine-tuning only these lightweight\nadapter modules, GeoAda preserves the model's geometric consistency while\nmitigating overfitting and catastrophic forgetting. We theoretically prove that\nthe proposed adapters maintain SE(3)-equivariance, ensuring that the geometric\ninductive biases of the pretrained diffusion model remain intact during\nadaptation. We demonstrate the wide applicability of GeoAda across diverse\ngeometric control types, including frame control, global control, subgraph\ncontrol, and a broad range of application domains such as particle dynamics,\nmolecular dynamics, human motion prediction, and molecule generation. Empirical\nresults show that GeoAda achieves state-of-the-art fine-tuning performance\nwhile preserving original task accuracy, whereas other baselines experience\nsignificant performance degradation due to overfitting and catastrophic\nforgetting.", "AI": {"tldr": "This paper introduces GeoAda, an SE(3)-equivariant adapter framework that facilitates efficient fine-tuning of geometric diffusion models for controlled generative tasks, ensuring performance consistency and minimal overfitting.", "motivation": "The motivation is to address the lack of efficient fine-tuning frameworks for geometric diffusion models to tackle downstream tasks with diverse geometric controls, while preserving model consistency and mitigating overfitting.", "method": "GeoAda is developed as an SE(3)-equivariant adapter framework that uses coupling operators, trainable copies of existing layers, and decoupling operators to encode and project control signals while ensuring geometric consistency. Fine-tuning is achieved by updating only these adapter components.", "result": "GeoAda demonstrates state-of-the-art fine-tuning performance across diverse geometric controls and application domains such as molecular dynamics and human motion prediction. It also maintains the pretrained model's accuracy, avoiding overfitting and catastrophic forgetting seen in baseline approaches.", "conclusion": "GeoAda presents a structured, parameter-efficient method for fine-tuning geometric diffusion models, proving its effectiveness across various tasks and controls by leveraging SE(3)-equivariant properties."}}
{"id": "2507.02353", "pdf": "https://arxiv.org/pdf/2507.02353", "abs": "https://arxiv.org/abs/2507.02353", "authors": ["Bowen Chen", "Zhao Wang", "Shingo Takamatsu"], "title": "OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent", "categories": ["cs.AI"], "comment": null, "summary": "Keyword decision in Sponsored Search Advertising is critical to the success\nof ad campaigns. While LLM-based methods offer automated keyword generation,\nthey face three major limitations: reliance on large-scale query-keyword pair\ndata, lack of online multi-objective performance monitoring and optimization,\nand weak quality control in keyword selection. These issues hinder the agentic\nuse of LLMs in fully automating keyword decisions by monitoring and reasoning\nover key performance indicators such as impressions, clicks, conversions, and\nCTA effectiveness. To overcome these challenges, we propose OMS, a keyword\ngeneration framework that is On-the-fly (requires no training data, monitors\nonline performance, and adapts accordingly), Multi-objective (employs agentic\nreasoning to optimize keywords based on multiple performance metrics), and\nSelf-reflective (agentically evaluates keyword quality). Experiments on\nbenchmarks and real-world ad campaigns show that OMS outperforms existing\nmethods; ablation and human evaluations confirm the effectiveness of each\ncomponent and the quality of generated keywords.", "AI": {"tldr": "OMS is a framework designed to address limitations in keyword generation for Sponsored Search Advertising by operating on-the-fly, optimizing across multiple objectives, and self-evaluating keyword quality.", "motivation": "Keyword generation in Sponsored Search Advertising faces challenges related to reliance on training data, lack of performance monitoring, and insufficient quality control.", "method": "The OMS framework operates without extensive training data, monitors online performance using agentic reasoning, optimizes for multiple metrics, and evaluates keyword quality through self-reflection.", "result": "OMS demonstrated superior performance compared to previous methods in experiments with benchmarks and real ad campaigns; ablation and human evaluations supported its component-wise and overall effectiveness.", "conclusion": "OMS successfully addresses major limitations in LLM-based keyword generation for advertising, demonstrating adaptability, multi-objective optimization, and improved keyword quality."}}
{"id": "2507.02378", "pdf": "https://arxiv.org/pdf/2507.02378", "abs": "https://arxiv.org/abs/2507.02378", "authors": ["Weijie Lyu", "Sheng-Jun Huang", "Xuan Xia"], "title": "Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection", "categories": ["cs.CL"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved code generation and program comprehension, accelerating the evolution\nof software engineering. Current methods primarily enhance model performance by\nleveraging vast amounts of data, focusing on data quantity while often\noverlooking data quality, thereby reducing training efficiency. To address\nthis, we introduce an approach that utilizes a parametric model for code data\nselection, aimed at improving both training efficiency and model performance.\nOur method optimizes the parametric model to ensure distribution consistency\nand diversity within the selected subset, guaranteeing high-quality data.\nExperimental results demonstrate that using only 10K samples, our method\nachieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled\nbaseline, outperforming other sampling approaches in both performance and\nefficiency. This underscores that our method effectively boosts model\nperformance while significantly reducing computational costs.", "AI": {"tldr": "The paper introduces a method to improve training efficiency and model performance in code generation by selecting high-quality data rather than relying on large datasets.", "motivation": "Current code generation methods focus on data quantity over quality, which reduces training efficiency and limits model performance improvements.", "method": "The authors propose a parametric model-based data selection approach that ensures distribution consistency and diversity within smaller, high-quality training subsets.", "result": "Their method, using only 10K samples, achieves a 2.4% improvement in HumanEval and a 2.3% improvement in MBPP compared to a 92K full-sampled baseline, while outperforming other sampling strategies.", "conclusion": "The approach enhances model performance and training efficiency, proving that prioritizing data quality over quantity effectively reduces computational costs."}}
{"id": "2507.02464", "pdf": "https://arxiv.org/pdf/2507.02464", "abs": "https://arxiv.org/abs/2507.02464", "authors": ["Craig S Wright"], "title": "Resolving CAP Through Automata-Theoretic Economic Design: A Unified Mathematical Framework for Real-Time Partition-Tolerant Systems", "categories": ["cs.GT", "cs.DC", "cs.FL", "cs.IR", "econ.GN", "q-fin.EC", "68M14, 91A05, 68Q85", "C.2.4; D.2.4; F.1.1"], "comment": "51 pages 4 tables, includes formal proofs, automata construction, and\n  case study on Bitcoin Script", "summary": "The CAP theorem asserts a trilemma between consistency, availability, and\npartition tolerance. This paper introduces a rigorous automata-theoretic and\neconomically grounded framework that reframes the CAP trade-off as a constraint\noptimization problem. We model distributed systems as partition-aware state\nmachines and embed economic incentive layers to stabilize consensus behavior\nacross adversarially partitioned networks. By incorporating game-theoretic\nmechanisms into the global transition semantics, we define provable bounds on\nconvergence, liveness, and correctness. Our results demonstrate that\navailability and consistency can be simultaneously preserved within bounded\nepsilon margins, effectively extending the classical CAP limits through formal\neconomic control.", "AI": {"tldr": "The paper reframes the CAP theorem's trade-off among consistency, availability, and partition tolerance as a constraint optimization problem using automata-theoretic and economic frameworks, showing that availability and consistency can coexist within bounded margins.", "motivation": "To address the classical CAP theorem's limiting trade-offs by leveraging automata and economic theories for stability in distributed systems.", "method": "Distributed systems are modeled as partition-aware state machines with economic incentive layers and game-theoretic mechanisms to analyze behavior under adversarial partitions.", "result": "The paper provides provable bounds for convergence, liveness, and correctness, showing that availability and consistency can co-exist within controlled margins.", "conclusion": "The CAP theorem's limits can be extended through formal economic control, offering a new perspective for optimizing distributed systems."}}
{"id": "2507.02533", "pdf": "https://arxiv.org/pdf/2507.02533", "abs": "https://arxiv.org/abs/2507.02533", "authors": ["Miguel Romero-Arjona", "Jos\u00e9 A. Parejo", "Juan C. Alonso", "Ana B. S\u00e1nchez", "Aitor Arrieta", "Sergio Segura"], "title": "Meta-Fair: AI-Assisted Fairness Testing of Large Language Models", "categories": ["cs.SE"], "comment": null, "summary": "Fairness--the absence of unjustified bias--is a core principle in the\ndevelopment of Artificial Intelligence (AI) systems, yet it remains difficult\nto assess and enforce. Current approaches to fairness testing in large language\nmodels (LLMs) often rely on manual evaluation, fixed templates, deterministic\nheuristics, and curated datasets, making them resource-intensive and difficult\nto scale. This work aims to lay the groundwork for a novel, automated method\nfor testing fairness in LLMs, reducing the dependence on domain-specific\nresources and broadening the applicability of current approaches. Our approach,\nMeta-Fair, is based on two key ideas. First, we adopt metamorphic testing to\nuncover bias by examining how model outputs vary in response to controlled\nmodifications of input prompts, defined by metamorphic relations (MRs). Second,\nwe propose exploiting the potential of LLMs for both test case generation and\noutput evaluation, leveraging their capability to generate diverse inputs and\nclassify outputs effectively. The proposal is complemented by three open-source\ntools supporting LLM-driven generation, execution, and evaluation of test\ncases. We report the findings of several experiments involving 12 pre-trained\nLLMs, 14 MRs, 5 bias dimensions, and 7.9K automatically generated test cases.\nThe results show that Meta-Fair is effective in uncovering bias in LLMs,\nachieving an average precision of 92% and revealing biased behaviour in 29% of\nexecutions. Additionally, LLMs prove to be reliable and consistent evaluators,\nwith the best-performing models achieving F1-scores of up to 0.79. Although\nnon-determinism affects consistency, these effects can be mitigated through\ncareful MR design. While challenges remain to ensure broader applicability, the\nresults indicate a promising path towards an unprecedented level of automation\nin LLM testing.", "AI": {"tldr": "This paper introduces Meta-Fair, an automated method for fairness testing in large language models (LLMs), addressing bias detection through metamorphic testing and LLMs' generative and evaluative capabilities.", "motivation": "Fairness in AI remains challenging to evaluate and enforce, with current methods being resource-intensive and limited by manual evaluation or curated resources. The paper seeks to automate and broaden the applicability of fairness testing in LLMs.", "method": "The authors propose Meta-Fair, which uses metamorphic testing to identify bias by analyzing response changes to modified input prompts. LLMs help generate test cases and classify outputs, supported by open-source tools.", "result": "Through experiments with 12 LLMs, 14 metamorphic relations, and 7.9K test cases, Meta-Fair achieved 92% average precision and detected biases in 29% of cases. Some LLMs showed reliable evaluation with up to 0.79 F1-scores.", "conclusion": "Meta-Fair shows promise in automating LLM fairness testing, achieving high bias detection precision and reliability despite challenges like non-determinism, which can be minimized with careful design."}}
{"id": "2507.02400", "pdf": "https://arxiv.org/pdf/2507.02400", "abs": "https://arxiv.org/abs/2507.02400", "authors": ["Maximilian Zipfl", "Pascal Zwick", "Patrick Schulz", "Marc Rene Zofka", "Albert Schotschneider", "Helen Gremmelmaier", "Nikolai Polley", "Ferdinand M\u00fctsch", "Kevin Simon", "Fabian Gottselig", "Michael Frey", "Sergio Marschall", "Akim Stark", "Maximilian M\u00fcller", "Marek Wehmer", "Mihai Kocsis", "Dominic Waldenmayer", "Florian Schnepf", "Erik Heinrich", "Sabrina Pletz", "Matthias K\u00f6lle", "Karin Langbein-Euchner", "Alexander Viehl", "Raoul Z\u00f6llner", "J. Marius Z\u00f6llner"], "title": "DigiT4TAF -- Bridging Physical and Digital Worlds for Future Transportation Systems", "categories": ["cs.RO", "cs.HC"], "comment": "Accepted at the IEEE IAVVC 2025 Conference", "summary": "In the future, mobility will be strongly shaped by the increasing use of\ndigitalization. Not only will individual road users be highly interconnected,\nbut also the road and associated infrastructure. At that point, a Digital Twin\nbecomes particularly appealing because, unlike a basic simulation, it offers a\ncontinuous, bilateral connection linking the real and virtual environments.\nThis paper describes the digital reconstruction used to develop the Digital\nTwin of the Test Area Autonomous Driving-Baden-W\\\"urttemberg (TAF-BW), Germany.\nThe TAF-BW offers a variety of different road sections, from high-traffic urban\nintersections and tunnels to multilane motorways. The test area is equipped\nwith a comprehensive Vehicle-to-Everything (V2X) communication infrastructure\nand multiple intelligent intersections equipped with camera sensors to\nfacilitate real-time traffic flow monitoring. The generation of authentic data\nas input for the Digital Twin was achieved by extracting object lists at the\nintersections. This process was facilitated by the combined utilization of\ncamera images from the intelligent infrastructure and LiDAR sensors mounted on\na test vehicle. Using a unified interface, recordings from real-world\ndetections of traffic participants can be resimulated. Additionally, the\nsimulation framework's design and the reconstruction process is discussed. The\nresulting framework is made publicly available for download and utilization at:\nhttps://digit4taf-bw.fzi.de The demonstration uses two case studies to\nillustrate the application of the digital twin and its interfaces: the analysis\nof traffic signal systems to optimize traffic flow and the simulation of\nsecurity-related scenarios in the communications sector.", "AI": {"tldr": "This paper discusses the development and application of a Digital Twin for the Test Area Autonomous Driving-Baden-W\u00fcrttemberg (TAF-BW), focusing on infrastructure, data collection, and simulation for traffic optimization and security scenarios.", "motivation": "To leverage digitalization and interconnected systems for enhanced mobility and create a continuous integration of real and virtual environments through the Digital Twin concept.", "method": "The method involves digital reconstruction of TAF-BW using V2X communication, camera sensors, and LiDAR-equipped test vehicles to extract data for traffic simulation. A unified interface allows real-world detections to be resimulated.", "result": "The framework created enables authentic, resimulated digital Twin data and is publicly available for download. Case studies demonstrate optimization of traffic signals and simulation of security-related communication scenarios.", "conclusion": "The paper successfully shows the applicability of Digital Twin technology for analyzing and optimizing traffic systems and security scenarios, while providing a robust and accessible framework for future research."}}
{"id": "2507.02089", "pdf": "https://arxiv.org/pdf/2507.02089", "abs": "https://arxiv.org/abs/2507.02089", "authors": ["Xingtu Liu", "Lin F. Yang", "Sharan Vaswani"], "title": "Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "We consider infinite-horizon $\\gamma$-discounted (linear) constrained Markov\ndecision processes (CMDPs) where the objective is to find a policy that\nmaximizes the expected cumulative reward subject to expected cumulative\nconstraints. Given access to a generative model, we propose to solve CMDPs with\na primal-dual framework that can leverage any black-box unconstrained MDP\nsolver. For linear CMDPs with feature dimension $d$, we instantiate the\nframework by using mirror descent value iteration\n(\\texttt{MDVI})~\\citep{kitamura2023regularization} an example MDP solver. We\nprovide sample complexity bounds for the resulting CMDP algorithm in two cases:\n(i) relaxed feasibility, where small constraint violations are allowed, and\n(ii) strict feasibility, where the output policy is required to exactly satisfy\nthe constraint. For (i), we prove that the algorithm can return an\n$\\epsilon$-optimal policy with high probability by using\n$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^4\\epsilon^2}\\right)$ samples. We note\nthat these results exhibit a near-optimal dependence on both $d$ and\n$\\epsilon$. For (ii), we show that the algorithm requires\n$\\tilde{O}\\left(\\frac{d^2}{(1-\\gamma)^6\\epsilon^2\\zeta^2}\\right)$ samples,\nwhere $\\zeta$ is the problem-dependent Slater constant that characterizes the\nsize of the feasible region. Finally, we instantiate our framework for tabular\nCMDPs and show that it can be used to recover near-optimal sample complexities\nin this setting.", "AI": {"tldr": "This paper develops a primal-dual framework for solving constrained Markov Decision Processes (CMDPs) using black-box MDP solvers, providing sample complexity results for relaxed and strict feasibility cases.", "motivation": "The motivation of the paper is to address the challenge of efficiently solving CMDPs, where constraints on cumulative rewards must be satisfied, by leveraging generative models and unconstrained MDP solvers.", "method": "The proposed method uses a primal-dual framework integrated with mirror descent value iteration (MDVI) to solve CMDPs, providing sample complexity guarantees for relaxed and strict feasibility cases.", "result": "The algorithm achieves sample complexity of $\\tilde{O}(\\frac{d^2}{(1-\\gamma)^4\\epsilon^2})$ for relaxed feasibility and $\\tilde{O}(\\frac{d^2}{(1-\\gamma)^6\\epsilon^2\\zeta^2})$ for strict feasibility, with near-optimal dependence on key problem parameters.", "conclusion": "The paper concludes that the proposed framework can effectively solve CMDPs with rigorous sample complexity bounds and recover near-optimal results in tabular settings."}}
{"id": "2507.02252", "pdf": "https://arxiv.org/pdf/2507.02252", "abs": "https://arxiv.org/abs/2507.02252", "authors": ["Zeyu Lei", "Hongyuan Yu", "Jinlin Wu", "Zhen Chen"], "title": "SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Precise surgical interventions are vital to patient safety, and advanced\nenhancement algorithms have been developed to assist surgeons in\ndecision-making. Despite significant progress, these algorithms are typically\ndesigned for single tasks in specific scenarios, limiting their effectiveness\nin complex real-world situations. To address this limitation, we propose\nSurgVisAgent, an end-to-end intelligent surgical vision agent built on\nmultimodal large language models (MLLMs). SurgVisAgent dynamically identifies\ndistortion categories and severity levels in endoscopic images, enabling it to\nperform a variety of enhancement tasks such as low-light enhancement,\noverexposure correction, motion blur elimination, and smoke removal.\nSpecifically, to achieve superior surgical scenario understanding, we design a\nprior model that provides domain-specific knowledge. Additionally, through\nin-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent\ndelivers customized image enhancements tailored to a wide range of distortion\ntypes and severity levels, thereby addressing the diverse requirements of\nsurgeons. Furthermore, we construct a comprehensive benchmark simulating\nreal-world surgical distortions, on which extensive experiments demonstrate\nthat SurgVisAgent surpasses traditional single-task models, highlighting its\npotential as a unified solution for surgical assistance.", "AI": {"tldr": "This paper introduces SurgVisAgent, a unified surgical vision agent based on multimodal large language models, capable of dynamically addressing varied image distortions in surgical scenarios.", "motivation": "Existing enhancement algorithms are designed for single specific tasks, limiting their application in complex real-world surgeries.", "method": "The authors proposed SurgVisAgent, which incorporates domain-specific knowledge, in-context few-shot learning, and chain-of-thought reasoning to enhance endoscopic images under diverse conditions.", "result": "Extensive experiments on a benchmark simulating real-world surgical distortions demonstrate that SurgVisAgent outperforms traditional single-task models.", "conclusion": "SurgVisAgent represents a promising unified approach to surgical assistance, offering enhanced flexibility and efficiency in real-world scenarios by addressing a wide range of image distortions dynamically."}}
{"id": "2507.02087", "pdf": "https://arxiv.org/pdf/2507.02087", "abs": "https://arxiv.org/abs/2507.02087", "authors": ["Eitan Anzenberg", "Arunava Samajpati", "Sivasankaran Chandrasekar", "Varun Kacholia"], "title": "Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions", "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": "10 pages, 2 figures, 2 tables. Submitted to NeurIPS 2025", "summary": "The use of large language models (LLMs) in hiring promises to streamline\ncandidate screening, but it also raises serious concerns regarding accuracy and\nalgorithmic bias where sufficient safeguards are not in place. In this work, we\nbenchmark several state-of-the-art foundational LLMs - including models from\nOpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our\nproprietary domain-specific hiring model (Match Score) for job candidate\nmatching. We evaluate each model's predictive accuracy (ROC AUC,\nPrecision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis\nacross declared gender, race, and intersectional subgroups). Our experiments on\na dataset of roughly 10,000 real-world recent candidate-job pairs show that\nMatch Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs\n0.77) and achieves significantly more equitable outcomes across demographic\ngroups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957\n(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the\nintersectionals, respectively). We discuss why pretraining biases may cause\nLLMs with insufficient safeguards to propagate societal biases in hiring\nscenarios, whereas a bespoke supervised model can more effectively mitigate\nthese biases. Our findings highlight the importance of domain-specific modeling\nand bias auditing when deploying AI in high-stakes domains such as hiring, and\ncaution against relying on off-the-shelf LLMs for such tasks without extensive\nfairness safeguards. Furthermore, we show with empirical evidence that there\nshouldn't be a dichotomy between choosing accuracy and fairness in hiring: a\nwell-designed algorithm can achieve both accuracy in hiring and fairness in\noutcomes.", "AI": {"tldr": "The study benchmarks general-purpose LLMs against a custom hiring model, showing the latter significantly outperforms in both accuracy and fairness for job candidate matching.", "motivation": "The motivation is to address the challenges of accuracy and fairness in using LLMs for hiring, considering the risks of bias in high-stakes AI applications.", "method": "The paper evaluates several LLMs and a proprietary hiring model using metrics like ROC AUC, F1-score, and fairness measures such as impact ratio across demographics.", "result": "The proprietary Match Score model outperformed general-purpose LLMs in accuracy (ROC AUC 0.85 vs. 0.77) and fairness (near-parity impact ratios, better than LLMs).", "conclusion": "Domain-specific AI models tailored for hiring can achieve both high accuracy and fairness, whereas general-purpose LLMs require extensive safeguards to mitigate bias."}}
{"id": "2507.02379", "pdf": "https://arxiv.org/pdf/2507.02379", "abs": "https://arxiv.org/abs/2507.02379", "authors": ["Mingyu Wu", "Zhaoguo Wang", "Jiabin Wang", "Zhiyuan Dong", "Jingkai Yang", "Qingting Li", "Tianyu Huang", "Lei Zhao", "Mingqiang Li", "Fei Wang", "Chunhai Fan", "Haibo Chen"], "title": "An AI-native experimental laboratory for autonomous biomolecular engineering", "categories": ["cs.AI", "q-bio.BM"], "comment": null, "summary": "Autonomous scientific research, capable of independently conducting complex\nexperiments and serving non-specialists, represents a long-held aspiration.\nAchieving it requires a fundamental paradigm shift driven by artificial\nintelligence (AI). While autonomous experimental systems are emerging, they\nremain confined to areas featuring singular objectives and well-defined, simple\nexperimental workflows, such as chemical synthesis and catalysis. We present an\nAI-native autonomous laboratory, targeting highly complex scientific\nexperiments for applications like autonomous biomolecular engineering. This\nsystem autonomously manages instrumentation, formulates experiment-specific\nprocedures and optimization heuristics, and concurrently serves multiple user\nrequests. Founded on a co-design philosophy of models, experiments, and\ninstruments, the platform supports the co-evolution of AI models and the\nautomation system. This establishes an end-to-end, multi-user autonomous\nlaboratory that handles complex, multi-objective experiments across diverse\ninstrumentation. Our autonomous laboratory supports fundamental nucleic acid\nfunctions-including synthesis, transcription, amplification, and sequencing. It\nalso enables applications in fields such as disease diagnostics, drug\ndevelopment, and information storage. Without human intervention, it\nautonomously optimizes experimental performance to match state-of-the-art\nresults achieved by human scientists. In multi-user scenarios, the platform\nsignificantly improves instrument utilization and experimental efficiency. This\nplatform paves the way for advanced biomaterials research to overcome\ndependencies on experts and resource barriers, establishing a blueprint for\nscience-as-a-service at scale.", "AI": {"tldr": "The paper introduces an AI-driven autonomous laboratory system capable of conducting complex biomolecular engineering experiments without human intervention, achieving state-of-the-art performance and improving efficiency.", "motivation": "The need for autonomous scientific research systems that can independently conduct complex experiments and overcome resource barriers while supporting non-expert users.", "method": "The system employs a co-design approach integrating AI models, experimental workflows, and instrumentation, enabling it to formulate experimental procedures, manage multi-user requests, and optimize results autonomously.", "result": "The autonomous laboratory successfully supports fundamental nucleic acid functions, achieves human-comparable experimental performance, and enhances efficiency in multi-user scenarios.", "conclusion": "This platform advances biomaterials research, removes reliance on specialists, improves experimental efficiency, and establishes a scalable framework for science-as-a-service applications."}}
{"id": "2507.02407", "pdf": "https://arxiv.org/pdf/2507.02407", "abs": "https://arxiv.org/abs/2507.02407", "authors": ["Mark Atta Mensah", "Isaac Wiafe", "Akon Ekpezu", "Justice Kwame Appati", "Jamal-Deen Abdulai", "Akosua Nyarkoa Wiafe-Akenten", "Frank Ernest Yeboah", "Gifty Odame"], "title": "Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "This version has been reviewed and accepted for presentation at the\n  Future Technologies Conference (FTC) 2025, to be held on 6 & 7 November 2025\n  in Munich, Germany. 17 pages, 4 figures, 1 table", "summary": "Most existing automatic speech recognition (ASR) research evaluate models\nusing in-domain datasets. However, they seldom evaluate how they generalize\nacross diverse speech contexts. This study addresses this gap by benchmarking\nseven Akan ASR models built on transformer architectures, such as Whisper and\nWav2Vec2, using four Akan speech corpora to determine their performance. These\ndatasets encompass various domains, including culturally relevant image\ndescriptions, informal conversations, biblical scripture readings, and\nspontaneous financial dialogues. A comparison of the word error rate and\ncharacter error rate highlighted domain dependency, with models performing\noptimally only within their training domains while showing marked accuracy\ndegradation in mismatched scenarios. This study also identified distinct error\nbehaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned\nWhisper Akan models led to more fluent but potentially misleading transcription\nerrors, Wav2Vec2 produced more obvious yet less interpretable outputs when\nencountering unfamiliar inputs. This trade-off between readability and\ntransparency in ASR errors should be considered when selecting architectures\nfor low-resource language (LRL) applications. These findings highlight the need\nfor targeted domain adaptation techniques, adaptive routing strategies, and\nmultilingual training frameworks for Akan and other LRLs.", "AI": {"tldr": "This study evaluated seven Akan ASR models using diverse datasets and found that performance is domain-dependent, with models struggling in mismatched domains.", "motivation": "To address the gap in ASR research regarding cross-domain generalization, especially for low-resource languages like Akan.", "method": "Benchmarked seven transformer-based ASR models (Whisper and Wav2Vec2) across four distinct Akan speech corpora and analyzed their word error rates and character error rates.", "result": "Found that models perform best in their training domains, but exhibit significant performance degradation in mismatched domains; Whisper and Wav2Vec2 displayed different error behaviors.", "conclusion": "Highlighted the need for domain adaptation techniques, adaptive routing strategies, and multilingual training frameworks for low-resource languages like Akan."}}
{"id": "2507.02613", "pdf": "https://arxiv.org/pdf/2507.02613", "abs": "https://arxiv.org/abs/2507.02613", "authors": ["Yalin E. Sagduyu", "Kemal Davaslioglu", "Tugba Erpek", "Sastry Kompella", "Gustave Anderson", "Jonathan Ashdown"], "title": "MULTI-SCOUT: Multistatic Integrated Sensing and Communications in 5G and Beyond for Moving Target Detection, Positioning, and Tracking", "categories": ["cs.NI", "cs.DC", "eess.SP"], "comment": null, "summary": "This paper presents a complete signal-processing chain for multistatic\nintegrated sensing and communications (ISAC) using 5G Positioning Reference\nSignal (PRS). We consider a distributed architecture in which one gNB transmits\na periodic OFDM-PRS waveform while multiple spatially separated receivers\nexploit the same signal for target detection, parameter estimation and\ntracking. A coherent cross-ambiguity function (CAF) is evaluated to form a\nrange-Doppler map from which the bistatic delay and radial velocity are\nextracted for every target. For a single target, the resulting bistatic delays\nare fused through nonlinear least-squares trilateration, yielding a geometric\nposition estimate, and a regularized linear inversion of the radial-speed\nequations yields a two-dimensional velocity vector, where speed and heading are\nobtained. The approach is applied to 2D and 3D settings, extended to account\nfor time synchronization bias, and generalized to multiple targets by resolving\ntarget association. The sequence of position-velocity estimates is then fed to\nstandard and extended Kalman filters to obtain smoothed tracks. Our results\nshow high-fidelity moving-target detection, positioning, and tracking using 5G\nPRS signals for multistatic ISAC.", "AI": {"tldr": "This paper outlines a system using 5G Positioning Reference Signal (PRS) for detecting, estimating, and tracking moving targets in multistatic ISAC systems.", "motivation": "To leverage the capabilities of 5G PRS for high-accuracy sensing and communication requirements in multistatic systems.", "method": "The paper uses a distributed architecture with multiple receivers, employing CAF for range-Doppler mapping, nonlinear least-squares trilateration for positioning, radial-speed inversion for velocity estimation, and Kalman filters for tracking.", "result": "The method demonstrates effective moving-target detection, position estimation, and velocity tracking, utilizing 5G PRS for multistatic ISAC.", "conclusion": "5G PRS is highly capable for multistatic ISAC, enabling accurate geometric positioning and velocity estimation, along with robust target tracking."}}
{"id": "2507.02564", "pdf": "https://arxiv.org/pdf/2507.02564", "abs": "https://arxiv.org/abs/2507.02564", "authors": ["Alexander Korn", "Samuel Gorsch", "Andreas Vogelsang"], "title": "LLMREI: Automating Requirements Elicitation Interviews with LLMs", "categories": ["cs.SE"], "comment": null, "summary": "Requirements elicitation interviews are crucial for gathering system\nrequirements but heavily depend on skilled analysts, making them\nresource-intensive, susceptible to human biases, and prone to miscommunication.\nRecent advancements in Large Language Models present new opportunities for\nautomating parts of this process. This study introduces LLMREI, a chat bot\ndesigned to conduct requirements elicitation interviews with minimal human\nintervention, aiming to reduce common interviewer errors and improve the\nscalability of requirements elicitation. We explored two main approaches,\nzero-shot prompting and least-to-most prompting, to optimize LLMREI for\nrequirements elicitation and evaluated its performance in 33 simulated\nstakeholder interviews. A third approach, fine-tuning, was initially considered\nbut abandoned due to poor performance in preliminary trials. Our study assesses\nthe chat bot's effectiveness in three key areas: minimizing common interview\nerrors, extracting relevant requirements, and adapting its questioning based on\ninterview context and user responses. Our findings indicate that LLMREI makes a\nsimilar number of errors compared to human interviewers, is capable of\nextracting a large portion of requirements, and demonstrates a notable ability\nto generate highly context-dependent questions. We envision the greatest\nbenefit of LLMREI in automating interviews with a large number of stakeholders.", "AI": {"tldr": "This paper introduces LLMREI, a chatbot leveraging Large Language Models for conducting requirements elicitation interviews, aiming to reduce errors and improve scalability compared to human interviewers.", "motivation": "Reduce reliance on skilled analysts in requirements elicitation interviews, minimize human biases and errors, and leverage recent advancements in Large Language Models to automate the interview process.", "method": "Developed LLMREI chatbot using zero-shot and least-to-most prompting approaches, evaluated its performance in 33 simulated interviews, and abandoned fine-tuning methods after poor preliminary results.", "result": "LLMREI demonstrated error rates similar to human interviewers, extracted a substantial portion of requirements, and generated context-dependent questions effectively.", "conclusion": "LLMREI shows significant potential in automating interviews, particularly in scenarios involving large numbers of stakeholders, though its error rates remain comparable to human interviewers."}}
{"id": "2507.02430", "pdf": "https://arxiv.org/pdf/2507.02430", "abs": "https://arxiv.org/abs/2507.02430", "authors": ["Maryem Fadili", "Mohamed Anis Ghaoui", "Louis Lecrosnier", "Steve Pechberti", "Redouane Khemmar"], "title": "A Late Collaborative Perception Framework for 3D Multi-Object and Multi-Source Association and Fusion", "categories": ["cs.RO", "eess.IV", "eess.SP"], "comment": null, "summary": "In autonomous driving, recent research has increasingly focused on\ncollaborative perception based on deep learning to overcome the limitations of\nindividual perception systems. Although these methods achieve high accuracy,\nthey rely on high communication bandwidth and require unrestricted access to\neach agent's object detection model architecture and parameters. These\nconstraints pose challenges real-world autonomous driving scenarios, where\ncommunication limitations and the need to safeguard proprietary models hinder\npractical implementation. To address this issue, we introduce a novel late\ncollaborative framework for 3D multi-source and multi-object fusion, which\noperates solely on shared 3D bounding box attributes-category, size, position,\nand orientation-without necessitating direct access to detection models. Our\nframework establishes a new state-of-the-art in late fusion, achieving up to\nfive times lower position error compared to existing methods. Additionally, it\nreduces scale error by a factor of 7.5 and orientation error by half, all while\nmaintaining perfect 100% precision and recall when fusing detections from\nheterogeneous perception systems. These results highlight the effectiveness of\nour approach in addressing real-world collaborative perception challenges,\nsetting a new benchmark for efficient and scalable multi-agent fusion.", "AI": {"tldr": "The paper introduces a late collaborative framework for 3D object fusion in autonomous driving, achieving state-of-the-art accuracy without requiring high bandwidth or access to detection model details.", "motivation": "The authors aim to overcome the constraints of high communication bandwidth and proprietary model access in collaborative perception methods for autonomous driving.", "method": "They propose a novel late collaborative framework that uses only shared 3D bounding box attributes (category, size, position, orientation) without accessing detection models.", "result": "The framework significantly improves error metrics, reducing position error fivefold, scale error by 7.5x, and halving orientation error, all with 100% precision and recall.", "conclusion": "The proposed method effectively addresses real-world challenges in collaborative perception, establishing a new benchmark for efficient and scalable multi-agent fusion."}}
{"id": "2507.02315", "pdf": "https://arxiv.org/pdf/2507.02315", "abs": "https://arxiv.org/abs/2507.02315", "authors": ["Sooyeon Kim", "Giung Nam", "Juho Lee"], "title": "Improving Constrained Generation in Language Models via Self-Distilled Twisted Sequential Monte Carlo", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Recent work has framed constrained text generation with autoregressive\nlanguage models as a probabilistic inference problem. Among these, Zhao et al.\n(2024) introduced a promising approach based on twisted Sequential Monte Carlo,\nwhich incorporates learned twist functions and twist-induced proposals to guide\nthe generation process. However, in constrained generation settings where the\ntarget distribution concentrates on outputs that are unlikely under the base\nmodel, learning becomes challenging due to sparse and uninformative reward\nsignals. We show that iteratively refining the base model through\nself-distillation alleviates this issue by making the model progressively more\naligned with the target, leading to substantial gains in generation quality.", "AI": {"tldr": "The paper revisits constrained text generation using autoregressive language models and improves generation quality through iterative self-distillation.", "motivation": "The paper addresses challenges in constrained text generation, particularly when the desired outputs (target distribution) are sparse or unlikely under the original language model, which hinders effective learning.", "method": "The authors propose an iterative self-distillation process to refine the base model, progressively aligning it with the target distribution for achieving better outcomes.", "result": "The iterative refinement approach results in marked improvements in generating high-quality constrained text outputs.", "conclusion": "By leveraging self-distillation, the study demonstrates that it is possible to overcome sparse reward challenges and significantly enhance the performance of constrained text generation models."}}
{"id": "2507.02265", "pdf": "https://arxiv.org/pdf/2507.02265", "abs": "https://arxiv.org/abs/2507.02265", "authors": ["Zhangding Liu", "Neda Mohammadi", "John E. Taylor"], "title": "Multi-Label Classification Framework for Hurricane Damage Assessment", "categories": ["cs.CV", "cs.AI"], "comment": "9 pages, 3 figures. Accepted at the ASCE International Conference on\n  Computing in Civil Engineering (i3CE 2025)", "summary": "Hurricanes cause widespread destruction, resulting in diverse damage types\nand severities that require timely and accurate assessment for effective\ndisaster response. While traditional single-label classification methods fall\nshort of capturing the complexity of post-hurricane damage, this study\nintroduces a novel multi-label classification framework for assessing damage\nusing aerial imagery. The proposed approach integrates a feature extraction\nmodule based on ResNet and a class-specific attention mechanism to identify\nmultiple damage types within a single image. Using the Rescuenet dataset from\nHurricane Michael, the proposed method achieves a mean average precision of\n90.23%, outperforming existing baseline methods. This framework enhances\npost-hurricane damage assessment, enabling more targeted and efficient disaster\nresponse and contributing to future strategies for disaster mitigation and\nresilience. This paper has been accepted at the ASCE International Conference\non Computing in Civil Engineering (i3CE 2025), and the camera-ready version\nwill appear in the official conference proceedings.", "AI": {"tldr": "This study introduces a multi-label classification framework for hurricane damage assessment using aerial imagery, achieving high accuracy compared to baseline methods.", "motivation": "The need for accurate, timely, and comprehensive damage assessment after hurricanes, as traditional single-label classification techniques fail to capture complex damage scenarios.", "method": "The framework employs ResNet for feature extraction and a class-specific attention mechanism to recognize multiple types of damage within a single aerial image, tested on the Rescuenet dataset from Hurricane Michael.", "result": "The proposed methodology outperforms existing methods with a mean average precision of 90.23%, demonstrating its efficacy in identifying various damage types.", "conclusion": "The study provides a robust tool for enhanced post-hurricane damage mapping, contributing to more effective disaster response and long-term resilience strategies."}}
{"id": "2507.02442", "pdf": "https://arxiv.org/pdf/2507.02442", "abs": "https://arxiv.org/abs/2507.02442", "authors": ["Moto Kamiura"], "title": "The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning", "categories": ["cs.AI", "math.CT", "stat.ME", "stat.ML"], "comment": null, "summary": "Enhancing the intelligibility and interpretability of machine learning is a\ncrucial task in responding to the demand for Explicability as an AI principle,\nand in promoting the better social implementation of AI. The aim of our\nresearch is to contribute to this improvement by reformulating machine learning\nmodels through the lens of category theory, thereby developing a semantic\nframework for structuring and understanding AI systems. Our categorical\nmodeling in this paper clarifies and formalizes the structural interplay\nbetween residuals and parameters in supervised learning. The present paper\nfocuses on the multiple linear regression model, which represents the most\nbasic form of supervised learning. By defining two concrete categories\ncorresponding to parameters and data, along with an adjoint pair of functors\nbetween them, we introduce our categorical formulation of supervised learning.\nWe show that the essential structure of this framework is captured by what we\ncall the Gauss-Markov Adjunction. Within this setting, the dual flow of\ninformation can be explicitly described as a correspondence between variations\nin parameters and residuals. The ordinary least squares estimator for the\nparameters and the minimum residual are related via the preservation of limits\nby the right adjoint functor. Furthermore, we position this formulation as an\ninstance of extended denotational semantics for supervised learning, and\npropose applying a semantic perspective developed in theoretical computer\nscience as a formal foundation for Explicability in AI.", "AI": {"tldr": "The paper uses category theory to reformulate supervised learning, focusing on multiple linear regression, introducing a structure called Gauss-Markov Adjunction.", "motivation": "To improve the intelligibility and interpretability of machine learning for better AI explicability and social implementation.", "method": "It employs category theory by defining categories for parameters and data, and linking them with adjoint functors to formalize supervised learning.", "result": "The paper derives a framework called Gauss-Markov Adjunction, describing the correspondence between parameters and residuals in the categorical setting.", "conclusion": "This categorical formulation provides a theoretical foundation for explicability in AI and leverages extended denotational semantics for supervised learning."}}
{"id": "2507.02428", "pdf": "https://arxiv.org/pdf/2507.02428", "abs": "https://arxiv.org/abs/2507.02428", "authors": ["Sumaya Ahmed Salihs", "Isaac Wiafe", "Jamal-Deen Abdulai", "Elikem Doe Atsakpo", "Gifty Ayoka", "Richard Cave", "Akon Obu Ekpezu", "Catherine Holloway", "Katrin Tomanek", "Fiifi Baffoe Payin Winful"], "title": "A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages", "categories": ["cs.CL"], "comment": "This version has been reviewed and accepted for presentation at the\n  InterSpeech 2025 conference to be held in Rotterdam from 17 to 21 August. 5\n  pages and 3 tables", "summary": "This study presents an approach for collecting speech samples to build\nAutomatic Speech Recognition (ASR) models for impaired speech, particularly,\nlow-resource languages. It aims to democratize ASR technology and data\ncollection by developing a \"cookbook\" of best practices and training for\ncommunity-driven data collection and ASR model building. As a proof-of-concept,\nthis study curated the first open-source dataset of impaired speech in Akan: a\nwidely spoken indigenous language in Ghana. The study involved participants\nfrom diverse backgrounds with speech impairments. The resulting dataset, along\nwith the cookbook and open-source tools, are publicly available to enable\nresearchers and practitioners to create inclusive ASR technologies tailored to\nthe unique needs of speech impaired individuals. In addition, this study\npresents the initial results of fine-tuning open-source ASR models to better\nrecognize impaired speech in Akan.", "AI": {"tldr": "This study builds a dataset and framework to create ASR models for speech-impaired speakers in Akan, Ghana.", "motivation": "To democratize ASR technology and make it accessible for low-resource and speech-impaired languages, specifically Akan in Ghana.", "method": "Created an open-source impaired Akan speech dataset, a cookbook of best practices, and trained ASR models with community participation.", "result": "Developed the first open-source dataset for impaired Akan speech and fine-tuned ASR models for better recognition of impaired speech in Akan.", "conclusion": "The resources and tools provided by this study promote the development of inclusive ASR technologies to support diverse speech needs."}}
{"id": "2507.02578", "pdf": "https://arxiv.org/pdf/2507.02578", "abs": "https://arxiv.org/abs/2507.02578", "authors": ["Zoe Pfister"], "title": "Human-Machine Collaboration and Ethical Considerations in Adaptive Cyber-Physical Systems", "categories": ["cs.SE", "cs.HC", "D.2.1"], "comment": "Copyright 2025 IEEE. Accepted for publication in: 2025 IEEE 33nd\n  International Requirements Engineering Conference (RE), Doctor Symposium\n  Paper, 5 pages", "summary": "Adaptive Cyber-Physical Systems (CPS) are systems that integrate both\nphysical and computational capabilities, which can adjust in response to\nchanging parameters. Furthermore, they increasingly incorporate human-machine\ncollaboration, allowing them to benefit from the individual strengths of humans\nand machines. Human-Machine Teaming (HMT) represents the most advanced paradigm\nof human-machine collaboration, envisioning seamless teamwork between humans\nand machines. However, achieving effective and seamless HMT in adaptive CPS is\nchallenging. While adaptive CPS already benefit from feedback loops such as\nMAPE-K, there is still a gap in integrating humans into these feedback loops\ndue to different operational cadences of humans and machines. Further, HMT\nrequires constant monitoring of human operators, collecting potentially\nsensitive information about their actions and behavior. Respecting the privacy\nand human values of the actors of the CPS is crucial for the success of\nhuman-machine teams. This research addresses these challenges by: (1)\ndeveloping novel methods and processes for integrating HMT into adaptive CPS,\nfocusing on human-machine interaction principles and their incorporation into\nadaptive feedback loops found in CPS, and (2) creating frameworks for\nintegrating, verifying, and validating ethics and human values throughout the\nsystem lifecycle, starting from requirements engineering.", "AI": {"tldr": "This paper addresses the integration of Human-Machine Teaming (HMT) into Adaptive Cyber-Physical Systems (CPS) by developing methods for seamless interaction and ethical frameworks.", "motivation": "Adaptive CPS increasingly involve Human-Machine Teaming, raising the need for seamless collaboration while addressing operational cadences and respecting human values and privacy.", "method": "The research introduces novel methods for integrating HMT into adaptive CPS feedback loops and frameworks for incorporating ethics and human values into system lifecycles, starting at requirements engineering.", "result": "The result includes improved human-machine interaction methods and processes, alongside ethical frameworks for CPS design and operation.", "conclusion": "Integrating HMT into adaptive CPS is accomplished through innovative methods and ethical frameworks, ensuring privacy and human values are respected."}}
{"id": "2507.02438", "pdf": "https://arxiv.org/pdf/2507.02438", "abs": "https://arxiv.org/abs/2507.02438", "authors": ["Shivam Chaubey", "Francesco Verdoja", "Shankar Deka", "Ville Kyrki"], "title": "MISC: Minimal Intervention Shared Control with Guaranteed Safety under Non-Convex Constraints", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Shared control combines human intention with autonomous decision-making, from\nlow-level safety overrides to high-level task guidance, enabling systems that\nadapt to users while ensuring safety and performance. This enhances task\neffectiveness and user experience across domains such as assistive robotics,\nteleoperation, and autonomous driving. However, existing shared control\nmethods, based on e.g. Model Predictive Control, Control Barrier Functions, or\nlearning-based control, struggle with feasibility, scalability, or safety\nguarantees, particularly since the user input is unpredictable.\n  To address these challenges, we propose an assistive controller framework\nbased on Constrained Optimal Control Problem that incorporates an\noffline-computed Control Invariant Set, enabling online computation of control\nactions that ensure feasibility, strict constraint satisfaction, and minimal\noverride of user intent. Moreover, the framework can accommodate structured\nclass of non-convex constraints, which are common in real-world scenarios. We\nvalidate the approach through a large-scale user study with 66\nparticipants--one of the most extensive in shared control research--using a\ncomputer game environment to assess task load, trust, and perceived control, in\naddition to performance. The results show consistent improvements across all\nthese aspects without compromising safety and user intent.", "AI": {"tldr": "The paper presents a shared control framework that improves user experience and safety by combining constrained optimal control and offline-computed control invariant sets, validated through a large-scale user study.", "motivation": "Existing shared control methods face challenges in feasibility, scalability, and safety, especially when dealing with unpredictable user input.", "method": "They propose a framework based on Constrained Optimal Control Problem with offline-computed Control Invariant Sets, enabling real-time safety guarantees and accommodating non-convex constraints.", "result": "A large-scale user study with 66 participants showed improvements in task load, trust, perceived control, and performance, without compromising safety or user intent.", "conclusion": "The proposed framework enhances shared control usability and effectiveness, offering a feasible and safe solution for unpredictable user input scenarios."}}
{"id": "2507.02268", "pdf": "https://arxiv.org/pdf/2507.02268", "abs": "https://arxiv.org/abs/2507.02268", "authors": ["Yuxiang Zhang", "Wei Li", "Wen Jia", "Mengmeng Zhang", "Ran Tao", "Shunlin Liang"], "title": "Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Utilizing hyperspectral remote sensing technology enables the extraction of\nfine-grained land cover classes. Typically, satellite or airborne images used\nfor training and testing are acquired from different regions or times, where\nthe same class has significant spectral shifts in different scenes. In this\npaper, we propose a Bi-directional Domain Adaptation (BiDA) framework for\ncross-domain hyperspectral image (HSI) classification, which focuses on\nextracting both domain-invariant features and domain-specific information in\nthe independent adaptive space, thereby enhancing the adaptability and\nseparability to the target scene. In the proposed BiDA, a triple-branch\ntransformer architecture (the source branch, target branch, and coupled branch)\nwith semantic tokenizer is designed as the backbone. Specifically, the source\nbranch and target branch independently learn the adaptive space of source and\ntarget domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is\ndeveloped in coupled branch for feature interaction and inter-domain\ncorrelation mining. Furthermore, a bi-directional distillation loss is designed\nto guide adaptive space learning using inter-domain correlation. Finally, we\npropose an Adaptive Reinforcement Strategy (ARS) to encourage the model to\nfocus on specific generalized feature extraction within both source and target\nscenes in noise condition. Experimental results on cross-temporal/scene\nairborne and satellite datasets demonstrate that the proposed BiDA performs\nsignificantly better than some state-of-the-art domain adaptation approaches.\nIn the cross-temporal tree species classification task, the proposed BiDA is\nmore than 3\\%$\\sim$5\\% higher than the most advanced method. The codes will be\navailable from the website:\nhttps://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.", "AI": {"tldr": "The study proposes the Bi-directional Domain Adaptation (BiDA) framework for cross-domain hyperspectral image classification, focusing on addressing spectral shifts across regions or timeframes using a triple-branch transformer architecture.", "motivation": "The paper addresses the problem of spectral shifts in hyperspectral remote sensing images due to different regions or timeframes, which affects the accuracy of land cover classification.", "method": "A Bi-directional Domain Adaptation (BiDA) framework is proposed using a triple-branch transformer architecture (source, target, and coupled branches) with a semantic tokenizer. It features Coupled Multi-head Cross-attention (CMCA) for inter-domain correlation and a bi-directional distillation loss to enhance adaptive space learning. Additionally, it introduces an Adaptive Reinforcement Strategy (ARS) for noise-resistant feature extraction.", "result": "Experimental results demonstrate that BiDA outperforms state-of-the-art domain adaptation approaches, achieving 3-5% higher accuracy in cross-temporal tree species classification tasks on multiple airborne and satellite datasets.", "conclusion": "The proposed BiDA framework significantly improves hyperspectral image classification across domains, effectively addressing spectral shifts. It demonstrates potential for wide applicability with its stronger adaptability and separability in various settings."}}
{"id": "2507.02092", "pdf": "https://arxiv.org/pdf/2507.02092", "abs": "https://arxiv.org/abs/2507.02092", "authors": ["Alexi Gladstone", "Ganesh Nanduru", "Md Mofijul Islam", "Peixuan Han", "Hyeonjeong Ha", "Aman Chadha", "Yilun Du", "Heng Ji", "Jundong Li", "Tariq Iqbal"], "title": "Energy-Based Transformers are Scalable Learners and Thinkers", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Inference-time computation techniques, analogous to human System 2 Thinking,\nhave recently become popular for improving model performances. However, most\nexisting approaches suffer from several limitations: they are modality-specific\n(e.g., working only in text), problem-specific (e.g., verifiable domains like\nmath and coding), or require additional supervision/training on top of\nunsupervised pretraining (e.g., verifiers or verifiable rewards). In this\npaper, we ask the question \"Is it possible to generalize these System 2\nThinking approaches, and develop models that learn to think solely from\nunsupervised learning?\" Interestingly, we find the answer is yes, by learning\nto explicitly verify the compatibility between inputs and\ncandidate-predictions, and then re-framing prediction problems as optimization\nwith respect to this verifier. Specifically, we train Energy-Based Transformers\n(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy\nvalue to every input and candidate-prediction pair, enabling predictions\nthrough gradient descent-based energy minimization until convergence. Across\nboth discrete (text) and continuous (visual) modalities, we find EBTs scale\nfaster than the dominant Transformer++ approach during training, achieving an\nup to 35% higher scaling rate with respect to data, batch size, parameters,\nFLOPs, and depth. During inference, EBTs improve performance with System 2\nThinking by 29% more than the Transformer++ on language tasks, and EBTs\noutperform Diffusion Transformers on image denoising while using fewer forward\npasses. Further, we find that EBTs achieve better results than existing models\non most downstream tasks given the same or worse pretraining performance,\nsuggesting that EBTs generalize better than existing approaches. Consequently,\nEBTs are a promising new paradigm for scaling both the learning and thinking\ncapabilities of models.", "AI": {"tldr": "This paper proposes Energy-Based Transformers (EBTs) as a general framework to improve model performance using System 2 Thinking, suitable for both text and image tasks, showing superior efficiency and generalization.", "motivation": "Current inference-time System 2 Thinking approaches are limited by being modality-specific, problem-specific, or requiring additional supervision. The authors aim to generalize System 2 Thinking to develop unsupervised models capable of improved decision-making.", "method": "The authors introduce Energy-Based Transformers (EBTs), which assign energy values to input-prediction pairs and use gradient descent-based energy minimization for predictions. EBTs enable better performance across discrete and continuous modalities.", "result": "EBTs achieve a 35% higher scaling rate than Transformer++ models and improve System 2 Thinking performance by 29% during inference on language tasks. They outperform diffusion-based models on image denoising and generalize better on downstream tasks, even under worse pretraining conditions.", "conclusion": "Energy-Based Transformers (EBTs) provide a promising and scalable solution for enhancing both learning and thinking capabilities across diverse modalities, offering a generalizable approach for unsupervised advanced reasoning."}}
{"id": "2507.02541", "pdf": "https://arxiv.org/pdf/2507.02541", "abs": "https://arxiv.org/abs/2507.02541", "authors": ["Yanzhen Lu", "Hanbin Yang", "Xiaodie Wang", "Ge Zhang", "Biao Li", "Chenxu Fu", "Chao Li", "Yang Yuan", "Andrew Chi-Chih Yao"], "title": "Clarifying Before Reasoning: A Coq Prover with Structural Context", "categories": ["cs.AI"], "comment": null, "summary": "In this work, we investigate whether improving task clarity can enhance\nreasoning ability of large language models, focusing on theorem proving in Coq.\nWe introduce a concept-level metric to evaluate task clarity and show that\nadding structured semantic context to the standard input used by modern LLMs,\nleads to a 1.85$\\times$ improvement in clarity score\n(44.5\\%~$\\rightarrow$~82.3\\%). Using the general-purpose model\n\\texttt{DeepSeek-V3}, our approach leads to a 2.1$\\times$ improvement in proof\nsuccess (21.8\\%~$\\rightarrow$~45.8\\%) and outperforms the previous\nstate-of-the-art \\texttt{Graph2Tac} (33.2\\%). We evaluate this on 1,386\ntheorems randomly sampled from 15 standard Coq packages, following the same\nevaluation protocol as \\texttt{Graph2Tac}. Furthermore, fine-tuning smaller\nmodels on our structured data can achieve even higher performance (48.6\\%). Our\nmethod uses selective concept unfolding to enrich task descriptions, and\nemploys a Planner--Executor architecture. These findings highlight the value of\nstructured task representations in bridging the gap between understanding and\nreasoning.", "AI": {"tldr": "This paper explores improving reasoning abilities of large language models (LLMs) in theorem proving by enhancing task clarity using structured semantic context, achieving a significant improvement in proof success.", "motivation": "The study is motivated by the need to enhance reasoning capabilities of LLMs in complex domains, such as theorem proving, by addressing the challenge of task clarity.", "method": "The authors introduce a concept-level clarity metric and a structured semantic context to task descriptions, leveraging selective concept unfolding and a Planner--Executor architecture to improve LLM performance.", "result": "Adding structured context improves the clarity score from 44.5% to 82.3% and increases proof success rate from 21.8% to 45.8% using the general-purpose DeepSeek-V3 model, outperforming the previous state-of-the-art Graph2Tac. Fine-tuned smaller models outperform even further (48.6%).", "conclusion": "The findings emphasize the importance of structured task representations in enhancing understanding and reasoning in LLMs, particularly for complex tasks like theorem proving."}}
{"id": "2507.02506", "pdf": "https://arxiv.org/pdf/2507.02506", "abs": "https://arxiv.org/abs/2507.02506", "authors": ["Sneha Deshmukh", "Prathmesh Kamble"], "title": "IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders", "categories": ["cs.CL", "cs.AI", "cs.LG", "91B14, 68T50", "I.2.7; K.4.1; K.5.2"], "comment": "9 pages, 9 figures, 2 tables. Dataset available at Hugging Face and\n  GitHub. Submitted to arXiv for open access", "summary": "Legal NLP remains underdeveloped in regions like India due to the scarcity of\nstructured datasets. We introduce IndianBailJudgments-1200, a new benchmark\ndataset comprising 1200 Indian court judgments on bail decisions, annotated\nacross 20+ attributes including bail outcome, IPC sections, crime type, and\nlegal reasoning. Annotations were generated using a prompt-engineered GPT-4o\npipeline and verified for consistency. This resource supports a wide range of\nlegal NLP tasks such as outcome prediction, summarization, and fairness\nanalysis, and is the first publicly available dataset focused specifically on\nIndian bail jurisprudence.", "AI": {"tldr": "The paper introduces IndianBailJudgments-1200, a publicly available dataset of 1200 Indian court bail decisions with 20+ attribute annotations to support various Legal NLP tasks.", "motivation": "Legal NLP in regions like India lacks progress due to the unavailability of structured datasets, particularly in bail jurisprudence.", "method": "The authors developed the IndianBailJudgments-1200 dataset by employing a GPT-4o pipeline for prompt-engineered annotations and ensuring consistency through verification.", "result": "Created a 1200-record legal dataset annotated with over 20 attributes, enabling legal NLP tasks like prediction, summarization, and analysis.", "conclusion": "IndianBailJudgments-1200 fills a significant gap in structured legal datasets for India, setting a foundation for developing Legal NLP applications in the region."}}
{"id": "2507.02665", "pdf": "https://arxiv.org/pdf/2507.02665", "abs": "https://arxiv.org/abs/2507.02665", "authors": ["Timo Kehrer", "Robert Haines", "Guido Juckeland", "Shurui Zhou", "David E. Bernholdt"], "title": "Do Research Software Engineers and Software Engineering Researchers Speak the Same Language?", "categories": ["cs.SE"], "comment": "Early access journal version: T. Kehrer, R. Haines, G. Juckeland, S.\n  Zhou and D. E. Bernholdt, \"Do Research Software Engineers and Software\n  Engineering Researchers Speak the Same Language?,\" in Computing in Science &\n  Engineering, doi: 10.1109/MCSE.2025.3557236", "summary": "Anecdotal evidence suggests that Research Software Engineers (RSEs) and\nSoftware Engineering Researchers (SERs) often use different terminologies for\nsimilar concepts, creating communication challenges. To better understand these\ndivergences, we have started investigating how SE fundamentals from the SER\ncommunity are interpreted within the RSE community, identifying aligned\nconcepts, knowledge gaps, and areas for potential adaptation. Our preliminary\nfindings reveal opportunities for mutual learning and collaboration, and our\nsystematic methodology for terminology mapping provides a foundation for a\ncrowd-sourced extension and validation in the future.", "AI": {"tldr": "The paper investigates the terminology differences between Research Software Engineers (RSEs) and Software Engineering Researchers (SERs), proposing a systematic approach to bridge gaps and foster collaboration.", "motivation": "To address communication challenges between RSEs and SERs caused by divergent terminologies and understand how software engineering fundamentals are interpreted within the RSE community.", "method": "A systematic terminology mapping methodology was employed to identify aligned concepts, knowledge gaps, and potential adaptations between the RSE and SER communities.", "result": "The study revealed opportunities for mutual learning and collaboration between the two fields.", "conclusion": "A foundation is provided for future crowd-sourced extension and validation to improve communication and collaboration between RSEs and SERs."}}
{"id": "2507.02447", "pdf": "https://arxiv.org/pdf/2507.02447", "abs": "https://arxiv.org/abs/2507.02447", "authors": ["Xiang Zhou", "Xinyu Zhang", "Qingrui Zhang"], "title": "HAC-LOCO: Learning Hierarchical Active Compliance Control for Quadruped Locomotion under Continuous External Disturbances", "categories": ["cs.RO"], "comment": "8 pages, 7 Figures", "summary": "Despite recent remarkable achievements in quadruped control, it remains\nchallenging to ensure robust and compliant locomotion in the presence of\nunforeseen external disturbances. Existing methods prioritize locomotion\nrobustness over compliance, often leading to stiff, high-frequency motions, and\nenergy inefficiency. This paper, therefore, presents a two-stage hierarchical\nlearning framework that can learn to take active reactions to external force\ndisturbances based on force estimation. In the first stage, a velocity-tracking\npolicy is trained alongside an auto-encoder to distill historical\nproprioceptive features. A neural network-based estimator is learned through\nsupervised learning, which estimates body velocity and external forces based on\nproprioceptive measurements. In the second stage, a compliance action module,\ninspired by impedance control, is learned based on the pre-trained encoder and\npolicy. This module is employed to actively adjust velocity commands in\nresponse to external forces based on real-time force estimates. With the\ncompliance action module, a quadruped robot can robustly handle minor\ndisturbances while appropriately yielding to significant forces, thus striking\na balance between robustness and compliance. Simulations and real-world\nexperiments have demonstrated that our method has superior performance in terms\nof robustness, energy efficiency, and safety. Experiment comparison shows that\nour method outperforms the state-of-the-art RL-based locomotion controllers.\nAblation studies are given to show the critical roles of the compliance action\nmodule.", "AI": {"tldr": "The study proposes a hierarchical learning framework integrating force estimation and compliance to achieve robust and energy-efficient quadruped locomotion under external disturbances.", "motivation": "Current quadruped control methods struggle to balance robustness and compliance, leading to stiff locomotion and energy inefficiency.", "method": "The framework consists of two stages: velocity-tracking policy training with a force estimator in stage one, and compliance action module inspired by impedance control in stage two.", "result": "The proposed approach improves quadruped performance in robustness, energy efficiency, and safety, outperforming state-of-the-art RL controllers with validated results in simulations and real-world experiments.", "conclusion": "The compliance action module plays a critical role in achieving a balance between robustness and compliance for quadruped locomotion, making the solution practical and highly effective."}}
{"id": "2507.02496", "pdf": "https://arxiv.org/pdf/2507.02496", "abs": "https://arxiv.org/abs/2507.02496", "authors": ["Vaidehi Srinivas"], "title": "Online Conformal Prediction with Efficiency Guarantees", "categories": ["cs.LG", "cs.DS", "math.ST", "stat.ML", "stat.TH"], "comment": null, "summary": "We study the problem of conformal prediction in a novel online framework that\ndirectly optimizes efficiency. In our problem, we are given a target\nmiscoverage rate $\\alpha > 0$, and a time horizon $T$. On each day $t \\le T$ an\nalgorithm must output an interval $I_t \\subseteq [0, 1]$, then a point $y_t \\in\n[0, 1]$ is revealed. The goal of the algorithm is to achieve coverage, that is,\n$y_t \\in I_t$ on (close to) a $(1 - \\alpha)$-fraction of days, while\nmaintaining efficiency, that is, minimizing the average volume (length) of the\nintervals played. This problem is an online analogue to the problem of\nconstructing efficient confidence intervals.\n  We study this problem over arbitrary and exchangeable (random order) input\nsequences. For exchangeable sequences, we show that it is possible to construct\nintervals that achieve coverage $(1 - \\alpha) - o(1)$, while having length\nupper bounded by the best fixed interval that achieves coverage in hindsight.\nFor arbitrary sequences however, we show that any algorithm that achieves a\n$\\mu$-approximation in average length compared to the best fixed interval\nachieving coverage in hindsight, must make a multiplicative factor more\nmistakes than $\\alpha T$, where the multiplicative factor depends on $\\mu$ and\nthe aspect ratio of the problem. Our main algorithmic result is a matching\nalgorithm that can recover all Pareto-optimal settings of $\\mu$ and number of\nmistakes. Furthermore, our algorithm is deterministic and therefore robust to\nan adaptive adversary.\n  This gap between the exchangeable and arbitrary settings is in contrast to\nthe classical online learning problem. In fact, we show that no single\nalgorithm can simultaneously be Pareto-optimal for arbitrary sequences and\noptimal for exchangeable sequences. On the algorithmic side, we give an\nalgorithm that achieves the near-optimal tradeoff between the two cases.", "AI": {"tldr": "The paper investigates conformal prediction in an online setting, focusing on achieving coverage while optimizing efficiency in terms of interval volume. It highlights differences between exchangeable and arbitrary sequences in achieving optimal performance, with a matching deterministic algorithm provided.", "motivation": "The motivation is to address the need for efficient confidence intervals in online and sequential settings, balancing coverage accuracy (maintaining miscoverage rates) and efficiency (minimizing interval lengths).", "method": "The method involves studying the problem for two settings: exchangeable and arbitrary sequences. They propose algorithms that achieve coverage targets and optimize interval lengths, with a focus on Pareto-optimal tradeoffs and deterministic algorithms robust to adversarial conditions.", "result": "For exchangeable sequences, intervals can achieve near-perfect coverage efficiency, while for arbitrary sequences, achieving optimal efficiency leads to more coverage mistakes. A matching algorithm is introduced that balances efficiency and mistakes, achieving Pareto-optimal settings.", "conclusion": "There is a fundamental difference between performance in exchangeable and arbitrary input sequences for this online conformal prediction problem. The paper provides theoretical guarantees and a deterministic algorithm for optimal tradeoffs between efficiency and coverage."}}
{"id": "2507.02270", "pdf": "https://arxiv.org/pdf/2507.02270", "abs": "https://arxiv.org/abs/2507.02270", "authors": ["Fanghai Yi", "Zehong Zheng", "Zexiao Liang", "Yihang Dong", "Xiyang Fang", "Wangyu Wu", "Xuhang Chen"], "title": "MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement", "categories": ["cs.CV"], "comment": "Accepted by IEEE SMC 2025", "summary": "Enhancing underwater images is crucial for exploration. These images face\nvisibility and color issues due to light changes, water turbidity, and bubbles.\nTraditional prior-based methods and pixel-based methods often fail, while deep\nlearning lacks sufficient high-quality datasets. We introduce the Multi-Axis\nConditional Lookup (MAC-Lookup) model, which enhances visual quality by\nimproving color accuracy, sharpness, and contrast. It includes Conditional 3D\nLookup Table Color Correction (CLTCC) for preliminary color and quality\ncorrection and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.\nThis model prevents over-enhancement and saturation while handling underwater\nchallenges. Extensive experiments show that MAC-Lookup excels in enhancing\nunderwater images by restoring details and colors better than existing methods.\nThe code is https://github.com/onlycatdoraemon/MAC-Lookup.", "AI": {"tldr": "The paper introduces the MAC-Lookup model, which significantly improves the quality of underwater images suffering from distortions caused by various underwater conditions.", "motivation": "Underwater images often suffer from visibility, color distortion, turbidity, and bubbles, yet many existing methods fail to address these issues adequately due to prior-based approaches or lack of high-quality datasets for deep learning.", "method": "The authors propose the MAC-Lookup model, which includes Conditional 3D Lookup Table Color Correction (CLTCC) for initial corrections and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.", "result": "Experimental results reveal that MAC-Lookup outperforms existing methods by efficiently restoring underwater image details and color accuracy without introducing over-enhancement or saturation.", "conclusion": "MAC-Lookup offers a robust solution for underwater image enhancement by addressing unique underwater challenges while improving color, sharpness, and contrast. The model demonstrates superiority over prior approaches."}}
{"id": "2507.02109", "pdf": "https://arxiv.org/pdf/2507.02109", "abs": "https://arxiv.org/abs/2507.02109", "authors": ["Florian Gr\u00f6tschla", "Luca A. Lanzend\u00f6rfer", "Longxiang Jiao", "Roger Wattenhofer"], "title": "Parametric Neural Amp Modeling with Active Learning", "categories": ["cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted at ISMIR 2025 as Late-Breaking Demo (LBD)", "summary": "We introduce PANAMA, an active learning framework for the training of\nend-to-end parametric guitar amp models using a WaveNet-like architecture. With\n\\model, one can create a virtual amp by recording samples that are determined\nby an active learning strategy to use a minimum amount of datapoints (i.e., amp\nknob settings). We show that gradient-based optimization algorithms can be used\nto determine the optimal datapoints to sample, and that the approach helps\nunder a constrained number of samples.", "AI": {"tldr": "This paper presents PANAMA, an active learning framework to efficiently train end-to-end parametric guitar amp models using minimal data.", "motivation": "The study aims to address the challenge of developing accurate guitar amp models using fewer data samples for training.", "method": "PANAMA employs a WaveNet-like architecture and utilizes gradient-based optimization for active learning, selecting the most effective data points for training.", "result": "The framework shows that active learning with gradient-based optimization can produce effective models with a constrained number of samples.", "conclusion": "Active learning strategies like PANAMA can efficiently create virtual guitar amps, reducing the data samples needed for high-quality modeling."}}
{"id": "2507.02554", "pdf": "https://arxiv.org/pdf/2507.02554", "abs": "https://arxiv.org/abs/2507.02554", "authors": ["Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Rishi Hazra", "Nicolas Baldwin", "Alexis Audran-Reiss", "Michael Kuchnik", "Despoina Magka", "Minqi Jiang", "Alisia Maria Lupidi", "Andrei Lupu", "Roberta Raileanu", "Kelvin Niu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Michael Shvartsman", "Shagun Sodhani", "Alexander H. Miller", "Abhishek Charnalia", "Derek Dunfield", "Carole-Jean Wu", "Pontus Stenetorp", "Nicola Cancedda", "Jakob Nicolaus Foerster", "Yoram Bachrach"], "title": "AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench", "categories": ["cs.AI", "cs.LG"], "comment": "Code: https://github.com/facebookresearch/aira-dojo", "summary": "AI research agents are demonstrating great potential to accelerate scientific\nprogress by automating the design, implementation, and training of machine\nlearning models. We focus on methods for improving agents' performance on\nMLE-bench, a challenging benchmark where agents compete in Kaggle competitions\nto solve real-world machine learning problems. We formalize AI research agents\nas search policies that navigate a space of candidate solutions, iteratively\nmodifying them using operators. By designing and systematically varying\ndifferent operator sets and search policies (Greedy, MCTS, Evolutionary), we\nshow that their interplay is critical for achieving high performance. Our best\npairing of search strategy and operator set achieves a state-of-the-art result\non MLE-bench lite, increasing the success rate of achieving a Kaggle medal from\n39.6% to 47.7%. Our investigation underscores the importance of jointly\nconsidering the search strategy, operator design, and evaluation methodology in\nadvancing automated machine learning.", "AI": {"tldr": "The paper explores methods to improve AI research agents' performance on MLE-bench, a benchmark for machine learning automation, achieving a 47.7% success rate in Kaggle medal attainment.", "motivation": "To advance the performance of AI agents in automating ML workflows, particularly in real-world Kaggle competition scenarios, and improve upon the limitations of current systems.", "method": "Formalized AI research agents as search policies, experimented with varying operator sets and search strategies (Greedy, MCTS, Evolutionary), and evaluated their performance systematically on MLE-bench lite.", "result": "The best combination of search strategy and operator sets achieved a significant improvement, raising the success rate for achieving Kaggle competition medals from 39.6% to 47.7%.", "conclusion": "The interplay between search strategy, operator design, and evaluation is vital for advancing automation in machine learning, paving the way for better-performing AI research agents."}}
{"id": "2507.02592", "pdf": "https://arxiv.org/pdf/2507.02592", "abs": "https://arxiv.org/abs/2507.02592", "authors": ["Kuan Li", "Zhongwang Zhang", "Huifeng Yin", "Liwen Zhang", "Litu Ou", "Jialong Wu", "Wenbiao Yin", "Baixuan Li", "Zhengwei Tao", "Xinyu Wang", "Weizhou Shen", "Junkai Zhang", "Dingchu Zhang", "Xixi Wu", "Yong Jiang", "Ming Yan", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebSailor: Navigating Super-human Reasoning for Web Agent", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all opensource agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.", "AI": {"tldr": "Introducing WebSailor, a post-training methodology designed to enhance reasoning in navigating uncertain information landscapes, achieving parity with proprietary models on complex benchmarks.", "motivation": "Addressing the critical frontier of improving LLM capabilities beyond human cognitive limits, especially for information-seeking tasks with high uncertainty.", "method": "A specialized pipeline involving high-uncertainty task generation via structured sampling, RFT cold start, and an efficient agentic RL algorithm called Duplicating Sampling Policy Optimization (DUPO).", "result": "WebSailor substantially outperforms open-source models in complex information-seeking tasks, achieving parity with proprietary systems like DeepResearch.", "conclusion": "WebSailor bridges the performance gap between open-source and proprietary models by introducing an advanced reasoning capability essential for navigating high-uncertainty data landscapes."}}
{"id": "2507.02690", "pdf": "https://arxiv.org/pdf/2507.02690", "abs": "https://arxiv.org/abs/2507.02690", "authors": ["Jiaxing Wang", "Yifeng Yu", "Jiahan Song", "Bin Cao", "Jing Fan", "Ji Zhang"], "title": "RLHGNN: Reinforcement Learning-driven Heterogeneous Graph Neural Network for Next Activity Prediction in Business Processes", "categories": ["cs.SE", "cs.LG"], "comment": "15 pages, 7 figures. Business process prediction using reinforcement\n  learning and heterogeneous graph neural networks", "summary": "Next activity prediction represents a fundamental challenge for optimizing\nbusiness processes in service-oriented architectures such as microservices\nenvironments, distributed enterprise systems, and cloud-native platforms, which\nenables proactive resource allocation and dynamic service composition. Despite\nthe prevalence of sequence-based methods, these approaches fail to capture\nnon-sequential relationships that arise from parallel executions and\nconditional dependencies. Even though graph-based approaches address structural\npreservation, they suffer from homogeneous representations and static\nstructures that apply uniform modeling strategies regardless of individual\nprocess complexity characteristics. To address these limitations, we introduce\nRLHGNN, a novel framework that transforms event logs into heterogeneous process\ngraphs with three distinct edge types grounded in established process mining\ntheory. Our approach creates four flexible graph structures by selectively\ncombining these edges to accommodate different process complexities, and\nemploys reinforcement learning formulated as a Markov Decision Process to\nautomatically determine the optimal graph structure for each specific process\ninstance. RLHGNN then applies heterogeneous graph convolution with\nrelation-specific aggregation strategies to effectively predict the next\nactivity. This adaptive methodology enables precise modeling of both sequential\nand non-sequential relationships in service interactions. Comprehensive\nevaluation on six real-world datasets demonstrates that RLHGNN consistently\noutperforms state-of-the-art approaches. Furthermore, it maintains an inference\nlatency of approximately 1 ms per prediction, representing a highly practical\nsolution suitable for real-time business process monitoring applications. The\nsource code is available at https://github.com/Joker3993/RLHGNN.", "AI": {"tldr": "This paper introduces RLHGNN, a framework for predicting next activities in complex business processes by transforming event logs into heterogeneous process graphs, achieving superior performance and practicality.", "motivation": "Existing sequence-based methods fail to capture non-sequential relationships, while graph-based methods suffer from homogeneous representations and static structures, both limiting their effectiveness in accurately modeling real-world process complexities.", "method": "RLHGNN transforms event logs into heterogeneous process graphs with three edge types and selectively combines these edges into four graph structures. Reinforcement learning identifies the optimal graph structure for specific instances, and heterogeneous graph convolution is applied for prediction.", "result": "RLHGNN consistently outperforms state-of-the-art approaches across six real-world datasets and achieves an inference latency of approximately 1 ms per prediction, suitable for real-time applications.", "conclusion": "RLHGNN is a novel, efficient framework that enables precise modeling of complex business processes, integrating sequential and non-sequential relationships for real-time applications."}}
{"id": "2507.02521", "pdf": "https://arxiv.org/pdf/2507.02521", "abs": "https://arxiv.org/abs/2507.02521", "authors": ["Ayodeji O. Abioye", "Jayati Deshmukh", "Athina Georgara", "Dominic Price", "Tuyen Nguyen", "Aleksandra Landowska", "Amel Bennaceur", "Joel E. Fischer", "Sarvapali D. Ramchurn"], "title": "Safe and Socially Aware Multi-Robot Coordination in Multi-Human Social Care Settings", "categories": ["cs.RO"], "comment": "3 pages, 1 figure. Accepted for poster presentation at the UK AI\n  Research Symposium (UKAIR) 2025, themed \"A Festival of Ideas\", being held in\n  Newcastle from 8th - 9th September, 2025. https://www.ukairs.ac.uk/", "summary": "This research investigates strategies for multi-robot coordination in\nmulti-human environments. It proposes a multi-objective learning-based\ncoordination approach to addressing the problem of path planning, navigation,\ntask scheduling, task allocation, and human-robot interaction in multi-human\nmulti-robot (MHMR) settings.", "AI": {"tldr": "The paper explores multi-objective strategies for coordination in multi-robot and multi-human settings, focusing on path planning, navigation, and task management.", "motivation": "To address challenges in coordinating multiple robots and humans, specifically in areas like path planning, task scheduling, and human-robot interaction, in dynamic environments.", "method": "A learning-based, multi-objective approach is introduced to optimize coordination in multi-human, multi-robot scenarios.", "result": "The paper evaluates improvements in path planning, navigation, task scheduling, allocation, and interaction in MHMR environments.", "conclusion": "This study provides a framework for enhancing coordination and functionality in multi-human, multi-robot systems by addressing key operational challenges."}}
{"id": "2507.02593", "pdf": "https://arxiv.org/pdf/2507.02593", "abs": "https://arxiv.org/abs/2507.02593", "authors": ["Cornelia Gruber", "Helen Alber", "Bernd Bischl", "G\u00f6ran Kauermann", "Barbara Plank", "Matthias A\u00dfenmacher"], "title": "Revisiting Active Learning under (Human) Label Variation", "categories": ["cs.CL", "cs.HC", "cs.LG", "stat.ML"], "comment": null, "summary": "Access to high-quality labeled data remains a limiting factor in applied\nsupervised learning. While label variation (LV), i.e., differing labels for the\nsame instance, is common, especially in natural language processing, annotation\nframeworks often still rest on the assumption of a single ground truth. This\noverlooks human label variation (HLV), the occurrence of plausible differences\nin annotations, as an informative signal. Similarly, active learning (AL), a\npopular approach to optimizing the use of limited annotation budgets in\ntraining ML models, often relies on at least one of several simplifying\nassumptions, which rarely hold in practice when acknowledging HLV. In this\npaper, we examine foundational assumptions about truth and label nature,\nhighlighting the need to decompose observed LV into signal (e.g., HLV) and\nnoise (e.g., annotation error). We survey how the AL and (H)LV communities have\naddressed -- or neglected -- these distinctions and propose a conceptual\nframework for incorporating HLV throughout the AL loop, including instance\nselection, annotator choice, and label representation. We further discuss the\nintegration of large language models (LLM) as annotators. Our work aims to lay\na conceptual foundation for HLV-aware active learning, better reflecting the\ncomplexities of real-world annotation.", "AI": {"tldr": "This paper highlights the importance of considering human label variation (HLV) in supervised learning and active learning (AL), proposing a framework to incorporate HLV in the AL loop and exploring the use of large language models (LLMs) as annotators.", "motivation": "The paper seeks to address the gap in current annotation frameworks and active learning methods, which often assume a single ground truth and overlook the informative signal from human label variation.", "method": "The authors propose a conceptual framework that decomposes label variation into signal (HLV) and noise, integrating HLV into active learning through instance selection, annotator choice, and label representation. They also explore the role of large language models as annotators.", "result": "By proposing this framework, the paper outlines how HLV can be systematically included in active learning processes, which can lead to improved annotation and model training.", "conclusion": "Recognizing and leveraging human label variation (HLV) in active learning can reflect real-world complexities more accurately, enhancing supervised learning outcomes."}}
{"id": "2507.02271", "pdf": "https://arxiv.org/pdf/2507.02271", "abs": "https://arxiv.org/abs/2507.02271", "authors": ["Feizhen Huang", "Yu Wu", "Yutian Lin", "Bo Du"], "title": "Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted by IJCAI 2025", "summary": "Video-to-Audio (V2A) Generation achieves significant progress and plays a\ncrucial role in film and video post-production. However, current methods\noverlook the cinematic language, a critical component of artistic expression in\nfilmmaking. As a result, their performance deteriorates in scenarios where\nFoley targets are only partially visible. To address this challenge, we propose\na simple self-distillation approach to extend V2A models to cinematic language\nscenarios. By simulating the cinematic language variations, the student model\nlearns to align the video features of training pairs with the same audio-visual\ncorrespondences, enabling it to effectively capture the associations between\nsounds and partial visual information. Our method not only achieves impressive\nimprovements under partial visibility across all evaluation metrics, but also\nenhances performance on the large-scale V2A dataset, VGGSound.", "AI": {"tldr": "The paper introduces a self-distillation approach for Video-to-Audio generation to better handle scenarios with partial visual information, improving performance on the VGGSound dataset.", "motivation": "Current Video-to-Audio generation methods struggle under scenarios where visual cues for Foley sounds are partially visible, due to their neglect of cinematic language.", "method": "The paper uses a self-distillation approach where a student model learns to align video features through simulated cinematic language variations, enhancing audio-visual correspondence under partial visibility.", "result": "The proposed method significantly improves performance across all metrics under scenarios of partial visibility, as well as on the large-scale VGGSound dataset.", "conclusion": "The method successfully addresses limitations in Video-to-Audio generation models by factoring in cinematic language, offering impactful performance boosts in practical applications."}}
{"id": "2507.02119", "pdf": "https://arxiv.org/pdf/2507.02119", "abs": "https://arxiv.org/abs/2507.02119", "authors": ["Shikai Qiu", "Lechao Xiao", "Andrew Gordon Wilson", "Jeffrey Pennington", "Atish Agarwala"], "title": "Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks", "categories": ["cs.LG"], "comment": "ICML 25. Code available at https://github.com/shikaiqiu/supercollapse", "summary": "What scaling limits govern neural network training dynamics when model size\nand training time grow in tandem? We show that despite the complex interactions\nbetween architecture, training algorithms, and data, compute-optimally trained\nmodels exhibit a remarkably precise universality. Specifically, loss curves\nfrom models of varying sizes collapse onto a single universal curve when\ntraining compute and loss are normalized to unity at the end of training. With\nlearning rate decay, the collapse becomes so tight that differences in the\nnormalized curves across models fall below the noise floor of individual loss\ncurves across random seeds, a phenomenon we term supercollapse. We observe\nsupercollapse across learning rate schedules, datasets, and architectures,\nincluding transformers trained on next-token prediction, and find it breaks\ndown when hyperparameters are scaled suboptimally, providing a precise and\npractical indicator of good scaling. We explain these phenomena by connecting\ncollapse to the power-law structure in typical neural scaling laws, and\nanalyzing a simple yet surprisingly effective model of SGD noise dynamics that\naccurately predicts loss curves across various learning rate schedules and\nquantitatively explains the origin of supercollapse.", "AI": {"tldr": "The paper introduces a universal scaling law for training neural networks, showing how training loss curves collapse into a single universal curve when training compute and loss are properly normalized.", "motivation": "To understand and characterize the universal scaling behavior of neural network training dynamics as model size and training time increase.", "method": "The authors demonstrate loss curve collapse by normalizing training compute and loss, derive insights from scaling laws, and utilize an analytical model of SGD noise dynamics to explain their observations.", "result": "They find that training loss curves from models of different sizes exhibit tight collapse (termed supercollapse) under optimal learning rate schedules, across datasets and architectures, but fail under poor hyperparameter scaling.", "conclusion": "The findings provide a practical test for hyperparameter optimization and deepen theoretical understanding of training dynamics using scaling laws and noise dynamics."}}
{"id": "2507.02582", "pdf": "https://arxiv.org/pdf/2507.02582", "abs": "https://arxiv.org/abs/2507.02582", "authors": ["Junli Jiang", "Pavel Naumov"], "title": "Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms", "categories": ["cs.AI"], "comment": null, "summary": "Responsibility has long been a subject of study in law and philosophy. More\nrecently, it became a focus of AI literature. The article investigates the\ncomputational complexity of two important properties of responsibility in\ncollective decision-making: diffusion and gap. It shows that the sets of\ndiffusion-free and gap-free decision-making mechanisms are $\\Pi_2$-complete and\n$\\Pi_3$-complete, respectively. At the same time, the intersection of these\nclasses is $\\Pi_2$-complete.", "AI": {"tldr": "This paper studies the computational complexity of responsibility properties in collective decision-making, focusing on diffusion and gap.", "motivation": "The motivation is to explore the computational aspects of responsibility, particularly its properties in the context of collective decision-making, where it has implications for AI systems.", "method": "The paper uses computational complexity theory to classify the difficulty of decision-making mechanisms, analyzing them in terms of $\nablu$-completeness for both diffusion and gap properties.", "result": "The sets of diffusion-free mechanisms are $\nabla_2$-complete, gap-free mechanisms are $\nabla_3$-complete, and their intersection is $\nabla_2$-complete.", "conclusion": "Understanding these complexity classes provides insight into how responsibility can be managed computationally in collective decision-making environments."}}
{"id": "2507.02695", "pdf": "https://arxiv.org/pdf/2507.02695", "abs": "https://arxiv.org/abs/2507.02695", "authors": ["Sahar Ahmadisakha", "Lech Bialek", "Mohamed Soliman", "Vasilios Andrikopoulos"], "title": "Sustainability Flags for the Identification of Sustainability Posts in Q&A Platforms", "categories": ["cs.SE"], "comment": null, "summary": "In recent years, sustainability in software systems has gained significant\nattention, especially with the rise of cloud computing and the shift towards\ncloud-based architectures. This shift has intensified the need to identify\nsustainability in architectural discussions to take informed architectural\ndecisions. One source to see these decisions is in online Q&A forums among\npractitioners' discussions. However, recognizing sustainability concepts within\nsoftware practitioners' discussions remains challenging due to the lack of\nclear and distinct guidelines for this task. To address this issue, we\nintroduce the notion of sustainability flags as pointers in relevant\ndiscussions, developed through thematic analysis of multiple sustainability\nbest practices from cloud providers. This study further evaluates the\neffectiveness of these flags in identifying sustainability within cloud\narchitecture posts, using a controlled experiment. Preliminary results suggest\nthat the use of flags results in classifying fewer posts as\nsustainability-related compared to a control group, with moderately higher\ncertainty and significantly improved performance. Moreover, sustainability\nflags are perceived as more useful and understandable than relying solely on\ndefinitions for identifying sustainability.", "AI": {"tldr": "The paper proposes sustainability flags derived from cloud practices to recognize sustainability discussions in software architecture posts, achieving improved performance and usability.", "motivation": "The growing emphasis on sustainability in software systems, particularly in cloud computing, necessitates methods to guide architectural decisions via sustainability identification, which currently lacks clear guidelines.", "method": "Thematic analysis of cloud provider best practices was used to develop sustainability flags; these were then evaluated through a controlled experiment analyzing cloud architecture discussion posts.", "result": "Using sustainability flags reduces the number of posts classified as sustainability-related but improves classification certainty, performance, and usability compared to traditional definitions.", "conclusion": "Sustainability flags hold promise for more effective and understandable identification of sustainability aspects in software architecture discussions compared to generic definitions."}}
{"id": "2507.02547", "pdf": "https://arxiv.org/pdf/2507.02547", "abs": "https://arxiv.org/abs/2507.02547", "authors": ["Yuhao Jiang", "Fuchen Chen", "Jamie Paik", "Daniel M. Aukes"], "title": "Vibration of Soft, Twisted Beams for Under-Actuated Quadrupedal Locomotion", "categories": ["cs.RO"], "comment": "This manuscript is under revision for possible publication in the\n  IEEE/ASME Transactions on Mechatronics. Copyright may be transferred to IEEE\n  if the manuscript is accepted for publication, without further notice.\n  Supplementary videos: https://youtu.be/T3d6FT3Rx-s,\n  https://youtu.be/nPQrhKlN02E", "summary": "Under-actuated compliant robotic systems offer a promising approach to\nmitigating actuation and control challenges by harnessing pre-designed,\nembodied dynamic behaviors. This paper presents Flix-Walker, a novel,\nuntethered, centimeter-scale quadrupedal robot inspired by compliant\nunder-actuated mechanisms. Flix-Walker employs flexible, helix-shaped beams as\nlegs, which are actuated by vibrations from just two motors to achieve three\ndistinct mobility modes. We analyze the actuation parameters required to\ngenerate various locomotion modes through both simulation and prototype\nexperiments. The effects of system and environmental variations on locomotion\nperformance are examined, and we propose a generic metric for selecting control\nparameters that produce robust and functional motions. Experiments validate the\neffectiveness and robustness of these actuation parameters within a closed-loop\ncontrol framework, demonstrating reliable trajectory-tracking and\nself-navigation capabilities.", "AI": {"tldr": "Flix-Walker, a unique compliant robot that uses helix-shaped legs and minimal motors, achieves multiple mobility modes validated by experiments.", "motivation": "To address challenges in actuation and control by leveraging dynamic behaviors in under-actuated compliant robotic systems.", "method": "Designing and testing Flix-Walker robot with flexible helix-shaped legs, actuated by vibrations from two motors, analyzing locomotion via simulation and experiments.", "result": "Flix-Walker demonstrated three mobility modes and reliable trajectory tracking and self-navigation functionalities in varied conditions.", "conclusion": "Validated actuation parameters enable robust and functional motions in under-actuated compliant robotic systems, paving the way for advanced control frameworks."}}
{"id": "2507.02732", "pdf": "https://arxiv.org/pdf/2507.02732", "abs": "https://arxiv.org/abs/2507.02732", "authors": ["Argimiro Arratia", "Mahmoud El Daou", "Henryk Gzyl"], "title": "Classification by Separating Hypersurfaces: An Entropic Approach", "categories": ["cs.LG", "cs.IT", "math.IT", "physics.data-an", "stat.ML", "90C05, 90C25, 90C47, 90C52, 68T01, 68T05, 68T07, 68T20, 68W01"], "comment": "15 pages, 10 tables, 4 figures", "summary": "We consider the following classification problem: Given a population of\nindividuals characterized by a set of attributes represented as a vector in\n${\\mathbb R}^N$, the goal is to find a hyperplane in ${\\mathbb R}^N$ that\nseparates two sets of points corresponding to two distinct classes. This\nproblem, with a history dating back to the perceptron model, remains central to\nmachine learning. In this paper we propose a novel approach by searching for a\nvector of parameters in a bounded $N$-dimensional hypercube centered at the\norigin and a positive vector in ${\\mathbb R}^M$, obtained through the\nminimization of an entropy-based function defined over the space of unknown\nvariables. The method extends to polynomial surfaces, allowing the separation\nof data points by more complex decision boundaries. This provides a robust\nalternative to traditional linear or quadratic optimization techniques, such as\nsupport vector machines and gradient descent. Numerical experiments demonstrate\nthe efficiency and versatility of the method in handling diverse classification\ntasks, including linear and non-linear separability.", "AI": {"tldr": "The paper introduces a new entropy-based method for finding classification hyperplanes in high-dimensional space, extending to polynomial decision boundaries as an alternative to traditional methods like SVMs.", "motivation": "To improve upon linear and quadratic optimization techniques for data classification, providing a more robust and versatile framework.", "method": "Developing an entropy-based optimization over bounded hypercubes in N-dimensional space, allowing for both linear and polynomial decision surfaces.", "result": "The proposed approach efficiently handles diverse classification problems, including both linear and non-linear separable datasets.", "conclusion": "The method presents a reliable alternative to existing optimization techniques with strong performance in handling a variety of classification tasks."}}
{"id": "2507.02279", "pdf": "https://arxiv.org/pdf/2507.02279", "abs": "https://arxiv.org/abs/2507.02279", "authors": ["Juntao Liu", "Liqiang Niu", "Wenchao Chen", "Jie Zhou", "Fandong Meng"], "title": "LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Existing visual token compression methods for Multimodal Large Language\nModels (MLLMs) predominantly operate as post-encoder modules, limiting their\npotential for efficiency gains. To address this limitation, we propose LaCo\n(Layer-wise Visual Token Compression), a novel framework that enables effective\ntoken compression within the intermediate layers of the vision encoder. LaCo\nintroduces two core components: 1) a layer-wise pixel-shuffle mechanism that\nsystematically merges adjacent tokens through space-to-channel transformations,\nand 2) a residual learning architecture with non-parametric shortcuts that\npreserves critical visual information during compression. Extensive experiments\nindicate that our LaCo outperforms all existing methods when compressing tokens\nin the intermediate layers of the vision encoder, demonstrating superior\neffectiveness. In addition, compared to external compression, our method\nimproves training efficiency beyond 20% and inference throughput over 15% while\nmaintaining strong performance.", "AI": {"tldr": "LaCo introduces intermediate-layer visual token compression to improve training efficiency and inference throughput while maintaining performance.", "motivation": "Current compression techniques operate post-encoding, limiting efficiency improvement potential.", "method": "LaCo uses a pixel-shuffle mechanism for token merging and a residual learning architecture with non-parametric shortcuts.", "result": "LaCo achieves better token compression, improving training efficiency by over 20% and inference throughput by 15%.", "conclusion": "LaCo effectively compresses visual tokens inside vision encoders, addressing limitations in existing methods and enhancing efficiency."}}
{"id": "2507.02128", "pdf": "https://arxiv.org/pdf/2507.02128", "abs": "https://arxiv.org/abs/2507.02128", "authors": ["Jingyu Pan", "Isaac Jacobson", "Zheng Zhao", "Tung-Chieh Chen", "Guanglei Zhou", "Chen-Chia Chang", "Vineet Rashingkar", "Yiran Chen"], "title": "CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs", "categories": ["cs.LG"], "comment": "Accepted by ICCAD 2025", "summary": "Modern very large-scale integration (VLSI) design requires the implementation\nof integrated circuits using electronic design automation (EDA) tools. Due to\nthe complexity of EDA algorithms, the vast parameter space poses a huge\nchallenge to chip design optimization, as the combination of even moderate\nnumbers of parameters creates an enormous solution space to explore. Manual\nparameter selection remains industrial practice despite being excessively\nlaborious and limited by expert experience. To address this issue, we present\nCROP, the first large language model (LLM)-powered automatic VLSI design flow\ntuning framework. Our approach includes: (1) a scalable methodology for\ntransforming RTL source code into dense vector representations, (2) an\nembedding-based retrieval system for matching designs with semantically similar\ncircuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided\nparameter search system that constrains the search process with prior knowledge\nfrom similar designs. Experiment results demonstrate CROP's ability to achieve\nsuperior quality-of-results (QoR) with fewer iterations than existing\napproaches on industrial designs, including a 9.9% reduction in power\nconsumption.", "AI": {"tldr": "CROP is an LLM-powered framework for automating VLSI design flow tuning, showcasing improved performance (e.g., 9.9% reduction in power consumption).", "motivation": "To address the challenges of optimizing chip design in EDA due to large parameter spaces and limitations of manual tuning.", "method": "CROP uses dense vector representations, embedding-based design retrieval, and an LLM-guided parameter search constrained by prior design knowledge.", "result": "Experimental results show superior quality-of-results with fewer iterations, including significant power consumption improvement.", "conclusion": "CROP demonstrates the capability of LLMs in optimizing VLSI designs efficiently, surpassing traditional manual and automated methods."}}
{"id": "2507.02616", "pdf": "https://arxiv.org/pdf/2507.02616", "abs": "https://arxiv.org/abs/2507.02616", "authors": ["Tianqi Shang", "Weiqing He", "Charles Zheng", "Lingyao Li", "Li Shen", "Bingxin Zhao"], "title": "DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making", "categories": ["cs.AI"], "comment": "16 pages", "summary": "The rise of Large Language Models (LLMs) has enabled the development of\nspecialized AI agents with domain-specific reasoning and interaction\ncapabilities, particularly in healthcare. While recent frameworks simulate\nmedical decision-making, they largely focus on single-turn tasks where a doctor\nagent receives full case information upfront -- diverging from the real-world\ndiagnostic process, which is inherently uncertain, interactive, and iterative.\nIn this paper, we introduce MIMIC-Patient, a structured dataset built from the\nMIMIC-III electronic health records (EHRs), designed to support dynamic,\npatient-level simulations. Building on this, we propose DynamiCare, a novel\ndynamic multi-agent framework that models clinical diagnosis as a multi-round,\ninteractive loop, where a team of specialist agents iteratively queries the\npatient system, integrates new information, and dynamically adapts its\ncomposition and strategy. We demonstrate the feasibility and effectiveness of\nDynamiCare through extensive experiments, establishing the first benchmark for\ndynamic clinical decision-making with LLM-powered agents.", "AI": {"tldr": "This paper introduces MIMIC-Patient, a dataset for dynamic healthcare simulations, and proposes DynamiCare, a multi-agent framework for iterative clinical diagnosis, powered by Large Language Models.", "motivation": "The authors aim to address the gap between real-world diagnostic processes and the limitations of single-turn medical decision-making simulations by leveraging dynamic interactions.", "method": "The paper develops MIMIC-Patient from MIMIC-III EHRs for patient-level simulations and designs DynamiCare, a framework that uses multiple specialist agents to query, integrate, and adapt dynamically during the diagnostic process.", "result": "Experiments highlight the feasibility and effectiveness of DynamiCare, establishing a benchmark for dynamic clinical decision-making using LLM-based agents.", "conclusion": "DynamiCare provides a realistic and efficient framework to simulate dynamic clinical diagnosis, advancing the utility of AI-powered agents in healthcare diagnostics."}}
{"id": "2507.02595", "pdf": "https://arxiv.org/pdf/2507.02595", "abs": "https://arxiv.org/abs/2507.02595", "authors": ["Xin Guan", "PeiHsin Lin", "Zekun Wu", "Ze Wang", "Ruibo Zhang", "Emre Kazim", "Adriano Koshiyama"], "title": "MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICML 2025 AIW Workshop", "summary": "Multiperspective Fusion (MPF) is a novel posttraining alignment framework for\nlarge language models (LLMs) developed in response to the growing need for easy\nbias mitigation. Built on top of the SAGED pipeline, an automated system for\nconstructing bias benchmarks and extracting interpretable baseline\ndistributions, MPF leverages multiperspective generations to expose and align\nbiases in LLM outputs with nuanced, humanlike baselines. By decomposing\nbaseline, such as sentiment distributions from HR professionals, into\ninterpretable perspective components, MPF guides generation through sampling\nand balancing of responses, weighted by the probabilities obtained in the\ndecomposition. Empirically, we demonstrate its ability to align LLM sentiment\ndistributions with both counterfactual baselines (absolute equality) and the HR\nbaseline (biased for Top Univeristy), resulting in small KL divergence,\nreduction of calibration error and generalization to unseen questions. This\nshows that MPF offers a scalable and interpretable method for alignment and\nbias mitigation, compatible with deployed LLMs and requiring no extensive\nprompt engineering or finetuning.", "AI": {"tldr": "MPF is a new method for minimizing biases in large language models (LLMs) using alternative perspective baselines, offering interpretable and scalable alignment.", "motivation": "The increasing demand for mitigating bias in LLMs inspired the development of a framework that is simple to deploy and effectively aligns biases.", "method": "MPF builds on the SAGED pipeline, employing multiperspective data to decompose bias benchmarks into interpretable parts and guiding generative sampling towards balanced responses.", "result": "MPF effectively aligns sentiment distributions with selected baselines, minimizes KL divergence, reduces calibration errors, and generalizes across unseen queries.", "conclusion": "MPF is an accessible, scalable, and effective method for bias alignment, requiring no extensive retraining or prompt engineering."}}
{"id": "2507.02846", "pdf": "https://arxiv.org/pdf/2507.02846", "abs": "https://arxiv.org/abs/2507.02846", "authors": ["Anmol Singhal", "Travis Breaux"], "title": "Legal Requirements Translation from Law", "categories": ["cs.SE", "cs.CL"], "comment": "13 pages, 7 figures, Accepted at the 33rd IEEE International\n  Requirements Engineering 2025", "summary": "Software systems must comply with legal regulations, which is a\nresource-intensive task, particularly for small organizations and startups\nlacking dedicated legal expertise. Extracting metadata from regulations to\nelicit legal requirements for software is a critical step to ensure compliance.\nHowever, it is a cumbersome task due to the length and complex nature of legal\ntext. Although prior work has pursued automated methods for extracting\nstructural and semantic metadata from legal text, key limitations remain: they\ndo not consider the interplay and interrelationships among attributes\nassociated with these metadata types, and they rely on manual labeling or\nheuristic-driven machine learning, which does not generalize well to new\ndocuments. In this paper, we introduce an approach based on textual entailment\nand in-context learning for automatically generating a canonical representation\nof legal text, encodable and executable as Python code. Our representation is\ninstantiated from a manually designed Python class structure that serves as a\ndomain-specific metamodel, capturing both structural and semantic legal\nmetadata and their interrelationships. This design choice reduces the need for\nlarge, manually labeled datasets and enhances applicability to unseen\nlegislation. We evaluate our approach on 13 U.S. state data breach notification\nlaws, demonstrating that our generated representations pass approximately 89.4%\nof test cases and achieve a precision and recall of 82.2 and 88.7,\nrespectively.", "AI": {"tldr": "The paper proposes an automated method to process legal text for software compliance using a Python-based representation, achieving notable accuracy and avoiding heavy reliance on manual data labeling.", "motivation": "Extracting legal requirements from regulations is challenging for small organizations due to the complexity of legal text and a lack of dedicated legal expertise.", "method": "The researchers use textual entailment and in-context learning to transform legal texts into a Python-executable canonical representation based on a designed Python class structure as a domain-specific metamodel.", "result": "Their method achieved approximately 89.4% correctness in test cases and a precision and recall of 82.2% and 88.7%, respectively, when tested on 13 U.S. state data breach notification laws.", "conclusion": "The approach minimizes the need for manual dataset labeling while providing an accurate and efficient way to process structural and semantic metadata from legal texts, ensuring better compliance for software systems."}}
{"id": "2507.02600", "pdf": "https://arxiv.org/pdf/2507.02600", "abs": "https://arxiv.org/abs/2507.02600", "authors": ["Qiaojun Yu", "Xibin Yuan", "Yu jiang", "Junting Chen", "Dongzhe Zheng", "Ce Hao", "Yang You", "Yixing Chen", "Yao Mu", "Liu Liu", "Cewu Lu"], "title": "ArtGS:3D Gaussian Splatting for Interactive Visual-Physical Modeling and Manipulation of Articulated Objects", "categories": ["cs.RO"], "comment": "Accepted by IROS 2025", "summary": "Articulated object manipulation remains a critical challenge in robotics due\nto the complex kinematic constraints and the limited physical reasoning of\nexisting methods. In this work, we introduce ArtGS, a novel framework that\nextends 3D Gaussian Splatting (3DGS) by integrating visual-physical modeling\nfor articulated object understanding and interaction. ArtGS begins with\nmulti-view RGB-D reconstruction, followed by reasoning with a vision-language\nmodel (VLM) to extract semantic and structural information, particularly the\narticulated bones. Through dynamic, differentiable 3DGS-based rendering, ArtGS\noptimizes the parameters of the articulated bones, ensuring physically\nconsistent motion constraints and enhancing the manipulation policy. By\nleveraging dynamic Gaussian splatting, cross-embodiment adaptability, and\nclosed-loop optimization, ArtGS establishes a new framework for efficient,\nscalable, and generalizable articulated object modeling and manipulation.\nExperiments conducted in both simulation and real-world environments\ndemonstrate that ArtGS significantly outperforms previous methods in joint\nestimation accuracy and manipulation success rates across a variety of\narticulated objects. Additional images and videos are available on the project\nwebsite: https://sites.google.com/view/artgs/home", "AI": {"tldr": "ArtGS is a new framework for robotic manipulation of articulated objects, combining 3D Gaussian Splatting with vision-language model reasoning to achieve improved joint estimation accuracy and manipulation success rates.", "motivation": "The motivation is to overcome challenges in articulated object manipulation by addressing complex kinematic constraints and the limitations in physical reasoning of current methods.", "method": "The method involves multi-view RGB-D reconstruction, vision-language model reasoning to identify articulated structures, and dynamic 3D Gaussian Splatting for closed-loop optimization and physically consistent modeling.", "result": "ArtGS demonstrates superior performance in joint estimation accuracy and manipulation success rates in both simulated and real-world scenarios compared to prior approaches.", "conclusion": "ArtGS offers an efficient, scalable, and adaptable solution for articulated object modeling and manipulation, marking progress in robotic physical interaction."}}
{"id": "2507.02762", "pdf": "https://arxiv.org/pdf/2507.02762", "abs": "https://arxiv.org/abs/2507.02762", "authors": ["Yixuan Zhang", "Ruihao Zhu", "Qiaomin Xie"], "title": "Contextual Online Pricing with (Biased) Offline Data", "categories": ["cs.LG", "stat.ML"], "comment": "47 pages, 4 figures", "summary": "We study contextual online pricing with biased offline data. For the scalar\nprice elasticity case, we identify the instance-dependent quantity $\\delta^2$\nthat measures how far the offline data lies from the (unknown) online optimum.\nWe show that the time length $T$, bias bound $V$, size $N$ and dispersion\n$\\lambda_{\\min}(\\hat{\\Sigma})$ of the offline data, and $\\delta^2$ jointly\ndetermine the statistical complexity. An Optimism-in-the-Face-of-Uncertainty\n(OFU) policy achieves a minimax-optimal, instance-dependent regret bound\n$\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T +\n\\frac{dT}{\\lambda_{\\min}(\\hat{\\Sigma}) + (N \\wedge T) \\delta^2})\\big)$. For\ngeneral price elasticity, we establish a worst-case, minimax-optimal rate\n$\\tilde{\\mathcal{O}}\\big(d\\sqrt{T} \\wedge (V^2T + \\frac{dT\n}{\\lambda_{\\min}(\\hat{\\Sigma})})\\big)$ and provide a generalized OFU algorithm\nthat attains it. When the bias bound $V$ is unknown, we design a robust variant\nthat always guarantees sub-linear regret and strictly improves on purely online\nmethods whenever the exact bias is small. These results deliver the first tight\nregret guarantees for contextual pricing in the presence of biased offline\ndata. Our techniques also transfer verbatim to stochastic linear bandits with\nbiased offline data, yielding analogous bounds.", "AI": {"tldr": "The paper addresses online pricing using biased offline data and provides statistical complexity and regret bounds based on key parameters. Introducing Optimism-in-the-Face-of-Uncertainty (OFU) policies achieves minimax-optimal regret.", "motivation": "To address the challenge of contextual online pricing utilizing biased offline data and design strategies achieving optimal performance while handling this bias.", "method": "Optimism-in-the-Face-of-Uncertainty (OFU) policies were designed and analyzed to achieve tight regret bounds in pricing models. Extensions to robust algorithms capable of handling unknown bias bounds are also constructed.", "result": "The paper provides tight regret guarantees for contextual pricing in biased offline data settings and demonstrates similar results for stochastic linear bandits with biased data.", "conclusion": "Optimism-in-the-Face-of-Uncertainty (OFU) policies and its robust variants deliver optimal strategies for contextual pricing, overcoming biases in offline data effectively."}}
{"id": "2507.02288", "pdf": "https://arxiv.org/pdf/2507.02288", "abs": "https://arxiv.org/abs/2507.02288", "authors": ["De Cheng", "Zhipeng Xu", "Xinyang Jiang", "Dongsheng Li", "Nannan Wang", "Xinbo Gao"], "title": "Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Domain Generalization (DG) seeks to develop a versatile model capable of\nperforming effectively on unseen target domains. Notably, recent advances in\npre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated\nconsiderable potential in enhancing the generalization capabilities of deep\nlearning models. Despite the increasing attention toward VFM-based domain\nprompt tuning within DG, the effective design of prompts capable of\ndisentangling invariant features across diverse domains remains a critical\nchallenge. In this paper, we propose addressing this challenge by leveraging\nthe controllable and flexible language prompt of the VFM. Noting that the text\nmodality of VFMs is naturally easier to disentangle, we introduce a novel\nframework for text feature-guided visual prompt tuning. This framework first\nautomatically disentangles the text prompt using a large language model (LLM)\nand then learns domain-invariant visual representation guided by the\ndisentangled text feature. However, relying solely on language to guide visual\nfeature disentanglement has limitations, as visual features can sometimes be\ntoo complex or nuanced to be fully captured by descriptive text. To address\nthis, we introduce Worst Explicit Representation Alignment (WERA), which\nextends text-guided visual prompts by incorporating an additional set of\nabstract prompts. These prompts enhance source domain diversity through\nstylized image augmentations, while alignment constraints ensure that visual\nrepresentations remain consistent across both the original and augmented\ndistributions. Experiments conducted on major DG datasets, including PACS,\nVLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method\noutperforms state-of-the-art DG methods.", "AI": {"tldr": "The paper introduces a method for domain generalization by leveraging pre-trained Visual Foundation Models' text prompts to guide visual prompt tuning, incorporating advanced techniques for disentangling and aligning domain-invariant representations.", "motivation": "The authors aim to address the challenge of disentangling invariant features across diverse domains in domain generalization tasks, especially leveraging the strengths of existing pre-trained Visual Foundation Models like CLIP.", "method": "They propose using text feature-guided visual prompt tuning where text prompts are disentangled using a large language model, and introduce Worst Explicit Representation Alignment (WERA) to improve visual representation alignment by using augmented prompts and ensuring consistency.", "result": "Experimental results on major DG datasets (PACS, VLCS, OfficeHome, DomainNet, and TerraInc) show that the proposed method outperforms state-of-the-art domain generalization approaches.", "conclusion": "The proposed framework effectively enhances domain generalization capabilities by harmonizing textual and visual features and addressing the limitations of relying solely on language for feature disentanglement."}}
{"id": "2507.02129", "pdf": "https://arxiv.org/pdf/2507.02129", "abs": "https://arxiv.org/abs/2507.02129", "authors": ["Xiao Li", "Liangji Zhu", "Anand Rangarajan", "Sanjay Ranka"], "title": "Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction", "categories": ["cs.LG", "cs.CV"], "comment": "10 pages", "summary": "Generative models have demonstrated strong performance in conditional\nsettings and can be viewed as a form of data compression, where the condition\nserves as a compact representation. However, their limited controllability and\nreconstruction accuracy restrict their practical application to data\ncompression. In this work, we propose an efficient latent diffusion framework\nthat bridges this gap by combining a variational autoencoder with a conditional\ndiffusion model. Our method compresses only a small number of keyframes into\nlatent space and uses them as conditioning inputs to reconstruct the remaining\nframes via generative interpolation, eliminating the need to store latent\nrepresentations for every frame. This approach enables accurate spatiotemporal\nreconstruction while significantly reducing storage costs. Experimental results\nacross multiple datasets show that our method achieves up to 10 times higher\ncompression ratios than rule-based state-of-the-art compressors such as SZ3,\nand up to 63 percent better performance than leading learning-based methods\nunder the same reconstruction error.", "AI": {"tldr": "The paper introduces a latent diffusion framework combining a variational autoencoder with a conditional diffusion model for spatiotemporal data compression through generative interpolation. It achieves high compression ratios and superior performance compared to existing methods.", "motivation": "The motivation is to address limitations in generative models related to controllability and reconstruction accuracy, particularly for data compression applications, by creating a more efficient framework.", "method": "The method involves using variational autoencoder combined with conditional diffusion models to compress keyframes into latent space and reconstruct other frames using generative interpolation, reducing storage needs.", "result": "The framework achieves up to 10x higher compression ratios compared to rule-based methods (like SZ3) and up to 63% better performance compared to learning-based methods, under the same reconstruction error.", "conclusion": "The proposed approach effectively balances reconstructive accuracy and storage efficiency, making it a promising solution for spatiotemporal data compression."}}
{"id": "2507.02618", "pdf": "https://arxiv.org/pdf/2507.02618", "abs": "https://arxiv.org/abs/2507.02618", "authors": ["Kenneth Payne", "Baptiste Alloui-Cros"], "title": "Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory", "categories": ["cs.AI", "cs.CL", "cs.GT"], "comment": "29 pages, 27 tables, 4 figures", "summary": "Are Large Language Models (LLMs) a new form of strategic intelligence, able\nto reason about goals in competitive settings? We present compelling supporting\nevidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for\nstudying decision-making. We conduct the first ever series of evolutionary IPD\ntournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)\nagainst agents from the leading frontier AI companies OpenAI, Google, and\nAnthropic. By varying the termination probability in each tournament (the\n\"shadow of the future\"), we introduce complexity and chance, confounding\nmemorisation.\n  Our results show that LLMs are highly competitive, consistently surviving and\nsometimes even proliferating in these complex ecosystems. Furthermore, they\nexhibit distinctive and persistent \"strategic fingerprints\": Google's Gemini\nmodels proved strategically ruthless, exploiting cooperative opponents and\nretaliating against defectors, while OpenAI's models remained highly\ncooperative, a trait that proved catastrophic in hostile environments.\nAnthropic's Claude emerged as the most forgiving reciprocator, showing\nremarkable willingness to restore cooperation even after being exploited or\nsuccessfully defecting. Analysis of nearly 32,000 prose rationales provided by\nthe models reveals that they actively reason about both the time horizon and\ntheir opponent's likely strategy, and we demonstrate that this reasoning is\ninstrumental to their decisions. This work connects classic game theory with\nmachine psychology, offering a rich and granular view of algorithmic\ndecision-making under uncertainty.", "AI": {"tldr": "This paper assesses the strategic intelligence of LLMs through evolutionary Iterated Prisoner's Dilemma tournaments, finding distinctive decision-making strategies for major AI models.", "motivation": "To evaluate whether LLMs possess strategic intelligence capable of reasoning about goals in competitive and uncertain environments.", "method": "The authors conducted evolutionary Iterated Prisoner's Dilemma tournaments with varied termination probabilities, involving LLM agents from OpenAI, Google, and Anthropic competing against canonical strategies.", "result": "LLMs showed competitiveness, surviving and sometimes proliferating, while exhibiting unique strategic behaviors: Gemini models were ruthless, OpenAI's models were highly cooperative (but disadvantageous in hostility), and Claude leaned toward forgiving reciprocity.", "conclusion": "LLMs demonstrate the ability to adaptively reason about decision-making with a granular grasp of competitive strategy, bridging game theory and machine psychology."}}
{"id": "2507.02679", "pdf": "https://arxiv.org/pdf/2507.02679", "abs": "https://arxiv.org/abs/2507.02679", "authors": ["Ahmed Sabir", "Rajesh Sharama"], "title": "Exploring Gender Bias Beyond Occupational Titles", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "In this work, we investigate the correlation between gender and contextual\nbiases, focusing on elements such as action verbs, object nouns, and\nparticularly on occupations. We introduce a novel dataset, GenderLexicon, and a\nframework that can estimate contextual bias and its related gender bias. Our\nmodel can interpret the bias with a score and thus improve the explainability\nof gender bias. Also, our findings confirm the existence of gender biases\nbeyond occupational stereotypes. To validate our approach and demonstrate its\neffectiveness, we conduct evaluations on five diverse datasets, including a\nJapanese dataset.", "AI": {"tldr": "The paper explores gender and contextual biases, particularly in occupations, introducing a dataset and framework to quantify and explain biases.", "motivation": "To investigate and quantify gender biases in contextual elements, improving explainability and addressing stereotypes.", "method": "The researchers developed GenderLexicon and a framework to estimate contextual and gender biases, validating it across five datasets.", "result": "The framework effectively identified gender biases beyond occupational stereotypes across diverse datasets.", "conclusion": "Gender biases extend beyond occupations, and the proposed approach enhances understanding through quantifiable bias representation."}}
{"id": "2507.02858", "pdf": "https://arxiv.org/pdf/2507.02858", "abs": "https://arxiv.org/abs/2507.02858", "authors": ["Yuchen Shen", "Anmol Singhal", "Travis Breaux"], "title": "Requirements Elicitation Follow-Up Question Generation", "categories": ["cs.SE", "cs.CL"], "comment": "13 pages, 2 figures, accepted at the 33rd IEEE International\n  Requirements Engineering 2025", "summary": "Interviews are a widely used technique in eliciting requirements to gather\nstakeholder needs, preferences, and expectations for a software system.\nEffective interviewing requires skilled interviewers to formulate appropriate\ninterview questions in real time while facing multiple challenges, including\nlack of familiarity with the domain, excessive cognitive load, and information\noverload that hinders how humans process stakeholders' speech. Recently, large\nlanguage models (LLMs) have exhibited state-of-the-art performance in multiple\nnatural language processing tasks, including text summarization and entailment.\nTo support interviewers, we investigate the application of GPT-4o to generate\nfollow-up interview questions during requirements elicitation by building on a\nframework of common interviewer mistake types. In addition, we describe methods\nto generate questions based on interviewee speech. We report a controlled\nexperiment to evaluate LLM-generated and human-authored questions with minimal\nguidance, and a second controlled experiment to evaluate the LLM-generated\nquestions when generation is guided by interviewer mistake types. Our findings\ndemonstrate that, for both experiments, the LLM-generated questions are no\nworse than the human-authored questions with respect to clarity, relevancy, and\ninformativeness. In addition, LLM-generated questions outperform human-authored\nquestions when guided by common mistakes types. This highlights the potential\nof using LLMs to help interviewers improve the quality and ease of requirements\nelicitation interviews in real time.", "AI": {"tldr": "The paper investigates using GPT-4 in generating follow-up interview questions for requirements elicitation, showing that GPT-4-generated questions rival, and sometimes exceed, human-generated questions when guided by common mistake types.", "motivation": "The motivation is to address challenges in requirements elicitation interviews, such as domain unfamiliarity, cognitive load, and information processing, by leveraging advancements in large language models (LLMs) like GPT-4.", "method": "The researchers applied GPT-4 to generate follow-up interview questions based on a framework for common interviewer mistake types, compared its outputs to human-generated questions in controlled experiments, and evaluated them on clarity, relevancy, and informativeness.", "result": "LLM-generated questions performed comparably to human-authored questions in unguided scenarios, and surpassed human-authored questions when guided by interviewer mistake types.", "conclusion": "LLMs, specifically GPT-4, have promising potential to enhance the quality and efficiency of requirements elicitation interviews by assisting interviewers with real-time question formulation."}}
{"id": "2507.02672", "pdf": "https://arxiv.org/pdf/2507.02672", "abs": "https://arxiv.org/abs/2507.02672", "authors": ["Qingyu Fan", "Yinghao Cai", "Chao Li", "Chunting Jiao", "Xudong Zheng", "Tao Lu", "Bin Liang", "Shuo Wang"], "title": "MISCGrasp: Leveraging Multiple Integrated Scales and Contrastive Learning for Enhanced Volumetric Grasping", "categories": ["cs.RO", "cs.CV"], "comment": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS), 2025", "summary": "Robotic grasping faces challenges in adapting to objects with varying shapes\nand sizes. In this paper, we introduce MISCGrasp, a volumetric grasping method\nthat integrates multi-scale feature extraction with contrastive feature\nenhancement for self-adaptive grasping. We propose a query-based interaction\nbetween high-level and low-level features through the Insight Transformer,\nwhile the Empower Transformer selectively attends to the highest-level\nfeatures, which synergistically strikes a balance between focusing on fine\ngeometric details and overall geometric structures. Furthermore, MISCGrasp\nutilizes multi-scale contrastive learning to exploit similarities among\npositive grasp samples, ensuring consistency across multi-scale features.\nExtensive experiments in both simulated and real-world environments demonstrate\nthat MISCGrasp outperforms baseline and variant methods in tabletop\ndecluttering tasks. More details are available at https://miscgrasp.github.io/.", "AI": {"tldr": "MISCGrasp introduces a volumetric grasping method combining multi-scale feature extraction and contrastive learning to improve self-adaptive grasping for diverse objects. It surpasses existing approaches in tabletop decluttering tasks.", "motivation": "The paper aims to overcome the challenges robotic grasping faces when dealing with objects of varying shapes and sizes.", "method": "The authors present a method that employs the Insight Transformer for query-based high and low-level feature interaction, and the Empower Transformer for focused attention on high-level features. Multi-scale contrastive learning is used to ensure feature consistency.", "result": "MISCGrasp demonstrated superior performance compared to baseline and variant methods in simulated and real-world tabletop decluttering experiments.", "conclusion": "The results confirm that the proposed MISCGrasp approach enhances robotic grasping capabilities and adaptability by effectively combining multi-scale feature analysis and contrastive learning techniques."}}
{"id": "2507.02294", "pdf": "https://arxiv.org/pdf/2507.02294", "abs": "https://arxiv.org/abs/2507.02294", "authors": ["Hanbo Bi", "Yulong Xu", "Ya Li", "Yongqiang Mao", "Boyuan Tong", "Chongyang Li", "Chunbo Lang", "Wenhui Diao", "Hongqi Wang", "Yingchao Feng", "Xian Sun"], "title": "ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits\nstrong generalization in generic segmentation tasks. However, applying SAM to\nremote sensing (RS) images still faces two major challenges. First, manually\nconstructing precise prompts for each image (e.g., points or boxes) is\nlabor-intensive and inefficient, especially in RS scenarios with dense small\nobjects or spatially fragmented distributions. Second, SAM lacks domain\nadaptability, as it is pre-trained primarily on natural images and struggles to\ncapture RS-specific semantics and spatial characteristics, especially when\nsegmenting novel or unseen classes. To address these issues, inspired by\nfew-shot learning, we propose ViRefSAM, a novel framework that guides SAM\nutilizing only a few annotated reference images that contain class-specific\nobjects. Without requiring manual prompts, ViRefSAM enables automatic\nsegmentation of class-consistent objects across RS images. Specifically,\nViRefSAM introduces two key components while keeping SAM's original\narchitecture intact: (1) a Visual Contextual Prompt Encoder that extracts\nclass-specific semantic clues from reference images and generates object-aware\nprompts via contextual interaction with target images; and (2) a Dynamic Target\nAlignment Adapter, integrated into SAM's image encoder, which mitigates the\ndomain gap by injecting class-specific semantics into target image features,\nenabling SAM to dynamically focus on task-relevant regions. Extensive\nexperiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,\nLoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and\nautomatic segmentation of unseen classes by leveraging only a few reference\nimages and consistently outperforms existing few-shot segmentation methods\nacross diverse datasets.", "AI": {"tldr": "ViRefSAM enhances SAM, a general segmentation model, to effectively work with remote sensing (RS) images by using few-shot learning without manual prompts.", "motivation": "Remote sensing image segmentation faces challenges like labor-intensive manual prompts and lack of adaptability to domain-specific semantics due to SAM's focus on natural images.", "method": "ViRefSAM incorporates a Visual Contextual Prompt Encoder for extracting and generating class-specific prompts and a Dynamic Target Alignment Adapter to adapt SAM to RS domain characteristics.", "result": "ViRefSAM achieves accurate segmentation of unseen RS classes using minimal reference images and outperforms other few-shot segmentation methods in diverse benchmarks.", "conclusion": "ViRefSAM successfully adapts SAM for RS segmentation by leveraging class-specific references without manual prompts, showcasing its efficacy and practical advantages."}}
{"id": "2507.02151", "pdf": "https://arxiv.org/pdf/2507.02151", "abs": "https://arxiv.org/abs/2507.02151", "authors": ["Tuo Wang", "Jian Kang", "Yujun Yan", "Adithya Kulkarni", "Dawei Zhou"], "title": "Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks", "categories": ["cs.LG", "H.1.0; I.2.0"], "comment": "accepted by KDD 2025", "summary": "Conformal prediction for graph neural networks (GNNs) offers a promising\nframework for quantifying uncertainty, enhancing GNN reliability in high-stakes\napplications. However, existing methods predominantly focus on static graphs,\nneglecting the evolving nature of real-world graphs. Temporal dependencies in\ngraph structure, node attributes, and ground truth labels violate the\nfundamental exchangeability assumption of standard conformal prediction\nmethods, limiting their applicability. To address these challenges, in this\npaper, we introduce NCPNET, a novel end-to-end conformal prediction framework\ntailored for temporal graphs. Our approach extends conformal prediction to\ndynamic settings, mitigating statistical coverage violations induced by\ntemporal dependencies. To achieve this, we propose a diffusion-based\nnon-conformity score that captures both topological and temporal uncertainties\nwithin evolving networks. Additionally, we develop an efficiency-aware\noptimization algorithm that improves the conformal prediction process,\nenhancing computational efficiency and reducing coverage violations. Extensive\nexperiments on diverse real-world temporal graphs, including WIKI, REDDIT,\nDBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to\nensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction\nin prediction set size on the WIKI dataset, significantly improving efficiency\ncompared to state-of-the-art methods. Our data and code are available at\nhttps://github.com/ODYSSEYWT/NCPNET.", "AI": {"tldr": "The paper proposes NCPNET, a framework for applying conformal prediction to temporal graphs, addressing issues of statistical coverage and efficiency.", "motivation": "Current conformal prediction methods fail to work well on temporal graphs due to the violation of exchangeability assumptions caused by dynamic graph properties.", "method": "NCPNET introduces a diffusion-based non-conformity score and an efficiency-aware optimization algorithm tailored to capture topological and temporal uncertainties in evolving graphs.", "result": "NCPNET ensures reliable coverage on temporal graphs, reducing prediction set size by up to 31% on the WIKI dataset and outperforming existing methods in efficiency.", "conclusion": "This work presents a significant advancement in conformal prediction for temporal graphs, improving both uncertainty quantification and computational efficiency for high-stakes applications."}}
{"id": "2507.02652", "pdf": "https://arxiv.org/pdf/2507.02652", "abs": "https://arxiv.org/abs/2507.02652", "authors": ["Jiajie Jin", "Xiaoxi Li", "Guanting Dong", "Yuyao Zhang", "Yutao Zhu", "Yang Zhao", "Hongjin Qian", "Zhicheng Dou"], "title": "Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": "9 pages", "summary": "Complex information needs in real-world search scenarios demand deep\nreasoning and knowledge synthesis across diverse sources, which traditional\nretrieval-augmented generation (RAG) pipelines struggle to address effectively.\nCurrent reasoning-based approaches suffer from a fundamental limitation: they\nuse a single model to handle both high-level planning and detailed execution,\nleading to inefficient reasoning and limited scalability. In this paper, we\nintroduce HiRA, a hierarchical framework that separates strategic planning from\nspecialized execution. Our approach decomposes complex search tasks into\nfocused subtasks, assigns each subtask to domain-specific agents equipped with\nexternal tools and reasoning capabilities, and coordinates the results through\na structured integration mechanism. This separation prevents execution details\nfrom disrupting high-level reasoning while enabling the system to leverage\nspecialized expertise for different types of information processing.\nExperiments on four complex, cross-modal deep search benchmarks demonstrate\nthat HiRA significantly outperforms state-of-the-art RAG and agent-based\nsystems. Our results show improvements in both answer quality and system\nefficiency, highlighting the effectiveness of decoupled planning and execution\nfor multi-step information seeking tasks. Our code is available at\nhttps://github.com/ignorejjj/HiRA.", "AI": {"tldr": "HiRA is a novel hierarchical framework for complex search, outperforming existing RAG systems by separating high-level planning from detailed execution.", "motivation": "To overcome inefficiencies and scalability issues in retrieval-augmented generation pipelines when addressing real-world complex search scenarios.", "method": "HiRA decomposes tasks into subtasks assigned to domain-specific agents with external tools, leveraging structured integration for better execution and planning decoupling.", "result": "HiRA achieves superior performance over state-of-the-art RAG and agent-based systems for complex search benchmarks, improving answer quality and efficiency.", "conclusion": "Decoupling strategic planning and specialized execution is key for effective and efficient multi-step information retrieval in complex search tasks."}}
{"id": "2507.02694", "pdf": "https://arxiv.org/pdf/2507.02694", "abs": "https://arxiv.org/abs/2507.02694", "authors": ["Zhijian Xu", "Yilun Zhao", "Manasi Patwardhan", "Lovekesh Vig", "Arman Cohan"], "title": "Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers", "categories": ["cs.CL"], "comment": null, "summary": "Peer review is fundamental to scientific research, but the growing volume of\npublications has intensified the challenges of this expertise-intensive\nprocess. While LLMs show promise in various scientific tasks, their potential\nto assist with peer review, particularly in identifying paper limitations,\nremains understudied. We first present a comprehensive taxonomy of limitation\ntypes in scientific research, with a focus on AI. Guided by this taxonomy, for\nstudying limitations, we present LimitGen, the first comprehensive benchmark\nfor evaluating LLMs' capability to support early-stage feedback and complement\nhuman peer review. Our benchmark consists of two subsets: LimitGen-Syn, a\nsynthetic dataset carefully created through controlled perturbations of\nhigh-quality papers, and LimitGen-Human, a collection of real human-written\nlimitations. To improve the ability of LLM systems to identify limitations, we\naugment them with literature retrieval, which is essential for grounding\nidentifying limitations in prior scientific findings. Our approach enhances the\ncapabilities of LLM systems to generate limitations in research papers,\nenabling them to provide more concrete and constructive feedback.", "AI": {"tldr": "The paper explores using large language models (LLMs) to assist in identifying limitations in scientific research, introducing a benchmark called LimitGen and proposing literature retrieval to enhance LLM performance.", "motivation": "The growing volume of scientific publications presents challenges for traditional peer review processes, especially in identifying limitations. The paper seeks to evaluate the potential of LLMs to improve this aspect of peer review.", "method": "The authors created a taxonomy of limitations in scientific research and developed LimitGen, a benchmark with synthetic and human-written datasets. They augmented LLMs with literature retrieval to ground their assessments using prior research.", "result": "The approach improved LLM capability for analyzing research paper limitations, showing potential for providing detailed, constructive feedback.", "conclusion": "LLMs augmented with literature retrieval can better assist researchers in identifying limitations, complementing traditional peer review processes."}}
{"id": "2507.02607", "pdf": "https://arxiv.org/pdf/2507.02607", "abs": "https://arxiv.org/abs/2507.02607", "authors": ["Frida Sundfeldt", "Bianca Widstam", "Mahshid Helali Moghadam", "Kuo-Yun Liang", "Anders Vesterberg"], "title": "Alleviating Attack Data Scarcity: SCANIA's Experience Towards Enhancing In-Vehicle Cyber Security Measures", "categories": ["cs.CR", "cs.LG", "cs.SE"], "comment": null, "summary": "The digital evolution of connected vehicles and the subsequent security risks\nemphasize the critical need for implementing in-vehicle cyber security measures\nsuch as intrusion detection and response systems. The continuous advancement of\nattack scenarios further highlights the need for adaptive detection mechanisms\nthat can detect evolving, unknown, and complex threats. The effective use of\nML-driven techniques can help address this challenge. However, constraints on\nimplementing diverse attack scenarios on test vehicles due to safety, cost, and\nethical considerations result in a scarcity of data representing attack\nscenarios. This limitation necessitates alternative efficient and effective\nmethods for generating high-quality attack-representing data. This paper\npresents a context-aware attack data generator that generates attack inputs and\ncorresponding in-vehicle network log, i.e., controller area network (CAN) log,\nrepresenting various types of attack including denial of service (DoS), fuzzy,\nspoofing, suspension, and replay attacks. It utilizes parameterized attack\nmodels augmented with CAN message decoding and attack intensity adjustments to\nconfigure the attack scenarios with high similarity to real-world scenarios and\npromote variability. We evaluate the practicality of the generated\nattack-representing data within an intrusion detection system (IDS) case study,\nin which we develop and perform an empirical evaluation of two deep neural\nnetwork IDS models using the generated data. In addition to the efficiency and\nscalability of the approach, the performance results of IDS models, high\ndetection and classification capabilities, validate the consistency and\neffectiveness of the generated data as well. In this experience study, we also\nelaborate on the aspects influencing the fidelity of the data to real-world\nscenarios and provide insights into its application.", "AI": {"tldr": "The paper introduces a context-aware attack data generator that creates simulated attack scenarios and corresponding in-vehicle network logs to address limitations in testing security on connected vehicles. It also evaluates the effectiveness of using this generated data for training intrusion detection systems.", "motivation": "The increasing digitization of connected vehicles and associated security risks require effective cybersecurity measures capable of addressing unknown and evolving threats. Limited real-world data on attack scenarios creates the need for efficient methods to simulate these conditions.", "method": "The paper proposes using a parameterized attack model combined with CAN message decoding and adjustable attack intensity to simulate diverse, realistic vehicle attack scenarios. The generated data is tested via deep neural network models within an intrusion detection system (IDS).", "result": "The generated attack data was successfully validated against intrusion detection systems, where models showed high detection and classification capabilities. The approach proved efficient, scalable, and consistent in simulating realistic threats.", "conclusion": "The data generation method effectively addresses the scarcity of real-world attack data, enabling robust IDS development and testing. The paper highlights the strategy\u2019s practicality, fidelity to real-world conditions, and the promising application in cybersecurity for connected vehicles."}}
{"id": "2507.02700", "pdf": "https://arxiv.org/pdf/2507.02700", "abs": "https://arxiv.org/abs/2507.02700", "authors": ["M\u00e1t\u00e9 B. Vizi", "D\u00e9nes T\u00e1k\u00e1cs", "G\u00e1bor St\u00e9p\u00e1n", "G\u00e1bor Orosz"], "title": "Integrating path-planning and control for robotic unicycles", "categories": ["cs.RO", "cs.SY", "eess.SY"], "comment": null, "summary": "This article focuses on integrating path-planning and control with\nspecializing on the unique needs of robotic unicycles. A unicycle design is\npresented which is capable of accelerating/breaking and carrying out a variety\nof maneuvers. The proposed path-planning method segments the path into straight\nand curved path sections dedicated for accelerating/breaking and turning\nmaneuvers, respectively. The curvature profiles of the curved sections are\noptimized while considering the control performance and the slipping limits of\nthe wheel. The performance of the proposed integrated approach is demonstrated\nvia numerical simulations.", "AI": {"tldr": "This paper explores path-planning and control integration tailored for robotic unicycles, presenting a design and method for optimized maneuvering.", "motivation": "Address the unique challenges of robotic unicycles by creating an efficient path-planning and control system.", "method": "Segment paths into straight and curved sections for acceleration/braking and turning, while optimizing curvature profiles considering slipping limits and control performance.", "result": "Numerical simulations showcase the performance of this integrated approach.", "conclusion": "The study confirms the feasibility of the proposed method for enhancing robotic unicycle navigation and maneuverability."}}
{"id": "2507.02299", "pdf": "https://arxiv.org/pdf/2507.02299", "abs": "https://arxiv.org/abs/2507.02299", "authors": ["Yunhan Yang", "Shuo Chen", "Yukun Huang", "Xiaoyang Wu", "Yuan-Chen Guo", "Edmund Y. Lam", "Hengshuang Zhao", "Tong He", "Xihui Liu"], "title": "DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation", "categories": ["cs.CV"], "comment": "Accepted by TPAMI, extension of CVPR 2024 paper DreamComposer", "summary": "Recent advancements in leveraging pre-trained 2D diffusion models achieve the\ngeneration of high-quality novel views from a single in-the-wild image.\nHowever, existing works face challenges in producing controllable novel views\ndue to the lack of information from multiple views. In this paper, we present\nDreamComposer++, a flexible and scalable framework designed to improve current\nview-aware diffusion models by incorporating multi-view conditions.\nSpecifically, DreamComposer++ utilizes a view-aware 3D lifting module to\nextract 3D representations of an object from various views. These\nrepresentations are then aggregated and rendered into the latent features of\ntarget view through the multi-view feature fusion module. Finally, the obtained\nfeatures of target view are integrated into pre-trained image or video\ndiffusion models for novel view synthesis. Experimental results demonstrate\nthat DreamComposer++ seamlessly integrates with cutting-edge view-aware\ndiffusion models and enhances their abilities to generate controllable novel\nviews from multi-view conditions. This advancement facilitates controllable 3D\nobject reconstruction and enables a wide range of applications.", "AI": {"tldr": "The paper introduces DreamComposer++, a framework enhancing view-aware diffusion models to generate controllable novel views using multi-view conditions.", "motivation": "Existing methods for generating novel views from a single image lack control due to insufficient multi-view information.", "method": "DreamComposer++ employs a view-aware 3D lifting module to extract and aggregate 3D object representations from multiple views, incorporating them into pre-trained diffusion models.", "result": "The framework seamlessly integrates with current diffusion models, improving their ability to synthesize controllable novel views and enabling robust 3D object reconstruction.", "conclusion": "DreamComposer++ enhances the controllability and scalability of view-aware diffusion models, enabling practical applications in 3D reconstruction and beyond."}}
