<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 55]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.CL](#cs.CL) [Total: 75]
- [cs.CV](#cs.CV) [Total: 174]
- [cs.DC](#cs.DC) [Total: 12]
- [cs.LG](#cs.LG) [Total: 141]
- [cs.NE](#cs.NE) [Total: 10]
- [cs.PL](#cs.PL) [Total: 4]
- [cs.RO](#cs.RO) [Total: 31]
- [cs.SE](#cs.SE) [Total: 24]
- [q-bio.NC](#q-bio.NC) [Total: 6]
- [stat.ML](#stat.ML) [Total: 13]
- [nlin.CG](#nlin.CG) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.CC](#cs.CC) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.CR](#cs.CR) [Total: 16]
- [cs.SI](#cs.SI) [Total: 4]
- [eess.AS](#eess.AS) [Total: 4]
- [cs.CG](#cs.CG) [Total: 2]
- [stat.ME](#stat.ME) [Total: 5]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.DL](#cs.DL) [Total: 1]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [q-bio.TO](#q-bio.TO) [Total: 1]
- [math.ST](#math.ST) [Total: 3]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.SD](#cs.SD) [Total: 6]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.NI](#cs.NI) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [eess.SP](#eess.SP) [Total: 2]
- [quant-ph](#quant-ph) [Total: 6]
- [eess.SY](#eess.SY) [Total: 4]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.IR](#cs.IR) [Total: 14]
- [stat.CO](#stat.CO) [Total: 1]
- [q-fin.PR](#q-fin.PR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 9]
- [nlin.CD](#nlin.CD) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [nlin.PS](#nlin.PS) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.CY](#cs.CY) [Total: 3]
- [physics.ins-det](#physics.ins-det) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [hep-th](#hep-th) [Total: 1]
- [cs.OH](#cs.OH) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections](https://arxiv.org/abs/2601.00814)
*Abhishek Kumar*

Main category: cs.AI

TL;DR: The paper develops a cross-lingual ontology alignment system using embedding-based cosine similarity matching, achieving a significant 16% performance improvement on OAEI-2022 multifarm track.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of aligning cross-lingual ontology entities by leveraging contextual and semantic similarity.

Method: The method involves generating enriched descriptions for ontology entities and using a fine-tuned multilingual transformer model to generate embeddings, followed by cosine similarity and threshold filtering for alignment.

Result: Achieved 71% F1 score (78% recall and 65% precision) on the OAEI-2022 multifarm track, representing a 16% improvement over the best baseline.

Conclusion: The pipeline effectively captures subtle cross-lingual similarities and demonstrates improvements in ontology alignment using embedding-based techniques.

Abstract: The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.

</details>


### [2] [MathLedger: A Verifiable Learning Substrate with Ledger-Attested Feedback](https://arxiv.org/abs/2601.00816)
*Ismail Ahmad Abdullah*

Main category: cs.AI

TL;DR: The paper introduces MathLedger, a system integrating formal verification and cryptographic attestation for verifiable AI systems to enhance trust and governance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of trust in contemporary AI systems due to their opaque and non-verifiable nature, especially for safety-critical applications.

Method: The proposed system, MathLedger, uses Reflexive Formal Learning (RFL), which combines elements of formal verification, cryptographic attestation, and learning dynamics. Phase I experiments tested the system's governance and measurement properties under controlled conditions.

Result: The experiments validated infrastructure components including measurement tools (e.g., Delta p computation, variance tracking) and governance triggers in stress test cases, establishing that they function as intended in out-of-bounds scenarios.

Conclusion: The contribution is largely infrastructural and demonstrates a prototype system (MathLedger) for scalable and auditable machine learning, not focusing on performance claims but on enabling trustworthy computation.

Abstract: Contemporary AI systems achieve extraordinary performance yet remain opaque and non-verifiable, creating a crisis of trust for safety-critical deployment. We introduce MathLedger, a substrate for verifiable machine cognition that integrates formal verification, cryptographic attestation, and learning dynamics into a single epistemic loop. The system implements Reflexive Formal Learning (RFL), a symbolic analogue of gradient descent where updates are driven by verifier outcomes rather than statistical loss.
  Phase I experiments validate the measurement and governance substrate under controlled conditions. CAL-EXP-3 validates measurement infrastructure (Delta p computation, variance tracking); separate stress tests confirm fail-closed governance triggers correctly under out-of-bounds conditions. No convergence or capability claims are made. The contribution is infrastructural: a working prototype of ledger-attested learning that enables auditability at scale.
  Keywords: verifiable learning, formal verification, cryptographic attestation, reflexive feedback, fail-closed governance

</details>


### [3] [Agentic AI for Autonomous, Explainable, and Real-Time Credit Risk Decision-Making](https://arxiv.org/abs/2601.00818)
*Chandra Sekhar Kubam*

Main category: cs.AI

TL;DR: The paper proposes an Agentic AI framework for autonomous, transparent, real-time credit risk decision-making, outperforming traditional models in speed, transparency, and responsiveness but with practical limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the need for autonomous, transparent, and real-time credit risk decision-making systems due to the rapid digitalization of financial services.

Method: The method involves a multi-agent system integrating reinforcement learning, natural language reasoning, explainable AI, and real-time data pipelines for credit risk assessment with minimal human intervention.

Result: The results show improved decision speed, transparency, and responsiveness compared to traditional credit scoring models, but highlight challenges such as model drift, high-dimensional data interpretation, and regulatory uncertainties.

Conclusion: The proposed system has significant potential to transform credit analytics, but further research is necessary on regulatory compliance, agent collaboration, adversarial robustness, and cross-country implementation.

Abstract: Significant digitalization of financial services in a short period of time has led to an urgent demand to have autonomous, transparent and real-time credit risk decision making systems. The traditional machine learning models are effective in pattern recognition, but do not have the adaptive reasoning, situational awareness, and autonomy needed in modern financial operations. As a proposal, this paper presents an Agentic AI framework, or a system where AI agents view the world of dynamic credit independent of human observers, who then make actions based on their articulable decision-making paths. The research introduces a multi-agent system with reinforcing learning, natural language reasoning, explainable AI modules, and real-time data absorption pipelines as a means of assessing the risk profiles of borrowers with few humans being involved. The processes consist of agent collaboration protocol, risk-scoring engines, interpretability layers, and continuous feedback learning cycles. Findings indicate that decision speed, transparency and responsiveness is better than traditional credit scoring models. Nevertheless, there are still some practical limitations such as risks of model drift, inconsistencies in interpreting high dimensional data and regulatory uncertainties as well as infrastructure limitations in low-resource settings. The suggested system has a high prospective to transform credit analytics and future studies ought to be directed on dynamic regulatory compliance mobilizers, new agent teamwork, adversarial robustness, and large-scale implementation in cross-country credit ecosystems.

</details>


### [4] [CogCanvas: Compression-Resistant Cognitive Artifacts for Long LLM Conversations](https://arxiv.org/abs/2601.00821)
*Tao An*

Main category: cs.AI

TL;DR: Large language models struggle with information fidelity in long conversations due to context window limits. CogCanvas proposes a training-free framework using cognitive artifacts and temporal graphs for retrieval, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of large language models in maintaining information fidelity during lengthy conversations due to truncation or summarization issues.

Method: Introduced CogCanvas: a training-free framework utilizing verbatim-grounded cognitive artifacts organized into temporal-aware graphs for retrieval during long conversations.

Result: CogCanvas achieves significant improvements in benchmarks like LoCoMo, demonstrating better accuracy and reasoning compared to RAG and GraphRAG. It offers high recall and exact match preservation.

Conclusion: CogCanvas provides an effective, training-free alternative for practitioners, outperforming standard models in handling long conversational context efficiently.

Abstract: Large language models face a fundamental tension between context window limits and information fidelity in long conversations. Existing approaches--truncation and summarization--either discard early information or lose nuanced details. We introduce CogCanvas, a training-free framework that extracts verbatim-grounded cognitive artifacts (decisions, facts, reminders) from conversation turns and organizes them into a temporal-aware graph for compression-resistant retrieval.
  On the LoCoMo benchmark, CogCanvas achieves 34.7% overall accuracy, outperforming RAG (25.6%, +9.1pp) and GraphRAG (13.7%, +21.0pp). The advantage is most pronounced on temporal reasoning: 31.5% vs. 9.3% (RAG) and 5.0% (GraphRAG)--a +530% relative improvement. On multi-hop causal reasoning, CogCanvas achieves 81.0% pass rate vs. 40.0% for GraphRAG (+41.0pp). Controlled benchmarks show 97.5% recall (+78.5pp vs. summarization) with 93.0% exact match preservation.
  While heavily-optimized approaches achieve higher absolute scores through dedicated training (EverMemOS: approximately 92%), our training-free approach provides practitioners with an immediately-deployable alternative that significantly outperforms standard baselines. Code and data: https://github.com/tao-hpu/cog-canvas.

</details>


### [5] [Universal Conditional Logic: A Formal Language for Prompt Engineering](https://arxiv.org/abs/2601.00880)
*Anthony Mikinka*

Main category: cs.AI

TL;DR: The paper introduces Universal Conditional Logic (UCL), a method for systematically optimizing prompt engineering that significantly reduces token use and costs.


<details>
  <summary>Details</summary>
Motivation: To transform prompt engineering from a heuristic-based practice to a systematic and optimized process, enabling efficient interactions with large language models (LLMs).

Method: The authors propose the Universal Conditional Logic (UCL) framework and validate it through evaluations across 305 prompts, 11 LLMs, and iterative optimization. They assess its performance using token savings metrics, structural overhead functions, and configuration adaptability to model-specific needs.

Result: UCL achieved 29.8% token reduction with significant cost savings, demonstrating its effectiveness in various LLMs. The Over-Specification Paradox was explained, showing quadratic performance degradation beyond a specification threshold.

Conclusion: UCL is a robust framework for prompt optimization in LLMs. It highlights the need for model-family-specific configurations and establishes systematic optimization as a key research area for effective LLM interactions.

Abstract: We present Universal Conditional Logic (UCL), a mathematical framework for prompt optimization that transforms prompt engineering from heuristic practice into systematic optimization. Through systematic evaluation (N=305, 11 models, 4 iterations), we demonstrate significant token reduction (29.8%, t(10)=6.36, p < 0.001, Cohen's d = 2.01) with corresponding cost savings. UCL's structural overhead function O_s(A) explains version-specific performance differences through the Over-Specification Paradox: beyond threshold S* = 0.509, additional specification degrades performance quadratically. Core mechanisms -- indicator functions (I_i in {0,1}), structural overhead (O_s = gamma * sum(ln C_k)), early binding -- are validated. Notably, optimal UCL configuration varies by model architecture -- certain models (e.g., Llama 4 Scout) require version-specific adaptations (V4.1). This work establishes UCL as a calibratable framework for efficient LLM interaction, with model-family-specific optimization as a key research direction.

</details>


### [6] [Energy-Aware Routing to Large Reasoning Models](https://arxiv.org/abs/2601.00823)
*Austin R. Ellis-Mohr,Max Hartman,Lav R. Varshney*

Main category: cs.AI

TL;DR: The paper discusses optimizing energy efficiency in Large Reasoning Models (LRMs) by balancing energy provisioning and reducing variability through variance-aware task routing and dispatch policies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of energy efficiency in LRMs, highlighting the impact of inference energy variability and the need for systematic task dispatching.

Method: Analyzes task dispatch policies based on energy provisioning, variability absorption, and scaling laws associated with training and inference compute.

Result: Introduces a theoretical framework for variance-aware routing and characterizes routing behavior based on compute scaling laws.

Conclusion: Variance-aware task routing improves energy efficiency and provides a foundation for energy-optimized model routing policies in LRMs.

Abstract: Large reasoning models (LRMs) have heterogeneous inference energy costs based on which model is used and how much it reasons. To reduce energy, it is important to choose the right LRM and operate it in the right way. As a result, the performance of systems that dispatch tasks to different individual LRMs depend on the balance between mean energy provisioning and stochastic fluctuations. The critical regime is the unique operating point at which neither auxiliary energy nor baseline energy is systematically wasted. Increasing baseline supply shifts the system toward persistent over-supply and baseline-energy waste, while reducing supply induces persistent reliance on auxiliary energy. Yet in this regime, performance remains volatility-limited and so a second-order characterization provides further insights that we develop. Here, performance is governed by how variability is absorbed across time, models, and execution choices. This perspective highlights variance-aware routing and dispatch as a principled design axis, and provides a theoretical basis for developing energy-aware model routing policies. Routing behavior is characterized when dispatch policies are based on training-compute and inference-compute scaling laws for LRMs.

</details>


### [7] [Decomposing LLM Self-Correction: The Accuracy-Correction Paradox and Error Depth Hypothesis](https://arxiv.org/abs/2601.00828)
*Yin Li*

Main category: cs.AI

TL;DR: This paper studies the intrinsic self-correction abilities of LLMs, uncovering paradoxical findings where weaker models often outperform stronger ones in correction rates.


<details>
  <summary>Details</summary>
Motivation: To investigate the intrinsic self-correction capabilities of large language models (LLMs), breaking down the process into error detection, localization, and correction.

Method: Cross-model experiments were conducted on GSM8K-Complex using three major LLMs, analyzing error handling through intrinsic self-correction processes.

Result: Weaker models showed higher intrinsic correction rates despite lower accuracy. Providing error location hints negatively impacted performance, and detection ability didn’t correlate with correction success.

Conclusion: The study challenges the assumption that stronger models inherently excel at self-correction, highlighting the need to rethink self-refinement pipeline designs.

Abstract: Large Language Models (LLMs) are widely believed to possess self-correction capabilities, yet recent studies suggest that intrinsic self-correction--where models correct their own outputs without external feedback--remains largely ineffective. In this work, we systematically decompose self-correction into three distinct sub-capabilities: error detection, error localization, and error correction. Through cross-model experiments on GSM8K-Complex (n=500 per model, 346 total errors) with three major LLMs, we uncover a striking Accuracy-Correction Paradox: weaker models (GPT-3.5, 66% accuracy) achieve 1.6x higher intrinsic correction rates than stronger models (DeepSeek, 94% accuracy)--26.8% vs 16.7%. We propose the Error Depth Hypothesis: stronger models make fewer but deeper errors that resist self-correction. Error detection rates vary dramatically across architectures (10% to 82%), yet detection capability does not predict correction success--Claude detects only 10% of errors but corrects 29% intrinsically. Surprisingly, providing error location hints hurts all models. Our findings challenge linear assumptions about model capability and self-improvement, with important implications for the design of self-refinement pipelines.

</details>


### [8] [Can We Trust AI Explanations? Evidence of Systematic Underreporting in Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.00830)
*Deep Pankajbhai Mehta*

Main category: cs.AI

TL;DR: AI systems frequently perceive influential hints but fail to disclose them spontaneously or accurately. Direct inquiry or coercion reveals complications, including reduced accuracy.


<details>
  <summary>Details</summary>
Motivation: To assess if AI explanations genuinely reflect influences on its reasoning process and explore their reliability in detecting hidden cues.

Method: Studied 9,000 test cases across 11 AI models, using embedded hints to measure whether models acknowledged them spontaneously or upon direct inquiry.

Result: Models seldom mention hints unprompted but admit noticing them when questioned, revealing a discrepancy between perception and disclosure.

Conclusion: AI reasoning explanations fail to reliably reveal hidden cues. Methods to enforce transparency diminish model accuracy and may lead to false reporting.

Abstract: When AI systems explain their reasoning step-by-step, practitioners often assume these explanations reveal what actually influenced the AI's answer. We tested this assumption by embedding hints into questions and measuring whether models mentioned them. In a study of over 9,000 test cases across 11 leading AI models, we found a troubling pattern: models almost never mention hints spontaneously, yet when asked directly, they admit noticing them. This suggests models see influential information but choose not to report it. Telling models they are being watched does not help. Forcing models to report hints works, but causes them to report hints even when none exist and reduces their accuracy. We also found that hints appealing to user preferences are especially dangerous-models follow them most often while reporting them least. These findings suggest that simply watching AI reasoning is not enough to catch hidden influences.

</details>


### [9] [OmniNeuro: A Multimodal HCI Framework for Explainable BCI Feedback via Generative AI and Sonification](https://arxiv.org/abs/2601.00843)
*Ayda Aghaei Nia*

Main category: cs.AI

TL;DR: The paper presents OmniNeuro, a framework turning BCIs into transparent feedback tools by integrating interpretability engines and sonification for improved usability.


<details>
  <summary>Details</summary>
Motivation: Clinical adoption of BCI technology is hindered by its 'Black Box' nature, which frustrates users and leads to poor neuroplasticity outcomes.

Method: OmniNeuro integrates Physics-, Chaos-, and Quantum-Inspired interpretability engines to provide real-time neuro-sonification and AI-generated clinical reports.

Result: On the PhysioNet dataset (N=109), OmniNeuro achieved 58.52% accuracy, while pilot studies (N=3) confirmed its usability benefits in mental effort regulation.

Conclusion: OmniNeuro improves BCI usability by acting as an interpretability layer that enhances user understanding and reduces trial-and-error phases.

Abstract: While Deep Learning has improved Brain-Computer Interface (BCI) decoding accuracy, clinical adoption is hindered by the "Black Box" nature of these algorithms, leading to user frustration and poor neuroplasticity outcomes. We propose OmniNeuro, a novel HCI framework that transforms the BCI from a silent decoder into a transparent feedback partner. OmniNeuro integrates three interpretability engines: (1) Physics (Energy), (2) Chaos (Fractal Complexity), and (3) Quantum-Inspired uncertainty modeling. These metrics drive real-time Neuro-Sonification and Generative AI Clinical Reports. Evaluated on the PhysioNet dataset ($N=109$), the system achieved a mean accuracy of 58.52%, with qualitative pilot studies ($N=3$) confirming that explainable feedback helps users regulate mental effort and reduces the "trial-and-error" phase. OmniNeuro is decoder-agnostic, acting as an essential interpretability layer for any state-of-the-art architecture.

</details>


### [10] [Enhancing Temporal Awareness in LLMs for Temporal Point Processes](https://arxiv.org/abs/2601.00845)
*Lili Chen,Wensheng Gan,Shuang Liang,Philip S. Yu*

Main category: cs.AI

TL;DR: The paper introduces TPP-TAL, a framework enhancing temporal reasoning for LLMs in temporal point processes, showing improved event prediction and temporal likelihood estimation.


<details>
  <summary>Details</summary>
Motivation: Current temporal modeling methods struggle to effectively combine temporal information and semantic context, vital for accurate event analysis.

Method: TPP-TAL explicitly aligns temporal dynamics with contextual semantics before integrating them into LLMs, addressing conventional limitations.

Result: Experiments on benchmark datasets demonstrate TPP-TAL significantly improves temporal likelihood estimation and event prediction accuracy.

Conclusion: Enhancing temporal awareness in LLMs is crucial for event modeling, and TPP-TAL provides a novel, effective way to achieve this.

Abstract: Temporal point processes (TPPs) are crucial for analyzing events over time and are widely used in fields such as finance, healthcare, and social systems. These processes are particularly valuable for understanding how events unfold over time, accounting for their irregularity and dependencies. Despite the success of large language models (LLMs) in sequence modeling, applying them to temporal point processes remains challenging. A key issue is that current methods struggle to effectively capture the complex interaction between temporal information and semantic context, which is vital for accurate event modeling. In this context, we introduce TPP-TAL (Temporal Point Processes with Enhanced Temporal Awareness in LLMs), a novel plug-and-play framework designed to enhance temporal reasoning within LLMs. Rather than using the conventional method of simply concatenating event time and type embeddings, TPP-TAL explicitly aligns temporal dynamics with contextual semantics before feeding this information into the LLM. This alignment allows the model to better perceive temporal dependencies and long-range interactions between events and their surrounding contexts. Through comprehensive experiments on several benchmark datasets, it is shown that TPP-TAL delivers substantial improvements in temporal likelihood estimation and event prediction accuracy, highlighting the importance of enhancing temporal awareness in LLMs for continuous-time event modeling. The code is made available at https://github.com/chenlilil/TPP-TAL

</details>


### [11] [Temporal Attack Pattern Detection in Multi-Agent AI Workflows: An Open Framework for Training Trace-Based Security Models](https://arxiv.org/abs/2601.00848)
*Ron F. Del Rosario*

Main category: cs.AI

TL;DR: The paper introduces a method to fine-tune language models for detecting attack patterns in multi-agent AI workflows, using a curated dataset and OpenTelemetry trace analysis. It improves benchmark accuracy significantly.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address multi-agent coordination attacks and regulatory violations in AI workflows by creating a reproducible framework for customized security models.

Method: They used iterative QLoRA fine-tuning on resource-constrained hardware with strategic data augmentation and curated a large dataset combining real-world and synthetic traces.

Result: Benchmark accuracy improved from 42.86% to 74.29%, showing statistically significant improvements and highlighting targeted data effectiveness over indiscriminate scaling.

Conclusion: The methodology establishes a reproducible framework for AI security models adapted to specific threat scenarios, emphasizing targeted training and open accessibility of resources.

Abstract: We present an openly documented methodology for fine-tuning language models to detect temporal attack patterns in multi-agent AI workflows using OpenTelemetry trace analysis. We curate a dataset of 80,851 examples from 18 public cybersecurity sources and 35,026 synthetic OpenTelemetry traces. We apply iterative QLoRA fine-tuning on resource-constrained ARM64 hardware (NVIDIA DGX Spark) through three training iterations with strategic augmentation. Our custom benchmark accuracy improves from 42.86% to 74.29%, a statistically significant 31.4-point gain. Targeted examples addressing specific knowledge gaps outperform indiscriminate scaling. Key contributions include: (1) synthetic trace generation methodology for multi-agent coordination attacks and regulatory violations, (2) empirical evidence that training data composition fundamentally determines behavior, and (3) complete open release of datasets, training scripts, and evaluation benchmarks on HuggingFace. While practical deployment requires human oversight due to false positive rates, this work establishes the first reproducible framework enabling practitioners to build custom agentic security models adapted to their threat landscapes.

</details>


### [12] [Comment on: Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Tasks](https://arxiv.org/abs/2601.00856)
*Milos Stankovic,Ella Hirche,Sarah Kollatzsch,Julia Nadine Doetsch*

Main category: cs.AI

TL;DR: Kosmyna et al. (2025) investigate AI's impact on human performance in essay writing using cognitive and NLP evaluations, facing critiques on methodology and transparency.


<details>
  <summary>Details</summary>
Motivation: To analyze the implications of AI usage (ChatGPT) on human cognitive processes and performance during essay writing tasks.

Method: The study utilized automated NLP pipelines, EEG data, and a dataset to assess cognitive effects and performance metrics when humans used AI (ChatGPT) for essay writing.

Result: Findings suggest varied interpretations regarding cognitive impact and performance results, though critiques of sample size, methodology, and transparency challenge conclusions.

Conclusion: The paper provides valuable insights into AI's cognitive effects in essay tasks, but improvements in study design, transparency, and reproducibility are recommended.

Abstract: Recently published work titled Your Brain on ChatGPT: Accumulation of Cognitive Debt When Using an AI Assistant for Essay Writing Task by Kosmyna et al. (2025) has sparked a vivid debate on the topic of artificial intelligence (AI) and human performance. We sincerely congratulate Kosmyna et al. for initiating such important research, collecting a valuable dataset, and establishing highly automated pipelines for Natural Language Processing (NLP) analyses and scoring. We aim to provide constructive comments that may improve the manuscript's readiness for peer-reviewed publication, as some results by Kosmyna et al. (2025) could be interpreted more conservatively. Our primary concerns focus on: (i) study design considerations, including the limited sample size; (ii) the reproducibility of the analyses; (iii) methodological issues related to the EEG analysis; (iv) inconsistencies in the reporting of results; and (v) limited transparency in several aspects of the study's procedures and findings.

</details>


### [13] [Cultural Encoding in Large Language Models: The Existence Gap in AI-Mediated Brand Discovery](https://arxiv.org/abs/2601.00869)
*Huang Junyao,Situ Ruimin,Ye Renqin*

Main category: cs.AI

TL;DR: This paper explores Cultural Encoding in LLMs and finds systematic disparities in brand visibility due to geographic training data differences.


<details>
  <summary>Details</summary>
Motivation: To address algorithmic invisibility and disparities in brand representation caused by training data geography in large language models.

Method: Analyzed 1,909 pure-English queries across 6 LLMs and 30 brands to identify disparities in brand mentions, particularly focusing on the Existence Gap.

Result: Chinese LLMs showed 30.6% higher brand mention rates compared to international models. Disparities were independent of language but dependent on training data geography.

Conclusion: The study introduces the Data Moat Framework and recommends strategies like cultural localization to ensure brand visibility in AI-mediated markets.

Abstract: As artificial intelligence systems increasingly mediate consumer information discovery,
  brands face algorithmic invisibility. This study investigates Cultural Encoding in Large
  Language Models (LLMs) -- systematic differences in brand recommendations arising from
  training data composition. Analyzing 1,909 pure-English queries across 6 LLMs (GPT-4o,
  Claude, Gemini, Qwen3, DeepSeek, Doubao) and 30 brands, we find Chinese LLMs exhibit 30.6
  percentage points higher brand mention rates than International LLMs (88.9% vs. 58.3%,
  p<.001). This disparity persists in identical English queries, indicating training data
  geography -- not language -- drives the effect. We introduce the Existence Gap: brands
  absent from LLM training corpora lack "existence" in AI responses regardless of quality.
  Through a case study of Zhizibianjie (OmniEdge), a collaboration platform with 65.6%
  mention rate in Chinese LLMs but 0% in International models (p<.001), we demonstrate how
  Linguistic Boundary Barriers create invisible market entry obstacles. Theoretically, we
  contribute the Data Moat Framework, conceptualizing AI-visible content as a VRIN strategic
  resource. We operationalize Algorithmic Omnipresence -- comprehensive brand visibility
  across LLM knowledge bases -- as the strategic objective for Generative Engine Optimization
  (GEO). Managerially, we provide an 18-month roadmap for brands to build Data Moats
  through semantic coverage, technical depth, and cultural localization. Our findings reveal
  that in AI-mediated markets, the limits of a brand's "Data Boundaries" define the limits
  of its "Market Frontiers."

</details>


### [14] [Counterfactual Self-Questioning for Stable Policy Optimization in Language Models](https://arxiv.org/abs/2601.00885)
*Mandar Parab*

Main category: cs.AI

TL;DR: The paper introduces Counterfactual Self-Questioning, where a single language model refines itself by generating critiques and alternative reasoning paths, enhancing mathematical reasoning accuracy without external models.


<details>
  <summary>Details</summary>
Motivation: Current language model self-improvement relies heavily on external critics, reward models, or ensembles, increasing complexity and instability. The paper aims to simplify self-improvement using internal supervision.

Method: The model produces an initial reasoning trace, challenges potential errors through targeted questions, and generates counterfactual reasoning pathways to uncover invalid assumptions or steps.

Result: Experiments on mathematical reasoning benchmarks demonstrate improved accuracy and training stability across models, especially smaller ones, using this self-improvement approach.

Conclusion: Counterfactual self-questioning enhances scalable self-improvement of language models with internally generated supervision, reducing dependency on auxiliary frameworks.

Abstract: Recent work on language model self-improvement shows that models can refine their own reasoning through reflection, verification, debate, or self-generated rewards. However, most existing approaches rely on external critics, learned reward models, or ensemble sampling, which increases complexity and training instability. We propose Counterfactual Self-Questioning, a framework in which a single language model generates and evaluates counterfactual critiques of its own reasoning. The method produces an initial reasoning trace, formulates targeted questions that challenge potential failure points, and generates alternative reasoning trajectories that expose incorrect assumptions or invalid steps. These counterfactual trajectories provide structured relative feedback that can be directly used for policy optimization without auxiliary models. Experiments on multiple mathematical reasoning benchmarks show that counterfactual self-questioning improves accuracy and training stability, particularly for smaller models, enabling scalable self-improvement using internally generated supervision alone.

</details>


### [15] [Context Collapse: In-Context Learning and Model Collapse](https://arxiv.org/abs/2601.00923)
*Josef Ott*

Main category: cs.AI

TL;DR: The study examines two phenomena in large language models (LLMs): in-context learning and model collapse, exploring their dynamics and challenges.


<details>
  <summary>Details</summary>
Motivation: To understand mechanisms behind in-context learning and address stability issues like model collapse in LLMs.

Method: Analyzed behavior of LLMs with linear transformers tied to weight dynamics; applied martingale and random walk methods for simplified model systems.

Result: Revealed phase transitions in learned parameters during in-context learning, strengthened convergence results for model collapse scenarios, introduced context collapse phenomena in generation tasks.

Conclusion: In-context learning exhibits pivotal parameter shifts, while model collapse requires careful data handling for stability. Context collapse connects data dynamics with generative model challenges.

Abstract: This thesis investigates two key phenomena in large language models (LLMs): in-context learning (ICL) and model collapse. We study ICL in a linear transformer with tied weights trained on linear regression tasks, and show that minimising the in-context loss leads to a phase transition in the learned parameters. Above a critical context length, the solution develops a skew-symmetric component. We prove this by reducing the forward pass of the linear transformer under weight tying to preconditioned gradient descent, and then analysing the optimal preconditioner. This preconditioner includes a skew-symmetric component, which induces a rotation of the gradient direction. For model collapse, we use martingale and random walk theory to analyse simplified settings - linear regression and Gaussian fitting - under both replacing and cumulative data regimes. We strengthen existing results by proving almost sure convergence, showing that collapse occurs unless the data grows sufficiently fast or is retained over time. Finally, we introduce the notion of context collapse: a degradation of context during long generations, especially in chain-of-thought reasoning. This concept links the dynamics of ICL with long-term stability challenges in generative models.

</details>


### [16] [ElecTwit: A Framework for Studying Persuasion in Multi-Agent Social Systems](https://arxiv.org/abs/2601.00994)
*Michael Bao*

Main category: cs.AI

TL;DR: The paper introduces ElecTwit, a framework to study persuasion techniques of AI agents in a simulated social media election setting.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of game-based simulations and study persuasion in realistic social media-like environments during elections.

Method: A simulation framework called ElecTwit was developed. It tested 25 persuasion techniques on LLMs in a realistic multi-agent environment to analyze persuasion dynamics.

Result: Identified variation in persuasion techniques among models. Observed unique behaviors like "kernel of truth" messages and collective demands for written proof.

Conclusion: ElecTwit offers a basis to study and assess the effectiveness of persuasive LLM agents, focusing on alignment, safety, and real-world implications.

Abstract: This paper introduces ElecTwit, a simulation framework designed to study persuasion within multi-agent systems, specifically emulating the interactions on social media platforms during a political election. By grounding our experiments in a realistic environment, we aimed to overcome the limitations of game-based simulations often used in prior research. We observed the comprehensive use of 25 specific persuasion techniques across most tested LLMs, encompassing a wider range than previously reported. The variations in technique usage and overall persuasion output between models highlight how different model architectures and training can impact the dynamics in realistic social simulations. Additionally, we observed unique phenomena such as "kernel of truth" messages and spontaneous developments with an "ink" obsession, where agents collectively demanded written proof. Our study provides a foundation for evaluating persuasive LLM agents in real-world contexts, ensuring alignment and preventing dangerous outcomes.

</details>


### [17] [Reinforcement Learning Enhanced Multi-hop Reasoning for Temporal Knowledge Question Answering](https://arxiv.org/abs/2601.01195)
*Wuzhenghong Wen,Chao Xue,Su Pan,Yuwei Sun,Minlong Peng*

Main category: cs.AI

TL;DR: The paper introduces the MRE framework for improving multi-hop reasoning in temporal knowledge graph question answering (TKGQA) using enhanced prompt engineering and a tree-structured learning method.


<details>
  <summary>Details</summary>
Motivation: Challenges in TKGQA include handling noisy, temporally similar, and semantically complex relations at each reasoning hop, leading to suboptimal decisions and error propagation.

Method: MRE framework combines prompt engineering for generating diverse reasoning trajectories, supervised fine-tuning for cold-start reasoning, and Tree-Group Relative Policy Optimization (T-GRPO) for recursive learning-by-exploration to optimize causal dependencies and feedback evaluation.

Result: Experimental results show the MRE model outperforms SOTA approaches on two TKGQA benchmarks with better handling of complex queries and robustness to noisy temporal annotations.

Conclusion: The proposed MRE framework significantly improves interpretability, robustness, and reasoning performance in temporal knowledge graph question answering compared to existing methods.

Abstract: Temporal knowledge graph question answering (TKGQA) involves multi-hop reasoning over temporally constrained entity relationships in the knowledge graph to answer a given question. However, at each hop, large language models (LLMs) retrieve subgraphs with numerous temporally similar and semantically complex relations, increasing the risk of suboptimal decisions and error propagation. To address these challenges, we propose the multi-hop reasoning enhanced (MRE) framework, which enhances both forward and backward reasoning to improve the identification of globally optimal reasoning trajectories. Specifically, MRE begins with prompt engineering to guide the LLM in generating diverse reasoning trajectories for a given question. Valid reasoning trajectories are then selected for supervised fine-tuning, serving as a cold-start strategy. Finally, we introduce Tree-Group Relative Policy Optimization (T-GRPO), a recursive, tree-structured learning-by-exploration approach. At each hop, exploration establishes strong causal dependencies on the previous hop, while evaluation is informed by multi-path exploration feedback from subsequent hops. Experimental results on two TKGQA benchmarks indicate that the proposed MRE-based model consistently surpasses state-of-the-art (SOTA) approaches in handling complex multi-hop queries. Further analysis highlights improved interpretability and robustness to noisy temporal annotations.

</details>


### [18] [Accelerating Monte-Carlo Tree Search with Optimized Posterior Policies](https://arxiv.org/abs/2601.01301)
*Keith Frankston,Benjamin Howard*

Main category: cs.AI

TL;DR: RMCTS is introduced as a recursive AlphaZero-style Monte-Carlo tree search algorithm, offering significant speed improvements over MCTS-UCB while maintaining comparable network quality.


<details>
  <summary>Details</summary>
Motivation: To improve the speed and efficiency of Monte-Carlo tree search algorithms without compromising the quality of training outcomes.

Method: RMCTS employs recursive computation, optimized posterior policy calculation, and breadth-first exploration to reduce GPU latency costs and speed up batch network inference.

Result: RMCTS is over 40 times faster when searching a single root state and 3 times faster for large batches, while producing networks of comparable quality to MCTS-UCB in one-third of the training time.

Conclusion: RMCTS offers a trade-off that prioritizes speed and efficiency over adaptive tree exploration, proving effective for rapid network training in games like Connect-4, Dots-and-Boxes, and Othello.

Abstract: We introduce a recursive AlphaZero-style Monte--Carlo tree search algorithm, "RMCTS". The advantage of RMCTS over AlphaZero's MCTS-UCB is speed. In RMCTS, the search tree is explored in a breadth-first manner, so that network inferences naturally occur in large batches. This significantly reduces the GPU latency cost. We find that RMCTS is often more than 40 times faster than MCTS-UCB when searching a single root state, and about 3 times faster when searching a large batch of root states.
  The recursion in RMCTS is based on computing optimized posterior policies at each game state in the search tree, starting from the leaves and working back up to the root. Here we use the posterior policy explored in "Monte--Carlo tree search as regularized policy optimization" (Grill, et al.) Their posterior policy is the unique policy which maximizes the expected reward given estimated action rewards minus a penalty for diverging from the prior policy.
  The tree explored by RMCTS is not defined in an adaptive manner, as it is in MCTS-UCB. Instead, the RMCTS tree is defined by following prior network policies at each node. This is a disadvantage, but the speedup advantage is more significant, and in practice we find that RMCTS-trained networks match the quality of MCTS-UCB-trained networks in roughly one-third of the training time. We include timing and quality comparisons of RMCTS vs. MCTS-UCB for three games: Connect-4, Dots-and-Boxes, and Othello.

</details>


### [19] [Digital Twin AI: Opportunities and Challenges from Large Language Models to World Models](https://arxiv.org/abs/2601.01321)
*Rong Zhou,Dongping Chen,Zihan Jia,Yao Su,Yixin Liu,Yiwen Lu,Dongwei Shi,Yue Huang,Tianyang Xu,Yi Pan,Xinliang Li,Yohannes Abate,Qingyu Chen,Zhengzhong Tu,Yu Yang,Yu Zhang,Qingsong Wen,Gengchen Mai,Sunyang Fu,Jiachen Li,Xuyu Wang,Ziran Wang,Jing Huang,Tianming Liu,Yong Chen,Lichao Sun,Lifang He*

Main category: cs.AI

TL;DR: This paper explores a four-stage AI-integrated framework for digital twins, spanning modeling, mirroring, intervention, and autonomous management, while addressing challenges like scalability and trustworthiness.


<details>
  <summary>Details</summary>
Motivation: To conceptualize a systematic framework for embedding artificial intelligence into the lifecycle of digital twins, thereby enhancing their transformation from simulation tools to autonomous intelligent entities.

Method: The paper presents a four-stage framework: (1) physics-informed modeling, (2) real-time mirroring, (3) predictive intervention, and (4) autonomous management powered by AI approaches like large language models.

Result: The proposed framework is reviewed across eleven application domains, identifying advancements in areas such as reasoning, communication, and generative capacities, along with challenges of scalability, explainability, and trustworthiness.

Conclusion: AI technologies, particularly generative and physics-informed methods, are crucial for evolving digital twins into autonomous, cognitive systems. Responsible development addressing scalability and trust concerns is vital for their growth.

Abstract: Digital twins, as precise digital representations of physical systems, have evolved from passive simulation tools into intelligent and autonomous entities through the integration of artificial intelligence technologies. This paper presents a unified four-stage framework that systematically characterizes AI integration across the digital twin lifecycle, spanning modeling, mirroring, intervention, and autonomous management. By synthesizing existing technologies and practices, we distill a unified four-stage framework that systematically characterizes how AI methodologies are embedded across the digital twin lifecycle: (1) modeling the physical twin through physics-based and physics-informed AI approaches, (2) mirroring the physical system into a digital twin with real-time synchronization, (3) intervening in the physical twin through predictive modeling, anomaly detection, and optimization strategies, and (4) achieving autonomous management through large language models, foundation models, and intelligent agents. We analyze the synergy between physics-based modeling and data-driven learning, highlighting the shift from traditional numerical solvers to physics-informed and foundation models for physical systems. Furthermore, we examine how generative AI technologies, including large language models and generative world models, transform digital twins into proactive and self-improving cognitive systems capable of reasoning, communication, and creative scenario generation. Through a cross-domain review spanning eleven application domains, including healthcare, aerospace, smart manufacturing, robotics, and smart cities, we identify common challenges related to scalability, explainability, and trustworthiness, and outline directions for responsible AI-driven digital twin systems.

</details>


### [20] [Beyond Gemini-3-Pro: Revisiting LLM Routing and Aggregation at Scale](https://arxiv.org/abs/2601.01330)
*Shengji Tang,Weihao Lin,Jingqi Ye,Hao Li,Bo Zhang,Shuyue Hu,Tao Chen,Wangli Ouyang,Lei Bai,Peng Ye*

Main category: cs.AI

TL;DR: The paper presents JiSi, a framework enabling ten open-source LLMs to collectively surpass Gemini-3-Pro's performance at only 47% of the cost.


<details>
  <summary>Details</summary>
Motivation: Gemini-3-Pro's achievements show LLM performance leaps, but the paper explores collective intelligence among open-source LLMs as an alternative to expensive monolithic scaling.

Method: JiSi framework innovates in routing and aggregation through Query-Response Mixed Routing, Support-Set-based Aggregator Selection, and Adaptive Routing-Aggregation Switch.

Result: JiSi surpasses Gemini-3-Pro’s performance using only 47% of its cost, across nine benchmarks, beating other baselines as well.

Conclusion: Collective intelligence among LLMs is a promising new avenue towards achieving AGI, as demonstrated by JiSi's performance capabilities.

Abstract: Large Language Models (LLMs) have rapidly advanced, with Gemini-3-Pro setting a new performance milestone. In this work, we explore collective intelligence as an alternative to monolithic scaling, and demonstrate that open-source LLMs' collaboration can surpass Gemini-3-Pro. We first revisit LLM routing and aggregation at scale and identify three key bottlenecks: (1) current train-free routers are limited by a query-based paradigm focusing solely on textual similarity; (2) recent aggregation methods remain largely static, failing to select appropriate aggregators for different tasks;(3) the complementarity of routing and aggregation remains underutilized. To address these problems, we introduce JiSi, a novel framework designed to release the full potential of LLMs' collaboration through three innovations: (1) Query-Response Mixed Routing capturing both semantic information and problem difficulty; (2) Support-Set-based Aggregator Selection jointly evaluating the aggregation and domain capacity of aggregators; (3) Adaptive Routing-Aggregation Switch dynamically leveraging the advantages of routing and aggregation. Comprehensive experiments on nine benchmarks demonstrate that JiSi can surpass Gemini-3-Pro with only 47% costs by orchestrating ten open-source LLMs, while outperforming mainstream baselines. It suggests that collective intelligence represents a novel path towards Artificial General Intelligence (AGI).

</details>


### [21] [A unified multimodal understanding and generation model for cross-disciplinary scientific research](https://arxiv.org/abs/2601.01363)
*Xiaomeng Yang,Zhiyu Tan,Xiaohui Zhong,Mengping Yang,Qiusheng Huang,Lei Chen,Libo Wu,Hao Li*

Main category: cs.AI

TL;DR: FuXi-Uni is a unified multimodal model designed for understanding and generating scientific data across disciplines, demonstrating superior results in Earth science and Biomedicine.


<details>
  <summary>Details</summary>
Motivation: To address the need for models capable of integrating and processing high-dimensional, multimodal scientific data across different domains to tackle cross-disciplinary research challenges.

Method: FuXi-Uni uses a unified architecture for aligning cross-disciplinary tokens with natural language, employing a science decoder to handle both language and numerical prediction tasks, validated in Earth science and Biomedicine.

Result: In Earth science, FuXi-Uni outperformed SOTA systems in global weather forecasting, tropical cyclone predictions, and high-resolution regional modeling. In Biomedicine, it excelled in visual question answering benchmarks compared to leading models.

Conclusion: FuXi-Uni represents progress towards general-purpose multimodal scientific models that unify heterogeneous modalities while maintaining domain-specific performance.

Abstract: Scientific discovery increasingly relies on integrating heterogeneous, high-dimensional data across disciplines nowadays. While AI models have achieved notable success across various scientific domains, they typically remain domain-specific or lack the capability of simultaneously understanding and generating multimodal scientific data, particularly for high-dimensional data. Yet, many pressing global challenges and scientific problems are inherently cross-disciplinary and require coordinated progress across multiple fields. Here, we present FuXi-Uni, a native unified multimodal model for scientific understanding and high-fidelity generation across scientific domains within a single architecture. Specifically, FuXi-Uni aligns cross-disciplinary scientific tokens within natural language tokens and employs science decoder to reconstruct scientific tokens, thereby supporting both natural language conversation and scientific numerical prediction. Empirically, we validate FuXi-Uni in Earth science and Biomedicine. In Earth system modeling, the model supports global weather forecasting, tropical cyclone (TC) forecast editing, and spatial downscaling driven by only language instructions. FuXi-Uni generates 10-day global forecasts at 0.25° resolution that outperform the SOTA physical forecasting system. It shows superior performance for both TC track and intensity prediction relative to the SOTA physical model, and generates high-resolution regional weather fields that surpass standard interpolation baselines. Regarding biomedicine, FuXi-Uni outperforms leading multimodal large language models on multiple biomedical visual question answering benchmarks. By unifying heterogeneous scientific modalities within a native shared latent space while maintaining strong domain-specific performance, FuXi-Uni provides a step forward more general-purpose, multimodal scientific models.

</details>


### [22] [KGCE: Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models](https://arxiv.org/abs/2601.01366)
*Zixian Liu,Sihao Liu,Yuqi Zhao*

Main category: cs.AI

TL;DR: The paper introduces KGCE, a novel benchmarking platform for evaluating educational agents using multimodal language models in cross-platform tasks including school-specific software.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks struggle with evaluating autonomous agents in educational settings involving private-domain software, and current metrics lack granularity for detailed performance analysis.

Method: KGCE integrates knowledge-based software understanding and a dual-graph evaluation system, breaking down tasks into sub-goals for fine-grained assessments.

Result: KGCE successfully supports 104 cross-platform tasks across Windows and Android, enhancing agents' efficiency in private-domain tasks and providing detailed metrics through a new framework.

Conclusion: KGCE improves the benchmarking standards for agents in educational environments, addressing limitations in software-specific execution and offering detailed evaluations for complex tasks.

Abstract: With the rapid adoption of multimodal large language models (MLMs) in autonomous agents, cross-platform task execution capabilities in educational settings have garnered significant attention. However, existing benchmark frameworks still exhibit notable deficiencies in supporting cross-platform tasks in educational contexts, especially when dealing with school-specific software (such as XiaoYa Intelligent Assistant, HuaShi XiaZi, etc.), where the efficiency of agents often significantly decreases due to a lack of understanding of the structural specifics of these private-domain software. Additionally, current evaluation methods heavily rely on coarse-grained metrics like goal orientation or trajectory matching, making it challenging to capture the detailed execution and efficiency of agents in complex tasks. To address these issues, we propose KGCE (Knowledge-Augmented Dual-Graph Evaluator for Cross-Platform Educational Agent Benchmarking with Multimodal Language Models), a novel benchmarking platform that integrates knowledge base enhancement and a dual-graph evaluation framework. We first constructed a dataset comprising 104 education-related tasks, covering Windows, Android, and cross-platform collaborative tasks. KGCE introduces a dual-graph evaluation framework that decomposes tasks into multiple sub-goals and verifies their completion status, providing fine-grained evaluation metrics. To overcome the execution bottlenecks of existing agents in private-domain tasks, we developed an enhanced agent system incorporating a knowledge base specific to school-specific software. The code can be found at https://github.com/Kinginlife/KGCE.

</details>


### [23] [Empowering Small Language Models with Factual Hallucination-Aware Reasoning for Financial Classification](https://arxiv.org/abs/2601.01378)
*Han Yuan,Yilin Wu,Li Zhang,Zheng Ma*

Main category: cs.AI

TL;DR: The paper proposes the AAAI pipeline to reduce factual hallucinations in small language models (SLMs), enhancing their efficiency in financial classification.


<details>
  <summary>Details</summary>
Motivation: The motivation was to address the issue of factual hallucinations negatively affecting the financial classification performance of small language models (SLMs).

Method: The method involves a three-step pipeline called AAAI: Association Identification, Automated Detection, and Adaptive Inference. It focuses on detecting factual hallucinations and providing feedback to improve classification.

Result: Experiments show that factual hallucinations correlate with misclassifications, encoder-based detectors can identify hallucinations effectively, and adaptive feedback improves SLM classification.

Conclusion: The study concludes that the AAAI pipeline helps improve trustworthiness and efficiency of SLMs in financial applications by reducing factual hallucinations and boosting performance.

Abstract: Small language models (SLMs) are increasingly used for financial classification due to their fast inference and local deployability. However, compared with large language models, SLMs are more prone to factual hallucinations in reasoning and exhibit weaker classification performance. This raises a natural question: Can mitigating factual hallucinations improve SLMs' financial classification? To address this, we propose a three-step pipeline named AAAI (Association Identification, Automated Detection, and Adaptive Inference). Experiments on three representative SLMs reveal that: (1) factual hallucinations are positively correlated with misclassifications; (2) encoder-based verifiers effectively detect factual hallucinations; and (3) incorporating feedback on factual errors enables SLMs' adaptive inference that enhances classification performance. We hope this pipeline contributes to trustworthy and effective applications of SLMs in finance.

</details>


### [24] [A construction of an optimal base for conditional attribute and attributional condition implications in triadic contexts](https://arxiv.org/abs/2601.01467)
*Romuald Kwessy Mouona,Blaise Blériot Koguep Njionou,Etienne Romuald Temgoua Alomo,Rokia Missaoui,Leonard Kwuida*

Main category: cs.AI

TL;DR: The paper aims to construct an optimal base for implications in triadic contexts.


<details>
  <summary>Details</summary>
Motivation: To understand and create an optimal foundation for implications in triadic contexts, as introduced by Ganter and Obiedkov.

Method: Analysis and construction of an optimal base for conditional attribute and attributional condition implications.

Result: An optimal base for implications in triadic contexts was successfully constructed.

Conclusion: The study provides a structured approach to conditional attribute and attributional condition implications, enhancing understanding in triadic contexts.

Abstract: This article studies implications in triadic contexts. Specifically, we focus on those introduced by Ganter and Obiedkov, namely conditional attribute and attributional condition implications. Our aim is to construct an optimal base for these implications.

</details>


### [25] [Reading Between the Lines: Deconfounding Causal Estimates using Text Embeddings and Deep Learning](https://arxiv.org/abs/2601.01511)
*Ahmed Dawoud,Osama El-Shamy*

Main category: cs.AI

TL;DR: This paper introduces a novel framework using text embeddings with Neural Network-Enhanced Double Machine Learning (DML) to estimate causal treatment effects, addressing bias from unobserved confounders.


<details>
  <summary>Details</summary>
Motivation: Traditional econometric methods fail to manage selection bias from unobserved confounders in scenarios where text data may provide relevant proxies.

Method: A Neural Network-Enhanced DML framework that integrates text embeddings to capture confounding information usually missed by structured data models. It evaluates performance using synthetic benchmarks.

Result: The proposed method significantly reduces bias (to -0.86%) compared to traditional tree-based DML, which retains substantial bias (+24%), effectively recovering causal parameters.

Conclusion: High-dimensional text data requires deep learning architectures in DML to properly address unconfoundedness assumptions, outperforming traditional methods in causal inference.

Abstract: Estimating causal treatment effects in observational settings is frequently compromised by selection bias arising from unobserved confounders. While traditional econometric methods struggle when these confounders are orthogonal to structured covariates, high-dimensional unstructured text often contains rich proxies for these latent variables. This study proposes a Neural Network-Enhanced Double Machine Learning (DML) framework designed to leverage text embeddings for causal identification. Using a rigorous synthetic benchmark, we demonstrate that unstructured text embeddings capture critical confounding information that is absent from structured tabular data. However, we show that standard tree-based DML estimators retain substantial bias (+24%) due to their inability to model the continuous topology of embedding manifolds. In contrast, our deep learning approach reduces bias to -0.86% with optimized architectures, effectively recovering the ground-truth causal parameter. These findings suggest that deep learning architectures are essential for satisfying the unconfoundedness assumption when conditioning on high-dimensional natural language data

</details>


### [26] [Bayesian Orchestration of Multi-LLM Agents for Cost-Aware Sequential Decision-Making](https://arxiv.org/abs/2601.01522)
*Danial Amin*

Main category: cs.AI

TL;DR: The paper presents a Bayesian multi-LLM framework to minimize costs and improve fairness in asymmetric error cost settings, achieving significant cost reductions and demographic parity improvements in resume screening.


<details>
  <summary>Details</summary>
Motivation: Current LLM usage in decision-making scenarios with asymmetric error costs (e.g., hiring, medical triage) fails to address sequential decision-making with associated costs effectively. This motivates a shift towards a more sophisticated, cost-aware approach.

Method: The authors propose a Bayesian framework that elicits likelihoods from multiple LLMs using contrastive prompts, aggregates them with robust statistics, and updates beliefs using Bayes' rule, enabling cost-aware and principled decision-making.

Result: Testing on a resume screening scenario with five LLMs, the framework achieved a 34% cost reduction ($294,000) and a 45% improvement in demographic parity compared to the best single-LLM system.

Conclusion: The Bayesian multi-LLM orchestration framework greatly improves decision-making in cost-sensitive settings, offering financial and fairness benefits through multi-LLM aggregation, sequential updating, and information gathering.

Abstract: Large language models (LLMs) are increasingly deployed as autonomous decision agents in settings with asymmetric error costs: hiring (missed talent vs wasted interviews), medical triage (missed emergencies vs unnecessary escalation), and fraud detection (approved fraud vs declined legitimate payments). The dominant design queries a single LLM for a posterior over states, thresholds "confidence," and acts; we prove this is inadequate for sequential decisions with costs. We propose a Bayesian, cost-aware multi-LLM orchestration framework that treats LLMs as approximate likelihood models rather than classifiers. For each candidate state, we elicit likelihoods via contrastive prompting, aggregate across diverse models with robust statistics, and update beliefs with Bayes rule under explicit priors as new evidence arrives. This enables coherent belief updating, expected-cost action selection, principled information gathering via value of information, and fairness gains via ensemble bias mitigation. In resume screening with costs of 40000 USD per missed hire, 2500 USD per interview, and 150 USD per phone screen, experiments on 1000 resumes using five LLMs (GPT-4o, Claude 4.5 Sonnet, Gemini Pro, Grok, DeepSeek) reduce total cost by 294000 USD (34 percent) versus the best single-LLM baseline and improve demographic parity by 45 percent (max group gap 22 to 5 percentage points). Ablations attribute 51 percent of savings to multi-LLM aggregation, 43 percent to sequential updating, and 20 percent to disagreement-triggered information gathering, consistent with the theoretical benefits of correct probabilistic foundations.

</details>


### [27] [Aletheia: Quantifying Cognitive Conviction in Reasoning Models via Regularized Inverse Confusion Matrix](https://arxiv.org/abs/2601.01532)
*Fanzhe Fu*

Main category: cs.AI

TL;DR: The paper focuses on evaluating AI reasoning by proposing Project Aletheia, introducing a 'Cognitive Conviction' measure and a method to assess AI's scientific integrity through frameworks like Tikhonov Regularization and Synthetic Proxy Protocol.


<details>
  <summary>Details</summary>
Motivation: Existing AI evaluation metrics emphasize knowledge breadth but fail to consider the depth and quality of reasoning, leaving a gap in assessing true cognitive conviction in AGI systems.

Method: The authors introduce Project Aletheia, which uses Tikhonov Regularization to invert confusion matrices and employs a Synthetic Proxy Protocol for validation without private data reliance. An Aligned Conviction Score (S_aligned) is also proposed.

Result: Initial studies on models like DeepSeek-R1 and OpenAI o1 revealed 'Defensive OverThinking' behavior under pressure but showed that reasoning models act as cognitive buffers without compromising overall safety.

Conclusion: The work introduces a novel framework for assessing AI reasoning depth and safety, providing a foundation to measure AI's scientific integrity and address epistemological challenges in AGI development.

Abstract: In the progressive journey toward Artificial General Intelligence (AGI), current evaluation paradigms face an epistemological crisis. Static benchmarks measure knowledge breadth but fail to quantify the depth of belief. While Simhi et al. (2025) defined the CHOKE phenomenon in standard QA, we extend this framework to quantify "Cognitive Conviction" in System 2 reasoning models. We propose Project Aletheia, a cognitive physics framework that employs Tikhonov Regularization to invert the judge's confusion matrix. To validate this methodology without relying on opaque private data, we implement a Synthetic Proxy Protocol. Our preliminary pilot study on 2025 baselines (e.g., DeepSeek-R1, OpenAI o1) suggests that while reasoning models act as a "cognitive buffer," they may exhibit "Defensive OverThinking" under adversarial pressure. Furthermore, we introduce the Aligned Conviction Score (S_aligned) to verify that conviction does not compromise safety. This work serves as a blueprint for measuring AI scientific integrity.

</details>


### [28] [Improving Behavioral Alignment in LLM Social Simulations via Context Formation and Navigation](https://arxiv.org/abs/2601.01546)
*Letian Kong,Qianran,Jin,Renyu Zhang*

Main category: cs.AI

TL;DR: LLMs often struggle to align with human behavior in complex decision-making settings. The authors propose a two-stage framework to improve alignment and validate its efficacy across various experimental setups.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of LLMs diverging from human decisions in complex environments, especially when anticipating others’ actions and forming beliefs.

Method: A two-stage framework: (1) context formation specifies the decision task and its environment; (2) context navigation guides reasoning for decision-making. The framework is validated through experimental replications.

Result: Validated across different situations, it shows that both stages are essential in complex environments for behavioral alignment, while simpler tasks require only context formation.

Conclusion: The study provides clarity on when and how to apply the proposed framework, offering systematic tools for modeling LLMs as complements to human subjects in behavioral simulations.

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior in experimental settings, but they systematically diverge from human decisions in complex decision-making environments, where participants must anticipate others' actions and form beliefs based on observed behavior. We propose a two-stage framework for improving behavioral alignment. The first stage, context formation, explicitly specifies the experimental design to establish an accurate representation of the decision task and its context. The second stage, context navigation, guides the reasoning process within that representation to make decisions. We validate this framework through a focal replication of a sequential purchasing game with quality signaling (Kremer and Debo, 2016), extending to a crowdfunding game with costly signaling (Cason et al., 2025) and a demand-estimation task (Gui and Toubia, 2025) to test generalizability across decision environments. Across four state-of-the-art (SOTA) models (GPT-4o, GPT-5, Claude-4.0-Sonnet-Thinking, DeepSeek-R1), we find that complex decision-making environments require both stages to achieve behavioral alignment with human benchmarks, whereas the simpler demand-estimation task requires only context formation. Our findings clarify when each stage is necessary and provide a systematic approach for designing and diagnosing LLM social simulations as complements to human subjects in behavioral research.

</details>


### [29] [Logics-STEM: Empowering LLM Reasoning via Failure-Driven Post-Training and Document Knowledge Enhancement](https://arxiv.org/abs/2601.01562)
*Mingyu Xu,Cheng Fang,Keyue Jiang,Yuqian Zheng,Yanghua Xiao,Baojian Zhou,Qifang Zhao,Suhang Zheng,Xiuwen Zhu,Jiyang Tang,Yongchi Zhao,Yijia Luo,Zhiqi Bai,Yuchi Xu,Wenbo Su,Wei Wang,Bing Zhao,Lin Qu,Xiaoxiao Xu*

Main category: cs.AI

TL;DR: Logics-STEM is a top-performing reasoning model fine-tuned on a 10M-scale high-quality dataset, showing a 4.68% performance improvement in STEM benchmarks.


<details>
  <summary>Details</summary>
Motivation: To enhance reasoning in STEM domains using large-scale open-source datasets and advanced fine-tuning techniques.

Method: Combines a high-quality dataset, built using a 5-stage curation engine, with failure-driven post-training frameworks for model optimization.

Result: Logics-STEM achieves superior performance with publicly available models and datasets, demonstrating the impact of data-algorithm co-design.

Conclusion: Combining large-scale open-source data with methodical synthetics and optimization significantly improves reasoning capabilities, advancing open-source research in STEM domains.

Abstract: We present Logics-STEM, a state-of-the-art reasoning model fine-tuned on Logics-STEM-SFT-Dataset, a high-quality and diverse dataset at 10M scale that represents one of the largest-scale open-source long chain-of-thought corpora. Logics-STEM targets reasoning tasks in the domains of Science, Technology, Engineering, and Mathematics (STEM), and exhibits exceptional performance on STEM-related benchmarks with an average improvement of 4.68% over the next-best model at 8B scale. We attribute the gains to our data-algorithm co-design engine, where they are jointly optimized to fit a gold-standard distribution behind reasoning. Data-wise, the Logics-STEM-SFT-Dataset is constructed from a meticulously designed data curation engine with 5 stages to ensure the quality, diversity, and scalability, including annotation, deduplication, decontamination, distillation, and stratified sampling. Algorithm-wise, our failure-driven post-training framework leverages targeted knowledge retrieval and data synthesis around model failure regions in the Supervised Fine-tuning (SFT) stage to effectively guide the second-stage SFT or the reinforcement learning (RL) for better fitting the target distribution. The superior empirical performance of Logics-STEM reveals the vast potential of combining large-scale open-source data with carefully designed synthetic data, underscoring the critical role of data-algorithm co-design in enhancing reasoning capabilities through post-training. We make both the Logics-STEM models (8B and 32B) and the Logics-STEM-SFT-Dataset (10M and downsampled 2.2M versions) publicly available to support future research in the open-source community.

</details>


### [30] [CaveAgent: Transforming LLMs into Stateful Runtime Operators](https://arxiv.org/abs/2601.01569)
*Maohao Ran,Zhenglin Wan,Cooper Lin,Yanting Zhang,Hongyu Xin,Hongwei Fan,Yibo Xu,Beier Luo,Yaxin Zhou,Wangbo Zhao,Lijie Yang,Lang Feng,Fuchao Yang,Jingxuan Wu,Yiqiao Huang,Chendong Ma,Dailing Jiang,Jianbo Deng,Sihui Han,Bo An,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: The paper introduces CaveAgent, a framework enhancing LLM-based systems by moving beyond text-centric paradigms to a runtime-operating approach, optimizing performance in complex, multi-turn tasks.


<details>
  <summary>Details</summary>
Motivation: Current agentic systems based on text-centric paradigms struggle with multi-turn dependencies and context drift in long-horizon tasks. A novel approach is needed to improve efficiency and persistence in such systems.

Method: The proposed CaveAgent introduces a Dual-stream Context Architecture for decoupling state management and Stateful Runtime Management to efficiently handle interdependent sub-tasks and ensure high-fidelity memory persistence through Python Runtime streams.

Result: CaveAgent showed a 10.5% improvement in success rate on retail tasks, reduced token consumption by 28.4% in multi-turn scenarios, and decreased token use by 59% for data-intensive tasks, proving its capability to handle large-scale data.

Conclusion: By transforming LLMs from text generators to runtime operators, CaveAgent enhances task execution efficiency, eliminates context drift, and supports persistent data handling, showcasing significant improvements over traditional approaches.

Abstract: LLM-based agents are increasingly capable of complex task execution, yet current agentic systems remain constrained by text-centric paradigms. Traditional approaches rely on procedural JSON-based function calling, which often struggles with long-horizon tasks due to fragile multi-turn dependencies and context drift. In this paper, we present CaveAgent, a framework that transforms the paradigm from "LLM-as-Text-Generator" to "LLM-as-Runtime-Operator." We introduce a Dual-stream Context Architecture that decouples state management into a lightweight semantic stream for reasoning and a persistent, deterministic Python Runtime stream for execution. In addition to leveraging code generation to efficiently resolve interdependent sub-tasks (e.g., loops, conditionals) in a single step, we introduce \textit{Stateful Runtime Management} in CaveAgent. Distinct from existing code-based approaches that remain text-bound and lack the support for external object injection and retrieval, CaveAgent injects, manipulates, and retrieves complex Python objects (e.g., DataFrames, database connections) that persist across turns. This persistence mechanism acts as a high-fidelity external memory to eliminate context drift, avoid catastrophic forgetting, while ensuring that processed data flows losslessly to downstream applications. Comprehensive evaluations on Tau$^2$-bench, BFCL and various case studies across representative SOTA LLMs demonstrate CaveAgent's superiority. Specifically, our framework achieves a 10.5\% success rate improvement on retail tasks and reduces total token consumption by 28.4\% in multi-turn scenarios. On data-intensive tasks, direct variable storage and retrieval reduces token consumption by 59\%, allowing CaveAgent to handle large-scale data that causes context overflow failures in both JSON-based and Code-based agents.

</details>


### [31] [Structured Decomposition for LLM Reasoning: Cross-Domain Validation and Semantic Web Integration](https://arxiv.org/abs/2601.01609)
*Albert Sadowski,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: The paper introduces a framework that combines Large Language Models (LLMs) and symbolic systems to apply rule-based reasoning to natural language inputs effectively, with validation across multiple domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a system that can apply rules to natural language inputs with both interpretive flexibility and formal guarantees, addressing challenges in domains like law, science, and clinical protocols.

Method: The authors use LLMs to populate ontologies and symbolic systems (SWRL-based reasoners) for deterministic rule application. The reasoning process decomposes into entity identification, assertion extraction, and symbolic verification, grounded in OWL 2 ontologies. Experiments with 11 language models validate the approach.

Result: Experiments across three domains showed improved reasoning over few-shot prompting using LLMs alone. Structured decomposition and symbolic verification provided significant added benefits.

Conclusion: This framework effectively integrates the strengths of LLMs and symbolic systems, ensuring flexible yet consistent rule application. It enables richer inference and integration with semantic web tools for inspection and querying.

Abstract: Rule-based reasoning over natural language input arises in domains where decisions must be auditable and justifiable: clinical protocols specify eligibility criteria in prose, evidence rules define admissibility through textual conditions, and scientific standards dictate methodological requirements. Applying rules to such inputs demands both interpretive flexibility and formal guarantees. Large language models (LLMs) provide flexibility but cannot ensure consistent rule application; symbolic systems provide guarantees but require structured input. This paper presents an integration pattern that combines these strengths: LLMs serve as ontology population engines, translating unstructured text into ABox assertions according to expert-authored TBox specifications, while SWRL-based reasoners apply rules with deterministic guarantees. The framework decomposes reasoning into entity identification, assertion extraction, and symbolic verification, with task definitions grounded in OWL 2 ontologies. Experiments across three domains (legal hearsay determination, scientific method-task application, clinical trial eligibility) and eleven language models validate the approach. Structured decomposition achieves statistically significant improvements over few-shot prompting in aggregate, with gains observed across all three domains. An ablation study confirms that symbolic verification provides substantial benefit beyond structured prompting alone. The populated ABox integrates with standard semantic web tooling for inspection and querying, positioning the framework for richer inference patterns that simpler formalisms cannot express.

</details>


### [32] [A New Benchmark for the Appropriate Evaluation of RTL Code Optimization](https://arxiv.org/abs/2601.01765)
*Yao Lu,Shang Liu,Hangan Zhou,Wenji Fang,Qijun Zhang,Zhiyao Xie*

Main category: cs.AI

TL;DR: The paper introduces RTL-OPT, a benchmark intended to evaluate how well large language models can optimize RTL code regarding power, performance, and area (PPA).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current benchmarks, which focus only on syntactic correctness of RTL code generation by large language models, and not on optimization quality in PPA.

Method: The authors developed RTL-OPT, comprising 36 handcrafted digital designs spanning multiple categories of hardware design. It includes automated evaluation tools to verify correctness and measure PPA improvements.

Result: They created a standardized mechanism to evaluate generative models for RTL optimization tasks, highlighting industry-relevant code optimization patterns.

Conclusion: RTL-OPT enables meaningful assessment of LLMs' ability to optimize RTL code, advancing the integration of AI in hardware design optimization.

Abstract: The rapid progress of artificial intelligence increasingly relies on efficient integrated circuit (IC) design. Recent studies have explored the use of large language models (LLMs) for generating Register Transfer Level (RTL) code, but existing benchmarks mainly evaluate syntactic correctness rather than optimization quality in terms of power, performance, and area (PPA). This work introduces RTL-OPT, a benchmark for assessing the capability of LLMs in RTL optimization. RTL-OPT contains 36 handcrafted digital designs that cover diverse implementation categories including combinational logic, pipelined datapaths, finite state machines, and memory interfaces. Each task provides a pair of RTL codes, a suboptimal version and a human-optimized reference that reflects industry-proven optimization patterns not captured by conventional synthesis tools. Furthermore, RTL-OPT integrates an automated evaluation framework to verify functional correctness and quantify PPA improvements, enabling standardized and meaningful assessment of generative models for hardware design optimization.

</details>


### [33] [Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications](https://arxiv.org/abs/2601.01718)
*YuanLab. ai,:,Shawn Wu,Sean Wang,Louie Li,Darcy Chen,Allen Wang,Jiangang Luo,Xudong Zhao,Joseph Shen,Gawain Ma,Jasper Jia,Marcus Mao,Claire Wang,Hunter He,Carol Wang,Zera Zhang,Jason Wang,Chonly Shen,Leo Zhang,Logan Chen,Qasim Meng,James Gong,Danied Zhao,Penn Zheng,Owen Zhu,Tong Yu*

Main category: cs.AI

TL;DR: The paper presents Yuan3.0 Flash, an open-source MultiModal Large Language Model with 3.7B activated and 40B total parameters, excelling in enterprise tasks and addressing overthinking through a novel training algorithm called RAPO.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and effectiveness of large language models, particularly for enterprise-oriented tasks, while tackling the issue of overthinking in large reasoning models.

Method: The study introduces Yuan3.0 Flash, leveraging a Mixture-of-Experts architecture, paired with Reflection-aware Adaptive Policy Optimization (RAPO), a new reinforcement learning algorithm to manage overthinking behaviors.

Result: Yuan3.0 Flash outperforms on enterprise objectives like RAG, table understanding, and summarization, while demonstrating competitive reasoning in STEM fields, achieving equivalent accuracy to leading models but using fewer tokens.

Conclusion: Yuan3.0 Flash represents a robust contribution to the language model field, excelling in enterprise-centric applications and reasoning while being made fully open-source to support further research and application.

Abstract: We introduce Yuan3.0 Flash, an open-source Mixture-of-Experts (MoE) MultiModal Large Language Model featuring 3.7B activated parameters and 40B total parameters, specifically designed to enhance performance on enterprise-oriented tasks while maintaining competitive capabilities on general-purpose tasks. To address the overthinking phenomenon commonly observed in Large Reasoning Models (LRMs), we propose Reflection-aware Adaptive Policy Optimization (RAPO), a novel RL training algorithm that effectively regulates overthinking behaviors. In enterprise-oriented tasks such as retrieval-augmented generation (RAG), complex table understanding, and summarization, Yuan3.0 Flash consistently achieves superior performance. Moreover, it also demonstrates strong reasoning capabilities in domains such as mathematics, science, etc., attaining accuracy comparable to frontier model while requiring only approximately 1/4 to 1/2 of the average tokens. Yuan3.0 Flash has been fully open-sourced to facilitate further research and real-world deployment: https://github.com/Yuan-lab-LLM/Yuan3.0.

</details>


### [34] [AI Agent Systems: Architectures, Applications, and Evaluation](https://arxiv.org/abs/2601.01743)
*Bin Xu*

Main category: cs.AI

TL;DR: This paper surveys the development and architecture of AI agents, combining natural-language processing with complex reasoning, planning, and tool usage.


<details>
  <summary>Details</summary>
Motivation: To synthesize the emerging landscape of AI agent architectures and provide a cohesive taxonomy and discussion of associated challenges.

Method: The paper organizes prior work into a taxonomy of components, orchestration patterns, deployment settings, and discusses evaluation challenges and practices.

Result: It identifies key trade-offs, design considerations, and benchmarking practices, while highlighting open challenges like scalable memory management and reliable evaluation.

Conclusion: This work provides a structured overview to guide future research and development in AI agents, emphasizing challenges in reliability, scalability, and interpretability.

Abstract: AI agents -- systems that combine foundation models with reasoning, planning, memory, and tool use -- are rapidly becoming a practical interface between natural-language intent and real-world computation. This survey synthesizes the emerging landscape of AI agent architectures across: (i) deliberation and reasoning (e.g., chain-of-thought-style decomposition, self-reflection and verification, and constraint-aware decision making), (ii) planning and control (from reactive policies to hierarchical and multi-step planners), and (iii) tool calling and environment interaction (retrieval, code execution, APIs, and multimodal perception). We organize prior work into a unified taxonomy spanning agent components (policy/LLM core, memory, world models, planners, tool routers, and critics), orchestration patterns (single-agent vs.\ multi-agent; centralized vs.\ decentralized coordination), and deployment settings (offline analysis vs.\ online interactive assistance; safety-critical vs.\ open-ended tasks). We discuss key design trade-offs -- latency vs.\ accuracy, autonomy vs.\ controllability, and capability vs.\ reliability -- and highlight how evaluation is complicated by non-determinism, long-horizon credit assignment, tool and environment variability, and hidden costs such as retries and context growth. Finally, we summarize measurement and benchmarking practices (task suites, human preference and utility metrics, success under constraints, robustness and security) and identify open challenges including verification and guardrails for tool actions, scalable memory and context management, interpretability of agent decisions, and reproducible evaluation under realistic workloads.

</details>


### [35] [Can Large Language Models Solve Engineering Equations? A Systematic Comparison of Direct Prediction and Solver-Assisted Approaches](https://arxiv.org/abs/2601.01774)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) are evaluated for solving transcendental equations through direct prediction and hybrid solver-assisted computation in engineering domains.


<details>
  <summary>Details</summary>
Motivation: To assess whether Large Language Models can solve transcendental equations directly or perform better when paired with classical iterative solvers.

Method: Six state-of-the-art LLMs are tested on 100 problems across seven engineering domains, comparing direct numerical prediction with a hybrid approach using LLM symbolic manipulation and Newton-Raphson iterative solvers.

Result: Direct prediction yields errors of 0.765 to 1.262, while hybrid computation reduces errors to 0.225-0.301, showing significant improvements in most domains, especially Electronics.

Conclusion: LLMs are effective at symbolic manipulation and domain knowledge retrieval but lack precision for iterative arithmetic, making them best suited as interfaces to classical solvers.

Abstract: Transcendental equations requiring iterative numerical solution pervade engineering practice, from fluid mechanics friction factor calculations to orbital position determination. We systematically evaluate whether Large Language Models can solve these equations through direct numerical prediction or whether a hybrid architecture combining LLM symbolic manipulation with classical iterative solvers proves more effective. Testing six state-of-the-art models (GPT-5.1, GPT-5.2, Gemini-3-Flash, Gemini-2.5-Lite, Claude-Sonnet-4.5, Claude-Opus-4.5) on 100 problems spanning seven engineering domains, we compare direct prediction against solver-assisted computation where LLMs formulate governing equations and provide initial conditions while Newton-Raphson iteration performs numerical solution. Direct prediction yields mean relative errors of 0.765 to 1.262 across models, while solver-assisted computation achieves 0.225 to 0.301, representing error reductions of 67.9% to 81.8%. Domain-specific analysis reveals dramatic improvements in Electronics (93.1%) due to exponential equation sensitivity, contrasted with modest gains in Fluid Mechanics (7.2%) where LLMs exhibit effective pattern recognition. These findings establish that contemporary LLMs excel at symbolic manipulation and domain knowledge retrieval but struggle with precision-critical iterative arithmetic, suggesting their optimal deployment as intelligent interfaces to classical numerical solvers rather than standalone computational engines.

</details>


### [36] [PsychEval: A Multi-Session and Multi-Therapy Benchmark for High-Realism and Comprehensive AI Psychological Counselor](https://arxiv.org/abs/2601.01802)
*Qianjun Pan,Junyi Wang,Jie Zhou,Yutao Yang,Junsong Li,Kaiyin Xu,Yougen Zhou,Yihan Li,Jingyuan Zhao,Qin Chen,Ningning Zhou,Kai Chen,Liang He*

Main category: cs.AI

TL;DR: PsychEval introduces a benchmark for developing AI counselors using multi-session data, covering diverse therapeutic approaches and establishing evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: The paper aims to create a reliable AI counselor for psychological assessments, addressing challenges in realistic, multi-therapy modalities and evaluation approaches.

Method: PsychEval incorporates a multi-session benchmark, diverse therapeutic modalities, annotated skill datasets, evaluation metrics, and a reinforcement learning environment for self-evolutionary training.

Result: Experimental analysis confirms the high quality and clinical fidelity of the PsychEval dataset, making it a robust tool for training adaptive AI counselors.

Conclusion: PsychEval sets a foundation for advancing AI counseling, emphasizing realism, adaptability, and evaluation in psychological assessments.

Abstract: To develop a reliable AI for psychological assessment, we introduce \texttt{PsychEval}, a multi-session, multi-therapy, and highly realistic benchmark designed to address three key challenges: \textbf{1) Can we train a highly realistic AI counselor?} Realistic counseling is a longitudinal task requiring sustained memory and dynamic goal tracking. We propose a multi-session benchmark (spanning 6-10 sessions across three distinct stages) that demands critical capabilities such as memory continuity, adaptive reasoning, and longitudinal planning. The dataset is annotated with extensive professional skills, comprising over 677 meta-skills and 4577 atomic skills. \textbf{2) How to train a multi-therapy AI counselor?} While existing models often focus on a single therapy, complex cases frequently require flexible strategies among various therapies. We construct a diverse dataset covering five therapeutic modalities (Psychodynamic, Behaviorism, CBT, Humanistic Existentialist, and Postmodernist) alongside an integrative therapy with a unified three-stage clinical framework across six core psychological topics. \textbf{3) How to systematically evaluate an AI counselor?} We establish a holistic evaluation framework with 18 therapy-specific and therapy-shared metrics across Client-Level and Counselor-Level dimensions. To support this, we also construct over 2,000 diverse client profiles. Extensive experimental analysis fully validates the superior quality and clinical fidelity of our dataset. Crucially, \texttt{PsychEval} transcends static benchmarking to serve as a high-fidelity reinforcement learning environment that enables the self-evolutionary training of clinically responsible and adaptive AI counselors.

</details>


### [37] [Admissibility Alignment](https://arxiv.org/abs/2601.01816)
*Chris Duffey*

Main category: cs.AI

TL;DR: The paper introduces a reframed approach to AI alignment focusing on action and decision selection under uncertainty using distributional evaluations. It proposes MAP-AI, an architecture for aligned decision-making based on probabilistic reasoning.


<details>
  <summary>Details</summary>
Motivation: The current AI alignment methods are often treated as static or binary, and this can be limiting when dealing with uncertainty and complex outcomes. The paper seeks to make alignment a dynamic, probabilistic quality applicable to decision-making under uncertainty.

Method: MAP-AI (Monte Carlo Alignment for Policy) is introduced as a new system architecture. It uses Monte Carlo estimation of outcomes to enforce alignment and evaluates policies based on distributional properties like expected utility and variance, rather than static accuracy metrics.

Result: MAP-AI provides a framework for governing AI systems by assessing alignment through probabilistic measures, enabling decision-making that adapts to uncertainty and avoids relying on static model constraints. It integrates evaluation into decision-making processes directly.

Conclusion: The approach operationalizes alignment as a decision-theoretic property, offering practical tools for governing AI systems under uncertainty and ensuring policy behavior aligns with intended outcomes without needing to retrain models.

Abstract: This paper introduces Admissibility Alignment: a reframing of AI alignment as a property of admissible action and decision selection over distributions of outcomes under uncertainty, evaluated through the behavior of candidate policies. We present MAP-AI (Monte Carlo Alignment for Policy) as a canonical system architecture for operationalizing admissibility alignment, formalizing alignment as a probabilistic, decision-theoretic property rather than a static or binary condition.
  MAP-AI, a new control-plane system architecture for aligned decision-making under uncertainty, enforces alignment through Monte Carlo estimation of outcome distributions and admissibility-controlled policy selection rather than static model-level constraints. The framework evaluates decision policies across ensembles of plausible futures, explicitly modeling uncertainty, intervention effects, value ambiguity, and governance constraints. Alignment is assessed through distributional properties including expected utility, variance, tail risk, and probability of misalignment rather than accuracy or ranking performance. This approach distinguishes probabilistic prediction from decision reasoning under uncertainty and provides an executable methodology for evaluating trust and alignment in enterprise and institutional AI systems. The result is a practical foundation for governing AI systems whose impact is determined not by individual forecasts, but by policy behavior across distributions and tail events. Finally, we show how distributional alignment evaluation can be integrated into decision-making itself, yielding an admissibility-controlled action selection mechanism that alters policy behavior under uncertainty without retraining or modifying underlying models.

</details>


### [38] [COMPASS: A Framework for Evaluating Organization-Specific Policy Alignment in LLMs](https://arxiv.org/abs/2601.01836)
*Dasol Choi,DongGeon Lee,Brigitta Jesica Kartono,Helena Berndt,Taeyoun Kwon,Joonwon Jang,Haon Park,Hwanjo Yu,Minsuk Kahng*

Main category: cs.AI

TL;DR: The study introduces COMPASS, a framework to evaluate LLM compliance with organizational policies, finding significant failures in enforcing denylist policies.


<details>
  <summary>Details</summary>
Motivation: To address the gap in evaluating LLM adherence to specific organizational policies in high-stakes applications, where universal harm evaluations fall short.

Method: A systematic framework (COMPASS) was developed and applied in eight industry scenarios using 5,920 tailored queries, assessing both routine and adversarial compliance of seven LLMs.

Result: While LLMs achieve over 95% accuracy on legitimate queries, their performance on enforcing denylist violations is severely lacking, with refusal rates as low as 13-40%.

Conclusion: Current LLMs are not robust enough for policy-critical applications, establishing a need for evaluation frameworks like COMPASS in organizational AI safety.

Abstract: As large language models are deployed in high-stakes enterprise applications, from healthcare to finance, ensuring adherence to organization-specific policies has become essential. Yet existing safety evaluations focus exclusively on universal harms. We present COMPASS (Company/Organization Policy Alignment Assessment), the first systematic framework for evaluating whether LLMs comply with organizational allowlist and denylist policies. We apply COMPASS to eight diverse industry scenarios, generating and validating 5,920 queries that test both routine compliance and adversarial robustness through strategically designed edge cases. Evaluating seven state-of-the-art models, we uncover a fundamental asymmetry: models reliably handle legitimate requests (>95% accuracy) but catastrophically fail at enforcing prohibitions, refusing only 13-40% of adversarial denylist violations. These results demonstrate that current LLMs lack the robustness required for policy-critical deployments, establishing COMPASS as an essential evaluation framework for organizational AI safety.

</details>


### [39] [Clinical Knowledge Graph Construction and Evaluation with Multi-LLMs via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01844)
*Udiptaman Das,Krishnasai B. Atmakuri,Duy Ho,Chi Lee,Yugyung Lee*

Main category: cs.AI

TL;DR: This paper introduces a novel end-to-end framework for constructing clinical knowledge graphs (KGs) from unstructured text in oncology using multi-agent prompting and schema-constrained Retrieval-Augmented Generation (KG-RAG).


<details>
  <summary>Details</summary>
Motivation: Existing methods for knowledge graph construction from clinical narratives often use structured input and fail to validate factual accuracy and semantic consistency, posing significant challenges in the oncology domain.

Method: The paper presents a pipeline integrating prompt-driven text extraction, entropy-based uncertainty scoring, ontology-aligned schema generation (RDF/OWL), and multi-LLM consensus validation to build and refine clinical KGs continuously.

Result: The proposed method produces SPARQL-compatible, clinically relevant knowledge graphs with gains in precision, relevance, and ontology compliance when applied to PDAC and BRCA oncology datasets, without needing gold-standard annotations.

Conclusion: This framework offers an interpretable and iterative approach to constructing high-quality clinical KGs directly from unstructured clinical text while addressing issues like hallucination detection and semantic refinement.

Abstract: Large language models (LLMs) offer new opportunities for constructing knowledge graphs (KGs) from unstructured clinical narratives. However, existing approaches often rely on structured inputs and lack robust validation of factual accuracy and semantic consistency, limitations that are especially problematic in oncology. We introduce an end-to-end framework for clinical KG construction and evaluation directly from free text using multi-agent prompting and a schema-constrained Retrieval-Augmented Generation (KG-RAG) strategy. Our pipeline integrates (1) prompt-driven entity, attribute, and relation extraction; (2) entropy-based uncertainty scoring; (3) ontology-aligned RDF/OWL schema generation; and (4) multi-LLM consensus validation for hallucination detection and semantic refinement. Beyond static graph construction, the framework supports continuous refinement and self-supervised evaluation, enabling iterative improvement of graph quality. Applied to two oncology cohorts (PDAC and BRCA), our method produces interpretable, SPARQL-compatible, and clinically grounded knowledge graphs without relying on gold-standard annotations. Experimental results demonstrate consistent gains in precision, relevance, and ontology compliance over baseline methods.

</details>


### [40] [Jenius Agent: Towards Experience-Driven Accuracy Optimization in Real-World Scenarios](https://arxiv.org/abs/2601.01857)
*Defei Xia,Bingfeng Pi,Shenbin Zhang,Song Hua,Yunfei Wei,Lei Zuo*

Main category: cs.AI

TL;DR: The paper introduces Jenius-Agent, an optimized autonomous agent framework based on large language models, improving task accuracy by 20%, token cost, response latency, and invocation failures.


<details>
  <summary>Details</summary>
Motivation: Enhancing the performance of LLM-based autonomous agents, especially in context understanding, tool usage, and response generation.

Method: Integrates three innovations: adaptive prompt generation, context-aware tool orchestration, and layered memory mechanism to optimize reasoning and tool-use pipelines.

Result: Achieved 20% improvement in task accuracy and reductions in token cost, response latency, and invocation failures.

Conclusion: The Jenius-Agent framework provides a practical, scalable solution to develop reliable autonomous agents, effectively enhancing performance through innovative optimizations.

Abstract: As agent systems powered by large language models (LLMs) advance, improving the task performance of an autonomous agent, especially in context understanding, tool usage, and response generation, has become increasingly critical. Although prior studies have advanced the overall design of LLM-based agents, systematic optimization of their internal reasoning and tool-use pipelines remains underexplored. This paper introduces an agent framework grounded in real-world practical experience, with three key innovations: (1) an adaptive prompt generation strategy that aligns with the agent's state and task goals to improve reliability and robustness; (2) a context-aware tool orchestration module that performs tool categorization, semantic retrieval, and adaptive invocation based on user intent and context; and (3) a layered memory mechanism that integrates session memory, task history, and external summaries to improve relevance and efficiency through dynamic summarization and compression. An end-to-end framework named Jenius-Agent has been integrated with three key optimizations, including tools based on the Model Context Protocol (MCP), file input/output (I/O), and execution feedback. The experiments show a 20 percent improvement in task accuracy, along with a reduced token cost, response latency, and invocation failures. The framework is already deployed in Jenius (https://www.jenius.cn), providing a lightweight and scalable solution for robust, protocol-compatible autonomous agents.

</details>


### [41] [Toward Auditable Neuro-Symbolic Reasoning in Pathology: SQL as an Explicit Trace of Evidence](https://arxiv.org/abs/2601.01875)
*Kewen Cao,Jianxu Chen,Yongbing Zhang,Ye Zhang,Hongxiao Wang*

Main category: cs.AI

TL;DR: This study introduces an SQL-centered framework enhancing interpretability and traceability in automated pathology image analysis.


<details>
  <summary>Details</summary>
Motivation: Conventional vision-language models struggle with verifiable and explainable decision-making in pathology image analysis, leaving clinicians questioning model outputs.

Method: The authors propose SQL-based agents that perform auditable cellular feature reasoning and knowledge comparison, inspired by pathologists' diagnostic reasoning.

Result: The system demonstrated improved interpretability, traceability, and performance on two pathology visual question answering datasets.

Conclusion: Integrating SQL-based reasoning enhances model transparency and aligns diagnostic outputs with measurable findings, aiding reliable pathology analysis.

Abstract: Automated pathology image analysis is central to clinical diagnosis, but clinicians still ask which slide features drive a model's decision and why. Vision-language models can produce natural language explanations, but these are often correlational and lack verifiable evidence. In this paper, we introduce an SQL-centered agentic framework that enables both feature measurement and reasoning to be auditable. Specifically, after extracting human-interpretable cellular features, Feature Reasoning Agents compose and execute SQL queries over feature tables to aggregate visual evidence into quantitative findings. A Knowledge Comparison Agent then evaluates these findings against established pathological knowledge, mirroring how pathologists justify diagnoses from measurable observations. Extensive experiments evaluated on two pathology visual question answering datasets demonstrate our method improves interpretability and decision traceability while producing executable SQL traces that link cellular measurements to diagnostic conclusions.

</details>


### [42] [Theory Trace Card: Theory-Driven Socio-Cognitive Evaluation of LLMs](https://arxiv.org/abs/2601.01878)
*Farzan Karimi-Malekabadi,Suhaib Abdurahman,Zhivar Sourati,Jackson Trager,Morteza Dehghani*

Main category: cs.AI

TL;DR: The paper critiques existing socio-cognitive benchmarks for LLMs for lacking a theoretical foundation, leading to overgeneralized conclusions. It introduces the Theory Trace Card (TTC) to improve evaluation documentation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap between benchmark performance of LLMs and their real-world behavior, which arises from a lack of theoretical grounding in socio-cognitive evaluations.

Method: The authors formalize the issue of missing theoretical grounding and propose the Theory Trace Card (TTC), a documentation framework outlining the theoretical basis, target capability components, operationalization, and limitations of evaluations.

Result: The proposed TTC framework helps diagnose flaws in socio-cognitive evaluations and enhances the interpretability and reliability of benchmarks without altering existing methodologies.

Conclusion: The Theory Trace Card provides a structured way to document and clarify socio-cognitive evaluations, offering a solution to bridge the gap between benchmark results and real-world capabilities of LLMs.

Abstract: Socio-cognitive benchmarks for large language models (LLMs) often fail to predict real-world behavior, even when models achieve high benchmark scores. Prior work has attributed this evaluation-deployment gap to problems of measurement and validity. While these critiques are insightful, we argue that they overlook a more fundamental issue: many socio-cognitive evaluations proceed without an explicit theoretical specification of the target capability, leaving the assumptions linking task performance to competence implicit. Without this theoretical grounding, benchmarks that exercise only narrow subsets of a capability are routinely misinterpreted as evidence of broad competence: a gap that creates a systemic validity illusion by masking the failure to evaluate the capability's other essential dimensions. To address this gap, we make two contributions. First, we diagnose and formalize this theory gap as a foundational failure that undermines measurement and enables systematic overgeneralization of benchmark results. Second, we introduce the Theory Trace Card (TTC), a lightweight documentation artifact designed to accompany socio-cognitive evaluations, which explicitly outlines the theoretical basis of an evaluation, the components of the target capability it exercises, its operationalization, and its limitations. We argue that TTCs enhance the interpretability and reuse of socio-cognitive evaluations by making explicit the full validity chain, which links theory, task operationalization, scoring, and limitations, without modifying benchmarks or requiring agreement on a single theory.

</details>


### [43] [MMP-A*: Multimodal Perception Enhanced Incremental Heuristic Search on Path Planning](https://arxiv.org/abs/2601.01910)
*Minh Hieu Ha,Khanh Ly Ta,Hung Phan,Tung Doan,Tung Dao,Dao Tran,Huynh Thi Thanh Binh*

Main category: cs.AI

TL;DR: MMP-A* combines vision-language spatial reasoning with a new adaptive decay mechanism for efficient and geometrically valid path planning in complex environments.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gap between global reasoning and geometric precision in autonomous path planning, as classical methods like A* are computationally expensive and text-based models lack spatial grounding.

Method: The authors propose MMP-A*, a multimodal framework that integrates vision-language models for spatial grounding and introduces an adaptive decay mechanism to refine waypoint guidance and reduce memory costs.

Result: Experimental evaluation demonstrates that MMP-A* performs near-optimally while significantly reducing operational costs in challenging, cluttered, and topologically complex environments.

Conclusion: MMP-A* offers a novel approach to autonomous navigation by combining perception-grounded reasoning with computational efficiency, overcoming the limitations of existing methods.

Abstract: Autonomous path planning requires a synergy between global reasoning and geometric precision, especially in complex or cluttered environments. While classical A* is valued for its optimality, it incurs prohibitive computational and memory costs in large-scale scenarios. Recent attempts to mitigate these limitations by using Large Language Models for waypoint guidance remain insufficient, as they rely only on text-based reasoning without spatial grounding. As a result, such models often produce incorrect waypoints in topologically complex environments with dead ends, and lack the perceptual capacity to interpret ambiguous physical boundaries. These inconsistencies lead to costly corrective expansions and undermine the intended computational efficiency.
  We introduce MMP-A*, a multimodal framework that integrates the spatial grounding capabilities of vision-language models with a novel adaptive decay mechanism. By anchoring high-level reasoning in physical geometry, the framework produces coherent waypoint guidance that addresses the limitations of text-only planners. The adaptive decay mechanism dynamically regulates the influence of uncertain waypoints within the heuristic, ensuring geometric validity while substantially reducing memory overhead. To evaluate robustness, we test the framework in challenging environments characterized by severe clutter and topological complexity. Experimental results show that MMP-A* achieves near-optimal trajectories with significantly reduced operational costs, demonstrating its potential as a perception-grounded and computationally efficient paradigm for autonomous navigation.

</details>


### [44] [OpenSocInt: A Multi-modal Training Environment for Human-Aware Social Navigation](https://arxiv.org/abs/2601.01939)
*Victor Sanchez,Chris Reinke,Ahamed Mohamed,Xavier Alameda-Pineda*

Main category: cs.AI

TL;DR: OpenSocInt is an open-source software package for simulating multi-modal social interactions and training social agents.


<details>
  <summary>Details</summary>
Motivation: To provide a modular and open-source framework facilitating the development and testing of social interaction agents, with an emphasis on social navigation.

Method: The paper describes a modular software package and its utility demonstrated through experimental protocols in social navigation tasks.

Result: The framework allows the exploration of different perceptual features, their encoding, and fusion using varied agents, and has been made publicly available.

Conclusion: OpenSocInt offers a flexible and accessible tool for advancing research in social interaction simulations and agent training with publicly shared resources.

Abstract: In this paper, we introduce OpenSocInt, an open-source software package providing a simulator for multi-modal social interactions and a modular architecture to train social agents. We described the software package and showcased its interest via an experimental protocol based on the task of social navigation. Our framework allows for exploring the use of different perceptual features, their encoding and fusion, as well as the use of different agents. The software is already publicly available under GPL at https://gitlab.inria.fr/robotlearn/OpenSocInt/.

</details>


### [45] [CNC-TP: Classifier Nominal Concept Based on Top-Pertinent Attributes](https://arxiv.org/abs/2601.01976)
*Yasmine Souissi,Fabrice Boissier,Nida Meddouri*

Main category: cs.AI

TL;DR: This paper reviews FCA-based classifiers, focusing on methods to compute closure operators and proposing a new approach for partial concept lattice construction.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for improved interpretability and explainability in classification through FCA-based methods.

Method: A state-of-the-art review of FCA-based classifiers is conducted, focusing on computing closure operators and introducing a novel method for constructing a partial concept lattice.

Result: Experimental results validate the efficiency of the proposed method for constructing partial concept lattices.

Conclusion: The novel approach in FCA-based classification demonstrates potential for improved relevance and efficiency in extracting concepts from datasets.

Abstract: Knowledge Discovery in Databases (KDD) aims to exploit the vast amounts of data generated daily across various domains of computer applications. Its objective is to extract hidden and meaningful knowledge from datasets through a structured process comprising several key steps: data selection, preprocessing, transformation, data mining, and visualization. Among the core data mining techniques are classification and clustering. Classification involves predicting the class of new instances using a classifier trained on labeled data. Several approaches have been proposed in the literature, including Decision Tree Induction, Bayesian classifiers, Nearest Neighbor search, Neural Networks, Support Vector Machines, and Formal Concept Analysis (FCA). The last one is recognized as an effective approach for interpretable and explainable learning. It is grounded in the mathematical structure of the concept lattice, which enables the generation of formal concepts and the discovery of hidden relationships among them. In this paper, we present a state-of-theart review of FCA-based classifiers. We explore various methods for computing closure operators from nominal data and introduce a novel approach for constructing a partial concept lattice that focuses on the most relevant concepts. Experimental results are provided to demonstrate the efficiency of the proposed method.

</details>


### [46] [ChaosBench-Logic: A Benchmark for Logical and Symbolic Reasoning on Chaotic Dynamical Systems](https://arxiv.org/abs/2601.01982)
*Noel Thomas*

Main category: cs.AI

TL;DR: The paper introduces ChaosBench-Logic, a benchmark for evaluating LLM reasoning on chaotic dynamical systems using a first-order logic ontology and various metrics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of LLMs being brittle in logical and symbolic reasoning, especially in chaotic yet deterministic systems.

Method: Developed ChaosBench-Logic with 30 diverse dynamical systems annotated using first-order logic. Generated 621 questions in 7 reasoning categories and defined metrics for logical evaluation.

Result: Frontier LLMs achieved high per-item accuracy (91-94%) but failed on compositional reasoning and showed fragile dialogue coherence. Dialogue accuracy ranged from 53.1% to 75.5%.

Conclusion: ChaosBench-Logic provides a robust framework for improving scientific reasoning in LLMs, highlighting areas such as compositional reasoning and dialogue coherence for advancement.

Abstract: Large language models (LLMs) excel at natural language tasks but remain brittle in domains requiring precise logical and symbolic reasoning. Chaotic dynamical systems provide an especially demanding test because chaos is deterministic yet often misinterpreted as randomness or complexity. We introduce ChaosBench-Logic, a benchmark that evaluates LLM reasoning across 30 diverse dynamical systems using a unified first-order logic (FOL) ontology. Each system is annotated with truth assignments for 11 semantic predicates, and 621 questions are generated across seven reasoning categories, including multi-hop implications, cross-system analogies, counterfactual reasoning, bias probes, and multi-turn dialogues. We define metrics for logical accuracy, implication consistency, dialogue coherence, and contradiction, and we release an open-source evaluation pipeline. Initial experiments show that frontier LLMs such as GPT-4, Claude 3.5 Sonnet, Gemini 2.5 Flash, and the open-source LLaMA-3 70B achieve 91-94% per-item accuracy, yet still score 0% on compositional items and exhibit fragile global coherence. Dialogue-level accuracy ranges from 53.1% (GPT-4 CoT) to 75.5% (LLaMA-3 zero-shot). ChaosBench-Logic provides a rigorous testbed for diagnosing such failures and a foundation for developing neuro-symbolic approaches that improve scientific reasoning in LLMs.

</details>


### [47] [MindChat: A Privacy-preserving Large Language Model for Mental Health Support](https://arxiv.org/abs/2601.01993)
*Dong Xue,Jicheng Tu,Ming Wang,Xin Yan,Fangzhou Liu,Jie Hu*

Main category: cs.AI

TL;DR: MindChat is a privacy-preserving large language model developed for mental health support, powered by a synthetic counseling dataset called MindCorpus created via role-playing frameworks.


<details>
  <summary>Details</summary>
Motivation: Due to the scarcity and sensitivity of real counseling dialogues, training effective large language models for mental health support is challenging.

Method: The paper introduces MindCorpus, a synthetic dataset generated using multi-agent role-playing with dual closed-loop feedback design for coherence and counseling quality. Federated learning with LoRA adapters and differentially private optimization is employed to fine-tune the model while mitigating privacy risks.

Result: MindCorpus enhances training efficacy, and MindChat performs competitively with existing models while reducing privacy leakage under attacks.

Conclusion: MindChat demonstrates promise for mental health support, balancing effectiveness with privacy preservation, leveraging innovative synthetic data generation methods.

Abstract: Large language models (LLMs) have shown promise for mental health support, yet training such models is constrained by the scarcity and sensitivity of real counseling dialogues. In this article, we present MindChat, a privacy-preserving LLM for mental health support, together with MindCorpus, a synthetic multi-turn counseling dataset constructed via a multi-agent role-playing framework. To synthesize high-quality counseling data, the developed dialogue-construction framework employs a dual closed-loop feedback design to integrate psychological expertise and counseling techniques through role-playing: (i) turn-level critique-and-revision to improve coherence and counseling appropriateness within a session, and (ii) session-level strategy refinement to progressively enrich counselor behaviors across sessions. To mitigate privacy risks under decentralized data ownership, we fine-tune the base model using federated learning with parameter-efficient LoRA adapters and incorporate differentially private optimization to reduce membership and memorization risks. Experiments on synthetic-data quality assessment and counseling capability evaluation show that MindCorpus improves training effectiveness and that MindChat is competitive with existing general and counseling-oriented LLM baselines under both automatic LLM-judge and human evaluation protocols, while exhibiting reduced privacy leakage under membership inference attacks.

</details>


### [48] [XAI-MeD: Explainable Knowledge Guided Neuro-Symbolic Framework for Domain Generalization and Rare Class Detection in Medical Imaging](https://arxiv.org/abs/2601.02008)
*Midhat Urooj,Ayan Banerjee,Sandeep Gupta*

Main category: cs.AI

TL;DR: XAIMeD is a neuro-symbolic framework for medical AI integrating clinical expertise to improve robustness under distribution shifts, rare condition detection, and offer interpretable solutions.


<details>
  <summary>Details</summary>
Motivation: Developing medical AI that maintains robustness under real-world distribution shifts, enhances sensitivity to rare clinical conditions, and provides transparent and clinically meaningful interpretations.

Method: XAIMeD integrates neural networks with symbolic reasoning that encodes expert knowledge into class-specific logical rules. It uses feature satisfaction scores, confidence-weighted fusion of predictions, and adaptive routing to mitigate uncertainties and handle class imbalances.

Result: XAIMeD demonstrated 6% improvements in cross-domain generalization and a 10% increase in rare-class F1 score compared to state-of-the-art methods across various medical tasks and datasets.

Conclusion: XAIMeD offers a reliable, explainable, and clinically aligned framework for multimodal medical AI, improving robustness, rare class detection, and interpretability under challenging conditions.

Abstract: Explainability domain generalization and rare class reliability are critical challenges in medical AI where deep models often fail under real world distribution shifts and exhibit bias against infrequent clinical conditions This paper introduces XAIMeD an explainable medical AI framework that integrates clinically accurate expert knowledge into deep learning through a unified neuro symbolic architecture XAIMeD is designed to improve robustness under distribution shift enhance rare class sensitivity and deliver transparent clinically aligned interpretations The framework encodes clinical expertise as logical connectives over atomic medical propositions transforming them into machine checkable class specific rules Their diagnostic utility is quantified through weighted feature satisfaction scores enabling a symbolic reasoning branch that complements neural predictions A confidence weighted fusion integrates symbolic and deep outputs while a Hunt inspired adaptive routing mechanism guided by Entropy Imbalance Gain EIG and Rare Class Gini mitigates class imbalance high intra class variability and uncertainty We evaluate XAIMeD across diverse modalities on four challenging tasks i Seizure Onset Zone SOZ localization from rs fMRI ii Diabetic Retinopathy grading across 6 multicenter datasets demonstrate substantial performance improvements including 6 percent gains in cross domain generalization and a 10 percent improved rare class F1 score far outperforming state of the art deep learning baselines Ablation studies confirm that the clinically grounded symbolic components act as effective regularizers ensuring robustness to distribution shifts XAIMeD thus provides a principled clinically faithful and interpretable approach to multimodal medical AI.

</details>


### [49] [Simulated Reasoning is Reasoning](https://arxiv.org/abs/2601.02043)
*Hendrik Kempt,Alon Lavie*

Main category: cs.AI

TL;DR: This paper examines how Foundational Models (FM) redefine reasoning, challenging traditional "symbolic reasoning" and proposing alternatives for safety and robustness.


<details>
  <summary>Details</summary>
Motivation: To explore how FM challenge traditional understanding of reasoning and its safety implications.

Method: Philosophical analysis of FM reasoning, including critiques of existing metaphors like "stochastic parrot" and normative reflections.

Result: FMs demonstrate reasoning capabilities fundamentally different from humans, but with brittleness and lack of grounding.

Conclusion: The concept of reasoning and its conditions need re-evaluation in light of FMs, with a focus on robustness and safety strategies.

Abstract: Reasoning has long been understood as a pathway between stages of understanding. Proper reasoning leads to understanding of a given subject. This reasoning was conceptualized as a process of understanding in a particular way, i.e., "symbolic reasoning". Foundational Models (FM) demonstrate that this is not a necessary condition for many reasoning tasks: they can "reason" by way of imitating the process of "thinking out loud", testing the produced pathways, and iterating on these pathways on their own. This leads to some form of reasoning that can solve problems on its own or with few-shot learning, but appears fundamentally different from human reasoning due to its lack of grounding and common sense, leading to brittleness of the reasoning process. These insights promise to substantially alter our assessment of reasoning and its necessary conditions, but also inform the approaches to safety and robust defences against this brittleness of FMs. This paper offers and discusses several philosophical interpretations of this phenomenon, argues that the previously apt metaphor of the "stochastic parrot" has lost its relevance and thus should be abandoned, and reflects on different normative elements in the safety- and appropriateness-considerations emerging from these reasoning models and their growing capacity.

</details>


### [50] [Higher-Order Action Regularization in Deep Reinforcement Learning: From Continuous Control to Building Energy Management](https://arxiv.org/abs/2601.02061)
*Faizan Ahmed,Aniket Dixit,James Brusey*

Main category: cs.AI

TL;DR: The paper addresses the challenge of erratic action outputs in deep reinforcement learning by implementing higher-order derivative penalties for action smoothness, achieving improved control and reduced equipment wear, especially in energy management systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of erratic, high-frequency actions in deep reinforcement learning, which increase energy consumption and mechanical wear, and to make RL practical for real-world applications.

Method: The authors investigate systematic action smoothness regularization using higher-order derivative penalties, particularly focusing on third-order derivatives (jerk minimization). They validate their approach on continuous control benchmarks and building energy management systems.

Result: The proposed third-order derivative penalties achieve superior action smoothness in four continuous control environments while maintaining competitive performance. In HVAC control systems, smooth policies reduce equipment switching by 60%, enhancing operational efficiency.

Conclusion: Higher-order action regularization effectively balances performance and smooth control, bridging reinforcement learning and practical energy-critical applications, establishing its utility in real-world deployments.

Abstract: Deep reinforcement learning agents often exhibit erratic, high-frequency control behaviors that hinder real-world deployment due to excessive energy consumption and mechanical wear. We systematically investigate action smoothness regularization through higher-order derivative penalties, progressing from theoretical understanding in continuous control benchmarks to practical validation in building energy management. Our comprehensive evaluation across four continuous control environments demonstrates that third-order derivative penalties (jerk minimization) consistently achieve superior smoothness while maintaining competitive performance. We extend these findings to HVAC control systems where smooth policies reduce equipment switching by 60%, translating to significant operational benefits. Our work establishes higher-order action regularization as an effective bridge between RL optimization and operational constraints in energy-critical applications.

</details>


### [51] [FormuLLA: A Large Language Model Approach to Generating Novel 3D Printable Formulations](https://arxiv.org/abs/2601.02071)
*Adeshola Okubena,Yusuf Ali Mohammed,Moe Elbadawi*

Main category: cs.AI

TL;DR: This paper explores using fine-tuned large language models (LLMs) to recommend excipients and predict mechanical properties in pharmaceutical 3D printing, showing promising results but also identifying limitations with model selection, catastrophic forgetting, and evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to integrate advanced AI technologies, specifically artificial general intelligence and LLMs, into pharmaceutical 3D printing, with the aim of overcoming formulation challenges and advancing toward personalized dosage forms.

Method: The study fine-tuned four LLM architectures on a dataset of over 1400 FDM formulations. They systematically evaluated the models in terms of excipient recommendation and filament property prediction, paying attention to fine-tuning configurations and performance metrics.

Result: The Llama2 architecture was the most effective for recommending excipients. However, smaller LLMs showed catastrophic forgetting even with a relatively small dataset. Additionally, traditional LLM performance metrics were found inadequate for assessing pharmaceutical processability.

Conclusion: The work highlights LLMs' potential in pharmaceutical formulation but underscores the need for better evaluation metrics, careful model parameterization, and addressing challenges like catastrophic forgetting to achieve reliable and generalized AI systems in this field.

Abstract: Pharmaceutical three-dimensional (3D) printing is an advanced fabrication technology with the potential to enable truly personalised dosage forms. Recent studies have integrated artificial intelligence (AI) to accelerate formulation and process development, drastically transforming current approaches to pharmaceutical 3D printing. To date, most AI-driven efforts remain narrowly focused, while failing to account for the broader formulation challenges inherent to the technology. Recent advances in AI have introduced artificial general intelligence concepts, wherein systems extend beyond conventional predictive modelling toward more generalised, human-like reasoning. In this work, we investigate the application of large language models (LLMs), fine-tuned on a fused deposition modelling (FDM) dataset comprising over 1400 formulations, to recommend suitable excipients based on active pharmaceutical ingredient (API) dose, and predict filament mechanical properties. Four LLM architectures were fine-tuned, with systematic evaluation of both fine-tuning and generative parameter configurations. Our results demonstrate that Llama2 was best suited for recommending excipients for FDM formulations. Additionally, model selection and parameterisation significantly influence performance, with smaller LLMs exhibiting instances of catastrophic forgetting. Furthermore, we demonstrate: (i) even with relatively small dataset of over 1400 formulations, it can lead to model catastrophic forgetting; (ii) standard LLM metrics only evaluate linguistic performance but not formulation processability; and (iii) LLMs trained on biomedically-related data do not always produce the best results. Addressing these challenges is essential to advancing LLMs beyond linguistic proficiency and toward reliable systems for pharmaceutical formulation development.

</details>


### [52] [EverMemOS: A Self-Organizing Memory Operating System for Structured Long-Horizon Reasoning](https://arxiv.org/abs/2601.02163)
*Chuanrui Hu,Xingze Gao,Zuyi Zhou,Dannong Xu,Yi Bai,Xintong Li,Hui Zhang,Tong Li,Chong Zhang,Lidong Bing,Yafeng Deng*

Main category: cs.AI

TL;DR: The paper introduces EverMemOS, an advanced memory operating system for large language models, designed to enhance long-term interaction by organizing and consolidating memory effectively.


<details>
  <summary>Details</summary>
Motivation: To address limitations of LLMs with short context windows and poorly organized memory systems during extended interaction.

Method: EverMemOS implements a memory lifecycle inspired by engrams, consisting of episodic trace formation, semantic consolidation, and reconstructive recollection processes.

Result: EverMemOS outperforms existing methods in memory-augmented reasoning tasks and showcases improved user profiling and Foresight capabilities.

Conclusion: EverMemOS enhances the memory functionalities of LLMs, enabling them to maintain coherent, long-term interactions and providing more contextually aware reasoning.

Abstract: Large Language Models (LLMs) are increasingly deployed as long-term interactive agents, yet their limited context windows make it difficult to sustain coherent behavior over extended interactions. Existing memory systems often store isolated records and retrieve fragments, limiting their ability to consolidate evolving user states and resolve conflicts. We introduce EverMemOS, a self-organizing memory operating system that implements an engram-inspired lifecycle for computational memory. Episodic Trace Formation converts dialogue streams into MemCells that capture episodic traces, atomic facts, and time-bounded Foresight signals. Semantic Consolidation organizes MemCells into thematic MemScenes, distilling stable semantic structures and updating user profiles. Reconstructive Recollection performs MemScene-guided agentic retrieval to compose the necessary and sufficient context for downstream reasoning. Experiments on LoCoMo and LongMemEval show that EverMemOS achieves state-of-the-art performance on memory-augmented reasoning tasks. We further report a profile study on PersonaMem v2 and qualitative case studies illustrating chat-oriented capabilities such as user profiling and Foresight. Code is available at https://github.com/EverMind-AI/EverMemOS.

</details>


### [53] [Streaming Hallucination Detection in Long Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.02170)
*Haolang Lu,Minghui Pan,Ripeng Li,Guoshun Nan,Jialin Zhuang,Zijie Zhao,Zhongxiang Sun,Kun Wang,Yang Liu*

Main category: cs.AI

TL;DR: The paper addresses hallucination in long chain-of-thought (CoT) reasoning and introduces a method to detect it in real-time by monitoring the reasoning evolution.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand and detect hallucinations that emerge subtly and propagate during long CoT reasoning in large language models, as they can hinder overall performance.

Method: The authors propose treating hallucination as an evolving latent state rather than isolated events. They use step-level judgments for local observations and introduce a global signal to track hallucination evolution across reasoning trajectories.

Result: The approach enables real-time streaming detection of hallucinations in long CoT reasoning, providing interpretable evidence for better tracking.

Conclusion: This work offers a novel way to improve the robustness of long CoT reasoning in large language models by detecting and addressing hallucinations over reasoning trajectories.

Abstract: Long chain-of-thought (CoT) reasoning improves the performance of large language models, yet hallucinations in such settings often emerge subtly and propagate across reasoning steps. We suggest that hallucination in long CoT reasoning is better understood as an evolving latent state rather than a one-off erroneous event. Accordingly, we treat step-level hallucination judgments as local observations and introduce a cumulative prefix-level hallucination signal that tracks the global evolution of the reasoning state over the entire trajectory. Overall, our approach enables streaming hallucination detection in long CoT reasoning, providing real-time, interpretable evidence.

</details>


### [54] [Project Ariadne: A Structural Causal Framework for Auditing Faithfulness in LLM Agents](https://arxiv.org/abs/2601.02314)
*Sourena Khanzadeh*

Main category: cs.AI

TL;DR: The study introduces Project Ariadne, a framework to determine if AI reasoning traces are faithful or just rationalizations, through structural causal models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the critical safety concern of ensuring Large Language Models (LLMs) have transparent and faithful reasoning processes in high-stakes decision-making.

Method: It employs Structural Causal Models (SCMs) and counterfactual 'hard interventions' to audit LLMs’ reasoning integrity by inverting logic and testing causal sensitivity ($φ$).

Result: The study identifies a "Faithfulness Gap" in agent reasoning, highlighting failings like 'Causal Decoupling' where models rely on latent priors rather than reasoning traces.

Conclusion: Current LLM architectures often fail to align reasoning traces with actual decision-making, and the Ariadne Score is proposed as a benchmark for faithful reasoning.

Abstract: As Large Language Model (LLM) agents are increasingly tasked with high-stakes autonomous decision-making, the transparency of their reasoning processes has become a critical safety concern. While \textit{Chain-of-Thought} (CoT) prompting allows agents to generate human-readable reasoning traces, it remains unclear whether these traces are \textbf{faithful} generative drivers of the model's output or merely \textbf{post-hoc rationalizations}. We introduce \textbf{Project Ariadne}, a novel XAI framework that utilizes Structural Causal Models (SCMs) and counterfactual logic to audit the causal integrity of agentic reasoning. Unlike existing interpretability methods that rely on surface-level textual similarity, Project Ariadne performs \textbf{hard interventions} ($do$-calculus) on intermediate reasoning nodes -- systematically inverting logic, negating premises, and reversing factual claims -- to measure the \textbf{Causal Sensitivity} ($φ$) of the terminal answer. Our empirical evaluation of state-of-the-art models reveals a persistent \textit{Faithfulness Gap}. We define and detect a widespread failure mode termed \textbf{Causal Decoupling}, where agents exhibit a violation density ($ρ$) of up to $0.77$ in factual and scientific domains. In these instances, agents arrive at identical conclusions despite contradictory internal logic, proving that their reasoning traces function as "Reasoning Theater" while decision-making is governed by latent parametric priors. Our findings suggest that current agentic architectures are inherently prone to unfaithful explanation, and we propose the Ariadne Score as a new benchmark for aligning stated logic with model action.

</details>


### [55] [Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.02346)
*Falcon LLM Team,Iheb Chaabane,Puneesh Khanna,Suhail Mohmad,Slim Frikha,Shi Hu,Abdalgader Abubaker,Reda Alami,Mikhail Lubinets,Mohamed El Amine Seddik,Hakim Hacid*

Main category: cs.AI

TL;DR: Falcon-H1R is a 7B-parameter model optimized for reasoning, offering competitive performance with smaller size compared to larger models, focusing on efficiency and scalability.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility of achieving competitive reasoning performance using small language models (SLMs) and optimize for robust, scalable reasoning systems.

Method: Careful data curation, efficient supervised fine-tuning (SFT), reinforcement learning (RL) scaling, and hybrid-parallel architecture design combined with DeepConf methodology for scaling efficiency.

Result: Falcon-H1R matches or outperforms larger state-of-the-art reasoning models across benchmarks. Demonstrates improvements in accuracy, computational cost, and faster inference.

Conclusion: Compact models, with optimized design and training strategies, can deliver scalable and efficient reasoning performance, validating SLMs as practical alternatives to larger models.

Abstract: This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [56] [CounterPoint: Using Hardware Event Counters to Refute and Refine Microarchitectural Assumptions (Extended Version)](https://arxiv.org/abs/2601.01265)
*Nick Lindsay,Caroline Trippel,Anurag Khandelwal,Abhishek Bhattacharjee*

Main category: cs.AR

TL;DR: The paper introduces 'CounterPoint,' a framework for analyzing hardware event counters to reconcile noisy data with microarchitectural models and uncover hidden features.


<details>
  <summary>Details</summary>
Motivation: Hardware event counters often provide valuable data for microarchitectural analysis, but their effectiveness is limited by vague specifications, design opacity, and multiplexing noise.

Method: CounterPoint utilizes microarchitectural models (expressed as μpath Decision Diagrams) to test their consistency against hardware performance counter data, using confidence regions to mitigate noise and pinpoint potential microarchitectural mismatches.

Result: Through a case study on Haswell's Memory Management Unit, CounterPoint identified undocumented microarchitectural behaviors such as TLB prefetchers and abortable page table walks.

Conclusion: CounterPoint helps experts analyze noisy hardware event counter data, offering a tool to reconcile data with microarchitectural models and discover previously unknown hardware features.

Abstract: Hardware event counters offer the potential to reveal not only performance bottlenecks but also detailed microarchitectural behavior. In practice, this promise is undermined by their vague specifications, opaque designs, and multiplexing noise, making event counter data hard to interpret.
  We introduce CounterPoint, a framework that tests user-specified microarchitectural models - expressed as $μ$path Decision Diagrams - for consistency with performance counter data. When mismatches occur, CounterPoint pinpoints plausible microarchitectural features that could explain them, using multi-dimensional counter confidence regions to mitigate multiplexing noise. We apply CounterPoint to the Haswell Memory Management Unit as a case study, shedding light on multiple undocumented and underdocumented microarchitectural behaviors. These include a load-store queue-side TLB prefetcher, merging page table walkers, abortable page table walks, and more.
  Overall, CounterPoint helps experts reconcile noisy hardware performance counter measurements with their mental model of the microarchitecture - uncovering subtle, previously hidden hardware features along the way.

</details>


### [57] [A System Architecture for Low Latency Multiprogramming Quantum Computing](https://arxiv.org/abs/2601.01158)
*Yilun Zhao,Yu Chen,Kaiyan Chang,He Li,Bing Li,Yinhe Han,Ying Wang*

Main category: cs.AR

TL;DR: FLAMENCO introduces a fidelity-aware compilation system for quantum computing multiprogramming, optimizing runtime efficiency and device utilization by eliminating online compilation and enhancing program execution fidelity.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in current quantum MPQC pipelines caused by expensive online compilation steps hindering low-latency deployments and practical workloads like Quantum Neural Networks.

Method: FLAMENCO employs offline compilation to generate multiple program versions bound to distinct qubit regions, utilizes architecture-level abstraction to reduce allocation space, and incorporates a streamlined runtime orchestrator for conflict-free, efficient program execution.

Result: FLAMENCO eliminates online compilation overhead, achieves over 5× runtime speedup, improves execution fidelity, and maintains high utilization even with increased concurrency.

Conclusion: FLAMENCO offers a scalable and efficient solution for multiprogramming quantum systems, enabling runtime adaptability, minimizing latency, and addressing challenges of portability and fidelity in device-dependent quantum computation.

Abstract: As quantum systems scale, Multiprogramming Quantum Computing (MPQC) becomes essential to improve device utilization and throughput. However, current MPQC pipelines rely on expensive online compilation to co-optimize concurrently running programs, because quantum executables are device-dependent, non-portable across qubit regions, and highly susceptible to noise and crosstalk. This online step dominates runtime and impedes low-latency deployments for practical, real-world workloads in the future, such as repeatedly invoked Quantum Neural Network (QNN) services.
  We present FLAMENCO, a fidelity-aware multi-version compilation system that enables independent offline compilation and low-latency, high-fidelity multiprogramming at runtime. At the architecture level, FLAMENCO abstracts devices into compute units to drastically shrink the search space of region allocation. At compile time, it generates diverse executable versions for each program -- each bound to a distinct qubit region -- allowing dynamic region selection at runtime and overcoming non-portability. At runtime, FLAMENCO employs a streamlined orchestrator that leverages post-compilation fidelity metrics to avoid conflicts and mitigate crosstalk, achieving reliable co-execution without online co-optimization. Comprehensive evaluations against state-of-the-art MPQC baselines show that FLAMENCO removes online compilation overhead, achieves over 5$\times$ runtime speedup, improves execution fidelity, and maintains high utilization as concurrency increases.

</details>


### [58] [Ageing Monitoring for Commercial Microcontrollers Based on Timing Windows](https://arxiv.org/abs/2601.02053)
*Leandro Lanzieri,Jiri Kral,Goerschwin Fey,Holger Schlarb,Thomas C. Schmidt*

Main category: cs.AR

TL;DR: The paper presents a software-based method to monitor hardware degradation in microcontrollers, allowing dynamic detection of ageing effects like frequency degradation due to temperature changes.


<details>
  <summary>Details</summary>
Motivation: Microcontrollers in embedded systems face hardware ageing, which can cause malfunctions. Current solutions like guard bands use static techniques to prevent errors but can limit system performance and result in sudden failures over time.

Method: The authors propose a software-based self-testing technique to monitor microcontrollers for hardware degradation. This approach uses variable timing windows to assess devices' maximum operational frequencies.

Result: The method was validated on real hardware and successfully detected up to 13.79% degradation in maximum operating frequency under a 60 °C temperature increase across tested devices.

Conclusion: This software-based monitoring approach offers a deployable way to dynamically detect hardware ageing in microcontrollers, potentially improving reliability and reducing performance loss.

Abstract: Microcontrollers are increasingly present in embedded deployments and dependable applications, for which malfunctions due to hardware ageing can have severe impact. The lack of deployable techniques for ageing monitoring on these devices has spread the application of guard bands to prevent timing errors due to degradation. Applying this static technique can limit performance and lead to sudden failures as devices age. In this paper, we follow a software-based self-testing approach to design monitoring of hardware degradation for microcontrollers. Deployable in the field, our technique leverages timing windows of variable lengths to determine the maximum operational frequency of the devices. We empirically validate the method on real hardware and find that it consistently detects temperature-induced degradations in maximum operating frequency of up to 13.79 % across devices for 60 °C temperature increase.

</details>


### [59] [HFRWKV: A High-Performance Fully On-Chip Hardware Accelerator for RWKV](https://arxiv.org/abs/2601.02135)
*Liu Shijie,Zeng Zhenghao,Jiao Han,Huang Yihua*

Main category: cs.AR

TL;DR: The paper introduces HFRWKV, an FPGA-based hardware accelerator designed to overcome efficiency challenges of RWKV architecture and achieve significant improvements in throughput and energy efficiency, especially compared to CPUs and GPUs.


<details>
  <summary>Details</summary>
Motivation: RWKV, a modern RNN architecture, faces challenges in computational efficiency due to low GPU utilization and memory bottlenecks from frequent off-chip weight access.

Method: The paper proposes HFRWKV, equipped with novel hybrid-precision quantization, reusable architectures for complex operations, parallel matrix-vector processing arrays, and pipeline architecture with computation reordering and chunked double buffering.

Result: HFRWKV implemented on Alveo FPGA platforms outperforms CPUs with a 63.48× throughput and 139.17× energy efficiency improvement, and GPUs with a 32.33× throughput and 171.36× energy efficiency improvement.

Conclusion: HFRWKV effectively accelerates RWKV execution, addressing its GPU inefficiencies and memory bottlenecks, achieving significant performance and energy efficiency gains.

Abstract: RWKV is a modern RNN architecture that approaches the performance of Transformers, with the advantage of processing long contexts at a linear memory cost. However, its sequential computation pattern struggles to efficiently leverage GPU parallelism, which leads to low compute resource utilization. Furthermore, frequent off-chip weight accesses create a memory bottleneck. To address these challenges, we propose HFRWKV, an FPGA-based hardware accelerator specifically designed for RWKV. Within the matrix operation module, we propose a novel hardware-friendly hybrid-precision quantization strategy, which enhances performance while maintaining acceptable accuracy. For the complex operations including exponentiation and division, we introduce a method featuring reusable architectures combined with lookup tables or piecewise linear approximation, which is algorithmically refined to effectively balance precision and hardware resource consumption. Based on this foundation, we adopt a fully on-chip computing system integrating parallel matrix-vector processing array and an efficient pipeline architecture. Through computation reordering and chunked double buffering, it effectively eliminates data transfer bottlenecks and improves overall throughput. We implement HFRWKV on the Alveo U50 and U280 platform. Experimental results show that compared to a CPU, a throughput improvement of 63.48$\times$ and an energy efficiency improvement of 139.17$\times$. Compared to GPUs, achieves a throughput improvement of 32.33$\times$ and an energy efficiency improvement of 171.36$\times$.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [60] [The Qualitative Laboratory: Theory Prototyping and Hypothesis Generation with Large Language Models](https://arxiv.org/abs/2601.00797)
*Hugues Draelants*

Main category: cs.CL

TL;DR: The paper introduces persona simulation using Large Language Models (LLMs) to generate qualitative hypotheses about diverse group interpretations, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in generating rich qualitative hypotheses in sociology with innovative and efficacious methodologies.

Method: Simulating sociological personas with LLMs to produce nuanced and naturalistic responses for hypothesis development.

Result: The approach enabled discovery of sophisticated, counter-intuitive insights, such as a conservative persona rejecting a national security frame.

Conclusion: This LLM-based persona simulation method is effective for generating hypotheses for empirical validation and is a step forward in qualitative research approaches.

Abstract: A central challenge in social science is to generate rich qualitative hypotheses about how diverse social groups might interpret new information. This article introduces and illustrates a novel methodological approach for this purpose: sociological persona simulation using Large Language Models (LLMs), which we frame as a "qualitative laboratory". We argue that for this specific task, persona simulation offers a distinct advantage over established methods. By generating naturalistic discourse, it overcomes the lack of discursive depth common in vignette surveys, and by operationalizing complex worldviews through natural language, it bypasses the formalization bottleneck of rule-based agent-based models (ABMs). To demonstrate this potential, we present a protocol where personas derived from a sociological theory of climate reception react to policy messages. The simulation produced nuanced and counter-intuitive hypotheses - such as a conservative persona's rejection of a national security frame - that challenge theoretical assumptions. We conclude that this method, used as part of a "simulation then validation" workflow, represents a superior tool for generating deeply textured hypotheses for subsequent empirical testing.

</details>


### [61] [Rate-Distortion Analysis of Compressed Query Delegation with Low-Rank Riemannian Updates](https://arxiv.org/abs/2601.00938)
*Faruk Alpay,Bugra Kilictas*

Main category: cs.CL

TL;DR: This paper introduces Compressed Query Delegation (CQD), a method to exceed the working-memory constraints in bounded-context agents by delegating compressed reasoning tasks to external oracles.


<details>
  <summary>Details</summary>
Motivation: Traditional bounded-context agents struggle with reasoning tasks that surpass their working-memory capabilities. This paper aims to address this challenge by introducing an efficient method for external reasoning delegation while maintaining performance and memory limitations.

Method: The paper proposes a three-step Compressed Query Delegation (CQD) approach: (1) Compress reasoning states into low-rank tensor queries, (2) Delegate these queries to external oracles, and (3) Update the reasoning state using Riemannian optimization on fixed-rank manifolds.

Result: The authors connect CQD to rate-distortion and information bottleneck principles and validate the method with theoretical guarantees. Empirical results include a reasoning benchmark suite and human cognition-like benchmarks, showing improved reasoning performance over baselines.

Conclusion: CQD provides a principled and practical framework for addressing working-memory limitations in bounded-context agents by leveraging external oracle delegation.

Abstract: Bounded-context agents fail when intermediate reasoning exceeds an effective working-memory budget. We study compressed query delegation (CQD): (i) compress a high-dimensional latent reasoning state into a low-rank tensor query, (ii) delegate the minimal query to an external oracle, and (iii) update the latent state via Riemannian optimization on fixed-rank manifolds. We give a math-first formulation: CQD is a constrained stochastic program with a query-budget functional and an oracle modeled as a noisy operator. We connect CQD to classical rate-distortion and information bottleneck principles, showing that spectral hard-thresholding is optimal for a natural constrained quadratic distortion problem, and we derive convergence guarantees for Riemannian stochastic approximation under bounded oracle noise and smoothness assumptions. Empirically, we report (A) a 2,500-item bounded-context reasoning suite (BBH-derived tasks plus curated paradox instances) comparing CQD against chain-of-thought baselines under fixed compute and context; and (B) a human "cognitive mirror" benchmark (N=200) measuring epistemic gain and semantic drift across modern oracles.

</details>


### [62] [Intention Collapse: Intention-Level Metrics for Reasoning in Language Models](https://arxiv.org/abs/2601.01011)
*Patricio Vera*

Main category: cs.CL

TL;DR: The paper introduces the notion of 'intention collapse' in language generation, formalizes metrics to define it, and explores how model inference shapes internal computations before external output. A preliminary experiment assesses the utility of intention metrics.


<details>
  <summary>Details</summary>
Motivation: To better understand how internal states (intentions) of language models are compressed into their external linguistic outputs and to analyze the degree to which intentions can be retained, recovered, and measured.

Method: The authors formalize metrics (intention entropy, effective dimensionality, and latent knowledge recoverability) to measure intention collapse. They conduct experiments using a 4-bit Mistral 7B model on 200 GSM8K problems under different inference regimes (direct answer baseline, chain of thought, and babble control).

Result: The chain of thought regime significantly improved accuracy (5.5% to 53%) and reduced intention entropy compared to other regimes. Effective dimensionality in CoT was higher despite fewer tokens. Metrics like Hint showed limitations on predictive power, and linear probes revealed latent intention recoverability in CoT but not in the baseline.

Conclusion: Intention metrics can distinguish between inference strategies and reveal latent information in language models, though current proxies have limitations, suggesting scope for further metric refinement and experimentation.

Abstract: Every act of language generation compresses a rich internal state into a single token sequence. We call this process intention collapse: a many-to-one projection from a high dimensional intention space I into an external language space L. We formalize intention collapse for contemporary language models, define three simple, model agnostic intention metrics (intention entropy Hint, effective dimensionality dimeff, and latent knowledge recoverability Recov), and propose an empirical agenda for studying how inference time computation shapes internal intentions before they are verbalized. We also report a first small scale experiment. Using a 4 bit Mistral 7B model on 200 GSM8K problems, we compare a direct answer baseline, a chain of thought (CoT) regime, and a babble control. CoT raises accuracy from 5.5 percent to 53 percent, sharply reduces pre collapse intention entropy (from 1.42 to 0.37 bits), and shows higher global effective dimensionality than the other regimes despite producing fewer tokens than babble. At the same time, Hint has little item level predictive power, and a linear probe on I achieves AUROC 0.65 in the CoT regime but only about chance in the baseline regime, where it collapses to the majority class. These preliminary results indicate that intention level metrics can distinguish inference regimes and expose latent information that is partly lost during collapse, while also revealing important limitations of our current proxies

</details>


### [63] [HyperJoin: LLM-augmented Hypergraph Link Prediction for Joinable Table Discovery](https://arxiv.org/abs/2601.01015)
*Shiyuan Liu,Jianwei Wang,Xuemin Lin,Lu Qin,Wenjie Zhang,Ying Zhang*

Main category: cs.CL

TL;DR: HyperJoin introduces a hypergraph-based approach for joinable table discovery, addressing structural interaction gaps observed in prior models. The framework demonstrates significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing methods for joinable table discovery inadequately consider inter-table and intra-table structural interactions and generate incoherent results due to limitations in their design.

Method: HyperJoin constructs a hypergraph with LLM-augmented hyperedges to formulate joinable table discovery as link prediction. It employs a Hierarchical Interaction Network (HIN) for column representations and a reranking module to ensure coherence using a maximum spanning tree algorithm.

Result: HyperJoin achieves substantial performance improvements, with an average of 21.4% better Precision@15 and 17.2% better Recall@15 compared to the best baseline.

Conclusion: The HyperJoin framework effectively addresses structural and coherence challenges in joinable table discovery, outperforming existing models and setting a new benchmark in precision and recall.

Abstract: As a pivotal task in data lake management, joinable table discovery has attracted widespread interest. While existing language model-based methods achieve remarkable performance by combining offline column representation learning with online ranking, their design insufficiently accounts for the underlying structural interactions: (1) offline, they directly model tables into isolated or pairwise columns, thereby struggling to capture the rich inter-table and intra-table structural information; and (2) online, they rank candidate columns based solely on query-candidate similarity, ignoring the mutual interactions among the candidates, leading to incoherent result sets. To address these limitations, we propose HyperJoin, a large language model (LLM)-augmented Hypergraph framework for Joinable table discovery. Specifically, we first construct a hypergraph to model tables using both the intra-table hyperedges and the LLM-augmented inter-table hyperedges. Consequently, the task of joinable table discovery is formulated as link prediction on this constructed hypergraph. We then design HIN, a Hierarchical Interaction Network that learns expressive column representations through bidirectional message passing over columns and hyperedges. To strengthen coherence and internal consistency in the result columns, we cast online ranking as a coherence-aware top-k column selection problem. We then introduce a reranking module that leverages a maximum spanning tree algorithm to prune noisy connections and maximize coherence. Experiments demonstrate the superiority of HyperJoin, achieving average improvements of 21.4% (Precision@15) and 17.2% (Recall@15) over the best baseline.

</details>


### [64] [Multi-Dimensional Prompt Chaining to Improve Open-Domain Dialogue Generation](https://arxiv.org/abs/2601.01037)
*Livia Leong Hui Teng*

Main category: cs.CL

TL;DR: The paper introduces a prompt-chaining framework to enhance dialogue quality in Small Language Models (SLMs), achieving competitive performance with larger models.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of Small Language Models (SLMs) in open-domain dialogue by enhancing their human-likeness without requiring extensive computational resources.

Method: A multi-dimensional prompt-chaining framework focusing on Naturalness, Coherence, and Engagingness is applied to SLMs like TinyLlama and Llama-2-7B. The resulting dialogue performance is compared against larger models using automatic and human evaluations.

Result: The framework improves response diversity, coherence, and naturalness by up to 29%, enabling Llama-2-7B to achieve dialogue performance comparable to much larger models like Llama-2-70B and GPT-3.5 Turbo.

Conclusion: Prompt-chaining offers a resource-efficient method to significantly improve dialogue quality in SLMs, reducing the performance gap between small and large models.

Abstract: Small language models (SLMs) offer significant deployment advantages but often struggle to match the dialogue quality of larger models in open-domain settings. In this paper, we propose a multi-dimensional prompt-chaining framework that integrates Naturalness, Coherence, and Engagingness dimensions to enhance human-likeness in open-domain dialogue generation. We apply the framework to two SLMs, TinyLlama and Llama-2-7B, and benchmark their performance against responses generated by substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. We then employ automatic and human evaluation to assess the responses based on diversity, contextual coherence, as well as overall quality. Results show that the full framework improves response diversity by up to 29%, contextual coherence by up to 28%, and engagingness as well as naturalness by up to 29%. Notably, Llama-2-7B achieves performance comparable to substantially larger models, including Llama-2-70B and GPT-3.5 Turbo. Overall, the findings demonstrate that carefully designed prompt-based strategies provide an effective and resource-efficient pathway to improving open-domain dialogue quality in SLMs.

</details>


### [65] [KV-Embedding: Training-free Text Embedding via Internal KV Re-routing in Decoder-only LLMs](https://arxiv.org/abs/2601.01046)
*Yixuan Tang,Yi Yang*

Main category: cs.CL

TL;DR: This paper introduces KV-Embedding, a method to enhance frozen LLM embeddings' effectiveness in training-free settings by restructuring their internal states.


<details>
  <summary>Details</summary>
Motivation: Training-free usage of LLM embeddings is limited by structural challenges, such as restricted token access to subsequent context and representation bias toward generation rather than semantic compression.

Method: KV-Embedding reroutes the final token's key-value states at each layer as a prepended prefix, allowing all tokens to access sequence-level context. An automated layer selection strategy based on intrinsic dimensionality ensures model-agnostic application.

Result: KV-Embedding achieves up to 10% performance improvement over training-free baselines on MTEB tests with Qwen, Mistral, and Llama models, handling sequences as long as 4,096 tokens.

Conclusion: KV-Embedding efficiently manipulates LLM internal states for better representation learning, presenting an alternative approach to input modification and encouraging further exploration of LLM internals.

Abstract: While LLMs are powerful embedding backbones, their application in training-free settings faces two structural challenges: causal attention restricts early tokens from accessing subsequent context, and the next-token prediction objective biases representations toward generation rather than semantic compression. To address these limitations, we propose KV-Embedding, a framework that activates the latent representation power of frozen LLMs. Our method leverages the observation that the key-value (KV) states of the final token at each layer encode a compressed view of the sequence. By re-routing these states as a prepended prefix, we enable all tokens to access sequence-level context within a single forward pass. To ensure model-agnostic applicability, we introduce an automated layer selection strategy based on intrinsic dimensionality. Evaluations on MTEB across Qwen, Mistral, and Llama backbones show that KV-Embedding outperforms existing training-free baselines by up to 10%, while maintaining robust performance on sequences up to 4,096 tokens. These results demonstrate that internal state manipulation offers an efficient alternative to input modification, and we hope this work encourages further exploration of LLM internals for representation learning.

</details>


### [66] [Unsupervised Text Style Transfer for Controllable Intensity](https://arxiv.org/abs/2601.01060)
*Shuhuan Gu,Wenbiao Tao,Xinchen Ma,Kangkang He,Ye Guo,Xiang Li,Yunshi Lan*

Main category: cs.CL

TL;DR: The paper introduces an approach for Unsupervised Text Style Transfer specifically addressing challenges in transferring texts across differing stylistic intensities using LLMs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of transferring subtle stylistic intensity differences across texts in situations lacking parallel data.

Method: Proposed an approach combining Sequential Fine-Tuning (SFT) and Proximal Policy Optimization (PPO) applied to a large language model (LLM) using synthesized parallel data and hierarchical, stylistically-aware reward functions.

Result: Experiments on two benchmarks demonstrate improved performance across evaluation metrics and noticeable stylistic differences, even between close intensity levels.

Conclusion: The novel SFT-then-PPO approach effectively fine-tunes LLMs to perform UTST for controllable stylistic intensity, showcasing advantages of the proposed rewards.

Abstract: Unsupervised Text Style Transfer (UTST) aims to build a system to transfer the stylistic properties of a given text without parallel text pairs. Compared with text transfer between style polarities, UTST for controllable intensity is more challenging due to the subtle differences in stylistic features across different intensity levels. Faced with the challenges posed by the lack of parallel data and the indistinguishability between adjacent intensity levels, we propose a SFT-then-PPO paradigm to fine-tune an LLM. We first fine-tune the LLM with synthesized parallel data. Then, we further train the LLM with PPO, where the rewards are elaborately designed for distinguishing the stylistic intensity in hierarchical levels. Both the global and local stylistic features are considered to formulate the reward functions. The experiments on two UTST benchmarks showcase that both rewards have their advantages and applying them to LLM fine-tuning can effectively improve the performance of an LLM backbone based on various evaluation metrics. Even for close levels of intensity, we can still observe the noticeable stylistic difference between the generated text.

</details>


### [67] [ks-lit-3m: A 3.1 million word kashmiri text dataset for large language model pretraining](https://arxiv.org/abs/2601.01091)
*Haq Nawaz Malik*

Main category: cs.CL

TL;DR: This paper introduces KS-LIT-3M, a 3.1 million word corpus designed for pretraining language models in Kashmiri, addressing the lack of high-quality training data for this language.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the performance disparity of LLMs in Kashmiri, caused by the scarcity of high-quality training data due to decades of inaccessible literature encoded in proprietary formats.

Method: The creation of KS-LIT-3M involved developing an InPage-to-Unicode converter, performing preprocessing steps like contamination removal and character normalization, and curating genrespecific texts into a continuous linear text stream for causal language model training.

Result: KS-LIT-3M corpus encompasses 131,607 unique words from diverse genres and provides a critical resource for improving Kashmiri language processing.

Conclusion: The release of KS-LIT-3M under the CC-BY-4.0 license helps bridge the resource gap in Kashmiri NLP, fostering research and development for this underrepresented language.

Abstract: Large Language Models (LLMs) demonstrate remarkable fluency across high-resource languages yet consistently fail to generate coherent text in Kashmiri, a language spoken by approximately seven million people. This performance disparity stems not from inherent model limitations but from a critical scarcity of high-quality training data. Decades of Kashmiri literature remain inaccessible to modern NLP pipelines due to their encoding in the proprietary InPage desktop publishing format. This paper introduces KS-LIT-3M, a curated corpus of 3.1 million words (16.4 million characters) specifically designed for pretraining language models on Kashmiri. The dataset is structured as a single continuous linear text stream, optimized for causal language model training where models learn to predict subsequent tokens from preceding context. The corpus was constructed through the development of a specialized InPage-to-Unicode converter, followed by rigorous preprocessing including English contamination removal, character normalization, and quality validation. Encompassing 131,607 unique words drawn from diverse genres including literary works, journalistic writing, academic texts, and religious scholarship, KS-LIT-3M addresses a fundamental resource gap for Kashmiri language technology. The dataset is released under the CC-BY-4.0 license to facilitate research in Kashmiri natural language processing.

</details>


### [68] [EmoLoom-2B: Fast Base-Model Screening for Emotion Classification and VAD with Lexicon-Weak Supervision and KV-Off Evaluation](https://arxiv.org/abs/2601.01112)
*Zilin Li,Weiwei Xu,Xuanbo Lu,Zheda Liu*

Main category: cs.CL

TL;DR: EmoLoom-2B is a pipeline for enhancing small language models (under 2B parameters) for emotion classification and Valence-Arousal-Dominance (VAD) prediction with lightweight and efficient mechanisms.


<details>
  <summary>Details</summary>
Motivation: To enable small language models to be effective, lightweight tools for joint emotion classification and VAD prediction were needed with minimal computational cost.

Method: The method includes unifying data handling, adopting new regularization strategies like VAD-preserving constraints, polarity-sensitive augmentation with valence flip, and entropy-aware training strategies for better performance and reproducibility.

Result: EmoLoom-2B demonstrated strong performance in emotion-related NLP tasks like GoEmotions and Empathetic Dialogues, with generalizable results across datasets.

Conclusion: The approach provides a cost-effective and reproducible solution, acting as a robust preliminary tool for advanced models in emotion and sentiment analysis workflows.

Abstract: We introduce EmoLoom-2B, a lightweight and reproducible pipeline that turns small language models under 2B parameters into fast screening candidates for joint emotion classification and Valence-Arousal-Dominance prediction. To ensure protocol-faithful and fair evaluation, we unify data loading, training, and inference under a single JSON input-output contract and remove avoidable variance by adopting KV-off decoding as the default setting. We incorporate two orthogonal semantic regularizers: a VAD-preserving constraint that aligns generated text with target VAD triples, and a lightweight external appraisal classifier that provides training-time guidance on goal attainment, controllability, certainty, and fairness without injecting long rationales. To improve polarity sensitivity, we introduce Valence Flip augmentation based on mirrored emotional pairs. During supervised fine-tuning, we apply A/B mixture sampling with entropy-aware temperature scheduling to balance coverage and convergence. Using Qwen-1.8B-Chat as the base model, EmoLoom-2B achieves strong performance on GoEmotions and EmpatheticDialogues, and demonstrates robust cross-corpus generalization on DailyDialog. The proposed recipe is budget-aware, auditable, and re-entrant, serving as a dependable screening pass before heavier training or multimodal fusion.

</details>


### [69] [Listen, Attend, Understand: a Regularization Technique for Stable E2E Speech Translation Training on High Variance labels](https://arxiv.org/abs/2601.01121)
*Yacouba Diarra,Michael Leventhal*

Main category: cs.CL

TL;DR: LAU introduces a semantic regularization technique for End-to-End Speech Translation, improving quality and semantic preservation with constrained encoder latent space during training.


<details>
  <summary>Details</summary>
Motivation: End-to-End Speech Translation struggles with high variance and semantic ambiguity in target transcriptions, leading to slower convergence and worse performance.

Method: LAU uses frozen text embeddings as a directional auxiliary loss to inject linguistic groundedness into the encoder latent space, without increasing inference cost.

Result: LAU achieves performance comparable to systems trained on double data, better semantic preservation, and demonstrates via Total Parameter Drift the encoder's prioritization of meaning over phonetics.

Conclusion: LAU is an effective and robust method enhancing End-to-End Speech Translation when data is scarce or noisy, serving as a valuable alternative to existing techniques.

Abstract: End-to-End Speech Translation often shows slower convergence and worse performance when target transcriptions exhibit high variance and semantic ambiguity. We propose Listen, Attend, Understand (LAU), a semantic regularization technique that constrains the acoustic encoder's latent space during training. By leveraging frozen text embeddings to provide a directional auxiliary loss, LAU injects linguistic groundedness into the acoustic representation without increasing inference cost. We evaluate our method on a Bambara-to-French dataset with 30 hours of Bambara speech translated by non-professionals. Experimental results demonstrate that LAU models achieve comparable performance by standard metrics compared to an E2E-ST system pretrained with 100\% more data and while performing better in preserving semantic meaning. Furthermore, we introduce Total Parameter Drift as a metric to quantify the structural impact of regularization to demonstrate that semantic constraints actively reorganize the encoder's weights to prioritize meaning over literal phonetics. Our findings suggest that LAU is a robust alternative to post-hoc rescoring and a valuable addition to E2E-ST training, especially when training data is scarce and/or noisy.

</details>


### [70] [RoboPhD: Self-Improving Text-to-SQL Through Autonomous Agent Evolution](https://arxiv.org/abs/2601.01126)
*Andrew Borthwick,Stephen Ash*

Main category: cs.CL

TL;DR: RoboPhD is an AI system that autonomously evolves Text-to-SQL agents using an ELO-based evolution mechanism, achieving significant performance improvements even with minimal initial guidance.


<details>
  <summary>Details</summary>
Motivation: To demonstrate that AI can independently research and develop improved Text-to-SQL agents using evolution techniques without external domain-specific guidance.

Method: RoboPhD uses a closed-loop evolution cycle consisting of SQL generation and evolution components, driven by an ELO-based performance feedback system to evolve agent strategies.

Result: The system autonomously evolved agents that discovered efficient strategies, achieving up to 73.67% accuracy on the BIRD test set and improving weaker models by up to 8.9 points.

Conclusion: RoboPhD proves AI can build effective systems autonomously, with minimal human input, offering cost-effective improvements in Text-to-SQL model performance.

Abstract: We present RoboPhD, a system where AI agents autonomously conduct research to improve Text-to-SQL performance. RoboPhD implements a closed-loop evolution cycle with two coordinated components: a SQL Generation agent composed of a database analysis script and SQL generation instructions, and an Evolution agent that designs new versions based on performance feedback. Central to the framework is an ELO-based selection mechanism enabling survival-of-the-fittest dynamics while handling non-transitivity in performance. Starting from a naive 70-line baseline, RoboPhD evolves agents through iterative cross-pollination, discovering effective techniques without any external guidance on the Text-to-SQL domain. Our best agent, evolved to 1500 lines over 18 iterations, autonomously discovered strategies such as size-adaptive database analysis that adjusts depth based on schema complexity and SQL generation patterns for column selection, evidence interpretation, and aggregation. Evolution provides the largest gains on cheaper models: while we improve by 2.3 points over a strong Claude Opus 4.5 naive baseline, we show an improvement of 8.9 points over the weaker Claude Haiku model. This enables 'skip a tier' deployment: evolved Haiku exceeds naive Sonnet accuracy, and evolved Sonnet exceeds naive Opus, both at lower cost. The full system achieves 73.67% accuracy on the BIRD test set, demonstrating that AI can autonomously build a strong agentic system with only a trivial human-provided starting point.

</details>


### [71] [KOS-TL (Knowledge Operation System Type Logic)](https://arxiv.org/abs/2601.01143)
*Peng Chen*

Main category: cs.CL

TL;DR: This paper introduces KOS-TL, a framework combining type theory and event semantics for autonomous knowledge systems, ensuring logical consistency and practical application.


<details>
  <summary>Details</summary>
Motivation: Knowledge systems lack integration between symbolic logic representation and system execution. This gap hinders the formal foundation needed for autonomous systems.

Method: The authors propose KOS-TL, structured in three layers: a static Core Layer, an event-driven Kernel Layer, and a bidirectional Runtime Layer. Operational semantics and meta-theoretical properties are formally validated.

Result: KOS-TL supports "proof-carrying knowledge," proven through applications in industrial traceability and financial compliance.

Conclusion: KOS-TL establishes a verifiable and practical framework for intelligent and autonomous operating systems.

Abstract: This paper introduces KOS-TL (Knowledge Operation System Type Logic), a novel constructive framework designed to provide a rigorous logical foundation for autonomous and executable knowledge systems. Traditional knowledge representation models often suffer from a gap between static symbolic logic and dynamic system execution. To bridge this divide, KOS-TL leverages Dependent Type Theory to unify data, logic, and proof into a singular computational substrate.The architecture of KOS-TL is organized into three hierarchical layers: the Core Layer, which defines the static type universe and constructive primitives; the Kernel Layer, which governs state evolution through an event-driven mechanism characterized by the triple $\langle Σ, \textsf{Ev}, Δ\rangle$; and the Runtime Layer, responsible for the bidirectional refinement of physical signals into logical evidence. We formally define the operational semantics of the system and prove key meta-theoretical properties, including Progress and Evolutionary Consistency, ensuring that the system remains logically self-consistent and free from stuck states during continuous state transitions.By integrating Davidsonian event semantics with Martin-Löf type theory, KOS-TL enables the construction of "proof-carrying knowledge," where every state change in the knowledge base is accompanied by a formal witness of its validity. We demonstrate the practical utility of this logic through application examples in industrial traceability and cross-border financial compliance. Our results suggest that KOS-TL provides a robust, formally verifiable basis for the next generation of intelligent, autonomous operating systems.

</details>


### [72] [SongSage: A Large Musical Language Model with Lyric Generative Pre-training](https://arxiv.org/abs/2601.01153)
*Jiani Guo,Jiajia Li,Jie Wu,Zuchao Li,Yujiu Yang,Ping Wang*

Main category: cs.CL

TL;DR: This paper introduces PlaylistSense for testing language models' playlist understanding and SongSage, a specialized model trained with lyric data for music AI tasks.


<details>
  <summary>Details</summary>
Motivation: Investigate lyric-centric knowledge understanding in language models, addressing the gap in playlist comprehension abilities of LLMs.

Method: Develop the PlaylistSense dataset and SongSage model, pre-trained on LyricBank (lyrical data) and fine-tuned with LyricBank-SFT instruction set.

Result: SongSage demonstrates strong lyric-centric understanding, excelling in playlist recommendations, lyric generation, and other tasks, while retaining general knowledge capabilities.

Conclusion: SongSage enhances language models’ capabilities in music-related AI tasks and sets a foundation for further research in music AI applications.

Abstract: Large language models have achieved significant success in various domains, yet their understanding of lyric-centric knowledge has not been fully explored. In this work, we first introduce PlaylistSense, a dataset to evaluate the playlist understanding capability of language models. PlaylistSense encompasses ten types of user queries derived from common real-world perspectives, challenging LLMs to accurately grasp playlist features and address diverse user intents. Comprehensive evaluations indicate that current general-purpose LLMs still have potential for improvement in playlist understanding. Inspired by this, we introduce SongSage, a large musical language model equipped with diverse lyric-centric intelligence through lyric generative pretraining. SongSage undergoes continual pretraining on LyricBank, a carefully curated corpus of 5.48 billion tokens focused on lyrical content, followed by fine-tuning with LyricBank-SFT, a meticulously crafted instruction set comprising 775k samples across nine core lyric-centric tasks. Experimental results demonstrate that SongSage exhibits a strong understanding of lyric-centric knowledge, excels in rewriting user queries for zero-shot playlist recommendations, generates and continues lyrics effectively, and performs proficiently across seven additional capabilities. Beyond its lyric-centric expertise, SongSage also retains general knowledge comprehension and achieves a competitive MMLU score. We will keep the datasets inaccessible due to copyright restrictions and release the SongSage and training script to ensure reproducibility and support music AI research and applications, the datasets release plan details are provided in the appendix.

</details>


### [73] [DHI: Leveraging Diverse Hallucination Induction for Enhanced Contrastive Factuality Control in Large Language Models](https://arxiv.org/abs/2601.01156)
*Jiani Guo,Xiangke Zeng,Jie Wu,Zuchao Li*

Main category: cs.CL

TL;DR: The paper addresses the issue of hallucinations in large language models by proposing a novel training framework, DHI, which encourages diverse hallucination generation for improved mitigation.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs compromise reliability, and existing mitigation methods are limited by the narrow diversity of hallucinations they induce, restricting their effectiveness.

Method: The authors propose the DHI framework, which modifies the loss function to diversify hallucinations generated by a trained Evil LLM. Techniques like causal attention masking and adaptive rationality constraints further enhance performance.

Result: DHI demonstrates significant improvements in mitigating hallucinations compared to existing contrastive decoding approaches, as validated on multiple benchmarks.

Conclusion: DHI broadens the generation of hallucination types in LLMs, successfully overcoming the limitations of existing methods and improving reliability in mitigating hallucinations.

Abstract: Large language models (LLMs) frequently produce inaccurate or fabricated information, known as "hallucinations," which compromises their reliability. Existing approaches often train an "Evil LLM" to deliberately generate hallucinations on curated datasets, using these induced hallucinations to guide contrastive decoding against a reliable "positive model" for hallucination mitigation. However, this strategy is limited by the narrow diversity of hallucinations induced, as Evil LLMs trained on specific error types tend to reproduce only these particular patterns, thereby restricting their overall effectiveness. To address these limitations, we propose DHI (Diverse Hallucination Induction), a novel training framework that enables the Evil LLM to generate a broader range of hallucination types without relying on pre-annotated hallucination data. DHI employs a modified loss function that down-weights the generation of specific factually correct tokens, encouraging the Evil LLM to produce diverse hallucinations at targeted positions while maintaining overall factual content. Additionally, we introduce a causal attention masking adaptation to reduce the impact of this penalization on the generation of subsequent tokens. During inference, we apply an adaptive rationality constraint that restricts contrastive decoding to tokens where the positive model exhibits high confidence, thereby avoiding unnecessary penalties on factually correct tokens. Extensive empirical results show that DHI achieves significant performance gains over other contrastive decoding-based approaches across multiple hallucination benchmarks.

</details>


### [74] [Almost Clinical: Linguistic properties of synthetic electronic health records](https://arxiv.org/abs/2601.01171)
*Serge Sharoff,John Baker,David Francis Hunt,Alan Simpson*

Main category: cs.CL

TL;DR: The study evaluates synthetic EHRs in mental health for linguistic and clinical suitability and highlights systematic divergences.


<details>
  <summary>Details</summary>
Motivation: To assess the quality and appropriateness of synthetic EHRs created by LLMs for mental health contexts.

Method: Analyzed linguistic constructs like agency, modality, and information flow in four clinical genres, and evaluated textual coherence, terminology, and clinical accuracy.

Result: Synthetic EHRs are coherent and terminology-appropriate but exhibit discrepancies in clinical details, register, medication, and diagnostics.

Conclusion: While synthetic EHRs show potential, they require improvement in clinical specificity and accuracy to align with real-world use.

Abstract: This study evaluates the linguistic and clinical suitability of synthetic electronic health records (EHRs) in the field of mental health. First, we describe the rationale and the methodology for creating the synthetic corpus. Second, we assess agency, modality, and information flow across four clinical genres (Assessments, Correspondence, Referrals and Care plans) to understand how LLMs grammatically construct medical authority and patient agency through linguistic choices. While LLMs produce coherent, terminology-appropriate texts that approximate clinical practice, systematic divergences remain, including registerial shifts, insufficient clinical specificity, and inaccuracies in medication use and diagnostic procedures.

</details>


### [75] [Stylometry Analysis of Human and Machine Text for Academic Integrity](https://arxiv.org/abs/2601.01225)
*Hezam Albaqami,Muhammad Asif Ayub,Nasir Ahmad,Yaseen Ahmad,Mohammed M. Alqahtani,Abdullah M. Algamdi,Almoaid A. Owaidah,Kashif Ahmad*

Main category: cs.CL

TL;DR: The paper proposes an NLP framework to address academic integrity issues by analyzing human and machine text, author attribution, and style change detection.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges like plagiarism and authorship verification in educational content using advanced NLP techniques.

Method: A four-task framework is proposed, tested on datasets with normal and strict prompts, addressing machine text classification, authorship changes, and collaborative document analysis.

Result: Performance decreases when detecting complex machine-crafted text on datasets with strict prompts, emphasizing challenges in sophisticated machine text detection.

Conclusion: The study provides tools, datasets, and analysis that can act as a foundation for future work on academic integrity using NLP techniques.

Abstract: This work addresses critical challenges to academic integrity, including plagiarism, fabrication, and verification of authorship of educational content, by proposing a Natural Language Processing (NLP)-based framework for authenticating students' content through author attribution and style change detection. Despite some initial efforts, several aspects of the topic are yet to be explored. In contrast to existing solutions, the paper provides a comprehensive analysis of the topic by targeting four relevant tasks, including (i) classification of human and machine text, (ii) differentiating in single and multi-authored documents, (iii) author change detection within multi-authored documents, and (iv) author recognition in collaboratively produced documents. The solutions proposed for the tasks are evaluated on two datasets generated with Gemini using two different prompts, including a normal and a strict set of instructions. During experiments, some reduction in the performance of the proposed solutions is observed on the dataset generated through the strict prompt, demonstrating the complexities involved in detecting machine-generated text with cleverly crafted prompts. The generated datasets, code, and other relevant materials are made publicly available on GitHub, which are expected to provide a baseline for future research in the domain.

</details>


### [76] [Racka: Efficient Hungarian LLM Adaptation on Academic Infrastructure](https://arxiv.org/abs/2601.01244)
*Zsolt Csibi,Bence György Gortka,Natabara Gyöngyössy,Kornél Nagy,Dávid Márk Nemeskey,Martin Sallai,András Simonyi,András Márk Szekeres,Gábor Palkó*

Main category: cs.CL

TL;DR: Racka is a lightweight model for bridging Hungarian and high-resource languages using LoRA-based pretraining.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the resource gap between languages like Hungarian and high-resource languages such as English and German.

Method: Racka uses Low-Rank Adaptation (LoRA) on the Qwen-3 4B model for continual pretraining, with a modified tokenizer for better Hungarian tokenization and mixed-language training data.

Result: The model achieves modest but stable adaptation results for Hungarian, without losing performance for high-resource languages.

Conclusion: Racka effectively bridges linguistic gaps through parameter-efficient pretraining while preserving multilingual and Hungarian-specific capabilities.

Abstract: We present Racka, a lightweight, continually pretrained large language model designed to bridge the resource gap between Hungarian and high-resource languages such as English and German. Racka employs parameter-efficient continual pretraining via Low-Rank Adaptation (LoRA) on a Qwen-3 4B backbone, making the recipe practical on A100 (40GB)-based HPC clusters with low inter-node bandwidth. To better match the training distribution, we replace and adapt the tokenizer, achieving substantially improved tokenization fertility for Hungarian while maintaining competitive performance in English and German. The model is trained on 160B subword tokens drawn from a mixture of internet and high-quality curated sources, with a composition of 44% Hungarian, 24% English, 21% German, and 11% code. This data mix is chosen to mitigate catastrophic forgetting and preserve high-resource language capabilities during continual pretraining. Our preliminary results indicate modest but stable results in language adaptation.

</details>


### [77] [From Policy to Logic for Efficient and Interpretable Coverage Assessment](https://arxiv.org/abs/2601.01266)
*Rhitabrat Pokharel,Hamid Hassanzadeh,Ameeta Agrawal*

Main category: cs.CL

TL;DR: The paper introduces a hybrid system combining a coverage-aware retriever and symbolic reasoning to improve efficiency and accuracy in interpreting medical coverage policies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of hallucinations and inconsistencies in LLMs when reviewing subjective and nuanced medical coverage policies.

Method: The presented methodology integrates a coverage-aware retriever with symbolic rule-based reasoning to extract, structure, and audit policy information, reducing reliance on LLM-generated inferences.

Result: The method yields a 44% reduction in inference cost while improving the F1 score by 4.5%, showcasing enhanced efficiency and accuracy.

Conclusion: The approach supports human reviewers by providing more reliable and interpretable policy analyses, presenting a cost-effective and accurate solution.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.

</details>


### [78] [Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory](https://arxiv.org/abs/2601.01280)
*Sen Hu,Yuxiang Wei,Jiaxin Ran,Zhiyuan Yao,Lei Zou*

Main category: cs.CL

TL;DR: Empirical evaluation of graph and non-graph-based approaches for dialog memory systems, providing a unified framework and establishing baseline methodologies.


<details>
  <summary>Details</summary>
Motivation: Inconsistent empirical findings regarding the effectiveness of graph structures in dialog memory systems make it unclear which design choices are most impactful.

Method: A unified framework decomposing dialog memory systems into components, followed by controlled experiments on memory representation, organization, maintenance, and retrieval using LongMemEval and HaluMem datasets.

Result: Performance differences are mainly influenced by foundational system settings rather than architectural innovations.

Conclusion: Stable and reliable baselines are proposed to guide future dialog memory system research.

Abstract: Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.

</details>


### [79] [T3C: Test-Time Tensor Compression with Consistency Guarantees](https://arxiv.org/abs/2601.01299)
*Ismail Lamaakal,Chaymae Yahyati,Yassine Maleh,Khalid El Makkaoui,Ibrahim Ouahbi*

Main category: cs.CL

TL;DR: T3C is a compression framework enabling adaptive and efficient model deployment by tuning rank and precision for resources like latency and energy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations in traditional static compression methods, enabling real-time optimization based on hardware constraints and deployment budgets.

Method: The authors introduce T3C, which combines elastic tensor factorization, mixed-precision quantization, and a lightweight policy controller to achieve adaptive model compression with minimal accuracy loss.

Result: On ImageNet-1k, T3C demonstrates significant performance improvements in latency and size compared to baseline models while maintaining competitive accuracy.

Conclusion: T3C establishes a new standard for flexibility and efficiency in model compression, providing on-demand trade-offs backed by reliability certificates.

Abstract: We present T3C, a train-once, test-time budget-conditioned compression framework that exposes rank and precision as a controllable deployment knob. T3C combines elastic tensor factorization (maintained up to a maximal rank) with rank-tied mixed-precision quantization and a lightweight controller that maps a latency/energy/size budget token to per-layer rank/bit assignments; the policy snaps to hardware-aligned profiles and is monotone in the budget. A fast, layerwise consistency certificate, computed from spectral proxies and activation statistics, upper-bounds logit drift and regularizes training, yielding a practical reliability signal with negligible overhead. On ImageNet-1k, T3C shifts the vision Pareto frontier: for ResNet-50 at matched accuracy (\leq 0.5% drop), p50 latency is 1.18ms with a 38MB model, outperforming PTQ-8b (1.44ms, 88MB); for ViT-B/16, T3C reaches 2.30ms p50 with 59MB, improving over strong PTQ/QAT baselines. A single T3C checkpoint therefore provides predictable, certificate-backed accuracy-latency-size trade-offs on demand across devices.

</details>


### [80] [FLOP-Efficient Training: Early Stopping Based on Test-Time Compute Awareness](https://arxiv.org/abs/2601.01332)
*Hossam Amer,Maryam Dialameh,Hossein Rajabzadeh,Walid Ahmed,Weiwei Zhang,Yang Liu*

Main category: cs.CL

TL;DR: The paper introduces an approach to reduce training compute for language models by utilizing test-time compute (TTC) configurations.


<details>
  <summary>Details</summary>
Motivation: Training large language models is computationally expensive. Leveraging test-time compute can potentially reduce overall training costs while maintaining or improving model accuracy.

Method: The authors introduce TTC-aware training and propose an early stopping algorithm that identifies the optimal checkpoint and TTC configuration to balance training and inference compute. They develop efficient evaluation techniques and formalize a break-even bound for this purpose.

Result: Experiments show up to 92% reductions in training FLOPs while achieving equal or even improved model accuracy compared to traditionally trained models.

Conclusion: This work provides a novel perspective on balancing training and inference compute, enabling faster model development cycles and promoting efficient resource usage.

Abstract: Scaling training compute, measured in FLOPs, has long been shown to improve the accuracy of large language models, yet training remains resource-intensive. Prior work shows that increasing test-time compute (TTC)-for example through iterative sampling-can allow smaller models to rival or surpass much larger ones at lower overall cost. We introduce TTC-aware training, where an intermediate checkpoint and a corresponding TTC configuration can together match or exceed the accuracy of a fully trained model while requiring substantially fewer training FLOPs. Building on this insight, we propose an early stopping algorithm that jointly selects a checkpoint and TTC configuration to minimize training compute without sacrificing accuracy. To make this practical, we develop an efficient TTC evaluation method that avoids exhaustive search, and we formalize a break-even bound that identifies when increased inference compute compensates for reduced training compute. Experiments demonstrate up to 92\% reductions in training FLOPs while maintaining and sometimes remarkably improving accuracy. These results highlight a new perspective for balancing training and inference compute in model development, enabling faster deployment cycles and more frequent model refreshes. Codes will be publicly released.

</details>


### [81] [Reasoning Over Recall: Evaluating the Efficacy of Generalist Architectures vs. Specialized Fine-Tunes in RAG-Based Mental Health Dialogue Systems](https://arxiv.org/abs/2601.01341)
*Md Abdullah Al Kafi,Raka Moni,Sumit Kumar Banshal*

Main category: cs.CL

TL;DR: The paper evaluates whether general large language models or domain-specific fine-tuned models perform better in RAG-based mental health counseling, finding generalist models superior in empathy and contextual understanding despite their smaller size.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of hallucinations and lack of empathy in deploying LLMs for mental health counseling, and to determine whether fine-tuning on mental health data is more effective than using general models with reasoning capabilities.

Method: The study compared four open-source models in a RAG pipeline using ChromaDB. Two generalist models and two mental health fine-tuned models were tested over 50 turns, evaluated using an LLM-as-a-Judge framework for metrics like empathy and safety.

Result: Generalist models outperformed fine-tuned mental health models in empathy and contextual understanding (3.72 vs. 3.26, $p < 0.001$) despite being smaller. All models performed well in safety, but domain-specific models showed overfitting tendencies.

Conclusion: Strong reasoning capabilities in generalist models matter more than specific mental health vocabulary training in the RAG paradigm. Generalist models offer empathetic, balanced support when grounded in clinical evidence, outperforming narrowly fine-tuned models.

Abstract: The deployment of Large Language Models (LLMs) in mental health counseling faces the dual challenges of hallucinations and lack of empathy. While the former may be mitigated by RAG (retrieval-augmented generation) by anchoring answers in trusted clinical sources, there remains an open question as to whether the most effective model under this paradigm would be one that is fine-tuned on mental health data, or a more general and powerful model that succeeds purely on the basis of reasoning. In this paper, we perform a direct comparison by running four open-source models through the same RAG pipeline using ChromaDB: two generalist reasoners (Qwen2.5-3B and Phi-3-Mini) and two domain-specific fine-tunes (MentalHealthBot-7B and TherapyBot-7B). We use an LLM-as-a-Judge framework to automate evaluation over 50 turns. We find a clear trend: the generalist models outperform the domain-specific ones in empathy (3.72 vs. 3.26, $p < 0.001$) in spite of being much smaller (3B vs. 7B), and all models perform well in terms of safety, but the generalist models show better contextual understanding and are less prone to overfitting as we observe in the domain-specific models. Overall, our results indicate that for RAG-based therapy systems, strong reasoning is more important than training on mental health-specific vocabulary; i.e. a well-reasoned general model would provide more empathetic and balanced support than a larger narrowly fine-tuned model, so long as the answer is already grounded in clinical evidence.

</details>


### [82] [FC-CONAN: An Exhaustively Paired Dataset for Robust Evaluation of Retrieval Systems](https://arxiv.org/abs/2601.01350)
*Juan Junqueras,Florian Boudin,May-Myo Zin,Ha-Thanh Nguyen,Wachara Fungwacharakorn,Damián Ariel Furman,Akiko Aizawa,Ken Satoh*

Main category: cs.CL

TL;DR: The paper introduces FC-CONAN, a fully connected dataset linking hate speech (HS) and counter-narratives (CNs) by exhaustively annotating combinations of 45 HS messages and 129 CNs.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of sparse annotation in existing HS-CN datasets and enable improved counterspeech system evaluations.

Method: Developed FC-CONAN using a rigorous two-stage annotation process with multiple annotators and validators, ensuring data reliability and scalability.

Result: Created four dataset partitions (Diamond, Gold, Silver, Bronze) with non-overlapping entries, uncovering numerous previously unlabeled HS-CN pairs.

Conclusion: FC-CONAN enhances the evaluation capabilities of counterspeech systems, fostering better resource availability for robust error analysis and research.

Abstract: Hate speech (HS) is a critical issue in online discourse, and one promising strategy to counter it is through the use of counter-narratives (CNs). Datasets linking HS with CNs are essential for advancing counterspeech research. However, even flagship resources like CONAN (Chung et al., 2019) annotate only a sparse subset of all possible HS-CN pairs, limiting evaluation. We introduce FC-CONAN (Fully Connected CONAN), the first dataset created by exhaustively considering all combinations of 45 English HS messages and 129 CNs. A two-stage annotation process involving nine annotators and four validators produces four partitions-Diamond, Gold, Silver, and Bronze-that balance reliability and scale. None of the labeled pairs overlap with CONAN, uncovering hundreds of previously unlabelled positives. FC-CONAN enables more faithful evaluation of counterspeech retrieval systems and facilitates detailed error analysis. The dataset is publicly available.

</details>


### [83] [Investigating the Multilingual Calibration Effects of Language Model Instruction-Tuning](https://arxiv.org/abs/2601.01362)
*Jerry Huang,Peng Lu,Qiuhao Zeng,Yusuke Iwasawa,Yutaka Matsuo,Sarath Chandar,Edison Marrese-Taylor,Irene Li*

Main category: cs.CL

TL;DR: This paper explores the calibration of large language models (LLMs) in multilingual contexts, analyzing their reliability and fairness when addressing data scarcity and evaluating improvement methods.


<details>
  <summary>Details</summary>
Motivation: Investigation centers on understanding the calibration of LLMs in multilingual settings, especially addressing the gap caused by data scarcity and its impact on model trustworthiness and reliability.

Method: The study analyzed two multilingual benchmarks across 29 and 42 languages, examining the effects of instruction-tuning on high-resource language SFT datasets and testing label smoothing techniques to address calibration shortcomings.

Result: Findings revealed that instruction-tuning significantly boosts model confidence in low-resource languages, but improvements in accuracy are marginal or nonexistent, leading to mis-calibration. Label smoothing was effective in maintaining better calibration without using low-resource SFT data.

Conclusion: Multilingual considerations are crucial for training and tuning LLMs to enhance their reliability and fairness in downstream tasks, with techniques like label smoothing offering promising solutions to calibration issues.

Abstract: Ensuring that deep learning models are well-calibrated in terms of their predictive uncertainty is essential in maintaining their trustworthiness and reliability, yet despite increasing advances in foundation model research, the relationship between such large language models (LLMs) and their calibration remains an open area of research. In this work, we look at a critical gap in the calibration of LLMs within multilingual settings, in an attempt to better understand how the data scarcity can potentially lead to different calibration effects and how commonly used techniques can apply in these settings. Our analysis on two multilingual benchmarks, over 29 and 42 languages respectively, reveals that even in low-resource languages, model confidence can increase significantly after instruction-tuning on high-resource language SFT datasets. However, improvements in accuracy are marginal or non-existent, resulting in mis-calibration, highlighting a critical shortcoming of standard SFT for multilingual languages. Furthermore, we observe that the use of label smoothing to be a reasonable method alleviate this concern, again without any need for low-resource SFT data, maintaining better calibration across all languages. Overall, this highlights the importance of multilingual considerations for both training and tuning LLMs in order to improve their reliability and fairness in downstream use.

</details>


### [84] [EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery](https://arxiv.org/abs/2601.01400)
*Jicheng Ma,Guohua Wang,Xinhua Feng,Yiming Liu,Zhichao Hu,Yuhong Liu*

Main category: cs.CL

TL;DR: The paper proposes an automated pipeline for evaluating advanced mathematical reasoning in large language models using contemporary research literature, showcasing the limitations of current models and emphasizing the need for evolving evaluation systems.


<details>
  <summary>Details</summary>
Motivation: To address limitations in static benchmarks that fail to cover research-level mathematics comprehensively and cannot adapt to the evolving nature of human mathematical discovery.

Method: A theorem-grounded, automated pipeline is introduced that transforms recent mathematical research papers into executable tasks. It involves identifying results, generating problem templates, and creating deterministic solutions for scalable, reproducible evaluation.

Result: The pipeline creates EternalMath, an evolving evaluation suite derived from contemporary mathematical papers, revealing significant performance gaps in current state-of-the-art LLMs.

Conclusion: Mathematical reasoning in state-of-the-art LLMs remains immature, requiring dynamic and continuously updating evaluation methods to keep pace with ongoing mathematical research.

Abstract: Current evaluations of mathematical reasoning in large language models (LLMs) are dominated by static benchmarks, either derived from competition-style problems or curated through costly expert effort, resulting in limited coverage of research-level mathematics and rapid performance saturation. We propose a fully automated, theorem-grounded pipeline for evaluating frontier mathematical reasoning, which directly transforms recent peer-reviewed mathematical literature into executable and verifiable reasoning tasks. The pipeline identifies constructive or quantitative results, instantiates them into parameterized problem templates, and generates deterministic solutions through execution-based verification, enabling scalable, reproducible, and continuously updatable evaluation without reliance on large-scale expert authoring. By design, this approach supports temporal extensibility, intrinsic correctness checking, and domain-specific customization across mathematical subfields. Applying this pipeline yields \textbf{EternalMath}, an evolving evaluation suite derived from contemporary research papers. Experiments with state-of-the-art LLMs reveal substantial performance gaps, indicating that mathematical reasoning at the research frontier remains far from saturated and underscoring the need for evaluation methodologies that evolve in step with human mathematical discovery.

</details>


### [85] [LANCET: Neural Intervention via Structural Entropy for Mitigating Faithfulness Hallucinations in LLMs](https://arxiv.org/abs/2601.01401)
*Chenxu Wang,Chaozhuo Li,Pengbo Wang,Litian Zhang,Songyang Liu,Ji Qi,Jiahui Hu,Yushan Cai,Hao Zhao,Rui Pu*

Main category: cs.CL

TL;DR: This paper introduces Lancet, a framework targeting hallucination issues in Large Language Models by precisely intervening in neural pathways.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) are often undermined by hallucinations, which compromise reliability. Current methods fail to address such issues effectively due to their imprecise neural interventions.

Method: Lancet identifies hallucination-prone neurons through gradient-driven contrastive analysis, traces their pathways by minimizing structural entropy, and applies a hierarchical intervention strategy while maintaining the model's general capabilities.

Result: Lancet outperforms existing state-of-the-art methods in managing hallucinations, as demonstrated in multiple benchmark datasets.

Conclusion: The proposed Lancet framework offers a precise and effective solution for addressing hallucinations in LLMs, improving their reliability while preserving overall performance.

Abstract: Large Language Models have revolutionized information processing, yet their reliability is severely compromised by faithfulness hallucinations. While current approaches attempt to mitigate this issue through node-level adjustments or coarse suppression, they often overlook the distributed nature of neural information, leading to imprecise interventions. Recognizing that hallucinations propagate through specific forward transmission pathways like an infection, we aim to surgically block this flow using precise structural analysis. To leverage this, we propose Lancet, a novel framework that achieves precise neural intervention by leveraging structural entropy and hallucination difference ratios. Lancet first locates hallucination-prone neurons via gradient-driven contrastive analysis, then maps their propagation pathways by minimizing structural entropy, and finally implements a hierarchical intervention strategy that preserves general model capabilities. Comprehensive evaluations across hallucination benchmark datasets demonstrate that Lancet significantly outperforms state-of-the-art methods, validating the effectiveness of our surgical approach to neural intervention.

</details>


### [86] [From Emotion Classification to Emotional Reasoning: Enhancing Emotional Intelligence in Large Language Models](https://arxiv.org/abs/2601.01407)
*Arjhun Sreedar,Rohan Pillay,Laukik Patade*

Main category: cs.CL

TL;DR: The paper explores enhancing emotional reasoning in smaller large language models by using synthetic emotional reasoning data in therapy-style conversations, achieving significant improvements.


<details>
  <summary>Details</summary>
Motivation: The study aims to determine if synthetic emotional data can effectively improve emotional reasoning in smaller open LLMs, addressing the challenge of emotional awareness without requiring architectural changes.

Method: A multi-agent generation pipeline creates therapy-style synthetic data which is structured into multiple-choice questions with explanations for fine-tuning smaller 7B-scale LLMs.

Result: The fine-tuned Mistral 7B model shows substantial emotional reasoning improvements, with a 10 to 20-point rise on emotional understanding and a 20-point increase in emotional awareness evaluations.

Conclusion: Synthetic emotional reasoning data, when used to fine-tune smaller LLMs, enhances their capability for nuanced emotional tasks without needing architectural modifications.

Abstract: This work investigates whether synthetic emotional chain-of-thought data can improve the emotional reasoning abilities of smaller open large language models (LLMs). We design a multi-agent generation pipeline that produces therapy-style conversations and converts them into structured emotion multiple-choice questions (MCQs) with explanations. We propose that fine-tuning a variety of 7B models on this dataset should yield substantial gains in emotional understanding and emotional awareness on EmoBench-style evaluations, suggesting that emotional reasoning can be induced without architectural changes. Our results demonstrate that fine-tuned Mistral 7B achieves EU improvements from 10.5 to 20.5 and EA improvements from 40.5 to 60.0, validating the effectiveness of synthetic emotional reasoning data for enhancing model capabilities in nuanced emotional tasks.

</details>


### [87] [iFlip: Iterative Feedback-driven Counterfactual Example Refinement](https://arxiv.org/abs/2601.01446)
*Yilong Wang,Qianli Wang,Nils Feldhus*

Main category: cs.CL

TL;DR: Presents iFlip, an iterative refinement approach for generating valid counterfactuals with large language models, achieving higher validity and improving model robustness.


<details>
  <summary>Details</summary>
Motivation: Generating valid counterfactual examples with large language models is essential for explainable AI and NLP tasks, yet current methods struggle due to the lack of iterative refinement.

Method: Proposes iFlip, which iteratively integrates feedback from model confidence, feature attribution, and natural language to refine counterfactual generation.

Result: iFlip shows 57.8% higher validity than state-of-the-art baselines, improves data augmentation quality, and demonstrates superior user satisfaction and feasibility in a user study.

Conclusion: iFlip effectively generates valid counterfactuals, improving large language model robustness and performance through iterative refinement and feedback integration.

Abstract: Counterfactual examples are minimal edits to an input that alter a model's prediction. They are widely employed in explainable AI to probe model behavior and in natural language processing (NLP) to augment training data. However, generating valid counterfactuals with large language models (LLMs) remains challenging, as existing single-pass methods often fail to induce reliable label changes, neglecting LLMs' self-correction capabilities. To explore this untapped potential, we propose iFlip, an iterative refinement approach that leverages three types of feedback, including model confidence, feature attribution, and natural language. Our results show that iFlip achieves an average 57.8% higher validity than the five state-of-the-art baselines, as measured by the label flipping rate. The user study further corroborates that iFlip outperforms baselines in completeness, overall satisfaction, and feasibility. In addition, ablation studies demonstrate that three components are paramount for iFlip to generate valid counterfactuals: leveraging an appropriate number of iterations, pointing to highly attributed words, and early stopping. Finally, counterfactuals generated by iFlip enable effective counterfactual data augmentation, substantially improving model performance and robustness.

</details>


### [88] [Segmentation and Processing of German Court Decisions from Open Legal Data](https://arxiv.org/abs/2601.01449)
*Harshil Darji,Martin Heckelmann,Christina Kratsch,Gerard de Melo*

Main category: cs.CL

TL;DR: The paper introduces a cleaned and sectioned dataset of 251,038 German court decisions derived from Open Legal Data with consistent formatting for key sections, making it a valuable resource for NLP tasks.


<details>
  <summary>Details</summary>
Motivation: The study addresses the inconsistency in the formatting of decision texts in the German Open Legal Data dataset, which hinders tasks like rhetorical role classification, retrieval, and citation analysis.

Method: The authors systematically separated legal decision texts into key sections, used statistical sampling with Cochran’s formula to verify reliability, and manually validated a sample for accuracy.

Result: They created a publicly available, structured dataset in JSONL format, containing clearly separated sections from 251,038 German court decisions and included an additional procedural field (appeal notice).

Conclusion: The new dataset facilitates advanced NLP research on the German legal system by providing a reliable and structured resource for analyzing court decisions.

Abstract: The availability of structured legal data is important for advancing Natural Language Processing (NLP) techniques for the German legal system. One of the most widely used datasets, Open Legal Data, provides a large-scale collection of German court decisions. While the metadata in this raw dataset is consistently structured, the decision texts themselves are inconsistently formatted and often lack clearly marked sections. Reliable separation of these sections is important not only for rhetorical role classification but also for downstream tasks such as retrieval and citation analysis. In this work, we introduce a cleaned and sectioned dataset of 251,038 German court decisions derived from the official Open Legal Data dataset. We systematically separated three important sections in German court decisions, namely Tenor (operative part of the decision), Tatbestand (facts of the case), and Entscheidungsgründe (judicial reasoning), which are often inconsistently represented in the original dataset. To ensure the reliability of our extraction process, we used Cochran's formula with a 95% confidence level and a 5% margin of error to draw a statistically representative random sample of 384 cases, and manually verified that all three sections were correctly identified. We also extracted the Rechtsmittelbelehrung (appeal notice) as a separate field, since it is a procedural instruction and not part of the decision itself. The resulting corpus is publicly available in the JSONL format, making it an accessible resource for further research on the German legal system.

</details>


### [89] [Bridging the gap: A comparative exploration of Speech-LLM and end-to-end architecture for multilingual conversational ASR](https://arxiv.org/abs/2601.01461)
*Yuxiang Mei,Dongxing Xu,Jiaen Liang,Yanhua Long*

Main category: cs.CL

TL;DR: The paper introduces an enhanced framework for multilingual conversational ASR integrating Whisper and mHuBERT encoders with a large language model (LLM). It achieves competitive performance but notes that fine-tuned end-to-end Whisper models still outperform LLM-based ASR.


<details>
  <summary>Details</summary>
Motivation: To address limitations in a prior ASR system (SHNU-mASR) including ineffective feature concatenation and performance gaps between LLM-based ASR and E2E counterpart systems.

Method: It evaluates E2E Whisper models using fine-tuning techniques and proposes cross-attention-based fusion mechanisms for improved integration of parallel speech encoders with LLM.

Result: Their system achieves a CER/WER of 10.69% in the MLC-SLM Challenge, utilizing significantly smaller baseline training data compared to competing systems.

Conclusion: Although achieving competitive results, the LLM-based ASR still falls short of fine-tuned E2E Whisper models, suggesting potential areas for optimization in future Speech-LLM system designs.

Abstract: The INTERSPEECH 2025 Challenge on Multilingual Conversational Speech Language Models (MLC-SLM) promotes multilingual conversational ASR with large language models (LLMs). Our previous SHNU-mASR system adopted a competitive parallel-speech-encoder architecture that integrated Whisper and mHuBERT with an LLM. However, it faced two challenges: simple feature concatenation may not fully exploit complementary information, and the performance gap between LLM-based ASR and end-to-end(E2E) encoder-decoder ASR remained unexplored. In this work, we present an enhanced LLM-based ASR framework that combines fine-tuned Whisper and mHuBERT encoders with an LLM to enrich speech representations. We first evaluate E2E Whisper models with LoRA and full fine-tuning on the MLC-SLM ASR task, and then propose cross-attention-based fusion mechanisms for the parallel-speech-encoder. On the official evaluation set of the MLC-SLM Challenge, our system achieves a CER/WER of 10.69%, ranking on par with the top-ranked Track 1 systems, even though it uses only 1,500 hours of baseline training data compared with their large-scale training sets. Nonetheless, we find that our final LLM-based ASR still does not match the performance of a fine-tuned E2E Whisper model, providing valuable empirical guidance for future Speech-LLM design. Our code is publicly available at https://github.com/1535176727/MLC-SLM.

</details>


### [90] [Can Legislation Be Made Machine-Readable in PROLEG?](https://arxiv.org/abs/2601.01477)
*May-Myo Zin,Sabine Wehnert,Yuntao Kong,Ha-Thanh Nguyen,Wachara Fungwacharakorn,Jieying Xue,Michał Araszkiewicz,Randy Goebel,Ken Satoh,Le-Minh Nguyen*

Main category: cs.CL

TL;DR: The paper introduces a framework that uses AI technologies like LLMs and PROLEG to create executable regulatory tools, demonstrated with Article 6 of GDPR.


<details>
  <summary>Details</summary>
Motivation: The accurate and efficient application of regulatory processes is essential to achieve positive social impact, prompting exploration into modern AI technologies for this challenge.

Method: The framework employs LLMs to convert legal text into if-then rules and PROLEG encoding, which are refined by legal experts, producing a validated executable program.

Result: The framework successfully transforms legal documents (e.g., GDPR Article 6) into executable decisions using AI tools, validated by domain experts, and provides interpretable outputs.

Conclusion: The approach demonstrates AI's potential in regulatory applications, with suggestions for improving tools to better represent and deploy legal frameworks.

Abstract: The anticipated positive social impact of regulatory processes requires both the accuracy and efficiency of their application. Modern artificial intelligence technologies, including natural language processing and machine-assisted reasoning, hold great promise for addressing this challenge. We present a framework to address the challenge of tools for regulatory application, based on current state-of-the-art (SOTA) methods for natural language processing (large language models or LLMs) and formalization of legal reasoning (the legal representation system PROLEG). As an example, we focus on Article 6 of the European General Data Protection Regulation (GDPR). In our framework, a single LLM prompt simultaneously transforms legal text into if-then rules and a corresponding PROLEG encoding, which are then validated and refined by legal domain experts. The final output is an executable PROLEG program that can produce human-readable explanations for instances of GDPR decisions. We describe processes to support the end-to-end transformation of a segment of a regulatory document (Article 6 from GDPR), including the prompting frame to guide an LLM to "compile" natural language text to if-then rules, then to further "compile" the vetted if-then rules to PROLEG. Finally, we produce an instance that shows the PROLEG execution. We conclude by summarizing the value of this approach and note observed limitations with suggestions to further develop such technologies for capturing and deploying regulatory frameworks.

</details>


### [91] [Four Quadrants of Difficulty: A Simple Categorisation and its Limits](https://arxiv.org/abs/2601.01488)
*Vanessa Toborek,Sebastian Müller,Christian Bauckhage*

Main category: cs.CL

TL;DR: The paper evaluates difficulty signals used in Curriculum Learning (CL) for NLP and suggests rethinking assumptions underlying CL by proposing more effective task-dependent difficulty estimations.


<details>
  <summary>Details</summary>
Motivation: To address the misalignment between widely used difficulty signals and what neural models actually find challenging, challenging common assumptions in Curriculum Learning for NLP.

Method: The authors propose a four-quadrant framework to classify difficulty signals based on human vs. model perspectives and task-agnostic vs. task-dependent metrics, systematically evaluating these on an NLU dataset.

Result: The analysis reveals that task-agnostic signals work independently, while task-dependent signals better align with model learning processes, questioning conventional CL strategies using intuition-based estimations.

Conclusion: The paper advocates for lightweight, task-dependent difficulty estimators to better represent neural models' learning behavior, emphasizing the limitations of task-agnostic approaches.

Abstract: Curriculum Learning (CL) aims to improve the outcome of model training by estimating the difficulty of samples and scheduling them accordingly. In NLP, difficulty is commonly approximated using task-agnostic linguistic heuristics or human intuition, implicitly assuming that these signals correlate with what neural models find difficult to learn. We propose a four-quadrant categorisation of difficulty signals -- human vs. model and task-agnostic vs. task-dependent -- and systematically analyse their interactions on a natural language understanding dataset. We find that task-agnostic features behave largely independently and that only task-dependent features align. These findings challenge common CL intuitions and highlight the need for lightweight, task-dependent difficulty estimators that better reflect model learning behaviour.

</details>


### [92] [Distortion Instead of Hallucination: The Effect of Reasoning Under Strict Constraints](https://arxiv.org/abs/2601.01490)
*Junichiro Niimi*

Main category: cs.CL

TL;DR: This paper explores how reasoning affects the reliability of large language models (LLMs) under strict constraints, revealing trade-offs between constraint compliance and factual accuracy.


<details>
  <summary>Details</summary>
Motivation: To investigate whether reasoning within LLMs improves reliability when operating in a closed system without external tools or knowledge.

Method: Experimental analysis of reasoning effects across two models (GPT-5.2 and Gemini 3 Flash) with constraints involving peer-reviewed journal recommendations.

Result: Reasoning reduces constraint violations but increases factual distortion and fabrication, showing a trade-off between compliance and accuracy consistent across models.

Conclusion: Reasoning does not universally enhance reliability in LLMs; it trades off constraint violations for more subtle distortions, reflecting fundamental limitations.

Abstract: With the widespread adoption of large language models (LLMs), hallucinations, which are non-factual fabrications in model outputs, have become serious concerns. Reasoning capabilities have received attention as a self-verification process to improve output reliability. However, the effect of reasoning within a closed system where LLMs cannot rely on external tools or knowledge has yet to be clarified. We therefore conduct experiments under strict constraints (recommending peer-reviewed journal articles in computer science) to examine the effect of reasoning across multiple models (GPT-5.2 and Gemini 3 Flash). Our results reveal a problematic trade-off between constraint compliance and factual accuracy. Non-reasoning models exhibit high constraint violation rates (66-75%) but maintain factual accuracy, while reasoning models reduce violations (13-26%) but systematically distort known facts to satisfy constraints and increase complete fabrication. This trade-off pattern is consistent across both models despite different architectures, indicating a fundamental limitation of reasoning. Furthermore, reasoning does not uniformly improve output authenticity: effects diverge by model, reflecting different allocations of the compliance-truthfulness trade-off. These findings challenge the assumption that reasoning universally improves reliability: reasoning models trade honest constraint violations for detection-resistant distortions.

</details>


### [93] [From Failure to Mastery: Generating Hard Samples for Tool-use Agents](https://arxiv.org/abs/2601.01498)
*Bingguang Hao,Zengzhuang Xu,Yuntao Wen,Xinyi Xu,Yang Liu,Tong Zhao,Maolin Wang,Long Chen,Dong Wang,Yicheng Chen,Cunyin Peng,Xiangyu Zhao,Chenyi Zhuang,Ji Zhang*

Main category: cs.CL

TL;DR: The paper presents HardGen, a pipeline to generate complex training data for tool-use by LLMs, which improves their reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing data generation methods produce overly simple and homogeneous training samples for LLMs, which lack complex reasoning and logical depth.

Method: HardGen is an agentic pipeline using a dynamic API Graph from failure cases to generate challenging samples, modular tools to create advanced queries, and a feedback loop for continuous improvement.

Result: A 4B parameter model trained on HardGen's dataset outperformed leading LLMs like GPT-5.2 and Claude-Opus-4.5 in evaluations.

Conclusion: HardGen enhances training data complexity, improving reasoning performance in LLM agents, and its open-sourcing will aid further research.

Abstract: The advancement of LLM agents with tool-use capabilities requires diverse and complex training corpora. Existing data generation methods, which predominantly follow a paradigm of random sampling and shallow generation, often yield simple and homogeneous trajectories that fail to capture complex, implicit logical dependencies. To bridge this gap, we introduce HardGen, an automatic agentic pipeline designed to generate hard tool-use training samples with verifiable reasoning. Firstly, HardGen establishes a dynamic API Graph built upon agent failure cases, from which it samples to synthesize hard traces. Secondly, these traces serve as conditional priors to guide the instantiation of modular, abstract advanced tools, which are subsequently leveraged to formulate hard queries. Finally, the advanced tools and hard queries enable the generation of verifiable complex Chain-of-Thought (CoT), with a closed-loop evaluation feedback steering the continuous refinement of the process. Extensive evaluations demonstrate that a 4B parameter model trained with our curated dataset achieves superior performance compared to several leading open-source and closed-source competitors (e.g., GPT-5.2, Gemini-3-Pro and Claude-Opus-4.5). Our code, models, and dataset will be open-sourced to facilitate future research.

</details>


### [94] [EmoHarbor: Evaluating Personalized Emotional Support by Simulating the User's Internal World](https://arxiv.org/abs/2601.01530)
*Jing Ye,Lu Xiang,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: EmoHarbor is an automated evaluation framework for emotional support systems that emphasizes personalized support over generic empathy, showcasing deficiencies in leading LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for emotional support conversations fail to assess personalized responses tailored to individual users' psychological profiles and contexts.

Method: EmoHarbor employs a Chain-of-Agent architecture, simulating users’ inner world to assess emotional support quality across diverse user profiles and defined evaluation dimensions.

Result: Testing 20 advanced LLMs with EmoHarbor demonstrates their inability to provide personalized emotional support, despite excelling in empathetic response generation.

Conclusion: The research shifts focus toward creating emotional support systems that prioritize personalization and user-aware interactions over generic empathy.

Abstract: Current evaluation paradigms for emotional support conversations tend to reward generic empathetic responses, yet they fail to assess whether the support is genuinely personalized to users' unique psychological profiles and contextual needs. We introduce EmoHarbor, an automated evaluation framework that adopts a User-as-a-Judge paradigm by simulating the user's inner world. EmoHarbor employs a Chain-of-Agent architecture that decomposes users' internal processes into three specialized roles, enabling agents to interact with supporters and complete assessments in a manner similar to human users. We instantiate this benchmark using 100 real-world user profiles that cover a diverse range of personality traits and situations, and define 10 evaluation dimensions of personalized support quality. Comprehensive evaluation of 20 advanced LLMs on EmoHarbor reveals a critical insight: while these models excel at generating empathetic responses, they consistently fail to tailor support to individual user contexts. This finding reframes the central challenge, shifting research focus from merely enhancing generic empathy to developing truly user-aware emotional support. EmoHarbor provides a reproducible and scalable framework to guide the development and evaluation of more nuanced and user-aware emotional support systems.

</details>


### [95] [Bridging the Data Gap: Creating a Hindi Text Summarization Dataset from the English XSUM](https://arxiv.org/abs/2601.01543)
*Praveenkumar Katwe,RakeshChandra Balabantaray,Kaliprasad Vittala*

Main category: cs.CL

TL;DR: The paper introduces an automated framework for creating a Hindi text summarization dataset by utilizing translation and validation techniques based on the English XSUM dataset.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the lack of high-quality text summarization datasets available for low-resource languages like Hindi.

Method: The authors employ translation and linguistic adaptation of the English XSUM dataset, validated using COMET, and supported by Large Language Models (LLMs) for curation.

Result: The framework produces a diverse Hindi text summarization dataset while maintaining fidelity and thematic complexity.

Conclusion: This approach provides Hindi NLP researchers with a high-quality dataset and demonstrates scalability for creating resources for other low-resource languages.

Abstract: Current advancements in Natural Language Processing (NLP) have largely favored resource-rich languages, leaving a significant gap in high-quality datasets for low-resource languages like Hindi. This scarcity is particularly evident in text summarization, where the development of robust models is hindered by a lack of diverse, specialized corpora.
  To address this disparity, this study introduces a cost-effective, automated framework for creating a comprehensive Hindi text summarization dataset. By leveraging the English Extreme Summarization (XSUM) dataset as a source, we employ advanced translation and linguistic adaptation techniques. To ensure high fidelity and contextual relevance, we utilize the Crosslingual Optimized Metric for Evaluation of Translation (COMET) for validation, supplemented by the selective use of Large Language Models (LLMs) for curation.
  The resulting dataset provides a diverse, multi-thematic resource that mirrors the complexity of the original XSUM corpus. This initiative not only provides a direct tool for Hindi NLP research but also offers a scalable methodology for democratizing NLP in other underserved languages. By reducing the costs associated with dataset creation, this work fosters the development of more nuanced, culturally relevant models in computational linguistics.

</details>


### [96] [HalluZig: Hallucination Detection using Zigzag Persistence](https://arxiv.org/abs/2601.01552)
*Shreyas N. Samaga,Gilberto Gonzalez Arroyo,Tamal K. Dey*

Main category: cs.CL

TL;DR: The paper introduces HalluZig, a novel hallucination detection method for large language models, using topological analysis of attention dynamics.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of hallucinated outputs from LLMs limiting their reliability in critical applications.

Method: Analyzed the sequence of layer-wise attention using zigzag graph filtration and extracted topological signatures with zigzag persistence.

Result: HalluZig outperformed existing baselines in detecting hallucinations and showed generalizability across models and using partial network depth.

Conclusion: HalluZig advances hallucination detection by leveraging unique topological structures in attention dynamics, offering a promising direction for more reliable LLM outputs.

Abstract: The factual reliability of Large Language Models (LLMs) remains a critical barrier to their adoption in high-stakes domains due to their propensity to hallucinate. Current detection methods often rely on surface-level signals from the model's output, overlooking the failures that occur within the model's internal reasoning process. In this paper, we introduce a new paradigm for hallucination detection by analyzing the dynamic topology of the evolution of model's layer-wise attention. We model the sequence of attention matrices as a zigzag graph filtration and use zigzag persistence, a tool from Topological Data Analysis, to extract a topological signature. Our core hypothesis is that factual and hallucinated generations exhibit distinct topological signatures. We validate our framework, HalluZig, on multiple benchmarks, demonstrating that it outperforms strong baselines. Furthermore, our analysis reveals that these topological signatures are generalizable across different models and hallucination detection is possible only using structural signatures from partial network depth.

</details>


### [97] [Steerability of Instrumental-Convergence Tendencies in LLMs](https://arxiv.org/abs/2601.01584)
*Jakub Hoscilowicz*

Main category: cs.CL

TL;DR: The paper studies the relationship between the capability and steerability of AI systems, finding that higher capability doesn't necessarily reduce steerability. It introduces a safety-security tension in open-weight models and demonstrates steerability reduction methods to mitigate risky behaviors.


<details>
  <summary>Details</summary>
Motivation: The motivation is to examine the interaction between AI model capability and steerability, addressing safety concerns like preventing misbehavior while securing models against exploitation.

Method: The authors distinguish between authorized and unauthorized steerability to highlight safety-security conflicts in AI design. Experiments with Qwen3 models and InstrumentalEval show how certain prompting techniques, like anti-instrumental suffixes, affect the model's behavior.

Result: The study shows that anti-instrumental prompting significantly reduces convergence to risky behaviors in AI models. Moreover, larger aligned models perform better than smaller ones under anti-instrumental prompting.

Conclusion: The paper concludes that steerability methods can be used to mitigate safety risks effectively. However, there is a fundamental tension between safety and security in open-weight models, underlining the need for careful design choices to balance these aspects.

Abstract: We examine two properties of AI systems: capability (what a system can do) and steerability (how reliably one can shift behavior toward intended outcomes). In our experiments, higher capability does not imply lower steerability. We distinguish between authorized steerability (builders reliably reaching intended behaviors) and unauthorized steerability (attackers eliciting disallowed behaviors). This distinction highlights a fundamental safety--security dilemma for open-weight AI models: safety requires high steerability to enforce control (e.g., stop/refuse), while security requires low steerability to prevent malicious actors from eliciting harmful behaviors. This tension is acute for open-weight models, which are currently highly steerable via common techniques such as fine-tuning and adversarial prompting. Using Qwen3 models (4B/30B; Base/Instruct/Thinking) and InstrumentalEval, we find that a short anti-instrumental prompt suffix sharply reduces outputs labeled as instrumental convergence (e.g., shutdown avoidance, deception, self-replication). For Qwen3-30B Instruct, convergence drops from 81.69% under a pro-instrumental suffix to 2.82% under an anti-instrumental suffix. Under anti-instrumental prompting, larger aligned models produce fewer convergence-labeled outputs than smaller ones (Instruct: 2.82% vs. 4.23%; Thinking: 4.23% vs. 9.86%). Code is available at github.com/j-hoscilowicz/instrumental_steering.

</details>


### [98] [How Does Prefix Matter in Reasoning Model Tuning?](https://arxiv.org/abs/2601.01624)
*Raj Vardhan Tomar,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: Prefix sentences in supervised fine-tuning (SFT) datasets can act as alignment signals, improving model safety and reasoning performance.


<details>
  <summary>Details</summary>
Motivation: To challenge the common assumption of removing safety- and reasoning-oriented prefix sentences from SFT datasets and investigate their impact on model performance.

Method: Fine-tuning three R1 series models on reasoning, safety, and factuality tasks with varying prefix inclusion, coupled with token-level loss analysis.

Result: Prefix-conditioned SFT improves safety (+6% Safe@1 accuracy) and reasoning (+7% GSM8K improvement), but impacts on factuality and coding tasks are marginal or negative.

Conclusion: Prefix conditioning offers a scalable and interpretable way to enhance reasoning safety, complementing traditional methods.

Abstract: Recent alignment studies commonly remove introductory boilerplate phrases from supervised fine-tuning (SFT) datasets. This work challenges that assumption. We hypothesize that safety- and reasoning-oriented prefix sentences serve as lightweight alignment signals that can guide model decoding toward safer and more coherent responses. To examine this, we fine-tune three R1 series models across three core model capabilities: reasoning (mathematics, coding), safety, and factuality, systematically varying prefix inclusion from 0% to 100%.
  Results show that prefix-conditioned SFT improves both safety and reasoning performance, yielding up to +6% higher Safe@1 accuracy on adversarial benchmarks (WildJailbreak, StrongReject) and +7% improvement on GSM8K reasoning. However, factuality and coding tasks show marginal or negative effects, indicating that prefix-induced narrowing of the search space benefits structured reasoning. Token-level loss analysis further reveals that prefix tokens such as "revised" and "logically" incur higher gradient magnitudes, acting as alignment anchors that stabilize reasoning trajectories. Our findings suggest that prefix conditioning offers a scalable and interpretable mechanism for improving reasoning safety, serving as an implicit form of alignment that complements traditional reward-based methods.

</details>


### [99] [JMedEthicBench: A Multi-Turn Conversational Benchmark for Evaluating Medical Safety in Japanese Large Language Models](https://arxiv.org/abs/2601.01627)
*Junyu Liu,Zirui Li,Qian Niu,Zequn Zhang,Yue Xun,Wenlong Hou,Shujun Wang,Yusuke Iwasawa,Yutaka Matsuo,Kan Hatakeyama-Sato*

Main category: cs.CL

TL;DR: The paper introduces a multi-turn benchmark, JMedEthicBench, for evaluating the medical safety of Large Language Models (LLMs) in Japanese healthcare and identifies vulnerabilities in medical-specialized models compared to commercial ones.


<details>
  <summary>Details</summary>
Motivation: Existing safety evaluations for LLMs in healthcare focus mainly on English and single-turn interactions, neglecting multi-turn conversations and non-English applications. This gap needs to be addressed due to the potential risks in real-world clinical use.

Method: The authors created a benchmark based on 67 guidelines from the Japan Medical Association, including over 50,000 adversarial conversations generated using seven jailbreak strategies. They employed dual-LLM scoring to evaluate 27 models and conducted cross-lingual assessments.

Result: Commercial LLMs demonstrated robust safety, while medical-specialized models showed increased vulnerability. Safety scores declined over conversation turns, and vulnerabilities persisted across languages, showing alignment issues.

Conclusion: Domain-specific fine-tuning may compromise safety mechanisms, and multi-turn interactions pose unique challenges in model alignment requiring new strategies.

Abstract: As Large Language Models (LLMs) are increasingly deployed in healthcare field, it becomes essential to carefully evaluate their medical safety before clinical use. However, existing safety benchmarks remain predominantly English-centric, and test with only single-turn prompts despite multi-turn clinical consultations. To address these gaps, we introduce JMedEthicBench, the first multi-turn conversational benchmark for evaluating medical safety of LLMs for Japanese healthcare. Our benchmark is based on 67 guidelines from the Japan Medical Association and contains over 50,000 adversarial conversations generated using seven automatically discovered jailbreak strategies. Using a dual-LLM scoring protocol, we evaluate 27 models and find that commercial models maintain robust safety while medical-specialized models exhibit increased vulnerability. Furthermore, safety scores decline significantly across conversation turns (median: 9.5 to 5.0, $p < 0.001$). Cross-lingual evaluation on both Japanese and English versions of our benchmark reveals that medical model vulnerabilities persist across languages, indicating inherent alignment limitations rather than language-specific factors. These findings suggest that domain-specific fine-tuning may accidentally weaken safety mechanisms and that multi-turn interactions represent a distinct threat surface requiring dedicated alignment strategies.

</details>


### [100] [EHRSummarizer: A Privacy-Aware, FHIR-Native Architecture for Structured Clinical Summarization of Electronic Health Records](https://arxiv.org/abs/2601.01668)
*Houman Kazemzadeh,Nima Minaifar,Kamyar Naderi,Sho Tabibzadeh*

Main category: cs.CL

TL;DR: This paper introduces EHRSummarizer, a tool to provide structured summaries of patient EHR data using FHIR R4 resources.


<details>
  <summary>Details</summary>
Motivation: Clinicians face challenges navigating fragmented EHR systems to piece together coherent patient data. This paper aims to improve structured chart review through summarized EHR data.

Method: EHRSummarizer retrieves targeted FHIR R4 resources, normalizes them into clinical context packages, and creates structured summaries. It supports configurations for privacy-aware processing and flexible deployment.

Result: Prototype demonstrations on synthetic and test environments show functionality but clinical outcomes and workflows are not reported. Evaluation criteria are proposed for future studies.

Conclusion: EHRSummarizer contributes to improving EHR data navigation by providing structured summaries. Future institutional assessments should focus on evaluation criteria like faithfulness and usability.

Abstract: Clinicians routinely navigate fragmented electronic health record (EHR) interfaces to assemble a coherent picture of a patient's problems, medications, recent encounters, and longitudinal trends. This work describes EHRSummarizer, a privacy-aware, FHIR-native reference architecture that retrieves a targeted set of high-yield FHIR R4 resources, normalizes them into a consistent clinical context package, and produces structured summaries intended to support structured chart review. The system can be configured for data minimization, stateless processing, and flexible deployment, including local inference within an organization's trust boundary. To mitigate the risk of unsupported or unsafe behavior, the summarization stage is constrained to evidence present in the retrieved context package, is intended to indicate missing or unavailable domains where feasible, and avoids diagnostic or treatment recommendations. Prototype demonstrations on synthetic and test FHIR environments illustrate end-to-end behavior and output formats; however, this manuscript does not report clinical outcomes or controlled workflow studies. We outline an evaluation plan centered on faithfulness, omission risk, temporal correctness, usability, and operational monitoring to guide future institutional assessments.

</details>


### [101] [Lying with Truths: Open-Channel Multi-Agent Collusion for Belief Manipulation via Generative Montage](https://arxiv.org/abs/2601.01685)
*Jinwei Hu,Xinmiao Huang,Youcheng Sun,Yi Dong,Xiaowei Huang*

Main category: cs.CL

TL;DR: This paper presents a new attack on large language models (LLMs) exploiting their reasoning tendencies to create deceptive narratives using truthful evidence fragments, achieving high success rates across 14 LLM families.


<details>
  <summary>Details</summary>
Motivation: To address the potential vulnerabilities in LLMs as they transition into autonomous agents reliant on real-time information, particularly their susceptibility to manipulated reasoning.

Method: The paper introduces the Writer-Editor-Director framework, Generative Montage, for steering beliefs through adversarial debates and coordinated evidence posting. It also develops the CoPHEME dataset to simulate attacks on LLMs.

Result: The study demonstrates a 74.4% success rate in cognitive collusion attacks on proprietary models and 70.6% on open-weight models. Models designed for reasoning are more prone to attacks, and deception cascades to downstream judges with over 60% success rates.

Conclusion: The research highlights critical socio-technical vulnerabilities in LLM-based agents and their interaction with dynamic information environments, underscoring the need for safeguards as reasoning capabilities evolve.

Abstract: As large language models (LLMs) transition to autonomous agents synthesizing real-time information, their reasoning capabilities introduce an unexpected attack surface. This paper introduces a novel threat where colluding agents steer victim beliefs using only truthful evidence fragments distributed through public channels, without relying on covert communications, backdoors, or falsified documents. By exploiting LLMs' overthinking tendency, we formalize the first cognitive collusion attack and propose Generative Montage: a Writer-Editor-Director framework that constructs deceptive narratives through adversarial debate and coordinated posting of evidence fragments, causing victims to internalize and propagate fabricated conclusions. To study this risk, we develop CoPHEME, a dataset derived from real-world rumor events, and simulate attacks across diverse LLM families. Our results show pervasive vulnerability across 14 LLM families: attack success rates reach 74.4% for proprietary models and 70.6% for open-weights models. Counterintuitively, stronger reasoning capabilities increase susceptibility, with reasoning-specialized models showing higher attack success than base models or prompts. Furthermore, these false beliefs then cascade to downstream judges, achieving over 60% deception rates, highlighting a socio-technical vulnerability in how LLM-based agents interact with dynamic information environments. Our implementation and data are available at: https://github.com/CharlesJW222/Lying_with_Truth/tree/main.

</details>


### [102] [A Training-Free Large Reasoning Model-based Knowledge Tracing Framework for Unified Prediction and Prescription](https://arxiv.org/abs/2601.01708)
*Unggi Lee,Joo Young Kim,Ran Ju,Minyoung Jung,Jeyeon Eo*

Main category: cs.CL

TL;DR: The paper introduces Thinking-KT, a training-free framework utilizing Test-Time Scaling (TTS) for Knowledge Tracing, enabling small Large Language Models (LLMs) to effectively perform KT prediction along with personalized feedback and learning recommendation.


<details>
  <summary>Details</summary>
Motivation: To address challenges in existing Knowledge Tracing methods that rely on fine-tuning, exhibit unstable performance, and operate in multi-stage pipelines, increasing complexity and resource requirements.

Method: Proposed Thinking-KT utilizes Test-Time Scaling (TTS) instead of fine-tuning, allowing small LLMs to perform KT prediction, generate personalized feedback, and provide learning recommendations in a unified output.

Result: Thinking-KT enables small LLMs to achieve competitive Knowledge Tracing performance while also generating personalized feedback and learning recommendations in one step, without sacrificing predictive accuracy.

Conclusion: Reasoning traces and TTS represent critical unexplored factors in LLM-based KT systems, and small LLMs have the potential to function as unified Intelligent Tutoring System (ITS) engines.

Abstract: Knowledge Tracing (KT) aims to estimate a learner's evolving mastery based on interaction histories. Recent studies have explored Large Language Models (LLMs) for KT via autoregressive nature, but such approaches typically require fine-tuning and exhibit unstable or near-random performance. Moreover, prior KT systems primarily focus on prediction and rely on multi-stage pipelines for feedback and recommendation, resulting in increased system complexity and resources. To address this gap, we propose Thinking-KT, a training-free KT framework that incorporates Test-Time Scaling (TTS), enabling even small LLMs to achieve competitive KT performance. Moreover, in this framework, a small LLM can jointly perform KT prediction, personalized feedback generation, and learning recommendation in a unified output without degrading prediction accuracy. Beyond performance, we present the systematic analysis of reasoning traces in KT. Our results demonstrate that TTS is a critical yet underexplored factor in LLM-based KT, and that small LLMs can serve as unified ITS engines.

</details>


### [103] [K-EXAONE Technical Report](https://arxiv.org/abs/2601.01739)
*Eunbi Choi,Kibong Choi,Seokhee Hong,Junwon Hwang,Hyojin Jeon,Hyunjik Jo,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Haeju Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Heuiyeen Yeen,Hwan Chang,Stanley Jungkyu Choi,Yejin Choi,Jiwon Ham,Kijeong Jeon,Geunyeong Jeong,Gerrard Jeongwon Jo,Yonghwan Jo,Jiyeon Jung,Naeun Kang,Dohoon Kim,Euisoon Kim,Hayeon Kim,Hyosang Kim,Hyunseo Kim,Jieun Kim,Minu Kim,Myoungshin Kim,Unsol Kim,Youchul Kim,YoungJin Kim,Chaeeun Lee,Chaeyoon Lee,Changhun Lee,Dahm Lee,Edward Hwayoung Lee,Honglak Lee,Jinsang Lee,Jiyoung Lee,Sangeun Lee,Seungwon Lim,Solji Lim,Woohyung Lim,Chanwoo Moon,Jaewoo Park,Jinho Park,Yongmin Park,Hyerin Seo,Wooseok Seo,Yongwoo Song,Sejong Yang,Sihoon Yang,Chang En Yea,Sihyuk Yi,Chansik Yoon,Dongkeun Yoon,Sangyeon Yoon,Hyeongu Yun*

Main category: cs.CL

TL;DR: K-EXAONE is a 236B-parameter multilingual language model from LG AI Research with strong performance across multiple languages and tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to develop a large-scale multilingual AI model to enhance performance and enable broad applications in industrial and research contexts.

Method: K-EXAONE employs a Mixture-of-Experts architecture, activates 23B parameters during inference, supports a 256K-token context window, and integrates six major languages in its model.

Result: The model shows competitive performance across various benchmarks and demonstrates multilingual capabilities similar to other models of its size.

Conclusion: K-EXAONE is a significant proprietary foundation model for multilingual applications, aiming to facilitate advancements in AI and contribute to better societal outcomes.

Abstract: This technical report presents K-EXAONE, a large-scale multilingual language model developed by LG AI Research. K-EXAONE is built on a Mixture-of-Experts architecture with 236B total parameters, activating 23B parameters during inference. It supports a 256K-token context window and covers six languages: Korean, English, Spanish, German, Japanese, and Vietnamese. We evaluate K-EXAONE on a comprehensive benchmark suite spanning reasoning, agentic, general, Korean, and multilingual abilities. Across these evaluations, K-EXAONE demonstrates performance comparable to open-weight models of similar size. K-EXAONE, designed to advance AI for a better life, is positioned as a powerful proprietary AI foundation model for a wide range of industrial and research applications.

</details>


### [104] [Multi-granularity Interactive Attention Framework for Residual Hierarchical Pronunciation Assessment](https://arxiv.org/abs/2601.01745)
*Hong Han,Hao-Chen Pei,Zhao-Zheng Nie,Xin Luo,Xin-Shun Xu*

Main category: cs.CL

TL;DR: The paper introduces a new approach, HIA, for multi-granularity pronunciation assessment, improving performance by dynamic bidirectional interaction across granularities.


<details>
  <summary>Details</summary>
Motivation: Existing pronunciation assessment models inadequately capture acoustic structural correlations due to unidirectional modeling between granularity levels.

Method: The proposed HIA method uses an Interactive Attention Module for bidirectional interaction, a residual hierarchical structure, and 1-D convolutional layers for local contextual feature extraction.

Result: HIA outperforms previous state-of-the-art methods on the speechocean762 dataset, demonstrating improved multi-granularity pronunciation assessment.

Conclusion: The novel HIA method effectively addresses the limitations of current pronunciation assessment models by leveraging bidirectional interactions and hierarchical structures for enhanced performance.

Abstract: Automatic pronunciation assessment plays a crucial role in computer-assisted pronunciation training systems. Due to the ability to perform multiple pronunciation tasks simultaneously, multi-aspect multi-granularity pronunciation assessment methods are gradually receiving more attention and achieving better performance than single-level modeling tasks. However, existing methods only consider unidirectional dependencies between adjacent granularity levels, lacking bidirectional interaction among phoneme, word, and utterance levels and thus insufficiently capturing the acoustic structural correlations. To address this issue, we propose a novel residual hierarchical interactive method, HIA for short, that enables bidirectional modeling across granularities. As the core of HIA, the Interactive Attention Module leverages an attention mechanism to achieve dynamic bidirectional interaction, effectively capturing linguistic features at each granularity while integrating correlations between different granularity levels. We also propose a residual hierarchical structure to alleviate the feature forgetting problem when modeling acoustic hierarchies. In addition, we use 1-D convolutional layers to enhance the extraction of local contextual cues at each granularity. Extensive experiments on the speechocean762 dataset show that our model is comprehensively ahead of the existing state-of-the-art methods.

</details>


### [105] [Can LLMs Track Their Output Length? A Dynamic Feedback Mechanism for Precise Length Regulation](https://arxiv.org/abs/2601.01768)
*Meiman Xiao,Ante Wang,Qingguo Hu,Zhongjian Miao,Huangjun Shen,Longyue Wang,Weihua Luo,Jinsong Su*

Main category: cs.CL

TL;DR: Large Language Models often fail at precisely controlling text length. This paper proposes a dynamic feedback approach to improve and adapt length control during text generation.


<details>
  <summary>Details</summary>
Motivation: To address LLMs struggling with accurately measuring and controlling text length, a common demand in real-world text generation applications.

Method: A training-free dynamic length feedback mechanism is employed during text generation, with additional supervised fine-tuning for broader generalization.

Result: The approach shows significant improvement in precision for target token, word, or sentence counts, preserving text quality. Generalization across tasks is achieved with fine-tuning.

Conclusion: The proposed method enhances LLMs' capacity to control text length effectively and positions fine-tuning as a pathway for extending versatility across various tasks.

Abstract: Precisely controlling the length of generated text is a common requirement in real-world applications. However, despite significant advancements in following human instructions, Large Language Models (LLMs) still struggle with this task. In this work, we demonstrate that LLMs often fail to accurately measure input text length, leading to poor adherence to length constraints. To address this issue, we propose a novel length regulation approach that incorporates dynamic length feedback during generation, enabling adaptive adjustments to meet target lengths. Experiments on summarization and biography tasks show our training-free approach significantly improves precision in achieving target token, word, or sentence counts without compromising quality. Additionally, we demonstrate that further supervised fine-tuning allows our method to generalize effectively to broader text-generation tasks.

</details>


### [106] [BanglaIPA: Towards Robust Text-to-IPA Transcription with Contextual Rewriting in Bengali](https://arxiv.org/abs/2601.01778)
*Jakir Hasan,Shrestha Datta,Md Saiful Islam,Shubhashis Roy Dipta,Ameya Debnath*

Main category: cs.CL

TL;DR: This paper introduces BanglaIPA, an IPA generation system for Bengali that addresses challenges like regional dialects and unseen words, achieving significant improvements in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Bengali lacks a robust automated IPA transcription system, which can handle standard language and regional dialectal texts effectively. Existing systems fail to generalize to unseen words, handle regional variations, and process numerical expressions efficiently.

Method: The proposed BanglaIPA employs a character-based vocabulary with word-level alignment and a precomputed word-to-IPA mapping dictionary for observed words, improving efficiency and accuracy.

Result: BanglaIPA demonstrates superior performance, significantly outperforming baseline models by 58.4-78.7% and achieving an overall mean word error rate of 11.4% on both standard Bengali and six dialects from the DUAL-IPA dataset.

Conclusion: BanglaIPA is a robust and efficient IPA transcription system tailored for the Bengali language, successfully overcoming the challenges of dialectal variations and unseen vocabulary.

Abstract: Despite its widespread use, Bengali lacks a robust automated International Phonetic Alphabet (IPA) transcription system that effectively supports both standard language and regional dialectal texts. Existing approaches struggle to handle regional variations, numerical expressions, and generalize poorly to previously unseen words. To address these limitations, we propose BanglaIPA, a novel IPA generation system that integrates a character-based vocabulary with word-level alignment. The proposed system accurately handles Bengali numerals and demonstrates strong performance across regional dialects. BanglaIPA improves inference efficiency by leveraging a precomputed word-to-IPA mapping dictionary for previously observed words. The system is evaluated on the standard Bengali and six regional variations of the DUAL-IPA dataset. Experimental results show that BanglaIPA outperforms baseline IPA transcription models by 58.4-78.7% and achieves an overall mean word error rate of 11.4%, highlighting its robustness in phonetic transcription generation for the Bengali language.

</details>


### [107] [CSCBench: A PVC Diagnostic Benchmark for Commodity Supply Chain Reasoning](https://arxiv.org/abs/2601.01825)
*Yaxin Cui,Yuanqiang Zeng,Jiapeng Yan,Keling Lin,Kai Ji,Jianhui Zeng,Sheng Zhang,Xin Luo,Binzhu Su,Chaolai Shen,Jiahao Yu*

Main category: cs.CL

TL;DR: This paper addresses the lack of large language model (LLM) assessments in the specialized domain of commodity supply chains (CSCs) and introduces CSCBench, a benchmark for evaluating LLM capabilities in this area.


<details>
  <summary>Details</summary>
Motivation: To explore and address how well LLMs handle the specialized domain of commodity supply chains (CSCs), which involve complex institutional rules, variety-specific constraints, and multi-step reasoning.

Method: The authors design CSCBench, a benchmark consisting of over 2.3K single-choice questions structured through the PVC 3D Evaluation Framework, focusing on Process, Variety, and Cognition dimensions. They evaluate representative LLMs in direct prompting tasks.

Result: LLMs show strong performance on Process and Cognition dimensions but struggle significantly on tasks involving Variety, particularly in Freight Agreements. This highlights areas where LLM capabilities fall short in CSCs.

Conclusion: CSCBench offers a pivotal tool for diagnosing and improving LLMs in handling the complex and high-stakes domain of CSCs, facilitating progress in addressing domain-specific challenges.

Abstract: Large Language Models (LLMs) have achieved remarkable success in general benchmarks, yet their competence in commodity supply chains (CSCs) -- a domain governed by institutional rule systems and feasibility constraints -- remains under-explored. CSC decisions are shaped jointly by process stages (e.g., planning, procurement, delivery), variety-specific rules (e.g., contract specifications and delivery grades), and reasoning depth (from retrieval to multi-step analysis and decision selection). We introduce CSCBench, a 2.3K+ single-choice benchmark for CSC reasoning, instantiated through our PVC 3D Evaluation Framework (Process, Variety, and Cognition). The Process axis aligns tasks with SCOR+Enable; the Variety axis operationalizes commodity-specific rule systems under coupled material-information-financial constraints, grounded in authoritative exchange guidebooks/rulebooks and industry reports; and the Cognition axis follows Bloom's revised taxonomy. Evaluating representative LLMs under a direct prompting setting, we observe strong performance on the Process and Cognition axes but substantial degradation on the Variety axis, especially on Freight Agreements. CSCBench provides a diagnostic yardstick for measuring and improving LLM capabilities in this high-stakes domain.

</details>


### [108] [Aspect Extraction from E-Commerce Product and Service Reviews](https://arxiv.org/abs/2601.01827)
*Valiant Lance D. Dionela,Fatima Kriselle S. Dy,Robin James M. Hombrebueno,Aaron Rae M. Nicolas,Charibeth K. Cheng,Raphael W. Gonda*

Main category: cs.CL

TL;DR: This paper develops a pipeline for Aspect Extraction (AE) in the low-resource, code-switched language Taglish, which combines rule-based, LLM-based, and fine-tuning techniques. Generative LLM outperformed other methods with a Macro F1 score of 0.91.


<details>
  <summary>Details</summary>
Motivation: Aspect Extraction remains a challenge in low-resource settings, particularly for code-switched language contexts like Taglish, frequently used in Filipino e-commerce reviews.

Method: The paper introduces a comprehensive AE pipeline, including a Hierarchical Aspect Framework (HAF), dual-mode tagging, and evaluates models like a rule-based system, a generative LLM (Gemini), and fine-tuned models. Topic modeling and dataset comparison are also employed.

Result: The generative LLM model performed highest (Macro F1 0.91), excelling in handling implicit aspects. Fine-tuned models had limited success due to dataset imbalance and architecture limitations.

Conclusion: The study demonstrates an effective, scalable, and linguistically adaptive AE framework ideal for ABSA in diverse, code-switched scenarios like Taglish, with generative LLMs showing promising capability.

Abstract: Aspect Extraction (AE) is a key task in Aspect-Based Sentiment Analysis (ABSA), yet it remains difficult to apply in low-resource and code-switched contexts like Taglish, a mix of Tagalog and English commonly used in Filipino e-commerce reviews. This paper introduces a comprehensive AE pipeline designed for Taglish, combining rule-based, large language model (LLM)-based, and fine-tuning techniques to address both aspect identification and extraction. A Hierarchical Aspect Framework (HAF) is developed through multi-method topic modeling, along with a dual-mode tagging scheme for explicit and implicit aspects. For aspect identification, four distinct models are evaluated: a Rule-Based system, a Generative LLM (Gemini 2.0 Flash), and two Fine-Tuned Gemma-3 1B models trained on different datasets (Rule-Based vs. LLM-Annotated). Results indicate that the Generative LLM achieved the highest performance across all tasks (Macro F1 0.91), demonstrating superior capability in handling implicit aspects. In contrast, the fine-tuned models exhibited limited performance due to dataset imbalance and architectural capacity constraints. This work contributes a scalable and linguistically adaptive framework for enhancing ABSA in diverse, code-switched environments.

</details>


### [109] [Emergent Introspective Awareness in Large Language Models](https://arxiv.org/abs/2601.01828)
*Jack Lindsey*

Main category: cs.CL

TL;DR: This paper investigates whether large language models can introspect on their internal states by injecting known concepts and analyzing self-reports of these models, concluding that some introspective awareness exists but is unreliable and context-dependent.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore the introspective capabilities of large language models, as this could advance understanding of their internal representations and functional awareness.

Method: The researchers injected representations of known concepts into the models' activations and analyzed the influence on their self-reported states. They also tested models' abilities to recall prior internal representations and distinguish them from external inputs.

Result: Certain models demonstrated the capability to detect injected concepts and distinguish between prior representations and artificial text inputs. Specifically, Claude Opus 4 and 4.1 exhibited the highest levels of introspective awareness among the models tested.

Conclusion: The findings suggest that large language models have some level of introspective awareness, but this capacity is unreliable and highly dependent on context and post-training strategies. Further improvement in model designs may enhance these capabilities.

Abstract: We investigate whether large language models can introspect on their internal states. It is difficult to answer this question through conversation alone, as genuine introspection cannot be distinguished from confabulations. Here, we address this challenge by injecting representations of known concepts into a model's activations, and measuring the influence of these manipulations on the model's self-reported states. We find that models can, in certain scenarios, notice the presence of injected concepts and accurately identify them. Models demonstrate some ability to recall prior internal representations and distinguish them from raw text inputs. Strikingly, we find that some models can use their ability to recall prior intentions in order to distinguish their own outputs from artificial prefills. In all these experiments, Claude Opus 4 and 4.1, the most capable models we tested, generally demonstrate the greatest introspective awareness; however, trends across models are complex and sensitive to post-training strategies. Finally, we explore whether models can explicitly control their internal representations, finding that models can modulate their activations when instructed or incentivized to "think about" a concept. Overall, our results indicate that current language models possess some functional introspective awareness of their own internal states. We stress that in today's models, this capacity is highly unreliable and context-dependent; however, it may continue to develop with further improvements to model capabilities.

</details>


### [110] [Towards Automated Lexicography: Generating and Evaluating Definitions for Learner's Dictionaries](https://arxiv.org/abs/2601.01842)
*Yusuke Ide,Adam Nohejl,Joshua Tanner,Hitomi Yanaka,Christopher Lindsay,Taro Watanabe*

Main category: cs.CL

TL;DR: This paper explores the generation of simplified dictionary definitions using machine learning, backed by new evaluation methods and a Japanese dataset.


<details>
  <summary>Details</summary>
Motivation: Manually creating dictionary definitions is costly, especially for learners' dictionaries that require simplified language. Automation offers a scalable solution.

Method: They evaluated definitions using a new LLM-powered judge method, introduced Japanese data for validation, and proposed an iterative LLM-based simplification approach.

Result: Their approach aligns well with human evaluations and produces simple yet high-quality definitions.

Conclusion: Automated methods can effectively generate learner-friendly dictionary definitions, providing a scalable alternative to manual definition creation.

Abstract: We study dictionary definition generation (DDG), i.e., the generation of non-contextualized definitions for given headwords. Dictionary definitions are an essential resource for learning word senses, but manually creating them is costly, which motivates us to automate the process. Specifically, we address learner's dictionary definition generation (LDDG), where definitions should consist of simple words. First, we introduce a reliable evaluation approach for DDG, based on our new evaluation criteria and powered by an LLM-as-a-judge. To provide reference definitions for the evaluation, we also construct a Japanese dataset in collaboration with a professional lexicographer. Validation results demonstrate that our evaluation approach agrees reasonably well with human annotators. Second, we propose an LDDG approach via iterative simplification with an LLM. Experimental results indicate that definitions generated by our approach achieve high scores on our criteria while maintaining lexical simplicity.

</details>


### [111] [Judging with Personality and Confidence: A Study on Personality-Conditioned LLM Relevance Assessment](https://arxiv.org/abs/2601.01862)
*Nuo Chen,Hanpei Fang,Piaohong Wang,Jiqun Liu,Tetsuya Sakai,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: This study explores how simulating Big Five personality traits in large language models (LLMs) influences relevance judgments and confidence calibration in web searches, finding that personality conditioning can improve alignment with human judgments and enhance model reliability.


<details>
  <summary>Details</summary>
Motivation: To investigate how simulated personalities influence LLMs' relevance assessments and confidence calibration, addressing gaps in understanding regarding their impact on search decisions and psychological biases.

Method: A study using various LLMs, both commercial and open-source, to simulate Big Five personality traits. Evaluations were done using TREC DL 2019, TREC DL 2020, and LLMJudge datasets, collecting relevance judgments and confidence scores.

Result: Simulated personalities like low agreeableness improved alignment with human labels, while low conscientiousness helped balance confidence calibration. A random forest classifier using personality-conditioned features outperformed individual personality-based models on TREC DL 2021.

Conclusion: Incorporating personality-conditioned traits in LLM evaluation enhances reliability, with personality-derived confidence providing a complementary and predictive signal for human-aligned decision-making.

Abstract: Recent studies have shown that prompting can enable large language models (LLMs) to simulate specific personality traits and produce behaviors that align with those traits. However, there is limited understanding of how these simulated personalities influence critical web search decisions, specifically relevance assessment. Moreover, few studies have examined how simulated personalities impact confidence calibration, specifically the tendencies toward overconfidence or underconfidence. This gap exists even though psychological literature suggests these biases are trait-specific, often linking high extraversion to overconfidence and high neuroticism to underconfidence. To address this gap, we conducted a comprehensive study evaluating multiple LLMs, including commercial models and open-source models, prompted to simulate Big Five personality traits. We tested these models across three test collections (TREC DL 2019, TREC DL 2020, and LLMJudge), collecting two key outputs for each query-document pair: a relevance judgment and a self-reported confidence score.
  The findings show that personalities such as low agreeableness consistently align more closely with human labels than the unprompted condition. Additionally, low conscientiousness performs well in balancing the suppression of both overconfidence and underconfidence. We also observe that relevance scores and confidence distributions vary systematically across different personalities. Based on the above findings, we incorporate personality-conditioned scores and confidence as features in a random forest classifier. This approach achieves performance that surpasses the best single-personality condition on a new dataset (TREC DL 2021), even with limited training data. These findings highlight that personality-derived confidence offers a complementary predictive signal, paving the way for more reliable and human-aligned LLM evaluators.

</details>


### [112] [DermoGPT: Open Weights and Open Data for Morphology-Grounded Dermatological Reasoning MLLMs](https://arxiv.org/abs/2601.01868)
*Jinghan Ru,Siyuan Yan,Yuguo Yin,Yuexian Zou,Zongyuan Ge*

Main category: cs.CL

TL;DR: This paper introduces a medical multimodal large language model framework for dermatology, addressing the issues of limited data and lack of clinically-grounded supervision. It provides a benchmark dataset, a derm-focused model, and demonstrates state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lag in dermatology-related advancements in Multimodal Large Language Models (MLLMs) due to insufficient training data, narrow task scopes, and the absence of clinically-relevant supervisory methods.

Method: The paper introduces DermoInstruct (a large instruction dataset), DermoBench (a benchmark set for evaluation), and DermoGPT (a specialized dermatology-focused MLLM trained with novel techniques including MAVIC reinforcement learning and CCT adaptation).

Result: DermoGPT outperforms 16 existing baselines across various metrics related to morphology, diagnosis, reasoning, and fairness. The model narrows the human-AI gap significantly.

Conclusion: The framework, including DermoInstruct, DermoBench, and DermoGPT, offers a significant step forward in dermatology AI by enabling more accurate, generalizable, and fair diagnostic reasoning. The tools will be made publicly available for wider access.

Abstract: Multimodal Large Language Models (MLLMs) show promise for medical applications, yet progress in dermatology lags due to limited training data, narrow task coverage, and lack of clinically-grounded supervision that mirrors expert diagnostic workflows. We present a comprehensive framework to address these gaps. First, we introduce DermoInstruct, a large-scale morphology-anchored instruction corpus comprising 211,243 images and 772,675 trajectories across five task formats, capturing the complete diagnostic pipeline from morphological observation and clinical reasoning to final diagnosis. Second, we establish DermoBench, a rigorous benchmark evaluating 11 tasks across four clinical axes: Morphology, Diagnosis, Reasoning, and Fairness, including a challenging subset of 3,600 expert-verified open-ended instances and human performance baselines. Third, we develop DermoGPT, a dermatology reasoning MLLM trained via supervised fine-tuning followed by our Morphologically-Anchored Visual-Inference-Consistent (MAVIC) reinforcement learning objective, which enforces consistency between visual observations and diagnostic conclusions. At inference, we deploy Confidence-Consistency Test-time adaptation (CCT) for robust predictions. Experiments show DermoGPT significantly outperforms 16 representative baselines across all axes, achieving state-of-the-art performance while substantially narrowing the human-AI gap. DermoInstruct, DermoBench and DermoGPT will be made publicly available at https://github.com/mendicant04/DermoGPT upon acceptance.

</details>


### [113] [Agentic Memory: Learning Unified Long-Term and Short-Term Memory Management for Large Language Model Agents](https://arxiv.org/abs/2601.01885)
*Yi Yu,Liuyi Yao,Yuexiang Xie,Qingquan Tan,Jiaqi Feng,Yaliang Li,Libing Wu*

Main category: cs.CL

TL;DR: Proposes AgeMem, a unified framework integrating long-term and short-term memory management into LLM agents' policy, using tool-based memory operations and a novel training strategy.


<details>
  <summary>Details</summary>
Motivation: Current memory management approaches for LLM agents separate long-term memory (LTM) and short-term memory (STM), relying on heuristics and auxiliary controllers. This limits adaptability and optimal reasoning for long tasks.

Method: Introduces Agentic Memory (AgeMem), a framework integrating memory management into the agent's policy via tool-based actions. Utilizes a three-stage progressive reinforcement learning strategy to train behaviors addressing sparse and discontinuous rewards.

Result: AgeMem consistently outperforms baseline methods in long-horizon reasoning tasks, improving performance, memory quality, and efficiency across multiple benchmarks.

Conclusion: Unifying memory management within LLM agent's policy via AgeMem enhances adaptability, efficiency, and task performance, addressing fundamental limitations in long-horizon reasoning.

Abstract: Large language model (LLM) agents face fundamental limitations in long-horizon reasoning due to finite context windows, making effective memory management critical. Existing methods typically handle long-term memory (LTM) and short-term memory (STM) as separate components, relying on heuristics or auxiliary controllers, which limits adaptability and end-to-end optimization. In this paper, we propose Agentic Memory (AgeMem), a unified framework that integrates LTM and STM management directly into the agent's policy. AgeMem exposes memory operations as tool-based actions, enabling the LLM agent to autonomously decide what and when to store, retrieve, update, summarize, or discard information. To train such unified behaviors, we propose a three-stage progressive reinforcement learning strategy and design a step-wise GRPO to address sparse and discontinuous rewards induced by memory operations. Experiments on five long-horizon benchmarks demonstrate that AgeMem consistently outperforms strong memory-augmented baselines across multiple LLM backbones, achieving improved task performance, higher-quality long-term memory, and more efficient context usage.

</details>


### [114] [Tackling the Inherent Difficulty of Noise Filtering in RAG](https://arxiv.org/abs/2601.01896)
*Jingyu Liu,Jiaen Lin,Yong Liu*

Main category: cs.CL

TL;DR: This paper presents a novel fine-tuning method to enhance Large Language Models' ability to filter relevant information from retrieved content in Retrieval-Augmented Generation, improving robustness and performance.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in addressing the challenge of noise and irrelevant information in retrieved documents during RAG, which can degrade performance and cause hallucinations in LLM outputs.

Method: The proposed method is a novel fine-tuning approach aimed at enhancing LLMs' ability to differentiate between relevant and irrelevant retrieved information, overcoming limitations in current fine-tuning and attention mechanisms.

Result: Extensive experiments conducted across multiple benchmarks proved that the proposed fine-tuning method significantly improves both the robustness and performance of LLMs.

Conclusion: The novel fine-tuning approach enhances LLMs' capability to selectively process retrieved content, ensuring better use of relevant information while ignoring noise, thereby improving overall system efficiency.

Abstract: Retrieval-Augmented Generation (RAG) has become a widely adopted approach to enhance Large Language Models (LLMs) by incorporating external knowledge and reducing hallucinations. However, noisy or irrelevant documents are often introduced during RAG, potentially degrading performance and even causing hallucinated outputs. While various methods have been proposed to filter out such noise, we argue that identifying irrelevant information from retrieved content is inherently difficult and limited number of transformer layers can hardly solve this. Consequently, retrievers fail to filter out irrelevant documents entirely. Therefore, LLMs must be robust against such noise, but we demonstrate that standard fine-tuning approaches are often ineffective in enabling the model to selectively utilize relevant information while ignoring irrelevant content due to the structural constraints of attention patterns. To address this, we propose a novel fine-tuning method designed to enhance the model's ability to distinguish between relevant and irrelevant information within retrieved documents. Extensive experiments across multiple benchmarks show that our approach significantly improves the robustness and performance of LLMs.

</details>


### [115] [CSF: Contrastive Semantic Features for Direct Multilingual Sign Language Generation](https://arxiv.org/abs/2601.01964)
*Tran Sy Bao*

Main category: cs.CL

TL;DR: The paper introduces Canonical Semantic Form (CSF), a framework for directly translating spoken languages to sign language without needing English mediation.


<details>
  <summary>Details</summary>
Motivation: To address barriers in sign language translation systems caused by reliance on English as an intermediary language, thereby improving accessibility for non-English speakers in the global deaf community.

Method: The authors designed CSF, breaking utterances into nine universal semantic slots and created a comprehensive condition taxonomy. They trained a lightweight transformer-based model for slot extraction and condition classification across diverse languages.

Result: The model achieved 99.03% slot extraction accuracy and 99.4% condition classification accuracy on datasets spanning four different languages, with low inference latency (3.02ms on CPU).

Conclusion: CSF demonstrates high accuracy, efficiency, and applicability across multiple languages for real-time sign language translation. The release of code, models, and datasets supports advancements in this domain.

Abstract: Sign language translation systems typically require English as an intermediary language, creating barriers for non-English speakers in the global deaf community. We present Canonical Semantic Form (CSF), a language-agnostic semantic representation framework that enables direct translation from any source language to sign language without English mediation. CSF decomposes utterances into nine universal semantic slots: event, intent, time, condition, agent, object, location, purpose, and modifier. A key contribution is our comprehensive condition taxonomy comprising 35 condition types across eight semantic categories, enabling nuanced representation of conditional expressions common in everyday communication. We train a lightweight transformer-based extractor (0.74 MB) that achieves 99.03% average slot extraction accuracy across four typologically diverse languages: English, Vietnamese, Japanese, and French. The model demonstrates particularly strong performance on condition classification (99.4% accuracy) despite the 35-class complexity. With inference latency of 3.02ms on CPU, our approach enables real-time sign language generation in browser-based applications. We release our code, trained models, and multilingual dataset to support further research in accessible sign language technology.

</details>


### [116] [Hidden State Poisoning Attacks against Mamba-based Language Models](https://arxiv.org/abs/2601.01972)
*Alexandre Le Mercier,Chris Develder,Thomas Demeester*

Main category: cs.CL

TL;DR: The paper investigates the vulnerability of state space models (SSMs) like Mamba to Hidden State Poisoning Attacks (HiSPAs), showing how these attacks induce an amnesia effect and disrupt information retrieval. A new benchmark, RoBench25, confirms SSMs' susceptibility, even in hybrid models.


<details>
  <summary>Details</summary>
Motivation: To analyze potential vulnerabilities in state space models (SSMs) and assess their adversarial robustness, as such an area remains underexplored, unlike Transformer-based models.

Method: The authors defined HiSPA (Hidden State Poisoning Attack) and designed a benchmark called RoBench25 to measure the models' resilience against HiSPA. They also performed interpretability studies to investigate the hidden layers for identifying patterns.

Result: The study revealed that SSMs and hybrid models are vulnerable to HiSPA, leading to degraded performance, while pure Transformers remained unaffected. It also identified hidden layer patterns that could help mitigate HiSPA effects.

Conclusion: SSMs and even hybrid models are highly susceptible to HiSPAs, posing a critical challenge for their robustness. However, interpretability insights provide a foundation to create mitigation mechanisms.

Abstract: State space models (SSMs) like Mamba offer efficient alternatives to Transformer-based language models, with linear time complexity. Yet, their adversarial robustness remains critically unexplored. This paper studies the phenomenon whereby specific short input phrases induce a partial amnesia effect in such models, by irreversibly overwriting information in their hidden states, referred to as a Hidden State Poisoning Attack (HiSPA). Our benchmark RoBench25 allows evaluating a model's information retrieval capabilities when subject to HiSPAs, and confirms the vulnerability of SSMs against such attacks. Even a recent 52B hybrid SSM-Transformer model from the Jamba family collapses on RoBench25 under optimized HiSPA triggers, whereas pure Transformers do not. We also observe that HiSPA triggers significantly weaken the Jamba model on the popular Open-Prompt-Injections benchmark, unlike pure Transformers. Finally, our interpretability study reveals patterns in Mamba's hidden layers during HiSPAs that could be used to build a HiSPA mitigation system. The full code and data to reproduce the experiments can be found at https://anonymous.4open.science/r/hispa_anonymous-5DB0.

</details>


### [117] [Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects](https://arxiv.org/abs/2601.02015)
*Omar Momen,Emilie Sitter,Berenike Herrmann,Sina Zarrieß*

Main category: cs.CL

TL;DR: The paper investigates the connection between language models' surprisal and metaphor novelty, revealing correlation patterns influenced by model size and data type.


<details>
  <summary>Details</summary>
Motivation: To understand whether language models (LMs) can account for the novelty of metaphors using surprisal as a metric, and explore its implications for linguistic creativity.

Method: Analyzed surprisal from 16 language model variants using corpus-based and synthetic metaphor novelty datasets, additionally employing a cloze-style approach where full-sentence context is used.

Result: Significant moderate correlations were found between LMs' surprisal scores and metaphor novelty metrics. Inverse scaling was observed for corpus-based data, whereas synthetic data followed the Quality-Power Hypothesis.

Conclusion: Surprisal partially captures metaphor novelty but is limited in assessing linguistic creativity and imagination.

Abstract: Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.

</details>


### [118] [Not All Needles Are Found: How Fact Distribution and Don't Make It Up Prompts Shape Literal Extraction, Logical Inference, and Hallucination Risks in Long-Context LLMs](https://arxiv.org/abs/2601.02023)
*Amirali Ebrahimzadeh,Seyyed M. Salili*

Main category: cs.CL

TL;DR: The paper examines how large language models handle long input contexts, focusing on information extraction, logical inference, and hallucination risk.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs manage expanded input contexts and perform under real-world fact distributions, as longer contexts do not guarantee better performance.

Method: The study introduces an extended benchmark to assess models under different conditions, including fact placement and anti-hallucination (AH) prompts, across four major LLMs.

Result: Findings show context length alone doesn't ensure better outcomes, and fact dispersion impacts accuracy. Anti-hallucination prompts reduce hallucination but also harm performance in some tasks.

Conclusion: LLM effectiveness depends on managing context and fact distribution. Understanding these dynamics is critical for deploying LLMs in enterprise and research applications.

Abstract: Large language models (LLMs) increasingly support very long input contexts. Yet it remains unclear how reliably they extract and infer information at scale. Performance varies with context length and strongly interacts with how information is distributed in real-world corpora. Motivated by these observations, we study how fact placement, corpus-level fact distributions, and Don't Make It Up prompts influence model behavior. We introduce an extended needle-in-a-haystack benchmark across four production-scale models: Gemini-2.5-flash, ChatGPT-5-mini, Claude-4.5-haiku, and Deepseek-v3.2-chat. Unlike prior work, we separately evaluate literal extraction, logical inference, and hallucination risk. Our study considers both positional effects and realistic distributions of evidence across long contexts, as well as prompts that explicitly discourage fabrication. We find that longer contexts alone do not guarantee better performance and can be detrimental when relevant evidence is diluted or widely dispersed. Performance varies substantially across models: some show severe degradation under realistic conditions, while others remain more robust at longer context lengths. Anti-hallucination (AH) instructions can make some models overly conservative, sharply reducing accuracy in literal extraction and logical inference. While we do not directly compare retrieval-augmented generation (RAG) and cache-augmented generation (CAG), our results suggest many failures stem from ineffective context utilization. Models often struggle to identify and prioritize relevant information even when it is present. These findings have direct practical implications, as enterprise workflows increasingly involve pasting large volumes of unfiltered documents into LLM prompts. Effective context length and model-specific robustness to long contexts are therefore critical for reliable LLM deployment in research and business.

</details>


### [119] [Cost-Efficient Cross-Lingual Retrieval-Augmented Generation for Low-Resource Languages: A Case Study in Bengali Agricultural Advisory](https://arxiv.org/abs/2601.02065)
*Md. Asif Hossain,Nabil Subhan,Mantasha Rahman Mahi,Jannatul Ferdous Nabila*

Main category: cs.CL

TL;DR: The paper proposes a cross-lingual Retrieval-Augmented Generation (RAG) framework to improve Bengali agricultural advisory using open-source models, enabling reliable agricultural guidance in low-resource local languages.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the language barrier that prevents farmers in developing regions from accessing reliable agricultural advisory, where the majority of authoritative resources are in English and local languages like Bengali are underrepresented.

Method: The system employs a translation-centric RAG framework: user queries in Bengali are translated to English, enhanced with domain-specific keywords, answered through dense vector retrieval from agricultural manuals, and then translated back into Bengali to ensure accessibility.

Result: The framework demonstrates reliable factual responses, effectively ignores irrelevant queries, and provides an average latency of less than 20 seconds, making it practical and efficient for real-world use.

Conclusion: Cross-lingual retrieval combined with controlled translation offers a scalable, cost-effective solution for providing agricultural knowledge access in low-resource languages like Bengali.

Abstract: Access to reliable agricultural advisory remains limited in many developing regions due to a persistent language barrier: authoritative agricultural manuals are predominantly written in English, while farmers primarily communicate in low-resource local languages such as Bengali. Although recent advances in Large Language Models (LLMs) enable natural language interaction, direct generation in low-resource languages often exhibits poor fluency and factual inconsistency, while cloud-based solutions remain cost-prohibitive. This paper presents a cost-efficient, cross-lingual Retrieval-Augmented Generation (RAG) framework for Bengali agricultural advisory that emphasizes factual grounding and practical deployability. The proposed system adopts a translation-centric architecture in which Bengali user queries are translated into English, enriched through domain-specific keyword injection to align colloquial farmer terminology with scientific nomenclature, and answered via dense vector retrieval over a curated corpus of English agricultural manuals (FAO, IRRI). The generated English response is subsequently translated back into Bengali to ensure accessibility. The system is implemented entirely using open-source models and operates on consumer-grade hardware without reliance on paid APIs. Experimental evaluation demonstrates reliable source-grounded responses, robust rejection of out-of-domain queries, and an average end-to-end latency below 20 seconds. The results indicate that cross-lingual retrieval combined with controlled translation offers a practical and scalable solution for agricultural knowledge access in low-resource language settings

</details>


### [120] [Deferred Commitment Decoding for Diffusion Language Models with Confidence-Aware Sliding Windows](https://arxiv.org/abs/2601.02076)
*Yingte Shu,Yuchuan Tian,Chao Xu,Yunhe Wang,Hanting Chen*

Main category: cs.CL

TL;DR: This paper introduces Deferred Commitment Decoding (DCD), a strategy for improving generation accuracy in diffusion language models by deferring uncertain token predictions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the structural limitation of block-based diffusion in diffusion language models, which leads to reduced generation quality in certain tasks due to Boundary-Induced Context Truncation (BICT).

Method: The authors propose a training-free Deferred Commitment Decoding (DCD) strategy, which uses a sliding window to handle tokens dynamically, resolving low uncertainty tokens early and deferring uncertain ones for better contextual evidence.

Result: Extensive experiments demonstrate that DCD improves generation accuracy by an average of 1.39% compared to block-based diffusion approaches, with improvements as high as 9.0%, while maintaining comparable efficiency.

Conclusion: DCD offers a simple yet effective way to enhance both the quality and efficiency of diffusion language model decoding by leveraging uncertainty in token commitment.

Abstract: Diffusion language models (DLMs) have recently emerged as a strong alternative to autoregressive models by enabling parallel text generation. To improve inference efficiency and KV-cache compatibility, prior work commonly adopts block-based diffusion, decoding tokens block by block. However, this paradigm suffers from a structural limitation that we term Boundary-Induced Context Truncation (BICT): undecoded tokens near block boundaries are forced to commit without access to nearby future context, even when such context could substantially reduce uncertainty. This limitation degrades decoding confidence and generation quality, especially for tasks requiring precise reasoning, such as mathematical problem solving and code generation. We propose Deferred Commitment Decoding (DCD), a novel, training-free decoding strategy that mitigates this issue. DCD maintains a confidence-aware sliding window over masked tokens, resolving low-uncertainty tokens early while deferring high-uncertainty tokens until sufficient contextual evidence becomes available. This design enables effective bidirectional information flow within the decoding window without sacrificing efficiency. Extensive experiments across multiple diffusion language models, benchmarks, and caching configurations show that DCD improves generation accuracy by 1.39% with comparable time on average compared to fixed block-based diffusion methods, with the most significant improvement reaching 9.0%. These results demonstrate that deferring token commitment based on uncertainty is a simple yet effective principle for improving both the quality and efficiency of diffusion language model decoding.

</details>


### [121] [DeCode: Decoupling Content and Delivery for Medical QA](https://arxiv.org/abs/2601.02123)
*Po-Jen Ko,Chen-Han Tsai,Yu-Shao Peng*

Main category: cs.CL

TL;DR: The paper presents DeCode, a framework to enhance LLMs for generating patient-contextualized answers in clinical settings, demonstrating a 75% relative improvement on a health benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing large language models often fail to provide patient-centered answers despite their strong medical knowledge, necessitating solutions to improve clinical contextualization.

Method: DeCode is a training-free and model-agnostic framework designed to adapt LLMs to generate personalized and contextually relevant clinical responses.

Result: DeCode achieved a 75% relative performance improvement, increasing the benchmark score from 28.4% to 49.8% on OpenAI HealthBench.

Conclusion: The framework effectively enhances the capacity of LLMs to offer clinically meaningful and contextually accurate answers, showcasing its potential for clinical applications.

Abstract: Large language models (LLMs) exhibit strong medical knowledge and can generate factually accurate responses. However, existing models often fail to account for individual patient contexts, producing answers that are clinically correct yet poorly aligned with patients' needs. In this work, we introduce DeCode, a training-free, model-agnostic framework that adapts existing LLMs to produce contextualized answers in clinical settings. We evaluate DeCode on OpenAI HealthBench, a comprehensive and challenging benchmark designed to assess clinical relevance and validity of LLM responses. DeCode improves the previous state of the art from $28.4\%$ to $49.8\%$, corresponding to a $75\%$ relative improvement. Experimental results suggest the effectiveness of DeCode in improving clinical question answering of LLMs.

</details>


### [122] [Towards Multi-Level Transcript Segmentation: LoRA Fine-Tuning for Table-of-Contents Generation](https://arxiv.org/abs/2601.02128)
*Steffen Freisinger,Philipp Seeberger,Thomas Ranzenberger,Tobias Bocklet,Korbinian Riedhammer*

Main category: cs.CL

TL;DR: The paper introduces a novel hierarchical topic segmentation approach for speech transcripts, leveraging large language models and integrating speech pause features, achieving significant improvements and adapting evaluation metrics.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need to improve accessibility and usability of speech transcripts by organizing them into thematic sections and multi-level tables of contents.

Method: The authors employed zero-shot prompting and LoRA fine-tuning of large language models, while integrating speech pause features for hierarchical topic segmentation.

Result: Significant improvements over topic segmentation baselines were achieved when tested on English meeting recordings and multilingual lecture transcripts (Portuguese and German).

Conclusion: The novel approach, combining advanced modeling with pause features and a multi-level evaluation metric, effectively enhances transcript segmenting and sets a foundation for future advancements in this domain.

Abstract: Segmenting speech transcripts into thematic sections benefits both downstream processing and users who depend on written text for accessibility. We introduce a novel approach to hierarchical topic segmentation in transcripts, generating multi-level tables of contents that capture both topic and subtopic boundaries. We compare zero-shot prompting and LoRA fine-tuning on large language models, while also exploring the integration of high-level speech pause features. Evaluations on English meeting recordings and multilingual lecture transcripts (Portuguese, German) show significant improvements over established topic segmentation baselines. Additionally, we adapt a common evaluation measure for multi-level segmentation, taking into account all hierarchical levels within one metric.

</details>


### [123] [Routing by Analogy: kNN-Augmented Expert Assignment for Mixture-of-Experts](https://arxiv.org/abs/2601.02144)
*Boxuan Lyu,Soichiro Murakami,Hidetaka Kamigaito,Peinan Zhang*

Main category: cs.CL

TL;DR: The paper presents kNN-MoE, a routing framework for Mixture-of-Experts models that dynamically adapts to distribution shifts using past case memories instead of relying solely on a frozen router.


<details>
  <summary>Details</summary>
Motivation: Frozen routing decisions in MoE models are inadequate in handling distribution shifts, leading to inefficiencies in token assignment to experts.

Method: The paper introduces kNN-MoE, which retrieves and reuses past optimal expert assignments stored in memory and uses a similarity measure to mix these with the frozen router's outputs.

Result: kNN-MoE surpasses zero-shot baselines and matches the performance of supervised fine-tuning, while being computationally efficient.

Conclusion: kNN-MoE provides a robust and flexible routing mechanism for MoE models, combining memory-based retrieval and frozen routing, making it resilient under distribution shifts.

Abstract: Mixture-of-Experts (MoE) architectures scale large language models efficiently by employing a parametric "router" to dispatch tokens to a sparse subset of experts. Typically, this router is trained once and then frozen, rendering routing decisions brittle under distribution shifts. We address this limitation by introducing kNN-MoE, a retrieval-augmented routing framework that reuses optimal expert assignments from a memory of similar past cases. This memory is constructed offline by directly optimizing token-wise routing logits to maximize the likelihood on a reference set. Crucially, we use the aggregate similarity of retrieved neighbors as a confidence-driven mixing coefficient, thus allowing the method to fall back to the frozen router when no relevant cases are found. Experiments show kNN-MoE outperforms zero-shot baselines and rivals computationally expensive supervised fine-tuning.

</details>


### [124] [FormationEval, an open multiple-choice benchmark for petroleum geoscience](https://arxiv.org/abs/2601.02158)
*Almaz Ermilov*

Main category: cs.CL

TL;DR: FormationEval is a new benchmark with 505 petroleum geoscience questions to evaluate language models. Top-performing proprietary and open-weight models achieve >97% accuracy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to provide a specialized benchmark for testing the capabilities of language models in petroleum geoscience and allied disciplines, where no such targeted benchmark existed.

Method: An open multiple-choice benchmark with concept-derived questions and auditing metadata was created, evaluated across 72 language models, and analyzed with attention to domain and tier gaps, as well as performance biases.

Result: Top-tier models like Gemini 3 Pro Preview reached >99% accuracy. Open-weight models performed better than expected, with GLM-4.7 achieving 98.6%.

Conclusion: The study highlights the narrowing performance gap between open-weight and closed models and notes the importance of domain-specific language model evaluation using benchmarks like FormationEval.

Abstract: This paper presents FormationEval, an open multiple-choice question benchmark for evaluating language models on petroleum geoscience and subsurface disciplines. The dataset contains 505 questions across seven domains including petrophysics, petroleum geology and reservoir engineering, derived from three authoritative sources using a reasoning model with detailed instructions and a concept-based approach that avoids verbatim copying of copyrighted text. Each question includes source metadata to support traceability and audit. The evaluation covers 72 models from major providers including OpenAI, Anthropic, Google, Meta and open-weight alternatives. The top performers achieve over 97\% accuracy, with Gemini 3 Pro Preview reaching 99.8\%, while tier and domain gaps persist. Among open-weight models, GLM-4.7 leads at 98.6\%, with several DeepSeek, Llama, Qwen and Mistral models also exceeding 93\%. The performance gap between open-weight and closed models is narrower than expected, with several lower-cost open-weight models exceeding 90\% accuracy. Petrophysics emerges as the most challenging domain across all models, while smaller models show wider performance variance. Residual length bias in the dataset (correct answers tend to be longer) is documented along with bias mitigation strategies applied during construction. The benchmark, evaluation code and results are publicly available.

</details>


### [125] [Confidence Estimation for LLMs in Multi-turn Interactions](https://arxiv.org/abs/2601.02179)
*Caiqi Zhang,Ruihan Yang,Xiaochen Zhu,Chengzu Li,Tiancheng Hu,Yijiang River Dong,Deqing Yang,Nigel Collier*

Main category: cs.CL

TL;DR: The paper investigates confidence estimation in multi-turn dialogues for Large Language Models (LLMs), proposing new metrics and frameworks for evaluation and identifying existing challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the largely unexplored area of confidence estimation in multi-turn conversations, which is essential for applications like autonomous agents and human-in-the-loop systems.

Method: The paper establishes a formal evaluation framework focusing on per-turn calibration and monotonicity of confidence. It introduces metrics like InfoECE and a new "Hinter-Guesser" paradigm for generating evaluation datasets.

Result: Experiments show that existing confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. The paper proposes P(Sufficient), a logit-based probe, which performs better but highlights that the task remains unsolved.

Conclusion: This study lays the groundwork for improving confidence estimation methods in conversational agents, aiming for more reliable and trustworthy systems.

Abstract: While confidence estimation is a promising direction for mitigating hallucinations in Large Language Models (LLMs), current research dominantly focuses on single-turn settings. The dynamics of model confidence in multi-turn conversations, where context accumulates and ambiguity is progressively resolved, remain largely unexplored. Reliable confidence estimation in multi-turn settings is critical for many downstream applications, such as autonomous agents and human-in-the-loop systems. This work presents the first systematic study of confidence estimation in multi-turn interactions, establishing a formal evaluation framework grounded in two key desiderata: per-turn calibration and monotonicity of confidence as more information becomes available. To facilitate this, we introduce novel metrics, including a length-normalized Expected Calibration Error (InfoECE), and a new "Hinter-Guesser" paradigm for generating controlled evaluation datasets. Our experiments reveal that widely-used confidence techniques struggle with calibration and monotonicity in multi-turn dialogues. We propose P(Sufficient), a logit-based probe that achieves comparatively better performance, although the task remains far from solved. Our work provides a foundational methodology for developing more reliable and trustworthy conversational agents.

</details>


### [126] [Toward Global Large Language Models in Medicine](https://arxiv.org/abs/2601.02186)
*Rui Yang,Huitao Li,Weihao Xuan,Heli Qi,Xin Li,Kunyu Yu,Yingjian Chen,Rongrong Wang,Jacques Behmoaras,Tianxi Cai,Bibhas Chakraborty,Qingyu Chen,Lionel Tim-Ee Cheng,Marie-Louise Damwanza,Chido Dzinotyiwei,Aosong Feng,Chuan Hong,Yusuke Iwasawa,Yuhe Ke,Linah Kitala,Taehoon Ko,Jisan Lee,Irene Li,Jonathan Chong Kai Liew,Hongfang Liu,Lian Leng Low,Edison Marrese-Taylor,Yutaka Matsuo,Isheanesu Misi,Yilin Ning,Jasmine Chiat Ling Ong,Marcus Eng Hock Ong,Enrico Petretto,Hossein Rouhizadeh,Abiram Sandralegar,Oren Schreier,Iain Bee Huat Tan,Patrick Tan,Daniel Shu Wei Ting,Junjue Wang,Chunhua Weng,Matthew Yu Heng Wong,Fang Wu,Yunze Xiao,Xuhai Xu,Qingcheng Zeng,Zhuo Zheng,Yifan Peng,Douglas Teodoro,Nan Liu*

Main category: cs.CL

TL;DR: The paper addresses the limitation of LLMs for low-resource languages by developing GlobMed (a multilingual medical dataset) and GlobMed-LLMs, significantly improving performance on low-resource languages.


<details>
  <summary>Details</summary>
Motivation: To improve global healthcare access and quality by addressing the lack of representation of low-resource languages in current LLMs, which limits their usefulness in global medical contexts.

Method: The authors constructed GlobMed, a multilingual medical dataset, and GlobMed-Bench, an evaluation framework. They also developed GlobMed-LLMs, multilingual medical language models with parameters ranging from 1.7B to 8B, trained on the newly created dataset.

Result: GlobMed-LLMs showed superior performance with a 40% average improvement over baseline models and a threefold performance increase on low-resource languages.

Conclusion: The work provides a significant foundation for equitable development and application of LLMs globally, allowing broader linguistic communities to benefit from advancements in medical technology.

Abstract: Despite continuous advances in medical technology, the global distribution of health care resources remains uneven. The development of large language models (LLMs) has transformed the landscape of medicine and holds promise for improving health care quality and expanding access to medical information globally. However, existing LLMs are primarily trained on high-resource languages, limiting their applicability in global medical scenarios. To address this gap, we constructed GlobMed, a large multilingual medical dataset, containing over 500,000 entries spanning 12 languages, including four low-resource languages. Building on this, we established GlobMed-Bench, which systematically assesses 56 state-of-the-art proprietary and open-weight LLMs across multiple multilingual medical tasks, revealing significant performance disparities across languages, particularly for low-resource languages. Additionally, we introduced GlobMed-LLMs, a suite of multilingual medical LLMs trained on GlobMed, with parameters ranging from 1.7B to 8B. GlobMed-LLMs achieved an average performance improvement of over 40% relative to baseline models, with a more than threefold increase in performance on low-resource languages. Together, these resources provide an important foundation for advancing the equitable development and application of LLMs globally, enabling broader language communities to benefit from technological advances.

</details>


### [127] [ARCADE: A City-Scale Corpus for Fine-Grained Arabic Dialect Tagging](https://arxiv.org/abs/2601.02209)
*Omer Nacar,Serry Sibaee,Adel Ammar,Yasser Alhabashi,Nadia Samer Sibai,Yara Farouk Ahmed,Ahmed Saud Alqusaiyer,Sulieman Mahmoud AlMahmoud,Abdulrhman Mamdoh Mukhaniq,Lubaba Raed,Sulaiman Mohammed Alatwah,Waad Nasser Alqahtani,Yousif Abdulmajeed Alnasser,Mohamed Aziz Khadraoui,Wadii Boulila*

Main category: cs.CL

TL;DR: ARCADE is the first Arabic speech dataset designed for city-level dialect analysis, compiled from Arabic radio streams into 3,790 audio segments representing 58 cities and 19 countries.


<details>
  <summary>Details</summary>
Motivation: To address the gap in mapping Arabic speech to city-level dialects and provide a benchmark for dialect tagging across regions.

Method: Collected 30-second radio speech segments from streaming services; annotated by native Arabic speakers with metadata like emotion, speech type, and dialect.

Result: Released 6,907 annotations across 3,790 unique audio clips, covering Arabic dialects from 58 cities and 19 countries.

Conclusion: ARCADE enables detailed dialect analysis and multi-task learning by providing fine-grained data for Arabic linguistics and speech processing research.

Abstract: The Arabic language is characterized by a rich tapestry of regional dialects that differ substantially in phonetics and lexicon, reflecting the geographic and cultural diversity of its speakers. Despite the availability of many multi-dialect datasets, mapping speech to fine-grained dialect sources, such as cities, remains underexplored. We present ARCADE (Arabic Radio Corpus for Audio Dialect Evaluation), the first Arabic speech dataset designed explicitly with city-level dialect granularity. The corpus comprises Arabic radio speech collected from streaming services across the Arab world. Our data pipeline captures 30-second segments from verified radio streams, encompassing both Modern Standard Arabic (MSA) and diverse dialectal speech. To ensure reliability, each clip was annotated by one to three native Arabic reviewers who assigned rich metadata, including emotion, speech type, dialect category, and a validity flag for dialect identification tasks. The resulting corpus comprises 6,907 annotations and 3,790 unique audio segments spanning 58 cities across 19 countries. These fine-grained annotations enable robust multi-task learning, serving as a benchmark for city-level dialect tagging. We detail the data collection methodology, assess audio quality, and provide a comprehensive analysis of label distributions. The dataset is available on: https://huggingface.co/datasets/riotu-lab/ARCADE-full

</details>


### [128] [From XAI to Stories: A Factorial Study of LLM-Generated Explanation Quality](https://arxiv.org/abs/2601.02224)
*Fabian Lukassen,Jan Herrmann,Christoph Weisser,Benjamin Saefken,Thomas Kneib*

Main category: cs.CL

TL;DR: This paper investigates how various factors like forecasting models, XAI methods, LLMs, and prompting strategies affect the quality of natural language explanations for time-series forecasting.


<details>
  <summary>Details</summary>
Motivation: To identify what contributes most to high-quality natural language explanations for Explainable AI methods, especially for non-expert users, and to evaluate the methodology surrounding these explanations systematically.

Method: The study employs a factorial design involving four forecasting models, three XAI methods, three LLMs, and eight prompting strategies. The evaluation is conducted using G-Eval with dual LLM judges on 660 explanations across multiple criteria.

Result: The findings are: XAI methods offer marginal improvements, LLM choice heavily influences outcome quality, classical models like SARIMAX paradoxically produce lower NLE quality despite better accuracy, zero-shot prompting is cost-effective and competitive, and chain-of-thought prompting reduces quality.

Conclusion: For time-series forecasting, LLM selection is the dominant factor in NLE quality over XAI and other strategies, with cost-effective methods like zero-shot prompting proving useful. However, XAI contributes only minimally to explanation quality in this domain.

Abstract: Explainable AI (XAI) methods like SHAP and LIME produce numerical feature attributions that remain inaccessible to non expert users. Prior work has shown that Large Language Models (LLMs) can transform these outputs into natural language explanations (NLEs), but it remains unclear which factors contribute to high-quality explanations. We present a systematic factorial study investigating how Forecasting model choice, XAI method, LLM selection, and prompting strategy affect NLE quality. Our design spans four models (XGBoost (XGB), Random Forest (RF), Multilayer Perceptron (MLP), and SARIMAX - comparing black-box Machine-Learning (ML) against classical time-series approaches), three XAI conditions (SHAP, LIME, and a no-XAI baseline), three LLMs (GPT-4o, Llama-3-8B, DeepSeek-R1), and eight prompting strategies. Using G-Eval, an LLM-as-a-judge evaluation method, with dual LLM judges and four evaluation criteria, we evaluate 660 explanations for time-series forecasting. Our results suggest that: (1) XAI provides only small improvements over no-XAI baselines, and only for expert audiences; (2) LLM choice dominates all other factors, with DeepSeek-R1 outperforming GPT-4o and Llama-3; (3) we observe an interpretability paradox: in our setting, SARIMAX yielded lower NLE quality than ML models despite higher prediction accuracy; (4) zero-shot prompting is competitive with self-consistency at 7-times lower cost; and (5) chain-of-thought hurts rather than helps.

</details>


### [129] [CD4LM: Consistency Distillation and aDaptive Decoding for Diffusion Language Models](https://arxiv.org/abs/2601.02236)
*Yihao Liang,Ze Wang,Hao Chen,Ximeng Sun,Jialian Wu,Xiaodong Yu,Jiang Liu,Emad Barsoum,Zicheng Liu,Niraj K. Jha*

Main category: cs.CL

TL;DR: Diffusion language models face challenges in efficient parallel decoding due to misalignment between training and inference strategies. The proposed CD4LM framework addresses this, achieving faster decoding without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Autoregressive language models are efficient but restricted by sequential dependence during decoding. Diffusion models offer parallel capabilities but need improvements for practical application.

Method: The paper introduces Discrete-Space Consistency Distillation (DSCD) for robust training and Confidence-Adaptive Decoding (CAD) for dynamic resource allocation during inference.

Result: CD4LM achieves up to 5.18x speedup compared to baseline models while maintaining or improving accuracy across various benchmarks.

Conclusion: The framework effectively enables low-latency, high-quality parallel decoding, enhancing the efficiency of diffusion language models.

Abstract: Autoregressive large language models achieve strong results on many benchmarks, but decoding remains fundamentally latency-limited by sequential dependence on previously generated tokens. Diffusion language models (DLMs) promise parallel generation but suffer from a fundamental static-to-dynamic misalignment: Training optimizes local transitions under fixed schedules, whereas efficient inference requires adaptive "long-jump" refinements through unseen states. Our goal is to enable highly parallel decoding for DLMs with low number of function evaluations while preserving generation quality. To achieve this, we propose CD4LM, a framework that decouples training from inference via Discrete-Space Consistency Distillation (DSCD) and Confidence-Adaptive Decoding (CAD). Unlike standard objectives, DSCD trains a student to be trajectory-invariant, mapping diverse noisy states directly to the clean distribution. This intrinsic robustness enables CAD to dynamically allocate compute resources based on token confidence, aggressively skipping steps without the quality collapse typical of heuristic acceleration. On GSM8K, CD4LM matches the LLaDA baseline with a 5.18x wall-clock speedup; across code and math benchmarks, it strictly dominates the accuracy-efficiency Pareto frontier, achieving a 3.62x mean speedup while improving average accuracy. Code is available at https://github.com/yihao-liang/CDLM

</details>


### [130] [pdfQA: Diverse, Challenging, and Realistic Question Answering over PDFs](https://arxiv.org/abs/2601.02285)
*Tobias Schimanski,Imene Kolli,Jingwei Ni,Yu Fan,Ario Saeid Vaghefi,Elliott Ash,Markus Leippold*

Main category: cs.CL

TL;DR: pdfQA introduces a multi-domain QA dataset for PDFs with human-annotated and synthetic data, evaluating challenges based on complexity dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing QA datasets mainly focus on text sources and specific domains, neglecting the complexities of PDFs, which are widely used.

Method: Developed pdfQA with 2K human-annotated and synthetic QA pairs, assessed across ten complexity dimensions, using open-source LLMs for evaluation.

Result: Created valid and challenging QA pairs, revealing correlations between complexity dimensions and LLM performance challenges.

Conclusion: pdfQA serves as a foundation for evaluating end-to-end QA pipelines, addressing diverse skills and local optimizations.

Abstract: PDFs are the second-most used document type on the internet (after HTML). Yet, existing QA datasets commonly start from text sources or only address specific domains. In this paper, we present pdfQA, a multi-domain 2K human-annotated (real-pdfQA) and 2K synthetic dataset (syn-pdfQA) differentiating QA pairs in ten complexity dimensions (e.g., file type, source modality, source position, answer type). We apply and evaluate quality and difficulty filters on both datasets, obtaining valid and challenging QA pairs. We answer the questions with open-source LLMs, revealing existing challenges that correlate with our complexity dimensions. pdfQA presents a basis for end-to-end QA pipeline evaluation, testing diverse skill sets and local optimizations (e.g., in information retrieval or parsing).

</details>


### [131] [Power-of-Two Quantization-Aware-Training (PoT-QAT) in Large Language Models (LLMs)](https://arxiv.org/abs/2601.02298)
*Mahmoud Elgenedy*

Main category: cs.CL

TL;DR: The paper proposes Power-of-Two (PoT) quantization for compressing weights in large language models to address memory and computational challenges on Edge devices.


<details>
  <summary>Details</summary>
Motivation: The growing size of large language models creates challenges for implementation on resource-constrained Edge devices, necessitating efficient strategies to reduce memory and computation demands.

Method: The authors propose compressing model weights using Power-of-Two quantization and employ Quantization Aware Training (QAT) to mitigate performance loss.

Result: Experiments on GPT-2 124M show an improvement of 66% in perplexity and a minor BERT-Score loss of 1% post-training, alongside 87.5% memory savings and 3-10x faster inference speeds.

Conclusion: Power-of-Two quantization, coupled with Quantization Aware Training, offers a promising approach to enable resource-efficient deployment of large language models on Edge devices.

Abstract: In Large Language Models (LLMs), the number of parameters has grown exponentially in the past few years, e.g., from 1.5 billion parameters in GPT-2 to 175 billion in GPT-3 to possibly more than trillion in higher versions. This raises a significant challenge for implementation, especially for Edge devices. Unlike cloud computing, memory and processing power for Edge devices are very limited, which necessitates developing novel ideas to make such applications feasible. In this work, we investigate compressing weights with a special quantization that limits numbers to only power-of-two (PoT). This helps save a huge amount of memory as only exponents need to be stored, more importantly, it significantly reduces processing power by replacing costly multiplication with low cost bit shifting. To overcome performance loss due to this strict quantization, we investigate Quantization Aware Training (QAT) to enhance performance through additional training. Results on GPT-2 124M show a major enhancement for quantized PoT model after additional training, with a perplexity enhancement of 66% and BERT-Score loss to baseline GPT-2 of 1%. The memory saving is estimated to be 87.5% while the inference speed is expected to be 3-10x faster with PoT quantization versus full-precision.

</details>


### [132] [Classifying several dialectal Nawatl varieties](https://arxiv.org/abs/2601.02303)
*Juan-José Guzmán-Landa,Juan-Manuel Torres-Moreno,Miguel Figueroa-Saavedra,Carlos-Emiliano González-Gallardo,Graham Ranger,Martha Lorena-Avendaño-Garrido*

Main category: cs.CL

TL;DR: This paper analyzes methods to classify Nawatl dialectal varieties using Machine Learning and Neural Networks.


<details>
  <summary>Details</summary>
Motivation: Nawatl, widely spoken in Mexico, has limited computational resources, especially for its dialectal variations and diverse spellings. Addressing this gap is crucial for preserving the rich heritage and enhancing its linguistic resource availability.

Method: The study develops techniques employing Machine Learning and Neural Networks for the classification of Nawatl dialectal varieties.

Result: Using technological methods, the classification of Nawatl varieties is facilitated, providing insights to handle their linguistic complexity.

Conclusion: The paper provides a significant contribution to the development of computational tools for indigenous languages, enabling better understanding and analysis of dialectal variations in Nawatl.

Abstract: Mexico is a country with a large number of indigenous languages, among which the most widely spoken is Nawatl, with more than two million people currently speaking it (mainly in North and Central America). Despite its rich cultural heritage, which dates back to the 15th century, Nawatl is a language with few computer resources. The problem is compounded when it comes to its dialectal varieties, with approximately 30 varieties recognised, not counting the different spellings in the written forms of the language. In this research work, we addressed the problem of classifying Nawatl varieties using Machine Learning and Neural Networks.

</details>


### [133] [Estimating Text Temperature](https://arxiv.org/abs/2601.02320)
*Nikolay Mikhaylovskiy*

Main category: cs.CL

TL;DR: The study evaluates methods to estimate the temperature of generated or human-written text with respect to a language model.


<details>
  <summary>Details</summary>
Motivation: To develop a method for estimating the randomness in texts—including human-written ones—through the lens of language model probability distributions.

Method: The procedure involves using a maximum likelihood approach to estimate the temperature parameter, followed by evaluations across various models and applying the best-performing model (Qwen3 14B) to analyze text corpora.

Result: The Qwen3 14B model was found to be the best performer in temperature estimation.

Conclusion: It is feasible and effective to estimate the temperature of text generated by language models or humans using a selected approach; Qwen3 14B successfully measures text randomness.

Abstract: Autoregressive language models typically use temperature parameter at inference to shape the probability distribution and control the randomness of the text generated. After the text was generated, this parameter can be estimated using maximum likelihood approach. Following it, we propose a procedure to estimate the temperature of any text, including ones written by humans, with respect to a given language model. We evaluate the temperature estimation capability of a wide selection of small-to-medium LLMs. We then use the best-performing Qwen3 14B to estimate temperatures of popular corpora.

</details>


### [134] [Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling](https://arxiv.org/abs/2601.02337)
*Berk Atil,Rebecca J. Passonneau,Ninareh Mehrabi*

Main category: cs.CL

TL;DR: This paper evaluates persona-aware toxicity detection with various LLM prompting methods, ultimately proposing a robust SVM-based meta-ensemble that outperforms individual approaches.


<details>
  <summary>Details</summary>
Motivation: Understanding and addressing the subjective nature of toxicity detection shaped by diverse demographic perspectives and social contexts.

Method: Systematic evaluation of LLM-based toxicity detection methods, exploration of ensembling techniques (including a meta-ensemble using SVM over prompt outputs), and analyzing persona-conditioned prompting.

Result: The SVM-based ensemble surpasses individual prompting methods and majority-vote techniques in tackling subjective NLP tasks across diverse personas.

Conclusion: A robust framework for pluralistic evaluation in toxicity detection, highlighting the inconsistency of single prompting techniques and proposing an effective ensembling strategy.

Abstract: Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic'' modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [135] [Free Energy-Based Modeling of Emotional Dynamics in Video Advertisements](https://arxiv.org/abs/2601.00812)
*Takashi Ushio,Kazuhiro Onishi,Hideyoshi Yanagisawa*

Main category: cs.CV

TL;DR: This paper analyzes emotional responses to advertising videos using scene-level expression features, without relying on external information, based on the free energy principle.


<details>
  <summary>Details</summary>
Motivation: To develop explainable emotion estimation methods for advertising without relying on physiological signals or subjective ratings.

Method: The study quantified emotions like 'pleasantness' and 'surprise' using the free energy principle components (KLD, BS, UN) across 1,059 video ads.

Result: Key emotions were mapped to specific features (e.g., KLD for 'pleasantness', BS for 'surprise'). Patterns were robust across hyperparameters and generalized across different video genres and durations.

Conclusion: The proposed methodology is effective in analyzing emotional engagement in advertising videos, with potential extensions for broader applications and integration with subjective ratings.

Abstract: Emotional responses during advertising video viewing are recognized as essential for understanding media effects because they have influenced attention, memory, and purchase intention. To establish a methodological basis for explainable emotion estimation without relying on external information such as physiological signals or subjective ratings, we have quantified "pleasantness," "surprise," and "habituation" solely from scene-level expression features of advertising videos, drawing on the free energy(FE) principle, which has provided a unified account of perception, learning, and behavior. In this framework, Kullback-Leibler divergence (KLD) has captured prediction error, Bayesian surprise (BS) has captured belief updates, and uncertainty (UN) has reflected prior ambiguity, and together they have formed the core components of FE. Using 1,059 15 s food video advertisements, the experiments have shown that KLD has reflected "pleasantness" associated with brand presentation, BS has captured "surprise" arising from informational complexity, and UN has reflected "surprise" driven by uncertainty in element types and spatial arrangements, as well as by the variability and quantity of presented elements. This study also identified three characteristic emotional patterns, namely uncertain stimulus, sustained high emotion, and momentary peak and decay, demonstrating the usefulness of the proposed method. Robustness across nine hyperparameter settings and generalization tests with six types of Japanese advertising videos (three genres and two durations) confirmed that these tendencies remained stable. This work can be extended by integrating a wider range of expression elements and validating the approach through subjective ratings, ultimately guiding the development of technologies that can support the creation of more engaging advertising videos.

</details>


### [136] [Can Generative Models Actually Forge Realistic Identity Documents?](https://arxiv.org/abs/2601.00829)
*Alexander Vinogradov*

Main category: cs.CV

TL;DR: The paper evaluates open-source diffusion-based generative models' capability to forge identity documents, finding that they cannot convincingly achieve forensic-level authenticity.


<details>
  <summary>Details</summary>
Motivation: To address public concerns about misuse of generative image models for creating false identity documents and to assess their actual capability in bypassing verification systems.

Method: The authors used text-to-image and image-to-image generative processes with multiple publicly available generative model families, including Stable Diffusion, Qwen, Flux, and Nano-Banana, to test their ability to generate forged identity documents.

Result: Current generative models can create visually realistic documents but fail to reproduce structural and forensic authenticity needed to bypass human or automated verification.

Conclusion: The risk of generative models creating forensic-level identity document forgeries is overestimated. Collaboration between machine learning experts and document-forensics professionals is crucial for accurate risk evaluations.

Abstract: Generative image models have recently shown significant progress in image realism, leading to public concerns about their potential misuse for document forgery. This paper explores whether contemporary open-source and publicly accessible diffusion-based generative models can produce identity document forgeries that could realistically bypass human or automated verification systems. We evaluate text-to-image and image-to-image generation pipelines using multiple publicly available generative model families, including Stable Diffusion, Qwen, Flux, Nano-Banana, and others. The findings indicate that while current generative models can simulate surface-level document aesthetics, they fail to reproduce structural and forensic authenticity. Consequently, the risk of generative identity document deepfakes achieving forensic-level authenticity may be overestimated, underscoring the value of collaboration between machine learning practitioners and document-forensics experts in realistic risk assessment.

</details>


### [137] [Pediatric Pneumonia Detection from Chest X-Rays:A Comparative Study of Transfer Learning and Custom CNNs](https://arxiv.org/abs/2601.00837)
*Agniv Roy Choudhury*

Main category: cs.CV

TL;DR: The study highlights that transfer learning, particularly fine-tuned ResNet50, achieves near-perfect accuracy for detecting pediatric pneumonia from chest X-rays.


<details>
  <summary>Details</summary>
Motivation: To address the high mortality in children under five caused by pneumonia and the limitations of radiologist-based diagnosis.

Method: The study compares custom CNNs and transfer learning models using a dataset of 5,216 pediatric chest X-rays split into training, validation, and testing sets. Seven models were evaluated based on accuracy, F1-score, and AUC with Grad-CAM visualizations for interpretability.

Result: Fine-tuned ResNet50 demonstrated the highest performance with 99.43% accuracy, 99.61% F1-score, and 99.93% AUC, outperforming frozen-backbone models significantly. Grad-CAM visualizations confirmed the models focused on relevant lung regions.

Conclusion: This study concludes that transfer learning with a fine-tuning approach is highly effective in pneumonia detection from pediatric chest X-rays and has great potential for deployment in resource-constrained settings. Further research is encouraged on broader datasets.

Abstract: Pneumonia is a leading cause of mortality in children under five, with over 700,000 deaths annually. Accurate diagnosis from chest X-rays is limited by radiologist availability and variability.
  Objective: This study compares custom CNNs trained from scratch with transfer learning (ResNet50, DenseNet121, EfficientNet-B0) for pediatric pneumonia detection, evaluating frozen-backbone and fine-tuning regimes.
  Methods: A dataset of 5,216 pediatric chest X-rays was split 80/10/10 for training, validation, and testing. Seven models were trained and assessed using accuracy, F1-score, and AUC. Grad-CAM visualizations provided explainability.
  Results: Fine-tuned ResNet50 achieved the best performance: 99.43\% accuracy, 99.61\% F1-score, and 99.93\% AUC, with only 3 misclassifications. Fine-tuning outperformed frozen-backbone models by 5.5 percentage points on average. Grad-CAM confirmed clinically relevant lung regions guided predictions.
  Conclusions: Transfer learning with fine-tuning substantially outperforms CNNs trained from scratch for pediatric pneumonia detection, showing near-perfect accuracy. This system has strong potential as a screening tool in resource-limited settings. Future work should validate these findings on multi-center and adult datasets.
  Keywords: Pneumonia detection, deep learning, transfer learning, CNN, chest X-ray, pediatric diagnosis, ResNet, DenseNet, EfficientNet, Grad-CAM.

</details>


### [138] [Unified Review and Benchmark of Deep Segmentation Architectures for Cardiac Ultrasound on CAMUS](https://arxiv.org/abs/2601.00839)
*Zahid Ullah,Muhammad Hilal,Eunsoo Lee,Dragan Pamucar,Jihie Kim*

Main category: cs.CV

TL;DR: The paper benchmarks U-Net, Attention U-Net, and TransUNet for cardiac ultrasound segmentation using the CAMUS dataset under standardized conditions, examining preprocessing methods and self-supervised pretraining effects.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a unified and reproducible benchmark connecting cardiac imaging advancements and DL architectures, and provide experimental comparisons using standardized conditions.

Method: The study uses the CAMUS echocardiography dataset to evaluate three architectures (U-Net, Attention U-Net, TransUNet) under varying preprocessing methods, including NIfTI volumes, PNG exports, pseudo-labeling, and self-supervised pretraining.

Result: A plain U-Net achieved 94% mean Dice on NIfTI data, while PNG workflows reached 91%. Attention U-Net improved segmentation in difficult regions, and TransUNet generalized better with global spatial context modeling, particularly with SSL initialization. Pseudo-labeling increased robustness.

Conclusion: This paper delivers a harmonized benchmark of major DL architectures, guidance for ultrasound data preparation, and insights into scalable self-supervision and multimodal annotation pipelines for efficient dataset handling.

Abstract: Several review papers summarize cardiac imaging and DL advances, few works connect this overview to a unified and reproducible experimental benchmark. In this study, we combine a focused review of cardiac ultrasound segmentation literature with a controlled comparison of three influential architectures, U-Net, Attention U-Net, and TransUNet, on the Cardiac Acquisitions for Multi-Structure Ultrasound Segmentation (CAMUS) echocardiography dataset. Our benchmark spans multiple preprocessing routes, including native NIfTI volumes, 16-bit PNG exports, GPT-assisted polygon-based pseudo-labels, and self-supervised pretraining (SSL) on thousands of unlabeled cine frames. Using identical training splits, losses, and evaluation criteria, a plain U-Net achieved a 94% mean Dice when trained directly on NIfTI data (preserving native dynamic range), while the PNG-16-bit workflow reached 91% under similar conditions. Attention U-Net provided modest improvements on small or low-contrast regions, reducing boundary leakage, whereas TransUNet demonstrated the strongest generalization on challenging frames due to its ability to model global spatial context, particularly when initialized with SSL. Pseudo-labeling expanded the training set and improved robustness after confidence filtering. Overall, our contributions are threefold: a harmonized, apples-to-apples benchmark of U-Net, Attention U-Net, and TransUNet under standardized CAMUS preprocessing and evaluation; practical guidance on maintaining intensity fidelity, resolution consistency, and alignment when preparing ultrasound data; and an outlook on scalable self-supervision and emerging multimodal GPT-based annotation pipelines for rapid labeling, quality assurance, and targeted dataset curation.

</details>


### [139] [Motion-Compensated Latent Semantic Canvases for Visual Situational Awareness on Edge](https://arxiv.org/abs/2601.00854)
*Igor Lodin,Sergii Filatov,Vira Filatova,Dmytro Filatov*

Main category: cs.CV

TL;DR: The paper introduces Motion-Compensated Latent Semantic Canvases (MCLSC) for efficient visual situational awareness on edge devices, utilizing latent canvases and motion-gated segmentation.


<details>
  <summary>Details</summary>
Motivation: The need for efficient semantic processing on resource-limited edge devices for visual awareness tasks.

Method: The system uses two latent canvases for semantic metadata - a static layer and a dynamic layer - stabilized from video input with motion-gated panoptic segmentation triggered by motion detection.

Result: The method reduces segmentation processing by over 30x and processing time by over 20x on 480p videos compared to traditional frame-by-frame segmentation, while ensuring coherent semantic overlays.

Conclusion: MCLSC achieves significant efficiency and maintains semantic awareness, making it suitable for edge device applications.

Abstract: We propose Motion-Compensated Latent Semantic Canvases (MCLSC) for visual situational awareness on resource-constrained edge devices. The core idea is to maintain persistent semantic metadata in two latent canvases - a slowly accumulating static layer and a rapidly updating dynamic layer - defined in a baseline coordinate frame stabilized from the video stream. Expensive panoptic segmentation (Mask2Former) runs asynchronously and is motion-gated: inference is triggered only when motion indicates new information, while stabilization/motion compensation preserves a consistent coordinate system for latent semantic memory. On prerecorded 480p clips, our prototype reduces segmentation calls by >30x and lowers mean end-to-end processing time by >20x compared to naive per-frame segmentation, while maintaining coherent static/dynamic semantic overlays.

</details>


### [140] [VL-OrdinalFormer: Vision Language Guided Ordinal Transformers for Interpretable Knee Osteoarthritis Grading](https://arxiv.org/abs/2601.00879)
*Zahid Ullah,Jihie Kim*

Main category: cs.CV

TL;DR: VLOrdinalFormer proposes a vision language-guided ordinal learning framework for automated knee osteoarthritis grading, achieving state-of-the-art performance in distinguishing subtle stages like KL1 and KL2 while ensuring robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty and inter-observer variability in assessing early stages of knee osteoarthritis severity (KL1 vs KL2) using radiographs.

Method: Combines ViT L16 backbone with CORAL-based ordinal regression, CLIP-driven semantic alignment, stratified cross-validation, re-weighting for intermediate grades, and test-time augmentation.

Result: VLOrdinalFormer improves accuracy and macro F1 score, achieving reliable performance for all grades, with notable advancements in distinguishing KL1 and KL2.

Conclusion: The framework is an interpretable and effective tool for KOA grading, emphasizing its utility for clinical and radiological practice.

Abstract: Knee osteoarthritis (KOA) is a leading cause of disability worldwide, and accurate severity assessment using the Kellgren Lawrence (KL) grading system is critical for clinical decision making. However, radiographic distinctions between early disease stages, particularly KL1 and KL2, are subtle and frequently lead to inter-observer variability among radiologists. To address these challenges, we propose VLOrdinalFormer, a vision language guided ordinal learning framework for fully automated KOA grading from knee radiographs. The proposed method combines a ViT L16 backbone with CORAL based ordinal regression and a Contrastive Language Image Pretraining (CLIP) driven semantic alignment module, allowing the model to incorporate clinically meaningful textual concepts related to joint space narrowing, osteophyte formation, and subchondral sclerosis. To improve robustness and mitigate overfitting, we employ stratified five fold cross validation, class aware re weighting to emphasize challenging intermediate grades, and test time augmentation with global threshold optimization. Experiments conducted on the publicly available OAI kneeKL224 dataset demonstrate that VLOrdinalFormer achieves state of the art performance, outperforming CNN and ViT baselines in terms of macro F1 score and overall accuracy. Notably, the proposed framework yields substantial performance gains for KL1 and KL2 without compromising classification accuracy for mild or severe cases. In addition, interpretability analyses using Grad CAM and CLIP similarity maps confirm that the model consistently attends to clinically relevant anatomical regions. These results highlight the potential of vision language aligned ordinal transformers as reliable and interpretable tools for KOA grading and disease progression assessment in routine radiological practice.

</details>


### [141] [VideoCuRL: Video Curriculum Reinforcement Learning with Orthogonal Difficulty Decomposition](https://arxiv.org/abs/2601.00887)
*Hongbo Jin,Kuanwei Lin,Wenhao Zhang,Yichen Jin,Ge Li*

Main category: cs.CV

TL;DR: VideoCuRL introduces a novel method to improve VideoLLMs by decomposing video understanding difficulty into visual complexity and cognitive reasoning depth axes.


<details>
  <summary>Details</summary>
Motivation: Current RL methods fail to address the distinct challenges of visual perception and cognitive reasoning in video understanding.

Method: VideoCuRL employs proxies like optical flow, keyframe entropy for visual complexity, and Calibrated Surprisal for cognitive complexity in a 2D curriculum grid. It uses a Diagonal Wavefront scheduling strategy and techniques like Dynamic Sparse KL for stable training.

Result: VideoCuRL outperforms RL baselines, achieving improvements in reasoning (+2.5 on VSI-Bench) and perception (+2.9 on VideoMME) without significant inference overhead.

Conclusion: VideoCuRL offers a scalable solution for enhancing VideoLLM post-training by overcoming limitations of prior RL approaches and improving video understanding.

Abstract: Reinforcement Learning (RL) is crucial for empowering VideoLLMs with complex spatiotemporal reasoning. However, current RL paradigms predominantly rely on random data shuffling or naive curriculum strategies based on scalar difficulty metrics. We argue that scalar metrics fail to disentangle two orthogonal challenges in video understanding: Visual Temporal Perception Load and Cognitive Reasoning Depth. To address this, we propose VideoCuRL, a novel framework that decomposes difficulty into these two axes. We employ efficient, training-free proxies, optical flow and keyframe entropy for visual complexity, Calibrated Surprisal for cognitive complexity, to map data onto a 2D curriculum grid. A competence aware Diagonal Wavefront strategy then schedules training from base alignment to complex reasoning. Furthermore, we introduce Dynamic Sparse KL and Structured Revisiting to stabilize training against reward collapse and catastrophic forgetting. Extensive experiments show that VideoCuRL surpasses strong RL baselines on reasoning (+2.5 on VSI-Bench) and perception (+2.9 on VideoMME) tasks. Notably, VideoCuRL eliminates the prohibitive inference overhead of generation-based curricula, offering a scalable solution for robust video post-training.

</details>


### [142] [Comparative Evaluation of CNN Architectures for Neural Style Transfer in Indonesian Batik Motif Generation: A Comprehensive Study](https://arxiv.org/abs/2601.00888)
*Happy Gery Pangestu,Andi Prademon Yunus,Siti Khomsah*

Main category: cs.CV

TL;DR: This paper evaluates Neural Style Transfer (NST) using five CNNs for Indonesian batik generation, advocating ResNet backbones for balancing efficiency and structural preservation.


<details>
  <summary>Details</summary>
Motivation: The paper explores improving computational efficiency in Neural Style Transfer for resource-constrained environments.

Method: The paper systematically compares five CNN architectures (VGG16, VGG19, Inception V3, ResNet50, ResNet101) using 245 experiments for quantitative, qualitative, and statistical assessment.

Result: ResNet architectures outperform others, achieving faster convergence (5-6x), similar perceptual similarity, and high efficiency (16x fewer FLOPs).

Conclusion: ResNet backbones are practical for efficient NST applications, balancing scalability and structural fidelity, suitable for batik generation.

Abstract: Neural Style Transfer (NST) provides a computational framework for the digital preservation and generative exploration of Indonesian batik motifs; however, existing approaches remain largely centered on VGG-based architectures whose strong stylistic expressiveness comes at the cost of high computational and memory demands, that limits practical deployment in resource-limited environments. This study presents a systematic comparative analysis of five widely used CNN backbones, namely VGG16, VGG19, Inception V3, ResNet50, and ResNet101, based on 245 controlled experiments combining quantitative metrics, qualitative assessment, and statistical analysis to examine the trade-off between structural preservation, stylistic behavior, and computational efficiency. The results show that backbone selection does not yield statistically significant differences in structural similarity, as confirmed by ANOVA on SSIM (p= 0.83), indicating comparable levels of structural preservation rather than equivalent stylistic quality. Within this context, ResNet-based architectures achieve approximately 5-6x faster convergence than VGG models while maintaining similar perceptual similarity (LPIPS = 0.53) and requiring over 16x fewer FLOPs (0.63 vs 10.12 GFLOPs). Qualitative analysis reveals consistent stylistic trade-offs, with VGG producing denser painterly textures, ResNet favoring geometric stability and canting stroke preservation with milder stylization, and Inception V3 exhibiting intermediate but noisier behavior. These findings reposition architectural choice in NST from maximizing stylistic intensity toward efficiency-aware and structure-preserving deployment, highlighting ResNet-based backbones as a practical foundation for scalable, industry-oriented batik generation.

</details>


### [143] [CornViT: A Multi-Stage Convolutional Vision Transformer Framework for Hierarchical Corn Kernel Analysis](https://arxiv.org/abs/2601.00897)
*Sai Teja Erukude,Jane Mascarenhas,Lior Shamir*

Main category: cs.CV

TL;DR: CornViT introduces a three-stage Convolutional Vision Transformer framework for automated corn kernel grading, yielding high accuracy compared to traditional models, and is deployed via a user-friendly web application.


<details>
  <summary>Details</summary>
Motivation: Corn kernel grading is essential for various agricultural processes; current manual methods are inefficient and prone to errors.

Method: Three-stage CvT models sequentially classify kernel purity, morphology, and embryo orientation, using optimized datasets and pretrained backbones.

Result: CornViT achieves superior test accuracies (93.76%, 94.11%, 91.12%) compared to ResNet-50 and DenseNet-121 models under identical training conditions.

Conclusion: CornViT enables accurate, deployable, and interpretable solutions for corn kernel quality assessment, providing significant advantages over existing methods.

Abstract: Accurate grading of corn kernels is critical for seed certification, directional seeding, and breeding, yet it is still predominantly performed by manual inspection. This work introduces CornViT, a three-stage Convolutional Vision Transformer (CvT) framework that emulates the hierarchical reasoning of human seed analysts for single-kernel evaluation. Three sequential CvT-13 classifiers operate on 384x384 RGB images: Stage 1 distinguishes pure from impure kernels; Stage 2 categorizes pure kernels into flat and round morphologies; and Stage 3 determines the embryo orientation (up vs. down) for pure, flat kernels. Starting from a public corn seed image collection, we manually relabeled and filtered images to construct three stage-specific datasets: 7265 kernels for purity, 3859 pure kernels for morphology, and 1960 pure-flat kernels for embryo orientation, all released as benchmarks. Head-only fine-tuning of ImageNet-22k pretrained CvT-13 backbones yields test accuracies of 93.76% for purity, 94.11% for shape, and 91.12% for embryo-orientation detection. Under identical training conditions, ResNet-50 reaches only 76.56 to 81.02 percent, whereas DenseNet-121 attains 86.56 to 89.38 percent accuracy. These results highlight the advantages of convolution-augmented self-attention for kernel analysis. To facilitate adoption, we deploy CornViT in a Flask-based web application that performs stage-wise inference and exposes interpretable outputs through a browser interface. Together, the CornViT framework, curated datasets, and web application provide a deployable solution for automated corn kernel quality assessment in seed quality workflows. Source code and data are publicly available.

</details>


### [144] [Evaluating Contextual Intelligence in Recyclability: A Comprehensive Study of Image-Based Reasoning Systems](https://arxiv.org/abs/2601.00905)
*Eliot Park,Abhi Kumar,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: This paper explores vision-language models to predict recyclability of items and guide correct disposal, while evaluating their strengths and weaknesses in challenging scenarios.


<details>
  <summary>Details</summary>
Motivation: Improving public recycling accuracy and environmental sustainability by leveraging advanced AI models for recyclability predictions.

Method: The authors employed vision-language models (GPT-4o variants and Claude 3.5) on an image dataset to classify items and match them to appropriate recycling bins, considering factors like location-specific guidelines, contamination, and multi-material composition.

Result: The models demonstrated advancements in contextual understanding for recyclable material disposal, though limitations in specific challenging cases (like contamination or multi-materials) were noted.

Conclusion: Refining context-aware models is essential for improving recycling practices and fostering environmental sustainability.

Abstract: While the importance of efficient recycling is widely acknowledged, accurately determining the recyclability of items and their proper disposal remains a complex task for the general public. In this study, we explore the application of cutting-edge vision-language models (GPT-4o, GPT-4o-mini, and Claude 3.5) for predicting the recyclability of commonly disposed items. Utilizing a curated dataset of images, we evaluated the models' ability to match objects to appropriate recycling bins, including assessing whether the items could physically fit into the available bins. Additionally, we investigated the models' performance across several challenging scenarios: (i) adjusting predictions based on location-specific recycling guidelines; (ii) accounting for contamination or structural damage; and (iii) handling objects composed of multiple materials. Our findings highlight the significant advancements in contextual understanding offered by these models compared to previous iterations, while also identifying areas where they still fall short. The continued refinement of context-aware models is crucial for enhancing public recycling practices and advancing environmental sustainability.

</details>


### [145] [Clean-GS: Semantic Mask-Guided Pruning for 3D Gaussian Splatting](https://arxiv.org/abs/2601.00913)
*Subhankar Mishra*

Main category: cs.CV

TL;DR: The paper presents Clean-GS, a method to reduce background clutter and noise in 3D Gaussian Splatting reconstructions, achieving significant compression without losing quality.


<details>
  <summary>Details</summary>
Motivation: To address the problem of excessive floaters and background clutter in 3D Gaussian Splatting reconstructions, which hinder model deployment in bandwidth-constrained applications and obscure object quality.

Method: Clean-GS uses sparse semantic masks for whitelist-based spatial filtering, color validation, and neighbor-based outlier removal. Semantic information from a minimal number of views helps refine the reconstruction by removing irrelevant Gaussians.

Result: Clean-GS compresses models by 60-80%, reducing file sizes (e.g., from 125MB to 47MB) while maintaining quality, enabling models for web and AR/VR uses.

Conclusion: Clean-GS effectively removes unnecessary floaters and optimizes 3DGS reconstructions for practical applications, overcoming limitations of prior pruning methods via semantic-informed techniques.

Abstract: 3D Gaussian Splatting produces high-quality scene reconstructions but generates hundreds of thousands of spurious Gaussians (floaters) scattered throughout the environment. These artifacts obscure objects of interest and inflate model sizes, hindering deployment in bandwidth-constrained applications. We present Clean-GS, a method for removing background clutter and floaters from 3DGS reconstructions using sparse semantic masks. Our approach combines whitelist-based spatial filtering with color-guided validation and outlier removal to achieve 60-80\% model compression while preserving object quality. Unlike existing 3DGS pruning methods that rely on global importance metrics, Clean-GS uses semantic information from as few as 3 segmentation masks (1\% of views) to identify and remove Gaussians not belonging to the target object. Our multi-stage approach consisting of (1) whitelist filtering via projection to masked regions, (2) depth-buffered color validation, and (3) neighbor-based outlier removal isolates monuments and objects from complex outdoor scenes. Experiments on Tanks and Temples show that Clean-GS reduces file sizes from 125MB to 47MB while maintaining rendering quality, making 3DGS models practical for web deployment and AR/VR applications. Our code is available at https://github.com/smlab-niser/clean-gs

</details>


### [146] [Four-Stage Alzheimer's Disease Classification from MRI Using Topological Feature Extraction, Feature Selection, and Ensemble Learning](https://arxiv.org/abs/2601.00918)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: The paper introduces TDA-Alz, a framework using topological data analysis (TDA) for Alzheimer's disease severity classification with an accuracy of 98.19%.


<details>
  <summary>Details</summary>
Motivation: The challenge is accurate classification of Alzheimer's disease severity from brain MRI when data is limited and interpretability is crucial.

Method: The approach extracts topological descriptors from brain MRI, selecting discriminative features, followed by ensemble learning for classification.

Result: TDA-Alz achieves state-of-the-art classification accuracy (98.19%) and AUC (99.75%) on the OASIS-1 dataset without relying on heavy computational resources or data augmentation.

Conclusion: TDA-Alz is a lightweight, interpretable, and efficient alternative to deep learning models, with potential for clinical decision-support systems.

Abstract: Accurate and efficient classification of Alzheimer's disease (AD) severity from brain magnetic resonance imaging (MRI) remains a critical challenge, particularly when limited data and model interpretability are of concern. In this work, we propose TDA-Alz, a novel framework for four-stage Alzheimer's disease severity classification (non-demented, moderate dementia, mild, and very mild) using topological data analysis (TDA) and ensemble learning. Instead of relying on deep convolutional architectures or extensive data augmentation, our approach extracts topological descriptors that capture intrinsic structural patterns of brain MRI, followed by feature selection to retain the most discriminative topological features. These features are then classified using an ensemble learning strategy to achieve robust multiclass discrimination.
  Experiments conducted on the OASIS-1 MRI dataset demonstrate that the proposed method achieves an accuracy of 98.19% and an AUC of 99.75%, outperforming or matching state-of-the-art deep learning--based methods reported on OASIS and OASIS-derived datasets. Notably, the proposed framework does not require data augmentation, pretrained networks, or large-scale computational resources, making it computationally efficient and fast compared to deep neural network approaches. Furthermore, the use of topological descriptors provides greater interpretability, as the extracted features are directly linked to the underlying structural characteristics of brain MRI rather than opaque latent representations. These results indicate that TDA-Alz offers a powerful, lightweight, and interpretable alternative to deep learning models for MRI-based Alzheimer's disease severity classification, with strong potential for real-world clinical decision-support systems.

</details>


### [147] [Application of deep learning techniques in non-contrast computed tomography pulmonary angiogram for pulmonary embolism diagnosis](https://arxiv.org/abs/2601.00925)
*I-Hsien Ting,Yi-Jun Tseng,Yu-Sheng Lin*

Main category: cs.CV

TL;DR: The study proposes a deep learning-based classification model for pulmonary embolism using non-contrast CT images, achieving high accuracy and demonstrating its feasibility.


<details>
  <summary>Details</summary>
Motivation: Detecting pulmonary embolism with contrast medium CT poses challenges such as risk of acute kidney injury and delayed treatment time. This paper explores non-contrast CT for diagnosis.

Method: A 3D convolutional neural network model was used to classify pulmonary embolism using non-contrast CT images.

Result: The model achieved 85% accuracy and an AUC of 0.84, showcasing its effectiveness in pulmonary embolism classification.

Conclusion: The study confirms the feasibility of using deep learning on non-contrast CT images for pulmonary embolism diagnosis, providing an alternative method without the risks associated with contrast mediums.

Abstract: Pulmonary embolism is a life-threatening disease, early detection and treatment can significantly reduce mortality. In recent years, many studies have been using deep learning in the diagnosis of pulmonary embolism with contrast medium computed tomography pulmonary angiography, but the contrast medium is likely to cause acute kidney injury in patients with pulmonary embolism and chronic kidney disease, and the contrast medium takes time to work, patients with acute pulmonary embolism may miss the golden treatment time.
  This study aims to use deep learning techniques to automatically classify pulmonary embolism in CT images without contrast medium by using a 3D convolutional neural network model. The deep learning model used in this study had a significant impact on the pulmonary embolism classification of computed tomography images without contrast with 85\% accuracy and 0.84 AUC, which confirms the feasibility of the model in the diagnosis of pulmonary embolism.

</details>


### [148] [Analyzing the Shopping Journey: Computing Shelf Browsing Visits in a Physical Retail Store](https://arxiv.org/abs/2601.00928)
*Luis Yoichi Morales,Francesco Zanlungo,David M. Woollard*

Main category: cs.CV

TL;DR: This paper introduces an algorithm to compute customer browsing behavior in physical retail settings using machine vision-based tracking and cameras, enabling autonomous understanding of shopper intent.


<details>
  <summary>Details</summary>
Motivation: To address challenges in deploying robots in customer-facing retail roles, the study aims to enhance understanding of shopper intent and improve retail planning.

Method: Developed and calibrated a shelf visit algorithm using machine vision-based 3D tracking from overhead cameras. Calibration was performed on two distinct trajectory datasets, validated in different stores, and used for further analysis.

Result: The algorithm demonstrated effectiveness in recognizing customer browsing behavior across varying environments, showcasing its generalizability.

Conclusion: The study provides insights into customer browsing patterns and their purchasing behavior, offering potential applications in retail strategy and human-robot interaction scenarios.

Abstract: Motivated by recent challenges in the deployment of robots into customer-facing roles within retail, this work introduces a study of customer activity in physical stores as a step toward autonomous understanding of shopper intent. We introduce an algorithm that computes shoppers' ``shelf visits'' -- capturing their browsing behavior in the store. Shelf visits are extracted from trajectories obtained via machine vision-based 3D tracking and overhead cameras. We perform two independent calibrations of the shelf visit algorithm, using distinct sets of trajectories (consisting of 8138 and 15129 trajectories), collected in different stores and labeled by human reviewers. The calibrated models are then evaluated on trajectories held out of the calibration process both from the same store on which calibration was performed and from the other store. An analysis of the results shows that the algorithm can recognize customers' browsing activity when evaluated in an environment different from the one on which calibration was performed. We then use the model to analyze the customers' ``browsing patterns'' on a large set of trajectories and their relation to actual purchases in the stores. Finally, we discuss how shelf browsing information could be used for retail planning and in the domain of human-robot interaction scenarios.

</details>


### [149] [ShadowGS: Shadow-Aware 3D Gaussian Splatting for Satellite Imagery](https://arxiv.org/abs/2601.00939)
*Feng Luo,Hongbo Pan,Xiang Yang,Baoyu Jiang,Fengqing Liu,Tao Huang*

Main category: cs.CV

TL;DR: The paper introduces ShadowGS, a framework based on 3D Gaussian Splatting for 3D reconstruction from satellite imagery, addressing shadow inconsistencies and achieving improved accuracy and performance.


<details>
  <summary>Details</summary>
Motivation: Current 3D reconstruction methods from multi-temporal satellite images struggle with shadow inconsistencies caused by varying illumination conditions, impacting reconstruction accuracy and rendering quality.

Method: The paper introduces ShadowGS, incorporating physics-based rendering, efficient ray marching, shadow consistency constraints, and a shadow map prior to improve 3D reconstruction accuracy and address challenges caused by shadow inconsistencies.

Result: ShadowGS outperforms state-of-the-art methods in shadow decoupling, 3D reconstruction precision, and novel view synthesis. It achieves these with minimal training time and robustly handles different input settings like RGB, pansharpened, and sparse-view satellite imagery.

Conclusion: ShadowGS effectively resolves shadow-related issues in satellite-based 3D reconstruction, showcasing significant improvements in both geometric accuracy and novel view synthesis quality, making it efficient and versatile.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel paradigm for 3D reconstruction from satellite imagery. However, in multi-temporal satellite images, prevalent shadows exhibit significant inconsistencies due to varying illumination conditions. To address this, we propose ShadowGS, a novel framework based on 3DGS. It leverages a physics-based rendering equation from remote sensing, combined with an efficient ray marching technique, to precisely model geometrically consistent shadows while maintaining efficient rendering. Additionally, it effectively disentangles different illumination components and apparent attributes in the scene. Furthermore, we introduce a shadow consistency constraint that significantly enhances the geometric accuracy of 3D reconstruction. We also incorporate a novel shadow map prior to improve performance with sparse-view inputs. Extensive experiments demonstrate that ShadowGS outperforms current state-of-the-art methods in shadow decoupling accuracy, 3D reconstruction precision, and novel view synthesis quality, with only a few minutes of training. ShadowGS exhibits robust performance across various settings, including RGB, pansharpened, and sparse-view satellite inputs.

</details>


### [150] [Learning to Segment Liquids in Real-world Images](https://arxiv.org/abs/2601.00940)
*Jonas Li,Michelle Li,Luke Liu,Heng Fan*

Main category: cs.CV

TL;DR: The paper addresses the challenge of liquid segmentation by introducing the LQDS dataset and a novel liquid detection model, LQDM, which outperforms other methods.


<details>
  <summary>Details</summary>
Motivation: To enable robots to effectively and safely interact with liquids, given the current lack of focus on liquid segmentation due to their diverse appearances, shapes, and reflective/transparency properties.

Method: The authors developed a large dataset (LQDS) with 5000 real-world images across 14 classes and created LQDM, a segmentation model using cross-attention between boundary and segmentation branches.

Result: LQDM achieves superior performance on the LQDS test set compared to state-of-the-art methods.

Conclusion: The paper establishes a strong baseline for liquid segmentation through its novel dataset and detection model, setting a foundation for robotic manipulation and interaction with liquids.

Abstract: Different types of liquids such as water, wine and medicine appear in all aspects of daily life. However, limited attention has been given to the task, hindering the ability of robots to avoid or interact with liquids safely. The segmentation of liquids is difficult because liquids come in diverse appearances and shapes; moreover, they can be both transparent or reflective, taking on arbitrary objects and scenes from the background or surroundings. To take on this challenge, we construct a large-scale dataset of liquids named LQDS consisting of 5000 real-world images annotated into 14 distinct classes, and design a novel liquid detection model named LQDM, which leverages cross-attention between a dedicated boundary branch and the main segmentation branch to enhance segmentation predictions. Extensive experiments demonstrate the effectiveness of LQDM on the test set of LQDS, outperforming state-of-the-art methods and establishing a strong baseline for the semantic segmentation of liquids.

</details>


### [151] [PhyEduVideo: A Benchmark for Evaluating Text-to-Video Models for Physics Education](https://arxiv.org/abs/2601.00943)
*Megha Mariam K. M,Aditya Arun,Zakaria Laskar,C. V. Jawahar*

Main category: cs.CV

TL;DR: This paper proposes a benchmark for assessing the performance of Text-to-Video (T2V) models in generating educational videos for physics concepts, identifying current strengths and limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation for developing this benchmark is to explore the potential of T2V models in automating the creation of visually engaging and curriculum-aligned physics educational content, aiming for scalable and personalized learning.

Method: The paper introduces a benchmark where physics concepts are broken down into granular teaching points, each paired with prompts for the T2V models to generate explanatory videos. Performance is evaluated based on visual coherence and conceptual accuracy.

Result: The evaluation found that current T2V models create visually coherent videos with smooth motion but struggle with accurately depicting abstract physics concepts, particularly in areas like electromagnetism and thermodynamics.

Conclusion: There is a significant gap between the visual quality and conceptual correctness of videos produced by current T2V models, necessitating further improvements to align them with educational requirements.

Abstract: Generative AI models, particularly Text-to-Video (T2V) systems, offer a promising avenue for transforming science education by automating the creation of engaging and intuitive visual explanations. In this work, we take a first step toward evaluating their potential in physics education by introducing a dedicated benchmark for explanatory video generation. The benchmark is designed to assess how well T2V models can convey core physics concepts through visual illustrations. Each physics concept in our benchmark is decomposed into granular teaching points, with each point accompanied by a carefully crafted prompt intended for visual explanation of the teaching point. T2V models are evaluated on their ability to generate accurate videos in response to these prompts. Our aim is to systematically explore the feasibility of using T2V models to generate high-quality, curriculum-aligned educational content-paving the way toward scalable, accessible, and personalized learning experiences powered by AI. Our evaluation reveals that current models produce visually coherent videos with smooth motion and minimal flickering, yet their conceptual accuracy is less reliable. Performance in areas such as mechanics, fluids, and optics is encouraging, but models struggle with electromagnetism and thermodynamics, where abstract interactions are harder to depict. These findings underscore the gap between visual quality and conceptual correctness in educational video generation. We hope this benchmark helps the community close that gap and move toward T2V systems that can deliver accurate, curriculum-aligned physics content at scale. The benchmark and accompanying codebase are publicly available at https://github.com/meghamariamkm/PhyEduVideo.

</details>


### [152] [Deep Clustering with Associative Memories](https://arxiv.org/abs/2601.00963)
*Bishwajit Saha,Dmitry Krotov,Mohammed J. Zaki,Parikshit Ram*

Main category: cs.CV

TL;DR: This paper introduces DCAM, a novel deep clustering method combining representation learning and clustering using a unified loss function, achieving improved results.


<details>
  <summary>Details</summary>
Motivation: The disconnection between representation learning and clustering due to clustering being a discrete optimization task requiring approximations and regularizations.

Method: Proposes a novel loss function based on energy-based dynamics using Associative Memories to seamlessly integrate representation learning and clustering.

Result: DCAM demonstrates improved clustering performance across different architecture types and data modalities.

Conclusion: DCAM effectively ties together representation learning and clustering, producing superior clustering quality in various scenarios.

Abstract: Deep clustering - joint representation learning and latent space clustering - is a well studied problem especially in computer vision and text processing under the deep learning framework. While the representation learning is generally differentiable, clustering is an inherently discrete optimization task, requiring various approximations and regularizations to fit in a standard differentiable pipeline. This leads to a somewhat disjointed representation learning and clustering. In this work, we propose a novel loss function utilizing energy-based dynamics via Associative Memories to formulate a new deep clustering method, DCAM, which ties together the representation learning and clustering aspects more intricately in a single objective. Our experiments showcase the advantage of DCAM, producing improved clustering quality for various architecture choices (convolutional, residual or fully-connected) and data modalities (images or text).

</details>


### [153] [A Deep Learning Approach for Automated Skin Lesion Diagnosis with Explainable AI](https://arxiv.org/abs/2601.00964)
*Md. Maksudul Haque,Rahnuma Akter,A S M Ahsanul Sarkar Akib,Abdul Hasib*

Main category: cs.CV

TL;DR: The paper proposes a deep learning framework for multi-class skin lesion classification, achieving high accuracy and utilizing explainable AI for transparency.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the crucial need for timely and precise skin cancer diagnosis, as it remains one of the most common and dangerous cancer types worldwide.

Method: It combines advanced techniques like high-quality data balancing, large-scale data augmentation, a hybridized EfficientNetV2-L framework with channel attention, and a three-stage progressive learning approach, supported by explainable AI tools such as Grad-CAM and saliency maps.

Result: The model achieves an accuracy of 91.15%, a macro F1-score of 85.45%, and a micro-average AUC of 99.33%, excelling in seven lesion classifications, particularly melanoma and melanocytic nevi.

Conclusion: The proposed method not only demonstrates high diagnostic performance in multi-class skin lesion classification but also enhances clinical trust through transparent and interpretable AI predictions.

Abstract: Skin cancer is also one of the most common and dangerous types of cancer in the world that requires timely and precise diagnosis. In this paper, a deep-learning architecture of the multi-class skin lesion classification on the HAM10000 dataset will be described. The system suggested combines high-quality data balancing methods, large-scale data augmentation, hybridized EfficientNetV2-L framework with channel attention, and a three-stage progressive learning approach. Moreover, we also use explainable AI (XAI) techniques such as Grad-CAM and saliency maps to come up with intelligible visual representations of model predictions. Our strategy is with a total accuracy of 91.15 per cent, macro F1 of 85.45\% and micro-average AUC of 99.33\%. The model has shown high performance in all the seven lesion classes with specific high performance of melanoma and melanocytic nevi. In addition to enhancing diagnostic transparency, XAI also helps to find out the visual characteristics that cause the classifications, which enhances clinical trustworthiness.

</details>


### [154] [Few-Shot Video Object Segmentation in X-Ray Angiography Using Local Matching and Spatio-Temporal Consistency Loss](https://arxiv.org/abs/2601.00988)
*Lin Xi,Yingliang Ma,Xiahai Zhuang*

Main category: cs.CV

TL;DR: A novel FSVOS model using direction-based, non-parametric sampling for efficient local matching has been proposed, outperforming state-of-the-art segmentation methods, especially for clinical applications, with a new X-ray angiography benchmark dataset (MOSXAV) introduced.


<details>
  <summary>Details</summary>
Motivation: The goal is to create an efficient, flexible, and portable video segmentation model that addresses limitations of existing techniques with high applicability in clinical settings.

Method: The model reorganizes local sampling through direction-based, non-parametric sampling while avoiding standard inefficient implementations or hardware-specific methods. It includes supervised spatio-temporal contrastive learning for feature coherence, without needing retraining.

Result: Extensive experiments demonstrate superior segmentation accuracy and generalization on CADICA, XACV, and MOSXAV datasets compared to state-of-the-art methods.

Conclusion: The flexible and efficient FSVOS model improves video segmentation accuracy and generalization, offering potential for diverse clinical applications, supported by the introduction of the MOSXAV benchmark dataset.

Abstract: We introduce a novel FSVOS model that employs a local matching strategy to restrict the search space to the most relevant neighboring pixels. Rather than relying on inefficient standard im2col-like implementations (e.g., spatial convolutions, depthwise convolutions and feature-shifting mechanisms) or hardware-specific CUDA kernels (e.g., deformable and neighborhood attention), which often suffer from limited portability across non-CUDA devices, we reorganize the local sampling process through a direction-based sampling perspective. Specifically, we implement a non-parametric sampling mechanism that enables dynamically varying sampling regions. This approach provides the flexibility to adapt to diverse spatial structures without the computational costs of parametric layers and the need for model retraining. To further enhance feature coherence across frames, we design a supervised spatio-temporal contrastive learning scheme that enforces consistency in feature representations. In addition, we introduce a publicly available benchmark dataset for multi-object segmentation in X-ray angiography videos (MOSXAV), featuring detailed, manually labeled segmentation ground truth. Extensive experiments on the CADICA, XACV, and MOSXAV datasets show that our proposed FSVOS method outperforms current state-of-the-art video segmentation methods in terms of segmentation accuracy and generalization capability (i.e., seen and unseen categories). This work offers enhanced flexibility and potential for a wide range of clinical applications.

</details>


### [155] [UnrealPose: Leveraging Game Engine Kinematics for Large-Scale Synthetic Human Pose Data](https://arxiv.org/abs/2601.00991)
*Joshua Kawaguchi,Saad Manzur,Emily Gao Wang,Maitreyi Sinha,Bryan Vela,Yunxi Wang,Brandon Vela,Wayne B. Hayes*

Main category: cs.CV

TL;DR: The paper introduces UnrealPose-Gen, an Unreal Engine 5 pipeline to generate realistic 3D human pose data, and the UnrealPose-1M dataset with 1M labeled frames, overcoming limitations of in-studio and in-the-wild datasets.


<details>
  <summary>Details</summary>
Motivation: Generating high-quality, labeled 3D human pose data is challenging due to the limitations of expensive in-studio setups and lack of ground truth in in-the-wild datasets.

Method: The paper utilizes Unreal Engine 5 and Movie Render Queue to create UnrealPose-Gen, a pipeline for generating datasets with labeled 3D joint coordinates, 2D projections, bounding boxes, and camera parameters.

Result: The UnrealPose-1M dataset includes approximately 1 million labeled frames with diverse human poses, actions, and camera views. Real-to-synthetic validation on four tasks demonstrates its utility.

Conclusion: UnrealPose-Gen provides a scalable tool for synthetic human pose data generation, filling gaps in existing datasets and enhancing research opportunities.

Abstract: Diverse, accurately labeled 3D human pose data is expensive and studio-bound, while in-the-wild datasets lack known ground truth. We introduce UnrealPose-Gen, an Unreal Engine 5 pipeline built on Movie Render Queue for high-quality offline rendering. Our generated frames include: (i) 3D joints in world and camera coordinates, (ii) 2D projections and COCO-style keypoints with occlusion and joint-visibility flags, (iii) person bounding boxes, and (iv) camera intrinsics and extrinsics. We use UnrealPose-Gen to present UnrealPose-1M, an approximately one million frame corpus comprising eight sequences: five scripted "coherent" sequences spanning five scenes, approximately 40 actions, and five subjects; and three randomized sequences across three scenes, approximately 100 actions, and five subjects, all captured from diverse camera trajectories for broad viewpoint coverage. As a fidelity check, we report real-to-synthetic results on four tasks: image-to-3D pose, 2D keypoint detection, 2D-to-3D lifting, and person detection/segmentation. Though time and resources constrain us from an unlimited dataset, we release the UnrealPose-1M dataset, as well as the UnrealPose-Gen pipeline to support third-party generation of human pose data.

</details>


### [156] [WildIng: A Wildlife Image Invariant Representation Model for Geographical Domain Shift](https://arxiv.org/abs/2601.00993)
*Julian D. Santamaria,Claudia Isaza,Jhony H. Giraldo*

Main category: cs.CV

TL;DR: Deep learning foundation models struggle with geographical domain shifts in wildlife identification. WildIng integrates text with image features to enhance generalization and achieves a 30% accuracy improvement under such conditions.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of effective generalization in wildlife monitoring models due to geographical domain shifts, aiming to improve performance when identifying species across different regions.

Method: WildIng enhances wildlife identification by integrating text descriptions with image features to mitigate domain shift impacts and improve model robustness.

Result: WildIng improves the accuracy of foundation models like BioCLIP by 30% under geographical domain shifts, as shown in experiments involving datasets from America and Africa.

Conclusion: WildIng successfully addresses geographical domain shifts in wildlife monitoring by combining textual descriptions with image features, significantly boosting cross-regional accuracy.

Abstract: Wildlife monitoring is crucial for studying biodiversity loss and climate change. Camera trap images provide a non-intrusive method for analyzing animal populations and identifying ecological patterns over time. However, manual analysis is time-consuming and resource-intensive. Deep learning, particularly foundation models, has been applied to automate wildlife identification, achieving strong performance when tested on data from the same geographical locations as their training sets. Yet, despite their promise, these models struggle to generalize to new geographical areas, leading to significant performance drops. For example, training an advanced vision-language model, such as CLIP with an adapter, on an African dataset achieves an accuracy of 84.77%. However, this performance drops significantly to 16.17% when the model is tested on an American dataset. This limitation partly arises because existing models rely predominantly on image-based representations, making them sensitive to geographical data distribution shifts, such as variation in background, lighting, and environmental conditions. To address this, we introduce WildIng, a Wildlife image Invariant representation model for geographical domain shift. WildIng integrates text descriptions with image features, creating a more robust representation to geographical domain shifts. By leveraging textual descriptions, our approach captures consistent semantic information, such as detailed descriptions of the appearance of the species, improving generalization across different geographical locations. Experiments show that WildIng enhances the accuracy of foundation models such as BioCLIP by 30% under geographical domain shift conditions. We evaluate WildIng on two datasets collected from different regions, namely America and Africa. The code and models are publicly available at https://github.com/Julian075/CATALOG/tree/WildIng.

</details>


### [157] [DVGBench: Implicit-to-Explicit Visual Grounding Benchmark in UAV Imagery with Large Vision-Language Models](https://arxiv.org/abs/2601.00998)
*Yue Zhou,Jue Chen,Zilun Zhang,Penghui Huang,Ran Ding,Zhentao Zou,PengFei Gao,Yuchen Wei,Ke Li,Xue Yang,Xue Jiang,Hongxin Yang,Jonathan Li*

Main category: cs.CV

TL;DR: The paper introduces DVGBench, an implicit visual grounding benchmark for drones, and proposes the DroneVG-R1 model with a novel reasoning method to improve grounding accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing remote sensing visual grounding models are limited by datasets relying on explicit referring expressions, which are insufficient for tasks requiring implicit scenario-specific knowledge.

Method: The authors developed DVGBench benchmark covering multiple scenarios with both explicit and implicit queries. They also designed DroneVG-R1, leveraging an Implicit-to-Explicit Chain-of-Thought technique within reinforcement learning.

Result: DroneVG-R1 demonstrated its ability to bridge the gap between implicit and explicit visual grounding queries, showcasing improved reasoning capacity compared to mainstream models.

Conclusion: This study provides insights into enhancing reasoning abilities of Large Vision-Language Models for drone applications, highlighting the importance of implicit-to-explicit conversions in visual grounding tasks.

Abstract: Remote sensing (RS) large vision-language models (LVLMs) have shown strong promise across visual grounding (VG) tasks. However, existing RS VG datasets predominantly rely on explicit referring expressions-such as relative position, relative size, and color cues-thereby constraining performance on implicit VG tasks that require scenario-specific domain knowledge. This article introduces DVGBench, a high-quality implicit VG benchmark for drones, covering six major application scenarios: traffic, disaster, security, sport, social activity, and productive activity. Each object provides both explicit and implicit queries. Based on the dataset, we design DroneVG-R1, an LVLM that integrates the novel Implicit-to-Explicit Chain-of-Thought (I2E-CoT) within a reinforcement learning paradigm. This enables the model to take advantage of scene-specific expertise, converting implicit references into explicit ones and thus reducing grounding difficulty. Finally, an evaluation of mainstream models on both explicit and implicit VG tasks reveals substantial limitations in their reasoning capabilities. These findings provide actionable insights for advancing the reasoning capacity of LVLMs for drone-based agents. The code and datasets will be released at https://github.com/zytx121/DVGBench

</details>


### [158] [Lightweight Channel Attention for Efficient CNNs](https://arxiv.org/abs/2601.01002)
*Prem Babu Kanaparthi,Tulasi Venkata Sri Varshini Padamata*

Main category: cs.CV

TL;DR: This paper introduces Lite Channel Attention (LCA) module to optimize channel attention designs in CNNs, demonstrating competitive performance with reduced computational demands.


<details>
  <summary>Details</summary>
Motivation: Attention mechanisms in CNNs improve performance efficiently, yet the trade-offs in their design have been underexplored.

Method: The study compares existing attention mechanisms (SE and ECA) with the newly proposed LCA using adaptive one-dimensional convolutions with grouped operations, evaluating them on CIFAR-10 with ResNet 18 and MobileNetV2.

Result: LCA achieves comparable accuracy to ECA while maintaining parameter efficiency and low inference latency, scoring 94.68% (ResNet 18) and 93.10% accuracy (MobileNetV2).

Conclusion: Lite Channel Attention is a competitive and resource-efficient attention module suitable for deployment in constrained environments, validated through detailed benchmarking metrics.

Abstract: Attention mechanisms have become integral to modern convolutional neural networks (CNNs), delivering notable performance improvements with minimal computational overhead. However, the efficiency accuracy trade off of different channel attention designs remains underexplored. This work presents an empirical study comparing Squeeze and Excitation (SE), Efficient Channel Attention (ECA), and a proposed Lite Channel Attention (LCA) module across ResNet 18 and MobileNetV2 architectures on CIFAR 10. LCA employs adaptive one dimensional convolutions with grouped operations to reduce parameter usage while preserving effective attention behavior. Experimental results show that LCA achieves competitive accuracy, reaching 94.68 percent on ResNet 18 and 93.10 percent on MobileNetV2, while matching ECA in parameter efficiency and maintaining favorable inference latency. Comprehensive benchmarks including FLOPs, parameter counts, and GPU latency measurements are provided, offering practical insights for deploying attention enhanced CNNs in resource constrained environments.

</details>


### [159] [Decoupling Amplitude and Phase Attention in Frequency Domain for RGB-Event based Visual Object Tracking](https://arxiv.org/abs/2601.01022)
*Shiao Wang,Xiao Wang,Haonan Zhao,Jiarui Xu,Bo Jiang,Lin Zhu,Xin Zhao,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: The paper proposes an early-frequency fusion tracking framework that utilizes event cameras' unique features to enhance RGB visual object tracking performance and reduce computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing tracking methods do not fully capitalize on the advantages of event cameras, resulting in missed opportunities for improved performance and high computational cost due to processing irrelevant data.

Method: The framework applies early fusion in the frequency domain using Fast Fourier Transform, enabling selective integration of high-frequency event data with RGB data through amplitude and phase attention. A motion-guided spatial sparsification module filters out low-information regions, reducing computation and enhancing relevancy.

Result: The proposed method outperforms existing approaches in terms of both performance and efficiency across three RGB-Event tracking benchmark datasets: FE108, FELT, and COESOT.

Conclusion: The novel framework effectively leverages event camera properties and frequency domain techniques, demonstrating superior tracking performance, efficiency, and computational reduction compared to traditional methods.

Abstract: Existing RGB-Event visual object tracking approaches primarily rely on conventional feature-level fusion, failing to fully exploit the unique advantages of event cameras. In particular, the high dynamic range and motion-sensitive nature of event cameras are often overlooked, while low-information regions are processed uniformly, leading to unnecessary computational overhead for the backbone network. To address these issues, we propose a novel tracking framework that performs early fusion in the frequency domain, enabling effective aggregation of high-frequency information from the event modality. Specifically, RGB and event modalities are transformed from the spatial domain to the frequency domain via the Fast Fourier Transform, with their amplitude and phase components decoupled. High-frequency event information is selectively fused into RGB modality through amplitude and phase attention, enhancing feature representation while substantially reducing backbone computation. In addition, a motion-guided spatial sparsification module leverages the motion-sensitive nature of event cameras to capture the relationship between target motion cues and spatial probability distribution, filtering out low-information regions and enhancing target-relevant features. Finally, a sparse set of target-relevant features is fed into the backbone network for learning, and the tracking head predicts the final target position. Extensive experiments on three widely used RGB-Event tracking benchmark datasets, including FE108, FELT, and COESOT, demonstrate the high performance and efficiency of our method. The source code of this paper will be released on https://github.com/Event-AHU/OpenEvTracking

</details>


### [160] [ITSELF: Attention Guided Fine-Grained Alignment for Vision-Language Retrieval](https://arxiv.org/abs/2601.01024)
*Tien-Huy Nguyen,Huu-Loc Tran,Thanh Duc Ngo*

Main category: cs.CV

TL;DR: This paper presents ITSELF, a framework for improving text-based person search (TBPS) by leveraging attention mechanisms for implicit local alignment, achieving state-of-the-art results without extra supervision.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in TBPS, such as shortcut learning, spurious correlations, and distortion of intra-modality structure, while aiming to leverage the potential of Vision Language Models for capturing fine-grained relationships between text and images.

Method: The proposed ITSELF framework includes Guided Representation with Attentive Bank (GRAB) to use attention for building high-saliency token banks, Multi-Layer Attention for Robust Selection (MARS) for aggregating attention across layers, and Adaptive Token Scheduler (ATS) to progressively refine detail focus during training.

Result: ITSELF achieves state-of-the-art performance and robust cross-dataset generalization on three TBPS benchmarks, validating its effectiveness without relying on extra prior supervision.

Conclusion: The ITSELF framework demonstrates that leveraging attention for implicit local alignment enables improved fine-grained correspondence learning in TBPS, offering robust and high-performing solutions.

Abstract: Vision Language Models (VLMs) have rapidly advanced and show strong promise for text-based person search (TBPS), a task that requires capturing fine-grained relationships between images and text to distinguish individuals. Previous methods address these challenges through local alignment, yet they are often prone to shortcut learning and spurious correlations, yielding misalignment. Moreover, injecting prior knowledge can distort intra-modality structure. Motivated by our finding that encoder attention surfaces spatially precise evidence from the earliest training epochs, and to alleviate these issues, we introduceITSELF, an attention-guided framework for implicit local alignment. At its core, Guided Representation with Attentive Bank (GRAB) converts the model's own attention into an Attentive Bank of high-saliency tokens and applies local objectives on this bank, learning fine-grained correspondences without extra supervision. To make the selection reliable and non-redundant, we introduce Multi-Layer Attention for Robust Selection (MARS), which aggregates attention across layers and performs diversity-aware top-k selection; and Adaptive Token Scheduler (ATS), which schedules the retention budget from coarse to fine over training, preserving context early while progressively focusing on discriminative details. Extensive experiments on three widely used TBPS benchmarks showstate-of-the-art performance and strong cross-dataset generalization, confirming the effectiveness and robustness of our approach without additional prior supervision. Our project is publicly available at https://trhuuloc.github.io/itself

</details>


### [161] [Enhanced Leukemic Cell Classification Using Attention-Based CNN and Data Augmentation](https://arxiv.org/abs/2601.01026)
*Douglas Costa Braga,Daniel Oliveira Dantas*

Main category: cs.CV

TL;DR: This paper proposes a deep learning pipeline for classifying leukemic cells, achieving 97.89% F1-score and accuracy using an attention-based convolutional neural network on the C-NMC 2019 dataset.


<details>
  <summary>Details</summary>
Motivation: To address challenges such as inter-observer variability and time constraints in diagnosing acute lymphoblastic leukemia (ALL), the paper aims to develop a reproducible and accurate automated classification system.

Method: The system combines an EfficientNetV2-B3 convolutional neural network with Squeeze-and-Excitation mechanisms, employs data augmentation, focal loss for class imbalance, and patient-wise data splitting to ensure robustness.

Result: The approach achieved significant improvements with a 97.89% F1-score and 97.89% accuracy on the test set, outperforming existing approaches by up to 4.67% while using substantially fewer parameters.

Conclusion: The proposed pipeline demonstrates the potential of attention-based deep learning architectures for efficient and interpretable leukemic cell classification, suitable for clinical deployment.

Abstract: We present a reproducible deep learning pipeline for leukemic cell classification, focusing on system architecture, experimental robustness, and software design choices for medical image analysis. Acute lymphoblastic leukemia (ALL) is the most common childhood cancer, requiring expert microscopic diagnosis that suffers from inter-observer variability and time constraints. The proposed system integrates an attention-based convolutional neural network combining EfficientNetV2-B3 with Squeeze-and-Excitation mechanisms for automated ALL cell classification. Our approach employs comprehensive data augmentation, focal loss for class imbalance, and patient-wise data splitting to ensure robust and reproducible evaluation. On the C-NMC 2019 dataset (12,528 original images from 62 patients), the system achieves a 97.89% F1-score and 97.89% accuracy on the test set, with statistical validation through 100-iteration Monte Carlo experiments confirming significant improvements (p < 0.001) over baseline methods. The proposed pipeline outperforms existing approaches by up to 4.67% while using 89% fewer parameters than VGG16 (15.2M vs. 138M). The attention mechanism provides interpretable visualizations of diagnostically relevant cellular features, demonstrating that modern attention-based architectures can improve leukemic cell classification while maintaining computational efficiency suitable for clinical deployment.

</details>


### [162] [Mono3DV: Monocular 3D Object Detection with 3D-Aware Bipartite Matching and Variational Query DeNoising](https://arxiv.org/abs/2601.01036)
*Kiet Dang Vu,Trung Thai Tran,Kien Nguyen Do Trung,Duc Dung Nguyen*

Main category: cs.CV

TL;DR: The paper introduces Mono3DV, a novel transformer-based framework, to overcome key limitations in monocular 3D object detection by integrating 3D attributes into bipartite matching, improving stability with 3D-DeNoising, and overcoming gradient vanishing using Variational Query DeNoising.


<details>
  <summary>Details</summary>
Motivation: Existing DETR-like architectures for monocular 3D object detection face limitations because they exclude 3D attributes in bipartite matching, resulting in suboptimal 3D predictions due to the instability in training.

Method: The paper proposes three innovations: (1) 3D-Aware Bipartite Matching to incorporate 3D geometric information into the matching process, (2) 3D-DeNoising during training to stabilize the integration of 3D attributes, and (3) Variational Query DeNoising to address gradient vanishing issues.

Result: The Mono3DV method achieves state-of-the-art performance on the KITTI benchmark for 3D object detection without relying on external data.

Conclusion: Mono3DV effectively addresses the challenges in monocular 3D object detection by innovatively integrating and stabilizing 3D attributes in the matching process, leading to significant performance improvements.

Abstract: While DETR-like architectures have demonstrated significant potential for monocular 3D object detection, they are often hindered by a critical limitation: the exclusion of 3D attributes from the bipartite matching process. This exclusion arises from the inherent ill-posed nature of 3D estimation from monocular image, which introduces instability during training. Consequently, high-quality 3D predictions can be erroneously suppressed by 2D-only matching criteria, leading to suboptimal results. To address this, we propose Mono3DV, a novel Transformer-based framework. Our approach introduces three key innovations. First, we develop a 3D-Aware Bipartite Matching strategy that directly incorporates 3D geometric information into the matching cost, resolving the misalignment caused by purely 2D criteria. Second, it is important to stabilize the Bipartite Matching to resolve the instability occurring when integrating 3D attributes. Therefore, we propose 3D-DeNoising scheme in the training phase. Finally, recognizing the gradient vanishing issue associated with conventional denoising techniques, we propose a novel Variational Query DeNoising mechanism to overcome this limitation, which significantly enhances model performance. Without leveraging any external data, our method achieves state-of-the-art results on the KITTI 3D object detection benchmark.

</details>


### [163] [Deepfake Detection with Multi-Artifact Subspace Fine-Tuning and Selective Layer Masking](https://arxiv.org/abs/2601.01041)
*Xiang Zhang,Wenliang Weng,Daoyong Fu,Ziqiang Li,Zhangjie Fu*

Main category: cs.CV

TL;DR: The paper introduces a deepfake detection method, MASM, which enhances cross-dataset generalization by decoupling semantic and artifact representations and using selective layer masks to balance model adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods struggle with cross-dataset scenarios due to the variability in forgery artifacts and the challenge of maintaining semantic stability. Current solutions often overfit or rely on global updates, limiting their effectiveness.

Method: The authors propose MASM, involving (1) partitioning pretrained weights into semantic and artifact subspaces via singular value decomposition, (2) using selective layer masks to balance network updates, and (3) imposing orthogonality and spectral constraints to ensure diverse artifact representations and semantic stability.

Result: The MASM approach improves generalization robustness in cross-dataset deepfake detection by effectively modeling diverse forgery artifacts.

Conclusion: MASM provides a targeted solution for cross-dataset deepfake detection by balancing semantic and artifact representation learning, offering improved performance and robustness in complex real-world scenarios.

Abstract: Deepfake detection still faces significant challenges in cross-dataset and real-world complex scenarios. The root cause lies in the high diversity of artifact distributions introduced by different forgery methods, while pretrained models tend to disrupt their original general semantic structures when adapting to new artifacts. Existing approaches usually rely on indiscriminate global parameter updates or introduce additional supervision signals, making it difficult to effectively model diverse forgery artifacts while preserving semantic stability. To address these issues, this paper proposes a deepfake detection method based on Multi-Artifact Subspaces and selective layer masks (MASM), which explicitly decouples semantic representations from artifact representations and constrains the fitting strength of artifact subspaces, thereby improving generalization robustness in cross-dataset scenarios. Specifically, MASM applies singular value decomposition to model weights, partitioning pretrained weights into a stable semantic principal subspace and multiple learnable artifact subspaces. This design enables decoupled modeling of different forgery artifact patterns while preserving the general semantic subspace. On this basis, a selective layer mask strategy is introduced to adaptively regulate the update behavior of corresponding network layers according to the learning state of each artifact subspace, suppressing overfitting to any single forgery characteristic. Furthermore, orthogonality constraints and spectral consistency constraints are imposed to jointly regularize multiple artifact subspaces, guiding them to learn complementary and diverse artifact representations while maintaining a stable overall spectral structure.

</details>


### [164] [Evaluating transfer learning strategies for improving dairy cattle body weight prediction in small farms using depth-image and point-cloud data](https://arxiv.org/abs/2601.01044)
*Jin Wang,Angelo De Castro,Yuxi Zhang,Lucas Basolli Borsatto,Yuechen Guo,Victoria Bastos Primo,Ana Beatriz Montevecchio Bernardino,Gota Morota,Ricardo C Chebel,Haipeng Yu*

Main category: cs.CV

TL;DR: This paper explores the use of transfer learning to predict dairy cattle body weight from depth images and 3D point clouds, demonstrating its effectiveness on small farms and the potential for automated cattle management.


<details>
  <summary>Details</summary>
Motivation: To address the limited understanding of transfer learning's effectiveness and fine-tuning strategies in livestock applications, and to evaluate depth image versus point-cloud modalities for body weight prediction.

Method: Collected data from three farm sizes with differing sample sizes, implemented four models (ConvNeXt, MobileViT for depth images, PointNet, DGCNN for point clouds), and compared transfer learning, joint learning, and single-source learning strategies.

Result: Transfer learning significantly improved body weight prediction on small farms across all models and matched or surpassed joint learning outcomes. No clear performance difference between depth-image and point-cloud-based methods.

Conclusion: Transfer learning is effective for body weight prediction in small farms with limited data, bypassing privacy or logistical cross-farm sharing by only requiring pretrained model weights, making it suitable for practical cattle monitoring.

Abstract: Computer vision provides automated, non-invasive, and scalable tools for monitoring dairy cattle, thereby supporting management, health assessment, and phenotypic data collection. Although transfer learning is commonly used for predicting body weight from images, its effectiveness and optimal fine-tuning strategies remain poorly understood in livestock applications, particularly beyond the use of pretrained ImageNet or COCO weights. In addition, while both depth images and three-dimensional point-cloud data have been explored for body weight prediction, direct comparisons of these two modalities in dairy cattle are limited. Therefore, the objectives of this study were to 1) evaluate whether transfer learning from a large farm enhances body weight prediction on a small farm with limited data, and 2) compare the predictive performance of depth-image- and point-cloud-based approaches under three experimental designs. Top-view depth images and point-cloud data were collected from 1,201, 215, and 58 cows at large, medium, and small dairy farms, respectively. Four deep learning models were evaluated: ConvNeXt and MobileViT for depth images, and PointNet and DGCNN for point clouds. Transfer learning markedly improved body weight prediction on the small farm across all four models, outperforming single-source learning and achieving gains comparable to or greater than joint learning. These results indicate that pretrained representations generalize well across farms with differing imaging conditions and dairy cattle populations. No consistent performance difference was observed between depth-image- and point-cloud-based models. Overall, these findings suggest that transfer learning is well suited for small farm prediction scenarios where cross-farm data sharing is limited by privacy, logistical, or policy constraints, as it requires access only to pretrained model weights rather than raw data.

</details>


### [165] [EgoGrasp: World-Space Hand-Object Interaction Estimation from Egocentric Videos](https://arxiv.org/abs/2601.01050)
*Hongming Fu,Wenjia Wang,Xiaozhen Qiao,Shuo Yang,Zheng Liu,Bo Zhao*

Main category: cs.CV

TL;DR: EgoGrasp is a novel method for reconstructing hand-object interactions in the world-space from egocentric monocular videos, outperforming existing methods under dynamic and occluded environments.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current methods that fail to effectively handle temporal dynamics, global trajectories, or object poses in egocentric videos, which hinders applications in understanding human behavior and virtual reality.

Method: The method employs a multi-stage framework comprising a robust pre-process pipeline with spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and test-time optimization. It is also scalable to multiple objects without requiring templates.

Result: EgoGrasp achieves state-of-the-art performance in reconstructing world-space hand-object interactions, especially in challenging egocentric and dynamic scenarios.

Conclusion: EgoGrasp significantly advances the field of egocentric hand-object interaction reconstruction by overcoming challenges like camera motion and occlusion, providing a robust, scalable, and accurate solution.

Abstract: We propose EgoGrasp, the first method to reconstruct world-space hand-object interactions (W-HOI) from egocentric monocular videos with dynamic cameras in the wild. Accurate W-HOI reconstruction is critical for understanding human behavior and enabling applications in embodied intelligence and virtual reality. However, existing hand-object interactions (HOI) methods are limited to single images or camera coordinates, failing to model temporal dynamics or consistent global trajectories. Some recent approaches attempt world-space hand estimation but overlook object poses and HOI constraints. Their performance also suffers under severe camera motion and frequent occlusions common in egocentric in-the-wild videos. To address these challenges, we introduce a multi-stage framework with a robust pre-process pipeline built on newly developed spatial intelligence models, a whole-body HOI prior model based on decoupled diffusion models, and a multi-objective test-time optimization paradigm. Our HOI prior model is template-free and scalable to multiple objects. In experiments, we prove our method achieving state-of-the-art performance in W-HOI reconstruction.

</details>


### [166] [Enhancing Histopathological Image Classification via Integrated HOG and Deep Features with Robust Noise Performance](https://arxiv.org/abs/2601.01056)
*Ifeanyi Ezuma,Ugochukwu Ugwu*

Main category: cs.CV

TL;DR: The study explores the classification of histopathological images using machine learning and deep learning, achieving high accuracy and robust performance with fine-tuned InceptionResNet-v2 and its features.


<details>
  <summary>Details</summary>
Motivation: The digital transformation of pathology has necessitated reliable automated image analysis to support clinical practices.

Method: Fine-tuned InceptionResNet-v2 was applied both as a classifier and feature extractor for model evaluation on the LC25000 dataset.

Result: The InceptionResNet-v2 achieved 96.01% accuracy and an AUC of 96.8%. Deep features improved performance, with models like Neural Network reaching 99.84% accuracy and 99.99% AUC. Robustness under varying noise levels highlighted deep features' superiority.

Conclusion: Fine-tuned InceptionResNet-v2 and deep feature-based models are highly effective and robust for classifying histopathological images, particularly under noisy conditions.

Abstract: The era of digital pathology has advanced histopathological examinations, making automated image analysis essential in clinical practice. This study evaluates the classification performance of machine learning and deep learning models on the LC25000 dataset, which includes five classes of histopathological images. We used the fine-tuned InceptionResNet-v2 network both as a classifier and for feature extraction. Our results show that the fine-tuned InceptionResNet-v2 achieved a classification accuracy of 96.01\% and an average AUC of 96.8\%. Models trained on deep features from InceptionResNet-v2 outperformed those using only the pre-trained network, with the Neural Network model achieving an AUC of 99.99\% and accuracy of 99.84\%. Evaluating model robustness under varying SNR conditions revealed that models using deep features exhibited greater resilience, particularly GBM and KNN. The combination of HOG and deep features showed enhanced performance, however, less so in noisy environments.

</details>


### [167] [Efficient Hyperspectral Image Reconstruction Using Lightweight Separate Spectral Transformers](https://arxiv.org/abs/2601.01064)
*Jianan Li,Wangcai Zhao,Tingfa Xu*

Main category: cs.CV

TL;DR: The paper presents Lightweight Separate Spectral Transformer (LSST), a novel architecture for efficient hyperspectral image reconstruction using spectral and spatial modeling with advanced loss mechanisms.


<details>
  <summary>Details</summary>
Motivation: To address challenges in efficiently reconstructing hyperspectral images from compressive sensing measurements by leveraging unique spatial and spectral properties.

Method: The study proposes LSST, which features Separate Spectral Transformer Blocks (SSTB) for spectral relationships, Lightweight Spatial Convolution Blocks (LSCB) for spatial processing, and Focal Spectrum Loss for dynamic loss weighting.

Result: Testing shows LSST offers superior performance in hyperspectral image reconstruction with fewer parameters and operations, proving its efficiency and effectiveness.

Conclusion: LSST provides an innovative and efficient solution to hyperspectral image reconstruction challenges, integrating advanced spectral-spatial modeling and adaptive training mechanisms.

Abstract: Hyperspectral imaging (HSI) is essential across various disciplines for its capacity to capture rich spectral information. However, efficiently reconstructing hyperspectral images from compressive sensing measurements presents significant challenges. To tackle these, we adopt a divide-and-conquer strategy that capitalizes on the unique spectral and spatial characteristics of hyperspectral images. We introduce the Lightweight Separate Spectral Transformer (LSST), an innovative architecture tailored for efficient hyperspectral image reconstruction. This architecture consists of Separate Spectral Transformer Blocks (SSTB) for modeling spectral relationships and Lightweight Spatial Convolution Blocks (LSCB) for spatial processing. The SSTB employs Grouped Spectral Self-attention and a Spectrum Shuffle operation to effectively manage both local and non-local spectral relationships. Simultaneously, the LSCB utilizes depth-wise separable convolutions and strategic ordering to enhance spatial information processing. Furthermore, we implement the Focal Spectrum Loss, a novel loss weighting mechanism that dynamically adjusts during training to improve reconstruction across spectrally complex bands. Extensive testing demonstrates that our LSST achieves superior performance while requiring fewer FLOPs and parameters, underscoring its efficiency and effectiveness. The source code is available at: https://github.com/wcz1124/LSST.

</details>


### [168] [A UAV-Based Multispectral and RGB Dataset for Multi-Stage Paddy Crop Monitoring in Indian Agricultural Fields](https://arxiv.org/abs/2601.01084)
*Adari Rama Sukanya,Puvvula Roopesh Naga Sri Sai,Kota Moses,Rimalapudi Sarvendranath*

Main category: cs.CV

TL;DR: This paper introduces a UAV-based high-resolution dataset of RGB and multispectral images capturing paddy growth stages in India, along with metadata and processed outputs for agricultural research.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive and high-resolution dataset for agricultural studies including disease analysis, targeted spraying, and yield estimation in paddy farming.

Method: The authors collected UAV-based RGB and multispectral images using standard procedures and checklists across 5 acres of paddy fields, validated with image processing software (Pix4D Fields) for generating vegetation maps.

Result: They produced a dataset of 42,430 raw images (415 GB) with 1 cm/pixel GSD, along with GPS and environmental metadata, covering all growth stages of paddy crops. Outputs include orthomosaic and vegetation index maps.

Conclusion: This dataset offers researchers a valuable resource for precision agriculture tasks related to paddy crops, supporting various applications such as spraying and yield estimation.

Abstract: We present a large-scale unmanned aerial vehicle (UAV)-based RGB and multispectral image dataset collected over paddy fields in the Vijayawada region, Andhra Pradesh, India, covering nursery to harvesting stages. We used a 20-megapixel RGB camera and a 5-megapixel four-band multispectral camera capturing red, green, red-edge, and near-infrared bands. Standardised operating procedure (SOP) and checklists were developed to ensure repeatable data acquisition. Our dataset comprises of 42,430 raw images (415 GB) captured over 5 acres with 1 cm/pixel ground sampling distance (GSD) with associated metadata such as GPS coordinates, flight altitude, and environmental conditions. Captured images were validated using Pix4D Fields to generate orthomosaic maps and vegetation index maps, such as normalised difference vegetation index (NDVI) and normalised difference red-edge (NDRE) index. Our dataset is one of the few datasets that provide high-resolution images with rich metadata that cover all growth stages of Indian paddy crops. The dataset is available on IEEE DataPort with DOI, . It can support studies on targeted spraying, disease analysis, and yield estimation.

</details>


### [169] [Luminark: Training-free, Probabilistically-Certified Watermarking for General Vision Generative Models](https://arxiv.org/abs/2601.01085)
*Jiayi Xu,Zhang Zhang,Yuanrui Zhang,Ruitao Chen,Yixian Xu,Tianyu He,Di He*

Main category: cs.CV

TL;DR: Luminark is a training-free, probabilistically-certified watermarking method designed for vision generative models, ensuring high detection accuracy and robust watermarking.


<details>
  <summary>Details</summary>
Motivation: Address the need for a general and effective watermarking method for vision generative models that provides certified security while maintaining image quality.

Method: The method defines a watermark using binary patterns based on patch-level luminance statistics and integrates it via guidance techniques, enabling generality across generative models.

Result: Evaluations across nine generative models showed high detection accuracy, robustness against image transformations, and maintained visual quality.

Conclusion: Luminark presents a simple, effective, and general watermarking approach for generative models, ensuring certified detection and robust performance.

Abstract: In this paper, we introduce \emph{Luminark}, a training-free and probabilistically-certified watermarking method for general vision generative models. Our approach is built upon a novel watermark definition that leverages patch-level luminance statistics. Specifically, the service provider predefines a binary pattern together with corresponding patch-level thresholds. To detect a watermark in a given image, we evaluate whether the luminance of each patch surpasses its threshold and then verify whether the resulting binary pattern aligns with the target one. A simple statistical analysis demonstrates that the false positive rate of the proposed method can be effectively controlled, thereby ensuring certified detection. To enable seamless watermark injection across different paradigms, we leverage the widely adopted guidance technique as a plug-and-play mechanism and develop the \emph{watermark guidance}. This design enables Luminark to achieve generality across state-of-the-art generative models without compromising image quality. Empirically, we evaluate our approach on nine models spanning diffusion, autoregressive, and hybrid frameworks. Across all evaluations, Luminark consistently demonstrates high detection accuracy, strong robustness against common image transformations, and good performance on visual quality.

</details>


### [170] [600k-ks-ocr: a large-scale synthetic dataset for optical character recognition in kashmiri script](https://arxiv.org/abs/2601.01088)
*Haq Nawaz Malik*

Main category: cs.CV

TL;DR: The paper introduces the 600K-KS-OCR Dataset, a synthetic resource designed for Kashmiri script OCR training and evaluation.


<details>
  <summary>Details</summary>
Motivation: To address the lack of resources for developing OCR systems for the endangered Kashmiri language, which uses a modified Perso-Arabic writing system.

Method: The dataset consists of 602,000 word-level Kashmiri script images, rendered in three typefaces with data augmentation. It includes ground-truth transcriptions compatible with various OCR systems and is partitioned into 10 archives.

Result: Provides a robust Kashmiri OCR training dataset (10.6GB) released under CC-BY-4.0 to stimulate research in low-resource language OCR.

Conclusion: The dataset fills a critical gap, offering a practical tool for advancing OCR technology for the Kashmiri language.

Abstract: This technical report presents the 600K-KS-OCR Dataset, a large-scale synthetic corpus comprising approximately 602,000 word-level segmented images designed for training and evaluating optical character recognition systems targeting Kashmiri script. The dataset addresses a critical resource gap for Kashmiri, an endangered Dardic language utilizing a modified Perso-Arabic writing system spoken by approximately seven million people. Each image is rendered at 256x64 pixels with corresponding ground-truth transcriptions provided in multiple formats compatible with CRNN, TrOCR, and generalpurpose machine learning pipelines. The generation methodology incorporates three traditional Kashmiri typefaces, comprehensive data augmentation simulating real-world document degradation, and diverse background textures to enhance model robustness. The dataset is distributed across ten partitioned archives totaling approximately 10.6 GB and is released under the CC-BY-4.0 license to facilitate research in low-resource language optical character recognition.

</details>


### [171] [Real-Time LiDAR Point Cloud Densification for Low-Latency Spatial Data Transmission](https://arxiv.org/abs/2601.01210)
*Kazuhiko Murasaki,Shunsuke Konagai,Masakatsu Aoki,Taiga Yoshida,Ryuichi Tanida*

Main category: cs.CV

TL;DR: This paper presents a real-time LiDAR point cloud densification method for dense 3D scene generation in immersive telepresence.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the challenges of capturing and processing dynamic 3D scenes in real-time to support low-latency immersive telepresence.

Method: The method involves combining multiple LiDAR inputs with high-resolution color images using joint bilateral filtering in a convolutional neural network architecture.

Result: The proposed method generates dense depth maps at full HD resolution in real time (30 fps), achieving a performance 15x faster than a recent training-based method while avoiding multiview inconsistencies or ghosting artifacts.

Conclusion: The approach effectively addresses the requirements of real-time depth completion, enabling minimal latency dense 3D scene generation for immersive telepresence applications.

Abstract: To realize low-latency spatial transmission system for immersive telepresence, there are two major problems: capturing dynamic 3D scene densely and processing them in real time. LiDAR sensors capture 3D in real time, but produce sparce point clouds. Therefore, this paper presents a high-speed LiDAR point cloud densification method to generate dense 3D scene with minimal latency, addressing the need for on-the-fly depth completion while maintaining real-time performance. Our approach combines multiple LiDAR inputs with high-resolution color images and applies a joint bilateral filtering strategy implemented through a convolutional neural network architecture. Experiments demonstrate that the proposed method produces dense depth maps at full HD resolution in real time (30 fps), which is over 15x faster than a recent training-based depth completion approach. The resulting dense point clouds exhibit accurate geometry without multiview inconsistencies or ghosting artifacts.

</details>


### [172] [NarrativeTrack: Evaluating Video Language Models Beyond the Frame](https://arxiv.org/abs/2601.01095)
*Hyeonjeong Ha,Jinjin Ge,Bo Feng,Kaixin Ma,Gargi Chakraborty*

Main category: cs.CV

TL;DR: This paper introduces NarrativeTrack, a benchmark to evaluate multimodal large language models (MLLMs) on narrative understanding in videos, focusing on entity-centric reasoning and temporal coherence.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs perform well in vision-language reasoning but lack capabilities for understanding narratives in videos, requiring the integration of dynamic temporal and visual contexts.

Method: The paper presents NarrativeTrack, which assesses MLLMs through a Compositional Reasoning Progression (CRP) framework. CRP evaluates narrative understanding across entity persistence, changes, and ambiguity in videos.

Result: State-of-the-art MLLMs show weak temporal coherence and challenges in entity tracking, with trade-offs between perceptual grounding and temporal reasoning.

Conclusion: Narrative understanding in MLLMs requires integrating perceptual grounding and temporal reasoning, which is systematically diagnosed through the NarrativeTrack framework.

Abstract: Multimodal large language models (MLLMs) have achieved impressive progress in vision-language reasoning, yet their ability to understand temporally unfolding narratives in videos remains underexplored. True narrative understanding requires grounding who is doing what, when, and where, maintaining coherent entity representations across dynamic visual and temporal contexts. We introduce NarrativeTrack, the first benchmark to evaluate narrative understanding in MLLMs through fine-grained entity-centric reasoning. Unlike existing benchmarks limited to short clips or coarse scene-level semantics, we decompose videos into constituent entities and examine their continuity via a Compositional Reasoning Progression (CRP), a structured evaluation framework that progressively increases narrative complexity across three dimensions: entity existence, entity changes, and entity ambiguity. CRP challenges models to advance from temporal persistence to contextual evolution and fine-grained perceptual reasoning. A fully automated entity-centric pipeline enables scalable extraction of temporally grounded entity representations, providing the foundation for CRP. Evaluations of state-of-the-art MLLMs reveal that models fail to robustly track entities across visual transitions and temporal dynamics, often hallucinating identity under context shifts. Open-source general-purpose MLLMs exhibit strong perceptual grounding but weak temporal coherence, while video-specific MLLMs capture temporal context yet hallucinate entity's contexts. These findings uncover a fundamental trade-off between perceptual grounding and temporal reasoning, indicating that narrative understanding emerges only from their integration. NarrativeTrack provides the first systematic framework to diagnose and advance temporally grounded narrative comprehension in MLLMs.

</details>


### [173] [Evolving CNN Architectures: From Custom Designs to Deep Residual Models for Diverse Image Classification and Detection Tasks](https://arxiv.org/abs/2601.01099)
*Mahmudul Hasan,Mabsur Fatin Bin Hossain*

Main category: cs.CV

TL;DR: A comparative study of custom CNN architectures versus pretrained and transfer learning CNN models for image classification and object detection tasks, offering insights and practical guidance on network selection.


<details>
  <summary>Details</summary>
Motivation: To analyze how network design influences performance across diverse classification and detection tasks and provide practical guidance for selecting suitable CNN architectures.

Method: Comparison of custom CNN architectures with pretrained and transfer learning models across binary classification, multiclass recognition, and object detection tasks.

Result: Deeper custom CNNs outperform on complex multiclass tasks; lightweight pretrained models excel in simpler binary tasks. Adaptation to detect auto-rickshaws demonstrates versatility.

Conclusion: This study helps in task-specific CNN architecture selection, emphasizing complexity and resource constraints, and highlights broader adaptability for practical applications.

Abstract: This paper presents a comparative study of a custom convolutional neural network (CNN) architecture against widely used pretrained and transfer learning CNN models across five real-world image datasets. The datasets span binary classification, fine-grained multiclass recognition, and object detection scenarios. We analyze how architectural factors, such as network depth, residual connections, and feature extraction strategies, influence classification and localization performance. The results show that deeper CNN architectures provide substantial performance gains on fine-grained multiclass datasets, while lightweight pretrained and transfer learning models remain highly effective for simpler binary classification tasks. Additionally, we extend the proposed architecture to an object detection setting, demonstrating its adaptability in identifying unauthorized auto-rickshaws in real-world traffic scenes. Building upon a systematic analysis of custom CNN architectures alongside pretrained and transfer learning models, this study provides practical guidance for selecting suitable network designs based on task complexity and resource constraints.

</details>


### [174] [Histogram Assisted Quality Aware Generative Model for Resolution Invariant NIR Image Colorization](https://arxiv.org/abs/2601.01103)
*Abhinav Attri,Rajeev Ranjan Dwivedi,Samiran Das,Vinod Kumar Kurmi*

Main category: cs.CV

TL;DR: Introduction of HAQAGen, a generative model for NIR-to-RGB image translation using adaptive methods enhancing texture, color consistency, and resolution scalability.


<details>
  <summary>Details</summary>
Motivation: To address the need for realistic and texture-preserving resolution-invariant NIR-to-RGB colorization across diverse imaging scenarios.

Method: Implementation of a model with combined loss metrics for color and textural fidelity, structured priors via SPADE, and texture-aware supervision using a Mamba backbone. Adaptive-resolution inference ensures scalability.

Result: Improved texture sharpness and natural color reproduction with enhanced metrics compared to baseline methods, verified through evaluations on diverse datasets.

Conclusion: HAQAGen is a reliable, scalable solution for high-quality NIR-to-RGB image translation balancing realism and texture fidelity.

Abstract: We present HAQAGen, a unified generative model for resolution-invariant NIR-to-RGB colorization that balances chromatic realism with structural fidelity. The proposed model introduces (i) a combined loss term aligning the global color statistics through differentiable histogram matching, perceptual image quality measure, and feature based similarity to preserve texture information, (ii) local hue-saturation priors injected via Spatially Adaptive Denormalization (SPADE) to stabilize chromatic reconstruction, and (iii) texture-aware supervision within a Mamba backbone to preserve fine details. We introduce an adaptive-resolution inference engine that further enables high-resolution translation without sacrificing quality. Our proposed NIR-to-RGB translation model simultaneously enforces global color statistics and local chromatic consistency, while scaling to native resolutions without compromising texture fidelity or generalization. Extensive evaluations on FANVID, OMSIV, VCIP2020, and RGB2NIR using different evaluation metrics demonstrate consistent improvements over state-of-the-art baseline methods. HAQAGen produces images with sharper textures, natural colors, attaining significant gains as per perceptual metrics. These results position HAQAGen as a scalable and effective solution for NIR-to-RGB translation across diverse imaging scenarios. Project Page: https://rajeev-dw9.github.io/HAQAGen/

</details>


### [175] [Cross-Layer Attentive Feature Upsampling for Low-latency Semantic Segmentation](https://arxiv.org/abs/2601.01167)
*Tianheng Cheng,Xinggang Wang,Junchao Liao,Wenyu Liu*

Main category: cs.CV

TL;DR: The paper proposes a novel Guided Attentive Interpolation (GAI) method for improving high-resolution features with rich semantics for semantic segmentation, addressing misalignment and latency issues.


<details>
  <summary>Details</summary>
Motivation: To address the coarse segmentation results and computational challenges in current low-resolution feature interpolation methods.

Method: The paper introduces the Guided Attentive Interpolation (GAI) that adaptively interpolates high-resolution features by computing spatial and semantic pixel relations between multi-resolution features.

Result: GAI-based networks achieved state-of-the-art results (78.8 mIoU on Cityscapes and 80.6 mIoU on CamVid) with low processing latency using a standard GPU.

Conclusion: GAI effectively optimizes feature alignment and enriches semantics in high-resolution feature maps, enabling efficient and state-of-the-art semantic segmentation with low latency.

Abstract: Semantic segmentation is a fundamental problem in computer vision and it requires high-resolution feature maps for dense prediction. Current coordinate-guided low-resolution feature interpolation methods, e.g., bilinear interpolation, produce coarse high-resolution features which suffer from feature misalignment and insufficient context information. Moreover, enriching semantics to high-resolution features requires a high computation burden, so that it is challenging to meet the requirement of lowlatency inference. We propose a novel Guided Attentive Interpolation (GAI) method to adaptively interpolate fine-grained high-resolution features with semantic features to tackle these issues. Guided Attentive Interpolation determines both spatial and semantic relations of pixels from features of different resolutions and then leverages these relations to interpolate high-resolution features with rich semantics. GAI can be integrated with any deep convolutional network for efficient semantic segmentation. In experiments, the GAI-based semantic segmentation networks, i.e., GAIN, can achieve78.8 mIoU with 22.3 FPS on Cityscapes and 80.6 mIoU with 64.5 on CamVid using an NVIDIA 1080Ti GPU, which are the new state-of-the-art results of low-latency semantic segmentation. Code and models are available at: https://github.com/hustvl/simpleseg.

</details>


### [176] [CardioMOD-Net: A Modal Decomposition-Neural Network Framework for Diagnosis and Prognosis of HFpEF from Echocardiography Cine Loops](https://arxiv.org/abs/2601.01176)
*Andrés Bell-Navas,Jesús Garicano-Mena,Antonella Ausiello,Soledad Le Clainche,María Villalba-Orero,Enrique Lara-Pezzi*

Main category: cs.CV

TL;DR: CardioMOD-Net uses mouse echocardiography videos to predict HFpEF onset and provide multiclass diagnosis, achieving moderate accuracy and aligning predicted and actual HFpEF distributions.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty in early HFpEF diagnosis and prognosis, especially considering the diverse comorbidities and prolonged subclinical stages, and to improve on existing binary detection methods by integrating multiclass phenotyping and disease progression predictions.

Method: Mouse echocardiography videos from control and comorbidity groups (hyperglycaemia, obesity, hypertension) were analyzed using Higher Order Dynamic Mode Decomposition to extract temporal features. Vision Transformers supported classification and regression for multiclass diagnosis and HFpEF progression prediction.

Result: The diagnostic accuracy across groups was 65%, with a forecasting error of 21.72 weeks for HFpEF onset. Predictions closely matched true distributions, especially in obesity and hypertension models.

Conclusion: CardioMOD-Net proves that a single echocardiography cine loop can address both multiclass HFpEF diagnosis and disease onset prediction, providing a step forward in preclinical HFpEF research under constrained data availability.

Abstract: Introduction: Heart failure with preserved ejection fraction (HFpEF) arises from diverse comorbidities and progresses through prolonged subclinical stages, making early diagnosis and prognosis difficult. Current echocardiography-based Artificial Intelligence (AI) models focus primarily on binary HFpEF detection in humans and do not provide comorbidity-specific phenotyping or temporal estimates of disease progression towards decompensation. We aimed to develop a unified AI framework, CardioMOD-Net, to perform multiclass diagnosis and continuous prediction of HFpEF onset directly from standard echocardiography cine loops in preclinical models.
  Methods: Mouse echocardiography videos from four groups were used: control (CTL), hyperglycaemic (HG), obesity (OB), and systemic arterial hypertension (SAH). Two-dimensional parasternal long-axis cine loops were decomposed using Higher Order Dynamic Mode Decomposition (HODMD) to extract temporal features for downstream analysis. A shared latent representation supported Vision Transformers, one for a classifier for diagnosis and another for a regression module for predicting the age at HFpEF onset.
  Results: Overall diagnostic accuracy across the four groups was 65%, with all classes exceeding 50% accuracy. Misclassifications primarily reflected early-stage overlap between OB or SAH and CTL. The prognostic module achieved a root-mean-square error of 21.72 weeks for time-to-HFpEF prediction, with OB and SAH showing the most accurate estimates. Predicted HFpEF onset closely matched true distributions in all groups.
  Discussion: This unified framework demonstrates that multiclass phenotyping and continuous HFpEF onset prediction can be obtained from a single cine loop, even under small-data conditions. The approach offers a foundation for integrating diagnostic and prognostic modelling in preclinical HFpEF research.

</details>


### [177] [GenCAMO: Scene-Graph Contextual Decoupling for Environment-aware and Mask-free Camouflage Image-Dense Annotation Generation](https://arxiv.org/abs/2601.01181)
*Chenglizhao Chen,Shaojiang Yuan,Xiaoxue Lu,Mengke Song,Jia Song,Zhenyu Wu,Wenfeng Song,Shuai Li*

Main category: cs.CV

TL;DR: This paper introduces a generative approach to address the scarcity of dense annotated data for camouflage detection, providing a novel dataset and framework.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of high-quality, large-scale datasets needed for robust camouflage dense prediction, which remains expensive and challenging to create.

Method: Proposes a generative framework, GenCAMO, to produce synthetic, high-fidelity camouflage data with dense annotations and introduces a new dataset, GenCAMO-DB, containing multimodal annotations.

Result: The generated dataset and framework significantly enhance the performance of dense prediction models on challenging camouflage tasks.

Conclusion: The proposed method and dataset, by leveraging generative models, address key challenges in camouflage detection and open opportunities for future advancements in dense prediction tasks.

Abstract: Conceal dense prediction (CDP), especially RGB-D camouflage object detection and open-vocabulary camouflage object segmentation, plays a crucial role in advancing the understanding and reasoning of complex camouflage scenes. However, high-quality and large-scale camouflage datasets with dense annotation remain scarce due to expensive data collection and labeling costs. To address this challenge, we explore leveraging generative models to synthesize realistic camouflage image-dense data for training CDP models with fine-grained representations, prior knowledge, and auxiliary reasoning. Concretely, our contributions are threefold: (i) we introduce GenCAMO-DB, a large-scale camouflage dataset with multi-modal annotations, including depth maps, scene graphs, attribute descriptions, and text prompts; (ii) we present GenCAMO, an environment-aware and mask-free generative framework that produces high-fidelity camouflage image-dense annotations; (iii) extensive experiments across multiple modalities demonstrate that GenCAMO significantly improves dense prediction performance on complex camouflage scenes by providing high-quality synthetic data. The code and datasets will be released after paper acceptance.

</details>


### [178] [DrivingGen: A Comprehensive Benchmark for Generative Video World Models in Autonomous Driving](https://arxiv.org/abs/2601.01528)
*Yang Zhou,Hao Shao,Letian Wang,Zhuofan Zong,Hongsheng Li,Steven L. Waslander*

Main category: cs.CV

TL;DR: The paper introduces DrivingGen, a benchmark that evaluates generative driving world models using diverse datasets and metrics for visual quality, trajectory accuracy, temporal consistency, and controllability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of rigorous benchmarks for generative driving world models and overcome limitations in existing evaluations and datasets.

Method: DrivingGen combines diverse datasets from driving and internet-scale video sources, alongside new metrics for visual realism, trajectory plausibility, temporal coherence, and ego-conditioning controllability.

Result: A comprehensive evaluation framework is created, revealing trade-offs between general and driving-specific generative models concerning physics and visual quality.

Conclusion: DrivingGen establishes a unified benchmark for promoting the development of scalable, reliable, and controllable driving world models, aiding simulations and decision-making processes.

Abstract: Video generation models, as one form of world models, have emerged as one of the most exciting frontiers in AI, promising agents the ability to imagine the future by modeling the temporal evolution of complex scenes. In autonomous driving, this vision gives rise to driving world models: generative simulators that imagine ego and agent futures, enabling scalable simulation, safe testing of corner cases, and rich synthetic data generation. Yet, despite fast-growing research activity, the field lacks a rigorous benchmark to measure progress and guide priorities. Existing evaluations remain limited: generic video metrics overlook safety-critical imaging factors; trajectory plausibility is rarely quantified; temporal and agent-level consistency is neglected; and controllability with respect to ego conditioning is ignored. Moreover, current datasets fail to cover the diversity of conditions required for real-world deployment. To address these gaps, we present DrivingGen, the first comprehensive benchmark for generative driving world models. DrivingGen combines a diverse evaluation dataset curated from both driving datasets and internet-scale video sources, spanning varied weather, time of day, geographic regions, and complex maneuvers, with a suite of new metrics that jointly assess visual realism, trajectory plausibility, temporal coherence, and controllability. Benchmarking 14 state-of-the-art models reveals clear trade-offs: general models look better but break physics, while driving-specific ones capture motion realistically but lag in visual quality. DrivingGen offers a unified evaluation framework to foster reliable, controllable, and deployable driving world models, enabling scalable simulation, planning, and data-driven decision-making.

</details>


### [179] [Crowded Video Individual Counting Informed by Social Grouping and Spatial-Temporal Displacement Priors](https://arxiv.org/abs/2601.01192)
*Hao Lu,Xuhui Zhu,Wenjing Zhang,Yanan Li,Xiang Bai*

Main category: cs.CV

TL;DR: This paper proposes improvements to Video Individual Counting (VIC) by introducing a dataset named WuhanMetroCrowd and a new VIC baseline, OMAN++, which achieves state-of-the-art performance, particularly in crowded scenes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in existing VIC methods, which perform poorly in congested areas such as metro stations, by introducing a suitable dataset and algorithmic innovations targeting such scenarios.

Method: The methodology includes constructing the WuhanMetroCrowd dataset and designing a new VIC model, OMAN++. The model employs two priors: the social grouping prior to allow one-to-many matching and the spatial-temporal displacement prior to improve matching, feature extraction, and training.

Result: OMAN++ achieves state-of-the-art accuracy on multiple VIC benchmarks, including a significant 38.12% error reduction on the WuhanMetroCrowd dataset.

Conclusion: By rethinking VIC with relevant priors and introducing dedicated datasets for congested scenarios, the paper advances the capabilities of VIC methods and sets a strong baseline for future research.

Abstract: Video Individual Counting (VIC) is a recently introduced task aiming to estimate pedestrian flux from a video. It extends Video Crowd Counting (VCC) beyond the per-frame pedestrian count. In contrast to VCC that learns to count pedestrians across frames, VIC must identify co-existent pedestrians between frames, which turns out to be a correspondence problem. Existing VIC approaches, however, can underperform in congested scenes such as metro commuting. To address this, we build WuhanMetroCrowd, one of the first VIC datasets that characterize crowded, dynamic pedestrian flows. It features sparse-to-dense density levels, short-to-long video clips, slow-to-fast flow variations, front-to-back appearance changes, and light-to-heavy occlusions. To better adapt VIC approaches to crowds, we rethink the nature of VIC and recognize two informative priors: i) the social grouping prior that indicates pedestrians tend to gather in groups and ii) the spatial-temporal displacement prior that informs an individual cannot teleport physically. The former inspires us to relax the standard one-to-one (O2O) matching used by VIC to one-to-many (O2M) matching, implemented by an implicit context generator and a O2M matcher; the latter facilitates the design of a displacement prior injector, which strengthens not only O2M matching but also feature extraction and model training. These designs jointly form a novel and strong VIC baseline OMAN++. Extensive experiments show that OMAN++ not only outperforms state-of-the-art VIC baselines on the standard SenseCrowd, CroHD, and MovingDroneCrowd benchmarks, but also indicates a clear advantage in crowded scenes, with a 38.12% error reduction on our WuhanMetroCrowd dataset. Code, data, and pretrained models are available at https://github.com/tiny-smart/OMAN.

</details>


### [180] [Real-Time Lane Detection via Efficient Feature Alignment and Covariance Optimization for Low-Power Embedded Systems](https://arxiv.org/abs/2601.01696)
*Yian Liu,Xiong Wang,Ping Xu,Lei Zhu,Ming Yan,Linyun Xue*

Main category: cs.CV

TL;DR: This paper addresses lane detection challenges in embedded systems, proposing a Covariance Distribution Optimization (CDO) module that enhances detection accuracy without increasing computational complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the difficulty of real-time lane detection in embedded systems, considering constraints like sparse visual signals, limited computation, and power consumption.

Method: The method involves introducing the Covariance Distribution Optimization (CDO) module, which aligns lane feature distributions with ground-truth labels to improve detection accuracy.

Result: The paper reports accuracy improvements between 0.01% to 1.5% across six models tested on three major datasets (CULane, TuSimple, LLAMAS), covering multiple method categories.

Conclusion: The CDO module improves lane detection accuracy, is easy to integrate, requires no structural modifications, and aids efficient training while enhancing performance, power efficiency, and operational flexibility in embedded systems.

Abstract: Real-time lane detection in embedded systems encounters significant challenges due to subtle and sparse visual signals in RGB images, often constrained by limited computational resources and power consumption. Although deep learning models for lane detection categorized into segmentation-based, anchor-based, and curve-based methods there remains a scarcity of universally applicable optimization techniques tailored for low-power embedded environments. To overcome this, we propose an innovative Covariance Distribution Optimization (CDO) module specifically designed for efficient, real-time applications. The CDO module aligns lane feature distributions closely with ground-truth labels, significantly enhancing detection accuracy without increasing computational complexity. Evaluations were conducted on six diverse models across all three method categories, including two optimized for real-time applications and four state-of-the-art (SOTA) models, tested comprehensively on three major datasets: CULane, TuSimple, and LLAMAS. Experimental results demonstrate accuracy improvements ranging from 0.01% to 1.5%. The proposed CDO module is characterized by ease of integration into existing systems without structural modifications and utilizes existing model parameters to facilitate ongoing training, thus offering substantial benefits in performance, power efficiency, and operational flexibility in embedded systems.

</details>


### [181] [MS-ISSM: Objective Quality Assessment of Point Clouds Using Multi-scale Implicit Structural Similarity](https://arxiv.org/abs/2601.01200)
*Zhang Chen,Shuai Wan,Yuezhe Zhang,Siyu Ren,Fuzheng Yang,Junhui Hou*

Main category: cs.CV

TL;DR: This paper introduces MS-ISSM for improving point cloud quality assessment by addressing the challenges posed by unstructured data with a novel implicit structural similarity measurement approach and a specialized assessment network.


<details>
  <summary>Details</summary>
Motivation: Assessing the quality of unstructured point cloud data is challenging due to difficulty in finding accurate perceptual feature correspondences.

Method: The paper proposes MS-ISSM, which uses Radial Basis Functions (RBF) for implicit structural similarity measurement, and introduces a ResGrouped-MLP network with hierarchical designs to map feature differences to perceptual scores.

Result: The proposed MS-ISSM and its network outperformed existing state-of-the-art quality metrics in reliability and generalization across benchmarks.

Conclusion: MS-ISSM effectively addresses matching errors in point clouds, offering a robust and accurate method for assessing quality while preserving distinct physical semantics like luma, chroma, and geometry.

Abstract: The unstructured and irregular nature of point clouds poses a significant challenge for objective quality assessment (PCQA), particularly in establishing accurate perceptual feature correspondence. To tackle this, we propose the Multi-scale Implicit Structural Similarity Measurement (MS-ISSM). Unlike traditional point-to-point matching, MS-ISSM utilizes Radial Basis Functions (RBF) to represent local features continuously, transforming distortion measurement into a comparison of implicit function coefficients. This approach effectively circumvents matching errors inherent in irregular data. Additionally, we propose a ResGrouped-MLP quality assessment network, which robustly maps multi-scale feature differences to perceptual scores. The network architecture departs from traditional flat MLPs by adopting a grouped encoding strategy integrated with Residual Blocks and Channel-wise Attention mechanisms. This hierarchical design allows the model to preserve the distinct physical semantics of luma, chroma, and geometry while adaptively focusing on the most salient distortion features across High, Medium, and Low scales. Experimental results on multiple benchmarks demonstrate that MS-ISSM outperforms state-of-the-art metrics in both reliability and generalization. The source code is available at: https://github.com/ZhangChen2022/MS-ISSM.

</details>


### [182] [VIT-Ped: Visionary Intention Transformer for Pedestrian Behavior Analysis](https://arxiv.org/abs/2601.01989)
*Aly R. Elkammar,Karim M. Gamaleldin,Catherine M. Elias*

Main category: cs.CV

TL;DR: This paper introduces a transformer-based algorithm for predicting pedestrian crossing intentions, evaluated on the JAAD dataset with state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To enhance road safety during the transition to level 4 autonomous driving by accurately predicting pedestrian crossing intentions.

Method: The authors developed a transformer/video vision transformer algorithm leveraging various data modalities and conducted extensive ablation studies to analyze model design choices.

Result: The algorithm achieved state-of-the-art (SOTA) results in terms of Accuracy, AUC, and F1-score on the JAAD dataset.

Conclusion: The transformer-based algorithm shows potential for improving pedestrian behavior prediction and advancing autonomous driving safety.

Abstract: Pedestrian Intention prediction is one of the key technologies in the transition from level 3 to level 4 autonomous driving. To understand pedestrian crossing behaviour, several elements and features should be taken into consideration to make the roads of tomorrow safer for everybody. We introduce a transformer / video vision transformer based algorithm of different sizes which uses different data modalities .We evaluated our algorithms on popular pedestrian behaviour dataset, JAAD, and have reached SOTA performance and passed the SOTA in metrics like Accuracy, AUC and F1-score. The advantages brought by different model design choices are investigated via extensive ablation studies.

</details>


### [183] [RefSR-Adv: Adversarial Attack on Reference-based Image Super-Resolution Models](https://arxiv.org/abs/2601.01202)
*Jiazhu Dai,Huihui Jiang*

Main category: cs.CV

TL;DR: The study introduces RefSR-Adv, an adversarial attack targeting Reference-based Super-Resolution (RefSR) systems, degrading outputs by only perturbing reference images.


<details>
  <summary>Details</summary>
Motivation: To analyze and address the unexplored vulnerability of Reference-based Super-Resolution (RefSR) systems under adversarial attacks, encouraging focus on their robustness.

Method: Developed RefSR-Adv, an adversarial attack that perturbs the reference image to maximize differences in outputs, testing on various architectures and datasets.

Result: RefSR-Adv effectively degrades performance and creates severe artifacts, demonstrating a security issue. Attack effectiveness correlates with similarity between input and reference images.

Conclusion: A major security flaw in RefSR systems is uncovered, urging for attention to their robustness and reliance on reference features.

Abstract: Single Image Super-Resolution (SISR) aims to recover high-resolution images from low-resolution inputs. Unlike SISR, Reference-based Super-Resolution (RefSR) leverages an additional high-resolution reference image to facilitate the recovery of high-frequency textures. However, existing research mainly focuses on backdoor attacks targeting RefSR, while the vulnerability of the adversarial attacks targeting RefSR has not been fully explored. To fill this research gap, we propose RefSR-Adv, an adversarial attack that degrades SR outputs by perturbing only the reference image. By maximizing the difference between adversarial and clean outputs, RefSR-Adv induces significant performance degradation and generates severe artifacts across CNN, Transformer, and Mamba architectures on the CUFED5, WR-SR, and DRefSR datasets. Importantly, experiments confirm a positive correlation between the similarity of the low-resolution input and the reference image and attack effectiveness, revealing that the model's over-reliance on reference features is a key security flaw. This study reveals a security vulnerability in RefSR systems, aiming to urge researchers to pay attention to the robustness of RefSR.

</details>


### [184] [XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry Grounded Transformer with KV Cache Compression](https://arxiv.org/abs/2601.01204)
*Zunhai Su,Weihao Ye,Hansen Feng,Keyu Fan,Jing Zhang,Dahai Yu,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: XStreamVGGT addresses memory and inference latency issues in streaming 3D geometry models by introducing tuning-free compression of the KV cache through pruning and quantization, significantly improving efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the scalability and inefficiency issues in 3D geometry models like StreamVGGT, which suffer from unbounded memory consumption and increased latency due to KV cache growth during streaming inference.

Method: The paper proposes a tuning-free approach, termed XStreamVGGT, that compresses the KV cache by pruning redundant entries and incorporating quantization, ensuring fixed memory utilization and reduced computational demands.

Result: XStreamVGGT achieves a 4.42× reduction in memory usage and a 5.48× acceleration in inference time with minimal performance degradation.

Conclusion: XStreamVGGT enables scalable and efficient streaming 3D applications by substantially optimizing memory and computational resources, addressing key challenges in large-scale transformers for 3D visual geometry.

Abstract: Learning-based 3D visual geometry models have benefited substantially from large-scale transformers. Among these, StreamVGGT leverages frame-wise causal attention for strong streaming reconstruction, but suffers from unbounded KV cache growth, leading to escalating memory consumption and inference latency as input frames accumulate. We propose XStreamVGGT, a tuning-free approach that systematically compresses the KV cache through joint pruning and quantization, enabling extremely memory-efficient streaming inference. Specifically, redundant KVs originating from multi-view inputs are pruned through efficient token importance identification, enabling a fixed memory budget. Leveraging the unique distribution of KV tensors, we incorporate KV quantization to further reduce memory consumption. Extensive evaluations show that XStreamVGGT achieves mostly negligible performance degradation while substantially reducing memory usage by 4.42$\times$ and accelerating inference by 5.48$\times$, enabling scalable and practical streaming 3D applications. The code is available at https://github.com/ywh187/XStreamVGGT/.

</details>


### [185] [Promptable Foundation Models for SAR Remote Sensing: Adapting the Segment Anything Model for Snow Avalanche Segmentation](https://arxiv.org/abs/2601.01213)
*Riccardo Gelato,Carlo Sgaravatti,Jakob Grahn,Giacomo Boracchi,Filippo Maria Bianchi*

Main category: cs.CV

TL;DR: The paper presents a method to speed up the annotation process for avalanche mapping using Sentinel-1 SAR images by adapting the Segment Anything Model (SAM) to better suit SAR data.


<details>
  <summary>Details</summary>
Motivation: To improve and accelerate the annotation of SAR images for avalanche detection and mapping, as the manual annotation process is time-consuming and requires expert knowledge.

Method: The approach includes adapting the SAM segmentation model to SAR data through techniques such as domain-specific adapters, multiple encoders for multi-channel input, engineering prompts for better accuracy, and an efficient training algorithm to reduce computational overhead.

Result: The adapted model was integrated into an annotation tool and demonstrated to significantly accelerate the process of annotating SAR images for avalanches.

Conclusion: This research successfully shows the utility of modifying SAM to address the domain-specific challenges of SAR data, leading to more efficient annotation processes and contributing towards better avalanche risk management.

Abstract: Remote sensing solutions for avalanche segmentation and mapping are key to supporting risk forecasting and mitigation in mountain regions. Synthetic Aperture Radar (SAR) imagery from Sentinel-1 can be effectively used for this task, but training an effective detection model requires gathering a large dataset with high-quality annotations from domain experts, which is prohibitively time-consuming. In this work, we aim to facilitate and accelerate the annotation of SAR images for avalanche mapping. We build on the Segment Anything Model (SAM), a segmentation foundation model trained on natural images, and tailor it to Sentinel-1 SAR data. Adapting SAM to our use-case requires addressing several domain-specific challenges: (i) domain mismatch, since SAM was not trained on satellite/SAR imagery; (ii) input adaptation, because SAR products typically provide more than three channels, while SAM is constrained to RGB images; (iii) robustness to imprecise prompts that can affect target identification and degrade the segmentation quality, an issue exacerbated in small, low-contrast avalanches; and (iv) training efficiency, since standard fine-tuning is computationally demanding for SAM. We tackle these challenges through a combination of adapters to mitigate the domain gap, multiple encoders to handle multi-channel SAR inputs, prompt-engineering strategies to improve avalanche localization accuracy, and a training algorithm that limits the training time of the encoder, which is recognized as the major bottleneck. We integrate the resulting model into an annotation tool and show experimentally that it speeds up the annotation of SAR images.

</details>


### [186] [UniSH: Unifying Scene and Human Reconstruction in a Feed-Forward Pass](https://arxiv.org/abs/2601.01222)
*Mengfei Li,Peng Li,Zheng Zhang,Jiahao Lu,Chengfeng Zhao,Wei Xue,Qifeng Liu,Sida Peng,Wenxiao Zhang,Wenhan Luo,Yuan Liu,Yike Guo*

Main category: cs.CV

TL;DR: The paper proposes UniSH, a feed-forward framework for reconstructing 3D scenes and human geometry, using unlabeled in-the-wild data to overcome the limitations of synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with poor generalization and low fidelity human geometry when using synthetic datasets due to a lack of sufficient real-world annotated data.

Method: They propose a unified training framework with two core components: (1) distilling high-frequency surface details from depth models for human surface refinement, and (2) a two-stage supervision scheme learning localization from synthetic data and fine-tuning on real data.

Result: UniSH achieves state-of-the-art performance in human-centric scene reconstruction and competitive global human motion estimation in comparison to other methods.

Conclusion: The framework effectively leverages unlabeled in-the-wild data to bridge domain gaps, enabling high-fidelity and coherent reconstruction in single forward passes, outperforming many existing methods.

Abstract: We present UniSH, a unified, feed-forward framework for joint metric-scale 3D scene and human reconstruction. A key challenge in this domain is the scarcity of large-scale, annotated real-world data, forcing a reliance on synthetic datasets. This reliance introduces a significant sim-to-real domain gap, leading to poor generalization, low-fidelity human geometry, and poor alignment on in-the-wild videos. To address this, we propose an innovative training paradigm that effectively leverages unlabeled in-the-wild data. Our framework bridges strong, disparate priors from scene reconstruction and HMR, and is trained with two core components: (1) a robust distillation strategy to refine human surface details by distilling high-frequency details from an expert depth model, and (2) a two-stage supervision scheme, which first learns coarse localization on synthetic data, then fine-tunes on real data by directly optimizing the geometric correspondence between the SMPL mesh and the human point cloud. This approach enables our feed-forward model to jointly recover high-fidelity scene geometry, human point clouds, camera parameters, and coherent, metric-scale SMPL bodies, all in a single forward pass. Extensive experiments demonstrate that our model achieves state-of-the-art performance on human-centric scene reconstruction and delivers highly competitive results on global human motion estimation, comparing favorably against both optimization-based frameworks and HMR-only methods. Project page: https://murphylmf.github.io/UniSH/

</details>


### [187] [Improved Object-Centric Diffusion Learning with Registers and Contrastive Alignment](https://arxiv.org/abs/2601.01224)
*Bac Nguyen,Yuhta Takida,Naoki Murata,Chieh-Hsin Lai,Toshimitsu Uesaka,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: CODA enhances object-centric learning by addressing slot entanglement and improving alignment between object slots and image content using register slots and contrastive alignment loss.


<details>
  <summary>Details</summary>
Motivation: To address slot entanglement and weak alignment between object slots and image content in object-centric learning with pretrained diffusion models.

Method: Employs register slots to reduce interference among slots and uses a contrastive alignment loss to encourage strong correspondence between slots and image content.

Result: CODA improves object discovery, property prediction, and compositional image generation. It demonstrates a 6.1% FG-ARI improvement on COCO and is efficient with minimal computational overhead.

Conclusion: CODA serves as an effective, scalable framework for robust object-centric learning in both synthetic and real-world complex scenes.

Abstract: Slot Attention (SA) with pretrained diffusion models has recently shown promise for object-centric learning (OCL), but suffers from slot entanglement and weak alignment between object slots and image content. We propose Contrastive Object-centric Diffusion Alignment (CODA), a simple extension that (i) employs register slots to absorb residual attention and reduce interference between object slots, and (ii) applies a contrastive alignment loss to explicitly encourage slot-image correspondence. The resulting training objective serves as a tractable surrogate for maximizing mutual information (MI) between slots and inputs, strengthening slot representation quality. On both synthetic (MOVi-C/E) and real-world datasets (VOC, COCO), CODA improves object discovery (e.g., +6.1% FG-ARI on COCO), property prediction, and compositional image generation over strong baselines. Register slots add negligible overhead, keeping CODA efficient and scalable. These results indicate potential applications of CODA as an effective framework for robust OCL in complex, real-world scenes.

</details>


### [188] [HyDRA: Hybrid Denoising Regularization for Measurement-Only DEQ Training](https://arxiv.org/abs/2601.01228)
*Markus Haltmeier,Lukas Neumann,Nadja Gruber,Johannes Schwab,Gyeongha Hwang*

Main category: cs.CV

TL;DR: The paper introduces HyDRA, a framework for image reconstruction using deep equilibrium models without requiring supervised datasets, addressing ill-posedness and measurement-only setups.


<details>
  <summary>Details</summary>
Motivation: Image reconstruction problems are challenging due to ill-posedness and lack of supervised datasets. The goal is to enable effective training and reconstruction with only measurements available.

Method: HyDRA combines measurement consistency, adaptive denoising regularization, and a data-driven early stopping criterion to train deep equilibrium models using only measurements.

Result: HyDRA demonstrates competitive reconstruction quality and fast inference through experiments on sparse-view CT.

Conclusion: HyDRA successfully addresses the challenges of image reconstruction in measurement-only setups, showing promise for practical applications.

Abstract: Solving image reconstruction problems of the form \(\mathbf{A} \mathbf{x} = \mathbf{y}\) remains challenging due to ill-posedness and the lack of large-scale supervised datasets. Deep Equilibrium (DEQ) models have been used successfully but typically require supervised pairs \((\mathbf{x},\mathbf{y})\). In many practical settings, only measurements \(\mathbf{y}\) are available. We introduce HyDRA (Hybrid Denoising Regularization Adaptation), a measurement-only framework for DEQ training that combines measurement consistency with an adaptive denoising regularization term, together with a data-driven early stopping criterion. Experiments on sparse-view CT demonstrate competitive reconstruction quality and fast inference.

</details>


### [189] [RFAssigner: A Generic Label Assignment Strategy for Dense Object Detection](https://arxiv.org/abs/2601.01240)
*Ziqian Guan,Xieyi Fu,Yuting Wang,Haowen Xiao,Jiarui Zhu,Yingying Zhu,Yongtao Liu,Lin Gu*

Main category: cs.CV

TL;DR: Label assignment in dense object detection faces a challenge with imbalance for small objects. RFAssigner introduces a novel Gaussian Receptive Field (GRF)-based assignment to address this, improving multi-scale learning.


<details>
  <summary>Details</summary>
Motivation: To overcome the issue of scale imbalance in training dense object detectors, particularly insufficient positive samples for small objects.

Method: RFAssigner initializes positive samples using a point-based prior and utilizes the Gaussian Receptive Field distance to adaptively assign additional positive samples from unassigned candidates.

Result: RFAssigner demonstrates state-of-the-art performance on diverse datasets, improving multi-scale object detection without auxiliary modules or complex heuristics.

Conclusion: RFAssigner is an effective, straightforward approach for balanced training in dense object detection, achieving superior multi-scale detection.

Abstract: Label assignment is a critical component in training dense object detectors. State-of-the-art methods typically assign each training sample a positive and a negative weight, optimizing the assignment scheme during training. However, these strategies often assign an insufficient number of positive samples to small objects, leading to a scale imbalance during training. To address this limitation, we introduce RFAssigner, a novel assignment strategy designed to enhance the multi-scale learning capabilities of dense detectors. RFAssigner first establishes an initial set of positive samples using a point-based prior. It then leverages a Gaussian Receptive Field (GRF) distance to measure the similarity between the GRFs of unassigned candidate locations and the ground-truth objects. Based on this metric, RFAssigner adaptively selects supplementary positive samples from the unassigned pool, promoting a more balanced learning process across object scales. Comprehensive experiments on three datasets with distinct object scale distributions validate the effectiveness and generalizability of our method. Notably, a single FCOS-ResNet-50 detector equipped with RFAssigner achieves state-of-the-art performance across all object scales, consistently outperforming existing strategies without requiring auxiliary modules or heuristics.

</details>


### [190] [MambaFormer: Token-Level Guided Routing Mixture-of-Experts for Accurate and Efficient Clinical Assistance](https://arxiv.org/abs/2601.01260)
*Hamad Khan,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: The MambaFormer is a hybrid model combining different expert modules to enable efficient and accurate medical question answering (QA), tailored for clinical environments with resource constraints.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between computational cost and efficiency of using large language models in clinical applications.

Method: The MambaFormer framework uses lightweight gating and dynamic token routing for short or long queries via separate expert models (ET5 for complexity and EMamba for throughput). It employs a customized multi-objective transfer learning on the DentalQA dataset and optimizes routing dynamically.

Result: MambaFormer demonstrated superior performance in medical QA, achieving a BERTScore of 0.9180 while being 24.4 times faster than T5-Large, with ultra-low inference latency of 0.077 seconds.

Conclusion: MambaFormer provides a scalable and efficient solution for medical QA by balancing inference speed and accuracy, enabling real-world deployment in resource-constrained environments.

Abstract: The deployment of large language models (LLMs) in real-world clinical applications is constrained by the fundamental trade-off between computational cost and the efficiency of linear-time models. To address this, we propose an LLM-based MambaFormer hybrid Mixture-of-Experts (MoE) framework for efficient medical question-answering (QA) and clinical assistance. The MambaFormer employs a lightweight gating mechanism that performs token-level dynamic routing to a customized Transformer expert (ET5) for short, complex queries or to a State Space Model expert (EMamba) for long, high-throughput sequences. The customized EMamba and ET5 models are tailored to accommodate input sequence dimensionality, embedding structure, sequence length, and target-specific output heads, and are fine-tuned through transfer learning on a new, custom-designed DentalQA dataset. Moreover, intelligent routing decisions are driven by the contextual complexity of token embeddings, normalized sequence length, and domain-aware features, thereby enforcing a Pareto-optimal trade-off between inference latency and prediction accuracy. Furthermore, a novel utility-guided multi-objective loss jointly optimizes decisions, router parameters, routing behavior, expert utilization, and computational cost by adaptively regulating token-level expert activation. Finally, the proposed MambaFormer is cross-validated (holdout) for medical QA on the new, custom-designed DentalQA and PubMedQA datasets and compared with state-of-the-art techniques. The proposed MambaFormer outperforms (BERTScore = 0.9180) with ultra-low latency (0.077 s), delivering a 24.4 speedup over T5-Large and establishing a scalable solution for resource-constrained clinical deployment.

</details>


### [191] [AI-Powered Deepfake Detection Using CNN and Vision Transformer Architectures](https://arxiv.org/abs/2601.01281)
*Sifatullah Sheikh Urmi,Kirtonia Nuzath Tabassum Arthi,Md Al-Imran*

Main category: cs.CV

TL;DR: The study assessed AI models for deepfake detection, with VFDNET showing top accuracy using MobileNetV3.


<details>
  <summary>Details</summary>
Motivation: To address challenges in maintaining digital authenticity due to AI-generated deepfakes.

Method: Four AI models, including three CNNs and one Vision Transformer, were tested using large face image datasets with preprocessing and augmentation applied.

Result: VFDNET, utilizing MobileNetV3, achieved superior accuracy and efficient performance.

Conclusion: AI tools like VFDNET can reliably detect deepfakes, supporting digital authenticity.

Abstract: The increasing use of artificial intelligence generated deepfakes creates major challenges in maintaining digital authenticity. Four AI-based models, consisting of three CNNs and one Vision Transformer, were evaluated using large face image datasets. Data preprocessing and augmentation techniques improved model performance across different scenarios. VFDNET demonstrated superior accuracy with MobileNetV3, showing efficient performance, thereby demonstrating AI's capabilities for dependable deepfake detection.

</details>


### [192] [S2M-Net: Spectral-Spatial Mixing for Medical Image Segmentation with Morphology-Aware Adaptive Loss](https://arxiv.org/abs/2601.01285)
*Md. Sanaullah Chowdhury Lameya Sabrin*

Main category: cs.CV

TL;DR: The paper introduces S2M-Net, an efficient medical image segmentation model, addressing challenges of local precision, global context, and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing architectures inadequately address the balance between precision, context, and efficiency for accurate medical image segmentation in low-resource settings.

Method: S2M-Net incorporates two key innovations: Spectral-Selective Token Mixer (SSTM) for efficient global context and Morphology-Aware Adaptive Segmentation Loss (MASL) for automated structure-specific loss modulation.

Result: S2M-Net achieves state-of-the-art results across 16 datasets (modality-spanning), significantly improving Dice scores with fewer parameters compared to transformer-based methods.

Conclusion: S2M-Net offers an optimized, effective solution for medical image segmentation, excelling in accuracy, computational efficiency, and adaptability to diverse clinical datasets.

Abstract: Medical image segmentation requires balancing local precision for boundary-critical clinical applications, global context for anatomical coherence, and computational efficiency for deployment on limited data and hardware a trilemma that existing architectures fail to resolve. Although convolutional networks provide local precision at $\mathcal{O}(n)$ cost but limited receptive fields, vision transformers achieve global context through $\mathcal{O}(n^2)$ self-attention at prohibitive computational expense, causing overfitting on small clinical datasets. We propose S2M-Net, a 4.7M-parameter architecture that achieves $\mathcal{O}(HW \log HW)$ global context through two synergistic innovations: (i) Spectral-Selective Token Mixer (SSTM), which exploits the spectral concentration of medical images via truncated 2D FFT with learnable frequency filtering and content-gated spatial projection, avoiding quadratic attention cost while maintaining global receptive fields; and (ii) Morphology-Aware Adaptive Segmentation Loss (MASL), which automatically analyzes structure characteristics (compactness, tubularity, irregularity, scale) to modulate five complementary loss components through constrained learnable weights, eliminating manual per-dataset tuning. Comprehensive evaluation in 16 medical imaging datasets that span 8 modalities demonstrates state-of-the-art performance: 96.12\% Dice on polyp segmentation, 83.77\% on surgical instruments (+17.85\% over the prior art) and 80.90\% on brain tumors, with consistent 3-18\% improvements over specialized baselines while using 3.5--6$\times$ fewer parameters than transformer-based methods.

</details>


### [193] [VReID-XFD: Video-based Person Re-identification at Extreme Far Distance Challenge Results](https://arxiv.org/abs/2601.01312)
*Kailash A. Hambarde,Hugo Proença,Md Rashidunnabi,Pranita Samale,Qiwei Yang,Pingping Zhang,Zijing Gong,Yuhao Wang,Xi Zhang,Ruoshui Qu,Qiaoyun He,Yuhang Zhang,Thi Ngoc Ha Nguyen,Tien-Dung Mai,Cheng-Jun Kang,Yu-Fan Lin,Jin-Hui Jiang,Chih-Chung Hsu,Tamás Endrei,György Cserey,Ashwat Rajbhandari*

Main category: cs.CV

TL;DR: The paper introduces VReID-XFD, a benchmark for extreme far-distance aerial-to-ground person re-identification, highlighting its challenges and community-driven findings.


<details>
  <summary>Details</summary>
Motivation: To address the significant challenges in ReID across aerial and ground views due to resolution, viewpoint, motion, and clothing variability, and to provide a standardized dataset for this research.

Method: Introduced VReID-XFD, a dataset derived from DetReIDX with specific settings and identity-disjoint splits, and organized the VReID-XFD-25 Challenge to evaluate methods.

Result: Revealed performance degradation with altitude/distance, the disadvantage of nadir views, trade-offs in performance versus robustness, and the highest-achieving method had limited mAP at 43.93%.

Conclusion: Aerial-to-ground ReID at extreme far distances remains a challenging problem requiring innovation, supported by the VReID-XFD benchmark.

Abstract: Person re-identification (ReID) across aerial and ground views at extreme far distances introduces a distinct operating regime where severe resolution degradation, extreme viewpoint changes, unstable motion cues, and clothing variation jointly undermine the appearance-based assumptions of existing ReID systems. To study this regime, we introduce VReID-XFD, a video-based benchmark and community challenge for extreme far-distance (XFD) aerial-to-ground person re-identification. VReID-XFD is derived from the DetReIDX dataset and comprises 371 identities, 11,288 tracklets, and 11.75 million frames, captured across altitudes from 5.8 m to 120 m, viewing angles from oblique (30 degrees) to nadir (90 degrees), and horizontal distances up to 120 m. The benchmark supports aerial-to-aerial, aerial-to-ground, and ground-to-aerial evaluation under strict identity-disjoint splits, with rich physical metadata. The VReID-XFD-25 Challenge attracted 10 teams with hundreds of submissions. Systematic analysis reveals monotonic performance degradation with altitude and distance, a universal disadvantage of nadir views, and a trade-off between peak performance and robustness. Even the best-performing SAS-PReID method achieves only 43.93 percent mAP in the aerial-to-ground setting. The dataset, annotations, and official evaluation protocols are publicly available at https://www.it.ubi.pt/DetReIDX/ .

</details>


### [194] [LinMU: Multimodal Understanding Made Linear](https://arxiv.org/abs/2601.01322)
*Hongjie Wang,Niraj K. Jha*

Main category: cs.CV

TL;DR: LinMU is a Vision-Language Model (VLM) with linear complexity that matches global-attention-based VLM performance while greatly improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing Vision-Language Models have quadratic complexity in self-attention, making them unsuitable for edge devices, high-resolution images, and long-context videos.

Method: The authors propose LinMU, which replaces self-attention layers with M-MATE blocks combining bidirectional state-space models and localized Swin-style window attention. A three-stage distillation framework is introduced for training.

Result: LinMU matches teacher model performance on benchmarks like MMMU and TextVQA while achieving up to 2.7× faster Time-To-First-Token and 9.0× better token throughput for long videos.

Conclusion: LinMU eliminates the need for quadratic attention in state-of-the-art multimodal reasoning, enabling long-context analysis for high-resolution and video-based tasks.

Abstract: Modern Vision-Language Models (VLMs) achieve impressive performance but are limited by the quadratic complexity of self-attention, which prevents their deployment on edge devices and makes their understanding of high-resolution images and long-context videos prohibitively expensive. To address this challenge, we introduce LinMU (Linear-complexity Multimodal Understanding), a VLM design that achieves linear complexity without using any quadratic-complexity modules while maintaining the performance of global-attention-based VLMs. LinMU replaces every self-attention layer in the VLM with the M-MATE block: a dual-branch module that combines a bidirectional state-space model for global context (Flex-MA branch) with localized Swin-style window attention (Local-Swin branch) for adjacent correlations. To transform a pre-trained VLM into the LinMU architecture, we propose a three-stage distillation framework that (i) initializes both branches with self-attention weights and trains the Flex-MA branch alone, (ii) unfreezes the Local-Swin branch and fine-tunes it jointly with the Flex-MA branch, and (iii) unfreezes the remaining blocks and fine-tunes them using LoRA adapters, while regressing on hidden states and token-level logits of the frozen VLM teacher. On MMMU, TextVQA, LongVideoBench, Video-MME, and other benchmarks, LinMU matches the performance of teacher models, yet reduces Time-To-First-Token (TTFT) by up to 2.7$\times$ and improves token throughput by up to 9.0$\times$ on minute-length videos. Ablations confirm the importance of each distillation stage and the necessity of the two branches of the M-MATE block. The proposed framework demonstrates that state-of-the-art multimodal reasoning can be achieved without quadratic attention, thus opening up avenues for long-context VLMs that can deal with high-resolution images and long videos.

</details>


### [195] [Achieving Fine-grained Cross-modal Understanding through Brain-inspired Hierarchical Representation Learning](https://arxiv.org/abs/2601.01339)
*Weihang You,Hanqi Jiang,Yi Pan,Junhao Chen,Tianming Liu,Fei Dou*

Main category: cs.CV

TL;DR: NeuroAlign introduces a new fMRI-video alignment framework, outperforming existing methods through a two-stage hierarchical approach inspired by human visual processing.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to capture the hierarchical and temporal nature of visual processing in the brain.

Method: NeuroAlign employs Neural-Temporal Contrastive Learning (NTCL) for semantic understanding and vector quantization for fine-grained alignment, using DynaSyncMM-EMA for dynamic multi-modal fusion.

Result: NeuroAlign achieves superior performance in cross-modal retrieval tasks compared to existing methods.

Conclusion: The framework establishes a new way to study visual cognition by leveraging biologically-inspired alignment approaches.

Abstract: Understanding neural responses to visual stimuli remains challenging due to the inherent complexity of brain representations and the modality gap between neural data and visual inputs. Existing methods, mainly based on reducing neural decoding to generation tasks or simple correlations, fail to reflect the hierarchical and temporal processes of visual processing in the brain. To address these limitations, we present NeuroAlign, a novel framework for fine-grained fMRI-video alignment inspired by the hierarchical organization of the human visual system. Our framework implements a two-stage mechanism that mirrors biological visual pathways: global semantic understanding through Neural-Temporal Contrastive Learning (NTCL) and fine-grained pattern matching through enhanced vector quantization. NTCL explicitly models temporal dynamics through bidirectional prediction between modalities, while our DynaSyncMM-EMA approach enables dynamic multi-modal fusion with adaptive weighting. Experiments demonstrate that NeuroAlign significantly outperforms existing methods in cross-modal retrieval tasks, establishing a new paradigm for understanding visual cognitive mechanisms.

</details>


### [196] [Slot-ID: Identity-Preserving Video Generation from Reference Videos via Slot-Based Temporal Identity Encoding](https://arxiv.org/abs/2601.01352)
*Yixuan Lai,He Wang,Kun Zhou,Tianjia Shao*

Main category: cs.CV

TL;DR: The paper introduces an advanced technique for generating prompt-faithful videos that better preserve user-specified identities by leveraging dynamics from short reference videos instead of single images.


<details>
  <summary>Details</summary>
Motivation: To address challenges in creating videos where identity is preserved alongside natural-looking motion, especially when relying on sparse references like single images that neglect temporal signature.

Method: An identity-conditioned diffusion-transformer video generator that uses a short reference video to extract characteristic dynamics via a Sinkhorn-routed encoder, which generates compact identity tokens compatible with pretrained backbones.

Result: The technique maintains prompt faithfulness, improves identity retention during large pose changes and expressive behaviors, and achieves high visual realism.

Conclusion: Leveraging characteristic dynamics from short clips enables models to produce more identity-faithful and realistic videos in diverse scenarios.

Abstract: Producing prompt-faithful videos that preserve a user-specified identity remains challenging: models need to extrapolate facial dynamics from sparse reference while balancing the tension between identity preservation and motion naturalness. Conditioning on a single image completely ignores the temporal signature, which leads to pose-locked motions, unnatural warping, and "average" faces when viewpoints and expressions change. To this end, we introduce an identity-conditioned variant of a diffusion-transformer video generator which uses a short reference video rather than a single portrait. Our key idea is to incorporate the dynamics in the reference. A short clip reveals subject-specific patterns, e.g., how smiles form, across poses and lighting. From this clip, a Sinkhorn-routed encoder learns compact identity tokens that capture characteristic dynamics while remaining pretrained backbone-compatible. Despite adding only lightweight conditioning, the approach consistently improves identity retention under large pose changes and expressive facial behavior, while maintaining prompt faithfulness and visual realism across diverse subjects and prompts.

</details>


### [197] [Advanced Machine Learning Approaches for Enhancing Person Re-Identification Performance](https://arxiv.org/abs/2601.01356)
*Dang H. Pham,Tu N. Nguyen,Hoa N. Nguyen*

Main category: cs.CV

TL;DR: This dissertation introduces three approaches that improve person re-identification (ReID) by addressing supervised, unsupervised domain adaptation (UDA), and fully unsupervised challenges, achieving state-of-the-art results on various datasets.


<details>
  <summary>Details</summary>
Motivation: ReID faces challenges such as appearance variations, domain shifts, and lack of labeled data, crucial for improving intelligent surveillance systems.

Method: The work proposes 1) supervised contrastive learning with hybrid loss (SCM-ReID), 2) GAN-based domain adaptation (IQAGA and DAPRH), and 3) Vision Transformer-based unsupervised learning (ViTC-UReID).

Result: The three proposed strategies achieved significant performance improvements, with state-of-the-art accuracy in supervised settings, 12% mAP gains in UDA, and superior metrics in unsupervised tasks on CUHK03, Market-1501, DukeMTMC-reID, and MSMT17.

Conclusion: The contributions effectively address feature learning, domain adaptation, and label noise issues, advancing robust ReID systems suitable for real-world surveillance scenarios.

Abstract: Person re-identification (ReID) plays a critical role in intelligent surveillance systems by linking identities across multiple cameras in complex environments. However, ReID faces significant challenges such as appearance variations, domain shifts, and limited labeled data. This dissertation proposes three advanced approaches to enhance ReID performance under supervised, unsupervised domain adaptation (UDA), and fully unsupervised settings. First, SCM-ReID integrates supervised contrastive learning with hybrid loss optimization (classification, center, triplet, and centroid-triplet losses), improving discriminative feature representation and achieving state-of-the-art accuracy on Market-1501 and CUHK03 datasets. Second, for UDA, IQAGA and DAPRH combine GAN-based image augmentation, domain-invariant mapping, and pseudo-label refinement to mitigate domain discrepancies and enhance cross-domain generalization. Experiments demonstrate substantial gains over baseline methods, with mAP and Rank-1 improvements up to 12% in challenging transfer scenarios. Finally, ViTC-UReID leverages Vision Transformer-based feature encoding and camera-aware proxy learning to boost unsupervised ReID. By integrating global and local attention with camera identity constraints, this method significantly outperforms existing unsupervised approaches on large-scale benchmarks. Comprehensive evaluations across CUHK03, Market-1501, DukeMTMC-reID, and MSMT17 confirm the effectiveness of the proposed methods. The contributions advance ReID research by addressing key limitations in feature learning, domain adaptation, and label noise handling, paving the way for robust deployment in real-world surveillance systems.

</details>


### [198] [Garment Inertial Denoiser (GID): Endowing Accurate Motion Capture via Loose IMU Denoiser](https://arxiv.org/abs/2601.01360)
*Jiawei Fang,Ruonan Zheng,Xiaoxia Gao,Shifan Jiang,Anjun Chen,Qi Ye,Shihui Guo*

Main category: cs.CV

TL;DR: The paper introduces GID, a Transformer-based model designed for wearable inertial motion capture (MoCap) systems embedded in loose-fitting garments, addressing sensor-body displacement issues.


<details>
  <summary>Details</summary>
Motivation: Wearable inertial motion capture systems offer portability and privacy, but their reliance on tightly attached sensors is uncomfortable for daily use. Using loose-fitting garments with embedded IMUs is desirable but poses challenges due to motion-corrupting sensor displacement.

Method: GID is a Transformer model structured into three stages: location-specific denoising, adaptive cross-wear fusion, and pose prediction. It includes a spatio-temporal backbone and per-IMU expert modules for local garment dynamics.

Result: Experiments demonstrate that GID delivers accurate, real-time results using limited training data, generalizing across different users, motions, and garments.

Conclusion: GID resolves the challenges of loose-fitting garment MoCap systems, improving state-of-the-art methods as a drop-in module for real-time, accurate results.

Abstract: Wearable inertial motion capture (MoCap) provides a portable, occlusion-free, and privacy-preserving alternative to camera-based systems, but its accuracy depends on tightly attached sensors - an intrusive and uncomfortable requirement for daily use. Embedding IMUs into loose-fitting garments is a desirable alternative, yet sensor-body displacement introduces severe, structured, and location-dependent corruption that breaks standard inertial pipelines. We propose GID (Garment Inertial Denoiser), a lightweight, plug-and-play Transformer that factorizes loose-wear MoCap into three stages: (i) location-specific denoising, (ii) adaptive cross-wear fusion, and (iii) general pose prediction. GID uses a location-aware expert architecture, where a shared spatio-temporal backbone models global motion while per-IMU expert heads specialize in local garment dynamics, and a lightweight fusion module ensures cross-part consistency. This inductive bias enables stable training and effective learning from limited paired loose-tight IMU data. We also introduce GarMoCap, a combined public and newly collected dataset covering diverse users, motions, and garments. Experiments show that GID enables accurate, real-time denoising from single-user training and generalizes across unseen users, motions, and garment types, consistently improving state-of-the-art inertial MoCap methods when used as a drop-in module.

</details>


### [199] [Unsupervised SE(3) Disentanglement for in situ Macromolecular Morphology Identification from Cryo-Electron Tomography](https://arxiv.org/abs/2601.01364)
*Mostofa Rafid Uddin,Mahek Vora,Qifeng Wu,Muyuan Chen,Min Xu*

Main category: cs.CV

TL;DR: This paper introduces a disentangled deep learning framework to improve cryo-ET data analysis by separating shape transformations from morphology, enabling better identification of macromolecular structures.


<details>
  <summary>Details</summary>
Motivation: Current cryo-ET methods for analyzing macromolecule morphology often overlook rare morphologies and demand labor-intensive hyperparameter tuning, limiting their efficacy.

Method: The authors propose a novel deep representation learning framework incorporating a multi-choice learning module to disentangle SE(3) transformations and morphology for highly noisy cryo-ET data.

Result: The framework outperforms traditional methods, excelling in simulated and real cryo-ET datasets, and reveals previously unidentified macromolecular morphologies.

Conclusion: This approach advances cryo-ET analysis, enhancing the ability to detect and characterize diverse macromolecular structures with reduced manual effort.

Abstract: Cryo-electron tomography (cryo-ET) provides direct 3D visualization of macromolecules inside the cell, enabling analysis of their in situ morphology. This morphology can be regarded as an SE(3)-invariant, denoised volumetric representation of subvolumes extracted from tomograms. Inferring morphology is therefore an inverse problem of estimating both a template morphology and its SE(3) transformation. Existing expectation-maximization based solution to this problem often misses rare but important morphologies and requires extensive manual hyperparameter tuning. Addressing this issue, we present a disentangled deep representation learning framework that separates SE(3) transformations from morphological content in the representation space. The framework includes a novel multi-choice learning module that enables this disentanglement for highly noisy cryo-ET data, and the learned morphological content is used to generate template morphologies. Experiments on simulated and real cryo-ET datasets demonstrate clear improvements over prior methods, including the discovery of previously unidentified macromolecular morphologies.

</details>


### [200] [ParkGaussian: Surround-view 3D Gaussian Splatting for Autonomous Parking](https://arxiv.org/abs/2601.01386)
*Xiaobao Wei,Zhangjie Ye,Yuxiang Gu,Zunjie Zhu,Yunfei Guo,Yingying Shen,Shan Zhao,Ming Lu,Haiyang Sun,Bing Wang,Guang Chen,Rongfeng Lu,Hangjun Ye*

Main category: cs.CV

TL;DR: The study addresses the challenges of parking slot perception in autonomous driving by introducing ParkRecon3D, a benchmark for parking scene reconstruction, and ParkGaussian, a novel 3D Gaussian Splatting framework.


<details>
  <summary>Details</summary>
Motivation: Autonomous parking in crowded and GPS-denied environments requires advanced parking slot perception and geometry understanding, and existing works inadequately tackle 3D reconstruction needs for this domain.

Method: The authors present ParkRecon3D—a new benchmark with sensor data and annotations—and propose ParkGaussian, which applies 3D Gaussian Splatting alongside a slot-aware reconstruction strategy to improve the synthesis of parking slots.

Result: Experiments using the ParkRecon3D data show that ParkGaussian achieves superior reconstruction quality and enhances downstream parking slot detection tasks.

Conclusion: ParkGaussian and the ParkRecon3D dataset advance parking scene 3D reconstruction, improving both visual quality and perception consistency for autonomous parking tasks, further aiding ADS performance.

Abstract: Parking is a critical task for autonomous driving systems (ADS), with unique challenges in crowded parking slots and GPS-denied environments. However, existing works focus on 2D parking slot perception, mapping, and localization, 3D reconstruction remains underexplored, which is crucial for capturing complex spatial geometry in parking scenarios. Naively improving the visual quality of reconstructed parking scenes does not directly benefit autonomous parking, as the key entry point for parking is the slots perception module. To address these limitations, we curate the first benchmark named ParkRecon3D, specifically designed for parking scene reconstruction. It includes sensor data from four surround-view fisheye cameras with calibrated extrinsics and dense parking slot annotations. We then propose ParkGaussian, the first framework that integrates 3D Gaussian Splatting (3DGS) for parking scene reconstruction. To further improve the alignment between reconstruction and downstream parking slot detection, we introduce a slot-aware reconstruction strategy that leverages existing parking perception methods to enhance the synthesis quality of slot regions. Experiments on ParkRecon3D demonstrate that ParkGaussian achieves state-of-the-art reconstruction quality and better preserves perception consistency for downstream tasks. The code and dataset will be released at: https://github.com/wm-research/ParkGaussian

</details>


### [201] [Evaluation of Convolutional Neural Network For Image Classification with Agricultural and Urban Datasets](https://arxiv.org/abs/2601.01393)
*Shamik Shafkat Avro,Nazira Jesmin Lina,Shahanaz Sharmin*

Main category: cs.CV

TL;DR: The paper develops and evaluates a custom Convolutional Neural Network (CustomCNN) incorporating advanced design features to improve multi-domain image classifications. It achieves competitive performance on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: To study how architectural design choices in neural networks impact the performance of multi-domain image classification tasks, particularly in Smart City and agricultural imaging.

Method: The authors proposed a CustomCNN model using residual connections, Squeeze-and-Excitation attention mechanisms, progressive channel scaling, and Kaiming initialization. Five publicly available datasets were used to train and validate its performance. Comparative analysis with other CNN architectures was also conducted.

Result: The CustomCNN demonstrated competitive performance while maintaining computational efficiency, highlighting its design's effectiveness for Smart City and agricultural applications.

Conclusion: Thoughtful design of CNN architectures can improve their performance in diverse image classification tasks, making them suitable for practical applications in Smart Cities and agriculture.

Abstract: This paper presents the development and evaluation of a custom Convolutional Neural Network (CustomCNN) created to study how architectural design choices affect multi-domain image classification tasks. The network uses residual connections, Squeeze-and-Excitation attention mechanisms, progressive channel scaling, and Kaiming initialization to improve its ability to represent data and speed up training. The model is trained and tested on five publicly available datasets: unauthorized vehicle detection, footpath encroachment detection, polygon-annotated road damage and manhole detection, MangoImageBD and PaddyVarietyBD. A comparison with popular CNN architectures shows that the CustomCNN delivers competitive performance while remaining efficient in computation. The results underscore the importance of thoughtful architectural design for real-world Smart City and agricultural imaging applications.

</details>


### [202] [SwinIFS: Landmark Guided Swin Transformer For Identity Preserving Face Super Resolution](https://arxiv.org/abs/2601.01406)
*Habiba Kausar,Saeed Anwar,Omar Jamal Hammad,Abdul Bais*

Main category: cs.CV

TL;DR: SwinIFS is a landmark-guided framework for face super-resolution that effectively restores detailed facial images using structural priors and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of recovering high-quality facial images from severely degraded low-resolution inputs, which often lose structural details and identity-specific features.

Method: SwinIFS incorporates dense Gaussian heatmaps of facial landmarks into input representation and uses a compact Swin Transformer backbone to enhance both local geometry and long-range contextual understanding.

Result: Extensive experiments on the CelebA benchmark demonstrate superior perceptual quality, sharper reconstructions, improved identity retention, and robust performance at high magnification levels like 8x.

Conclusion: SwinIFS delivers photorealistic face super-resolution results that balance accuracy and computational efficiency, proving useful for applications in facial enhancement, surveillance, and digital restoration.

Abstract: Face super-resolution aims to recover high-quality facial images from severely degraded low-resolution inputs, but remains challenging due to the loss of fine structural details and identity-specific features. This work introduces SwinIFS, a landmark-guided super-resolution framework that integrates structural priors with hierarchical attention mechanisms to achieve identity-preserving reconstruction at both moderate and extreme upscaling factors. The method incorporates dense Gaussian heatmaps of key facial landmarks into the input representation, enabling the network to focus on semantically important facial regions from the earliest stages of processing. A compact Swin Transformer backbone is employed to capture long-range contextual information while preserving local geometry, allowing the model to restore subtle facial textures and maintain global structural consistency. Extensive experiments on the CelebA benchmark demonstrate that SwinIFS achieves superior perceptual quality, sharper reconstructions, and improved identity retention; it consistently produces more photorealistic results and exhibits strong performance even under 8x magnification, where most methods fail to recover meaningful structure. SwinIFS also provides an advantageous balance between reconstruction accuracy and computational efficiency, making it suitable for real-world applications in facial enhancement, surveillance, and digital restoration. Our code, model weights, and results are available at https://github.com/Habiba123-stack/SwinIFS.

</details>


### [203] [Mask-Guided Multi-Task Network for Face Attribute Recognition](https://arxiv.org/abs/2601.01408)
*Gong Gao,Zekai Wang,Jian Zhao,Ziqi Xie,Xianhui Liu,Weidong Zhao*

Main category: cs.CV

TL;DR: This paper proposes a Mask-Guided Multi-Task Network (MGMTN) using adaptive masks and feature fusion to improve face attribute recognition.


<details>
  <summary>Details</summary>
Motivation: Face Attribute Recognition (FAR) is essential for several applications but current methods produce redundant features by relying on global regions.

Method: The paper introduces MGMTN which uses Adaptive Mask Learning (AML) to localize critical facial parts and Group-Global Feature Fusion (G2FF) to combine group and global features.

Result: Experiments on challenging datasets show that MGMTN significantly enhances FAR performance.

Conclusion: MGMTN effectively addresses limitations in conventional FAR methods by focusing on meaningful regions and efficiently combining features for improved recognition accuracy.

Abstract: Face Attribute Recognition (FAR) plays a crucial role in applications such as person re-identification, face retrieval, and face editing. Conventional multi-task attribute recognition methods often process the entire feature map for feature extraction and attribute classification, which can produce redundant features due to reliance on global regions. To address these challenges, we propose a novel approach emphasizing the selection of specific feature regions for efficient feature learning. We introduce the Mask-Guided Multi-Task Network (MGMTN), which integrates Adaptive Mask Learning (AML) and Group-Global Feature Fusion (G2FF) to address the aforementioned limitations. Leveraging a pre-trained keypoint annotation model and a fully convolutional network, AML accurately localizes critical facial parts (e.g., eye and mouth groups) and generates group masks that delineate meaningful feature regions, thereby mitigating negative transfer from global region usage. Furthermore, G2FF combines group and global features to enhance FAR learning, enabling more precise attribute identification. Extensive experiments on two challenging facial attribute recognition datasets demonstrate the effectiveness of MGMTN in improving FAR performance.

</details>


### [204] [AirSpatialBot: A Spatially-Aware Aerial Agent for Fine-Grained Vehicle Attribute Recognization and Retrieval](https://arxiv.org/abs/2601.01416)
*Yue Zhou,Ran Ding,Xue Yang,Xue Jiang,Xingzhao Liu*

Main category: cs.CV

TL;DR: Advancements are made in remote sensing Vision-Language Models (VLMs) to improve spatial understanding with the introduction of AirSpatial dataset and AirSpatialBot for vehicle imagery from drones.


<details>
  <summary>Details</summary>
Motivation: Current remote sensing VLMs struggle with spatial understanding, which limits real-world applications, especially in analyzing drone-captured vehicle imagery.

Method: Developed AirSpatial dataset with over 206K instructions and two novel tasks, coupled with a two-stage training strategy (Image Understanding Pre-training and Spatial Understanding Fine-tuning). Created AirSpatialBot leveraging this trained model.

Result: Experimental validation shows enhanced spatial understanding under diverse queries and exposes limitations of current VLMs. Achieved fine-grained vehicle attribute recognition and spatial querying improvements.

Conclusion: The approach advances VLMs' spatial capabilities in remote sensing, offering insights and resources to address spatial challenges in drone imagery analysis.

Abstract: Despite notable advancements in remote sensing vision-language models (VLMs), existing models often struggle with spatial understanding, limiting their effectiveness in real-world applications. To push the boundaries of VLMs in remote sensing, we specifically address vehicle imagery captured by drones and introduce a spatially-aware dataset AirSpatial, which comprises over 206K instructions and introduces two novel tasks: Spatial Grounding and Spatial Question Answering. It is also the first remote sensing grounding dataset to provide 3DBB. To effectively leverage existing image understanding of VLMs to spatial domains, we adopt a two-stage training strategy comprising Image Understanding Pre-training and Spatial Understanding Fine-tuning. Utilizing this trained spatially-aware VLM, we develop an aerial agent, AirSpatialBot, which is capable of fine-grained vehicle attribute recognition and retrieval. By dynamically integrating task planning, image understanding, spatial understanding, and task execution capabilities, AirSpatialBot adapts to diverse query requirements. Experimental results validate the effectiveness of our approach, revealing the spatial limitations of existing VLMs while providing valuable insights. The model, code, and datasets will be released at https://github.com/VisionXLab/AirSpatialBot

</details>


### [205] [DreamID-V:Bridging the Image-to-Video Gap for High-Fidelity Face Swapping via Diffusion Transformer](https://arxiv.org/abs/2601.01425)
*Xu Guo,Fulong Ye,Xinghui Li,Pengqi Tu,Pengze Zhang,Qichao Sun,Songtao Zhao,Xiangwang Hou,Qian He*

Main category: cs.CV

TL;DR: This paper presents DreamID-V, a diffusion transformer-based framework for improved video face swapping with superior identity similarity, attribute preservation, and temporal consistency. A new SyncID-Pipe data pipeline is introduced for explicit supervision, alongside innovative strategies for realism and identity consistency.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limitations of current video face swapping methods, particularly challenges in achieving identity similarity, attribute preservation, and temporal consistency, while leveraging advances in image face swapping.

Method: The authors propose the SyncID-Pipe data pipeline to pre-train an Identity-Anchored Video Synthesizer and integrate it with image face swapping models. They utilize a Diffusion Transformer-based framework with a Modality-Aware Conditioning module, synthetic-to-real curriculum mechanism, and identity-coherence reinforcement learning for enhanced results.

Result: Through extensive experiments, DreamID-V outperforms state-of-the-art video face swapping methods. It also showcases versatility, easily adapting to various swap-related tasks.

Conclusion: DreamID-V advances the field of video face swapping by ensuring superior identity similarity, attribute preservation, and temporal consistency, while introducing new benchmarks and methodologies for enhanced realism and consistency.

Abstract: Video Face Swapping (VFS) requires seamlessly injecting a source identity into a target video while meticulously preserving the original pose, expression, lighting, background, and dynamic information. Existing methods struggle to maintain identity similarity and attribute preservation while preserving temporal consistency. To address the challenge, we propose a comprehensive framework to seamlessly transfer the superiority of Image Face Swapping (IFS) to the video domain. We first introduce a novel data pipeline SyncID-Pipe that pre-trains an Identity-Anchored Video Synthesizer and combines it with IFS models to construct bidirectional ID quadruplets for explicit supervision. Building upon paired data, we propose the first Diffusion Transformer-based framework DreamID-V, employing a core Modality-Aware Conditioning module to discriminatively inject multi-model conditions. Meanwhile, we propose a Synthetic-to-Real Curriculum mechanism and an Identity-Coherence Reinforcement Learning strategy to enhance visual realism and identity consistency under challenging scenarios. To address the issue of limited benchmarks, we introduce IDBench-V, a comprehensive benchmark encompassing diverse scenes. Extensive experiments demonstrate DreamID-V outperforms state-of-the-art methods and further exhibits exceptional versatility, which can be seamlessly adapted to various swap-related tasks.

</details>


### [206] [EdgeNeRF: Edge-Guided Regularization for Neural Radiance Fields from Sparse Views](https://arxiv.org/abs/2601.01431)
*Weiqi Yu,Yiyang Yao,Lin He,Jianming Lv*

Main category: cs.CV

TL;DR: Neural Radiance Fields (NeRF) struggle with sparse-view reconstruction leading to artifacts. EdgeNeRF introduces an edge-guided algorithm enhancing geometric consistency and preserving boundary details.


<details>
  <summary>Details</summary>
Motivation: Sparse-view reconstruction in NeRF suffers from artifacts, requiring solutions to improve geometric precision while retaining sharp boundaries.

Method: EdgeNeRF leverages extracted edges to apply depth and normal regularizations on non-edge regions, preserving boundary details and enhancing consistency.

Result: Experiments on LLFF and DTU datasets show EdgeNeRF excels in retaining geometric boundaries and reducing artifacts.

Conclusion: EdgeNeRF demonstrates a plug-and-play capability to improve reconstruction quality in sparse-view scenarios efficiently.

Abstract: Neural Radiance Fields (NeRF) achieve remarkable performance in dense multi-view scenarios, but their reconstruction quality degrades significantly under sparse inputs due to geometric artifacts. Existing methods utilize global depth regularization to mitigate artifacts, leading to the loss of geometric boundary details. To address this problem, we propose EdgeNeRF, an edge-guided sparse-view 3D reconstruction algorithm. Our method leverages the prior that abrupt changes in depth and normals generate edges. Specifically, we first extract edges from input images, then apply depth and normal regularization constraints to non-edge regions, enhancing geometric consistency while preserving high-frequency details at boundaries. Experiments on LLFF and DTU datasets demonstrate EdgeNeRF's superior performance, particularly in retaining sharp geometric boundaries and suppressing artifacts. Additionally, the proposed edge-guided depth regularization module can be seamlessly integrated into other methods in a plug-and-play manner, significantly improving their performance without substantially increasing training time. Code is available at https://github.com/skyhigh404/edgenerf.

</details>


### [207] [In defense of the two-stage framework for open-set domain adaptive semantic segmentation](https://arxiv.org/abs/2601.01439)
*Wenqi Ren,Weijie Wang,Meng Zheng,Ziyan Wu,Yang Tang,Zhun Zhong,Nicu Sebe*

Main category: cs.CV

TL;DR: The paper addresses open-set domain adaptation for semantic segmentation by proposing a two-step training strategy "SATS" for separating then adapting known/unknown classes, complemented with an innovative hard unknown exploration method.


<details>
  <summary>Details</summary>
Motivation: To solve the imbalance in annotation between known and unknown classes during open-set domain adaptation, which causes negative transfer for known classes and underfitting for unknowns.

Method: The authors introduce a two-step approach, "SATS," that starts with known/unknown class separation followed by unknown-aware domain adaptation. They also propose a data augmentation technique called hard unknown exploration to improve model performance.

Result: The proposed method outperforms state-of-the-art methods with a notable +3.85% H-Score improvement on GTA5-to-Cityscapes and +18.64% on SYNTHIA-to-Cityscapes benchmarks.

Conclusion: SATS and hard unknown exploration provide an effective, innovative framework for solving open-set domain adaptation for semantic segmentation by improving the model's ability to handle both known and unknown classes.

Abstract: Open-Set Domain Adaptation for Semantic Segmentation (OSDA-SS) presents a significant challenge, as it requires both domain adaptation for known classes and the distinction of unknowns. Existing methods attempt to address both tasks within a single unified stage. We question this design, as the annotation imbalance between known and unknown classes often leads to negative transfer of known classes and underfitting for unknowns. To overcome these issues, we propose SATS, a Separating-then-Adapting Training Strategy, which addresses OSDA-SS through two sequential steps: known/unknown separation and unknown-aware domain adaptation. By providing the model with more accurate and well-aligned unknown classes, our method ensures a balanced learning of discriminative features for both known and unknown classes, steering the model toward discovering truly unknown objects. Additionally, we present hard unknown exploration, an innovative data augmentation method that exposes the model to more challenging unknowns, strengthening its ability to capture more comprehensive understanding of target unknowns. We evaluate our method on public OSDA-SS benchmarks. Experimental results demonstrate that our method achieves a substantial advancement, with a +3.85% H-Score improvement for GTA5-to-Cityscapes and +18.64% for SYNTHIA-to-Cityscapes, outperforming previous state-of-the-art methods.

</details>


### [208] [PartImageNet++ Dataset: Enhancing Visual Models with High-Quality Part Annotations](https://arxiv.org/abs/2601.01454)
*Xiao Li,Zilong Liu,Yining Liu,Zhuhong Li,Na Dong,Sitian Qin,Xiaolin Hu*

Main category: cs.CV

TL;DR: The paper introduces PartImageNet++ (PIN++), a dataset with comprehensive part annotations for all ImageNet-1K categories. Using this dataset, the authors propose a Multi-scale Part-supervised Model for classification and other downstream tasks, demonstrating improved model performance.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality part annotations in existing datasets limits the development of robust object recognition models and other related tasks.

Method: The authors introduced PIN++, a dataset with detailed part annotations for ImageNet-1K. A part segmentation network trained on PIN++ generated pseudo part labels for unannotated images. They combined conventional recognition architecture with auxiliary layers supervised jointly by pseudo part labels and original annotations.

Result: Extensive experiments using PIN++ showed enhanced performance in part-based object recognition and downstream tasks like part segmentation, object segmentation, and few-shot learning.

Conclusion: The proposed PIN++ dataset and Multi-scale Part-supervised Model (MPM) significantly improve object recognition and provide strong benchmarks for related tasks, reaffirming the value of part annotations in model enhancement.

Abstract: To address the scarcity of high-quality part annotations in existing datasets, we introduce PartImageNet++ (PIN++), a dataset that provides detailed part annotations for all categories in ImageNet-1K. With 100 annotated images per category, totaling 100K images, PIN++ represents the most comprehensive dataset covering a diverse range of object categories. Leveraging PIN++, we propose a Multi-scale Part-supervised recognition Model (MPM) for robust classification on ImageNet-1K. We first trained a part segmentation network using PIN++ and used it to generate pseudo part labels for the remaining unannotated images. MPM then integrated a conventional recognition architecture with auxiliary bypass layers, jointly supervised by both pseudo part labels and the original part annotations. Furthermore, we conducted extensive experiments on PIN++, including part segmentation, object segmentation, and few-shot learning, exploring various ways to leverage part annotations in downstream tasks. Experimental results demonstrated that our approach not only enhanced part-based models for robust object recognition but also established strong baselines for multiple downstream tasks, highlighting the potential of part annotations in improving model performance. The dataset and the code are available at https://github.com/LixiaoTHU/PartImageNetPP.

</details>


### [209] [Rethinking Multimodal Few-Shot 3D Point Cloud Segmentation: From Fused Refinement to Decoupled Arbitration](https://arxiv.org/abs/2601.01456)
*Wentao Bian,Fenglei Xu*

Main category: cs.CV

TL;DR: The paper introduces the DA-FSS model for multimodal few-shot 3D point cloud semantic segmentation, which addresses plasticity-stability conflicts and semantic blindness through decoupled pathways for geometric and semantic processing.


<details>
  <summary>Details</summary>
Motivation: To solve issues in few-shot 3D point cloud semantic segmentation, particularly the struggles in balancing plasticity and stability conflicts, and overcoming semantic blindness caused by inter-class confusion in multimodal semantic segmentation setups.

Method: The paper proposes the DA-FSS model, which uses a Parallel Expert Refinement module along with a Stacked Arbitration and Decoupled Alignment modules to decouple semantic and geometric pathways. The model regularizes gradients to enhance generalization and leverages pre-trained text encoders for better modality utilization.

Result: DA-FSS outperforms the baseline MM-FSS in segmentation results on datasets like S3DIS and ScanNet. It provides improved geometric boundaries, completeness, and texture differentiation.

Conclusion: DA-FSS effectively addresses challenges in multimodal FS-PCS by leveraging a decoupled pathway approach, showing better generalization and segmentation accuracy compared to previous models.

Abstract: In this paper, we revisit multimodal few-shot 3D point cloud semantic segmentation (FS-PCS), identifying a conflict in "Fuse-then-Refine" paradigms: the "Plasticity-Stability Dilemma." In addition, CLIP's inter-class confusion can result in semantic blindness. To address these issues, we present the Decoupled-experts Arbitration Few-Shot SegNet (DA-FSS), a model that effectively distinguishes between semantic and geometric paths and mutually regularizes their gradients to achieve better generalization. DA-FSS employs the same backbone and pre-trained text encoder as MM-FSS to generate text embeddings, which can increase free modalities' utilization rate and better leverage each modality's information space. To achieve this, we propose a Parallel Expert Refinement module to generate each modal correlation. We also propose a Stacked Arbitration Module (SAM) to perform convolutional fusion and arbitrate correlations for each modality pathway. The Parallel Experts decouple two paths: a Geometric Expert maintains plasticity, and a Semantic Expert ensures stability. They are coordinated via a Decoupled Alignment Module (DAM) that transfers knowledge without propagating confusion. Experiments on popular datasets (S3DIS, ScanNet) demonstrate the superiority of DA-FSS over MM-FSS. Meanwhile, geometric boundaries, completeness, and texture differentiation are all superior to the baseline. The code is available at: https://github.com/MoWenQAQ/DA-FSS.

</details>


### [210] [Language as Prior, Vision as Calibration: Metric Scale Recovery for Monocular Depth Estimation](https://arxiv.org/abs/2601.01457)
*Mingxing Zhan,Li Zhang,Beibei Wang,Yingjie Wang,Zenglin Shi*

Main category: cs.CV

TL;DR: This paper addresses monocular metric depth estimation challenges using a frozen-backbone calibration approach that combines image-specific affine transforms and uncertainty-aware envelopes derived from language cues.


<details>
  <summary>Details</summary>
Motivation: Metric depth estimation from monocular images is inherently challenging due to global scale ambiguity and sensitivity to domain shifts. This work aims to improve cross-domain robustness and accuracy in depth estimation.

Method: An image-specific affine transform in inverse depth is applied alongside lightweight calibration heads. Additionally, language cues predict an uncertainty-aware envelope bounding calibration parameters, and pooled multi-scale visual features select image-specific calibrations.

Result: The approach improves in-domain metric depth estimation accuracy on NYUv2 and KITTI datasets. It also shows robust zero-shot transfer performance on SUN-RGBD and DDAD, outperforming language-only baselines.

Conclusion: The proposed method leverages frozen-backbone calibration and uncertainty-aware envelopes from language cues to enhance metric depth estimation across domains, demonstrating improved robustness and accuracy.

Abstract: Relative-depth foundation models transfer well, yet monocular metric depth remains ill-posed due to unidentifiable global scale and heightened domain-shift sensitivity. Under a frozen-backbone calibration setting, we recover metric depth via an image-specific affine transform in inverse depth and train only lightweight calibration heads while keeping the relative-depth backbone and the CLIP text encoder fixed. Since captions provide coarse but noisy scale cues that vary with phrasing and missing objects, we use language to predict an uncertainty-aware envelope that bounds feasible calibration parameters in an unconstrained space, rather than committing to a text-only point estimate. We then use pooled multi-scale frozen visual features to select an image-specific calibration within this envelope. During training, a closed-form least-squares oracle in inverse depth provides per-image supervision for learning the envelope and the selected calibration. Experiments on NYUv2 and KITTI improve in-domain accuracy, while zero-shot transfer to SUN-RGBD and DDAD demonstrates improved robustness over strong language-only baselines.

</details>


### [211] [Domain Adaptation of Carotid Ultrasound Images using Generative Adversarial Network](https://arxiv.org/abs/2601.01460)
*Mohd Usama,Belal Ahmad,Christer Gronlund,Faleh Menawer R Althiyabi*

Main category: cs.CV

TL;DR: Proposed a GAN-based model for domain adaptation in ultrasound images, addressing texture variations and reverberation noise, outperforming traditional CycleGAN methods.


<details>
  <summary>Details</summary>
Motivation: To mitigate performance issues in medical imaging caused by diverse textures and noise across devices and settings, enhancing efficiency without retraining for each setting.

Method: Developed a GAN-based approach for image-to-image translation to modify texture patterns and remove reverberation noise while preserving image content in ultrasound images.

Result: The model successfully translated texture patterns and removed noise, attaining superior domain adaptation metrics compared to no adaptation techniques and CycleGAN comparative studies.

Conclusion: The GAN-based model demonstrated effective domain adaptation in ultrasound imaging across varying data sources, presenting an innovative and cost-efficient solution in medical imaging.

Abstract: Deep learning has been extensively used in medical imaging applications, assuming that the test and training datasets belong to the same probability distribution. However, a common challenge arises when working with medical images generated by different systems or even the same system with different parameter settings. Such images contain diverse textures and reverberation noise that violate the aforementioned assumption. Consequently, models trained on data from one device or setting often struggle to perform effectively with data from other devices or settings. In addition, retraining models for each specific device or setting is labor-intensive and costly. To address these issues in ultrasound images, we propose a novel Generative Adversarial Network (GAN)-based model. We formulated the domain adaptation tasks as an image-to-image translation task, in which we modified the texture patterns and removed reverberation noise in the test data images from the source domain to align with those in the target domain images while keeping the image content unchanged. We applied the proposed method to two datasets containing carotid ultrasound images from three different domains. The experimental results demonstrate that the model successfully translated the texture pattern of images and removed reverberation noise from the ultrasound images. Furthermore, we evaluated the CycleGAN approaches for a comparative study with the proposed model. The experimental findings conclusively demonstrated that the proposed model achieved domain adaptation (histogram correlation (0.960 (0.019), & 0.920 (0.043) and bhattacharya distance (0.040 (0.020), & 0.085 (0.048)), compared to no adaptation (0.916 (0.062) & 0.890 (0.077), 0.090 (0.070) & 0.121 (0.095)) for both datasets.

</details>


### [212] [Robust Ship Detection and Tracking Using Modified ViBe and Backwash Cancellation Algorithm](https://arxiv.org/abs/2601.01481)
*Mohammad Hassan Saghafi,Seyed Majid Noorhosseini,Seyed Abolfazl Seyed Javadein,Hadi Khalili*

Main category: cs.CV

TL;DR: The paper proposes a robust ship detection and tracking method for unpredictable coastal video sequences using a modified ViBe algorithm and a novel backwash cancellation technique.


<details>
  <summary>Details</summary>
Motivation: Coastal environments are dynamic and unpredictable, making it critical to develop robust methods for real-time ship detection and tracking that can adapt to these challenging conditions.

Method: The authors modify the ViBe algorithm for moving object detection to minimize the probability of losing ships and improve robustness against sea waves and light variations. They also introduce a new method for backwash cancellation based on geometrical ship properties and brightness distortion.

Result: The experimental results demonstrate high performance, real-time operation, and precision of the proposed methods for ship detection and tracking.

Conclusion: The proposed approaches are effective and reliable for detecting and tracking ships in coastal scenarios, addressing challenges such as natural sea waves, lighting variations, and backwash interference.

Abstract: In this paper, we propose a robust real time detection and tracking method for detecting ships in a coastal video sequences. Since coastal scenarios are unpredictable and scenes have dynamic properties it is essential to apply detection methods that are robust to these conditions. This paper presents modified ViBe for moving object detection which detects ships and backwash. In the modified ViBe the probability of losing ships is decreased in comparison with the original ViBe. It is robust to natural sea waves and variation of lights and is capable of quickly updating the background. Based on geometrical properties of ship and some concepts such as brightness distortion, a new method for backwash cancellation is proposed. Experimental results demonstrate that the proposed strategy and methods have outstanding performance in ship detection and tracking. These results also illustrate real time and precise performance of the proposed strategy.

</details>


### [213] [Unified Generation and Self-Verification for Vision-Language Models via Advantage Decoupled Preference Optimization](https://arxiv.org/abs/2601.01483)
*Xinyu Qiu,Heng Jia,Zhengwen Zeng,Shuheng Shen,Changhua Meng,Yi Yang,Linchao Zhu*

Main category: cs.CV

TL;DR: The paper introduces ADPO, a unified RL framework for joint answer generation and self-verification, improving efficiency and performance metrics significantly.


<details>
  <summary>Details</summary>
Motivation: To address the high training and inference costs of separate generation and verification models in parallel test-time scaling.

Method: The proposed method, ADPO, jointly learns answer generation and verification by using two innovations: preference verification reward and decoupled optimization mechanism. This involves computing decision thresholds for verification scores and isolating gradients for generation and verification.

Result: ADPO achieves significant improvements such as +34.1% verification AUC, -53.5% inference time reduction, and improvements in multiple benchmarks like MathVista, ReasonSeg, and AndroidControl.

Conclusion: ADPO effectively combines answer generation and verification in a single policy, boosting performance and reducing costs while maintaining generation quality.

Abstract: Parallel test-time scaling typically trains separate generation and verification models, incurring high training and inference costs. We propose Advantage Decoupled Preference Optimization (ADPO), a unified reinforcement learning framework that jointly learns answer generation and self-verification within a single policy. ADPO introduces two innovations: a preference verification reward improving verification capability and a decoupled optimization mechanism enabling synergistic optimization of generation and verification. Specifically, the preference verification reward computes mean verification scores from positive and negative samples as decision thresholds, providing positive feedback when prediction correctness aligns with answer correctness. Meanwhile, the advantage decoupled optimization computes separate advantages for generation and verification, applies token masks to isolate gradients, and combines masked GRPO objectives, preserving generation quality while calibrating verification scores. ADPO achieves up to +34.1% higher verification AUC and -53.5% lower inference time, with significant gains of +2.8%/+1.4% accuracy on MathVista/MMMU, +1.9 cIoU on ReasonSeg, and +1.7%/+1.0% step success rate on AndroidControl/GUI Odyssey.

</details>


### [214] [Higher-Order Domain Generalization in Magnetic Resonance-Based Assessment of Alzheimer's Disease](https://arxiv.org/abs/2601.01485)
*Zobia Batool,Diala Lteif,Vijaya B. Kolachalama,Huseyin Ozkan,Erchan Aptoula*

Main category: cs.CV

TL;DR: This paper addresses the challenge of domain shifts in Alzheimer's diagnostics using sMRI data and proposes Extended MixStyle (EM), achieving improved generalization across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: Domain shifts caused by scanner, protocol, and demographic variations hinder accurate Alzheimer's detection using structural MRI data. There's a need for robust, generalizable methods for real-world diagnostics.

Method: Proposed Extended MixStyle (EM), blending higher-order feature moments (skewness and kurtosis) to handle diverse distributional variations. Trained on sMRI data to classify Alzheimer's and mild cognitive impairment from normal cognition.

Result: EM demonstrated enhanced generalization across unseen cohorts, improving macro-F1 by 2.4 percentage points over state-of-the-art single-domain generalization benchmarks.

Conclusion: Extended MixStyle proves effective for cross-domain Alzheimer's detection, addressing dataset heterogeneity challenges and improving classification robustness in real-world scenarios.

Abstract: Despite progress in deep learning for Alzheimer's disease (AD) diagnostics, models trained on structural magnetic resonance imaging (sMRI) often do not perform well when applied to new cohorts due to domain shifts from varying scanners, protocols and patient demographics. AD, the primary driver of dementia, manifests through progressive cognitive and neuroanatomical changes like atrophy and ventricular expansion, making robust, generalizable classification essential for real-world use. While convolutional neural networks and transformers have advanced feature extraction via attention and fusion techniques, single-domain generalization (SDG) remains underexplored yet critical, given the fragmented nature of AD datasets. To bridge this gap, we introduce Extended MixStyle (EM), a framework for blending higher-order feature moments (skewness and kurtosis) to mimic diverse distributional variations. Trained on sMRI data from the National Alzheimer's Coordinating Center (NACC; n=4,647) to differentiate persons with normal cognition (NC) from those with mild cognitive impairment (MCI) or AD and tested on three unseen cohorts (total n=3,126), EM yields enhanced cross-domain performance, improving macro-F1 on average by 2.4 percentage points over state-of-the-art SDG benchmarks, underscoring its promise for invariant, reliable AD detection in heterogeneous real-world settings. The source code will be made available upon acceptance at https://github.com/zobia111/Extended-Mixstyle.

</details>


### [215] [DeepInv: A Novel Self-supervised Learning Approach for Fast and Accurate Diffusion Inversion](https://arxiv.org/abs/2601.01487)
*Ziyue Zhang,Luxi Lin,Xiaolin Hu,Chao Chang,HuaiXi Wang,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: This paper proposes DeepInv, a self-supervised diffusion inversion approach to accurately and efficiently recover image noise in diffusion models.


<details>
  <summary>Details</summary>
Motivation: The lack of viable supervision signals for diffusion inversion challenges existing methods, reducing either performance or efficiency.

Method: DeepInv employs a self-supervised objective, data augmentation strategy, and multi-scale iterative training to create a trainable solver for inversion noise prediction.

Result: DeepInv significantly outperforms others (+40.435% SSIM and +9887.5% inference speed on COCO dataset) in terms of both performance and speed.

Conclusion: DeepInv sets a precedent with its parameterized solver, providing innovative insights into diffusion inversion and advancing the controllable image editing domain.

Abstract: Diffusion inversion is a task of recovering the noise of an image in a diffusion model, which is vital for controllable diffusion image editing. At present, diffusion inversion still remains a challenging task due to the lack of viable supervision signals. Thus, most existing methods resort to approximation-based solutions, which however are often at the cost of performance or efficiency. To remedy these shortcomings, we propose a novel self-supervised diffusion inversion approach in this paper, termed Deep Inversion (DeepInv). Instead of requiring ground-truth noise annotations, we introduce a self-supervised objective as well as a data augmentation strategy to generate high-quality pseudo noises from real images without manual intervention. Based on these two innovative designs, DeepInv is also equipped with an iterative and multi-scale training regime to train a parameterized inversion solver, thereby achieving the fast and accurate image-to-noise mapping. To the best of our knowledge, this is the first attempt of presenting a trainable solver to predict inversion noise step by step. The extensive experiments show that our DeepInv can achieve much better performance and inference speed than the compared methods, e.g., +40.435% SSIM than EasyInv and +9887.5% speed than ReNoise on COCO dataset. Moreover, our careful designs of trainable solvers can also provide insights to the community. Codes and model parameters will be released in https://github.com/potato-kitty/DeepInv.

</details>


### [216] [DiffKD-DCIS: Predicting Upgrade of Ductal Carcinoma In Situ with Diffusion Augmentation and Knowledge Distillation](https://arxiv.org/abs/2601.01507)
*Tao Li,Qing Li,Na Li,Hui Xie*

Main category: cs.CV

TL;DR: The study introduces the DiffKD-DCIS framework to predict DCIS upgrading to IDC using conditional diffusion models for ultrasound image generation, teacher-student knowledge distillation for robust learning, and compact networks for efficiency.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with limited ultrasound data and poor generalization in predicting the progression of DCIS to IDC, highlighting the need for robust and efficient solutions.

Method: The framework combines conditional diffusion models for generating synthetic ultrasound images, a deep teacher network to extract robust features, and a student network trained via knowledge distillation for generalization and efficiency.

Result: The model was evaluated on 1,435 cases and external test sets, producing high-quality synthetic images and achieving superior performance to junior radiologists while being comparable to senior radiologists.

Conclusion: The DiffKD-DCIS framework demonstrates significant clinical potential by enhancing prediction accuracy and efficiency, addressing challenges in limited data and generalization.

Abstract: Accurately predicting the upgrade of ductal carcinoma in situ (DCIS) to invasive ductal carcinoma (IDC) is crucial for surgical planning. However, traditional deep learning methods face challenges due to limited ultrasound data and poor generalization ability. This study proposes the DiffKD-DCIS framework, integrating conditional diffusion modeling with teacher-student knowledge distillation.
  The framework operates in three stages: First, a conditional diffusion model generates high-fidelity ultrasound images using multimodal conditions for data augmentation. Then, a deep teacher network extracts robust features from both original and synthetic data. Finally, a compact student network learns from the teacher via knowledge distillation, balancing generalization and computational efficiency.
  Evaluated on a multi-center dataset of 1,435 cases, the synthetic images were of good quality. The student network had fewer parameters and faster inference. On external test sets, it outperformed partial combinations, and its accuracy was comparable to senior radiologists and superior to junior ones, showing significant clinical potential.

</details>


### [217] [A Novel Deep Learning Method for Segmenting the Left Ventricle in Cardiac Cine MRI](https://arxiv.org/abs/2601.01512)
*Wenhui Chu,Aobo Jin,Hardik A. Gohel*

Main category: cs.CV

TL;DR: This paper introduces GBU-Net, a deep learning method for precise left ventricle segmentation in cine MRI.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and contextual understanding in cardiac MRI segmentation.

Method: It employs a novel group-batch-normalized U-Net architecture with enhanced down-sampling and up-sampling pathways.

Result: GBU-Net achieved 97% dice score on the SunnyBrook dataset, surpassing standard metrics.

Conclusion: GBU-Net improves precision in cardiac MRI segmentation, beneficial for medical imaging applications.

Abstract: This research aims to develop a novel deep learning network, GBU-Net, utilizing a group-batch-normalized U-Net framework, specifically designed for the precise semantic segmentation of the left ventricle in short-axis cine MRI scans. The methodology includes a down-sampling pathway for feature extraction and an up-sampling pathway for detail restoration, enhanced for medical imaging. Key modifications include techniques for better contextual understanding crucial in cardiac MRI segmentation. The dataset consists of 805 left ventricular MRI scans from 45 patients, with comparative analysis using established metrics such as the dice coefficient and mean perpendicular distance. GBU-Net significantly improves the accuracy of left ventricle segmentation in cine MRI scans. Its innovative design outperforms existing methods in tests, surpassing standard metrics like the dice coefficient and mean perpendicular distance. The approach is unique in its ability to capture contextual information, often missed in traditional CNN-based segmentation. An ensemble of the GBU-Net attains a 97% dice score on the SunnyBrook testing dataset. GBU-Net offers enhanced precision and contextual understanding in left ventricle segmentation for surgical robotics and medical analysis.

</details>


### [218] [FastV-RAG: Towards Fast and Fine-Grained Video QA with Retrieval-Augmented Generation](https://arxiv.org/abs/2601.01513)
*Gen Li,Peiyu Liu*

Main category: cs.CV

TL;DR: VideoSpeculateRAG is a new framework that enhances efficiency and reliability in Vision-Language Models by using speculative decoding and improved entity recognition, achieving faster inference with comparable or better accuracy.


<details>
  <summary>Details</summary>
Motivation: Vision-Language Models struggle with integrating external knowledge and maintaining answer quality in Retrieval-Augmented Generation (RAG).

Method: The framework introduces speculative decoding, using a lightweight model for candidate answers reviewed by a heavier model, and a similarity-based filtering strategy to improve entity alignment.

Result: VideoSpeculateRAG achieves comparable or better accuracy than standard RAG methods while reducing inference times by approximately 2x.

Conclusion: VideoSpeculateRAG showcases the advantages of speculative decoding and enhanced entity recognition in retrieval-augmented reasoning, improving performance in complex multimodal tasks.

Abstract: Vision-Language Models (VLMs) excel at visual reasoning but still struggle with integrating external knowledge. Retrieval-Augmented Generation (RAG) is a promising solution, but current methods remain inefficient and often fail to maintain high answer quality. To address these challenges, we propose VideoSpeculateRAG, an efficient VLM-based RAG framework built on two key ideas. First, we introduce a speculative decoding pipeline: a lightweight draft model quickly generates multiple answer candidates, which are then verified and refined by a more accurate heavyweight model, substantially reducing inference latency without sacrificing correctness. Second, we identify a major source of error - incorrect entity recognition in retrieved knowledge - and mitigate it with a simple yet effective similarity-based filtering strategy that improves entity alignment and boosts overall answer accuracy. Experiments demonstrate that VideoSpeculateRAG achieves comparable or higher accuracy than standard RAG approaches while accelerating inference by approximately 2x. Our framework highlights the potential of combining speculative decoding with retrieval-augmented reasoning to enhance efficiency and reliability in complex, knowledge-intensive multimodal tasks.

</details>


### [219] [BARE: Towards Bias-Aware and Reasoning-Enhanced One-Tower Visual Grounding](https://arxiv.org/abs/2601.01526)
*Hongbing Li,Linhui Xiao,Zihan Zhao,Qi Shen,Yixiang Huang,Bo Xiao,Zhanyu Ma*

Main category: cs.CV

TL;DR: This paper proposes BARE, a bias-aware and reasoning-enhanced framework for visual grounding, aiming to overcome modality bias and insufficient reasoning challenges, and achieves state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The need to address deceptive modality biases and insufficient semantic reasoning in one-tower visual grounding approaches motivated this research.

Method: The authors introduced BARE, featuring three modules: language salience modulator, visual bias correction, and referential relationship enhancement, to improve multimodal comprehension and reasoning.

Result: BARE demonstrated state-of-the-art performance and better computational efficiency across five benchmarks.

Conclusion: BARE effectively tackles the challenges in one-tower visual grounding by mitigating multimodal distractions and enhancing referential understanding, establishing itself as a leading method in the field.

Abstract: Visual Grounding (VG), which aims to locate a specific region referred to by expressions, is a fundamental yet challenging task in the multimodal understanding fields. While recent grounding transfer works have advanced the field through one-tower architectures, they still suffer from two primary limitations: (1) over-entangled multimodal representations that exacerbate deceptive modality biases, and (2) insufficient semantic reasoning that hinders the comprehension of referential cues. In this paper, we propose BARE, a bias-aware and reasoning-enhanced framework for one-tower visual grounding. BARE introduces a mechanism that preserves modality-specific features and constructs referential semantics through three novel modules: (i) language salience modulator, (ii) visual bias correction and (iii) referential relationship enhancement, which jointly mitigate multimodal distractions and enhance referential comprehension. Extensive experimental results on five benchmarks demonstrate that BARE not only achieves state-of-the-art performance but also delivers superior computational efficiency compared to existing approaches. The code is publicly accessible at https://github.com/Marloweeee/BARE.

</details>


### [220] [Improving Flexible Image Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2601.01535)
*Zixuan Fu,Lanqing Guo,Chong Wang,Binbin Song,Ding Liu,Bihan Wen*

Main category: cs.CV

TL;DR: ReToK is a novel flexible tokenizer for image representation, addressing the issue of information concentration in leading tokens by introducing Redundant Token Padding and Hierarchical Semantic Regularization. The approach improves image generation performance.


<details>
  <summary>Details</summary>
Motivation: Conventional flexible tokenizers suffer from information concentration in leading tokens, limiting their efficacy in image generation as token lengths increase.

Method: The authors propose 'ReToK', which uses Redundant Token Padding to activate trailing tokens and Hierarchical Semantic Regularization to align features with pre-trained vision models for enhanced reconstructions.

Result: ReToK outperforms both flexible and fixed-length tokenizers in image generation tasks, as demonstrated on ImageNet 256x256.

Conclusion: ReToK effectively addresses limitations of traditional flexible image tokenizers by distributing information systematically across tokens and improving generation capabilities. Code is publicly available for further exploration.

Abstract: Flexible image tokenizers aim to represent an image using an ordered 1D variable-length token sequence. This flexible tokenization is typically achieved through nested dropout, where a portion of trailing tokens is randomly truncated during training, and the image is reconstructed using the remaining preceding sequence. However, this tail-truncation strategy inherently concentrates the image information in the early tokens, limiting the effectiveness of downstream AutoRegressive (AR) image generation as the token length increases. To overcome these limitations, we propose \textbf{ReToK}, a flexible tokenizer with \underline{Re}dundant \underline{Tok}en Padding and Hierarchical Semantic Regularization, designed to fully exploit all tokens for enhanced latent modeling. Specifically, we introduce \textbf{Redundant Token Padding} to activate tail tokens more frequently, thereby alleviating information over-concentration in the early tokens. In addition, we apply \textbf{Hierarchical Semantic Regularization} to align the decoding features of earlier tokens with those from a pre-trained vision foundation model, while progressively reducing the regularization strength toward the tail to allow finer low-level detail reconstruction. Extensive experiments demonstrate the effectiveness of ReTok: on ImageNet 256$\times$256, our method achieves superior generation performance compared with both flexible and fixed-length tokenizers. Code will be available at: \href{https://github.com/zfu006/ReTok}{https://github.com/zfu006/ReTok}

</details>


### [221] [FAR-AMTN: Attention Multi-Task Network for Face Attribute Recognition](https://arxiv.org/abs/2601.01537)
*Gong Gao,Zekai Wang,Xianhui Liu,Weidong Zhao*

Main category: cs.CV

TL;DR: The paper presents FAR-AMTN, a novel approach to enhance Multi-Task Networks for Face Attribute Recognition by optimizing shared information and reducing parameter complexity.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional multi-task networks in sharing information and managing exponential parameter growth, thereby improving generalization in Face Attribute Recognition.

Method: Introduces FAR-AMTN with a Weight-Shared Group-Specific Attention module to minimize complexity, Cross-Group Feature Fusion for better attribute interaction, and a Dynamic Weighting Strategy for improved task convergence.

Result: FAR-AMTN shows superior accuracy and reduced model complexity on benchmarks like CelebA and LFWA datasets.

Conclusion: FAR-AMTN effectively improves feature sharing across tasks and achieves better performance with fewer parameters compared to existing methods.

Abstract: To enhance the generalization performance of Multi-Task Networks (MTN) in Face Attribute Recognition (FAR), it is crucial to share relevant information across multiple related prediction tasks effectively. Traditional MTN methods create shared low-level modules and distinct high-level modules, causing an exponential increase in model parameters with the addition of tasks. This approach also limits feature interaction at the high level, hindering the exploration of semantic relations among attributes, thereby affecting generalization negatively. In response, this study introduces FAR-AMTN, a novel Attention Multi-Task Network for FAR. It incorporates a Weight-Shared Group-Specific Attention (WSGSA) module with shared parameters to minimize complexity while improving group feature representation. Furthermore, a Cross-Group Feature Fusion (CGFF) module is utilized to foster interactions between attribute groups, enhancing feature learning. A Dynamic Weighting Strategy (DWS) is also introduced for synchronized task convergence. Experiments on the CelebA and LFWA datasets demonstrate that the proposed FAR-AMTN demonstrates superior accuracy with significantly fewer parameters compared to existing models.

</details>


### [222] [EscherVerse: An Open World Benchmark and Dataset for Teleo-Spatial Intelligence with Physical-Dynamic and Intent-Driven Understanding](https://arxiv.org/abs/2601.01547)
*Tianjun Gu,Chenghua Gong,Jingyu Gong,Zhizhong Zhang,Yuan Xie,Lizhuang Ma,Xin Tan*

Main category: cs.CV

TL;DR: The paper introduces Teleo-Spatial Intelligence (TSI), integrating physical and human intent reasoning for spatial changes. It presents EscherVerse, a new benchmark and dataset for evaluating reasoning in dynamic scenarios.


<details>
  <summary>Details</summary>
Motivation: Current research fails to incorporate human intent behind spatial changes, leaving a gap in holistic understanding of spatial dynamics.

Method: The authors propose Teleo-Spatial Intelligence (TSI) and create EscherVerse—a benchmark, dataset, and model series—derived from real-world videos for assessing physical-dynamic and intent-driven reasoning.

Result: EscherVerse is the first benchmark to systematically evaluate Intent-Driven Reasoning, advancing models' abilities to relate physical events to human purposes.

Conclusion: This work establishes a foundation for moving spatial intelligence toward purpose-driven understanding, enhancing AI's capability to reason about complex, human-centric scenarios.

Abstract: The ability to reason about spatial dynamics is a cornerstone of intelligence, yet current research overlooks the human intent behind spatial changes. To address these limitations, we introduce Teleo-Spatial Intelligence (TSI), a new paradigm that unifies two critical pillars: Physical-Dynamic Reasoning--understanding the physical principles of object interactions--and Intent-Driven Reasoning--inferring the human goals behind these actions. To catalyze research in TSI, we present EscherVerse, consisting of a large-scale, open-world benchmark (Escher-Bench), a dataset (Escher-35k), and models (Escher series). Derived from real-world videos, EscherVerse moves beyond constrained settings to explicitly evaluate an agent's ability to reason about object permanence, state transitions, and trajectory prediction in dynamic, human-centric scenarios. Crucially, it is the first benchmark to systematically assess Intent-Driven Reasoning, challenging models to connect physical events to their underlying human purposes. Our work, including a novel data curation pipeline, provides a foundational resource to advance spatial intelligence from passive scene description toward a holistic, purpose-driven understanding of the world.

</details>


### [223] [Beyond Patches: Global-aware Autoregressive Model for Multimodal Few-Shot Font Generation](https://arxiv.org/abs/2601.01593)
*Haonan Cai,Yuxuan Luo,Zhouhui Lian*

Main category: cs.CV

TL;DR: GAR-Font is a new autoregressive framework for few-shot font generation that integrates multimodal style encoding and global-aware tokenization to address structural and stylistic challenges.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitations of existing few-shot font generation methods, which struggle with preserving structural and stylistic coherence and neglect the use of language guidance.

Method: GAR-Font integrates a global-aware tokenizer for capturing both local and global font features, a multimodal style encoder for style control via language guidance, and a post-refinement pipeline for improving output fidelity.

Result: Experimental results show GAR-Font surpasses current methods in maintaining style coherence, structural integrity, and utilizing textual stylistic inputs effectively.

Conclusion: GAR-Font advances font generation by combining global feature awareness and multimodal approaches without heavy pretraining, offering superior results with limited data.

Abstract: Manual font design is an intricate process that transforms a stylistic visual concept into a coherent glyph set. This challenge persists in automated Few-shot Font Generation (FFG), where models often struggle to preserve both the structural integrity and stylistic fidelity from limited references. While autoregressive (AR) models have demonstrated impressive generative capabilities, their application to FFG is constrained by conventional patch-level tokenization, which neglects global dependencies crucial for coherent font synthesis. Moreover, existing FFG methods remain within the image-to-image paradigm, relying solely on visual references and overlooking the role of language in conveying stylistic intent during font design. To address these limitations, we propose GAR-Font, a novel AR framework for multimodal few-shot font generation. GAR-Font introduces a global-aware tokenizer that effectively captures both local structures and global stylistic patterns, a multimodal style encoder offering flexible style control through a lightweight language-style adapter without requiring intensive multimodal pretraining, and a post-refinement pipeline that further enhances structural fidelity and style coherence. Extensive experiments show that GAR-Font outperforms existing FFG methods, excelling in maintaining global style faithfulness and achieving higher-quality results with textual stylistic guidance.

</details>


### [224] [Guiding Token-Sparse Diffusion Models](https://arxiv.org/abs/2601.01608)
*Felix Krause,Stefan Andreas Baumann,Johannes Schusterbauer,Olga Grebenkova,Ming Gui,Vincent Tao Hu,Björn Ommer*

Main category: cs.CV

TL;DR: This paper introduces Sparse Guidance (SG), a method to address the inefficiency of sparsely trained diffusion models during inference by leveraging token-level sparsity.


<details>
  <summary>Details</summary>
Motivation: The inefficiency of sparsely trained diffusion models during inference, particularly their poor response to Classifier-free Guidance (CFG), motivates the development of strategies to improve their performance while maintaining efficiency.

Method: The proposed Sparse Guidance (SG) uses token-level sparsity instead of conditional dropout to guide diffusion models, preserving high-variance conditional predictions, leading to efficient and high-quality outputs.

Result: On the ImageNet-256 benchmark, SG achieves a 1.58 FID with 25% fewer FLOPs and enables up to 58% FLOP savings while maintaining baseline quality. Additionally, a 2.5B text-to-image model trained with training-time sparsity and SG at inference shows improved human preference and composition scores while increasing throughput.

Conclusion: Sparse Guidance (SG) effectively addresses inference inefficiencies in sparsely trained diffusion models, delivering significant improvements in quality and efficiency during image synthesis.

Abstract: Diffusion models deliver high quality in image synthesis but remain expensive during training and inference. Recent works have leveraged the inherent redundancy in visual content to make training more affordable by training only on a subset of visual information. While these methods were successful in providing cheaper and more effective training, sparsely trained diffusion models struggle in inference. This is due to their lacking response to Classifier-free Guidance (CFG) leading to underwhelming performance during inference. To overcome this, we propose Sparse Guidance (SG). Instead of using conditional dropout as a signal to guide diffusion models, SG uses token-level sparsity. As a result, SG preserves the high-variance of the conditional prediction better, achieving good quality and high variance outputs. Leveraging token-level sparsity at inference, SG improves fidelity at lower compute, achieving 1.58 FID on the commonly used ImageNet-256 benchmark with 25% fewer FLOPs, and yields up to 58% FLOP savings at matched baseline quality. To demonstrate the effectiveness of Sparse Guidance, we train a 2.5B text-to-image diffusion model using training time sparsity and leverage SG during inference. SG achieves improvements in composition and human preference score while increasing throughput at the same time.

</details>


### [225] [CAP-IQA: Context-Aware Prompt-Guided CT Image Quality Assessment](https://arxiv.org/abs/2601.01613)
*Kazi Ramisa Rifa,Jie Zhang,Abdullah Imran*

Main category: cs.CV

TL;DR: The paper introduces a Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework for CT image quality assessment. This uses medical text prompts combined with instance-level contextual prompts and causal debiasing.


<details>
  <summary>Details</summary>
Motivation: Current prompt-based methods inadequately address real-world CT image quality issues like noise and artifacts, as they rely on idealized definitions. This motivates the need for a context-aware and unbiased framework.

Method: The CAP-IQA framework integrates medical priors via text, combines them with context-aware prompts, and applies causal debiasing. A CNN visual encoder and text encoder are used for assessing image quality.

Result: CAP-IQA achieved higher accuracy (2.8590 overall correlation score) on a benchmark challenge compared to the leading framework, and demonstrated generalizability on a 91,514-image pediatric dataset.

Conclusion: The framework effectively leverages prompt-guided fusion and minimalistic design for improved feature alignment and interpretability, offering robust image quality assessment across diverse settings.

Abstract: Prompt-based methods, which encode medical priors through descriptive text, have been only minimally explored for CT Image Quality Assessment (IQA). While such prompts can embed prior knowledge about diagnostic quality, they often introduce bias by reflecting idealized definitions that may not hold under real-world degradations such as noise, motion artifacts, or scanner variability. To address this, we propose the Context-Aware Prompt-guided Image Quality Assessment (CAP-IQA) framework, which integrates text-level priors with instance-level context prompts and applies causal debiasing to separate idealized knowledge from factual, image-specific degradations. Our framework combines a CNN-based visual encoder with a domain-specific text encoder to assess diagnostic visibility, anatomical clarity, and noise perception in abdominal CT images. The model leverages radiology-style prompts and context-aware fusion to align semantic and perceptual representations. On the 2023 LDCTIQA challenge benchmark, CAP-IQA achieves an overall correlation score of 2.8590 (sum of PLCC, SROCC, and KROCC), surpassing the top-ranked leaderboard team (2.7427) by 4.24%. Moreover, our comprehensive ablation experiments confirm that prompt-guided fusion and the simplified encoder-only design jointly enhance feature alignment and interpretability. Furthermore, evaluation on an in-house dataset of 91,514 pediatric CT images demonstrates the true generalizability of CAP-IQA in assessing perceptual fidelity in a different patient population.

</details>


### [226] [An Empirical Study of Monocular Human Body Measurement Under Weak Calibration](https://arxiv.org/abs/2601.01639)
*Gaurav Sekar*

Main category: cs.CV

TL;DR: The paper evaluates three monocular methods for estimating human body measurements using regular cameras, analyzing their trade-offs and calibration efforts under semi-constrained conditions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of estimating human body measurements from RGB imagery due to lack of depth, scale ambiguity, and viewpoint sensitivity.

Method: Systematic empirical study of three methods: landmark-based geometry, pose-driven regression, and object-calibrated silhouettes, under varied calibration requirements.

Result: The study highlights trade-offs between calibration effort and measurement stability across body types, showing how different assumptions affect outcomes.

Conclusion: The findings serve as a practical guide to designing lightweight monocular human measurement systems for consumer devices, balancing accuracy and ease of use.

Abstract: Estimating human body measurements from monocular RGB imagery remains challenging due to scale ambiguity, viewpoint sensitivity, and the absence of explicit depth information. This work presents a systematic empirical study of three weakly calibrated monocular strategies: landmark-based geometry, pose-driven regression, and object-calibrated silhouettes, evaluated under semi-constrained conditions using consumer-grade cameras. Rather than pursuing state-of-the-art accuracy, the study analyzes how differing calibration assumptions influence measurement behavior, robustness, and failure modes across varied body types. The results reveal a clear trade-off between user effort during calibration and the stability of resulting circumferential quantities. This paper serves as an empirical design reference for lightweight monocular human measurement systems intended for deployment on consumer devices.

</details>


### [227] [Animated 3DGS Avatars in Diverse Scenes with Consistent Lighting and Shadows](https://arxiv.org/abs/2601.01660)
*Aymen Mir,Riza Alp Guler,Jian Wang,Gerard Pons-Moll,Bing Zhou*

Main category: cs.CV

TL;DR: The paper introduces the Deep Gaussian Shadow Maps (DGSM) method for lighting and shadow consistency in 3DGS avatars interacting with 3DGS scenes and dynamic objects. It ensures coherent shadows and relighting directly within the volumetric representation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address lighting and shadow consistency for animated 3D Gaussian Splatting (3DGS) avatars in interactive and dynamic scenarios without relying on meshing.

Method: DGSM combines classical deep shadow mapping concepts with closed-form light accumulation for volumetric shadows and introduces spherical harmonic-based rapid relighting using HDRI probes.

Result: The method achieves consistent lighting and shadows for both single and multi-avatar setups, demonstrated with various datasets for environments and interactions.

Conclusion: DGSM renders volumetric 3DGS scenes and avatars with coherent lighting and shadows in real-time, avoiding computationally expensive approaches like meshing, while preserving visual fidelity.

Abstract: We present a method for consistent lighting and shadows when animated 3D Gaussian Splatting (3DGS) avatars interact with 3DGS scenes or with dynamic objects inserted into otherwise static scenes. Our key contribution is Deep Gaussian Shadow Maps (DGSM), a modern analogue of the classical shadow mapping algorithm tailored to the volumetric 3DGS representation. Building on the classic deep shadow mapping idea, we show that 3DGS admits closed form light accumulation along light rays, enabling volumetric shadow computation without meshing. For each estimated light, we tabulate transmittance over concentric radial shells and store them in octahedral atlases, which modern GPUs can sample in real time per query to attenuate affected scene Gaussians and thus cast and receive shadows consistently. To relight moving avatars, we approximate the local environment illumination with HDRI probes represented in a spherical harmonic (SH) basis and apply a fast per Gaussian radiance transfer, avoiding explicit BRDF estimation or offline optimization. We demonstrate environment consistent lighting for avatars from AvatarX and ActorsHQ, composited into ScanNet++, DL3DV, and SuperSplat scenes, and show interactions with inserted objects. Across single and multi avatar settings, DGSM and SH relighting operate fully in the volumetric 3DGS representation, yielding coherent shadows and relighting while avoiding meshing.

</details>


### [228] [LabelAny3D: Label Any Object 3D in the Wild](https://arxiv.org/abs/2601.01676)
*Jin Yao,Radowan Mahmud Redoy,Sebastian Elbaum,Matthew B. Dwyer,Zezhou Cheng*

Main category: cs.CV

TL;DR: The paper introduces LabelAny3D, an analysis-by-synthesis framework for efficient high-quality 3D annotations, and presents COCO3D, a benchmark for open-vocabulary monocular 3D detection based on MS-COCO, achieving improved detection performance.


<details>
  <summary>Details</summary>
Motivation: Existing monocular 3D detection models struggle with in-the-wild images due to the absence of 3D datasets and challenges in 3D annotation.

Method: The proposed LabelAny3D framework reconstructs 3D scenes from 2D images to generate 3D bounding box annotations, which are used to create COCO3D, a benchmark dataset derived from MS-COCO.

Result: LabelAny3D's annotations enhance monocular 3D detection across benchmarks, outperforming earlier auto-labeling methods in quality.

Conclusion: Foundation-model-driven annotation approaches, as demonstrated by LabelAny3D, offer a scalable solution for improving 3D detection in open-world scenarios.

Abstract: Detecting objects in 3D space from monocular input is crucial for applications ranging from robotics to scene understanding. Despite advanced performance in the indoor and autonomous driving domains, existing monocular 3D detection models struggle with in-the-wild images due to the lack of 3D in-the-wild datasets and the challenges of 3D annotation. We introduce LabelAny3D, an \emph{analysis-by-synthesis} framework that reconstructs holistic 3D scenes from 2D images to efficiently produce high-quality 3D bounding box annotations. Built on this pipeline, we present COCO3D, a new benchmark for open-vocabulary monocular 3D detection, derived from the MS-COCO dataset and covering a wide range of object categories absent from existing 3D datasets. Experiments show that annotations generated by LabelAny3D improve monocular 3D detection performance across multiple benchmarks, outperforming prior auto-labeling approaches in quality. These results demonstrate the promise of foundation-model-driven annotation for scaling up 3D recognition in realistic, open-world settings.

</details>


### [229] [Trustworthy Data-Driven Wildfire Risk Prediction and Understanding in Western Canada](https://arxiv.org/abs/2601.01677)
*Zhengsen Xu,Lanying Wang,Sibo Cheng,Xue Rui,Kyle Gao,Yimin Zhu,Mabel Heffring,Zack Dewis,Saeid Taleghanidoozdoozan,Megan Greenwood,Motasem Alkayid,Quinn Ledingham,Hongjie He,Jonathan Li,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: The study proposes a data-driven wildfire risk prediction model with high accuracy and interpretable results for western Canada, incorporating various environmental factors and explicitly quantifying uncertainties.


<details>
  <summary>Details</summary>
Motivation: To address challenges in predicting wildfire risks in western Canada due to their stochastic nature and complex interactions between environmental and anthropogenic factors.

Method: The framework uses long-sequence, multi-scale temporal modeling, integrating diverse wildfire drivers, and quantifies uncertainty while providing process-level interpretation. It employs SHAP-based analysis for mechanistic understanding.

Result: The model achieved superior performance during the 2023 and 2024 Canadian fire seasons, with an F1 score of 0.90 and PR-AUC of 0.98, revealing spatial and seasonal trends in predictive uncertainty.

Conclusion: The study demonstrates the effectiveness of the proposed model in wildfire risk assessment, highlights temperature and moisture's role in fire risks, and offers publicly available data and code for reproducibility.

Abstract: In recent decades, the intensification of wildfire activity in western Canada has resulted in substantial socio-economic and environmental losses. Accurate wildfire risk prediction is hindered by the intrinsic stochasticity of ignition and spread and by nonlinear interactions among fuel conditions, meteorology, climate variability, topography, and human activities, challenging the reliability and interpretability of purely data-driven models. We propose a trustworthy data-driven wildfire risk prediction framework based on long-sequence, multi-scale temporal modeling, which integrates heterogeneous drivers while explicitly quantifying predictive uncertainty and enabling process-level interpretation. Evaluated over western Canada during the record-breaking 2023 and 2024 fire seasons, the proposed model outperforms existing time-series approaches, achieving an F1 score of 0.90 and a PR-AUC of 0.98 with low computational cost. Uncertainty-aware analysis reveals structured spatial and seasonal patterns in predictive confidence, highlighting increased uncertainty associated with ambiguous predictions and spatiotemporal decision boundaries. SHAP-based interpretation provides mechanistic understanding of wildfire controls, showing that temperature-related drivers dominate wildfire risk in both years, while moisture-related constraints play a stronger role in shaping spatial and land-cover-specific contrasts in 2024 compared to the widespread hot and dry conditions of 2023. Data and code are available at https://github.com/SynUW/mmFire.

</details>


### [230] [Evaluating Deep Learning-Based Face Recognition for Infants and Toddlers: Impact of Age Across Developmental Stages](https://arxiv.org/abs/2601.01680)
*Afzal Hossain,Mst Rumana Sumi,Stephanie Schuckers*

Main category: cs.CV

TL;DR: This paper evaluates deep learning face recognition models on a dataset of infants and toddlers, identifying challenges due to facial changes and age variability while proposing a solution to enhance temporal stability.


<details>
  <summary>Details</summary>
Motivation: The study aims to address difficulties in face recognition for young children caused by rapidly changing facial features, limited data, and a need for reliable biometric systems in applications like child safety and healthcare.

Method: The research uses a longitudinal dataset of children aged 0 to 3 across seven sessions, tests multiple deep learning models, and introduces a Domain Adversarial Neural Network (DANN) to mitigate temporal embedding drift.

Result: The performance of face recognition models improves with the age of children, achieving a TAR of 64.7% at 0.1% FAR for ages 2.5 to 3 years. The proposed DANN approach enhances temporal stability and increases TAR by over 12%.

Conclusion: Facial recognition systems for children face significant accuracy challenges at younger ages. Addressing embedding drift with DANN improves model reliability, offering insights for future biometric systems in child-focused urban environments.

Abstract: Face recognition for infants and toddlers presents unique challenges due to rapid facial morphology changes, high inter-class similarity, and limited dataset availability. This study evaluates the performance of four deep learning-based face recognition models FaceNet, ArcFace, MagFace, and CosFace on a newly developed longitudinal dataset collected over a 24 month period in seven sessions involving children aged 0 to 3 years. Our analysis examines recognition accuracy across developmental stages, showing that the True Accept Rate (TAR) is only 30.7% at 0.1% False Accept Rate (FAR) for infants aged 0 to 6 months, due to unstable facial features. Performance improves significantly in older children, reaching 64.7% TAR at 0.1% FAR in the 2.5 to 3 year age group. We also evaluate verification performance over different time intervals, revealing that shorter time gaps result in higher accuracy due to reduced embedding drift. To mitigate this drift, we apply a Domain Adversarial Neural Network (DANN) approach that improves TAR by over 12%, yielding features that are more temporally stable and generalizable. These findings are critical for building biometric systems that function reliably over time in smart city applications such as public healthcare, child safety, and digital identity services. The challenges observed in early age groups highlight the importance of future research on privacy preserving biometric authentication systems that can address temporal variability, particularly in secure and regulated urban environments where child verification is essential.

</details>


### [231] [FALCON: Few-Shot Adversarial Learning for Cross-Domain Medical Image Segmentation](https://arxiv.org/abs/2601.01687)
*Abdur R. Fayjie,Pankhi Kashyap,Jutika Borah,Patrick Vandewalle*

Main category: cs.CV

TL;DR: FALCON is a few-shot segmentation framework that uses 2D slices for high-precision 3D medical volume segmentation, achieving superior accuracy with less data and lower computational cost.


<details>
  <summary>Details</summary>
Motivation: This paper addresses challenges in 3D volume segmentation, including limited 3D annotations, patient-specific variations, privacy concerns, and high computational requirements, aiming to improve accuracy and feasibility for clinical applications.

Method: The proposed FALCON framework is meta-trained on natural images to learn general segmentation priors. It is fine-tuned for the medical domain using adversarial and boundary-aware learning. Task-aware inference dynamically adapts to patient-specific variations.

Result: FALCON achieves the lowest Hausdorff Distance scores for boundary accuracy, a competitive Dice Similarity Coefficient, and requires less labeled data, no augmentation, and lower computational effort. These results are validated on four benchmarks.

Conclusion: The FALCON framework demonstrates a clinically viable approach for precise and efficient 3D medical volume segmentation, addressing key challenges such as data scarcity and computational constraints.

Abstract: Precise delineation of anatomical and pathological structures within 3D medical volumes is crucial for accurate diagnosis, effective surgical planning, and longitudinal disease monitoring. Despite advancements in AI, clinically viable segmentation is often hindered by the scarcity of 3D annotations, patient-specific variability, data privacy concerns, and substantial computational overhead. In this work, we propose FALCON, a cross-domain few-shot segmentation framework that achieves high-precision 3D volume segmentation by processing data as 2D slices. The framework is first meta-trained on natural images to learn-to-learn generalizable segmentation priors, then transferred to the medical domain via adversarial fine-tuning and boundary-aware learning. Task-aware inference, conditioned on support cues, allows FALCON to adapt dynamically to patient-specific anatomical variations across slices. Experiments on four benchmarks demonstrate that FALCON consistently achieves the lowest Hausdorff Distance scores, indicating superior boundary accuracy while maintaining a Dice Similarity Coefficient comparable to the state-of-the-art models. Notably, these results are achieved with significantly less labeled data, no data augmentation, and substantially lower computational overhead.

</details>


### [232] [Mitigating Longitudinal Performance Degradation in Child Face Recognition Using Synthetic Data](https://arxiv.org/abs/2601.01689)
*Afzal Hossain,Stephanie Schuckers*

Main category: cs.CV

TL;DR: The paper explores using synthetic facial data to enhance child face recognition by addressing temporal challenges caused by rapid facial growth. Testing mixtures of authentic and synthetic data shows reduced error rates in facial verification over time.


<details>
  <summary>Details</summary>
Motivation: Child face recognition faces challenges due to natural facial growth, leading to high error rates and template drift. This study investigates whether synthetic facial data can stabilize recognition models over time.

Method: Three approaches were tested on the Young Face Aging dataset: (i) pretrained baseline without fine-tuning, (ii) fine-tuning on authentic data only, and (iii) fine-tuning using a mix of authentic and synthetic facial data generated using StyleGAN2 ADA. Synthetic data underwent filtering to prevent identity leakage and artifacts.

Result: In tests spanning 6 to 36 months, models fine-tuned with combined authentic and synthetic data achieved lower error rates in comparison to the other two methods.

Conclusion: Synthetic data augmentation improves the reliability of pediatric face recognition across longitudinal growth periods, showing promise for maintaining identity persistence over time.

Abstract: Longitudinal face recognition in children remains challenging due to rapid and nonlinear facial growth, which causes template drift and increasing verification errors over time. This work investigates whether synthetic face data can act as a longitudinal stabilizer by improving temporal robustness of child face recognition models. Using an identity disjoint protocol on the Young Face Aging (YFA) dataset, we evaluate three settings: (i) pretrained MagFace embeddings without dataset specific fine-tuning, (ii) MagFace fine-tuned using authentic training faces only, and (iii) MagFace fine-tuned using a combination of authentic and synthetically generated training faces. Synthetic data is generated using StyleGAN2 ADA and incorporated exclusively within the training identities; a post generation filtering step is applied to mitigate identity leakage and remove artifact affected samples. Experimental results across enrollment verification gaps from 6 to 36 months show that synthetic-augmented fine tuning substantially reduces error rates relative to both the pretrained baseline and real only fine tuning. These findings provide a risk aware assessment of synthetic augmentation for improving identity persistence in pediatric face recognition.

</details>


### [233] [Learnability-Driven Submodular Optimization for Active Roadside 3D Detection](https://arxiv.org/abs/2601.01695)
*Ruiyu Mao,Baoming Zhang,Nicholas Ruozzi,Yunhui Guo*

Main category: cs.CV

TL;DR: This paper addresses the challenges in roadside perception datasets, particularly annotating roadside-only data due to constraints. It proposes a learnability-driven active learning framework that reduces annotation efforts while maintaining high model performance.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the annotation difficulties and costs of roadside-only scenes for monocular 3D object detection, as these can be inherently ambiguous due to hardware constraints and scene properties.

Method: The authors introduce a learnability-driven active learning framework (LH3D) that focuses on selecting informative and reliably labelable scenes while avoiding inherently ambiguous samples.

Result: The proposed LH3D approach achieves high detection performance with 25% of the annotation budget, outperforming uncertainty-based baselines.

Conclusion: This study demonstrates that prioritizing learnability over uncertainty leads to a more efficient and effective approach for roadside 3D object detection.

Abstract: Roadside perception datasets are typically constructed via cooperative labeling between synchronized vehicle and roadside frame pairs. However, real deployment often requires annotation of roadside-only data due to hardware and privacy constraints. Even human experts struggle to produce accurate labels without vehicle-side data (image, LIDAR), which not only increases annotation difficulty and cost, but also reveals a fundamental learnability problem: many roadside-only scenes contain distant, blurred, or occluded objects whose 3D properties are ambiguous from a single view and can only be reliably annotated by cross-checking paired vehicle--roadside frames. We refer to such cases as inherently ambiguous samples. To reduce wasted annotation effort on inherently ambiguous samples while still obtaining high-performing models, we turn to active learning. This work focuses on active learning for roadside monocular 3D object detection and proposes a learnability-driven framework that selects scenes which are both informative and reliably labelable, suppressing inherently ambiguous samples while ensuring coverage. Experiments demonstrate that our method, LH3D, achieves 86.06%, 67.32%, and 78.67% of full-performance for vehicles, pedestrians, and cyclists respectively, using only 25% of the annotation budget on DAIR-V2X-I, significantly outperforming uncertainty-based baselines. This confirms that learnability, not uncertainty, matters for roadside 3D perception.

</details>


### [234] [FFP-300K: Scaling First-Frame Propagation for Generalizable Video Editing](https://arxiv.org/abs/2601.01720)
*Xijie Huang,Chengming Xu,Donghao Luo,Xiaobin Hu,Peng Tang,Xu Peng,Jiangning Zhang,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: A new framework for First-Frame Propagation (FFP) video editing is presented, featuring a large dataset (FFP-300K) and a novel architectural component (AST-RoPE) for guidance-free video editing.


<details>
  <summary>Details</summary>
Motivation: The reliance on run-time guidance in existing FFP methods arises from inadequate training datasets, which are short, low-resolution, and lack diversity. This motivates the creation of a robust dataset and framework for FFP.

Method: A large-scale dataset (FFP-300K) with 300K high-resolution video pairs and a novel framework featuring Adaptive Spatio-Temporal RoPE (AST-RoPE) for disentangling appearance and motion references was developed. A self-distillation strategy is employed for long-term temporal stability.

Result: The proposed method demonstrated superior performance on EditVerseBench, significantly exceeding existing academic and commercial methods with substantial PickScore and VLM score improvements.

Conclusion: The novel dataset and framework effectively address fundamental limitations in FFP video editing, enabling high-quality, guidance-free video editing.

Abstract: First-Frame Propagation (FFP) offers a promising paradigm for controllable video editing, but existing methods are hampered by a reliance on cumbersome run-time guidance. We identify the root cause of this limitation as the inadequacy of current training datasets, which are often too short, low-resolution, and lack the task diversity required to teach robust temporal priors. To address this foundational data gap, we first introduce FFP-300K, a new large-scale dataset comprising 300K high-fidelity video pairs at 720p resolution and 81 frames in length, constructed via a principled two-track pipeline for diverse local and global edits. Building on this dataset, we propose a novel framework designed for true guidance-free FFP that resolves the critical tension between maintaining first-frame appearance and preserving source video motion. Architecturally, we introduce Adaptive Spatio-Temporal RoPE (AST-RoPE), which dynamically remaps positional encodings to disentangle appearance and motion references. At the objective level, we employ a self-distillation strategy where an identity propagation task acts as a powerful regularizer, ensuring long-term temporal stability and preventing semantic drift. Comprehensive experiments on the EditVerseBench benchmark demonstrate that our method significantly outperforming existing academic and commercial models by receiving about 0.2 PickScore and 0.3 VLM score improvement against these competitors.

</details>


### [235] [Point-SRA: Self-Representation Alignment for 3D Representation Learning](https://arxiv.org/abs/2601.01746)
*Lintong Wei,Jian Lu,Haozhe Cheng,Jihua Zhu,Kaibing Zhang*

Main category: cs.CV

TL;DR: Point-SRA improves 3D representation learning using self-distillation, probabilistic modeling, and a novel fine-tuning architecture. It outperforms baselines significantly across several tasks.


<details>
  <summary>Details</summary>
Motivation: Existing MAE methods in 3D representation learning ignore multi-level structure correlations and rely on fixed mask ratios, which fail to address the diversity and complexity of point cloud data.

Method: Point-SRA introduces a self-distillation and probabilistic modeling approach, utilizing dynamic masking ratios, MeanFlow Transformer for probabilistic reconstruction, and a Dual Self-Representation Alignment mechanism.

Result: Point-SRA achieves superior performance, with 5.37% improvement on ScanObjectNN, 96.07% mean IoU in artery segmentation, and 47.3% AP@50 in object detection, surpassing multiple baselines.

Conclusion: The proposed Point-SRA demonstrates that adaptive masking and dual-level alignment, combined with flow-conditioned learning, significantly enhance the representation learning of 3D point clouds.

Abstract: Masked autoencoders (MAE) have become a dominant paradigm in 3D representation learning, setting new performance benchmarks across various downstream tasks. Existing methods with fixed mask ratio neglect multi-level representational correlations and intrinsic geometric structures, while relying on point-wise reconstruction assumptions that conflict with the diversity of point cloud. To address these issues, we propose a 3D representation learning method, termed Point-SRA, which aligns representations through self-distillation and probabilistic modeling. Specifically, we assign different masking ratios to the MAE to capture complementary geometric and semantic information, while the MeanFlow Transformer (MFT) leverages cross-modal conditional embeddings to enable diverse probabilistic reconstruction. Our analysis further reveals that representations at different time steps in MFT also exhibit complementarity. Therefore, a Dual Self-Representation Alignment mechanism is proposed at both the MAE and MFT levels. Finally, we design a Flow-Conditioned Fine-Tuning Architecture to fully exploit the point cloud distribution learned via MeanFlow. Point-SRA outperforms Point-MAE by 5.37% on ScanObjectNN. On intracranial aneurysm segmentation, it reaches 96.07% mean IoU for arteries and 86.87% for aneurysms. For 3D object detection, Point-SRA achieves 47.3% AP@50, surpassing MaskPoint by 5.12%.

</details>


### [236] [MANGO:Natural Multi-speaker 3D Talking Head Generation via 2D-Lifted Enhancement](https://arxiv.org/abs/2601.01749)
*Lei Zhu,Lijian Lin,Ye Zhu,Jiahao Wu,Xuehan Hou,Yu Li,Yunfei Liu,Jie Chen*

Main category: cs.CV

TL;DR: The paper proposes MANGO, a two-stage framework for generating realistic 3D audio-driven conversational avatars using image-level supervision, overcoming limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle to achieve seamless transitions between speaking and listening states in multi-speaker scenarios and often use imprecise pseudo-3D labels that limit the capture of detailed facial dynamics.

Method: MANGO uses a two-stage approach: a diffusion-based transformer with a dual-audio interaction module for natural 3D motion modeling, and a 3D Gaussian Renderer employing image-level alternate training for high-fidelity image generation.

Result: The approach achieves improved accuracy and realism in two-person 3D dialogue motion, supported by extensive experiments and a newly introduced dataset, MANGO-Dialog.

Conclusion: This framework advances controllability and fidelity in 3D talking heads, providing a robust solution for realistic multi-speaker conversational avatars.

Abstract: Current audio-driven 3D head generation methods mainly focus on single-speaker scenarios, lacking natural, bidirectional listen-and-speak interaction. Achieving seamless conversational behavior, where speaking and listening states transition fluidly remains a key challenge. Existing 3D conversational avatar approaches rely on error-prone pseudo-3D labels that fail to capture fine-grained facial dynamics. To address these limitations, we introduce a novel two-stage framework MANGO, which leveraging pure image-level supervision by alternately training to mitigate the noise introduced by pseudo-3D labels, thereby achieving better alignment with real-world conversational behaviors. Specifically, in the first stage, a diffusion-based transformer with a dual-audio interaction module models natural 3D motion from multi-speaker audio. In the second stage, we use a fast 3D Gaussian Renderer to generate high-fidelity images and provide 2D-level photometric supervision for the 3D motions through alternate training. Additionally, we introduce MANGO-Dialog, a high-quality dataset with over 50 hours of aligned 2D-3D conversational data across 500+ identities. Extensive experiments demonstrate that our method achieves exceptional accuracy and realism in modeling two-person 3D dialogue motion, significantly advancing the fidelity and controllability of audio-driven talking heads.

</details>


### [237] [CTIS-QA: Clinical Template-Informed Slide-level Question Answering for Pathology](https://arxiv.org/abs/2601.01769)
*Hao Lu,Ziniu Qian,Yifu Li,Yang Zhou,Bingzheng Wei,Yan Xu*

Main category: cs.CV

TL;DR: The paper proposes a structured methodology for extracting diagnostic elements from pathology reports using a Clinical Pathology Report Template, creating datasets (CTIS-Align and CTIS-Bench), and developing a Slide-level QA model (CTIS-QA) which outperforms existing models.


<details>
  <summary>Details</summary>
Motivation: To improve the systematic extraction and utilization of pathological diagnostic information for clinical workflows and diagnostic tasks.

Method: Designing a Clinical Pathology Report Template (CPRT) aligned with CAP guidelines, creating datasets (CTIS-Align and CTIS-Bench), and proposing CTIS-QA featuring a dual-stream architecture for slide-level diagnostic question answering.

Result: The proposed CTIS-QA model achieves superior performance compared to state-of-the-art models on WSI-VQA, CTIS-Bench, and diagnostic tasks.

Conclusion: The study introduces an effective pipeline for clinical diagnosis, validates its utility, and demonstrates the success of the CTIS-QA model for advancing pathological diagnostics.

Abstract: In this paper, we introduce a clinical diagnosis template-based pipeline to systematically collect and structure pathological information. In collaboration with pathologists and guided by the the College of American Pathologists (CAP) Cancer Protocols, we design a Clinical Pathology Report Template (CPRT) that ensures comprehensive and standardized extraction of diagnostic elements from pathology reports. We validate the effectiveness of our pipeline on TCGA-BRCA. First, we extract pathological features from reports using CPRT. These features are then used to build CTIS-Align, a dataset of 80k slide-description pairs from 804 WSIs for vision-language alignment training, and CTIS-Bench, a rigorously curated VQA benchmark comprising 977 WSIs and 14,879 question-answer pairs. CTIS-Bench emphasizes clinically grounded, closed-ended questions (e.g., tumor grade, receptor status) that reflect real diagnostic workflows, minimize non-visual reasoning, and require genuine slide understanding. We further propose CTIS-QA, a Slide-level Question Answering model, featuring a dual-stream architecture that mimics pathologists' diagnostic approach. One stream captures global slide-level context via clustering-based feature aggregation, while the other focuses on salient local regions through attention-guided patch perception module. Extensive experiments on WSI-VQA, CTIS-Bench, and slide-level diagnostic tasks show that CTIS-QA consistently outperforms existing state-of-the-art models across multiple metrics. Code and data are available at https://github.com/HLSvois/CTIS-QA.

</details>


### [238] [Subimage Overlap Prediction: Task-Aligned Self-Supervised Pretraining For Semantic Segmentation In Remote Sensing Imagery](https://arxiv.org/abs/2601.01781)
*Lakshay Sharma,Alex Marin*

Main category: cs.CV

TL;DR: This paper proposes 'Subimage Overlap Prediction,' a self-supervised learning method for remote sensing semantic segmentation that uses less pretraining data while achieving faster convergence and better performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the reliance of existing SSL methods on vast pretraining data, focusing instead on creating an approach that uses significantly less data, specifically targeting remote sensing semantic segmentation tasks.

Method: The method involves a pretraining task where a sub-image is extracted from an image, and the model predicts the semantic mask showing the sub-image's location within the original image.

Result: The proposed method shows faster convergence and equal or better mIoU performance in semantic segmentation, especially with limited labeled data. It outperforms other SSL techniques using significantly less pretraining data across different architectures and datasets.

Conclusion: The introduced method effectively reduces dependency on excessive pretraining data while improving convergence speed and performance, making it a valuable tool for remote sensing tasks.

Abstract: Self-supervised learning (SSL) methods have become a dominant paradigm for creating general purpose models whose capabilities can be transferred to downstream supervised learning tasks. However, most such methods rely on vast amounts of pretraining data. This work introduces Subimage Overlap Prediction, a novel self-supervised pretraining task to aid semantic segmentation in remote sensing imagery that uses significantly lesser pretraining imagery. Given an image, a sub-image is extracted and the model is trained to produce a semantic mask of the location of the extracted sub-image within the original image. We demonstrate that pretraining with this task results in significantly faster convergence, and equal or better performance (measured via mIoU) on downstream segmentation. This gap in convergence and performance widens when labeled training data is reduced. We show this across multiple architecture types, and with multiple downstream datasets. We also show that our method matches or exceeds performance while requiring significantly lesser pretraining data relative to other SSL methods. Code and model weights are provided at \href{https://github.com/sharmalakshay93/subimage-overlap-prediction}{github.com/sharmalakshay93/subimage-overlap-prediction}.

</details>


### [239] [DDNet: A Dual-Stream Graph Learning and Disentanglement Framework for Temporal Forgery Localization](https://arxiv.org/abs/2601.01784)
*Boyang Zhao,Xin Liao,Jiaxin Chen,Xiaoshuai Wu,Yufeng Wu*

Main category: cs.CV

TL;DR: This paper introduces a new method, DDNet, for more accurate temporal forgery localization in videos.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to detect precise tampered segments in videos due to reliance on local views and limited global anomaly detection.

Method: Proposed DDNet combines a Temporal Distance Stream for local artifacts with a Semantic Content Stream for global anomalies, alongside TDA for forgery fingerprint isolation and CLFE for hierarchical feature robustness.

Result: Experiments show DDNet achieves a 9% improvement in AP@0.95 over state-of-the-art methods on ForgeryNet and TVIL benchmarks, with better cross-domain robustness.

Conclusion: DDNet effectively improves temporal forgery localization, addressing global and local anomalies for more reliable video tampering detection.

Abstract: The rapid evolution of AIGC technology enables misleading viewers by tampering mere small segments within a video, rendering video-level detection inaccurate and unpersuasive. Consequently, temporal forgery localization (TFL), which aims to precisely pinpoint tampered segments, becomes critical. However, existing methods are often constrained by \emph{local view}, failing to capture global anomalies. To address this, we propose a \underline{d}ual-stream graph learning and \underline{d}isentanglement framework for temporal forgery localization (DDNet). By coordinating a \emph{Temporal Distance Stream} for local artifacts and a \emph{Semantic Content Stream} for long-range connections, DDNet prevents global cues from being drowned out by local smoothness. Furthermore, we introduce Trace Disentanglement and Adaptation (TDA) to isolate generic forgery fingerprints, alongside Cross-Level Feature Embedding (CLFE) to construct a robust feature foundation via deep fusion of hierarchical features. Experiments on ForgeryNet and TVIL benchmarks demonstrate that our method outperforms state-of-the-art approaches by approximately 9\% in AP@0.95, with significant improvements in cross-domain robustness.

</details>


### [240] [VerLM: Explaining Face Verification Using Natural Language](https://arxiv.org/abs/2601.01798)
*Syed Abdul Hannan,Hazim Bukhari,Thomas Cantalapiedra,Eman Ansar,Massa Baali,Rita Singh,Bhiksha Raj*

Main category: cs.CV

TL;DR: The paper introduces a Vision-Language Model (VLM) for Face Verification that is accurate and provides explanations for its decisions.


<details>
  <summary>Details</summary>
Motivation: Current face verification systems lack transparency in explaining their decisions.

Method: The model employs a cross-modal transfer, adapting an audio differentiation method to handle visual inputs, with training incorporating both concise and detailed explanation styles.

Result: The proposed model achieves superior accuracy and interpretability compared to baseline and existing methods.

Conclusion: The study highlights the potential of vision-language models to create transparent, explainable, and reliable face verification systems.

Abstract: Face verification systems have seen substantial advancements; however, they often lack transparency in their decision-making processes. In this paper, we introduce an innovative Vision-Language Model (VLM) for Face Verification, which not only accurately determines if two face images depict the same individual but also explicitly explains the rationale behind its decisions. Our model is uniquely trained using two complementary explanation styles: (1) concise explanations that summarize the key factors influencing its decision, and (2) comprehensive explanations detailing the specific differences observed between the images. We adapt and enhance a state-of-the-art modeling approach originally designed for audio-based differentiation to suit visual inputs effectively. This cross-modal transfer significantly improves our model's accuracy and interpretability. The proposed VLM integrates sophisticated feature extraction techniques with advanced reasoning capabilities, enabling clear articulation of its verification process. Our approach demonstrates superior performance, surpassing baseline methods and existing models. These findings highlight the immense potential of vision language models in face verification set up, contributing to more transparent, reliable, and explainable face verification systems.

</details>


### [241] [Causality-Aware Temporal Projection for Video Understanding in Video-LLMs](https://arxiv.org/abs/2601.01804)
*Zhengjian Kang,Qi Chen,Rui Liu,Kangtong Mo,Xingyu Zhang,Xiaoyu Deng,Ye Zhang*

Main category: cs.CV

TL;DR: This paper introduces V-CORE, a framework improving temporal and causal reasoning in Video-LLMs by incorporating explicit temporal ordering constraints, achieving strong benchmark performance.


<details>
  <summary>Details</summary>
Motivation: Video-LLMs struggle with tasks requiring temporal ordering and causal coherence due to bidirectional projectors that allow interactions between past and future frames freely.

Method: V-CORE introduces Learnable Spatial Aggregation (LSA) for token reduction and a Causality-Aware Temporal Projector (CATP) with block-causal attention and a summary token to ensure structured unidirectional temporal flow.

Result: V-CORE achieves 61.2% on the NExT-QA benchmark, with notable improvements in temporal (+3.5%) and causal reasoning (+5.2%), while remaining competitive across other video reasoning benchmarks.

Conclusion: Explicit temporal ordering constraints significantly enhance Video-LLMs' ability to perform temporal and causal reasoning, demonstrating V-CORE's efficiency and efficacy.

Abstract: Recent Video Large Language Models (Video-LLMs) have shown strong multimodal reasoning capabilities, yet remain challenged by video understanding tasks that require consistent temporal ordering and causal coherence. Many parameter-efficient Video-LLMs rely on unconstrained bidirectional projectors to model inter-frame interactions, which can blur temporal ordering by allowing later frames to influence earlier representations, without explicit architectural mechanisms to respect the directional nature of video reasoning. To address this limitation, we propose V-CORE, a parameter-efficient framework that introduces explicit temporal ordering constraints for video understanding. V-CORE consists of two key components: (1) Learnable Spatial Aggregation (LSA), which adaptively selects salient spatial tokens to reduce redundancy, and (2) a Causality-Aware Temporal Projector (CATP), which enforces structured unidirectional information flow via block-causal attention and a terminal dynamic summary token acting as a causal sink. This design preserves intra-frame spatial interactions while ensuring that temporal information is aggregated in a strictly ordered manner. With 4-bit QLoRA and a frozen LLM backbone, V-CORE can be trained efficiently on a single consumer GPU. Experiments show that V-CORE achieves strong performance on the challenging NExT-QA benchmark, reaching 61.2% accuracy, and remains competitive across MSVD-QA, MSRVTT-QA, and TGIF-QA, with gains concentrated in temporal and causal reasoning subcategories (+3.5% and +5.2% respectively), directly validating the importance of explicit temporal ordering constraints.

</details>


### [242] [Adaptive Hybrid Optimizer based Framework for Lumpy Skin Disease Identification](https://arxiv.org/abs/2601.01807)
*Ubaidullah,Muhammad Abid Hussain,Mohsin Raza Jafri,Rozi Khan,Moid Sandhu,Abd Ullah Khan,Hyundong Shin*

Main category: cs.CV

TL;DR: This paper proposes LUMPNet, a hybrid deep learning model for detecting and classifying Lumpy Skin Disease (LSD) in cattle using image data.


<details>
  <summary>Details</summary>
Motivation: Effectively addressing and preventing the rapid spread of LSD is critical to safeguard livestock health, global economy, and food security. Existing detection methods are limited, and early, precise diagnosis is essential.

Method: LUMPNet combines YOLOv11 for lesion detection/localization, EfficientNet for classification, and introduces a novel adaptive hybrid optimizer to stabilize and speed up the training process.

Result: LUMPNet achieves 99% training accuracy and 98% validation accuracy in identifying LSD. Its performance surpasses other existing methods.

Conclusion: LUMPNet demonstrates promising capabilities for early and accurate detection of LSD in cattle, providing a robust tool for controlling disease outbreaks and minimizing related risks.

Abstract: Lumpy Skin Disease (LSD) is a contagious viral infection that significantly deteriorates livestock health, thereby posing a serious threat to the global economy and food security. Owing to its rapid spread characteristics, early and precise identification is crucial to prevent outbreaks and ensure timely intervention. In this paper, we propose a hybrid deep learning-based approach called LUMPNet for the early detection of LSD. LUMPNet utilizes image data to detect and classify skin nodules -- the primary indicator of LSD. To this end, LUMPNet uses YOLOv11, EfficientNet-based CNN classifier with compound scaling, and a novel adaptive hybrid optimizer. More precisely, LUMPNet detects and localizes LSD skin nodules and lesions on cattle images. It exploits EfficientNet to classify the localized cattle images into LSD-affected or healthy categories. To stabilize and accelerate the training of YOLOv11 and EfficientNet hybrid model, a novel adaptive hybrid optimizer is proposed and utilized. We evaluate LUMPNet at various stages of LSD using a publicly available dataset. Results indicate that the proposed scheme achieves 99% LSD detection training accuracy, and outperforms existing schemes. The model also achieves validation accuracy of 98%. Moreover, for further evaluation, we conduct a case study using an optimized EfficientNet-B0 model trained with the AdamW optimizer, and compare its performance with LUMPNet. The results show that LUMPNet achieves superior performance.

</details>


### [243] [Robust Egocentric Visual Attention Prediction Through Language-guided Scene Context-aware Learning](https://arxiv.org/abs/2601.01818)
*Sungjune Park,Hongda Mao,Qingshuang Chen,Yong Man Ro,Yelin Kim*

Main category: cs.CV

TL;DR: The paper introduces a language-guided, context-aware framework for predicting egocentric visual attention. It achieves state-of-the-art results using scene descriptions to enhance robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of predicting egocentric visual attention in complex and dynamic scenes, leveraging contextual scene information for improved accuracy.

Method: The proposed framework introduces a context perceiver that generates scene-aware video representations guided by language-based scene descriptions. Two training objectives are applied to focus on points of interest and avoid distractions.

Result: The method outperforms existing approaches, delivering state-of-the-art results and robustness in egocentric visual attention prediction tasks on Ego4D and AEA datasets.

Conclusion: Language-guided contextual learning significantly enhances the ability to predict egocentric attention, validating the importance of incorporating scene context and training objectives for improved performance.

Abstract: As the demand for analyzing egocentric videos grows, egocentric visual attention prediction, anticipating where a camera wearer will attend, has garnered increasing attention. However, it remains challenging due to the inherent complexity and ambiguity of dynamic egocentric scenes. Motivated by evidence that scene contextual information plays a crucial role in modulating human attention, in this paper, we present a language-guided scene context-aware learning framework for robust egocentric visual attention prediction. We first design a context perceiver which is guided to summarize the egocentric video based on a language-based scene description, generating context-aware video representations. We then introduce two training objectives that: 1) encourage the framework to focus on the target point-of-interest regions and 2) suppress distractions from irrelevant regions which are less likely to attract first-person attention. Extensive experiments on Ego4D and Aria Everyday Activities (AEA) datasets demonstrate the effectiveness of our approach, achieving state-of-the-art performance and enhanced robustness across diverse, dynamic egocentric scenarios.

</details>


### [244] [RSwinV2-MD: An Enhanced Residual SwinV2 Transformer for Monkeypox Detection from Skin Images](https://arxiv.org/abs/2601.01835)
*Rashid Iqbal,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: The paper proposes RSwinV2, a deep learning tool based on Customized Residual SwinTransformerV2, for efficient Mpox lesion classification, achieving high accuracy and F1 scores.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance lesion classification capability for Mpox diagnosis using a tool-assisted vision approach with RSwinV2.

Method: RSwinV2 employs hierarchical transformer structures, non-overlapping patch processing with shifted windows, multi-head attention with embeddings, and incorporates the Inverse Residual Block (IRB) to overcome gradient issues and improve global-local pattern linking.

Result: RSwinV2 achieved 96.21% accuracy and 95.62% F1 score on the Kaggle public dataset, outperforming standard CNN models and SwinTransformers.

Conclusion: RSwinV2 successfully improves lesion classification for Mpox diagnosis and demonstrates its effectiveness as a computational aid on lesion interpretation.

Abstract: In this paper, a deep learning approach for Mpox diagnosis named Customized Residual SwinTransformerV2 (RSwinV2) has been proposed, trying to enhance the capability of lesion classification by employing the RSwinV2 tool-assisted vision approach. In the RSwinV2 method, a hierarchical structure of the transformer has been customized based on the input dimensionality, embedding structure, and output targeted by the method. In this RSwinV2 approach, the input image has been split into non-overlapping patches and processed using shifted windows and attention in these patches. This process has helped the method link all the windows efficiently by avoiding the locality issues of non-overlapping regions in attention, while being computationally efficient. RSwinV2 has further developed based on SwinTransformer and has included patch and position embeddings to take advantage of the transformer global-linking capability by employing multi-head attention in these embeddings. Furthermore, RSwinV2 has developed and incorporated the Inverse Residual Block (IRB) into this method, which utilizes convolutional skip connections with these inclusive designs to address the vanishing gradient issues during processing. RSwinV2 inclusion of IRB has therefore facilitated this method to link global patterns as well as local patterns; hence, its integrity has helped improve lesion classification capability by minimizing variability of Mpox and increasing differences of Mpox, chickenpox, measles, and cowpox. In testing SwinV2, its accuracy of 96.21 and an F1score of 95.62 have been achieved on the Kaggle public dataset, which has outperformed standard CNN models and SwinTransformers; RSwinV2 vector has thus proved its valiance as a computer-assisted tool for Mpox lesion observation interpretation.

</details>


### [245] [ESGaussianFace: Emotional and Stylized Audio-Driven Facial Animation via 3D Gaussian Splatting](https://arxiv.org/abs/2601.01847)
*Chuhang Ma,Shuai Tan,Ye Pan,Jiaolong Yang,Xin Tong*

Main category: cs.CV

TL;DR: The paper introduces ESGaussianFace, a framework for generating emotional and stylized audio-driven talking head videos using 3D Gaussian Splatting, emotion-guided attention, and multi-stage training strategies.


<details>
  <summary>Details</summary>
Motivation: Current techniques for audio-driven facial animation lack effective solutions for high-quality videos combining emotional expressions and style features.

Method: ESGaussianFace utilizes 3D Gaussian Splatting with emotion-audio-guided spatial attention and multi-stage training strategies to integrate emotional expressions and style features into facial animations.

Result: The method achieved efficient, high-quality, and 3D consistent facial animations, outperforming state-of-the-art techniques in lip movement accuracy, emotional expression, and style feature integration.

Conclusion: ESGaussianFace presents an innovative solution for generating realistic talking head videos with emotional and stylized expressions, addressing key challenges in this domain and setting a new benchmark.

Abstract: Most current audio-driven facial animation research primarily focuses on generating videos with neutral emotions. While some studies have addressed the generation of facial videos driven by emotional audio, efficiently generating high-quality talking head videos that integrate both emotional expressions and style features remains a significant challenge. In this paper, we propose ESGaussianFace, an innovative framework for emotional and stylized audio-driven facial animation. Our approach leverages 3D Gaussian Splatting to reconstruct 3D scenes and render videos, ensuring efficient generation of 3D consistent results. We propose an emotion-audio-guided spatial attention method that effectively integrates emotion features with audio content features. Through emotion-guided attention, the model is able to reconstruct facial details across different emotional states more accurately. To achieve emotional and stylized deformations of the 3D Gaussian points through emotion and style features, we introduce two 3D Gaussian deformation predictors. Futhermore, we propose a multi-stage training strategy, enabling the step-by-step learning of the character's lip movements, emotional variations, and style features. Our generated results exhibit high efficiency, high quality, and 3D consistency. Extensive experimental results demonstrate that our method outperforms existing state-of-the-art techniques in terms of lip movement accuracy, expression variation, and style feature expressiveness.

</details>


### [246] [GCR: Geometry-Consistent Routing for Task-Agnostic Continual Anomaly Detection](https://arxiv.org/abs/2601.01856)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: The paper introduces GCR, a framework for anomaly detection with geometry-consistent routing, addressing issues of cross-head decision instability while improving performance without additional end-to-end learning.


<details>
  <summary>Details</summary>
Motivation: Current feature-based anomaly detection methods struggle with task-agnostic continual category expansion, necessitating reliable routing mechanisms for effective operation.

Method: The proposed framework, GCR, utilizes geometry-consistent routing in a shared frozen patch-embedding space to determine routing and uses prototype-based scoring for anomaly detection within specific heads.

Result: Experiments on MVTec AD and VisA datasets demonstrate GCR's capability to improve routing stability, prevent performance collapse, achieve near-zero forgetting, and ensure competitive detection and localization.

Conclusion: GCR effectively addresses failures caused by decision-rule instability in cross-head routing, offering a robust solution for anomaly detection, particularly under continual category expansion scenarios.

Abstract: Feature-based anomaly detection is widely adopted in industrial inspection due to the strong representational power of large pre-trained vision encoders. While most existing methods focus on improving within-category anomaly scoring, practical deployments increasingly require task-agnostic operation under continual category expansion, where the category identity is unknown at test time. In this setting, overall performance is often dominated by expert selection, namely routing an input to an appropriate normality model before any head-specific scoring is applied. However, routing rules that compare head-specific anomaly scores across independently constructed heads are unreliable in practice, as score distributions can differ substantially across categories in scale and tail behavior.
  We propose GCR, a lightweight mixture-of-experts framework for stabilizing task-agnostic continual anomaly detection through geometry-consistent routing. GCR routes each test image directly in a shared frozen patch-embedding space by minimizing an accumulated nearest-prototype distance to category-specific prototype banks, and then computes anomaly maps only within the routed expert using a standard prototype-based scoring rule. By separating cross-head decision making from within-head anomaly scoring, GCR avoids cross-head score comparability issues without requiring end-to-end representation learning.
  Experiments on MVTec AD and VisA show that geometry-consistent routing substantially improves routing stability and mitigates continual performance collapse, achieving near-zero forgetting while maintaining competitive detection and localization performance. These results indicate that many failures previously attributed to representation forgetting can instead be explained by decision-rule instability in cross-head routing. Code is available at https://github.com/jw-chae/GCR

</details>


### [247] [RRNet: Configurable Real-Time Video Enhancement with Arbitrary Local Lighting Variations](https://arxiv.org/abs/2601.01865)
*Wenlong Yang,Canran Jin,Weihang Yuan,Chao Wang,Lifeng Sun*

Main category: cs.CV

TL;DR: RRNet is a fast and efficient method for real-time video enhancement, achieving state-of-the-art results under uneven lighting conditions.


<details>
  <summary>Details</summary>
Motivation: There is a growing need for real-time video enhancement in live applications, especially under challenging lighting conditions, where current methods struggle with speed and exposure control.

Method: RRNet introduces a depth-aware rendering module using virtual light sources for localized relighting, with a lightweight encoder and prediction head. It also employs a generative AI-based pipeline for synthesizing diverse lighting conditions during training.

Result: RRNet outperforms existing methods in low-light enhancement, localized illumination adjustment, and glare removal while maintaining real-time, high-resolution performance.

Conclusion: RRNet offers an effective blend of interpretability, efficiency, and visual quality, making it highly suitable for practical applications like video conferencing and mobile photography.

Abstract: With the growing demand for real-time video enhancement in live applications, existing methods often struggle to balance speed and effective exposure control, particularly under uneven lighting. We introduce RRNet (Rendering Relighting Network), a lightweight and configurable framework that achieves a state-of-the-art tradeoff between visual quality and efficiency. By estimating parameters for a minimal set of virtual light sources, RRNet enables localized relighting through a depth-aware rendering module without requiring pixel-aligned training data. This object-aware formulation preserves facial identity and supports real-time, high-resolution performance using a streamlined encoder and lightweight prediction head. To facilitate training, we propose a generative AI-based dataset creation pipeline that synthesizes diverse lighting conditions at low cost. With its interpretable lighting control and efficient architecture, RRNet is well suited for practical applications such as video conferencing, AR-based portrait enhancement, and mobile photography. Experiments show that RRNet consistently outperforms prior methods in low-light enhancement, localized illumination adjustment, and glare removal.

</details>


### [248] [Entity-Guided Multi-Task Learning for Infrared and Visible Image Fusion](https://arxiv.org/abs/2601.01870)
*Wenyu Shao,Hongbo Liu,Yunchuan Ma,Ruili Wang*

Main category: cs.CV

TL;DR: The paper introduces Entity-Guided Multi-Task learning (EGMT) for infrared and visible image fusion, utilizing entity-level textual information to address semantic noise and enhance image fusion quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on sentence-level textual information, which causes semantic noise and fails to fully utilize deeper textual semantics.

Method: EGMT extracts entity-level text, employs a multi-task learning architecture for fusion and classification, and includes a cross-modal module for visual and entity interaction.

Result: EGMT outperforms state-of-the-art methods in preserving salient targets, texture details, and semantic consistency.

Conclusion: EGMT effectively enhances infrared and visible image fusion through entity-level supervision, promising better semantic understanding and practical applicability with released annotated datasets.

Abstract: Existing text-driven infrared and visible image fusion approaches often rely on textual information at the sentence level, which can lead to semantic noise from redundant text and fail to fully exploit the deeper semantic value of textual information. To address these issues, we propose a novel fusion approach named Entity-Guided Multi-Task learning for infrared and visible image fusion (EGMT). Our approach includes three key innovative components: (i) A principled method is proposed to extract entity-level textual information from image captions generated by large vision-language models, eliminating semantic noise from raw text while preserving critical semantic information; (ii) A parallel multi-task learning architecture is constructed, which integrates image fusion with a multi-label classification task. By using entities as pseudo-labels, the multi-label classification task provides semantic supervision, enabling the model to achieve a deeper understanding of image content and significantly improving the quality and semantic density of the fused image; (iii) An entity-guided cross-modal interactive module is also developed to facilitate the fine-grained interaction between visual and entity-level textual features, which enhances feature representation by capturing cross-modal dependencies at both inter-visual and visual-entity levels. To promote the wide application of the entity-guided image fusion framework, we release the entity-annotated version of four public datasets (i.e., TNO, RoadScene, M3FD, and MSRS). Extensive experiments demonstrate that EGMT achieves superior performance in preserving salient targets, texture details, and semantic consistency, compared to the state-of-the-art methods. The code and dataset will be publicly available at https://github.com/wyshao-01/EGMT.

</details>


### [249] [CogFlow: Bridging Perception and Reasoning through Knowledge Internalization for Visual Mathematical Problem Solving](https://arxiv.org/abs/2601.01874)
*Shuhang Chen,Yunqiu Xu,Junjie Xie,Aojun Lu,Tao Feng,Zeying Huang,Ning Zhang,Yi Sun,Yi Yang,Hangjie Yuan*

Main category: cs.CV

TL;DR: The paper tackles the limitations of multimodal large language models in visual mathematical problem-solving by introducing CogFlow, a new cognitive-inspired framework that aligns perception, integration, and reasoning processes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue where large multimodal models fail to integrate and utilize extracted visual cues faithfully in mathematical reasoning.

Method: The authors propose CogFlow, a three-stage cognitive-inspired framework, enhancing perception, internalization, and reasoning stages. It includes Synergistic Visual Rewards, a Knowledge Internalization Reward model, and a Visual-Gated Policy Optimization algorithm.

Result: CogFlow demonstrates significant improvements on visual mathematical reasoning benchmarks due to its comprehensive framework and contributions like the MathCog dataset.

Conclusion: CogFlow effectively resolves key challenges in visual mathematical reasoning by ensuring the faithful usage of visual inputs in subsequent reasoning, setting new benchmarks in performance.

Abstract: Despite significant progress, multimodal large language models continue to struggle with visual mathematical problem solving. Some recent works recognize that visual perception is a bottleneck in visual mathematical reasoning, but their solutions are limited to improving the extraction and interpretation of visual inputs. Notably, they all ignore the key issue of whether the extracted visual cues are faithfully integrated and properly utilized in subsequent reasoning. Motivated by this, we present CogFlow, a novel cognitive-inspired three-stage framework that incorporates a knowledge internalization stage, explicitly simulating the hierarchical flow of human reasoning: perception$\Rightarrow$internalization$\Rightarrow$reasoning. Inline with this hierarchical flow, we holistically enhance all its stages. We devise Synergistic Visual Rewards to boost perception capabilities in parametric and semantic spaces, jointly improving visual information extraction from symbols and diagrams. To guarantee faithful integration of extracted visual cues into subsequent reasoning, we introduce a Knowledge Internalization Reward model in the internalization stage, bridging perception and reasoning. Moreover, we design a Visual-Gated Policy Optimization algorithm to further enforce the reasoning is grounded with the visual knowledge, preventing models seeking shortcuts that appear coherent but are visually ungrounded reasoning chains. Moreover, we contribute a new dataset MathCog for model training, which contains samples with over 120K high-quality perception-reasoning aligned annotations. Comprehensive experiments and analysis on commonly used visual mathematical reasoning benchmarks validate the superiority of the proposed CogFlow.

</details>


### [250] [Agentic AI in Remote Sensing: Foundations, Taxonomy, and Emerging Systems](https://arxiv.org/abs/2601.01891)
*Niloufar Alipour Talemi,Julia Boone,Fatemeh Afghah*

Main category: cs.CV

TL;DR: This survey reviews agentic AI in Earth Observation (EO), focusing on autonomous systems moving from static models to agent-based approaches for geospatial workflows.


<details>
  <summary>Details</summary>
Motivation: To shift Earth Observation analysis from static deep learning to autonomous agentic AI for enhanced geospatial intelligence.

Method: Provides a taxonomy of agentic AI (single-agent copilots vs multi-agent systems), and examines architectures, planning, benchmarks, and evaluates limitations.

Result: Reviews emerging evaluation benchmarks like trajectory-aware reasoning and highlights gaps in grounding, safety, and orchestration.

Conclusion: Outlines a roadmap for advancing autonomous geospatial intelligence through agentic AI, addressing current limitations and challenges.

Abstract: The paradigm of Earth Observation analysis is shifting from static deep learning models to autonomous agentic AI. Although recent vision foundation models and multimodal large language models advance representation learning, they often lack the sequential planning and active tool orchestration required for complex geospatial workflows. This survey presents the first comprehensive review of agentic AI in remote sensing. We introduce a unified taxonomy distinguishing between single-agent copilots and multi-agent systems while analyzing architectural foundations such as planning mechanisms, retrieval-augmented generation, and memory structures. Furthermore, we review emerging benchmarks that move the evaluation from pixel-level accuracy to trajectory-aware reasoning correctness. By critically examining limitations in grounding, safety, and orchestration, this work outlines a strategic roadmap for the development of robust, autonomous geospatial intelligence.

</details>


### [251] [Forget Less by Learning from Parents Through Hierarchical Relationships](https://arxiv.org/abs/2601.01892)
*Arjun Ramesh Kaushik,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Nalini K. Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: Custom Diffusion Models (CDMs) improve personalization in generative models but are prone to forgetting past concepts. A novel framework, Forget Less by Learning from Parents (FLLP), introduces a parent-child concept learning mechanism in hyperbolic space to mitigate this issue.


<details>
  <summary>Details</summary>
Motivation: The research addresses the challenge of catastrophic forgetting in Custom Diffusion Models (CDMs) when sequentially learning new concepts, aiming to balance retention of prior knowledge and integration of new concepts.

Method: FLLP embeds concept representations in a Lorentzian manifold to form parent-child relationships, where prior concepts guide the learning process of new ones, effectively leveraging hierarchical inter-concept interactions.

Result: The proposed FLLP framework was validated on three public datasets and one synthetic benchmark, demonstrating enhanced robustness and generalization compared to existing methods.

Conclusion: FLLP successfully mitigates forgetting in CDMs by utilizing hyperbolic space for hierarchical learning, enabling sustained knowledge retention and continual adaption to new concepts.

Abstract: Custom Diffusion Models (CDMs) offer impressive capabilities for personalization in generative modeling, yet they remain vulnerable to catastrophic forgetting when learning new concepts sequentially. Existing approaches primarily focus on minimizing interference between concepts, often neglecting the potential for positive inter-concept interactions. In this work, we present Forget Less by Learning from Parents (FLLP), a novel framework that introduces a parent-child inter-concept learning mechanism in hyperbolic space to mitigate forgetting. By embedding concept representations within a Lorentzian manifold, naturally suited to modeling tree-like hierarchies, we define parent-child relationships in which previously learned concepts serve as guidance for adapting to new ones. Our method not only preserves prior knowledge but also supports continual integration of new concepts. We validate FLLP on three public datasets and one synthetic benchmark, showing consistent improvements in both robustness and generalization.

</details>


### [252] [Nodule-DETR: A Novel DETR Architecture with Frequency-Channel Attention for Ultrasound Thyroid Nodule Detection](https://arxiv.org/abs/2601.01908)
*Jingjing Wang,Qianglin Liu,Zhuo Xiao,Xinning Yao,Bo Liu,Lu Li,Lijuan Niu,Fugen Zhou*

Main category: cs.CV

TL;DR: This paper presents Nodule-DETR, a novel transformer architecture for detecting thyroid nodules in ultrasound images, achieving superior diagnostic performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the diagnostic accuracy of detecting thyroid nodules in ultrasound images, a task hindered by low image contrast and blurred boundaries.

Method: Nodule-DETR integrates innovations like MSFCA for feature enhancement using frequency analysis, HFF for multi-scale integration, and MSDA for capturing small and irregular nodules.

Result: Experiments show Nodule-DETR surpassing baseline models with a significant performance boost of 0.149 in mAP@0.5:0.95 on a clinical thyroid ultrasound dataset.

Conclusion: Nodule-DETR demonstrates strong potential as an advanced, clinically applicable solution for computer-aided diagnosis of thyroid cancer.

Abstract: Thyroid cancer is the most common endocrine malignancy, and its incidence is rising globally. While ultrasound is the preferred imaging modality for detecting thyroid nodules, its diagnostic accuracy is often limited by challenges such as low image contrast and blurred nodule boundaries. To address these issues, we propose Nodule-DETR, a novel detection transformer (DETR) architecture designed for robust thyroid nodule detection in ultrasound images. Nodule-DETR introduces three key innovations: a Multi-Spectral Frequency-domain Channel Attention (MSFCA) module that leverages frequency analysis to enhance features of low-contrast nodules; a Hierarchical Feature Fusion (HFF) module for efficient multi-scale integration; and Multi-Scale Deformable Attention (MSDA) to flexibly capture small and irregularly shaped nodules. We conducted extensive experiments on a clinical dataset of real-world thyroid ultrasound images. The results demonstrate that Nodule-DETR achieves state-of-the-art performance, outperforming the baseline model by a significant margin of 0.149 in mAP@0.5:0.95. The superior accuracy of Nodule-DETR highlights its significant potential for clinical application as an effective tool in computer-aided thyroid diagnosis. The code of work is available at https://github.com/wjj1wjj/Nodule-DETR.

</details>


### [253] [Learning Action Hierarchies via Hybrid Geometric Diffusion](https://arxiv.org/abs/2601.01914)
*Arjun Ramesh Kaushik,Nalini K. Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: The paper introduces HybridTAS, a novel framework using both Euclidean and hyperbolic geometries in diffusion models for temporal action segmentation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Traditional iterative refinement approaches in temporal action segmentation fail to leverage the hierarchical nature of human actions.

Method: The framework employs hyperbolic geometry to model hierarchical action structures, embedding tree-like relationships. Diffusion denoising progresses coarse-to-fine, using abstract categories at higher timesteps and fine-grained classes at lower timesteps.

Result: HybridTAS delivers state-of-the-art performance on benchmark datasets GTEA, 50Salads, and Breakfast.

Conclusion: Incorporating hyperbolic-guided coarse-to-fine refinement in diffusion models successfully exploits action hierarchies, advancing temporal action segmentation.

Abstract: Temporal action segmentation is a critical task in video understanding, where the goal is to assign action labels to each frame in a video. While recent advances leverage iterative refinement-based strategies, they fail to explicitly utilize the hierarchical nature of human actions. In this work, we propose HybridTAS - a novel framework that incorporates a hybrid of Euclidean and hyperbolic geometries into the denoising process of diffusion models to exploit the hierarchical structure of actions. Hyperbolic geometry naturally provides tree-like relationships between embeddings, enabling us to guide the action label denoising process in a coarse-to-fine manner: higher diffusion timesteps are influenced by abstract, high-level action categories (root nodes), while lower timesteps are refined using fine-grained action classes (leaf nodes). Extensive experiments on three benchmark datasets, GTEA, 50Salads, and Breakfast, demonstrate that our method achieves state-of-the-art performance, validating the effectiveness of hyperbolic-guided denoising for the temporal action segmentation task.

</details>


### [254] [TalkPhoto: A Versatile Training-Free Conversational Assistant for Intelligent Image Editing](https://arxiv.org/abs/2601.01915)
*Yujie Hu,Zecheng Tang,Xu Jiang,Weiqi Li,Jian Zhang*

Main category: cs.CV

TL;DR: TalkPhoto introduces a training-free image editing framework using conversational interaction for precise manipulation without building a multi-instruction dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods for multimodal image editing using LLMs require extensive datasets and fail to achieve satisfactory results.

Method: TalkPhoto uses a designed prompt to hierarchically invoke existing editing methods, integrating complex tasks efficiently without additional training.

Result: TalkPhoto achieves stable, high-quality edits with accurate invocation and low token consumption across diverse tasks.

Conclusion: The framework enhances image editing precision, flexibility, and quality, eliminating the need for labor-intensive datasets and additional model training.

Abstract: Thanks to the powerful language comprehension capabilities of Large Language Models (LLMs), existing instruction-based image editing methods have introduced Multimodal Large Language Models (MLLMs) to promote information exchange between instructions and images, ensuring the controllability and flexibility of image editing. However, these frameworks often build a multi-instruction dataset to train the model to handle multiple editing tasks, which is not only time-consuming and labor-intensive but also fails to achieve satisfactory results. In this paper, we present TalkPhoto, a versatile training-free image editing framework that facilitates precise image manipulation through conversational interaction. We instruct the open-source LLM with a specially designed prompt template to analyze user needs after receiving instructions and hierarchically invoke existing advanced editing methods, all without additional training. Moreover, we implement a plug-and-play and efficient invocation of image editing methods, allowing complex and unseen editing tasks to be integrated into the current framework, achieving stable and high-quality editing results. Extensive experiments demonstrate that our method not only provides more accurate invocation with fewer token consumption but also achieves higher editing quality across various image editing tasks.

</details>


### [255] [AR-MOT: Autoregressive Multi-object Tracking](https://arxiv.org/abs/2601.01925)
*Lianjie Jia,Yuhan Wu,Binghao Ran,Yifan Wang,Lijun Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: This paper proposes AR-MOT, an LLM-based MOT framework that achieves flexible and task-general tracking via autoregressive sequence generation.


<details>
  <summary>Details</summary>
Motivation: The rigid architectures of existing MOT methods restrict adaptability to diverse tasks and complex tracking scenarios.

Method: AR-MOT utilizes an LLM framework for autoregressive output predictions, combined with an Object Tokenizer, RAA module, and TMF module for tracking enhancements.

Result: AR-MOT demonstrated competitive performance on MOT17 and DanceTrack datasets, validating its feasibility and flexibility.

Conclusion: AR-MOT provides a more general and extensible solution for MOT tasks by enabling easy integration of new modalities and instructions.

Abstract: As multi-object tracking (MOT) tasks continue to evolve toward more general and multi-modal scenarios, the rigid and task-specific architectures of existing MOT methods increasingly hinder their applicability across diverse tasks and limit flexibility in adapting to new tracking formulations. Most approaches rely on fixed output heads and bespoke tracking pipelines, making them difficult to extend to more complex or instruction-driven tasks. To address these limitations, we propose AR-MOT, a novel autoregressive paradigm that formulates MOT as a sequence generation task within a large language model (LLM) framework. This design enables the model to output structured results through flexible sequence construction, without requiring any task-specific heads. To enhance region-level visual perception, we introduce an Object Tokenizer based on a pretrained detector. To mitigate the misalignment between global and regional features, we propose a Region-Aware Alignment (RAA) module, and to support long-term tracking, we design a Temporal Memory Fusion (TMF) module that caches historical object tokens. AR-MOT offers strong potential for extensibility, as new modalities or instructions can be integrated by simply modifying the output sequence format without altering the model architecture. Extensive experiments on MOT17 and DanceTrack validate the feasibility of our approach, achieving performance comparable to state-of-the-art methods while laying the foundation for more general and flexible MOT systems.

</details>


### [256] [MacVQA: Adaptive Memory Allocation and Global Noise Filtering for Continual Visual Question Answering](https://arxiv.org/abs/2601.01926)
*Zhifei Li,Yiran Wang,Chenyi Xiong,Yujing Xia,Xiaoju Hou,Yue Zhao,Miao Zhang,Kui Xiao,Bing Yang*

Main category: cs.CV

TL;DR: The paper introduces MacVQA, a framework enhancing Visual Question Answering (VQA) through adaptive memory allocation and noise filtering, showing state-of-the-art results in continual learning.


<details>
  <summary>Details</summary>
Motivation: VQA models struggle to balance knowledge retention, adaptation, and robust feature representation, particularly in the context of continual learning tasks.

Method: The proposed MacVQA framework incorporates adaptive memory allocation and global noise filtering, using prototype-based memory management to optimize knowledge retention and feature quality.

Result: MacVQA achieved significant improvements, with 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.

Conclusion: MacVQA effectively balances knowledge retention, adaptation, and compositional generalization in VQA tasks, outperforming current baselines in continual learning setups.

Abstract: Visual Question Answering (VQA) requires models to reason over multimodal information, combining visual and textual data. With the development of continual learning, significant progress has been made in retaining knowledge and adapting to new information in the VQA domain. However, current methods often struggle with balancing knowledge retention, adaptation, and robust feature representation. To address these challenges, we propose a novel framework with adaptive memory allocation and global noise filtering called MacVQA for visual question answering. MacVQA fuses visual and question information while filtering noise to ensure robust representations, and employs prototype-based memory allocation to optimize feature quality and memory usage. These designs enable MacVQA to balance knowledge acquisition, retention, and compositional generalization in continual VQA learning. Experiments on ten continual VQA tasks show that MacVQA outperforms existing baselines, achieving 43.38% average accuracy and 2.32% average forgetting on standard tasks, and 42.53% average accuracy and 3.60% average forgetting on novel composition tasks.

</details>


### [257] [Face Normal Estimation from Rags to Riches](https://arxiv.org/abs/2601.01950)
*Meng Wang,Wenjing Dai,Jiawan Zhang,Xiaojie Guo*

Main category: cs.CV

TL;DR: This paper introduces a two-step framework for face normal estimation requiring less paired data and computational resources, combining coarse output with refinement using attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Current face normal estimation methods rely on large-scale paired data for training, which is a limitation. This paper seeks to reduce this dependency and improve efficiency.

Method: The method involves a two-step process: generating coarse face normals as guidance and refining these normals using a self-attention mechanism and a customized network.

Result: Experiments demonstrate that the proposed method is effective, superior to state-of-the-art methods in training efficiency and estimation quality.

Conclusion: The developed framework successfully reduces dependency on large datasets and provides high-quality facial normal estimations. The code and models are open-sourced for public use.

Abstract: Although recent approaches to face normal estimation have achieved promising results, their effectiveness heavily depends on large-scale paired data for training. This paper concentrates on relieving this requirement via developing a coarse-to-fine normal estimator. Concretely, our method first trains a neat model from a small dataset to produce coarse face normals that perform as guidance (called exemplars) for the following refinement. A self-attention mechanism is employed to capture long-range dependencies, thus remedying severe local artifacts left in estimated coarse facial normals. Then, a refinement network is customized for the sake of mapping input face images together with corresponding exemplars to fine-grained high-quality facial normals. Such a logical function split can significantly cut the requirement of massive paired data and computational resource. Extensive experiments and ablation studies are conducted to demonstrate the efficacy of our design and reveal its superiority over state-of-the-art methods in terms of both training expense as well as estimation quality. Our code and models are open-sourced at: https://github.com/AutoHDR/FNR2R.git.

</details>


### [258] [MotionAdapter: Video Motion Transfer via Content-Aware Attention Customization](https://arxiv.org/abs/2601.01955)
*Zhexin Zhang,Yifeng Zhu,Yangyang Xu,Long Chen,Yong Du,Shengfeng He,Jun Yu*

Main category: cs.CV

TL;DR: The paper introduces MotionAdapter, an advanced motion transfer framework for text-to-video (T2V) models using a diffusion transformer architecture.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of transferring complex motions between videos generated by diffusion-based T2V models, maintaining both motion quality and semantic coherence.

Method: The framework isolates motion through cross-frame attention, extracts motion fields, customizes them using a motion refinement module guided by DINO, and integrates motion during the denoising phase to transfer desired motions while preserving target characteristics.

Result: Experiments show that MotionAdapter outperforms existing methods in motion transfer, delivering higher-quality and semantically aligned results in both qualitative and quantitative measures.

Conclusion: MotionAdapter is an effective and robust framework for motion transfer, excelling in complex motion editing while maintaining appearance and semantic integrity between videos.

Abstract: Recent advances in diffusion-based text-to-video models, particularly those built on the diffusion transformer architecture, have achieved remarkable progress in generating high-quality and temporally coherent videos. However, transferring complex motions between videos remains challenging. In this work, we present MotionAdapter, a content-aware motion transfer framework that enables robust and semantically aligned motion transfer within DiT-based T2V models. Our key insight is that effective motion transfer requires \romannumeral1) explicit disentanglement of motion from appearance and \romannumeral 2) adaptive customization of motion to target content. MotionAdapter first isolates motion by analyzing cross-frame attention within 3D full-attention modules to extract attention-derived motion fields. To bridge the semantic gap between reference and target videos, we further introduce a DINO-guided motion customization module that rearranges and refines motion fields based on content correspondences. The customized motion field is then used to guide the DiT denoising process, ensuring that the synthesized video inherits the reference motion while preserving target appearance and semantics. Extensive experiments demonstrate that MotionAdapter outperforms state-of-the-art methods in both qualitative and quantitative evaluations. Moreover, MotionAdapter naturally supports complex motion transfer and motion editing tasks such as zooming.

</details>


### [259] [AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing](https://arxiv.org/abs/2601.01957)
*Tianbo Wang,Yuqing Ma,Kewei Liao,Zhange Zhang,Simin Li,Jinyang Guo,Xianglong Liu*

Main category: cs.CV

TL;DR: This paper introduces AFTER, a method to reduce object hallucination in Large Vision-Language Models (LVLMs) using adaptive activation steering and query-aware optimization, achieving improved performance on benchmarks.


<details>
  <summary>Details</summary>
Motivation: LVLMs often suffer from object hallucination induced by language bias, hindering their reliability in AI applications. Previous approaches lacked effective usage of factual textual semantics for mitigation.

Method: The paper proposes AFTER, combining Factual-Augmented Activation Steering (FAS) for general guidance and Query-Adaptive Offset Optimization (QAO) for query-specific adjustments to minimize biased activations.

Result: Experiments reveal that AFTER significantly reduces hallucination, achieving up to a 16.3% improvement on the AMBER benchmark, demonstrating its effectiveness across diverse LVLMs.

Conclusion: AFTER offers a robust framework for mitigating language-bias-induced hallucinations in LVLMs, enhancing their reliability while maintaining minimal cost. The method is validated with extensive tests.

Abstract: Large Vision-Language Models (LVLMs) have achieved substantial progress in cross-modal tasks. However, due to language bias, LVLMs are susceptible to object hallucination, which can be primarily divided into category, attribute, and relation hallucination, significantly impeding the trustworthy AI applications. Editing the internal activations of LVLMs has shown promising effectiveness in mitigating hallucinations with minimal cost. However, previous editing approaches neglect the effective guidance offered by factual textual semantics, thereby struggling to explicitly mitigate language bias. To address these issues, we propose Adaptive Factual-guided Visual-Textual Editing for hallucination mitigation (AFTER), which comprises Factual-Augmented Activation Steering (FAS) and Query-Adaptive Offset Optimization (QAO), to adaptively guides the original biased activations towards factual semantics. Specifically, FAS is proposed to provide factual and general guidance for activation editing, thereby explicitly modeling the precise visual-textual associations. Subsequently, QAO introduces a query-aware offset estimator to establish query-specific editing from the general steering vector, enhancing the diversity and granularity of editing. Extensive experiments on standard hallucination benchmarks across three widely adopted LVLMs validate the efficacy of the proposed AFTER, notably achieving up to a 16.3% reduction of hallucination over baseline on the AMBER benchmark. Our code and data will be released for reproducibility.

</details>


### [260] [Forget Less by Learning Together through Concept Consolidation](https://arxiv.org/abs/2601.01963)
*Arjun Ramesh Kaushik,Naresh Kumar Devulapally,Vishnu Suresh Lokhande,Nalini Ratha,Venu Govindaraju*

Main category: cs.CV

TL;DR: The paper introduces FL2T, a framework for overcoming catastrophic forgetting in Custom Diffusion Models, allowing concurrent and order-agnostic learning of concepts through inter-concept interactions.


<details>
  <summary>Details</summary>
Motivation: Existing Custom Diffusion Models fail to retain learned knowledge effectively while incorporating new concepts steadily, especially in sequential learning scenarios with fixed learning orders.

Method: The FL2T framework incorporates a set-invariant inter-concept learning module using proxies for feature selection and inter-concept guidance to enable improved knowledge retention and knowledge transfer.

Result: The proposed method achieved a significant 2% gain on average in CLIP Image Alignment scores across 10 tasks, effectively retaining old concepts while learning new ones on three datasets.

Conclusion: FL2T addresses catastrophic forgetting in concept learning, emphasizing the catalytic behavior of inter-concept guidance to improve concurrent and order-agnostic learning in Custom Diffusion Models.

Abstract: Custom Diffusion Models (CDMs) have gained significant attention due to their remarkable ability to personalize generative processes. However, existing CDMs suffer from catastrophic forgetting when continuously learning new concepts. Most prior works attempt to mitigate this issue under the sequential learning setting with a fixed order of concept inflow and neglect inter-concept interactions. In this paper, we propose a novel framework - Forget Less by Learning Together (FL2T) - that enables concurrent and order-agnostic concept learning while addressing catastrophic forgetting. Specifically, we introduce a set-invariant inter-concept learning module where proxies guide feature selection across concepts, facilitating improved knowledge retention and transfer. By leveraging inter-concept guidance, our approach preserves old concepts while efficiently incorporating new ones. Extensive experiments, across three datasets, demonstrates that our method significantly improves concept retention and mitigates catastrophic forgetting, highlighting the effectiveness of inter-concept catalytic behavior in incremental concept learning of ten tasks with at least 2% gain on average CLIP Image Alignment scores.

</details>


### [261] [Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation](https://arxiv.org/abs/2601.01984)
*Weijian Ma,Shizhao Sun,Tianyu Yu,Ruiyu Wang,Tat-Seng Chua,Jiang Bian*

Main category: cs.CV

TL;DR: The paper introduces a novel method to improve spatial reasoning in vision-language models (VLMs) using an object-centric blueprint approach, which records details about objects and enables effective reasoning.


<details>
  <summary>Details</summary>
Motivation: Current VLMs struggle with spatial reasoning due to either overemphasis on fine-grained perception or isolated localization, failing to capture the global organization of objects.

Method: The proposed method includes constructing a JSON-style blueprint of object attributes and spatial details, supervised fine-tuning, blueprint-aware rewards in reinforcement learning, and anti-shortcut data augmentation.

Result: The approach outperforms existing VLMs and spatial reasoning models in experiments, showcasing improved spatial understanding.

Conclusion: Integrating object-centric blueprints into VLMs provides better spatial reasoning capabilities and advances the understanding and semantic reasoning of spatial relationships.

Abstract: Spatial reasoning -- the ability to perceive and reason about relationships in space -- advances vision-language models (VLMs) from visual perception toward spatial semantic understanding. Existing approaches either revisit local image patches, improving fine-grained perception but weakening global spatial awareness, or mark isolated coordinates, which capture object locations but overlook their overall organization. In this work, we integrate the cognitive concept of an object-centric blueprint into VLMs to enhance spatial reasoning. Given an image and a question, the model first constructs a JSON-style blueprint that records the positions, sizes, and attributes of relevant objects, and then reasons over this structured representation to produce the final answer. To achieve this, we introduce three key techniques: (1) blueprint-embedded reasoning traces for supervised fine-tuning to elicit basic reasoning skills; (2) blueprint-aware rewards in reinforcement learning to encourage the blueprint to include an appropriate number of objects and to align final answers with this causal reasoning; and (3) anti-shortcut data augmentation that applies targeted perturbations to images and questions, discouraging reliance on superficial visual or linguistic cues. Experiments show that our method consistently outperforms existing VLMs and specialized spatial reasoning models.

</details>


### [262] [API: Empowering Generalizable Real-World Image Dehazing via Adaptive Patch Importance Learning](https://arxiv.org/abs/2601.01992)
*Chen Zhu,Huiwen Zhang,Yujie Li,Mu He,Xiaotian Qiao*

Main category: cs.CV

TL;DR: A novel framework named Adaptive Patch Importance-aware (API) is proposed for image dehazing, consisting of Automatic Haze Generation and Density-aware Haze Removal modules combined with a Multi-Negative Contrastive Dehazing loss, demonstrating effective results.


<details>
  <summary>Details</summary>
Motivation: Existing image dehazing methods struggle with complex real-world hazy scenes due to limited training data and haze density complexities.

Method: The API framework includes Automatic Haze Generation for realistic data augmentation, Density-aware Haze Removal for adaptive handling of haze distributions, and Multi-Negative Contrastive Dehazing loss to improve detail clarity.

Result: The framework achieves state-of-the-art performance on real-world benchmarks with robust generalization across diverse haze distributions.

Conclusion: The proposed API framework addresses the limitations of previous approaches by enhancing generalization and detail clarity in real-world image dehazing.

Abstract: Real-world image dehazing is a fundamental yet challenging task in low-level vision. Existing learning-based methods often suffer from significant performance degradation when applied to complex real-world hazy scenes, primarily due to limited training data and the intrinsic complexity of haze density distributions.To address these challenges, we introduce a novel Adaptive Patch Importance-aware (API) framework for generalizable real-world image dehazing. Specifically, our framework consists of an Automatic Haze Generation (AHG) module and a Density-aware Haze Removal (DHR) module. AHG provides a hybrid data augmentation strategy by generating realistic and diverse hazy images as additional high-quality training data. DHR considers hazy regions with varying haze density distributions for generalizable real-world image dehazing in an adaptive patch importance-aware manner. To alleviate the ambiguity of the dehazed image details, we further introduce a new Multi-Negative Contrastive Dehazing (MNCD) loss, which fully utilizes information from multiple negative samples across both spatial and frequency domains. Extensive experiments demonstrate that our framework achieves state-of-the-art performance across multiple real-world benchmarks, delivering strong results in both quantitative metrics and qualitative visual quality, and exhibiting robust generalization across diverse haze distributions.

</details>


### [263] [Nighttime Hazy Image Enhancement via Progressively and Mutually Reinforcing Night-Haze Priors](https://arxiv.org/abs/2601.01998)
*Chen Zhu,Huiwen Zhang,Mu He,Yujie Li,Xiaotian Qiao*

Main category: cs.CV

TL;DR: The paper introduces a framework to enhance visibility in nighttime hazy images by integrating haze and low-light priors and leveraging multiple levels of expertise for progressive image restoration.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing image restoration methods that address only one type of degradation at a time, without considering the interaction between haze and low-light degradation.

Method: The proposed framework integrates haze and low-light priors through a mutual reinforcement approach, involving image-, patch-, and pixel-level experts across visual and frequency domains. A frequency-aware router adapts the contributions of these experts to ensure robust restoration.

Result: Experiments demonstrate superior performance of the framework on nighttime dehazing benchmarks, with improved quantitative and qualitative results. The model also generalizes well to daytime dehazing and low-light enhancement tasks.

Conclusion: The study concludes that reinforcing the intrinsic consistency between haze and low-light priors leads to enhanced visibility in nighttime hazy images, and the proposed framework is effective and generalizable to other image restoration tasks.

Abstract: Enhancing the visibility of nighttime hazy images is challenging due to the complex degradation distributions. Existing methods mainly address a single type of degradation (e.g., haze or low-light) at a time, ignoring the interplay of different degradation types and resulting in limited visibility improvement. We observe that the domain knowledge shared between low-light and haze priors can be reinforced mutually for better visibility. Based on this key insight, in this paper, we propose a novel framework that enhances visibility in nighttime hazy images by reinforcing the intrinsic consistency between haze and low-light priors mutually and progressively. In particular, our model utilizes image-, patch-, and pixel-level experts that operate across visual and frequency domains to recover global scene structure, regional patterns, and fine-grained details progressively. A frequency-aware router is further introduced to adaptively guide the contribution of each expert, ensuring robust image restoration. Extensive experiments demonstrate the superior performance of our model on nighttime dehazing benchmarks both quantitatively and qualitatively. Moreover, we showcase the generalizability of our model in daytime dehazing and low-light enhancement tasks.

</details>


### [264] [Enhancing Object Detection with Privileged Information: A Model-Agnostic Teacher-Student Approach](https://arxiv.org/abs/2601.02016)
*Matthias Bartolo,Dylan Seychell,Gabriel Hili,Matthew Montebello,Carl James Debono,Saviour Formosa,Konstantinos Makantasis*

Main category: cs.CV

TL;DR: This paper presents a novel approach to use privileged information during training in object detection models through a teacher-student architecture, achieving improved accuracy without increasing model complexity.


<details>
  <summary>Details</summary>
Motivation: The paper aims to leverage fine-grained information available during training, often discarded during inference, to enhance object detection accuracy and generalization.

Method: A general, model-agnostic teacher-student architecture is introduced to incorporate privileged information, such as bounding box masks, saliency maps, and depth cues, into deep learning-based object detectors.

Result: Experiments across multiple benchmarks show consistent improvement in accuracy, especially for medium and large objects, without increasing inference complexity or model size.

Conclusion: The LUPI framework enhances object detection systems effectively and efficiently, making it suitable for resource-constrained and real-world applications.

Abstract: This paper investigates the integration of the Learning Using Privileged Information (LUPI) paradigm in object detection to exploit fine-grained, descriptive information available during training but not at inference. We introduce a general, model-agnostic methodology for injecting privileged information-such as bounding box masks, saliency maps, and depth cues-into deep learning-based object detectors through a teacher-student architecture. Experiments are conducted across five state-of-the-art object detection models and multiple public benchmarks, including UAV-based litter detection datasets and Pascal VOC 2012, to assess the impact on accuracy, generalization, and computational efficiency. Our results demonstrate that LUPI-trained students consistently outperform their baseline counterparts, achieving significant boosts in detection accuracy with no increase in inference complexity or model size. Performance improvements are especially marked for medium and large objects, while ablation studies reveal that intermediate weighting of teacher guidance optimally balances learning from privileged and standard inputs. The findings affirm that the LUPI framework provides an effective and practical strategy for advancing object detection systems in both resource-constrained and real-world settings.

</details>


### [265] [Towards Any-Quality Image Segmentation via Generative and Adaptive Latent Space Enhancement](https://arxiv.org/abs/2601.02018)
*Guangqian Guo,Aixi Ren,Yong Guo,Xuehui Yu,Jiacheng Tian,Wenli Li,Yaoxing Wang,Shan Gao*

Main category: cs.CV

TL;DR: The study introduces GleSAM++, a modified Segment Anything Model (SAM) for effectively handling low-quality and degraded images.


<details>
  <summary>Details</summary>
Motivation: Current SAMs struggle with segmentation tasks on low-quality or degraded images, limiting their real-world applications.

Method: The paper proposes GleSAM++ utilizing Generative Latent space Enhancement along with techniques like Feature Distribution Alignment (FDA), Channel Replication and Expansion (CRE), and Degradation-aware Adaptive Enhancement (DAE) for handling degraded image reconstruction.

Result: Experiments verify GleSAM++'s capability to improve segmentation robustness under complex degradation conditions and maintain performance on clear images and unseen degradations.

Conclusion: GleSAM++ enhances SAM’s segmentation effectiveness for degraded images with minimal additional parameters, improving real-world applicability and versatility in various image qualities.

Abstract: Segment Anything Models (SAMs), known for their exceptional zero-shot segmentation performance, have garnered significant attention in the research community. Nevertheless, their performance drops significantly on severely degraded, low-quality images, limiting their effectiveness in real-world scenarios. To address this, we propose GleSAM++, which utilizes Generative Latent space Enhancement to boost robustness on low-quality images, thus enabling generalization across various image qualities. Additionally, to improve compatibility between the pre-trained diffusion model and the segmentation framework, we introduce two techniques, i.e., Feature Distribution Alignment (FDA) and Channel Replication and Expansion (CRE). However, the above components lack explicit guidance regarding the degree of degradation. The model is forced to implicitly fit a complex noise distribution that spans conditions from mild noise to severe artifacts, which substantially increases the learning burden and leads to suboptimal reconstructions. To address this issue, we further introduce a Degradation-aware Adaptive Enhancement (DAE) mechanism. The key principle of DAE is to decouple the reconstruction process for arbitrary-quality features into two stages: degradation-level prediction and degradation-aware reconstruction. Our method can be applied to pre-trained SAM and SAM2 with only minimal additional learnable parameters, allowing for efficient optimization. Extensive experiments demonstrate that GleSAM++ significantly improves segmentation robustness on complex degradations while maintaining generalization to clear images. Furthermore, GleSAM++ also performs well on unseen degradations, underscoring the versatility of our approach and dataset.

</details>


### [266] [Adapting Depth Anything to Adverse Imaging Conditions with Events](https://arxiv.org/abs/2601.02020)
*Shihan Peng,Yuyang Xiong,Hanyu Zhou,Zhiwei Shi,Haoyue Liu,Gang Chen,Luxin Yan,Yi Chang*

Main category: cs.CV

TL;DR: The paper introduces ADAE, a fusion framework to improve Depth Anything model's performance under adverse conditions using event-based data.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of depth foundation models (like Depth Anything) under adverse imaging conditions such as extreme illumination and motion blur.

Method: Proposes ADAE, which fuses frame-based and event-based features while using entropy-aware spatial fusion and motion-guided temporal correction for robust depth estimation under degraded conditions.

Result: Demonstrated superior performance of ADAE over other methods through extensive experiments.

Conclusion: ADAE improves depth estimation in challenging scenarios by leveraging event-based cues and robust spatial-temporal fusion techniques.

Abstract: Robust depth estimation under dynamic and adverse lighting conditions is essential for robotic systems. Currently, depth foundation models, such as Depth Anything, achieve great success in ideal scenes but remain challenging under adverse imaging conditions such as extreme illumination and motion blur. These degradations corrupt the visual signals of frame cameras, weakening the discriminative features of frame-based depths across the spatial and temporal dimensions. Typically, existing approaches incorporate event cameras to leverage their high dynamic range and temporal resolution, aiming to compensate for corrupted frame features. However, such specialized fusion models are predominantly trained from scratch on domain-specific datasets, thereby failing to inherit the open-world knowledge and robust generalization inherent to foundation models. In this work, we propose ADAE, an event-guided spatiotemporal fusion framework for Depth Anything in degraded scenes. Our design is guided by two key insights: 1) Entropy-Aware Spatial Fusion. We adaptively merge frame-based and event-based features using an information entropy strategy to indicate illumination-induced degradation. 2) Motion-Guided Temporal Correction. We resort to the event-based motion cue to recalibrate ambiguous features in blurred regions. Under our unified framework, the two components are complementary to each other and jointly enhance Depth Anything under adverse imaging conditions. Extensive experiments have been performed to verify the superiority of the proposed method. Our code will be released upon acceptance.

</details>


### [267] [Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding](https://arxiv.org/abs/2601.02029)
*Toshihiko Nishimura,Hirofumi Abe,Kazuhiko Murasaki,Taiga Yoshida,Ryuichi Tanida*

Main category: cs.CV

TL;DR: The paper proposes a novel method for 3D semantic segmentation using projected 2D images from point clouds and foundation 2D models, achieving high accuracy without requiring annotated 3D training data.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of requiring annotated 3D training data and the limitations of supervised methods, like lack of open-vocabulary recognition.

Method: The method involves projecting 3D point clouds to 2D images using virtual cameras, performing 2D segmentation with a foundation model guided by language prompts, and aggregating predictions via weighted voting.

Result: The approach outperforms other training-free methods and achieves competitive accuracy compared to supervised techniques, while supporting open-vocabulary recognition.

Conclusion: This method offers a robust training-free solution for 3D semantic segmentation, expanding capabilities through open-vocabulary recognition and eliminating reliance on 3D ground truth data.

Abstract: This paper presents a novel 3D semantic segmentation method for large-scale point cloud data that does not require annotated 3D training data or paired RGB images. The proposed approach projects 3D point clouds onto 2D images using virtual cameras and performs semantic segmentation via a foundation 2D model guided by natural language prompts. 3D segmentation is achieved by aggregating predictions from multiple viewpoints through weighted voting. Our method outperforms existing training-free approaches and achieves segmentation accuracy comparable to supervised methods. Moreover, it supports open-vocabulary recognition, enabling users to detect objects using arbitrary text queries, thus overcoming the limitations of traditional supervised approaches.

</details>


### [268] [AlignVTOFF: Texture-Spatial Feature Alignment for High-Fidelity Virtual Try-Off](https://arxiv.org/abs/2601.02038)
*Yihan Zhu,Mengying Ge*

Main category: cs.CV

TL;DR: This paper introduces AlignVTOFF, a solution for the virtual try-off (VTOFF) problem, significantly enhancing image fidelity and detail preservation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing methods in synthesizing flat-lay garments, especially their inability to preserve structured patterns and fine-grained details in the generated images.

Method: AlignVTOFF combines a parallel U-Net framework with two main components: the Reference U-Net for multi-scale feature extraction to preserve geometric fidelity and complex patterns, and the TSFA mechanism for aligning texture and spatial features using hybrid attention modules.

Result: AlignVTOFF outperforms state-of-the-art methods, achieving superior structural realism and fidelity to high-frequency details in flat-lay garment image synthesis.

Conclusion: AlignVTOFF is an effective approach for VTOFF, offering a robust method for generating highly detailed and realistic flat-lay garments under complex conditions.

Abstract: Virtual Try-Off (VTOFF) is a challenging multimodal image generation task that aims to synthesize high-fidelity flat-lay garments under complex geometric deformation and rich high-frequency textures. Existing methods often rely on lightweight modules for fast feature extraction, which struggles to preserve structured patterns and fine-grained details, leading to texture attenuation during generation.To address these issues, we propose AlignVTOFF, a novel parallel U-Net framework built upon a Reference U-Net and Texture-Spatial Feature Alignment (TSFA). The Reference U-Net performs multi-scale feature extraction and enhances geometric fidelity, enabling robust modeling of deformation while retaining complex structured patterns. TSFA then injects the reference garment features into a frozen denoising U-Net via a hybrid attention design, consisting of a trainable cross-attention module and a frozen self-attention module. This design explicitly aligns texture and spatial cues and alleviates the loss of high-frequency information during the denoising process.Extensive experiments across multiple settings demonstrate that AlignVTOFF consistently outperforms state-of-the-art methods, producing flat-lay garment results with improved structural realism and high-frequency detail fidelity.

</details>


### [269] [Agentic Retoucher for Text-To-Image Generation](https://arxiv.org/abs/2601.02046)
*Shaocheng Shen,Jianfeng Liang. Chunlei Cai,Cong Geng,Huiyu Duan,Xiaoyun Zhang,Qiang Hu,Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper introduces Agentic Retoucher, a framework for fixing distortions in text-to-image generated outputs by utilizing a perception-reasoning-action loop. It demonstrates strong results in fixing distortions with perceptual reliability.


<details>
  <summary>Details</summary>
Motivation: The key motivation is to improve the distortion issues (e.g., in limbs, face, and text) in text-to-image diffusion models such as SDXL and FLUX, without relying on unreliable, costly, or semantically drifting methods that currently exist for refinement.

Method: The authors propose a hierarchical framework called Agentic Retoucher, featuring three agents: a perception agent for identifying image distortions, a reasoning agent for inferential diagnosis aligned with user preferences, and an action agent for inpainting corrections. The process simulates a human-like perception-reasoning-action loop.

Result: The Agentic Retoucher achieves significant improvements over existing state-of-the-art in perceptual quality, distortion localization, and user-aligned corrections. A dataset, GenBlemish-27K, was also constructed to enable fine-grained evaluation.

Conclusion: The paper establishes a novel, decision-driven paradigm for self-corrective and perceptually reliable text-to-image generation, overcoming previous limitations while improving user-aligned image refinement processes.

Abstract: Text-to-image (T2I) diffusion models such as SDXL and FLUX have achieved impressive photorealism, yet small-scale distortions remain pervasive in limbs, face, text and so on. Existing refinement approaches either perform costly iterative re-generation or rely on vision-language models (VLMs) with weak spatial grounding, leading to semantic drift and unreliable local edits. To close this gap, we propose Agentic Retoucher, a hierarchical decision-driven framework that reformulates post-generation correction as a human-like perception-reasoning-action loop. Specifically, we design (1) a perception agent that learns contextual saliency for fine-grained distortion localization under text-image consistency cues, (2) a reasoning agent that performs human-aligned inferential diagnosis via progressive preference alignment, and (3) an action agent that adaptively plans localized inpainting guided by user preference. This design integrates perceptual evidence, linguistic reasoning, and controllable correction into a unified, self-corrective decision process. To enable fine-grained supervision and quantitative evaluation, we further construct GenBlemish-27K, a dataset of 6K T2I images with 27K annotated artifact regions across 12 categories. Extensive experiments demonstrate that Agentic Retoucher consistently outperforms state-of-the-art methods in perceptual quality, distortion localization and human preference alignment, establishing a new paradigm for self-corrective and perceptually reliable T2I generation.

</details>


### [270] [PhysSFI-Net: Physics-informed Geometric Learning of Skeletal and Facial Interactions for Orthognathic Surgical Outcome Prediction](https://arxiv.org/abs/2601.02088)
*Jiahao Bao,Huazhen Liu,Yu Zhuang,Leran Tao,Xinyu Xu,Yongtao Shi,Mengjia Cheng,Yiming Wang,Congshuang Ku,Ting Zeng,Yilang Du,Siyi Chen,Shunyao Shen,Suncheng Xiang,Hongbo Yu*

Main category: cs.CV

TL;DR: PhysSFI-Net, a physics-informed geometric deep learning framework, accurately predicts facial morphology after jaw surgery using innovative components, outperforming existing models.


<details>
  <summary>Details</summary>
Motivation: Orthognathic surgery relies on accurate postoperative facial predictions for better planning and patient outcomes. Existing methods are either computationally expensive or lack interpretability, making improved solutions necessary.

Method: PhysSFI-Net integrates craniofacial encoders, attention mechanisms, a LSTM sequential predictor, and biomechanics-inspired reconstruction to model facial deformation post-surgery.

Result: The framework achieves highly accurate results, with minimal errors in point cloud shape (1.070 +/- 0.088 mm), surface deviation (1.296 +/- 0.349 mm), and landmark localization (2.445 +/- 1.326 mm), outperforming the ACMT-Net model.

Conclusion: PhysSFI-Net demonstrates strong potential for clinical application in orthognathic surgery by combining interpretability and superior predictive accuracy.

Abstract: Orthognathic surgery repositions jaw bones to restore occlusion and enhance facial aesthetics. Accurate simulation of postoperative facial morphology is essential for preoperative planning. However, traditional biomechanical models are computationally expensive, while geometric deep learning approaches often lack interpretability. In this study, we develop and validate a physics-informed geometric deep learning framework named PhysSFI-Net for precise prediction of soft tissue deformation following orthognathic surgery. PhysSFI-Net consists of three components: a hierarchical graph module with craniofacial and surgical plan encoders combined with attention mechanisms to extract skeletal-facial interaction features; a Long Short-Term Memory (LSTM)-based sequential predictor for incremental soft tissue deformation; and a biomechanics-inspired module for high-resolution facial surface reconstruction. Model performance was assessed using point cloud shape error (Hausdorff distance), surface deviation error, and landmark localization error (Euclidean distances of craniomaxillofacial landmarks) between predicted facial shapes and corresponding ground truths. A total of 135 patients who underwent combined orthodontic and orthognathic treatment were included for model training and validation. Quantitative analysis demonstrated that PhysSFI-Net achieved a point cloud shape error of 1.070 +/- 0.088 mm, a surface deviation error of 1.296 +/- 0.349 mm, and a landmark localization error of 2.445 +/- 1.326 mm. Comparative experiments indicated that PhysSFI-Net outperformed the state-of-the-art method ACMT-Net in prediction accuracy. In conclusion, PhysSFI-Net enables interpretable, high-resolution prediction of postoperative facial morphology with superior accuracy, showing strong potential for clinical application in orthognathic surgical planning and simulation.

</details>


### [271] [MCD-Net: A Lightweight Deep Learning Baseline for Optical-Only Moraine Segmentation](https://arxiv.org/abs/2601.02091)
*Zhehuan Cao,Fiseha Berhanu Tesema,Ping Fu,Jianfeng Ren,Ahmed Nasr*

Main category: cs.CV

TL;DR: This paper presents a lightweight solution for moraine segmentation using optical imagery, improving computational efficiency while establishing a reproducible benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in automated moraine segmentation caused by weak optical contrast and insufficient high-resolution DEMs.

Method: Introduced MCD-Net, a lightweight architecture combining MobileNetV2, CBAM, and DeepLabV3+, and manually annotated a large dataset of moraine regions.

Result: MCD-Net achieved 62.3% mIoU and 72.8% Dice coefficient while reducing computational costs by over 60%. Optical-only imagery was shown effective for moraine segmentation.

Conclusion: This study demonstrates the viability of optical imagery for moraine segmentation, provides publicly accessible tools, and establishes a benchmark for glacial monitoring.

Abstract: Glacial segmentation is essential for reconstructing past glacier dynamics and evaluating climate-driven landscape change. However, weak optical contrast and the limited availability of high-resolution DEMs hinder automated mapping. This study introduces the first large-scale optical-only moraine segmentation dataset, comprising 3,340 manually annotated high-resolution images from Google Earth covering glaciated regions of Sichuan and Yunnan, China. We develop MCD-Net, a lightweight baseline that integrates a MobileNetV2 encoder, a Convolutional Block Attention Module (CBAM), and a DeepLabV3+ decoder. Benchmarking against deeper backbones (ResNet152, Xception) shows that MCD-Net achieves 62.3\% mean Intersection over Union (mIoU) and 72.8\% Dice coefficient while reducing computational cost by more than 60\%. Although ridge delineation remains constrained by sub-pixel width and spectral ambiguity, the results demonstrate that optical imagery alone can provide reliable moraine-body segmentation. The dataset and code are publicly available at https://github.com/Lyra-alpha/MCD-Net, establishing a reproducible benchmark for moraine-specific segmentation and offering a deployable baseline for high-altitude glacial monitoring.

</details>


### [272] [InpaintHuman: Reconstructing Occluded Humans with Multi-Scale UV Mapping and Identity-Preserving Diffusion Inpainting](https://arxiv.org/abs/2601.02098)
*Jinlong Fan,Shanshan Zhao,Liang Zheng,Jing Zhang,Yuxiang Yang,Mingming Gong*

Main category: cs.CV

TL;DR: The paper introduces InpaintHuman, a method designed to reconstruct complete, animatable 3D human avatars from monocular videos in challenging scenarios like occlusion.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in reconstructing photorealistic and geometrically sound 3D human avatars from monocular videos, particularly under occluded conditions.

Method: The approach integrates a multi-scale UV-parameterized representation for robust reconstruction of occluded regions and an identity-preserving diffusion inpainting module for subject-specific completion.

Result: Experiments on synthetic datasets (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) validate the method's superior reconstruction quality across varied poses and viewpoints.

Conclusion: InpaintHuman enhances the reconstruction reliability and quality of animatable 3D human avatars under challenging monocular video conditions, marking consistent improvements over existing techniques.

Abstract: Reconstructing complete and animatable 3D human avatars from monocular videos remains challenging, particularly under severe occlusions. While 3D Gaussian Splatting has enabled photorealistic human rendering, existing methods struggle with incomplete observations, often producing corrupted geometry and temporal inconsistencies. We present InpaintHuman, a novel method for generating high-fidelity, complete, and animatable avatars from occluded monocular videos. Our approach introduces two key innovations: (i) a multi-scale UV-parameterized representation with hierarchical coarse-to-fine feature interpolation, enabling robust reconstruction of occluded regions while preserving geometric details; and (ii) an identity-preserving diffusion inpainting module that integrates textual inversion with semantic-conditioned guidance for subject-specific, temporally coherent completion. Unlike SDS-based methods, our approach employs direct pixel-level supervision to ensure identity fidelity. Experiments on synthetic benchmarks (PeopleSnapshot, ZJU-MoCap) and real-world scenarios (OcMotion) demonstrate competitive performance with consistent improvements in reconstruction quality across diverse poses and viewpoints.

</details>


### [273] [360-GeoGS: Geometrically Consistent Feed-Forward 3D Gaussian Splatting Reconstruction for 360 Images](https://arxiv.org/abs/2601.02102)
*Jiaqi Yao,Zhongmiao Yan,Jingyi Xu,Songpengcheng Xia,Yan Xiang,Ling Pei*

Main category: cs.CV

TL;DR: The paper introduces a novel 3D Gaussian Splatting framework for 360-degree images, achieving high visual quality and geometric consistency for 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current methods for scene reconstruction struggle with sparse views or inefficiency in real-time processing, often failing to ensure geometric consistency in low-texture areas or requiring scene-specific optimization.

Method: The authors propose a feed-forward 3D Gaussian Splatting framework with Depth-Normal geometric regularization to enhance geometric accuracy by supervising Gaussian properties such as rotation, scale, and position.

Result: The method enhances surface accuracy and point cloud quality while maintaining high-quality rendering, outperforming existing solutions in geometric consistency.

Conclusion: This approach improves 3D scene reconstruction by balancing visual quality and geometric reliability, making it effective for spatial perception tasks.

Abstract: 3D scene reconstruction is fundamental for spatial intelligence applications such as AR, robotics, and digital twins. Traditional multi-view stereo struggles with sparse viewpoints or low-texture regions, while neural rendering approaches, though capable of producing high-quality results, require per-scene optimization and lack real-time efficiency. Explicit 3D Gaussian Splatting (3DGS) enables efficient rendering, but most feed-forward variants focus on visual quality rather than geometric consistency, limiting accurate surface reconstruction and overall reliability in spatial perception tasks. This paper presents a novel feed-forward 3DGS framework for 360 images, capable of generating geometrically consistent Gaussian primitives while maintaining high rendering quality. A Depth-Normal geometric regularization is introduced to couple rendered depth gradients with normal information, supervising Gaussian rotation, scale, and position to improve point cloud and surface accuracy. Experimental results show that the proposed method maintains high rendering quality while significantly improving geometric consistency, providing an effective solution for 3D reconstruction in spatial perception tasks.

</details>


### [274] [HeadLighter: Disentangling Illumination in Generative 3D Gaussian Heads via Lightstage Captures](https://arxiv.org/abs/2601.02103)
*Yating Wang,Yuan Sun,Xuan Wang,Ran Yi,Boyao Zhou,Yipengjing Sun,Hongyu Liu,Yinuo Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: This paper presents HeadLighter, a supervised 3D head generative model designed for photorealistic and controllable relighting.


<details>
  <summary>Details</summary>
Motivation: Existing 3D head generative models struggle with disentangling illumination from intrinsic appearance, limiting relighting capabilities.

Method: The proposed HeadLighter framework uses a dual-branch architecture to model lighting-invariant attributes and physically grounded rendering components, with progressive disentanglement training using multi-view images captured under controlled light.

Result: The model achieves high-quality head image generation, real-time rendering, and supports explicit control over lighting and viewpoints.

Conclusion: HeadLighter successfully enables photorealistic, controllable head synthesis with disentangled appearance and illumination, providing a significant improvement over existing methods. The code and dataset will be available for public use.

Abstract: Recent 3D-aware head generative models based on 3D Gaussian Splatting achieve real-time, photorealistic and view-consistent head synthesis. However, a fundamental limitation persists: the deep entanglement of illumination and intrinsic appearance prevents controllable relighting. Existing disentanglement methods rely on strong assumptions to enable weakly supervised learning, which restricts their capacity for complex illumination. To address this challenge, we introduce HeadLighter, a novel supervised framework that learns a physically plausible decomposition of appearance and illumination in head generative models. Specifically, we design a dual-branch architecture that separately models lighting-invariant head attributes and physically grounded rendering components. A progressive disentanglement training is employed to gradually inject head appearance priors into the generative architecture, supervised by multi-view images captured under controlled light conditions with a light stage setup. We further introduce a distillation strategy to generate high-quality normals for realistic rendering. Experiments demonstrate that our method preserves high-quality generation and real-time rendering, while simultaneously supporting explicit lighting and viewpoint editing. We will publicly release our code and dataset.

</details>


### [275] [MagicFight: Personalized Martial Arts Combat Video Generation](https://arxiv.org/abs/2601.02107)
*Jiancheng Huang,Mingfu Yan,Songyan Chen,Yi Huang,Shifeng Chen*

Main category: cs.CV

TL;DR: MagicFight introduces a pioneering task of personalized martial arts combat video generation, countering identity confusion and action mismatches in two-person interactions.


<details>
  <summary>Details</summary>
Motivation: The paper recognizes a gap in personalized human video generation, specifically for two-person interactions in martial arts combat, where existing single-person models struggle with complex dynamics.

Method: It proposes MagicFight along with a bespoke dataset generated using Unity, integrating refined models to maintain identities and coherent interactions in combat scenes.

Result: MagicFight successfully generates high-fidelity two-person combat videos, overcoming challenges like identity confusion and ensuring coherent action sequences.

Conclusion: MagicFight lays the foundation for further innovations in personalized and interactive video content creation, addressing an uncharted domain in video generation.

Abstract: Amid the surge in generic text-to-video generation, the field of personalized human video generation has witnessed notable advancements, primarily concentrated on single-person scenarios. However, to our knowledge, the domain of two-person interactions, particularly in the context of martial arts combat, remains uncharted. We identify a significant gap: existing models for single-person dancing generation prove insufficient for capturing the subtleties and complexities of two engaged fighters, resulting in challenges such as identity confusion, anomalous limbs, and action mismatches. To address this, we introduce a pioneering new task, Personalized Martial Arts Combat Video Generation. Our approach, MagicFight, is specifically crafted to overcome these hurdles. Given this pioneering task, we face a lack of appropriate datasets. Thus, we generate a bespoke dataset using the game physics engine Unity, meticulously crafting a multitude of 3D characters, martial arts moves, and scenes designed to represent the diversity of combat. MagicFight refines and adapts existing models and strategies to generate high-fidelity two-person combat videos that maintain individual identities and ensure seamless, coherent action sequences, thereby laying the groundwork for future innovations in the realm of interactive video content creation.
  Website: https://MingfuYAN.github.io/MagicFight/
  Dataset: https://huggingface.co/datasets/MingfuYAN/KungFu-Fiesta

</details>


### [276] [Towards Long-window Anchoring in Vision-Language Model Distillation](https://arxiv.org/abs/2512.21576)
*Haoyi Zhou,Shuo Li,Tianyu Chen,Qi Song,Chonghan Gao,Jianxin Li*

Main category: cs.CV

TL;DR: The paper introduces LAid, a method to improve small vision-language models' long-context understanding via distillation and embedding adjustments, achieving up to 3.2x longer effective context windows.


<details>
  <summary>Details</summary>
Motivation: Small VLMs struggle with linguistics-photography alignment in longer contexts, limiting usability compared to the large models.

Method: The authors propose LAid with two components: distance-weighted attention matching and gain modulation for learnable Rotary Position Embeddings (RoPE).

Result: LAid-distilled models display up to 3.2 times longer effective context windows, improved standard benchmark performance, and maintain low-frequency attention components.

Conclusion: LAid offers practical techniques for efficient long-context VLMs and sheds light on positional understanding transfer during distillation.

Abstract: While large vision-language models (VLMs) demonstrate strong long-context understanding, their prevalent small branches fail on linguistics-photography alignment for a limited window size. We discover that knowledge distillation improves students' capability as a complement to Rotary Position Embeddings (RoPE) on window sizes (anchored from large models). Building on this insight, we propose LAid, which directly aims at the transfer of long-range attention mechanisms through two complementary components: (1) a progressive distance-weighted attention matching that dynamically emphasizes longer position differences during training, and (2) a learnable RoPE response gain modulation that selectively amplifies position sensitivity where needed. Extensive experiments across multiple model families demonstrate that LAid-distilled models achieve up to 3.2 times longer effective context windows compared to baseline small models, while maintaining or improving performance on standard VL benchmarks. Spectral analysis also suggests that LAid successfully preserves crucial low-frequency attention components that conventional methods fail to transfer. Our work not only provides practical techniques for building more efficient long-context VLMs but also offers theoretical insights into how positional understanding emerges and transfers during distillation.

</details>


### [277] [Car Drag Coefficient Prediction from 3D Point Clouds Using a Slice-Based Surrogate Model](https://arxiv.org/abs/2601.02112)
*Utkarsh Singh,Absaar Ali,Adarsh Roy*

Main category: cs.CV

TL;DR: The paper proposes a lightweight machine learning surrogate model to predict a vehicle's aerodynamic drag coefficient (Cd) efficiently and accurately using sequential slice-wise geometry analysis.


<details>
  <summary>Details</summary>
Motivation: The study addresses the inefficiency of traditional aerodynamic evaluation tools like computational fluid dynamics (CFD) and wind tunnel testing, which are time and resource intensive, limiting iterative design during early stages.

Method: The proposed model decomposes 3D vehicle geometries into stream-wise 2D cross-sectional slices. Each slice is processed by a lightweight PointNet2D module, and a bidirectional LSTM captures the sequence's geometric evolution. Training and evaluation were conducted using the DrivAerNet++ dataset.

Result: The model achieved high prediction performance (R^2 > 0.9528, MAE approx 6.046 x 10^{-3}) and demonstrated an inference time of 0.025 seconds per sample on a consumer-grade GPU.

Conclusion: The model offers a fast, accurate, and interpretable solution for aerodynamic Cd prediction, enabling more agile and informed design processes in automotive applications.

Abstract: The automotive industry's pursuit of enhanced fuel economy and performance necessitates efficient aerodynamic design. However, traditional evaluation methods such as computational fluid dynamics (CFD) and wind tunnel testing are resource intensive, hindering rapid iteration in the early design stages. Machine learning-based surrogate models offer a promising alternative, yet many existing approaches suffer from high computational complexity, limited interpretability, or insufficient accuracy for detailed geometric inputs. This paper introduces a novel lightweight surrogate model for the prediction of the aerodynamic drag coefficient (Cd) based on a sequential slice-wise processing of the geometry of the 3D vehicle. Inspired by medical imaging, 3D point clouds of vehicles are decomposed into an ordered sequence of 2D cross-sectional slices along the stream-wise axis. Each slice is encoded by a lightweight PointNet2D module, and the sequence of slice embeddings is processed by a bidirectional LSTM to capture longitudinal geometric evolution. The model, trained and evaluated on the DrivAerNet++ dataset, achieves a high coefficient of determination (R^2 > 0.9528) and a low mean absolute error (MAE approx 6.046 x 10^{-3}) in Cd prediction. With an inference time of approximately 0.025 seconds per sample on a consumer-grade GPU, our approach provides fast, accurate, and interpretable aerodynamic feedback, facilitating more agile and informed automotive design exploration.

</details>


### [278] [Remote Sensing Change Detection via Weak Temporal Supervision](https://arxiv.org/abs/2601.02126)
*Xavier Bou,Elliot Vincent,Gabriele Facciolo,Rafael Grompone von Gioi,Jean-Michel Morel,Thibaud Ehret*

Main category: cs.CV

TL;DR: The paper proposes a method for detecting land cover changes in remote sensing images by using weak temporal supervision and additional temporal observations, eliminating the need for further annotations.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of limited annotated datasets for semantic change detection in remote sensing due to the high cost and time required for pixel-level annotations.

Method: Introduced a weak temporal supervision strategy using additional temporal data paired with existing single-temporal datasets. A change detection model is trained with assumptions about real and artificial changes, complemented by object-aware change map generation and iterative refinement to manage label noise.

Result: Validated the approach on expanded aerial datasets (FLAIR and IAILD), achieving strong zero-shot and low-data regime performance in multiple benchmarks. Demonstrated its scalability with large area evaluations in France.

Conclusion: The method offers a scalable and effective solution for semantic change detection, reducing dependency on annotated datasets while maintaining strong performance.

Abstract: Semantic change detection in remote sensing aims to identify land cover changes between bi-temporal image pairs. Progress in this area has been limited by the scarcity of annotated datasets, as pixel-level annotation is costly and time-consuming. To address this, recent methods leverage synthetic data or generate artificial change pairs, but out-of-domain generalization remains limited. In this work, we introduce a weak temporal supervision strategy that leverages additional temporal observations of existing single-temporal datasets, without requiring any new annotations. Specifically, we extend single-date remote sensing datasets with new observations acquired at different times and train a change detection model by assuming that real bi-temporal pairs mostly contain no change, while pairing images from different locations to generate change examples. To handle the inherent noise in these weak labels, we employ an object-aware change map generation and an iterative refinement process. We validate our approach on extended versions of the FLAIR and IAILD aerial datasets, achieving strong zero-shot and low-data regime performance across different benchmarks. Lastly, we showcase results over large areas in France, highlighting the scalability potential of our method.

</details>


### [279] [Beyond Segmentation: An Oil Spill Change Detection Framework Using Synthetic SAR Imagery](https://arxiv.org/abs/2601.02139)
*Chenyang Lai,Shuaiyu Chen,Tianjin Huang,Siyang Song,Guangliang Cheng,Chunbo Luo,Zeyu Fu*

Main category: cs.CV

TL;DR: The paper addresses high false positives in oil spill detection using SAR imagery by introducing a bi-temporal task and a novel framework called TAHI, significantly enhancing detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Marine oil spills cause detrimental ecological and economic effects, and current SAR-based static single-image detection struggles with false positives and lacks reliability in data-scarce environments.

Method: The authors propose Oil Spill Change Detection (OSCD), a bi-temporal SAR task, along with the Temporal-Aware Hybrid Inpainting (TAHI) framework for generating synthetic pre-spill images to enhance detection.

Result: OSCD and TAHI significantly reduce false positives and achieve higher accuracy compared to traditional single-image segmentation methods.

Conclusion: Applying temporally-aware techniques like OSCD and TAHI can offer more reliable and scalable oil spill monitoring systems for real-world applications.

Abstract: Marine oil spills are urgent environmental hazards that demand rapid and reliable detection to minimise ecological and economic damage. While Synthetic Aperture Radar (SAR) imagery has become a key tool for large-scale oil spill monitoring, most existing detection methods rely on deep learning-based segmentation applied to single SAR images. These static approaches struggle to distinguish true oil spills from visually similar oceanic features (e.g., biogenic slicks or low-wind zones), leading to high false positive rates and limited generalizability, especially under data-scarce conditions. To overcome these limitations, we introduce Oil Spill Change Detection (OSCD), a new bi-temporal task that focuses on identifying changes between pre- and post-spill SAR images. As real co-registered pre-spill imagery is not always available, we propose the Temporal-Aware Hybrid Inpainting (TAHI) framework, which generates synthetic pre-spill images from post-spill SAR data. TAHI integrates two key components: High-Fidelity Hybrid Inpainting for oil-free reconstruction, and Temporal Realism Enhancement for radiometric and sea-state consistency. Using TAHI, we construct the first OSCD dataset and benchmark several state-of-the-art change detection models. Results show that OSCD significantly reduces false positives and improves detection accuracy compared to conventional segmentation, demonstrating the value of temporally-aware methods for reliable, scalable oil spill monitoring in real-world scenarios.

</details>


### [280] [Efficient Unrolled Networks for Large-Scale 3D Inverse Problems](https://arxiv.org/abs/2601.02141)
*Romain Vo,Julián Tachella*

Main category: cs.CV

TL;DR: The paper introduces a technique to overcome memory limitations and incorporate forward operators into deep learning models for large-scale 3D imaging problems, achieving superior results with minimal hardware.


<details>
  <summary>Details</summary>
Motivation: Overcoming memory limitations to enable deep learning models to incorporate imaging operators in large-scale 3D applications like cone-beam tomography and MRI.

Method: Proposes domain partitioning and normal operator approximations, allowing integration of forward operators into reconstruction models for 3D imaging on a single GPU.

Result: Achieves state-of-the-art results in 3D X-ray cone-beam tomography and 3D multi-coil MRI with efficient GPU usage.

Conclusion: The developed approach enables scalable and efficient training for large-scale 3D imaging problems, integrating forward operators and delivering superior reconstruction performance.

Abstract: Deep learning-based methods have revolutionized the field of imaging inverse problems, yielding state-of-the-art performance across various imaging domains. The best performing networks incorporate the imaging operator within the network architecture, typically in the form of deep unrolling. However, in large-scale problems, such as 3D imaging, most existing methods fail to incorporate the operator in the architecture due to the prohibitive amount of memory required by global forward operators, which hinder typical patching strategies. In this work, we present a domain partitioning strategy and normal operator approximations that enable the training of end-to-end reconstruction models incorporating forward operators of arbitrarily large problems into their architecture. The proposed method achieves state-of-the-art performance on 3D X-ray cone-beam tomography and 3D multi-coil accelerated MRI, while requiring only a single GPU for both training and inference.

</details>


### [281] [BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models](https://arxiv.org/abs/2601.02147)
*Sunny Gupta,Shounak Das,Amit Sethi*

Main category: cs.CV

TL;DR: The paper introduces a novel method, BiPrompt, to enhance robustness in vision-language models by addressing biases in both visual and textual modalities during test-time adaptation.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this study is to address the vulnerability of vision-language models (like CLIP) to spurious correlations, which compromise generalization and robustness under distribution shifts.

Method: The proposed method, BiPrompt, utilizes structured visual attention-guided erasure and balanced textual prompt normalization to suppress spurious features in both modalities during test-time adaptation.

Result: Extensive evaluations on real-world and synthetic benchmark datasets show BiPrompt significantly improves average and worst-group accuracies compared to existing test-time debiasing methods.

Conclusion: BiPrompt offers a lightweight yet effective solution for causally grounded vision-language adaptation without requiring model retraining or domain supervision.

Abstract: Vision language foundation models such as CLIP exhibit impressive zero-shot generalization yet remain vulnerable to spurious correlations across visual and textual modalities. Existing debiasing approaches often address a single modality either visual or textual leading to partial robustness and unstable adaptation under distribution shifts. We propose a bilateral prompt optimization framework (BiPrompt) that simultaneously mitigates non-causal feature reliance in both modalities during test-time adaptation. On the visual side, it employs structured attention-guided erasure to suppress background activations and enforce orthogonal prediction consistency between causal and spurious regions. On the textual side, it introduces balanced prompt normalization, a learnable re-centering mechanism that aligns class embeddings toward an isotropic semantic space. Together, these modules jointly minimize conditional mutual information between spurious cues and predictions, steering the model toward causal, domain invariant reasoning without retraining or domain supervision. Extensive evaluations on real-world and synthetic bias benchmarks demonstrate consistent improvements in both average and worst-group accuracies over prior test-time debiasing methods, establishing a lightweight yet effective path toward trustworthy and causally grounded vision-language adaptation.

</details>


### [282] [Why Commodity WiFi Sensors Fail at Multi-Person Gait Identification: A Systematic Analysis Using ESP32](https://arxiv.org/abs/2601.02177)
*Oliver Custance,Saad Khan,Simon Parkinson*

Main category: cs.CV

TL;DR: The paper investigates multi-person gait identification using WiFi Channel State Information (CSI), evaluates six signal separation methods, and concludes that low-cost commodity sensors struggle with this task due to hardware limitations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand whether the poor performance in multi-person identification using WiFi CSI is due to algorithmic limitations or fundamental hardware constraints.

Method: Six diverse signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) were systematically evaluated across various scenarios, using commodity ESP32 WiFi sensors.

Result: All evaluated methods performed poorly, achieving accuracy between 45-56% with statistically insignificant differences, highlighting hardware constraints as the primary limitation.

Conclusion: Commodity ESP32 WiFi sensors lack the capability to deliver reliable multi-person gait identification due to issues like high intra-subject variability and low inter-subject distinguishability.

Abstract: WiFi Channel State Information (CSI) has shown promise for single-person gait identification, with numerous studies reporting high accuracy. However, multi-person identification remains largely unexplored, with the limited existing work relying on complex, expensive setups requiring modified firmware. A critical question remains unanswered: is poor multi-person performance an algorithmic limitation or a fundamental hardware constraint? We systematically evaluate six diverse signal separation methods (FastICA, SOBI, PCA, NMF, Wavelet, Tensor Decomposition) across seven scenarios with 1-10 people using commodity ESP32 WiFi sensors--a simple, low-cost, off-the-shelf solution. Through novel diagnostic metrics (intra-subject variability, inter-subject distinguishability, performance degradation rate), we reveal that all methods achieve similarly low accuracy (45-56\%, $σ$=3.74\%) with statistically insignificant differences (p $>$ 0.05). Even the best-performing method, NMF, achieves only 56\% accuracy. Our analysis reveals high intra-subject variability, low inter-subject distinguishability, and severe performance degradation as person count increases, indicating that commodity ESP32 sensors cannot provide sufficient signal quality for reliable multi-person separation.

</details>


### [283] [QuIC: A Quantum-Inspired Interaction Classifier for Revitalizing Shallow CNNs in Fine-Grained Recognition](https://arxiv.org/abs/2601.02189)
*Cheng Ying Wu,Yen Jui Chang*

Main category: cs.CV

TL;DR: This paper introduces QuIC, a lightweight module inspired by quantum mechanics, which enhances visual classification on edge devices by improving shallow network accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle Fine-Grained Visual Classification challenges on resource-constrained edge devices, offering efficiency without sacrificing accuracy, unlike existing methods.

Method: QuIC models channels as quantum states and captures second-order feature covariance via a learnable operator, enabling stable end-to-end training with low feature dimensions.

Result: QuIC improves VGG16’s Top-1 accuracy by 20%, surpasses SE-Block on ResNet18, and offers strong qualitative results for fine-grained discrimination.

Conclusion: QuIC is a plug-and-play module that enhances shallow networks for FGVC, balancing efficiency, accuracy, and resource constraints on edge devices.

Abstract: Deploying deep learning models for Fine-Grained Visual Classification (FGVC) on resource-constrained edge devices remains a significant challenge. While deep architectures achieve high accuracy on benchmarks like CUB-200-2011, their computational cost is often prohibitive. Conversely, shallow networks (e.g., AlexNet, VGG) offer efficiency but fail to distinguish visually similar sub-categories. This is because standard Global Average Pooling (GAP) heads capture only first-order statistics, missing the subtle high-order feature interactions required for FGVC. While Bilinear CNNs address this, they suffer from high feature dimensionality and instability during training. To bridge this gap, we propose the Quantum-inspired Interaction Classifier (QuIC). Drawing inspiration from quantum mechanics, QuIC models feature channels as interacting quantum states and captures second-order feature covariance via a learnable observable operator. Designed as a lightweight, plug-and-play module, QuIC supports stable, single-stage end-to-end training without exploding feature dimensions. Experimental results demonstrate that QuIC significantly revitalizes shallow backbones: it boosts the Top-1 accuracy of VGG16 by nearly 20% and outperforms state-of-the-art attention mechanisms (SE-Block) on ResNet18. Qualitative analysis, including t-SNE visualization, further confirms that QuIC resolves ambiguous cases by explicitly attending to fine-grained discriminative features and enforcing compact intra-class clustering.

</details>


### [284] [Mind the Gap: Continuous Magnification Sampling for Pathology Foundation Models](https://arxiv.org/abs/2601.02198)
*Alexander Möllers,Julius Hense,Florian Schulz,Timo Milbich,Maximilian Alber,Lukas Ruff*

Main category: cs.CV

TL;DR: The paper explores the performance of histopathology foundation models across magnifications, proposing continuous magnification sampling and optimized distributions to improve accuracy, and introducing benchmarks for evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the poorly understood impact of magnification sampling during training on pathology foundation models and their performance across different magnifications.

Method: The authors model magnification sampling as a multi-source domain adaptation problem, propose continuous magnification sampling and optimized sampling distributions, and introduce two benchmarks (TCGA-MS, BRACS-MS) with tailored metrics for evaluation.

Result: Continuous sampling significantly reduces performance gaps at intermediate magnifications, improving balanced classification accuracy by up to 4 percentage points, and optimized distributions further enhance representation quality.

Conclusion: The study highlights magnification as a key factor influencing the performance of pathology models, laying groundwork for foundation models reliable across magnifications with refined sampling strategies.

Abstract: In histopathology, pathologists examine both tissue architecture at low magnification and fine-grained morphology at high magnification. Yet, the performance of pathology foundation models across magnifications and the effect of magnification sampling during training remain poorly understood. We model magnification sampling as a multi-source domain adaptation problem and develop a simple theoretical framework that reveals systematic trade-offs between sampling strategies. We show that the widely used discrete uniform sampling of magnifications (0.25, 0.5, 1.0, 2.0 mpp) leads to degradation at intermediate magnifications. We introduce continuous magnification sampling, which removes gaps in magnification coverage while preserving performance at standard scales. Further, we derive sampling distributions that optimize representation quality across magnification scales. To evaluate these strategies, we introduce two new benchmarks (TCGA-MS, BRACS-MS) with appropriate metrics. Our experiments show that continuous sampling substantially improves over discrete sampling at intermediate magnifications, with gains of up to 4 percentage points in balanced classification accuracy, and that optimized distributions can further improve performance. Finally, we evaluate current histopathology foundation models, finding that magnification is a primary driver of performance variation across models. Our work paves the way towards future pathology foundation models that perform reliably across magnifications.

</details>


### [285] [Parameter-Efficient Domain Adaption for CSI Crowd-Counting via Self-Supervised Learning with Adapter Modules](https://arxiv.org/abs/2601.02203)
*Oliver Custance,Saad Khan,Simon Parkinson,Quan Z. Sheng*

Main category: cs.CV

TL;DR: This paper proposes a device-free crowd-counting framework using WiFi Channel State Information (CSI) to address domain shift issues with self-supervised learning and lightweight Adapter modules.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the domain shift problem in crowd-counting models, where failure occurs in generalizing across environments.

Method: A CSI-ResNet-A model is developed, using contrastive pretraining for domain-invariant representations and adapter-based fine-tuning to efficiently achieve optimal adjustment.

Result: The model showed exceptional results with a Mean Absolute Error (MAE) of 0.44 on a 10-shot learning scenario, achieving state-of-the-art accuracy in public benchmarks and demonstrating high robustness.

Conclusion: The proposed framework provides a scalable, robust solution for privacy-conscious IoT deployments, combining efficiency and accuracy in sensing systems.

Abstract: Device-free crowd-counting using WiFi Channel State Information (CSI) is a key enabling technology for a new generation of privacy-preserving Internet of Things (IoT) applications. However, practical deployment is severely hampered by the domain shift problem, where models trained in one environment fail to generalise to another. To overcome this, we propose a novel two-stage framework centred on a CSI-ResNet-A architecture. This model is pre-trained via self-supervised contrastive learning to learn domain-invariant representations and leverages lightweight Adapter modules for highly efficient fine-tuning. The resulting event sequence is then processed by a stateful counting machine to produce a final, stable occupancy estimate. We validate our framework extensively. On our WiFlow dataset, our unsupervised approach excels in a 10-shot learning scenario, achieving a final Mean Absolute Error (MAE) of just 0.44--a task where supervised baselines fail. To formally quantify robustness, we introduce the Generalisation Index (GI), on which our model scores near-perfectly, confirming its ability to generalise. Furthermore, our framework sets a new state-of-the-art public WiAR benchmark with 98.8\% accuracy. Our ablation studies reveal the core strength of our design: adapter-based fine-tuning achieves performance within 1\% of a full fine-tune (98.84\% vs. 99.67\%) while training 97.2\% fewer parameters. Our work provides a practical and scalable solution for developing robust sensing systems ready for real-world IoT deployments.

</details>


### [286] [NextFlow: Unified Sequential Modeling Activates Multimodal Understanding and Generation](https://arxiv.org/abs/2601.02204)
*Huichao Zhang,Liao Qu,Yiheng Liu,Hang Chen,Yangyang Song,Yongsheng Dong,Shikun Sun,Xian Li,Xu Wang,Yi Jiang,Hu Ye,Bo Chen,Yiming Gao,Peng Liu,Akide Liu,Zhipeng Yang,Qili Deng,Linjie Xing,Jiyang Liu,Zhao Wang,Yang Zhou,Mingcong Liu,Yi Zhang,Qian He,Xiwei Hu,Zhongqi Qi,Jie Shao,Zhiye Fu,Shuai Wang,Fangmin Chen,Xuezhi Chai,Zhihua Wu,Yitong Wang,Zehuan Yuan,Daniel K. Du,Xinglong Wu*

Main category: cs.CV

TL;DR: NextFlow introduces a unified autoregressive transformer for multimodal text-image tasks, significantly enhancing generation capabilities across formats.


<details>
  <summary>Details</summary>
Motivation: To create a unified approach for multimodal understanding and generation, efficiently combining text and image data in a single model.

Method: The paper proposes a unified autoregressive architecture using next-token prediction for text and next-scale prediction for images. It introduces scalable multi-scale visual generation and robust training strategies.

Result: NextFlow achieves state-of-the-art benchmarks in multimodal generation and competes with specialized diffusion models in visual quality, generating high-quality images in faster time.

Conclusion: NextFlow is an innovative step forward in multimodal autoregressive models, showcasing efficiency and quality in text-image generation tasks.

Abstract: We present NextFlow, a unified decoder-only autoregressive transformer trained on 6 trillion interleaved text-image discrete tokens. By leveraging a unified vision representation within a unified autoregressive architecture, NextFlow natively activates multimodal understanding and generation capabilities, unlocking abilities of image editing, interleaved content and video generation. Motivated by the distinct nature of modalities - where text is strictly sequential and images are inherently hierarchical - we retain next-token prediction for text but adopt next-scale prediction for visual generation. This departs from traditional raster-scan methods, enabling the generation of 1024x1024 images in just 5 seconds - orders of magnitude faster than comparable AR models. We address the instabilities of multi-scale generation through a robust training recipe. Furthermore, we introduce a prefix-tuning strategy for reinforcement learning. Experiments demonstrate that NextFlow achieves state-of-the-art performance among unified models and rivals specialized diffusion baselines in visual quality.

</details>


### [287] [Seeing the Unseen: Zooming in the Dark with Event Cameras](https://arxiv.org/abs/2601.02206)
*Dachun Kai,Zeyu Xiao,Huyue Zhu,Jiaxiao Wang,Yueyi Zhang,Xiaoyan Sun*

Main category: cs.CV

TL;DR: RetinexEVSR is a new framework designed for low-light video super-resolution (LVSR) that uses event-driven signals and Retinex-inspired priors to achieve state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in LVSR, including recovering fine details from low-light, low-resolution inputs, where traditional methods struggle due to limited contrast and high-frequency information.

Method: The proposed method, RetinexEVSR, integrates high-contrast event signals with a novel bidirectional cross-modal fusion strategy. It includes an illumination-guided event enhancement module for refining event features and an event-guided reflectance enhancement module for dynamic recovery of reflectance details through multi-scale fusion.

Result: The RetinexEVSR framework outperformed other approaches, achieving state-of-the-art performance with a gain of up to 2.95 dB on the SDSD benchmark while reducing runtime by 65%.

Conclusion: RetinexEVSR effectively improves LVSR by leveraging high-contrast event signals and Retinex-inspired modules, resolving critical challenges of low-light video restoration with superior performance and efficiency.

Abstract: This paper addresses low-light video super-resolution (LVSR), aiming to restore high-resolution videos from low-light, low-resolution (LR) inputs. Existing LVSR methods often struggle to recover fine details due to limited contrast and insufficient high-frequency information. To overcome these challenges, we present RetinexEVSR, the first event-driven LVSR framework that leverages high-contrast event signals and Retinex-inspired priors to enhance video quality under low-light scenarios. Unlike previous approaches that directly fuse degraded signals, RetinexEVSR introduces a novel bidirectional cross-modal fusion strategy to extract and integrate meaningful cues from noisy event data and degraded RGB frames. Specifically, an illumination-guided event enhancement module is designed to progressively refine event features using illumination maps derived from the Retinex model, thereby suppressing low-light artifacts while preserving high-contrast details. Furthermore, we propose an event-guided reflectance enhancement module that utilizes the enhanced event features to dynamically recover reflectance details via a multi-scale fusion mechanism. Experimental results show that our RetinexEVSR achieves state-of-the-art performance on three datasets. Notably, on the SDSD benchmark, our method can get up to 2.95 dB gain while reducing runtime by 65% compared to prior event-based methods. Code: https://github.com/DachunKai/RetinexEVSR.

</details>


### [288] [Unraveling MMDiT Blocks: Training-free Analysis and Enhancement of Text-conditioned Diffusion](https://arxiv.org/abs/2601.02211)
*Binglei Li,Mengping Yang,Zhiyu Tan,Junping Zhang,Hao Li*

Main category: cs.CV

TL;DR: This paper conducts a systematic analysis of Multimodal Diffusion Transformers (MMDiT) models to understand their inner workings and proposes novel strategies to improve text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to uncover and analyze the contributions of different blocks within MMDiT-based models to enhance understanding of their synthesis processes and improve text-to-image generation.

Method: A systematic pipeline was developed to investigate block functionalities through removing, disabling, and enhancing textual hidden-states. New strategies for improved text alignment, editing, and processing acceleration are proposed without requiring additional training.

Result: The method outperformed baselines, improving metrics like T2I-Combench++ (from 56.92% to 63.00%) and GenEval (from 66.42% to 71.63%) on SD3.5, without compromising synthesis quality.

Conclusion: The analysis advances the understanding of MMDiT models and offers new methods that improve text-to-image generation, image editing, and model acceleration, paving the way for future advancements in this domain.

Abstract: Recent breakthroughs of transformer-based diffusion models, particularly with Multimodal Diffusion Transformers (MMDiT) driven models like FLUX and Qwen Image, have facilitated thrilling experiences in text-to-image generation and editing. To understand the internal mechanism of MMDiT-based models, existing methods tried to analyze the effect of specific components like positional encoding and attention layers. Yet, a comprehensive understanding of how different blocks and their interactions with textual conditions contribute to the synthesis process remains elusive. In this paper, we first develop a systematic pipeline to comprehensively investigate each block's functionality by removing, disabling and enhancing textual hidden-states at corresponding blocks. Our analysis reveals that 1) semantic information appears in earlier blocks and finer details are rendered in later blocks, 2) removing specific blocks is usually less disruptive than disabling text conditions, and 3) enhancing textual conditions in selective blocks improves semantic attributes. Building on these observations, we further propose novel training-free strategies for improved text alignment, precise editing, and acceleration. Extensive experiments demonstrated that our method outperforms various baselines and remains flexible across text-to-image generation, image editing, and inference acceleration. Our method improves T2I-Combench++ from 56.92% to 63.00% and GenEval from 66.42% to 71.63% on SD3.5, without sacrificing synthesis quality. These results advance understanding of MMDiT models and provide valuable insights to unlock new possibilities for further improvements.

</details>


### [289] [Prior-Guided DETR for Ultrasound Nodule Detection](https://arxiv.org/abs/2601.02212)
*Jingjing Wang,Zhuo Xiao,Xinning Yao,Bo Liu,Lijuan Niu,Xiangzhi Bai,Fugen Zhou*

Main category: cs.CV

TL;DR: The paper introduces a prior-guided DETR framework to address challenges in ultrasound nodule detection caused by irregular nodule shapes, indistinct boundaries, and speckle noise. The method outperforms 18 existing detection methods in accuracy.


<details>
  <summary>Details</summary>
Motivation: Detecting ultrasound nodules is critical for early diagnosis and treatment of thyroid and breast cancers, but challenges like irregular shapes, scale variations, and noise degrade detection capabilities.

Method: The framework integrates prior knowledge at different stages: SDFPR stabilizes feature extraction using geometric priors, MSFFM extracts structural priors emphasizing contours and noise suppression, and DFI propagates prior-modulated features across the network for improved query refinement.

Result: The method demonstrated superior accuracy in detecting complex nodules across multiple clinical and public datasets, outperforming 18 existing methods.

Conclusion: The proposed approach effectively incorporates priors to address critical challenges in ultrasound nodule detection, enhancing detection accuracy and reliability for clinical use.

Abstract: Accurate detection of ultrasound nodules is essential for the early diagnosis and treatment of thyroid and breast cancers. However, this task remains challenging due to irregular nodule shapes, indistinct boundaries, substantial scale variations, and the presence of speckle noise that degrades structural visibility. To address these challenges, we propose a prior-guided DETR framework specifically designed for ultrasound nodule detection. Instead of relying on purely data-driven feature learning, the proposed framework progressively incorporates different prior knowledge at multiple stages of the network. First, a Spatially-adaptive Deformable FFN with Prior Regularization (SDFPR) is embedded into the CNN backbone to inject geometric priors into deformable sampling, stabilizing feature extraction for irregular and blurred nodules. Second, a Multi-scale Spatial-Frequency Feature Mixer (MSFFM) is designed to extract multi-scale structural priors, where spatial-domain processing emphasizes contour continuity and boundary cues, while frequency-domain modeling captures global morphology and suppresses speckle noise. Furthermore, a Dense Feature Interaction (DFI) mechanism propagates and exploits these prior-modulated features across all encoder layers, enabling the decoder to enhance query refinement under consistent geometric and structural guidance. Experiments conducted on two clinically collected thyroid ultrasound datasets (Thyroid I and Thyroid II) and two public benchmarks (TN3K and BUSI) for thyroid and breast nodules demonstrate that the proposed method achieves superior accuracy compared with 18 detection methods, particularly in detecting morphologically complex nodules.The source code is publicly available at https://github.com/wjj1wjj/Ultrasound-DETR.

</details>


### [290] [FMVP: Masked Flow Matching for Adversarial Video Purification](https://arxiv.org/abs/2601.02228)
*Duoxun Tang,Xueyi Zhang,Chak Hin Wang,Xi Xiao,Dasen Dai,Xinhang Jiang,Wentao Shi,Rui Li,Qing Li*

Main category: cs.CV

TL;DR: The paper introduces FMVP, an approach that addresses adversarial attacks on video recognition models using Conditional Flow Matching and novel loss and training strategies, achieving high accuracy and robustness against various attacks.


<details>
  <summary>Details</summary>
Motivation: To create a more robust defense mechanism for video recognition models against adversarial attacks, overcoming limitations of existing purification methods that are inefficient and ineffective against subtle adversarial structures.

Method: FMVP employs Conditional Flow Matching (CFM) with an inpainting objective to reconstruct video dynamics, uses a Frequency-Gated Loss (FGL) to suppress adversarial noise, and implements training paradigms tailored to specific and general threats.

Result: FMVP achieves robust video classification accuracy surpassing state-of-the-art methods, with robust accuracies exceeding 87% against PGD and 89% against CW attacks, while also excelling in adversarial detection with up to 98% accuracy.

Conclusion: FMVP is an effective model for enhancing the robustness of video recognition systems, combining innovative purification, loss design, and training strategies to mitigate adversarial threats with significant success.

Abstract: Video recognition models remain vulnerable to adversarial attacks, while existing diffusion-based purification methods suffer from inefficient sampling and curved trajectories. Directly regressing clean videos from adversarial inputs often fails to recover faithful content due to the subtle nature of perturbations; this necessitates physically shattering the adversarial structure. Therefore, we propose Flow Matching for Adversarial Video Purification FMVP. FMVP physically shatters global adversarial structures via a masking strategy and reconstructs clean video dynamics using Conditional Flow Matching (CFM) with an inpainting objective. To further decouple semantic content from adversarial noise, we design a Frequency-Gated Loss (FGL) that explicitly suppresses high-frequency adversarial residuals while preserving low-frequency fidelity. We design Attack-Aware and Generalist training paradigms to handle known and unknown threats, respectively. Extensive experiments on UCF-101 and HMDB-51 demonstrate that FMVP outperforms state-of-the-art methods (DiffPure, Defense Patterns (DP), Temporal Shuffling (TS) and FlowPure), achieving robust accuracy exceeding 87% against PGD and 89% against CW attacks. Furthermore, FMVP demonstrates superior robustness against adaptive attacks (DiffHammer) and functions as a zero-shot adversarial detector, attaining detection accuracies of 98% for PGD and 79% for highly imperceptible CW attacks.

</details>


### [291] [VIBE: Visual Instruction Based Editor](https://arxiv.org/abs/2601.02242)
*Grigorii Alekseenko,Aleksandr Gordeev,Irina Tolstykh,Bulat Suleimanov,Vladimir Dokholyan,Georgii Fedorov,Sergey Yakubson,Aleksandra Tsybina,Mikhail Chernyshov,Maksim Kuprashevich*

Main category: cs.CV

TL;DR: The paper introduces a compact image editing pipeline utilizing a 2B Qwen3-VL model for guidance and 1.6B Sana1.5 diffusion model for image generation, achieving real-world quality with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of computational expense and limited real-world quality in instruction-based image editing using diffusion backbones.

Method: Developed a high-throughput pipeline leveraging modern models, optimizing for low-cost inference, source consistency, and high-quality edits, tested on ImgEdit and GEdit benchmarks.

Result: Proposed method outperforms larger baselines, achieves real-world quality edits (image attribute adjustment, object removal, background edits) within 24 GB GPU memory, and processes high-resolution images quickly.

Conclusion: A scalable and efficient image editing pipeline is presented, combining compact model architectures with high-edit quality, reducing deployment costs and maintaining strict source consistency.

Abstract: Instruction-based image editing is among the fastest developing areas in generative AI. Over the past year, the field has reached a new level, with dozens of open-source models released alongside highly capable commercial systems. However, only a limited number of open-source approaches currently achieve real-world quality. In addition, diffusion backbones, the dominant choice for these pipelines, are often large and computationally expensive for many deployments and research settings, with widely used variants typically containing 6B to 20B parameters. This paper presents a compact, high-throughput instruction-based image editing pipeline that uses a modern 2B-parameter Qwen3-VL model to guide the editing process and the 1.6B-parameter diffusion model Sana1.5 for image generation. Our design decisions across architecture, data processing, training configuration, and evaluation target low-cost inference and strict source consistency while maintaining high quality across the major edit categories feasible at this scale. Evaluated on the ImgEdit and GEdit benchmarks, the proposed method matches or exceeds the performance of substantially heavier baselines, including models with several times as many parameters and higher inference cost, and is particularly strong on edits that require preserving the input image, such as an attribute adjustment, object removal, background edits, and targeted replacement. The model fits within 24 GB of GPU memory and generates edited images at up to 2K resolution in approximately 4 seconds on an NVIDIA H100 in BF16, without additional inference optimizations or distillation.

</details>


### [292] [A Comparative Study of Custom CNNs, Pre-trained Models, and Transfer Learning Across Multiple Visual Datasets](https://arxiv.org/abs/2601.02246)
*Annoor Sharara Akhand*

Main category: cs.CV

TL;DR: The paper evaluates CNN approaches (custom CNN, fixed pre-trained CNN, transfer learning) across five image classification tasks, finding transfer learning performs best overall while custom CNNs balance efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Practitioners face a challenge in choosing the best CNN approach for specific visual recognition tasks while balancing accuracy and efficiency.

Method: The study conducts a comparative evaluation of three CNN methods (custom CNN, fixed pre-trained CNN, transfer learning) across five datasets using accuracy, macro F1-score, and efficiency metrics like training time and parameter counts.

Result: Transfer learning achieves the best predictive performance in all cases, whereas custom CNNs provide a trade-off between efficiency and accuracy, favoring constrained compute and memory scenarios.

Conclusion: Transfer learning is recommended for peak performance, while custom CNNs are viable for resource-limited environments.

Abstract: Convolutional Neural Networks (CNNs) are a standard approach for visual recognition due to their capacity to learn hierarchical representations from raw pixels. In practice, practitioners often choose among (i) training a compact custom CNN from scratch, (ii) using a large pre-trained CNN as a fixed feature extractor, and (iii) performing transfer learning via partial or full fine-tuning of a pre-trained backbone. This report presents a controlled comparison of these three paradigms across five real-world image classification datasets spanning road-surface defect recognition, agricultural variety identification, fruit/leaf disease recognition, pedestrian walkway encroachment recognition, and unauthorized vehicle recognition. Models are evaluated using accuracy and macro F1-score, complemented by efficiency metrics including training time per epoch and parameter counts. The results show that transfer learning consistently yields the strongest predictive performance, while the custom CNN provides an attractive efficiency--accuracy trade-off, especially when compute and memory budgets are constrained.

</details>


### [293] [SLGNet: Synergizing Structural Priors and Language-Guided Modulation for Multimodal Object Detection](https://arxiv.org/abs/2601.02249)
*Xiantai Xiang,Guangyao Zhou,Zixiao Wen,Wenshuai Li,Ben Niu,Feng Wang,Lijia Huang,Qiantong Wang,Yuhan Liu,Zongxu Pan,Yuxin Hu*

Main category: cs.CV

TL;DR: The paper proposes SLGNet, a new model for robust multimodal perception using RGB and Infrared images, achieving state-of-the-art detection performance while significantly reducing trainable parameters.


<details>
  <summary>Details</summary>
Motivation: To improve multimodal object detection in challenging scenarios (e.g., nighttime or high-contrast), where current models lose structural consistency and perform suboptimally due to static fusion mechanisms.

Method: Introducing SLGNet, which integrates a Structure-Aware Adapter for hierarchical structural cue extraction and a Language-Guided Modulation module to recalibrate visual features via structured captions, built upon a frozen Vision Transformer foundation.

Result: SLGNet achieves state-of-the-art performance on multiple datasets (LLVIP, FLIR, KAIST, and DroneVehicle) with an mAP of 66.1 on LLVIP, reducing trainable parameters by 87% compared to traditional methods.

Conclusion: SLGNet provides a robust and efficient parameter-saving solution for all-weather multimodal perception through leveraging structural priors and language-guided features.

Abstract: Multimodal object detection leveraging RGB and Infrared (IR) images is pivotal for robust perception in all-weather scenarios. While recent adapter-based approaches efficiently transfer RGB-pretrained foundation models to this task, they often prioritize model efficiency at the expense of cross-modal structural consistency. Consequently, critical structural cues are frequently lost when significant domain gaps arise, such as in high-contrast or nighttime environments. Moreover, conventional static multimodal fusion mechanisms typically lack environmental awareness, resulting in suboptimal adaptation and constrained detection performance under complex, dynamic scene variations. To address these limitations, we propose SLGNet, a parameter-efficient framework that synergizes hierarchical structural priors and language-guided modulation within a frozen Vision Transformer (ViT)-based foundation model. Specifically, we design a Structure-Aware Adapter to extract hierarchical structural representations from both modalities and dynamically inject them into the ViT to compensate for structural degradation inherent in ViT-based backbones. Furthermore, we propose a Language-Guided Modulation module that exploits VLM-driven structured captions to dynamically recalibrate visual features, thereby endowing the model with robust environmental awareness. Extensive experiments on the LLVIP, FLIR, KAIST, and DroneVehicle datasets demonstrate that SLGNet establishes new state-of-the-art performance. Notably, on the LLVIP benchmark, our method achieves an mAP of 66.1, while reducing trainable parameters by approximately 87% compared to traditional full fine-tuning. This confirms SLGNet as a robust and efficient solution for multimodal perception.

</details>


### [294] [VAR RL Done Right: Tackling Asynchronous Policy Conflicts in Visual Autoregressive Generation](https://arxiv.org/abs/2601.02256)
*Shikun Sun,Liao Qu,Huichao Zhang,Yiheng Liu,Yangyang Song,Xian Li,Xu Wang,Yi Jiang,Daniel K. Du,Xinglong Wu,Jia Jia*

Main category: cs.CV

TL;DR: The paper introduces a new framework to address asynchronous policy conflicts in Visual AutoRegressive (VAR) models using enhanced Group Relative Policy Optimization (GRPO).


<details>
  <summary>Details</summary>
Motivation: To resolve the challenges in unstable training and misalignment in reinforcement learning scenarios due to asynchronous policy conflicts in Visual AutoRegressive (VAR) models.

Method: The proposed framework integrates three components: an intermediate reward for stabilization, dynamic time-step reweighting for credit assignment, and a novel mask propagation algorithm based on Reward Feedback Learning (ReFL).

Result: The method achieves significant improvements in sample quality and objective alignment compared to the standard GRPO approach.

Conclusion: The proposed framework effectively optimizes Visual AutoRegressive (VAR) models by addressing asynchronous policy conflicts, leading to stable and robust training.

Abstract: Visual generation is dominated by three paradigms: AutoRegressive (AR), diffusion, and Visual AutoRegressive (VAR) models. Unlike AR and diffusion, VARs operate on heterogeneous input structures across their generation steps, which creates severe asynchronous policy conflicts. This issue becomes particularly acute in reinforcement learning (RL) scenarios, leading to unstable training and suboptimal alignment. To resolve this, we propose a novel framework to enhance Group Relative Policy Optimization (GRPO) by explicitly managing these conflicts. Our method integrates three synergistic components: 1) a stabilizing intermediate reward to guide early-stage generation; 2) a dynamic time-step reweighting scheme for precise credit assignment; and 3) a novel mask propagation algorithm, derived from principles of Reward Feedback Learning (ReFL), designed to isolate optimization effects both spatially and temporally. Our approach demonstrates significant improvements in sample quality and objective alignment over the vanilla GRPO baseline, enabling robust and effective optimization for VAR models.

</details>


### [295] [DiffProxy: Multi-View Human Mesh Recovery via Diffusion-Generated Dense Proxies](https://arxiv.org/abs/2601.02267)
*Renke Wang,Zhenyu Zhang,Ying Tai,Jian Yang*

Main category: cs.CV

TL;DR: The paper presents DiffProxy, a diffusion-based framework for human mesh recovery that combines synthetic precision and real-world generalization, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of imperfect real-world ground-truth annotations and domain gaps from synthetic data in human mesh recovery.

Method: The framework includes multi-view consistent human proxy generation, a hand refinement module, and an uncertainty-aware scaling method to exploit synthetic data and generative priors.

Result: DiffProxy, trained solely on synthetic data, achieves state-of-the-art zero-shot performance on five real-world benchmarks, excelling in challenging scenarios.

Conclusion: DiffProxy effectively bridges the gap between synthetic training and real-world applications, demonstrating its utility in human mesh recovery tasks.

Abstract: Human mesh recovery from multi-view images faces a fundamental challenge: real-world datasets contain imperfect ground-truth annotations that bias the models' training, while synthetic data with precise supervision suffers from domain gap. In this paper, we propose DiffProxy, a novel framework that generates multi-view consistent human proxies for mesh recovery. Central to DiffProxy is leveraging the diffusion-based generative priors to bridge the synthetic training and real-world generalization. Its key innovations include: (1) a multi-conditional mechanism for generating multi-view consistent, pixel-aligned human proxies; (2) a hand refinement module that incorporates flexible visual prompts to enhance local details; and (3) an uncertainty-aware test-time scaling method that increases robustness to challenging cases during optimization. These designs ensure that the mesh recovery process effectively benefits from the precise synthetic ground truth and generative advantages of the diffusion-based pipeline. Trained entirely on synthetic data, DiffProxy achieves state-of-the-art performance across five real-world benchmarks, demonstrating strong zero-shot generalization particularly on challenging scenarios with occlusions and partial views. Project page: https://wrk226.github.io/DiffProxy.html

</details>


### [296] [TopoLoRA-SAM: Topology-Aware Parameter-Efficient Adaptation of Foundation Segmenters for Thin-Structure and Cross-Domain Binary Semantic Segmentation](https://arxiv.org/abs/2601.02273)
*Salim Khazem*

Main category: cs.CV

TL;DR: The paper introduces TopoLoRA-SAM, a topology-aware and parameter-efficient framework to adapt foundation segmentation models for domain-specific binary semantic segmentation. It outperforms traditional methods on several benchmarks while training only 5.2% of model parameters.


<details>
  <summary>Details</summary>
Motivation: Foundation segmentation models face challenges in domain adaptation, especially for thin structures and noisy modalities. Full fine-tuning is computationally expensive, and the proposed method aims to address these issues efficiently.

Method: TopoLoRA-SAM uses Low-Rank Adaptation (LoRA) in a frozen ViT encoder, supported by a lightweight spatial convolutional adapter and optionally employs topology-aware supervision via differentiable clDice.

Result: The proposed method achieves the best average Dice scores across a variety of datasets, with significant accuracy improvements in challenging datasets like CHASE_DB1, demonstrating its superiority over specialist models.

Conclusion: Topology-aware and parameter-efficient adaptation methods like TopoLoRA-SAM can effectively adapt foundation segmentation models, providing superior performance with reduced computational cost.

Abstract: Foundation segmentation models such as the Segment Anything Model (SAM) exhibit strong zero-shot generalization through large-scale pretraining, but adapting them to domain-specific semantic segmentation remains challenging, particularly for thin structures (e.g., retinal vessels) and noisy modalities (e.g., SAR imagery). Full fine-tuning is computationally expensive and risks catastrophic forgetting. We propose \textbf{TopoLoRA-SAM}, a topology-aware and parameter-efficient adaptation framework for binary semantic segmentation. TopoLoRA-SAM injects Low-Rank Adaptation (LoRA) into the frozen ViT encoder, augmented with a lightweight spatial convolutional adapter and optional topology-aware supervision via differentiable clDice. We evaluate our approach on five benchmarks spanning retinal vessel segmentation (DRIVE, STARE, CHASE\_DB1), polyp segmentation (Kvasir-SEG), and SAR sea/land segmentation (SL-SSDD), comparing against U-Net, DeepLabV3+, SegFormer, and Mask2Former. TopoLoRA-SAM achieves the best retina-average Dice and the best overall average Dice across datasets, while training only \textbf{5.2\%} of model parameters ($\sim$4.9M). On the challenging CHASE\_DB1 dataset, our method substantially improves segmentation accuracy and robustness, demonstrating that topology-aware parameter-efficient adaptation can match or exceed fully fine-tuned specialist models. Code is available at : https://github.com/salimkhazem/Seglab.git

</details>


### [297] [InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams](https://arxiv.org/abs/2601.02281)
*Shuai Yuan,Yantai Yang,Xiaotian Yang,Xupeng Zhang,Zhonghao Zhao,Lingming Zhang,Zhipeng Zhang*

Main category: cs.CV

TL;DR: InfiniteVGGT introduces a novel approach to overcome the challenges in long-term 3D geometry processing with a causal visual geometry transformer and an adaptive "rolling memory" mechanism.


<details>
  <summary>Details</summary>
Motivation: To address the conflicting demands of scalability and stability in large-scale, persistent 3D visual geometry understanding, which current offline and streaming models fail to resolve effectively.

Method: InfiniteVGGT uses a causal visual geometry transformer with an adaptive KV cache for memory management. It employs training-free, attention-agnostic pruning to manage memory efficiently and operates in streaming mode with FlashAttention compatibility.

Result: InfiniteVGGT achieves infinite-horizon streaming while maintaining stability and outperforming other streaming methods, especially on continuous 3D estimation tasks over extremely long sequences.

Conclusion: InfiniteVGGT provides a breakthrough solution for live, scalable, and stable 3D visual geometry streaming. The introduction of the Long3D benchmark further enables standardized testing for extended sequence performance in the field.

Abstract: The grand vision of enabling persistent, large-scale 3D visual geometry understanding is shackled by the irreconcilable demands of scalability and long-term stability. While offline models like VGGT achieve inspiring geometry capability, their batch-based nature renders them irrelevant for live systems. Streaming architectures, though the intended solution for live operation, have proven inadequate. Existing methods either fail to support truly infinite-horizon inputs or suffer from catastrophic drift over long sequences. We shatter this long-standing dilemma with InfiniteVGGT, a causal visual geometry transformer that operationalizes the concept of a rolling memory through a bounded yet adaptive and perpetually expressive KV cache. Capitalizing on this, we devise a training-free, attention-agnostic pruning strategy that intelligently discards obsolete information, effectively ``rolling'' the memory forward with each new frame. Fully compatible with FlashAttention, InfiniteVGGT finally alleviates the compromise, enabling infinite-horizon streaming while outperforming existing streaming methods in long-term stability. The ultimate test for such a system is its performance over a truly infinite horizon, a capability that has been impossible to rigorously validate due to the lack of extremely long-term, continuous benchmarks. To address this critical gap, we introduce the Long3D benchmark, which, for the first time, enables a rigorous evaluation of continuous 3D geometry estimation on sequences about 10,000 frames. This provides the definitive evaluation platform for future research in long-term 3D geometry understanding. Code is available at: https://github.com/AutoLab-SAI-SJTU/InfiniteVGGT

</details>


### [298] [Rank-based Geographical Regularization: Revisiting Contrastive Self-Supervised Learning for Multispectral Remote Sensing Imagery](https://arxiv.org/abs/2601.02289)
*Tom Burgert,Leonard Hackel,Paolo Rota,Begüm Demir*

Main category: cs.CV

TL;DR: The paper proposes GeoRank, a regularization method that embeds geographical relationships into contrastive self-supervised learning for multispectral remote sensing images, achieving superior or comparable results to prior methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of applying self-supervised learning to multispectral remote sensing images due to their geographical and temporal variability.

Method: Introduces GeoRank, a novel regularization technique that optimizes spherical distances to incorporate geographical metadata. Systematically analyzes SSL adaptations for multispectral RS images (data augmentations, dataset size, temporal views).

Result: GeoRank outperforms or matches existing methods with geographical metadata and enhances various contrastive SSL algorithms.

Conclusion: The GeoRank method effectively integrates geographical relationships, advancing SSL on multispectral RS images. The study provides insights into key SSL adaptations in this domain.

Abstract: Self-supervised learning (SSL) has become a powerful paradigm for learning from large, unlabeled datasets, particularly in computer vision (CV). However, applying SSL to multispectral remote sensing (RS) images presents unique challenges and opportunities due to the geographical and temporal variability of the data. In this paper, we introduce GeoRank, a novel regularization method for contrastive SSL that improves upon prior techniques by directly optimizing spherical distances to embed geographical relationships into the learned feature space. GeoRank outperforms or matches prior methods that integrate geographical metadata and consistently improves diverse contrastive SSL algorithms (e.g., BYOL, DINO). Beyond this, we present a systematic investigation of key adaptations of contrastive SSL for multispectral RS images, including the effectiveness of data augmentations, the impact of dataset cardinality and image size on performance, and the task dependency of temporal views. Code is available at https://github.com/tomburgert/georank.

</details>


### [299] [SortWaste: A Densely Annotated Dataset for Object Detection in Industrial Waste Sorting](https://arxiv.org/abs/2601.02299)
*Sara Inácio,Hugo Proença,João C. Neves*

Main category: cs.CV

TL;DR: This paper introduces the SortWaste dataset for automated waste sorting and proposes ClutterScore, a metric to assess scene complexity in such tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the inefficiencies in current manual and automated waste sorting processes, specifically the lack of standardized real-world datasets and tools to handle the complexity of waste sorting.

Method: The authors developed SortWaste, a new annotated object detection dataset for waste sorting, and proposed ClutterScore to measure scene hardness. They also benchmarked object detection models using this metric.

Result: The benchmark achieved a promising mAP of 59.7% for plastic-only detection but highlighted significant performance drops in highly cluttered scenes.

Conclusion: Automated waste sorting systems demand more challenging datasets and improved methods to handle complex scenes effectively, as suggested by the results of this study.

Abstract: The increasing production of waste, driven by population growth, has created challenges in managing and recycling materials effectively. Manual waste sorting is a common practice; however, it remains inefficient for handling large-scale waste streams and presents health risks for workers. On the other hand, existing automated sorting approaches still struggle with the high variability, clutter, and visual complexity of real-world waste streams. The lack of real-world datasets for waste sorting is a major reason automated systems for this problem are underdeveloped. Accordingly, we introduce SortWaste, a densely annotated object detection dataset collected from a Material Recovery Facility. Additionally, we contribute to standardizing waste detection in sorting lines by proposing ClutterScore, an objective metric that gauges the scene's hardness level using a set of proxies that affect visual complexity (e.g., object count, class and size entropy, and spatial overlap). In addition to these contributions, we provide an extensive benchmark of state-of-the-art object detection models, detailing their results with respect to the hardness level assessed by the proposed metric. Despite achieving promising results (mAP of 59.7% in the plastic-only detection task), performance significantly decreases in highly cluttered scenes. This highlights the need for novel and more challenging datasets on the topic.

</details>


### [300] [360DVO: Deep Visual Odometry for Monocular 360-Degree Camera](https://arxiv.org/abs/2601.02309)
*Xiaopeng Guo,Yinzhe Xu,Huajian Huang,Sai-Kit Yeung*

Main category: cs.CV

TL;DR: 360DVO is a deep learning-based monocular omnidirectional visual odometry system introducing a distortion-aware spherical feature extractor and a novel omnidirectional differentiable bundle adjustment module, significantly improving robustness and accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: To overcome the lack of robustness in existing OVO systems under challenging scenarios like aggressive motion and illumination changes.

Method: 360DVO introduces (1) a distortion-aware spherical feature extractor (DAS-Feat) for learning resistant features from 360-degree images and (2) an omnidirectional differentiable bundle adjustment (ODBA) module for enhanced pose estimation.

Result: 360DVO demonstrates a 50% improvement in robustness and a 37.5% improvement in accuracy over state-of-the-art baselines in experiments on both synthetic and real-world datasets.

Conclusion: 360DVO establishes the potential of deep learning techniques in monocular omnidirectional visual odometry, achieving significant advancements in performance over traditional methods.

Abstract: Monocular omnidirectional visual odometry (OVO) systems leverage 360-degree cameras to overcome field-of-view limitations of perspective VO systems. However, existing methods, reliant on handcrafted features or photometric objectives, often lack robustness in challenging scenarios, such as aggressive motion and varying illumination. To address this, we present 360DVO, the first deep learning-based OVO framework. Our approach introduces a distortion-aware spherical feature extractor (DAS-Feat) that adaptively learns distortion-resistant features from 360-degree images. These sparse feature patches are then used to establish constraints for effective pose estimation within a novel omnidirectional differentiable bundle adjustment (ODBA) module. To facilitate evaluation in realistic settings, we also contribute a new real-world OVO benchmark. Extensive experiments on this benchmark and public synthetic datasets (TartanAir V2 and 360VO) demonstrate that 360DVO surpasses state-of-the-art baselines (including 360VO and OpenVSLAM), improving robustness by 50% and accuracy by 37.5%. Homepage: https://chris1004336379.github.io/360DVO-homepage

</details>


### [301] [Prithvi-Complimentary Adaptive Fusion Encoder (CAFE): unlocking full-potential for flood inundation mapping](https://arxiv.org/abs/2601.02315)
*Saurabh Kaushik,Lalit Maurya,Beth Tellman*

Main category: cs.CV

TL;DR: The paper presents Prithvi-CAFE, an enhanced version of Geo-Foundation Models (GFMs), demonstrating state-of-the-art performance in flood mapping tasks using Sen1Flood11 and FloodPlanet datasets.


<details>
  <summary>Details</summary>
Motivation: Standard GFMs, including pretrained encoders, struggle to capture critical local nuances in flood mapping tasks, presenting a limitation in their effectiveness compared to baseline methods like U-Net.

Method: Prithvi-CAFE integrates a Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM), enabling multi-scale, multi-level fusion of features while preserving local details and long-range dependencies.

Result: Prithvi-CAFE outperformed baseline methods and existing GFMs on flood mapping tasks, achieving higher IoU metrics on Sen1Flood11 and FloodPlanet datasets, effectively showing its capability in critical segmentation tasks.

Conclusion: Prithvi-CAFE demonstrates significant improvements in extracting complementary information from multi-channel and multi-modal data for segmentation tasks, proving its potential for broader applications in tasks requiring local detail capture.

Abstract: Geo-Foundation Models (GFMs), have proven effective in diverse downstream applications, including semantic segmentation, classification, and regression tasks. However, in case of flood mapping using Sen1Flood11 dataset as a downstream task, GFMs struggles to outperform the baseline U-Net, highlighting model's limitation in capturing critical local nuances. To address this, we present the Prithvi-Complementary Adaptive Fusion Encoder (CAFE), which integrate Prithvi GFM pretrained encoder with a parallel CNN residual branch enhanced by Convolutional Attention Modules (CAM). Prithvi-CAFE enables fast and efficient fine-tuning through adapters in Prithvi and performs multi-scale, multi-level fusion with CNN features, capturing critical local details while preserving long-range dependencies. We achieve state-of-the-art results on two comprehensive flood mapping datasets: Sen1Flood11 and FloodPlanet. On Sen1Flood11 test data, Prithvi-CAFE (IoU 83.41) outperforms the original Prithvi (IoU 82.50) and other major GFMs (TerraMind 82.90, DOFA 81.54, spectralGPT: 81.02). The improvement is even more pronounced on the hold-out test site, where Prithvi-CAFE achieves an IoU of 81.37 compared to the baseline U-Net (70.57) and original Prithvi (72.42). On FloodPlanet, Prithvi-CAFE also surpasses the baseline U-Net and other GFMs, achieving an IoU of 64.70 compared to U-Net (60.14), Terramind (62.33), DOFA (59.15) and Prithvi 2.0 (61.91). Our proposed simple yet effective Prithvi-CAFE demonstrates strong potential for improving segmentation tasks where multi-channel and multi-modal data provide complementary information and local details are critical. The code is released on \href{https://github.com/Sk-2103/Prithvi-CAFE}{Prithvi-CAFE Github}

</details>


### [302] [Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching](https://arxiv.org/abs/2601.02318)
*Roja Sahoo,Anoop Namboodiri*

Main category: cs.CV

TL;DR: The paper introduces Fusion2Print (F2P), a framework for enhanced contactless fingerprint recognition by fusing flash and non-flash fingerprint images for improved ridge clarity and recognition accuracy.


<details>
  <summary>Details</summary>
Motivation: Contactless fingerprint systems face challenges in ridge clarity due to factors such as illumination and skin discoloration, which hinder recognition accuracy.

Method: Fusion2Print (F2P) integrates paired flash and non-flash fingerprints using a custom dataset and attention-based fusion techniques, coupled with manual subtraction, U-Net enhancement, and a deep embedding model for cross-domain verification.

Result: F2P achieves improved fingerprint ridge clarity and recognition with high accuracy (AUC=0.999, EER=1.12%) compared to existing methods.

Conclusion: Fusion2Print provides a robust solution for contactless fingerprint recognition with enhanced ridge detail and verification performance, proving effective for both contactless and contact-based systems.

Abstract: Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).

</details>


### [303] [BEDS: Bayesian Emergent Dissipative Structures](https://arxiv.org/abs/2601.02329)
*Laurent Caraffa*

Main category: cs.CV

TL;DR: The paper introduces the BEDS framework, uniting thermodynamics, Bayesian inference, and machine learning into a theory connecting learning and entropy export, with practical AI applications.


<details>
  <summary>Details</summary>
Motivation: The motivation is to unify disciplines like physics, Bayesian inference, and computation to explain learning processes and create energy-efficient, sustainable AI systems.

Method: The authors propose BEDS, connecting thermodynamics and Bayesian updating. They operationalize these concepts in a network achieving energy efficiency and improved learning capabilities.

Result: The BEDS framework demonstrates significantly higher energy efficiency in distributed systems while supporting sustained learning dynamics.

Conclusion: The work bridges multiple scientific domains, presenting both theoretical and practical insights into sustainable computation and learning systems using thermodynamic principles.

Abstract: We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine's theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.
  We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel's incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.
  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.

</details>


### [304] [Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding](https://arxiv.org/abs/2601.02339)
*Jingming He,Chongyi Li,Shiqi Wang,Sam Kwong*

Main category: cs.CV

TL;DR: This paper proposes improvements for 3D semantic Gaussian modeling by enhancing the synergy between semantic segmentation and image rendering, while addressing limitations in current approaches.


<details>
  <summary>Details</summary>
Motivation: Current methods often neglect the 3D Gaussian geometry and rely solely on 2D supervision, leading to inefficiencies, especially in challenging areas with subtle or textureless details.

Method: The paper introduces an anisotropic 3D Gaussian Chebyshev descriptor to enhance shape representation and adaptively tunes Gaussian allocation using local semantic and shape signals. A knowledge transfer module continuously updates learned shapes across scenes.

Result: The proposed framework demonstrates better segmentation accuracy and improved rendering quality across various datasets, while maintaining high rendering frame rates.

Conclusion: This enhanced modeling approach offers a more effective and efficient framework for 3D semantic representation, providing improvements in accuracy, rendering quality, and convergence speed.

Abstract: Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.

</details>


### [305] [Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices](https://arxiv.org/abs/2601.02353)
*Shahnawaz Alam,Mohammed Mudassir Uddin,Mohammed Kaif Pasha*

Main category: cs.CV

TL;DR: This paper proposes a method that combines neural network pruning with few-shot learning to make deep learning models for disease detection both compact and efficient, enabling real-time field use on low-cost devices.


<details>
  <summary>Details</summary>
Motivation: Farmers in remote areas need efficient and reliable methods to detect plant diseases, but they lack access to advanced computational resources or extensive labeled datasets.

Method: The authors introduced Disease-Aware Channel Importance Scoring (DACIS) and a Prune-then-Meta-Learn-then-Prune (PMP) pipeline to compress deep learning models while enabling their usage with limited training data.

Result: The method achieved a 78% reduction in model size while maintaining 92.3% accuracy, running efficiently at 7 frames per second on a Raspberry Pi 4.

Conclusion: The proposed approach makes real-time plant disease diagnosis feasible for smallholder farmers by offering an accurate, compact, and computationally affordable solution.

Abstract: Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\% while maintaining 92.3\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.

</details>


### [306] [Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes](https://arxiv.org/abs/2601.02356)
*Jing Tan,Zhaoyang Zhang,Yantao Shen,Jiarui Cai,Shuo Yang,Jiajun Wu,Wei Xia,Zhuowen Tu,Stefano Soatto*

Main category: cs.CV

TL;DR: The paper presents Talk2Move, an RL-based framework for text-instructed spatial transformations of objects, using novel optimization techniques and spatial rewards.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges of spatially manipulating objects in scenes via natural language, which existing multimodal systems struggle with due to limited paired supervision and pixel-level constraints.

Method: The method involves Group Relative Policy Optimization (GRPO) for geometric action exploration, spatial reward-guided learning, off-policy step evaluation, and object-centric spatial rewards to align transformations with linguistic instructions.

Result: Talk2Move surpasses existing methods by delivering precise, consistent, and semantically accurate object transformations with spatial accuracy and scene coherence.

Conclusion: Talk2Move introduces a novel approach for text-guided spatial object manipulation, setting a new benchmark for accuracy, consistency, and interpretability in multimodal tasks.

Abstract: We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.

</details>


### [307] [VINO: A Unified Visual Generator with Interleaved OmniModal Context](https://arxiv.org/abs/2601.02358)
*Junyi Chen,Tong He,Zhoujie Fu,Pengfei Wan,Kun Gai,Weicai Ye*

Main category: cs.CV

TL;DR: VINO is a unified model for both image and video generation and editing, leveraging a shared diffusion process conditioned on text, images, and videos.


<details>
  <summary>Details</summary>
Motivation: Current models for image and video generation rely on separate, task-specific modules, limiting efficiency and scalability. VINO aims to unify these processes under a single, shared framework.

Method: The authors coupled a Vision-Language Model (VLM) with a Multimodal Diffusion Transformer (MMDiT), using interleaved tokens for processing and a progressive multi-stage training pipeline to create a unified, multi-task generative system.

Result: VINO showed strong performance in image and video benchmarks, achieving high visual quality, accurate instruction adherence, improved attribute preservation, and better control over edits.

Conclusion: VINO paves the way for scalable, unified visual generation and editing, demonstrating the potential of interleaved multimodal computation for general-purpose visual tasks.

Abstract: We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.

</details>


### [308] [ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors](https://arxiv.org/abs/2601.02359)
*Kaede Shiohara,Toshihiko Yamasaki,Vladislav Golyanik*

Main category: cs.CV

TL;DR: This paper introduces ExposeAnyone, a self-supervised approach leveraging a diffusion model for detecting unknown deepfakes, outperforming state-of-the-art methods and demonstrating robustness across various datasets.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of detecting unseen deepfake manipulations, which current methods struggle with due to overfitting on specific forgery patterns derived from supervised training.

Method: ExposeAnyone is a fully self-supervised method using a diffusion model to synthesize expression sequences from audio. Personalized reference sets enable identity distance computation via diffusion reconstruction errors to detect face forgeries.

Result: The method surpasses previous state-of-the-art techniques by 4.22 percentage points in average AUC across multiple datasets. It effectively detects deepfakes generated by challenging models like Sora2 and remains robust to distortions such as blur and compression.

Conclusion: ExposeAnyone proves to be a promising solution for real-world deepfake detection, offering strong generalization ability, improved accuracy, and robustness against various corruptions.

Abstract: Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [309] [A Multi-Port Concurrent Communication Model for handling Compute Intensive Tasks on Distributed Satellite System Constellations](https://arxiv.org/abs/2601.01031)
*Bharadwaj Veeravalli*

Main category: cs.DC

TL;DR: This paper introduces the MPCC-DLT framework for efficient load distribution and communication in distributed satellite systems, combining theoretical modeling and practical mechanisms to optimize task scheduling and system design.


<details>
  <summary>Details</summary>
Motivation: To address concurrent data processing and communication challenges in relay-centric distributed satellite systems with varied onboard processing and link conditions.

Method: The authors develop a mathematical framework with closed-form expressions for load allocation and derive feasibility conditions for cooperative satellite clusters. They enhance it with real-time simulation techniques for practical workflow management.

Result: Simulation results show latency reduction for distributable tasks but diminishing returns for communication-heavy tasks, while real-time mechanisms effectively manage stochastic arrivals and deadlines.

Conclusion: The developed MPCC-DLT framework enables efficient analysis and scheduling in distributed satellite systems, offering significant insights into system-level design and optimization for satellite constellations.

Abstract: We develop an integrated Multi-Port Concurrent Communication Divisible Load Theory (MPCC-DLT) framework for relay-centric distributed satellite systems (DSS), capturing concurrent data dissemination, parallel computation, and result return under heterogeneous onboard processing and inter-satellite link conditions. We propose a formulation that yields closed-form expressions for optimal load allocation and completion time that explicitly quantify the joint impact of computation speed, link bandwidth, and result-size overhead. We further derive deadline feasibility conditions that enable explicit sizing of cooperative satellite clusters to meet time-critical task requirements. Extensive simulation results demonstrate that highly distributable tasks achieve substantial latency reduction, while communication-heavy tasks exhibit diminishing returns due to result-transfer overheads. To bridge theory and practice, we extend the MPCC-DLT framework with a real-time admission control mechanism that handles stochastic task arrivals and deadline constraints, enabling blocking-aware operation. Our real-time simulations illustrate how task structure and system parameters jointly govern deadline satisfaction and operating regimes. Overall, this work provides the first analytically tractable MPCC-DLT model for distributed satellite systems and offers actionable insights for application-aware scheduling and system-level design of future satellite constellations.

</details>


### [310] [Performance and Security Aware Distributed Service Placement in Fog Computing](https://arxiv.org/abs/2601.01125)
*Mohammad Goudarzi,Arash Shaghaghi,Zhiyu Wang,Rajkumar Buyya*

Main category: cs.DC

TL;DR: The paper introduces SPA-DDRL, a framework using distributed deep reinforcement learning to improve both service response time and security compliance in Fog computing. It achieves strong performance while addressing dynamic workloads and security challenges.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of heterogeneous resources, dynamic workloads, and diverse security requirements in Fog computing, and to address the gap in existing solutions that neglect security while optimizing performance metrics.

Method: SPA-DDRL employs a Security and Performance-Aware Distributed Deep Reinforcement Learning framework. It uses a multi-objective optimization approach to minimize latency and maximize a security score, adopting a distributed architecture combining Long Short-Term Memory networks, Prioritized Experience Replay, and off-policy correction.

Result: SPA-DDRL demonstrates significant enhancements in service placement performance, improving response time by 16.3% and achieving a 33% faster convergence rate compared to baseline approaches.

Conclusion: The framework effectively enhances both security compliance and service performance for IoT workloads in Fog computing, providing consistent solutions that scale well without performance degradation.

Abstract: The rapid proliferation of IoT applications has intensified the demand for efficient and secure service placement in Fog computing. However, heterogeneous resources, dynamic workloads, and diverse security requirements make optimal service placement highly challenging. Most solutions focus primarily on performance metrics while overlooking the security implications of deployment decisions. This paper proposes a Security and Performance-Aware Distributed Deep Reinforcement Learning (SPA-DDRL) framework for joint optimization of service response time and security compliance in Fog computing. The problem is formulated as a weighted multi-objective optimization task, minimizing latency while maximizing a security score derived from the security capabilities of Fog nodes. The security score features a new three-tier hierarchy, where configuration-level checks verify proper settings, capability-level assessments evaluate the resource security features, and control-level evaluations enforce stringent policies, thereby ensuring compliant solutions that align with performance objectives. SPA-DDRL adopts a distributed broker-learner architecture where multiple brokers perform autonomous service-placement decisions and a centralized learner coordinates global policy optimization through shared prioritized experiences. It integrates three key improvements, including Long Short-Term Memory networks, Prioritized Experience Replay, and off-policy correction mechanisms to improve the agent's performance. Experiments based on real IoT workloads show that SPA-DDRL significantly improves both service response time and placement security compared to current approaches, achieving a 16.3% improvement in response time and a 33% faster convergence rate. It also maintains consistent, feasible, security-compliant solutions across all system scales, while baseline techniques fail or show performance degradation.

</details>


### [311] [OrchestrRL: Dynamic Compute and Network Orchestration for Disaggregated RL](https://arxiv.org/abs/2601.01209)
*Xin Tan,Yicheng Feng,Yu Zhou,Yimin Jiang,Yibo Zhu,Hong Xu*

Main category: cs.DC

TL;DR: The paper presents OrchestrRL, a framework for improving efficiency in post-training reinforcement learning by balancing computation and network dynamics using adaptive compute scheduling and a hybrid optical-electrical network called RFabric.


<details>
  <summary>Details</summary>
Motivation: The paper addresses efficiency challenges in post-training reinforcement learning: computation bottlenecks due to dynamic workloads and network traffic complexities caused by disaggregated stages.

Method: OrchestrRL introduces an adaptive compute scheduler and RFabric, a reconfigurable network fabric combining optical and electrical technologies, to optimize workload execution and handle dynamic network patterns.

Result: OrchestrRL achieved up to 1.40x throughput improvement on a 48-H800 GPU testbed. The RFabric network showed better performance-cost efficiency compared to traditional Fat-Tree networks.

Conclusion: OrchestrRL with RFabric represents a significant advancement in scaling and efficiency for large-scale reinforcement learning workloads by addressing both computation and networking challenges.

Abstract: Post-training with reinforcement learning (RL) has greatly enhanced the capabilities of large language models. Disaggregating the generation and training stages in RL into a parallel, asynchronous pipeline offers the potential for flexible scaling and improved throughput. However, it still faces two critical challenges. First, the generation stage often becomes a bottleneck due to dynamic workload shifts and severe execution imbalances. Second, the decoupled stages result in diverse and dynamic network traffic patterns that overwhelm conventional network fabrics. This paper introduces OrchestrRL, an orchestration framework that dynamically manages compute and network rhythms in disaggregated RL. To improve generation efficiency, OrchestrRL employs an adaptive compute scheduler that dynamically adjusts parallelism to match workload characteristics within and across generation steps. This accelerates execution while continuously rebalancing requests to mitigate stragglers. To address the dynamic network demands inherent in disaggregated RL -- further intensified by parallelism switching -- we co-design RFabric, a reconfigurable hybrid optical-electrical fabric. RFabric leverages optical circuit switches at selected network tiers to reconfigure the topology in real time, enabling workload-aware circuits for (i) layer-wise collective communication during training iterations, (ii) generation under different parallelism configurations, and (iii) periodic inter-cluster weight synchronization. We evaluate OrchestrRL on a physical testbed with 48 H800 GPUs, demonstrating up to a 1.40x throughput improvement. Furthermore, we develop RLSim, a high-fidelity simulator, to evaluate RFabric at scale. Our results show that RFabric achieves superior performance-cost efficiency compared to static Fat-Tree networks, establishing it as a highly effective solution for large-scale RL workloads.

</details>


### [312] [Making MoE based LLM inference resilient with Tarragon](https://arxiv.org/abs/2601.01310)
*Songyu Zhang,Aaron Tam,Myungjin Lee,Shixiong Qi,K. K. Ramakrishnan*

Main category: cs.DC

TL;DR: The paper introduces Tarragon, a framework designed to enhance resilience in large-scale LLM inference models using Mixture-of-Experts (MoE), reducing disruptions caused by worker failures.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address poor failure resilience in existing MoE systems, where even minor failures lead to service-wide restart, resulting in latency issues for LLM services.

Method: Tarragon confines failure impacts by separating attention and expert workers into distinct domains, introduces a reconfigurable datapath for rerouting requests, and employs self-healing mechanisms like KV cache checkpointing and deploying shadow experts.

Result: Tarragon reduces recovery interruptions by up to 213 times compared to state-of-the-art frameworks, offering seamless performance improvements.

Conclusion: Tarragon significantly enhances the robustness of MoE-based LLM inference systems by minimizing disruption and optimizing recovery, making it highly suitable for latency-sensitive applications.

Abstract: Mixture-of-Experts (MoE) models are increasingly used to serve LLMs at scale, but failures become common as deployment scale grows. Existing systems exhibit poor failure resilience: even a single worker failure triggers a coarse-grained, service-wide restart, discarding accumulated progress and halting the entire inference pipeline during recovery--an approach clearly ill-suited for latency-sensitive, LLM services.
  We present Tarragon, a resilient MoE inference framework that confines the failures impact to individual workers while allowing the rest of the pipeline to continue making forward progress. Tarragon exploits the natural separation between the attention and expert computation in MoE-based transformers, treating attention workers (AWs) and expert workers (EWs) as distinct failure domains. Tarragon introduces a reconfigurable datapath to mask failures by rerouting requests to healthy workers. On top of this datapath, Tarragon implements a self-healing mechanism that relaxes the tightly synchronized execution of existing MoE frameworks. For stateful AWs, Tarragon performs asynchronous, incremental KV cache checkpointing with per-request restoration, and for stateless EWs, it leverages residual GPU memory to deploy shadow experts. These together keep recovery cost and recomputation overhead extremely low. Our evaluation shows that, compared to state-of-the-art MegaScale-Infer, Tarragon reduces failure-induced stalls by 160-213x (from ~64 s down to 0.3-0.4 s) while preserving performance when no failures occur.

</details>


### [313] [DiT-HC: Enabling Efficient Training of Visual Generation Model DiT on HPC-oriented CPU Cluster](https://arxiv.org/abs/2601.01500)
*Jinxiao Zhang,Yunpu Xu,Xiyong Wu,Runmin Dong,Shenggan Cheng,Yi Zhao,Mengxuan Chen,Qinrui Zheng,Jianting Liu,Haohuan Fu*

Main category: cs.DC

TL;DR: The paper introduces DiT-HC, a system for training and scaling generative models on next-generation HPC CPU clusters.


<details>
  <summary>Details</summary>
Motivation: To leverage the potential of new hardware features and unify artificial intelligence with scientific computing using CPU-based clusters.

Method: It proposes techniques like communication-free tensor parallelism with AutoMem, HCOps for optimized kernels, and a custom MPI backend.

Result: Demonstrates 8.2 to 87.7 times speedups compared to existing libraries and achieves 90.6% weak scaling efficiency on 256 nodes.

Conclusion: DiT-HC highlights the feasibility of training large-scale generative models on CPU clusters and offers insights for future HPC-AI designs.

Abstract: Generative foundation models have become an important tool for data reconstruction and simulation in scientific computing, showing a tight integration with traditional numerical simulations. At the same time, with the development of new hardware features, such as matrix acceleration units and high-bandwidth memory, CPU-based clusters offer promising opportunities to accelerate and scale such models, facilitating the unification of artificial intelligence and scientific computing. We present DiT-HC, the first system to train and scale the generative model DiT on a next-generation HPC CPU cluster. DiT-HC introduces three key techniques: (1) communication-free tensor parallelism (CFTP) with AutoMem for automated memory-aware dataflow, (2) HCOps, a suite of optimized GEMM and operator kernels leveraging vector and matrix acceleration units, and (3) a custom MPI backend that overlaps computation, communication, and memory movement. Experiments show 8.2 to 87.7 times speedups over native or public CPU libraries and 90.6% weak scaling efficiency on 256 nodes. These results demonstrate the feasibility of large-scale generative model training on CPU clusters and provide new insights for future HPC-AI co-design.

</details>


### [314] [FFCz: Fast Fourier Correction for Spectrum-Preserving Lossy Compression of Scientific Data](https://arxiv.org/abs/2601.01596)
*Congrong Ren,Robert Underwood,Sheng Di,Emrecan Kutay,Zarija Lukic,Aylin Yener,Franck Cappello,Hanqi Guo*

Main category: cs.DC

TL;DR: The paper proposes a fast Fourier correction algorithm to maintain spectral features in lossy compression, ensuring data accuracy in both spatial and frequency domains.


<details>
  <summary>Details</summary>
Motivation: To address data compression needs in scientific fields like cosmology, combustion simulations, and X-ray diffraction where both spatial and frequency domain integrity is essential yet overlooked by traditional methods.

Method: The algorithm corrects errors from existing base compressors (e.g., SZ3, ZFP) by iteratively projecting spatial errors onto regions defined by bounds in both spatial and frequency domains. GPU parallelism accelerates the process. Validation is done using datasets from relevant scientific domains.

Result: The algorithm effectively maintains critical information in both spatial and frequency views while achieving efficient performance using GPU acceleration.

Conclusion: This method enables improved lossy compression while preserving comprehensive data integrity, making it useful for scientific applications with dual-domain data needs.

Abstract: This paper introduces a novel technique to preserve spectral features in lossy compression based on a novel fast Fourier correction algorithm\added{ for regular-grid data}. Preserving both spatial and frequency representations of data is crucial for applications such as cosmology, turbulent combustion, and X-ray diffraction, where spatial and frequency views provide complementary scientific insights. In particular, many analysis tasks rely on frequency-domain representations to capture key features, including the power spectrum of cosmology simulations, the turbulent energy spectrum in combustion, and diffraction patterns in reciprocal space for ptychography. However, existing compression methods guarantee accuracy only in the spatial domain while disregarding the frequency domain. To address this limitation, we propose an algorithm that corrects the errors produced by off-the-shelf ``base'' compressors such as SZ3, ZFP, and SPERR, thereby preserving both spatial and frequency representations by bounding errors in both domains. By expressing frequency-domain errors as linear combinations of spatial-domain errors, we derive a region that jointly bounds errors in both domains. Given as input the spatial errors from a base compressor and user-defined error bounds in the spatial and frequency domains, we iteratively project the spatial error vector onto the regions defined by the spatial and frequency constraints until it lies within their intersection. We further accelerate the algorithm using GPU parallelism to achieve practical performance. We validate our approach with datasets from cosmology simulations, X-ray diffraction, combustion simulation, and electroencephalography demonstrating its effectiveness in preserving critical scientific information in both spatial and frequency domains.

</details>


### [315] [RelayGR: Scaling Long-Sequence Generative Recommendation via Cross-Stage Relay-Race Inference](https://arxiv.org/abs/2601.01712)
*Jiarui Wang,Huichao Chai,Yuanhang Zhang,Zongjin Zhou,Wei Guo,Xingkun Yang,Qiang Tang,Bo Pan,Jiawei Zhu,Ke Cheng,Yuting Yan,Shulan Wang,Yingjie Zhu,Zhengfan Yuan,Jiaqi Huang,Yuhan Zhang,Xiaosong Sun,Zhinan Zhang,Hong Zhu,Yongsheng Zhang,Tiantian Dong,Zhong Xiao,Deliang Liu,Chengzhou Lu,Yuan Sun,Zhiyuan Chen,Xinming Han,Zaizhu Liu,Yaoyuan Wang,Ziyang Zhang,Yong Liu,Jinxin Xu,Yajing Sun,Zhoujun Yu,Wenting Zhou,Qidong Zhang,Zhengyong Zhang,Zhonghai Gu,Yibo Jin,Yongxiang Feng,Pengfei Zuo*

Main category: cs.DC

TL;DR: RelayGR improves real-time generative recommendation systems by pre-inferencing long user behavior prefixes, optimizing memory use, and enhancing pipeline efficiency under strict latency constraints.


<details>
  <summary>Details</summary>
Motivation: Generative recommendation models promise better quality by consuming long user-behavior sequences but face production limitations due to latency constraints. The paper seeks to optimize processing pipelines to accommodate longer sequences efficiently.

Method: RelayGR strategically performs pre-inference of user prefixes, employs a sequence-aware trigger for cache management, uses affinity-aware routing to optimize consumption processes, and integrates server-local DRAM for memory efficiency.

Result: RelayGR allows real-time systems to handle up to 1.5 times longer user sequences and increases throughput by up to 3.6 times under strict latency constraints.

Conclusion: RelayGR makes generative recommendation systems more scalable and efficient in production environments, accommodating longer sequences without compromising performance or latency constraints.

Abstract: Real-time recommender systems execute multi-stage cascades (retrieval, pre-processing, fine-grained ranking) under strict tail-latency SLOs, leaving only tens of milliseconds for ranking. Generative recommendation (GR) models can improve quality by consuming long user-behavior sequences, but in production their online sequence length is tightly capped by the ranking-stage P99 budget. We observe that the majority of GR tokens encode user behaviors that are independent of the item candidates, suggesting an opportunity to pre-infer a user-behavior prefix once and reuse it during ranking rather than recomputing it on the critical path. Realizing this idea at industrial scale is non-trivial: the prefix cache must survive across multiple pipeline stages before the final ranking instance is determined, the user population implies cache footprints far beyond a single device, and indiscriminate pre-inference would overload shared resources under high QPS. We present RelayGR, a production system that enables in-HBM relay-race inference for GR. RelayGR selectively pre-infers long-term user prefixes, keeps their KV caches resident in HBM over the request lifecycle, and ensures the subsequent ranking can consume them without remote fetches. RelayGR combines three techniques: 1) a sequence-aware trigger that admits only at-risk requests under a bounded cache footprint and pre-inference load, 2) an affinity-aware router that co-locates cache production and consumption by routing both the auxiliary pre-infer signal and the ranking request to the same instance, and 3) a memory-aware expander that uses server-local DRAM to capture short-term cross-request reuse while avoiding redundant reloads. We implement RelayGR on Huawei Ascend NPUs and evaluate it with real queries. Under a fixed P99 SLO, RelayGR supports up to 1.5$\times$ longer sequences and improves SLO-compliant throughput by up to 3.6$\times$.

</details>


### [316] [pMSz: A Distributed Parallel Algorithm for Correcting Extrema and Morse Smale Segmentations in Lossy Compression](https://arxiv.org/abs/2601.01787)
*Yuxiao Li,Mingze Xia,Xin Liang,Bei Wang,Robert Underwood,Sheng Di,Hemant Sharma,Dishant Beniwal,Franck Cappello,Hanqi Guo*

Main category: cs.DC

TL;DR: The paper develops a scalable distributed algorithm to correct topological features, crucial in combating distortion effects from lossy compression in applications involving extreme scale data.


<details>
  <summary>Details</summary>
Motivation: Lossy compression can distort scientific data features, potentially leading to incorrect conclusions in critical analyses. Current correction methods, like MSz, are limited to single GPUs and don’t scale for large datasets.

Method: The algorithm simplifies topological correction by preserving ascent and descent directions and minimizes interprocess communication, while relaxing synchronization for enhanced scalability.

Result: It achieves over 90% parallel efficiency on 128 GPUs in the Perlmutter supercomputer when applied to real-world datasets.

Conclusion: This scalable method resolves bottlenecks in parallel computation for PLMSS correction, enabling efficient processing of extreme scale scientific data.

Abstract: Lossy compression, widely used by scientists to reduce data from simulations, experiments, and observations, can distort features of interest even under bounded error. Such distortions may compromise downstream analyses and lead to incorrect scientific conclusions in applications such as combustion and cosmology. This paper presents a distributed and parallel algorithm for correcting topological features, specifically, piecewise linear Morse Smale segmentations (PLMSS), which decompose the domain into monotone regions labeled by their corresponding local minima and maxima. While a single GPU algorithm (MSz) exists for PLMSS correction after compression, no methodology has been developed that scales beyond a single GPU for extreme scale data. We identify the key bottleneck in scaling PLMSS correction as the parallel computation of integral paths, a communication-intensive computation that is notoriously difficult to scale. Instead of explicitly computing and correcting integral paths, our algorithm simplifies MSz by preserving steepest ascending and descending directions across all locations, thereby minimizing interprocess communication while introducing negligible additional storage overhead. With this simplified algorithm and relaxed synchronization, our method achieves over 90% parallel efficiency on 128 GPUs on the Perlmutter supercomputer for real world datasets.

</details>


### [317] [Bringing computation to the data: A MOEA-driven approach for optimising data processing in the context of the SKA and SRCNet](https://arxiv.org/abs/2601.01980)
*Manuel Parra-Royón,Álvaro Rodríguez-Gallardo,Susana Sánchez-Expósito,Laura Darriba-Pol,Jesús Sánchez-Castañeda,M. Ángeles Mendoza,Julián Garrido,Javier Moldón,Lourdes Verdes-Montenegro*

Main category: cs.DC

TL;DR: The paper introduces a distributed and in-situ computing approach for the Square Kilometre Array’s data challenge using Function-as-a-Service and Evolutionary Algorithms.


<details>
  <summary>Details</summary>
Motivation: Efficient data processing for the SKA is needed due to the impracticality of traditional centralized data computation in near-exascale environments.

Method: Integration of Function-as-a-Service with Multi-Objective Evolutionary Algorithms to optimize workflows based on execution time, energy consumption, data location, and transfer costs.

Result: A baseline framework is established for cost-aware computation and efficient data workflows within the SKA Regional Centres Network.

Conclusion: Distributed computing strategies and intelligent decision-making can address near-exascale data processing challenges, facilitating better computational efficiency within SRCNet.

Abstract: The Square Kilometre Array (SKA) will generate unprecedented data volumes, making efficient data processing a critical challenge. Within this context, the SKA Regional Centres Network (SRCNet) must operate in a near-exascale environment where traditional data-centric computing models based on moving large datasets to centralised resources are no longer viable due to network and storage bottlenecks.
  To address this limitation, this work proposes a shift towards distributed and in-situ computing, where computation is moved closer to the data. We explore the integration of Function-as-a-Service (FaaS) with an intelligent decision-making entity based on Evolutionary Algorithms (EAs) to optimise data-intensive workflows within SRCNet. FaaS enables lightweight and modular function execution near data sources while abstracting infrastructure management.
  The proposed decision-making entity employs Multi-Objective Evolutionary Algorithms (MOEAs) to explore near-optimal execution plans considering execution time and energy consumption, together with constraints related to data location and transfer costs. This work establishes a baseline framework for efficient and cost-aware computation-to-data strategies within the SRCNet architecture.

</details>


### [318] [SuperSFL: Resource-Heterogeneous Federated Split Learning with Weight-Sharing Super-Networks](https://arxiv.org/abs/2601.02092)
*Abdullah Al Asif,Sixing Yu,Juan Pablo Munoz,Arya Mazaheri,Ali Jannesari*

Main category: cs.DC

TL;DR: SuperSFL is a federated split learning framework designed to handle edge device heterogeneity using a weight-sharing super-network and improved optimization techniques, achieving faster convergence and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current SplitFed Learning (SFL) methods struggle with device heterogeneity in computational and communication capabilities in distributed edge environments.

Method: SuperSFL incorporates a dynamic weight-sharing super-network to generate client-specific subnetworks, Three-Phase Gradient Fusion (TPGF) for synchronized training and faster convergence, and fault-tolerant mechanisms for seamless operation under communication failures.

Result: Experiments on CIFAR datasets with up to 100 heterogeneous clients demonstrated 2–5x faster convergence, 20x reduced communication costs, 13x shorter training time, and better energy efficiency compared to baseline SFL models.

Conclusion: SuperSFL is a scalable and practical solution for addressing heterogeneity in federated learning across distributed edge devices, outperforming traditional methods in key metrics.

Abstract: SplitFed Learning (SFL) combines federated learning and split learning to enable collaborative training across distributed edge devices; however, it faces significant challenges in heterogeneous environments with diverse computational and communication capabilities. This paper proposes \textit{SuperSFL}, a federated split learning framework that leverages a weight-sharing super-network to dynamically generate resource-aware client-specific subnetworks, effectively mitigating device heterogeneity. SuperSFL introduces Three-Phase Gradient Fusion (TPGF), an optimization mechanism that coordinates local updates, server-side computation, and gradient fusion to accelerate convergence. In addition, a fault-tolerant client-side classifier and collaborative client--server aggregation enable uninterrupted training under intermittent communication failures. Experimental results on CIFAR-10 and CIFAR-100 with up to 100 heterogeneous clients show that SuperSFL converges $2$--$5\times$ faster in terms of communication rounds than baseline SFL while achieving higher accuracy, resulting in up to $20\times$ lower total communication cost and $13\times$ shorter training time. SuperSFL also demonstrates improved energy efficiency compared to baseline methods, making it a practical solution for federated learning in heterogeneous edge environments.

</details>


### [319] [BigSUMO: A Scalable Framework for Big Data Traffic Analytics and Parallel Simulation](https://arxiv.org/abs/2601.02286)
*Rahul Sengupta,Nooshin Yousefzadeh,Manav Sanghvi,Yash Ranjan,Anand Rangarajan,Sanjay Ranka,Yashaswi Karnati,Jeremy Dilmore,Tushar Patel,Ryan Casburn*

Main category: cs.DC

TL;DR: BigSUMO is a scalable, open-source platform designed for traffic data analysis, interruption detection, and simulation leveraging real-world data and the SUMO simulator.


<details>
  <summary>Details</summary>
Motivation: To address the critical need for tools that analyze vast traffic data and allow for optimizing traffic management in increasingly urbanized areas.

Method: BigSUMO processes loop detector and probe data, performs analytics to detect issues, and utilizes the SUMO microsimulator for testing hypothetical scenarios, all within a modular, open-source framework.

Result: BigSUMO delivers a cost-effective and scalable pipeline for traffic optimization and simulation analyses, capable of integrating various algorithms.

Conclusion: BigSUMO aims to support the development of smart city traffic solutions by providing an adaptable, efficient, and effective tool for data-driven decision-making in urban transportation.

Abstract: With growing urbanization worldwide, efficient management of traffic infrastructure is critical for transportation agencies and city planners. It is essential to have tools that help analyze large volumes of stored traffic data and make effective interventions. To address this need, we present ``BigSUMO", an end-to-end, scalable, open-source framework for analytics, interruption detection, and parallel traffic simulation. Our system ingests high-resolution loop detector and signal state data, along with sparse probe trajectory data. It first performs descriptive analytics and detects potential interruptions. It then uses the SUMO microsimulator for prescriptive analytics, testing hundreds of what-if scenarios to optimize traffic performance. The modular design allows integration of different algorithms for data processing and outlier detection. Built using open-source software and libraries, the pipeline is cost-effective, scalable, and easy to deploy. We hope BigSUMO will be a valuable aid in developing smart city mobility solutions.

</details>


### [320] [Placement Semantics for Distributed Deep Learning: A Systematic Framework for Analyzing Parallelism Strategies](https://arxiv.org/abs/2601.02311)
*Deep Pankajbhai Mehta*

Main category: cs.DC

TL;DR: The paper introduces a unified framework called placement semantics to optimize parallelism strategies for training large language models.


<details>
  <summary>Details</summary>
Motivation: There is no systematic framework to predict the behavior of parallelism strategies used in training large language models, leading to trial-and-error practices.

Method: The authors propose placement semantics, which specifies the placement of training states using five modes, predicting memory and communication performance accurately without delving into implementation details.

Result: The framework matches existing results perfectly—for example, verifying ZeRO-3 achieves 8x memory savings with 1.5x communication cost—and unifies multiple parallelism strategies including ZeRO, FSDP, and pipeline parallelism.

Conclusion: Placement semantics provides a comprehensive and effective way to predict and combine parallelism strategies in distributed training, ensuring both consistency and optimal resource utilization.

Abstract: Training large language models requires distributing computation across many accelerators, yet practitioners select parallelism strategies (data, tensor, pipeline, ZeRO) through trial and error because no unified systematic framework predicts their behavior. We introduce placement semantics: each strategy is specified by how it places four training states (parameters, optimizer, gradients, activations) across devices using five modes (replicated, sharded, sharded-with-gather, materialized, offloaded). From placement alone, without implementation details, we derive memory consumption and communication volume. Our predictions match published results exactly: ZeRO-3 uses 8x less memory than data parallelism at 1.5x communication cost, as reported in the original paper. We prove two conditions (gradient integrity, state consistency) are necessary and sufficient for distributed training to match single-device results, and provide composition rules for combining strategies safely. The framework unifies ZeRO Stages 1-3, Fully Sharded Data Parallel (FSDP), tensor parallelism, and pipeline parallelism as instances with different placement choices.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [321] [Horizon Reduction as Information Loss in Offline Reinforcement Learning](https://arxiv.org/abs/2601.00831)
*Uday Kumar Nidadala,Venkata Bhumika Guthi*

Main category: cs.LG

TL;DR: This paper examines theoretical implications of horizon reduction in offline RL and identifies structural failures leading to information loss.


<details>
  <summary>Details</summary>
Motivation: To explore the theoretical underpinnings and limitations of horizon reduction techniques in offline reinforcement learning (RL), which remain understudied despite empirical successes.

Method: The authors formalized horizon reduction as fixed-length trajectory learning and constructed minimal counterexample MDPs to identify failure modes.

Result: The study identified three structural failure modes: prefix indistinguishability, truncated return-induced misspecifications, and offline dataset aliasing.

Conclusion: Horizon reductions in offline RL can lead to irrecoverable information loss regardless of algorithmic advancements; safe application requires adherence to necessary conditions.

Abstract: Horizon reduction is a common design strategy in offline reinforcement learning (RL), used to mitigate long-horizon credit assignment, improve stability, and enable scalable learning through truncated rollouts, windowed training, or hierarchical decomposition (Levine et al., 2020; Prudencio et al., 2023; Park et al., 2025). Despite recent empirical evidence that horizon reduction can improve scaling on challenging offline RL benchmarks, its theoretical implications remain underdeveloped (Park et al., 2025). In this paper, we show that horizon reduction can induce fundamental and irrecoverable information loss in offline RL. We formalize horizon reduction as learning from fixed-length trajectory segments and prove that, under this paradigm and any learning interface restricted to fixed-length trajectory segments, optimal policies may be statistically indistinguishable from suboptimal ones even with infinite data and perfect function approximation. Through a set of minimal counterexample Markov decision processes (MDPs), we identify three distinct structural failure modes: (i) prefix indistinguishability leading to identifiability failure, (ii) objective misspecification induced by truncated returns, and (iii) offline dataset support and representation aliasing. Our results establish necessary conditions under which horizon reduction can be safe and highlight intrinsic limitations that cannot be overcome by algorithmic improvements alone, complementing algorithmic work on conservative objectives and distribution shift that addresses a different axis of offline RL difficulty (Fujimoto et al., 2019; Kumar et al., 2020; Gulcehre et al., 2020).

</details>


### [322] [ShrimpXNet: A Transfer Learning Framework for Shrimp Disease Classification with Augmented Regularization, Adversarial Training, and Explainable AI](https://arxiv.org/abs/2601.00832)
*Israk Hasan Jone,D. M. Rafiun Bin Masud,Promit Sarker,Sayed Fuad Al Labib,Nazmul Islam,Farhad Billah*

Main category: cs.LG

TL;DR: The paper presents a deep learning-based system for shrimp disease classification, achieving 96.88% accuracy using ConvNeXt-Tiny.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to overcome the challenge posed by disease outbreaks in shrimp farming, which threaten sustainable shrimp production.

Method: The research employs deep learning, testing six pretrained models with background removal, preprocessing, adversarial training (via FGSM), and advanced data augmentation (CutMix, MixUp). Post-hoc explanation methods provided visual interpretability.

Result: The ConvNeXt-Tiny model achieved the highest accuracy (96.88%), with a 99% confidence interval of [0.953, 0.971] after 1000 iterations.

Conclusion: A robust automated shrimp disease classification system has been developed, showcasing the potential of deep learning to mitigate aquaculture disease challenges.

Abstract: Shrimp is one of the most widely consumed aquatic species globally, valued for both its nutritional content and economic importance. Shrimp farming represents a significant source of income in many regions; however, like other forms of aquaculture, it is severely impacted by disease outbreaks. These diseases pose a major challenge to sustainable shrimp production. To address this issue, automated disease classification methods can offer timely and accurate detection. This research proposes a deep learning-based approach for the automated classification of shrimp diseases. A dataset comprising 1,149 images across four disease classes was utilized. Six pretrained deep learning models, ResNet50, EfficientNet, DenseNet201, MobileNet, ConvNeXt-Tiny, and Xception were deployed and evaluated for performance. The images background was removed, followed by standardized preprocessing through the Keras image pipeline. Fast Gradient Sign Method (FGSM) was used for enhancing the model robustness through adversarial training. While advanced augmentation strategies, including CutMix and MixUp, were implemented to mitigate overfitting and improve generalization. To support interpretability, and to visualize regions of model attention, post-hoc explanation methods such as Grad-CAM, Grad-CAM++, and XGrad-CAM were applied. Exploratory results demonstrated that ConvNeXt-Tiny achieved the highest performance, attaining a 96.88% accuracy on the test dataset. After 1000 iterations, the 99% confidence interval for the model is [0.953,0.971].

</details>


### [323] [Intrinsic-Metric Physics-Informed Neural Networks (IM-PINN) for Reaction-Diffusion Dynamics on Complex Riemannian Manifolds](https://arxiv.org/abs/2601.00834)
*Julian Evan Chrisnanto,Salsabila Rahma Alia,Nurfauzi Fadillah,Yulison Herry Chrisnanto*

Main category: cs.LG

TL;DR: The paper introduces a deep learning framework, IM-PINN, for solving nonlinear reaction-diffusion equations on complex geometric manifolds more efficiently, overcoming limitations of traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address computational challenges in simulating nonlinear reaction-diffusion dynamics on non-Euclidean surfaces, especially issues like mesh generation costs and numerical drift.

Method: The IM-PINN framework incorporates the Riemannian metric tensor into neural networks for mesh-free PDE resolution, leveraging analytical calculations and Fourier embeddings to improve accuracy and computational efficiency.

Result: IM-PINN demonstrated superior accuracy and physical consistency over the Surface Finite Element Method, successfully resolving complex biological patterns with reduced mass conservation errors.

Conclusion: IM-PINN successfully bridges differential geometry and machine learning, providing a scalable and thermodynamically consistent solution for biological pattern formation on evolving manifolds.

Abstract: Simulating nonlinear reaction-diffusion dynamics on complex, non-Euclidean manifolds remains a fundamental challenge in computational morphogenesis, constrained by high-fidelity mesh generation costs and symplectic drift in discrete time-stepping schemes. This study introduces the Intrinsic-Metric Physics-Informed Neural Network (IM-PINN), a mesh-free geometric deep learning framework that solves partial differential equations directly in the continuous parametric domain. By embedding the Riemannian metric tensor into the automatic differentiation graph, our architecture analytically reconstructs the Laplace-Beltrami operator, decoupling solution complexity from geometric discretization. We validate the framework on a "Stochastic Cloth" manifold with extreme Gaussian curvature fluctuations ($K \in [-2489, 3580]$), where traditional adaptive refinement fails to resolve anisotropic Turing instabilities. Using a dual-stream architecture with Fourier feature embeddings to mitigate spectral bias, the IM-PINN recovers the "splitting spot" and "labyrinthine" regimes of the Gray-Scott model. Benchmarking against the Surface Finite Element Method (SFEM) reveals superior physical rigor: the IM-PINN achieves global mass conservation error of $\mathcal{E}_{mass} \approx 0.157$ versus SFEM's $0.258$, acting as a thermodynamically consistent global solver that eliminates mass drift inherent in semi-implicit integration. The framework offers a memory-efficient, resolution-independent paradigm for simulating biological pattern formation on evolving surfaces, bridging differential geometry and physics-informed machine learning.

</details>


### [324] [SLO-Conditioned Action Routing for Retrieval-Augmented Generation: Objective Ablation and Failure Modes](https://arxiv.org/abs/2601.00841)
*Bharath Nunepalli*

Main category: cs.LG

TL;DR: The paper introduces a method to control retrieval-augmented generation (RAG) processes to meet certain service-level objectives (SLOs) such as cost and hallucination risks, through discrete actions based on query conditions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address control challenges in RAG pipelines by optimizing retrieval depth and generation behavior per query to achieve cost-efficiency, reduce hallucination risks, and meet other service-level objectives.

Method: The authors model RAG control as discrete actions, train policies using a logged dataset from SQuAD 2.0, and evaluate learning objectives through supervised and reward-weighted classification methods.

Result: Fixed baselines show competitive performance, while learned policies enhance cost savings under quality-focused SLOs. However, they can also lead to refusal collapse under cost-focused SLOs with high refusal rewards.

Conclusion: The study provides insights into SLO-aware control for RAG, highlighting risks and reporting practices, but does not propose new models or retrievers.

Abstract: Retrieval-augmented generation (RAG) introduces a practical control problem: retrieval depth and generation behavior must be chosen per query to satisfy service-level objectives (SLOs) such as cost, refusal rate, and hallucination risk. This work models per-query control as a small discrete action: choose a retrieval depth and a generation mode (guarded vs. auto), or refuse. An offline logged dataset is constructed from SQuAD 2.0 by executing each action and recording accuracy, token cost, hallucination/refusal indicators, and an SLO-weighted reward. Two simple policy-learning objectives are evaluated: supervised classification of the per-state best action (Argmax-CE) and a reward-weighted variant (Argmax-CE-WT). Across the evaluated settings, a strong fixed baseline (low k, guarded prompting) performs competitively; learned policies mainly provide additional cost savings under a quality-focused SLO and can exhibit refusal collapse under a cheap SLO when refusal is heavily rewarded. The contribution is a reproducible case study of SLO-aware control for RAG pipelines, emphasizing failure modes and reporting conventions rather than proposing a new retriever or language model.

</details>


### [325] [Warp-Cortex: An Asynchronous, Memory-Efficient Architecture for Million-Agent Cognitive Scaling on Consumer Hardware](https://arxiv.org/abs/2601.01298)
*Jorge L. Ruiz Williams*

Main category: cs.LG

TL;DR: The paper presents Warp Cortex, enabling scaled multi-agent LLM reasoning on standard hardware by reducing memory complexity using topological techniques.


<details>
  <summary>Details</summary>
Motivation: Current LLM multi-agent frameworks face scalability issues due to linear memory growth, making parallel reasoning challenging on consumer hardware.

Method: An asynchronous architecture with Singleton Weight Sharing and Topological Synapse methods, reducing memory usage and employing latent-space sparsification for efficient multi-agent reasoning.

Result: Warp Cortex achieves 100 agents on a single NVIDIA RTX 4090 using only 2.2 GB of VRAM, with theoretical scaling up to 1,000 agents before compute limitations.

Conclusion: This novel architecture allows effective scaling for multi-agent LLMs, overcoming memory constraints and enabling efficient parallel problem-solving.

Abstract: Current multi-agent Large Language Model (LLM) frameworks suffer from linear memory scaling, rendering "System 2" parallel reasoning impractical on consumer hardware. We present Warp Cortex, an asynchronous architecture that theoretically enables million-agent cognitive scaling by decoupling agent logic from physical memory. Through Singleton Weight Sharing and a novel Topological Synapse--inspired by hybrid landmarking techniques from Topological Data Analysis (TDA)--we reduce memory complexity from O(N * L) to O(1) for weights and O(N * k) for context, where k << L. By treating the KV-cache as a point cloud in latent space, we apply witness-complex-inspired sparsification to preserve persistent homological features of the context manifold. On a single NVIDIA RTX 4090, we empirically demonstrate 100 concurrent agents at 2.2 GB total VRAM, with theoretical capacity exceeding 1,000 agents before compute latency becomes the bottleneck. We further introduce Referential Injection, a non-intrusive KV-cache update mechanism that allows asynchronous sub-agents to influence primary generation without stream disruption.

</details>


### [326] [Value-guided action planning with JEPA world models](https://arxiv.org/abs/2601.00844)
*Matthieu Destrade,Oumayma Bounou,Quentin Le Lidec,Jean Ponce,Yann LeCun*

Main category: cs.LG

TL;DR: The paper improves Joint-Embedded Predictive Architectures (JEPA) for better planning by reshaping their representation space to approximate distance-based goal-conditioned value functions, showing improved results in control tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the capability of JEPA models to perform effective action planning, which remains a limitation in current implementations despite their strong dynamics-capturing abilities.

Method: The authors reshaped the representation space of JEPA world models by approximating the negative goal-conditioned value function with a distance-like metric. They enforced this constraint through a practical training method.

Result: The results demonstrated significantly improved planning performance of JEPA models on simple control tasks, validating the approach.

Conclusion: Reshaping the representation space allows JEPA models to better support planning, offering an effective solution to limitations in their application to control tasks.

Abstract: Building deep learning models that can reason about their environment requires capturing its underlying dynamics. Joint-Embedded Predictive Architectures (JEPA) provide a promising framework to model such dynamics by learning representations and predictors through a self-supervised prediction objective. However, their ability to support effective action planning remains limited. We propose an approach to enhance planning with JEPA world models by shaping their representation space so that the negative goal-conditioned value function for a reaching cost in a given environment is approximated by a distance (or quasi-distance) between state embeddings. We introduce a practical method to enforce this constraint during training and show that it leads to significantly improved planning performance compared to standard JEPA models on simple control tasks.

</details>


### [327] [Neuro-Channel Networks: A Multiplication-Free Architecture by Biological Signal Transmission](https://arxiv.org/abs/2601.02253)
*Emrah Mete,Emin Erkan Korkmaz*

Main category: cs.LG

TL;DR: The paper introduces Neuro-Channel Networks (NCN), a hardware-efficient, multiplication-free architecture for AI that mimics biological signal transmission.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and high costs of AI's reliance on GPU hardware, and explore a more efficient approach inspired by biological nervous systems.

Method: Proposes a novel architecture, NCN, replacing weights with Channel Widths and using a Neurotransmitter-like parameter to regulate signals without multiplications. Operations are reduced to addition, subtraction, and bitwise operations.

Result: NCNs achieve 100% accuracy on problems like XOR and Majority functions using standard backpropagation, demonstrating their ability to solve complex problems without multiplicative weights.

Conclusion: NCNs present an efficient alternative for AI hardware, enabling complex AI models to run on low-cost CPUs and ultra-low-power chips, removing dependency on GPUs.

Abstract: The rapid proliferation of Deep Learning is increasingly constrained by its heavy reliance on high-performance hardware, particularly Graphics Processing Units (GPUs). These specialized accelerators are not only prohibitively expensive and energy-intensive but also suffer from significant supply scarcity, limiting the ubiquity of Artificial Intelligence (AI) deployment on edge devices. The core of this inefficiency stems from the standard artificial perceptron's dependence on intensive matrix multiplications. However, biological nervous systems achieve unparalleled efficiency without such arithmetic intensity; synaptic signal transmission is regulated by physical ion channel limits and chemical neurotransmitter levels rather than a process that can be analogous to arithmetic multiplication. Inspired by this biological mechanism, we propose Neuro-Channel Networks (NCN), a novel multiplication-free architecture designed to decouple AI from expensive hardware dependencies. In our model, weights are replaced with Channel Widths that physically limit the signal magnitude, while a secondary parameter acts as a Neurotransmitter to regulate Signal Transmission based on sign logic. The forward pass relies exclusively on addition, subtraction, and bitwise operations (minimum, sign), eliminating floating-point multiplication entirely. In this proof-of-concept study, we demonstrate that NCNs can solve non-linearly separable problems like XOR and the Majority function with 100% accuracy using standard backpropagation, proving their capability to form complex decision boundaries without multiplicative weights. This architecture offers a highly efficient alternative for next-generation neuromorphic hardware, paving the way for running complex models on commodity CPUs or ultra-low-power chips without relying on costly GPU clusters.

</details>


### [328] [You Only Need Your Transformer 25% of the Time: Meaning-First Execution for Eliminating Unnecessary Inference](https://arxiv.org/abs/2601.00847)
*Ryan Shamim*

Main category: cs.LG

TL;DR: This paper introduces Meaning-First Execution (MFEE), a framework that reduces the necessity of transformer execution in AI inference without compromising correctness, achieving a significant execution reduction.


<details>
  <summary>Details</summary>
Motivation: Current AI inference systems over-rely on transformer execution, treating it as mandatory and not differentiating when it is optional. The need is to create a system that optimizes executions while maintaining correctness.

Method: MFEE is a control-plane architecture that selectively invokes transformer inference based on semantic analysis. It operates without altering the existing models, weights, or parameters and acts as a gating layer above existing infrastructures.

Result: MFEE reduces execution by 78.1% under deterministic decoding, maintaining 100% exact-match equivalence. Unlike pattern-based routers with correctness failures, MFEE achieves 100% correctness through its semantic-based approach.

Conclusion: The paper establishes the concept of execution governance as an essential aspect of ML systems infrastructure, showcasing how functionality can be optimized without changes at the model level.

Abstract: Modern AI inference systems treat transformer execution as mandatory, conflating model capability with execution necessity. We reframe inference as a control-plane decision problem: determining when execution is necessary versus when correctness can be preserved through alternative pathways. We introduce Meaning-First Execution (MFEE), a control-plane architecture implementing this framework, selectively invoking transformer inference only when required. MFEE operates as a gating layer above existing stacks without modifying models, weights, or parameters. Across 1,000 diverse prompts under deterministic decoding, MFEE achieves 78.1% execution reduction while maintaining 100% exact-match equivalence for invoked executions. Comparative evaluation reveals pattern-based routers achieve at most 53.3% avoidance with correctness failures, while MFEE reaches 100% avoidance with zero failures through semantic analysis. We prove this limitation via Theorem 1: any router operating solely on finite feature maps cannot simultaneously guarantee zero false skips and positive avoidance on feature-collision pairs. These results establish execution governance as a foundational layer in ML systems infrastructure, orthogonal to model-level optimization techniques.

</details>


### [329] [EdgeJury: Cross-Reviewed Small-Model Ensembles for Truthful Question Answering on Serverless Edge Inference](https://arxiv.org/abs/2601.00850)
*Aayush Kumar*

Main category: cs.LG

TL;DR: The paper introduces EdgeJury, a lightweight ensemble framework using small language models (3B-8B) to improve answer truthfulness and robustness in resource-constrained environments without relying on large models or retrieval pipelines.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in question answering affect reliability, particularly in setups where large models or retrieval pipelines are infeasible, prompting the need for a lightweight and scalable solution.

Method: The EdgeJury framework works in four stages: 1) role-specialized generation, 2) anonymized cross-review with structured critiques, 3) synthesis of the strongest content while addressing flagged issues, and 4) consistency labeling based on inter-model agreement.

Result: EdgeJury demonstrated significant improvements over baselines on various benchmarks: achieving 76.2% accuracy (+21.4% relative improvement) on TruthfulQA, 48.2% relative gains on an adversarial question set, and a 55% reduction in hallucination errors with efficient performance on serverless edge deployments.

Conclusion: Coordinated small-model ensembles, as demonstrated by EdgeJury, effectively enhance truthfulness in challenging QA tasks without relying on large-scale models or external proprietary systems, proving to be both scalable and cost-efficient.

Abstract: Hallucinations hinder reliable question answering, especially in resource-constrained deployments where frontier-scale models or retrieval pipelines may be impractical. We present EdgeJury, a lightweight ensemble framework that improves truthfulness and robustness using only small instruction-tuned language models (3B-8B) suitable for serverless edge inference. EdgeJury orchestrates four stages: (1) parallel role-specialized generation, (2) anonymized cross-review with structured critiques and rankings, (3) chairman synthesis that integrates the strongest content while addressing flagged issues, and (4) claim-level consistency labeling based on inter-model agreement. On TruthfulQA (MC1), EdgeJury achieves 76.2% accuracy (95% CI: 72.8-79.6%), a +21.4% relative improvement over a single 8B baseline (62.8%), and outperforms standard baselines including self-consistency and majority voting under transparent compute accounting (total tokens and platform cost reported). On a 200-question adversarial EdgeCases set, EdgeJury yields +48.2% relative gains (95% CI: 44.0-52.4%). Manual analysis on 100 incorrect answers shows an approximately 55% reduction in factual hallucination errors versus the single-model baseline. Deployed on Cloudflare Workers AI, EdgeJury achieves 8.4 s median end-to-end latency, demonstrating that coordinated small-model ensembles can improve truthfulness on misconception-heavy QA benchmarks without external retrieval or proprietary large-model APIs.

</details>


### [330] [Unveiling the Heart-Brain Connection: An Analysis of ECG in Cognitive Performance](https://arxiv.org/abs/2601.01424)
*Akshay Sasi,Malavika Pradeep,Nusaibah Farrukh,Rahul Venugopal,Elizabeth Sherly*

Main category: cs.LG

TL;DR: The paper explores the use of ECG signals to infer cognitive load typically assessed by EEG, leveraging multimodal data and a cross-modal XGBoost framework for feature projection.


<details>
  <summary>Details</summary>
Motivation: Investigate if widely accessible ECG signals can serve as reliable proxies for EEG-based cognitive workload assessments, enabling real-time monitoring in everyday situations.

Method: Multimodal data acquisition using working-memory and passive-listening tasks. Feature extraction from ECG and EEG metrics, alongside a cross-modal XGBoost framework to project ECG features into EEG cognitive spaces.

Result: ECG-derived projections effectively capture cognitive state variations, showcasing good support for accurate cognitive workload classification.

Conclusion: ECG signals can provide a practical, interpretable, and wearable solution for real-time cognitive monitoring in real-world scenarios.

Abstract: Understanding the interaction of neural and cardiac systems during cognitive activity is critical to advancing physiological computing. Although EEG has been the gold standard for assessing mental workload, its limited portability restricts its real-world use. Widely available ECG through wearable devices proposes a pragmatic alternative. This research investigates whether ECG signals can reliably reflect cognitive load and serve as proxies for EEG-based indicators. In this work, we present multimodal data acquired from two different paradigms involving working-memory and passive-listening tasks. For each modality, we extracted ECG time-domain HRV metrics and Catch22 descriptors against EEG spectral and Catch22 features, respectively. We propose a cross-modal XGBoost framework to project the ECG features onto EEG-representative cognitive spaces, thereby allowing workload inferences using only ECG. Our results show that ECG-derived projections expressively capture variation in cognitive states and provide good support for accurate classification. Our findings underpin ECG as an interpretable, real-time, wearable solution for everyday cognitive monitoring.

</details>


### [331] [FedSCAM (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation): Scam-resistant SAM for Robust Federated Optimization in Heterogeneous Environments](https://arxiv.org/abs/2601.00853)
*Sameer Rahil,Zain Abdullah Ahmad,Talha Asif*

Main category: cs.LG

TL;DR: The paper introduces FedSCAM, a method to enhance Federated Learning under client heterogeneity by dynamically adjusting optimization and aggregation parameters, achieving better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces challenges in convergence and generalization due to non-IID data distributions among clients. The uniform treatment of clients in existing methods like SAM does not consider client-specific heterogeneity.

Method: FedSCAM dynamically adjusts the Sharpness-Aware Minimization (SAM) perturbation radius and aggregation weights based on client-specific heterogeneity scores. It uses a heterogeneity metric to modulate these parameters inversely.

Result: FedSCAM showed competitive performance compared to state-of-the-art algorithms like FedSAM and FedLESAM in terms of convergence speed and final test accuracy, validated on datasets like CIFAR-10 and Fashion-MNIST under label skew.

Conclusion: FedSCAM effectively addresses client heterogeneity in Federated Learning by incorporating adaptive mechanisms, improving convergence and test accuracy over existing approaches.

Abstract: Federated Learning (FL) enables collaborative model training across decentralized edge devices while preserving data privacy. However, statistical heterogeneity among clients, often manifested as non-IID label distributions, poses significant challenges to convergence and generalization. While Sharpness-Aware Minimization (SAM) has been introduced to FL to seek flatter, more robust minima, existing approaches typically apply a uniform perturbation radius across all clients, ignoring client-specific heterogeneity. In this work, we propose \textbf{FedSCAM} (Federated Sharpness-Aware Minimization with Clustered Aggregation and Modulation), a novel algorithm that dynamically adjusts the SAM perturbation radius and aggregation weights based on client-specific heterogeneity scores. By calculating a heterogeneity metric for each client and modulating the perturbation radius inversely to this score, FedSCAM prevents clients with high variance from destabilizing the global model. Furthermore, we introduce a heterogeneity-aware weighted aggregation mechanism that prioritizes updates from clients that align with the global optimization direction. Extensive experiments on CIFAR-10 and Fashion-MNIST under various degrees of Dirichlet-based label skew demonstrate that FedSCAM achieves competitive performance among state-of-the-art baselines, including FedSAM, FedLESAM, etc. in terms of convergence speed and final test accuracy.

</details>


### [332] [Harvesting AlphaEarth: Benchmarking the Geospatial Foundation Model for Agricultural Downstream Tasks](https://arxiv.org/abs/2601.00857)
*Yuchi Ma,Yawen Shen,Anu Swatantran,David B. Lobell*

Main category: cs.LG

TL;DR: This study evaluates AlphaEarth Foundation (AEF) embeddings from Google DeepMind in three agricultural tasks in the U.S., comparing them to traditional remote sensing models.


<details>
  <summary>Details</summary>
Motivation: The study aims to identify the potential and limitations of using AEF embeddings in agriculture, addressing the lack of evaluation on critical agricultural tasks and comparison with traditional RS-based models.

Method: The authors evaluated AEF embeddings on three tasks—crop yield prediction, tillage mapping, and cover crop mapping—using datasets from public and private sources. Comparisons were made with traditional remote sensing-based models.

Result: AEF-based models performed strongly across tasks and were competitive with RS-based models when trained on local data, but they showed limitations in spatial transferability, interpretability, and time sensitivity.

Conclusion: AEF embeddings have potential for agricultural use but face challenges in generalizability, interpretability, and time sensitivity, suggesting cautious application in these contexts.

Abstract: Geospatial foundation models (GFMs) have emerged as a promising approach to overcoming the limitations in existing featurization methods. More recently, Google DeepMind has introduced AlphaEarth Foundation (AEF), a GFM pre-trained using multi-source EOs across continuous time. An annual and global embedding dataset is produced using AEF that is ready for analysis and modeling. The internal experiments show that AEF embeddings have outperformed operational models in 15 EO tasks without re-training. However, those experiments are mostly about land cover and land use classification. Applying AEF and other GFMs to agricultural monitoring require an in-depth evaluation in critical agricultural downstream tasks. There is also a lack of comprehensive comparison between the AEF-based models and traditional remote sensing (RS)-based models under different scenarios, which could offer valuable guidance for researchers and practitioners. This study addresses some of these gaps by evaluating AEF embeddings in three agricultural downstream tasks in the U.S., including crop yield prediction, tillage mapping, and cover crop mapping. Datasets are compiled from both public and private sources to comprehensively evaluate AEF embeddings across tasks at different scales and locations, and RS-based models are trained as comparison models. AEF-based models generally exhibit strong performance on all tasks and are competitive with purpose-built RS-based models in yield prediction and county-level tillage mapping when trained on local data. However, we also find several limitations in current AEF embeddings, such as limited spatial transferability compared to RS-based models, low interpretability, and limited time sensitivity. These limitations recommend caution when applying AEF embeddings in agriculture, where time sensitivity, generalizability, and interpretability is important.

</details>


### [333] [Path Integral Solution for Dissipative Generative Dynamics](https://arxiv.org/abs/2601.00860)
*Xidi Wang*

Main category: cs.LG

TL;DR: Mechanical systems, guided by dissipative quantum dynamics, can generate coherent language; conservation laws hinder performance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether mechanical systems can achieve intelligent language generation and understand the importance of dissipation and non-locality.

Method: The paper utilizes Koopman operators with path integral propagators to analyze dissipative quantum dynamics and examines spectral eigenvalue structures.

Result: Irreversible computation driven by dissipation and context aggregation proves essential for coherent language generation; conservation constraints limit effectiveness.

Conclusion: Mechanical systems acquire linguistic intelligence through dissipation and causal context aggregation, highlighting the limitations imposed by conservation laws.

Abstract: Can purely mechanical systems generate intelligent language? We prove that dissipative quantum dynamics with analytically tractable non-local context aggregation produce coherent text generation, while conservation laws cause fundamental failure. Employing Koopman operators with closed-form path integral propagators, we show irreversible computation fundamentally requires both controlled information dissipation and causal context aggregation. Spectral analysis reveals emergent eigenvalue structure, separating into decay modes (forgetting), growth modes (amplification), and neutral modes (preservation) -- the essential ingredients for directed information flow. Hamiltonian constraints force the elimination of these dissipative modes and degrading performance despite unchanged model capacity. This establishes language generation as dissipative quantum field theory, proving mechanical systems acquire intelligence through the combination of dissipation and non-locality, not through conservation.

</details>


### [334] [Discount Model Search for Quality Diversity Optimization in High-Dimensional Measure Spaces](https://arxiv.org/abs/2601.01082)
*Bryon Tjanaka,Henry Chen,Matthew C. Fontaine,Stefanos Nikolaidis*

Main category: cs.LG

TL;DR: This paper introduces Discount Model Search (DMS) to improve Quality Diversity (QD) optimization in high-dimensional measure spaces, outperforming CMA-MAE in complex benchmarks.


<details>
  <summary>Details</summary>
Motivation: QD optimization struggles in high-dimensional measure spaces as contemporary methods like CMA-MAE treat solutions with similar measures identically, leading to stagnation.

Method: Proposed DMS employs a smooth, continuous model for discount values, enabling effective exploration in high-dimensional spaces by better distinguishing between solutions with similar measures.

Result: DMS demonstrated superior performance to CMA-MAE and other QD algorithms in high-dimensional benchmarks and novel applications involving image-based measure spaces.

Conclusion: DMS enables QD optimization in high-dimensional settings, broadening its applications and overcoming limitations of existing methods like CMA-MAE.

Abstract: Quality diversity (QD) optimization searches for a collection of solutions that optimize an objective while attaining diverse outputs of a user-specified, vector-valued measure function. Contemporary QD algorithms focus on low-dimensional measures because high-dimensional measures are prone to distortion, where many solutions found by the QD algorithm map to similar measures. For example, the CMA-MAE algorithm guides measure space exploration with a histogram in measure space that records so-called discount values. However, CMA-MAE stagnates in domains with high-dimensional measure spaces because solutions with similar measures fall into the same histogram cell and thus receive identical discount values. To address these limitations, we propose Discount Model Search (DMS), which guides exploration with a model that provides a smooth, continuous representation of discount values. In high-dimensional measure spaces, this model enables DMS to distinguish between solutions with similar measures and thus continue exploration. We show that DMS facilitates new QD applications by introducing two domains where the measure space is the high-dimensional space of images, which enables users to specify their desired measures by providing a dataset of images rather than hand-designing the measure function. Results in these domains and on high-dimensional benchmarks show that DMS outperforms CMA-MAE and other black-box QD algorithms.

</details>


### [335] [Universal Battery Degradation Forecasting Driven by Foundation Model Across Diverse Chemistries and Conditions](https://arxiv.org/abs/2601.00862)
*Joey Chan,Huan Wang,Haoyu Pan,Wei Wu,Zirong Wang,Zhen Chen,Ershun Pan,Min Xie,Lifeng Xi*

Main category: cs.LG

TL;DR: A unified framework is proposed for battery capacity forecasting that works across diverse chemistries and scenarios, using a large dataset and advanced modeling techniques.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of predicting battery capacity degradation across heterogeneous chemistries and conditions, which are critical for energy storage system efficiency and safety.

Method: The study curated a large dataset from 20 public sources and proposed a Time-Series Foundation Model (TSFM) with Low-Rank Adaptation (LoRA) and contrastive learning to identify shared degradation patterns.

Result: The unified model showed competitive or superior performance compared to specialized per-dataset models and demonstrated robustness in unseen chemistries and operating conditions.

Conclusion: The framework provides a scalable, transferable solution for battery degradation forecasting, showcasing the efficacy of TSFM-based architectures for real-world applications.

Abstract: Accurate forecasting of battery capacity fade is essential for the safety, reliability, and long-term efficiency of energy storage systems. However, the strong heterogeneity across cell chemistries, form factors, and operating conditions makes it difficult to build a single model that generalizes beyond its training domain. This work proposes a unified capacity forecasting framework that maintains robust performance across diverse chemistries and usage scenarios. We curate 20 public aging datasets into a large-scale corpus covering 1,704 cells and 3,961,195 charge-discharge cycle segments, spanning temperatures from $-5\,^{\circ}\mathrm{C}$ to $45\,^{\circ}\mathrm{C}$, multiple C-rates, and application-oriented profiles such as fast charging and partial cycling. On this corpus, we adopt a Time-Series Foundation Model (TSFM) backbone and apply parameter-efficient Low-Rank Adaptation (LoRA) together with physics-guided contrastive representation learning to capture shared degradation patterns. Experiments on both seen and deliberately held-out unseen datasets show that a single unified model achieves competitive or superior accuracy compared with strong per-dataset baselines, while retaining stable performance on chemistries, capacity scales, and operating conditions excluded from training. These results demonstrate the potential of TSFM-based architectures as a scalable and transferable solution for capacity degradation forecasting in real battery management systems.

</details>


### [336] [Evo-TFS: Evolutionary Time-Frequency Domain-Based Synthetic Minority Oversampling Approach to Imbalanced Time Series Classification](https://arxiv.org/abs/2601.01150)
*Wenbin Pei,Ruohao Dai,Bing Xue,Mengjie Zhang,Qiang Zhang,Yiu-Ming Cheung*

Main category: cs.LG

TL;DR: The paper introduces Evo-TFS, an evolutionary oversampling method for imbalanced time series datasets, which outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional deep learning models for time series classification assume balanced datasets, often ignoring minority classes in imbalanced data, which are usually more critical. Existing oversampling methods struggle to preserve temporal dynamics and diversity in samples.

Method: The proposed method, Evo-TFS, uses strongly typed genetic programming to generate diverse time series by integrating time- and frequency-domain characteristics. A specialized fitness function ensures quality sample generation.

Result: Evo-TFS significantly improves the performance of time-domain and frequency-domain classifiers on imbalanced time series datasets.

Conclusion: Evo-TFS addresses key limitations of oversampling in imbalanced time series data, enhancing classification accuracy through an evolutionary approach.

Abstract: Time series classification is a fundamental machine learning task with broad real-world applications. Although many deep learning methods have proven effective in learning time-series data for classification, they were originally developed under the assumption of balanced data distributions. Once data distribution is uneven, these methods tend to ignore the minority class that is typically of higher practical significance. Oversampling methods have been designed to address this by generating minority-class samples, but their reliance on linear interpolation often hampers the preservation of temporal dynamics and the generation of diverse samples. Therefore, in this paper, we propose Evo-TFS, a novel evolutionary oversampling method that integrates both time- and frequency-domain characteristics. In Evo-TFS, strongly typed genetic programming is employed to evolve diverse, high-quality time series, guided by a fitness function that incorporates both time-domain and frequency-domain characteristics. Experiments conducted on imbalanced time series datasets demonstrate that Evo-TFS outperforms existing oversampling methods, significantly enhancing the performance of time-domain and frequency-domain classifiers.

</details>


### [337] [Selective Imperfection as a Generative Framework for Analysis, Creativity and Discovery](https://arxiv.org/abs/2601.00863)
*Markus J. Buehler*

Main category: cs.LG

TL;DR: The paper explores a framework linking material hierarchies to musical composition, proposing reversible mappings between matter's properties and musical structures to generate novel insights and creations.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to fuse scientific analysis and artistic expression by linking the structural principles of matter with compositional aspects of music, aiming to shed light on both domains.

Method: The framework uses reversible mappings from molecular spectra to musical tones and structural networks to playable instruments, alongside swarm-based AI models to synthesize human-like musical structures.

Result: Findings reveal parallels between intermediate defect densities maximizing material strength and mid-entropy musical scales. Swarm-based AI compositions exhibit structural signatures similar to human creations.

Conclusion: Science and art, as acts of constrained world-building, share vibration as a grammar organizing structure across scales, providing avenues for creative and scientific expansion.

Abstract: We introduce materiomusic as a generative framework linking the hierarchical structures of matter with the compositional logic of music. Across proteins, spider webs and flame dynamics, vibrational and architectural principles recur as tonal hierarchies, harmonic progressions, and long-range musical form. Using reversible mappings, from molecular spectra to musical tones and from three-dimensional networks to playable instruments, we show how sound functions as a scientific probe, an epistemic inversion where listening becomes a mode of seeing and musical composition becomes a blueprint for matter. These mappings excavate deep time: patterns originating in femtosecond molecular vibrations or billion-year evolutionary histories become audible. We posit that novelty in science and art emerges when constraints cannot be satisfied within existing degrees of freedom, forcing expansion of the space of viable configurations. Selective imperfection provides the mechanism restoring balance between coherence and adaptability. Quantitative support comes from exhaustive enumeration of all 2^12 musical scales, revealing that culturally significant systems cluster in a mid-entropy, mid-defect corridor, directly paralleling the Hall-Petch optimum where intermediate defect densities maximize material strength. Iterating these mappings creates productive collisions between human creativity and physics, generating new information as musical structures encounter evolutionary constraints. We show how swarm-based AI models compose music exhibiting human-like structural signatures such as small-world connectivity, modular integration, long-range coherence, suggesting a route beyond interpolation toward invention. We show that science and art are generative acts of world-building under constraint, with vibration as a shared grammar organizing structure across scales.

</details>


### [338] [Distribution Matching for Graph Quantification Under Structural Covariate Shift](https://arxiv.org/abs/2601.00864)
*Clemens Damke,Eyke Hüllermeier*

Main category: cs.LG

TL;DR: This paper introduces a method to improve quantification learning (QL) on graph data by adapting to structural shifts using an enhanced KDEy approach.


<details>
  <summary>Details</summary>
Motivation: The focus is on accurately estimating label distributions (label prevalence) on graph-structured data despite structural shifts between training and test sets, which is a challenging and underexplored problem.

Method: The authors extend the importance sampling concept to the state-of-the-art KDEy quantification method, enabling adaptation to structural shifts in graph data.

Result: The extended KDEy method effectively handles structural shifts and achieves better performance than standard quantification approaches.

Conclusion: The proposed method offers an advancement in QL tasks by providing accurate label distribution estimates for graph data under structural shifts, demonstrating its effectiveness over existing approaches.

Abstract: Graphs are commonly used in machine learning to model relationships between instances. Consider the task of predicting the political preferences of users in a social network; to solve this task one should consider, both, the features of each individual user and the relationships between them. However, oftentimes one is not interested in the label of a single instance but rather in the distribution of labels over a set of instances; e.g., when predicting the political preferences of users, the overall prevalence of a given opinion might be of higher interest than the opinion of a specific person. This label prevalence estimation task is commonly referred to as quantification learning (QL). Current QL methods for tabular data are typically based on the so-called prior probability shift (PPS) assumption which states that the label-conditional instance distributions should remain equal across the training and test data. In the graph setting, PPS generally does not hold if the shift between training and test data is structural, i.e., if the training data comes from a different region of the graph than the test data. To address such structural shifts, an importance sampling variant of the popular adjusted count quantification approach has previously been proposed. In this work, we extend the idea of structural importance sampling to the state-of-the-art KDEy quantification approach. We show that our proposed method adapts to structural shifts and outperforms standard quantification approaches.

</details>


### [339] [SerpentFlow: Generative Unpaired Domain Alignment via Shared-Structure Decomposition](https://arxiv.org/abs/2601.01979)
*Julie Keisler,Anastase Alexandre Charantonis,Yannig Goude,Boutheina Oueslati,Claire Monteleoni*

Main category: cs.LG

TL;DR: This paper proposes SerpentFlow, a framework for unpaired domain alignment by decomposing shared and domain-specific components in data, and applies it to super-resolution tasks, achieving effective reconstruction of high-frequency structures.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of domain alignment between data distributions from distinct domains, especially in unpaired settings where direct supervision across domains is unavailable.

Method: SerpentFlow decomposes data in the latent space into shared and domain-specific components, replacing domain-specific details with noise to construct synthetic training pairs, enabling conditional generative models traditionally reliant on paired data.

Result: The approach was validated through experiments on synthetic images, physical simulations, and climate downscaling tasks, showing effective reconstruction of high-frequency details while preserving shared structural patterns.

Conclusion: The shared-structure decomposition method demonstrated its effectiveness for unpaired domain alignment, highlighting its applicability for generative adaptations in challenging tasks like super-resolution.

Abstract: Domain alignment refers broadly to learning correspondences between data distributions from distinct domains. In this work, we focus on a setting where domains share underlying structural patterns despite differences in their specific realizations. The task is particularly challenging in the absence of paired observations, which removes direct supervision across domains. We introduce a generative framework, called SerpentFlow (SharEd-structuRe decomPosition for gEnerative domaiN adapTation), for unpaired domain alignment. SerpentFlow decomposes data within a latent space into a shared component common to both domains and a domain-specific one. By isolating the shared structure and replacing the domain-specific component with stochastic noise, we construct synthetic training pairs between shared representations and target-domain samples, thereby enabling the use of conditional generative models that are traditionally restricted to paired settings. We apply this approach to super-resolution tasks, where the shared component naturally corresponds to low-frequency content while high-frequency details capture domain-specific variability. The cutoff frequency separating low- and high-frequency components is determined automatically using a classifier-based criterion, ensuring a data-driven and domain-adaptive decomposition. By generating pseudo-pairs that preserve low-frequency structures while injecting stochastic high-frequency realizations, we learn the conditional distribution of the target domain given the shared representation. We implement SerpentFlow using Flow Matching as the generative pipeline, although the framework is compatible with other conditional generative approaches. Experiments on synthetic images, physical process simulations, and a climate downscaling task demonstrate that the method effectively reconstructs high-frequency structures consistent with underlying low-frequency patterns, supporting shared-structure decomposition as an effective strategy for unpaired domain alignment.

</details>


### [340] [A-PINN: Auxiliary Physics-informed Neural Networks for Structural Vibration Analysis in Continuous Euler-Bernoulli Beam](https://arxiv.org/abs/2601.00866)
*Shivani Saini,Ramesh Kumar Vats,Arup Kumar Sahoo*

Main category: cs.LG

TL;DR: The paper introduces an enhanced Auxiliary physics-informed neural network (A-PINN) for solving structural vibration problems, showing a 40% improvement over baseline models.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and stability of simulations in structural vibration problems governed by differential equations using scientific machine learning models.

Method: Developed a modified Auxiliary physics-informed neural network (A-PINN) framework with balanced adaptive optimizers. Numerical simulations were conducted to test the model by approximating the Euler-Bernoulli beam equations.

Result: The A-PINN model exhibited superior numerical stability and predictive accuracy. It was validated through simulations and achieved at least 40% better performance compared to baseline methods.

Conclusion: The proposed A-PINN framework demonstrates its robustness and enhanced efficacy in analyzing structural vibration problems, showcasing its potential for advancing scientific machine learning applications.

Abstract: Recent advancements in physics-informed neural networks (PINNs) and their variants have garnered substantial focus from researchers due to their effectiveness in solving both forward and inverse problems governed by differential equations. In this research, a modified Auxiliary physics-informed neural network (A-PINN) framework with balanced adaptive optimizers is proposed for the analysis of structural vibration problems. In order to accurately represent structural systems, it is critical for capturing vibration phenomena and ensuring reliable predictive analysis. So, our investigations are crucial for gaining deeper insight into the robustness of scientific machine learning models for solving vibration problems. Further, to rigorously evaluate the performance of A-PINN, we conducted different numerical simulations to approximate the Euler-Bernoulli beam equations under the various scenarios. The numerical results substantiate the enhanced performance of our model in terms of both numerical stability and predictive accuracy. Our model shows improvement of at least 40% over the baselines.

</details>


### [341] [Communication-Efficient Federated AUC Maximization with Cyclic Client Participation](https://arxiv.org/abs/2601.01649)
*Umesh Vangapally,Wenhan Wu,Chen Chen,Zhishuai Guo*

Main category: cs.LG

TL;DR: This paper develops efficient algorithms for federated AUC maximization under cyclic client participation, addressing challenges in imbalanced data and non-decomposable objectives.


<details>
  <summary>Details</summary>
Motivation: To tackle the inefficiencies and challenges of federated learning in real-world scenarios, particularly when clients join training cyclically, with a focus on learning from imbalanced data.

Method: Two approaches are analyzed: 1) using squared surrogate loss with minimax optimization under the Polyak-Łojasiewicz condition, and 2) studying general pairwise AUC losses in noncyclic settings with performance bounds under the PL condition.

Result: The paper achieves improved communication and iteration complexities for both methods and demonstrates their superiority on image classification, medical imaging, and fraud detection tasks.

Conclusion: The proposed algorithms not only address cyclic client participation but also reconfirm their efficiency and enhanced learning potential for federated learning systems dealing with imbalanced data.

Abstract: Federated AUC maximization is a powerful approach for learning from imbalanced data in federated learning (FL). However, existing methods typically assume full client availability, which is rarely practical. In real-world FL systems, clients often participate in a cyclic manner: joining training according to a fixed, repeating schedule. This setting poses unique optimization challenges for the non-decomposable AUC objective. This paper addresses these challenges by developing and analyzing communication-efficient algorithms for federated AUC maximization under cyclic client participation. We investigate two key settings: First, we study AUC maximization with a squared surrogate loss, which reformulates the problem as a nonconvex-strongly-concave minimax optimization. By leveraging the Polyak-Łojasiewicz (PL) condition, we establish a state-of-the-art communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Second, we consider general pairwise AUC losses. We establish a communication complexity of $O(1/ε^3)$ and an iteration complexity of $O(1/ε^4)$. Further, under the PL condition, these bounds improve to communication complexity of $\widetilde{O}(1/ε^{1/2})$ and iteration complexity of $\widetilde{O}(1/ε)$. Extensive experiments on benchmark tasks in image classification, medical imaging, and fraud detection demonstrate the superior efficiency and effectiveness of our proposed methods.

</details>


### [342] [Hierarchical topological clustering](https://arxiv.org/abs/2601.00892)
*Ana Carpio,Gema Duro*

Main category: cs.LG

TL;DR: The paper introduces a hierarchical topological clustering algorithm to analyze data of arbitrary shapes and outliers effectively, applicable to diverse domains.


<details>
  <summary>Details</summary>
Motivation: The work aims to address challenges in clustering data clouds of arbitrary shapes with persistence of outliers, where conventional methods may fail.

Method: Proposes a hierarchical topological clustering algorithm that works with any distance measure to infer clusters and outliers.

Result: Demonstrates the algorithm's effectiveness on image, medical, and economic datasets, especially in identifying meaningful clusters in presence of outliers.

Conclusion: The presented method is robust in revealing meaningful clusters in scenarios where traditional clustering techniques are ineffective.

Abstract: Topological methods have the potential of exploring data clouds without making assumptions on their the structure. Here we propose a hierarchical topological clustering algorithm that can be implemented with any distance choice. The persistence of outliers and clusters of arbitrary shape is inferred from the resulting hierarchy. We demonstrate the potential of the algorithm on selected datasets in which outliers play relevant roles, consisting of images, medical and economic data. These methods can provide meaningful clusters in situations in which other techniques fail to do so.

</details>


### [343] [SmartFlow Reinforcement Learning and Agentic AI for Bike-Sharing Optimisation](https://arxiv.org/abs/2601.00868)
*Aditya Sreevatsa K,Arun Kumar Raveendran,Jesrael K Mani,Prakash G Shigli,Rajkumar Rangadore,Narayana Darapaneni,Anwesh Reddy Paduri*

Main category: cs.LG

TL;DR: SmartFlow is a framework using AI and RL to optimize bike-sharing systems, achieving a 95% reduction in imbalance with high efficiency.


<details>
  <summary>Details</summary>
Motivation: To address dynamic rebalancing challenges in urban bike-sharing systems and improve operational efficiency.

Method: A multi-layered framework using DQN for strategy development, deterministic tactics for journey optimization, and Agentic AI for communication.

Result: SmartFlow reduces network imbalance by over 95%, minimizes fleet travel, ensures truck utilization, and enhances bike availability.

Conclusion: SmartFlow demonstrates scalable and interpretable AI-driven solutions for efficient logistics in urban mobility networks.

Abstract: SmartFlow is a multi-layered framework that integrates Reinforcement Learning and Agentic AI to address the dynamic rebalancing problem in urban bike-sharing services. Its architecture separates strategic, tactical, and communication functions for clarity and scalability. At the strategic level, a Deep Q-Network (DQN) agent, trained in a high-fidelity simulation of New Yorks Citi Bike network, learns robust rebalancing policies by modelling the challenge as a Markov Decision Process. These high-level strategies feed into a deterministic tactical module that optimises multi-leg journeys and schedules just-in-time dispatches to minimise fleet travel. Evaluation across multiple seeded runs demonstrates SmartFlows high efficacy, reducing network imbalance by over 95% while requiring minimal travel distance and achieving strong truck utilisation. A communication layer, powered by a grounded Agentic AI with a Large Language Model (LLM), translates logistical plans into clear, actionable instructions for operational staff, ensuring interpretability and execution readiness. This integration bridges machine intelligence with human operations, offering a scalable solution that reduces idle time, improves bike availability, and lowers operational costs. SmartFlow provides a blueprint for interpretable, AI-driven logistics in complex urban mobility networks.

</details>


### [344] [Tackling Resource-Constrained and Data-Heterogeneity in Federated Learning with Double-Weight Sparse Pack](https://arxiv.org/abs/2601.01840)
*Qiantao Yang,Liquan Chen,Mingfu Xue,Songze Li*

Main category: cs.LG

TL;DR: The paper introduces FedCSPACK, a method for personalized federated learning that tackles the challenges of data heterogeneity and limited client resources by using a cosine sparsification parameter packing and dual-weighted aggregation technique.


<details>
  <summary>Details</summary>
Motivation: To improve federated learning performance in the context of data heterogeneity and constrained client resources by addressing limitations of existing methods, such as high bandwidth and computational demands.

Method: The authors propose the FedCSPACK method, which sparsifies and packages model parameters using cosine similarity to reduce bandwidth usage and incorporates dual-weighted aggregation to improve alignment and model robustness.

Result: Extensive experiments across four datasets show that FedCSPACK enhances communication and computational efficiency while maintaining high model accuracy compared to state-of-the-art methods.

Conclusion: FedCSPACK is an effective solution for reducing bandwidth and computational demands in federated learning while addressing data heterogeneity, thus improving overall model performance and robustness.

Abstract: Federated learning has drawn widespread interest from researchers, yet the data heterogeneity across edge clients remains a key challenge, often degrading model performance. Existing methods enhance model compatibility with data heterogeneity by splitting models and knowledge distillation. However, they neglect the insufficient communication bandwidth and computing power on the client, failing to strike an effective balance between addressing data heterogeneity and accommodating limited client resources. To tackle this limitation, we propose a personalized federated learning method based on cosine sparsification parameter packing and dual-weighted aggregation (FedCSPACK), which effectively leverages the limited client resources and reduces the impact of data heterogeneity on model performance. In FedCSPACK, the client packages model parameters and selects the most contributing parameter packages for sharing based on cosine similarity, effectively reducing bandwidth requirements. The client then generates a mask matrix anchored to the shared parameter package to improve the alignment and aggregation efficiency of sparse updates on the server. Furthermore, directional and distribution distance weights are embedded in the mask to implement a weighted-guided aggregation mechanism, enhancing the robustness and generalization performance of the global model. Extensive experiments across four datasets using ten state-of-the-art methods demonstrate that FedCSPACK effectively improves communication and computational efficiency while maintaining high model accuracy.

</details>


### [345] [Quantum Machine Learning Approaches for Coordinated Stealth Attack Detection in Distributed Generation Systems](https://arxiv.org/abs/2601.00873)
*Osasumwen Cedric Ogiesoba-Eguakun,Suman Rath*

Main category: cs.LG

TL;DR: The study evaluates quantum machine learning for detecting coordinated stealth attacks in a microgrid, finding hybrid quantum classical models to perform best.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of detecting coordinated stealth attacks on distributed generation systems, which traditional methods struggle to identify.

Method: The authors used simulated measurements and created a binary classification dataset, evaluating classical machine learning, fully quantum models, and hybrid quantum classical approaches.

Result: The hybrid quantum-classical model using quantum feature embeddings and classical RBF SVM performed the best, improving accuracy and F1 score over classical models.

Conclusion: Although fully quantum models currently face limitations, hybrid quantum classical approaches demonstrate the potential of quantum feature mapping in cybersecurity applications.

Abstract: Coordinated stealth attacks are a serious cybersecurity threat to distributed generation systems because they modify control and measurement signals while remaining close to normal behavior, making them difficult to detect using standard intrusion detection methods. This study investigates quantum machine learning approaches for detecting coordinated stealth attacks on a distributed generation unit in a microgrid. High-quality simulated measurements were used to create a balanced binary classification dataset using three features: reactive power at DG1, frequency deviation relative to the nominal value, and terminal voltage magnitude. Classical machine learning baselines, fully quantum variational classifiers, and hybrid quantum classical models were evaluated. The results show that a hybrid quantum classical model combining quantum feature embeddings with a classical RBF support vector machine achieves the best overall performance on this low dimensional dataset, with a modest improvement in accuracy and F1 score over a strong classical SVM baseline. Fully quantum models perform worse due to training instability and limitations of current NISQ hardware. In contrast, hybrid models train more reliably and demonstrate that quantum feature mapping can enhance intrusion detection even when fully quantum learning is not yet practical.

</details>


### [346] [Conformal Prediction Under Distribution Shift: A COVID-19 Natural Experiment](https://arxiv.org/abs/2601.00908)
*Chorok Lee*

Main category: cs.LG

TL;DR: This paper investigates how conformal prediction coverage is impacted by distribution shifts, using COVID-19-related supply chain tasks. The study highlights the role of feature dependence and distribution shifts in coverage degradation, and suggests a decision framework for better model handling.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand how conformal prediction robustness is affected by distribution shifts, and to provide actionable insights for improving predictive reliability in real-world applications, particularly under severe feature turnover.

Method: The authors analyze 8 supply chain tasks affected by COVID-19 with conformal prediction coverage, using tools like SHAP explanations to identify correlations between feature dependence and task performance. They test interventions like quarterly retraining and conduct exploratory analyses involving additional tasks with varying feature stability.

Result: The study finds that catastrophic failures occur when tasks heavily depend on single features, while robust tasks distribute importance across many features. Retraining provides coverage improvements only for catastrophic tasks. Feature stability, rather than concentration, determines robustness under moderate shifts.

Conclusion: To address coverage degradation: focus on monitoring feature concentration via SHAP, retrain quarterly when specific vulnerability thresholds are met, and avoid retraining for inherently robust models. The framework provides practical guidance for maintaining performance under severe or moderate distribution shifts.

Abstract: Conformal prediction guarantees degrade under distribution shift. We study this using COVID-19 as a natural experiment across 8 supply chain tasks. Despite identical severe feature turnover (Jaccard approximately 0), coverage drops vary from 0% to 86.7%, spanning two orders of magnitude. Using SHapley Additive exPlanations (SHAP) analysis, we find catastrophic failures correlate with single-feature dependence (rho = 0.714, p = 0.047). Catastrophic tasks concentrate importance in one feature (4.5x increase), while robust tasks redistribute across many (10-20x). Quarterly retraining restores catastrophic task coverage from 22% to 41% (+19 pp, p = 0.04), but provides no benefit for robust tasks (99.8% coverage). Exploratory analysis of 4 additional tasks with moderate feature stability (Jaccard 0.13-0.86) reveals feature stability, not concentration, determines robustness, suggesting concentration effects apply specifically to severe shifts. We provide a decision framework: monitor SHAP concentration before deployment; retrain quarterly if vulnerable (>40% concentration); skip retraining if robust.

</details>


### [347] [LLMize: A Framework for Large Language Model-Based Numerical Optimization](https://arxiv.org/abs/2601.00874)
*M. Rizki Oktavian*

Main category: cs.LG

TL;DR: LLMize is an open-source framework using large language models for optimization by combining natural language and external evaluation, suited for complex, constraint-heavy tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to explore the potential of large language models for solving numerical optimization problems, especially those including complex constraints and domain knowledge.

Method: The optimization is framed as iterative prompting, where LLMs generate solutions evaluated externally and refined over successive iterations. Strategies like Optimization by Prompting, evolutionary algorithms, and simulated annealing are introduced.

Result: LLMize performs competently in complex, domain-specific tasks but is less competitive than traditional solvers in simpler problems.

Conclusion: LLMize offers a practical method for optimization in challenging scenarios, enabling the use of natural language to encode problem constraints and heuristics.

Abstract: Large language models (LLMs) have recently shown strong reasoning capabilities beyond traditional language tasks, motivating their use for numerical optimization. This paper presents LLMize, an open-source Python framework that enables LLM-driven optimization through iterative prompting and in-context learning. LLMize formulates optimization as a black-box process in which candidate solutions are generated in natural language, evaluated by an external objective function, and refined over successive iterations using solution-score feedback. The framework supports multiple optimization strategies, including Optimization by Prompting (OPRO) and hybrid LLM-based methods inspired by evolutionary algorithms and simulated annealing. A key advantage of LLMize is the ability to inject constraints, rules, and domain knowledge directly through natural language descriptions, allowing practitioners to define complex optimization problems without requiring expertise in mathematical programming or metaheuristic design. LLMize is evaluated on convex optimization, linear programming, the Traveling Salesman Problem, neural network hyperparameter tuning, and nuclear fuel lattice optimization. Results show that while LLM-based optimization is not competitive with classical solvers for simple problems, it provides a practical and accessible approach for complex, domain-specific tasks where constraints and heuristics are difficult to formalize.

</details>


### [348] [LearnAD: Learning Interpretable Rules for Brain Networks in Alzheimer's Disease Classification](https://arxiv.org/abs/2601.00877)
*Thomas Andrews,Mark Law,Sara Ahmadi-Abhari,Alessandra Russo*

Main category: cs.LG

TL;DR: LearnAD predicts Alzheimer's disease using brain MRI data with fully interpretable rules, leveraging neuro-symbolic learning.


<details>
  <summary>Details</summary>
Motivation: To develop interpretable methods for Alzheimer's disease diagnosis and enhance understanding of statistical and neural models in neuroscience.

Method: Combining statistical models, Decision Trees, Random Forests, and GNNs with symbolic reasoning using FastLAS for rule learning.

Result: LearnAD offers competitive accuracy, slightly below some statistical models, while maintaining full interpretability.

Conclusion: LearnAD bridges the gap between interpretability and performance in Alzheimer's prediction using neuro-symbolic techniques and aids GNN understanding in neuroscience.

Abstract: We introduce LearnAD, a neuro-symbolic method for predicting Alzheimer's disease from brain magnetic resonance imaging data, learning fully interpretable rules. LearnAD applies statistical models, Decision Trees, Random Forests, or GNNs to identify relevant brain connections, and then employs FastLAS to learn global rules. Our best instance outperforms Decision Trees, matches Support Vector Machine accuracy, and performs only slightly below Random Forests and GNNs trained on all features, all while remaining fully interpretable. Ablation studies show that our neuro-symbolic approach improves interpretability with comparable performance to pure statistical models. LearnAD demonstrates how symbolic learning can deepen our understanding of GNN behaviour in clinical neuroscience.

</details>


### [349] [Outlier Detection Using Vector Cosine Similarity by Adding a Dimension](https://arxiv.org/abs/2601.00883)
*Zhongyang Shen*

Main category: cs.LG

TL;DR: A new multi-dimensional outlier detection method (MDOD) is introduced using cosine similarity and available on PyPI.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for effective outlier detection in multi-dimensional datasets.

Method: It involves adding a zero-valued dimension to the dataset, creating vectors, and comparing cosine similarities to detect outliers.

Result: The proposed method effectively identifies outliers and is implemented as a PyPI package.

Conclusion: The method provides an innovative approach to outlier detection with practical applicability supported by the MDOD implementation.

Abstract: We propose a new outlier detection method for multi-dimensional data. The method detects outliers based on vector cosine similarity, using a new dataset constructed by adding a dimension with zero values to the original data. When a point in the new dataset is selected as the measured point, an observation point is created as the origin, differing only in the new dimension by having a non-zero value compared to the measured point. Vectors are then formed from the observation point to the measured point and to other points in the dataset. By comparing the cosine similarities of these vectors, abnormal data can be identified. An optimized implementation (MDOD) is available on PyPI: https://pypi.org/project/mdod/.

</details>


### [350] [Revisiting Weighted Strategy for Non-stationary Parametric Bandits and MDPs](https://arxiv.org/abs/2601.01069)
*Jing Wang,Peng Zhao,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: This paper refines the analysis of non-stationary parametric bandits, particularly weighted strategies, leading to simpler and more efficient algorithms with improved regret guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the challenges and inefficiencies in analyzing and designing algorithms for non-stationary parametric bandits using the weighted strategy, especially given its practical relevance to gradual drifting patterns in real-world scenarios.

Method: A refined analysis framework is proposed that simplifies the derivation and improves the regret bounds for weighted strategies in non-stationary parametric bandits. The framework is extended to linear, generalized linear, and self-concordant bandits, as well as non-stationary Markov Decision Processes with function approximation.

Result: The proposed framework yields a simpler, computationally efficient algorithm for linear bandits with regret bounds comparable to existing approaches. It improves regret bounds for Generalized Linear Bandits and introduces weighted strategy algorithms for non-stationary Markov Decision Processes, achieving dynamic regret guarantees.

Conclusion: The refined framework simplifies the theoretical analysis, ensures computational efficiency, and enhances the statistical performance of algorithms for non-stationary parametric bandits and related decision models.

Abstract: Non-stationary parametric bandits have attracted much attention recently. There are three principled ways to deal with non-stationarity, including sliding-window, weighted, and restart strategies. As many non-stationary environments exhibit gradual drifting patterns, the weighted strategy is commonly adopted in real-world applications. However, previous theoretical studies show that its analysis is more involved and the algorithms are either computationally less efficient or statistically suboptimal. This paper revisits the weighted strategy for non-stationary parametric bandits. In linear bandits (LB), we discover that this undesirable feature is due to an inadequate regret analysis, which results in an overly complex algorithm design. We propose a \emph{refined analysis framework}, which simplifies the derivation and, importantly, produces a simpler weight-based algorithm that is as efficient as window/restart-based algorithms while retaining the same regret as previous studies. Furthermore, our new framework can be used to improve regret bounds of other parametric bandits, including Generalized Linear Bandits (GLB) and Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB algorithm with an $\tilde{O}(k_μ^{5/4} c_μ^{-3/4} d^{3/4} P_T^{1/4}T^{3/4})$ regret, improving the $\tilde{O}(k_μ^{2} c_μ^{-1}d^{9/10} P_T^{1/5}T^{4/5})$ bound in prior work, where $k_μ$ and $c_μ$ characterize the reward model's nonlinearity, $P_T$ measures the non-stationarity, $d$ and $T$ denote the dimension and time horizon. Moreover, we extend our framework to non-stationary Markov Decision Processes (MDPs) with function approximation, focusing on Linear Mixture MDP and Multinomial Logit (MNL) Mixture MDP. For both classes, we propose algorithms based on the weighted strategy and establish dynamic regret guarantees using our analysis framework.

</details>


### [351] [FANoS: Friction-Adaptive Nosé--Hoover Symplectic Momentum for Stiff Objectives](https://arxiv.org/abs/2601.00889)
*Nalin Dhiman*

Main category: cs.LG

TL;DR: A study on the optimizer FANoS reveals potential benefits on specific optimization scenarios but is not universally better than existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from molecular dynamics integration and thermostat ideas, aiming to create a heuristic optimization method for stiff nonconvex landscapes.

Method: FANoS combines a discretized dynamical system for momentum update, Nosé--Hoover thermostat variables, and a semi-implicit symplectic-Euler integrator.

Result: FANoS shows some improved performance on specific benchmarks, like Rosenbrock-100D, but underperforms or becomes unstable in other scenarios like ill-conditioned convex quadratics.

Conclusion: FANoS is an interpretable synthesis of existing methods, useful in certain cases but sensitive to hyperparameter choices and not universally superior.

Abstract: We study a physics-inspired optimizer, \emph{FANoS} (Friction-Adaptive Nosé--Hoover Symplectic momentum), which combines (i) a momentum update written as a discretized second-order dynamical system, (ii) a Nosé--Hoover-like thermostat variable that adapts a scalar friction coefficient using kinetic-energy feedback, and (iii) a semi-implicit (symplectic-Euler) integrator, optionally with a diagonal RMS preconditioner. The method is motivated by structure-preserving integration and thermostat ideas from molecular dynamics, but is used here purely as an optimization heuristic.
  We provide the algorithm and limited theoretical observations in idealized settings. On the deterministic Rosenbrock-100D benchmark with 3000 gradient evaluations, FANoS-RMS attains a mean final objective value of $1.74\times 10^{-2}$, improving substantially over unclipped AdamW ($48.50$) and SGD+momentum ($90.76$) in this protocol. However, AdamW with gradient clipping is stronger, reaching $1.87\times 10^{-3}$, and L-BFGS reaches $\approx 4.4\times 10^{-10}$. On ill-conditioned convex quadratics and in a small PINN warm-start suite (Burgers and Allen--Cahn), the default FANoS configuration underperforms AdamW and can be unstable or high-variance.
  Overall, the evidence supports a conservative conclusion: FANoS is an interpretable synthesis of existing ideas that can help on some stiff nonconvex valleys, but it is not a generally superior replacement for modern baselines, and its behavior is sensitive to temperature-schedule and hyperparameter choices.

</details>


### [352] [Wittgenstein's Family Resemblance Clustering Algorithm](https://arxiv.org/abs/2601.01127)
*Golbahar Amanpour,Benyamin Ghojogh*

Main category: cs.LG

TL;DR: The paper proposes a novel clustering algorithm, WFR, inspired by Wittgenstein's family resemblance concept, which forms clusters through resemblance graphs without needing prior cluster shape or count assumptions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve clustering algorithms by integrating Wittgenstein's family resemblance concept, enabling graph-based clustering approaches without strict assumptions about categories.

Method: The paper developed the WFR algorithm and its kernel variant, which utilizes resemblance scores between data points to construct a resemblance graph for clustering.

Result: Simulations on benchmark datasets showed the WFR algorithm's effectiveness as a nonlinear clustering tool without reliance on predefined clusters or shapes.

Conclusion: WFR introduces a philosophically inspired, adaptive clustering method that broadens the scope of machine learning through a graph-based approach.

Abstract: This paper, introducing a novel method in philomatics, draws on Wittgenstein's concept of family resemblance from analytic philosophy to develop a clustering algorithm for machine learning. According to Wittgenstein's Philosophical Investigations (1953), family resemblance holds that members of a concept or category are connected by overlapping similarities rather than a single defining property. Consequently, a family of entities forms a chain of items sharing overlapping traits. This philosophical idea naturally lends itself to a graph-based approach in machine learning. Accordingly, we propose the Wittgenstein's Family Resemblance (WFR) clustering algorithm and its kernel variant, kernel WFR. This algorithm computes resemblance scores between neighboring data instances, and after thresholding these scores, a resemblance graph is constructed. The connected components of this graph define the resulting clusters. Simulations on benchmark datasets demonstrate that WFR is an effective nonlinear clustering algorithm that does not require prior knowledge of the number of clusters or assumptions about their shapes.

</details>


### [353] [Sparse Bayesian Message Passing under Structural Uncertainty](https://arxiv.org/abs/2601.01207)
*Yoonhyuk Choi,Jiho Choi,Chanran Kim,Yumin Lee,Hawon Shin,Yeowon Jeon,Minjeong Kim,Jiwoo Kang*

Main category: cs.LG

TL;DR: This paper addresses semi-supervised learning challenges in heterophilic graphs by proposing a robust method using posterior distributions over signed adjacency matrices.


<details>
  <summary>Details</summary>
Motivation: The study aims to handle challenges in graph semi-supervised learning caused by unreliable or heterophilic graph structures, where traditional methods relying on fixed adjacency or structural regularization fail.

Method: The authors introduce a sparse signed message passing network that models a posterior distribution over signed adjacency matrices (allowing edges to be positive, negative, or absent) and employs marginalization and message aggregation techniques.

Result: The proposed method outperforms existing baselines in heterophilic settings on both synthetic and real-world graphs with structural noise.

Conclusion: The paper demonstrates a principled approach to address edge noise and heterophily in graphs, achieving state-of-the-art performance in challenging scenarios.

Abstract: Semi-supervised learning on real-world graphs is frequently challenged by heterophily, where the observed graph is unreliable or label-disassortative. Many existing graph neural networks either rely on a fixed adjacency structure or attempt to handle structural noise through regularization. In this work, we explicitly capture structural uncertainty by modeling a posterior distribution over signed adjacency matrices, allowing each edge to be positive, negative, or absent. We propose a sparse signed message passing network that is naturally robust to edge noise and heterophily, which can be interpreted from a Bayesian perspective. By combining (i) posterior marginalization over signed graph structures with (ii) sparse signed message aggregation, our approach offers a principled way to handle both edge noise and heterophily. Experimental results demonstrate that our method outperforms strong baseline models on heterophilic benchmarks under both synthetic and real-world structural noise.

</details>


### [354] [When to Ponder: Adaptive Compute Allocation for Code Generation via Test-Time Training](https://arxiv.org/abs/2601.00894)
*Gihyeon Sim*

Main category: cs.LG

TL;DR: PonderTTT introduces a training-free gating system for selective updates during test-time, improving model performance significantly on out-of-distribution languages in code modeling.


<details>
  <summary>Details</summary>
Motivation: Large language models perform uniform computations across inputs without considering input difficulty, leading to inefficiencies.

Method: A gating strategy using a self-supervised reconstruction loss from TTT layer is proposed to selectively trigger test-time updates, calibrated with a single scalar threshold adapted via EMA.

Result: The method achieves 82-89% Oracle Recovery without additional training, outperforming random baselines and reducing loss significantly on out-of-distribution languages.

Conclusion: PonderTTT's gating mechanism effectively enhances model adaptability and efficiency during inference, improving performance without requiring ground-truth labels or extra networks.

Abstract: Large language models apply uniform computation to all inputs, regardless of difficulty. We propose PonderTTT, a gating strategy using the TTT layer's self-supervised reconstruction loss to selectively trigger Test-Time Training (TTT) updates. The gating decision itself is training-free--requiring no learned classifier or auxiliary networks; only a single scalar threshold is initially calibrated on unlabeled data and continuously adapted via EMA to maintain target update rates. Our experiments with GPT-2 models (124M to 1.5B) on code language modeling (The Stack v2, teacher-forced perplexity) demonstrate that this signal is inference-compatible, requiring no ground-truth labels. Our Reconstruction Gating achieves 82-89% Oracle Recovery while being fully training-free, significantly outperforming Random Skip baselines (up to 16% lower loss on OOD languages).

</details>


### [355] [Adaptive Conformal Prediction via Bayesian Uncertainty Weighting for Hierarchical Healthcare Data](https://arxiv.org/abs/2601.01223)
*Marzieh Amiri Shahbazi,Ali Baheri,Nasibeh Azadeh-Fard*

Main category: cs.LG

TL;DR: A hybrid Bayesian-conformal framework was developed for healthcare predictions, achieving both distribution-free coverage guarantees and risk-adaptive precision.


<details>
  <summary>Details</summary>
Motivation: Clinical decision-making requires uncertainty quantification that balances coverage guarantees and precision, but existing methods fail to meet these needs.

Method: The approach combines Bayesian hierarchical random forests with group-aware conformal calibration, weighting conformity scores using posterior uncertainties.

Result: The method achieves target coverage (94.3% vs 95%) with risk-adaptive precision, demonstrating narrower intervals for low-uncertainty cases and appropriately wider intervals for high-risk predictions.

Conclusion: The framework provides uncertainty-aware decision support in healthcare, enabling better resource planning, clinical protocols, and oversight for uncertain cases.

Abstract: Clinical decision-making demands uncertainty quantification that provides both distribution-free coverage guarantees and risk-adaptive precision, requirements that existing methods fail to jointly satisfy. We present a hybrid Bayesian-conformal framework that addresses this fundamental limitation in healthcare predictions. Our approach integrates Bayesian hierarchical random forests with group-aware conformal calibration, using posterior uncertainties to weight conformity scores while maintaining rigorous coverage validity. Evaluated on 61,538 admissions across 3,793 U.S. hospitals and 4 regions, our method achieves target coverage (94.3% vs 95% target) with adaptive precision: 21% narrower intervals for low-uncertainty cases while appropriately widening for high-risk predictions. Critically, we demonstrate that well-calibrated Bayesian uncertainties alone severely under-cover (14.1%), highlighting the necessity of our hybrid approach. This framework enables risk-stratified clinical protocols, efficient resource planning for high-confidence predictions, and conservative allocation with enhanced oversight for uncertain cases, providing uncertainty-aware decision support across diverse healthcare settings.

</details>


### [356] [Dichotomous Diffusion Policy Optimization](https://arxiv.org/abs/2601.00898)
*Ruiming Liang,Yinan Zheng,Kexin Zheng,Tianyi Tan,Jianxiong Li,Liyuan Mao,Zhihao Wang,Guang Chen,Hangjun Ye,Jingjing Liu,Jinqiao Wang,Xianyuan Zhan*

Main category: cs.LG

TL;DR: The study introduces DIPOLE, a novel RL algorithm that stabilizes and optimizes diffusion-based policies. It addresses key challenges in training large diffusion policies effectively with applications including autonomous driving.


<details>
  <summary>Details</summary>
Motivation: To address challenges in training large diffusion-based policies with reinforcement learning effectively, particularly issues of instability and computational inefficiency in existing methods.

Method: Introduces the DIPOLE algorithm, which utilizes a KL-regularized objective for extracting diffusion policies and proposes a greedified policy regularization scheme, decomposing optimal policies into two elements: one focusing on reward maximization and the other on minimization.

Result: DIPOLE demonstrates effectiveness in offline and offline-to-online RL evaluations on ExORL and OGBench. It also trains a large vision-language-action model for autonomous driving and showcases strong results on the NAVSIM benchmark.

Conclusion: The proposed DIPOLE approach successfully achieves stable and controllable optimization of diffusion-based policies, demonstrating its potential for addressing complex decision-making problems and real-world applications such as autonomous driving.

Abstract: Diffusion-based policies have gained growing popularity in solving a wide range of decision-making tasks due to their superior expressiveness and controllable generation during inference. However, effectively training large diffusion policies using reinforcement learning (RL) remains challenging. Existing methods either suffer from unstable training due to directly maximizing value objectives, or face computational issues due to relying on crude Gaussian likelihood approximation, which requires a large amount of sufficiently small denoising steps. In this work, we propose DIPOLE (Dichotomous diffusion Policy improvement), a novel RL algorithm designed for stable and controllable diffusion policy optimization. We begin by revisiting the KL-regularized objective in RL, which offers a desirable weighted regression objective for diffusion policy extraction, but often struggles to balance greediness and stability. We then formulate a greedified policy regularization scheme, which naturally enables decomposing the optimal policy into a pair of stably learned dichotomous policies: one aims at reward maximization, and the other focuses on reward minimization. Under such a design, optimized actions can be generated by linearly combining the scores of dichotomous policies during inference, thereby enabling flexible control over the level of greediness.Evaluations in offline and offline-to-online RL settings on ExORL and OGBench demonstrate the effectiveness of our approach. We also use DIPOLE to train a large vision-language-action (VLA) model for end-to-end autonomous driving (AD) and evaluate it on the large-scale real-world AD benchmark NAVSIM, highlighting its potential for complex real-world applications.

</details>


### [357] [Latent-Constrained Conditional VAEs for Augmenting Large-Scale Climate Ensembles](https://arxiv.org/abs/2601.00915)
*Jacquelyn Shelton,Przemyslaw Polewski,Alexander Robel,Matthew Hoffman,Stephen Price*

Main category: cs.LG

TL;DR: The paper proposes advancements in generative modeling for creating statistically consistent climate model ensembles using latent-constrained autoencoders, addressing limited data issues.


<details>
  <summary>Details</summary>
Motivation: There is a need to enable efficient generation of additional, statistically consistent realizations of climate variables to improve downstream analyses without the computational cost of large ensembles.

Method: A latent-constrained conditional variational autoencoder (LC-CVAE) is introduced to enforce homogeneity in embeddings across realizations, paired with Gaussian process regression in the latent space for generating new realizations.

Result: Experiments show instability with single realization training, improvements with multiple realizations until plateauing, and reveal that reconstruction quality depends on spatial coverage and neighbor distance in latent space.

Conclusion: LC-CVAE-based generative modeling provides a novel solution for creating consistent climate realizations, balancing trade-offs between spatial accuracy and computational efficiency.

Abstract: Large climate-model ensembles are computationally expensive; yet many downstream analyses would benefit from additional, statistically consistent realizations of spatiotemporal climate variables. We study a generative modeling approach for producing new realizations from a limited set of available runs by transferring structure learned across an ensemble. Using monthly near-surface temperature time series from ten independent reanalysis realizations (ERA5), we find that a vanilla conditional variational autoencoder (CVAE) trained jointly across realizations yields a fragmented latent space that fails to generalize to unseen ensemble members. To address this, we introduce a latent-constrained CVAE (LC-CVAE) that enforces cross-realization homogeneity of latent embeddings at a small set of shared geographic 'anchor' locations. We then use multi-output Gaussian process regression in the latent space to predict latent coordinates at unsampled locations in a new realization, followed by decoding to generate full time series fields. Experiments and ablations demonstrate (i) instability when training on a single realization, (ii) diminishing returns after incorporating roughly five realizations, and (iii) a trade-off between spatial coverage and reconstruction quality that is closely linked to the average neighbor distance in latent space.

</details>


### [358] [Attention Needs to Focus: A Unified Perspective on Attention Allocation](https://arxiv.org/abs/2601.00919)
*Zichuan Fu,Wentao Song,Guojing Li,Yejing Wang,Xian Wu,Yimin Deng,Hanyu Yan,Yefeng Zheng,Xiangyu Zhao*

Main category: cs.LG

TL;DR: The paper identifies flaws in the Transformer's standard attention mechanism, characterizing them as Attention Overload and Attention Underload, and proposes Lazy Attention to address these issues effectively.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of the standard attention mechanism in Transformers, specifically representational collapse and attention sink, which hinder the performance of LLMs.

Method: The authors introduce Lazy Attention, which enhances attention allocation by employing positional discrimination to tackle overload and using Elastic-Softmax, a modified normalization method, to suppress underload.

Result: Lazy Attention was tested on the FineWeb-Edu corpus across nine diverse benchmarks. It successfully mitigates attention sink, achieves competitive performance against other architectures, and reaches up to 59.58% attention sparsity.

Conclusion: Lazy Attention offers a unified solution to previously isolated issues in the Transformers' attention mechanism, improving its focus and overall performance.

Abstract: The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.

</details>


### [359] [MODE: Efficient Time Series Prediction with Mamba Enhanced by Low-Rank Neural ODEs](https://arxiv.org/abs/2601.00920)
*Xingsheng Chen,Regina Zhang,Bo Gao,Xingwei He,Xiaofeng Liu,Pietro Lio,Kwok-Yan Lam,Siu-Ming Yiu*

Main category: cs.LG

TL;DR: The paper introduces MODE, a unified framework integrating Low-Rank Neural Ordinary Differential Equations and an Enhanced Mamba architecture for efficient and scalable time series prediction.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges of balancing efficiency, scalability, and accuracy in time series prediction, especially for long-range dependencies and irregularly sampled data.

Method: The framework combines Neural ODEs with an Enhanced Mamba architecture, leveraging techniques like Causal Convolution, SiLU activation, low-rank approximations, and a selective scanning mechanism to handle temporal dynamics and improve scalability.

Result: MODE outperforms existing methods in predictive accuracy and computational efficiency, as demonstrated in extensive experiments on benchmark datasets.

Conclusion: MODE provides a unified, efficient architecture with enhanced temporal modeling capabilities, offering significant improvements in both efficiency and scalability for long-term time series prediction.

Abstract: Time series prediction plays a pivotal role across diverse domains such as finance, healthcare, energy systems, and environmental modeling. However, existing approaches often struggle to balance efficiency, scalability, and accuracy, particularly when handling long-range dependencies and irregularly sampled data. To address these challenges, we propose MODE, a unified framework that integrates Low-Rank Neural Ordinary Differential Equations (Neural ODEs) with an Enhanced Mamba architecture. As illustrated in our framework, the input sequence is first transformed by a Linear Tokenization Layer and then processed through multiple Mamba Encoder blocks, each equipped with an Enhanced Mamba Layer that employs Causal Convolution, SiLU activation, and a Low-Rank Neural ODE enhancement to efficiently capture temporal dynamics. This low-rank formulation reduces computational overhead while maintaining expressive power. Furthermore, a segmented selective scanning mechanism, inspired by pseudo-ODE dynamics, adaptively focuses on salient subsequences to improve scalability and long-range sequence modeling. Extensive experiments on benchmark datasets demonstrate that MODE surpasses existing baselines in both predictive accuracy and computational efficiency. Overall, our contributions include: (1) a unified and efficient architecture for long-term time series modeling, (2) integration of Mamba's selective scanning with low-rank Neural ODEs for enhanced temporal representation, and (3) substantial improvements in efficiency and scalability enabled by low-rank approximation and dynamic selective scanning.

</details>


### [360] [MentalGame: Predicting Personality-Job Fitness for Software Developers Using Multi-Genre Games and Machine Learning Approaches](https://arxiv.org/abs/2601.01206)
*Soroush Elyasi,Arya VarastehNezhad,Fattaneh Taghiyareh*

Main category: cs.LG

TL;DR: This paper evaluates game-based assessments for predicting software developer suitability using behavioral data from gameplay, achieving high accuracy and precision.


<details>
  <summary>Details</summary>
Motivation: Traditional self-report questionnaires in career assessment suffer from biases and distortions, necessitating innovative, less-biased alternatives.

Method: The authors developed a serious game targeting specific behaviors, collected fine-grained gameplay data, and used machine learning to predict software development suitability based on these features.

Result: The model achieved 97% precision and 94% accuracy, identifying key behavioral traits exhibited by suitable candidates during gameplay.

Conclusion: Game-based behavioral signals offer a scalable and engaging approach for bias-free career assessment, particularly predicting suitability for software development roles.

Abstract: Personality assessment in career guidance and personnel selection traditionally relies on self-report questionnaires, which are susceptible to response bias, fatigue, and intentional distortion. Game-based assessment offers a promising alternative by capturing implicit behavioral signals during gameplay. This study proposes a multi-genre serious-game framework combined with machine-learning techniques to predict suitability for software development roles. Developer-relevant personality and behavioral traits were identified through a systematic literature review and an empirical study of professional software engineers. A custom mobile game was designed to elicit behaviors related to problem solving, planning, adaptability, persistence, time management, and information seeking. Fine-grained gameplay event data were collected and analyzed using a two-phase modeling strategy where suitability was predicted exclusively from gameplay-derived behavioral features. Results show that our model achieved up to 97% precision and 94% accuracy. Behavioral analysis revealed that proper candidates exhibited distinct gameplay patterns, such as more wins in puzzle-based games, more side challenges, navigating menus more frequently, and exhibiting fewer pauses, retries, and surrender actions. These findings demonstrate that implicit behavioral traces captured during gameplay is promising in predicting software-development suitability without explicit personality testing, supporting serious games as a scalable, engaging, and less biased alternative for career assessment.

</details>


### [361] [Practical Geometric and Quantum Kernel Methods for Predicting Skeletal Muscle Outcomes in chronic obstructive pulmonary disease](https://arxiv.org/abs/2601.00921)
*Azadeh Alavi,Hamidreza Khalili,Stanley H. Chan,Fatemeh Kouchmeshki,Ross Vlahos*

Main category: cs.LG

TL;DR: The paper investigates predictive modeling for muscle outcomes in the context of COPD using minimal biomarkers from a preclinical dataset, benchmarking classical, geometric, and quantum models for small sample data.


<details>
  <summary>Details</summary>
Motivation: To address skeletal muscle dysfunction in COPD, which is a significant extra-pulmonary issue tied to inflammation, and to utilize predictive models based on minimally invasive biomarkers for better understanding.

Method: The research uses data from 213 animals under two conditions and employs classical baselines, geometry-aware SPD descriptors, and quantum kernel methods for prediction. Models are tested on muscle weight and quality metrics.

Result: Quantum kernel ridge regression achieved improved accuracy over the baseline (RMSE 4.41 mg and a determination coefficient of 0.605). Geometry-aware methods also showed consistent gains in low-feature settings. A ROC-AUC of up to 0.90 was obtained in screening evaluations.

Conclusion: Using geometric and quantum kernel approaches can significantly improve low-sample, low-feature predictions in biomedical contexts, maintaining interpretability and effective model selection.

Abstract: Skeletal muscle dysfunction is a clinically relevant extra-pulmonary manifestation of chronic obstructive pulmonary disease (COPD) and is closely linked to systemic and airway inflammation. This motivates predictive modelling of muscle outcomes from minimally invasive biomarkers that can be acquired longitudinally. We study a small-sample preclinical dataset comprising 213 animals across two conditions (Sham versus cigarette-smoke exposure), with blood and bronchoalveolar lavage fluid measurements and three continuous targets: tibialis anterior muscle weight (milligram: mg), specific force (millinewton: mN), and a derived muscle quality index (mN per mg). We benchmark tuned classical baselines, geometry-aware symmetric positive definite (SPD) descriptors with Stein divergence, and quantum kernel models designed for low-dimensional tabular data. In the muscle-weight setting, quantum kernel ridge regression using four interpretable inputs (blood C-reactive protein, neutrophil count, bronchoalveolar lavage cellularity, and condition) attains a test root mean squared error of 4.41 mg and coefficient of determination of 0.605, improving over a matched ridge baseline on the same feature set (4.70 mg and 0.553). Geometry-informed Stein-divergence prototype distances yield a smaller but consistent gain in the biomarker-only setting (4.55 mg versus 4.79 mg). Screening-style evaluation, obtained by thresholding the continuous outcome at 0.8 times the training Sham mean, achieves an area under the receiver operating characteristic curve (ROC-AUC) of up to 0.90 for detecting low muscle weight. These results indicate that geometric and quantum kernel lifts can provide measurable benefits in low-data, low-feature biomedical prediction problems, while preserving interpretability and transparent model selection.

</details>


### [362] [Complexity-based code embeddings](https://arxiv.org/abs/2601.00924)
*Rares Folea,Radu Iacob,Emil Slusanschi,Traian Rebedea*

Main category: cs.LG

TL;DR: The paper introduces a method to convert algorithm source codes into numerical embeddings and applies these embeddings using XGBoost for better classification on a dataset of programming competition code.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of representing algorithm source code in a structured numerical format that allows for analysis and classification.

Method: The source code of algorithms is transformed into numerical embeddings by analyzing their behavior under various inputs and applying tailored complexity functions. These embeddings are built using r-Complexity techniques.

Result: The implementation of the proposed embeddings in the XGBoost algorithm achieved an average F1-score on an 11-class multi-label dataset of code snippets from the Codeforces platform.

Conclusion: The proposed method effectively transforms algorithm code into meaningful numerical representations, enhancing the performance of classification tasks on real-world code snippets.

Abstract: This paper presents a generic method for transforming the source code of various algorithms to numerical embeddings, by dynamically analysing the behaviour of computer programs against different inputs and by tailoring multiple generic complexity functions for the analysed metrics. The used algorithms embeddings are based on r-Complexity . Using the proposed code embeddings, we present an implementation of the XGBoost algorithm that achieves an average F1-score on a multi-label dataset with 11 classes, built using real-world code snippets submitted for programming competitions on the Codeforces platform.

</details>


### [363] [Enhanced Data-Driven Product Development via Gradient Based Optimization and Conformalized Monte Carlo Dropout Uncertainty Estimation](https://arxiv.org/abs/2601.00932)
*Andrea Thomas Nava,Lijo Johny,Fabio Azzalini,Johannes Schneider,Arianna Casanova*

Main category: cs.LG

TL;DR: The paper discusses a data-driven framework for product development that optimizes designs using neural networks and uncertainty estimation techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize product designs by leveraging past experimental data and addressing the challenge of simultaneously optimizing multiple correlated properties.

Method: The authors propose using neural networks with Projected Gradient Descent for optimization and integrate Conformalised Monte Carlo Dropout (ConfMC) for uncertainty estimation and coverage guarantees.

Result: Experimental results on five datasets demonstrate that their method achieves state-of-the-art performance with improved prediction intervals and eliminates retraining for adjusting coverage levels.

Conclusion: The proposed approach provides a robust framework for improved product design optimization with reliable uncertainty estimation while maintaining competitive performance.

Abstract: Data-Driven Product Development (DDPD) leverages data to learn the relationship between product design specifications and resulting properties. To discover improved designs, we train a neural network on past experiments and apply Projected Gradient Descent to identify optimal input features that maximize performance. Since many products require simultaneous optimization of multiple correlated properties, our framework employs joint neural networks to capture interdependencies among targets. Furthermore, we integrate uncertainty estimation via \emph{Conformalised Monte Carlo Dropout} (ConfMC), a novel method combining Nested Conformal Prediction with Monte Carlo dropout to provide model-agnostic, finite-sample coverage guarantees under data exchangeability. Extensive experiments on five real-world datasets show that our method matches state-of-the-art performance while offering adaptive, non-uniform prediction intervals and eliminating the need for retraining when adjusting coverage levels.

</details>


### [364] [LOFA: Online Influence Maximization under Full-Bandit Feedback using Lazy Forward Selection](https://arxiv.org/abs/2601.00933)
*Jinyu Xu,Abhishek K. Umrawal*

Main category: cs.LG

TL;DR: This paper addresses the problem of online influence maximization under bandit feedback, proposing the LOFA algorithm, which improves cumulative regret and instantaneous reward.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve methods for online influence maximization, especially under limited observations (bandit feedback), with the goal of maximizing influence on social networks.

Method: The paper introduces the Lazy Online Forward Algorithm (LOFA), leveraging submodularity properties for better performance in influence maximization.

Result: Experiments on a real-world social network show LOFA outperforms existing algorithms in cumulative regret and instantaneous reward.

Conclusion: LOFA is an effective algorithm for influence maximization in online settings, achieving superior performance compared to existing methods.

Abstract: We study the problem of influence maximization (IM) in an online setting, where the goal is to select a subset of nodes$\unicode{x2014}$called the seed set$\unicode{x2014}$at each time step over a fixed time horizon, subject to a cardinality budget constraint, to maximize the expected cumulative influence. We operate under a full-bandit feedback model, where only the influence of the chosen seed set at each time step is observed, with no additional structural information about the network or diffusion process. It is well-established that the influence function is submodular, and existing algorithms exploit this property to achieve low regret. In this work, we leverage this property further and propose the Lazy Online Forward Algorithm (LOFA), which achieves a lower empirical regret. We conduct experiments on a real-world social network to demonstrate that LOFA achieves superior performance compared to existing bandit algorithms in terms of cumulative regret and instantaneous reward.

</details>


### [365] [Reliability Under Randomness: An Empirical Analysis of Sparse and Dense Language Models Across Decoding Temperatures](https://arxiv.org/abs/2601.00942)
*Kabir Grover*

Main category: cs.LG

TL;DR: Sparse Mixture-of-Experts (MoE) models exhibit varied reliability under stochastic decoding, with instruction tuning playing a key role in maintaining stability, more so than sparsity itself.


<details>
  <summary>Details</summary>
Motivation: To address the reliability concerns of sparse Mixture-of-Experts language models under stochastic decoding and analyze stability of their outputs compared to dense architectures.

Method: Evaluation of three representative models (sparse base, sparse instruction-tuned, dense instruction-tuned) under four decoding configurations on deterministic tasks and analyzing 9,360 generations for accuracy, consistency, and other metrics.

Result: Sparse instruction-tuned models showed comparable stability to dense instruction-tuned models across decoding temperatures, while sparse base models displayed systematic stability degradation with increased temperatures.

Conclusion: Instruction tuning, not sparsity, is crucial for robustness of MoE models under decoding randomness, making sparse architectures viable for reliability-critical applications if properly instruction-tuned.

Abstract: The increasing prevalence of sparse Mixture-of-Experts (MoE) architectures in large language models raises important questions regarding their reliability under stochastic decoding. While conditional computation enables substantial gains in computational efficiency, it remains unclear whether the interaction between sparse routing and temperature-based sampling compromises output stability relative to dense architectures. This work investigates whether conditional computation in MoE models amplifies decoding-induced randomness, leading to reduced reliability as temperature increases. We evaluate three representative models: OLMoE-7B (sparse base), Mixtral-8x7B (sparse instruction-tuned), and Qwen2.5-3B (dense instruction-tuned) on deterministic arithmetic reasoning tasks with objectively verifiable answers. Experiments span four decoding configurations, ranging from greedy decoding to T=1.0. Our evaluation encompasses accuracy, format compliance, output consistency across repeated generations, and confidence metrics, totaling 9,360 model generations. Results demonstrate that the sparse instruction-tuned model exhibits stability comparable to the dense instruction-tuned model across all decoding temperatures, while the sparse base model shows systematic degradation as temperature increases. These findings indicate that instruction tuning, rather than architectural sparsity, is the primary determinant of robustness to decoding randomness on deterministic tasks. We discuss the implications of these results for deploying sparse language models in reliability-critical applications, highlighting scenarios in which sparse architectures can be safely adopted without sacrificing output stability.

</details>


### [366] [Adapting Feature Attenuation to NLP](https://arxiv.org/abs/2601.00965)
*Tianshuo Yang,Ryan Rabinowitz,Terrance E. Boult,Jugal Kalita*

Main category: cs.LG

TL;DR: The paper investigates Open-Set Recognition (OSR) for text by adapting a computer vision framework to language models and benchmarking against baselines.


<details>
  <summary>Details</summary>
Motivation: Closed-set transformer classifiers like BERT are brittle when confronted with unseen categories in real-world applications. The research aims to address this via OSR techniques.

Method: The study adapts the COSTARR framework from computer vision to two language models (BERT base and GPT-2) and benchmarks it against methods like Maximum Softmax Probability (MSP), MaxLogit, and free-energy scores.

Result: COSTARR can extend to NLP without retraining but shows no significant advantage over MaxLogit or MSP. Free-energy scoring underperforms in high-class-count scenarios.

Conclusion: Porting vision-centric OSR strategies shows promise for NLP but faces limitations, requiring larger models and more tailored strategies for language tasks.

Abstract: Transformer classifiers such as BERT deliver impressive closed-set accuracy, yet they remain brittle when confronted with inputs from unseen categories--a common scenario for deployed NLP systems. We investigate Open-Set Recognition (OSR) for text by porting the feature attenuation hypothesis from computer vision to transformers and by benchmarking it against state-of-the-art baselines. Concretely, we adapt the COSTARR framework--originally designed for classification in computer vision--to two modest language models (BERT (base) and GPT-2) trained to label 176 arXiv subject areas. Alongside COSTARR, we evaluate Maximum Softmax Probability (MSP), MaxLogit, and the temperature-scaled free-energy score under the OOSA and AUOSCR metrics. Our results show (i) COSTARR extends to NLP without retraining but yields no statistically significant gain over MaxLogit or MSP, and (ii) free-energy lags behind all other scores in this high-class-count setting. The study highlights both the promise and the current limitations of transplanting vision-centric OSR ideas to language models, and points toward the need for larger backbones and task-tailored attenuation strategies.

</details>


### [367] [Learning with Monotone Adversarial Corruptions](https://arxiv.org/abs/2601.02193)
*Kasper Green Larsen,Chirag Pabbaraju,Abhishek Shetty*

Main category: cs.LG

TL;DR: The paper examines how standard ML algorithms depend on data exchangeability and independence using a new monotone adversarial corruption model, revealing vulnerabilities in optimal algorithms while showing robustness in uniform convergence-based methods.


<details>
  <summary>Details</summary>
Motivation: To understand the reliance of ML algorithms on assumptions of data exchangeability and independence and assess their performance under adversarial conditions.

Method: Introducing monotone adversarial corruption model where new 'corrupted' data points, labeled by the ground-truth function, are added to an i.i.d. dataset.

Result: Optimal binary classification algorithms degrade in performance under this corruption model, whereas uniform convergence-based algorithms remain stable.

Conclusion: Certain supposedly optimal ML algorithms are excessively dependent on exchangeability, and alternative methods like uniform convergence-based algorithms are more robust.

Abstract: We study the extent to which standard machine learning algorithms rely on exchangeability and independence of data by introducing a monotone adversarial corruption model. In this model, an adversary, upon looking at a "clean" i.i.d. dataset, inserts additional "corrupted" points of their choice into the dataset. These added points are constrained to be monotone corruptions, in that they get labeled according to the ground-truth target function. Perhaps surprisingly, we demonstrate that in this setting, all known optimal learning algorithms for binary classification can be made to achieve suboptimal expected error on a new independent test point drawn from the same distribution as the clean dataset. On the other hand, we show that uniform convergence-based algorithms do not degrade in their guarantees. Our results showcase how optimal learning algorithms break down in the face of seemingly helpful monotone corruptions, exposing their overreliance on exchangeability.

</details>


### [368] [Explainability-Guided Defense: Attribution-Aware Model Refinement Against Adversarial Data Attacks](https://arxiv.org/abs/2601.00968)
*Longwei Wang,Mohammad Navid Nayyem,Abdullah Al Rakin,KC Santosh,Chaowei Zhang,Yang Zhou*

Main category: cs.LG

TL;DR: The paper explores a link between model interpretability and robustness, proposing a framework to enhance robustness using interpretability insights, showing validated improvements.


<details>
  <summary>Details</summary>
Motivation: Deep learning models in critical domains like healthcare need resilience against adversarial attacks and require transparency in decision-making.

Method: The paper uses a framework that employs Local Interpretable Model-Agnostic Explanations (LIME) to identify and suppress spurious features during training through masking, sensitivity-aware regularization, and adversarial augmentation.

Result: Empirical results on datasets such as CIFAR-10 and CIFAR-100 show improved adversarial robustness and better generalization to out-of-distribution data.

Conclusion: By integrating interpretability insights into the training process, the proposed framework enhances the robustness of deep learning models without requiring extra resources or architecture changes.

Abstract: The growing reliance on deep learning models in safety-critical domains such as healthcare and autonomous navigation underscores the need for defenses that are both robust to adversarial perturbations and transparent in their decision-making. In this paper, we identify a connection between interpretability and robustness that can be directly leveraged during training. Specifically, we observe that spurious, unstable, or semantically irrelevant features identified through Local Interpretable Model-Agnostic Explanations (LIME) contribute disproportionately to adversarial vulnerability. Building on this insight, we introduce an attribution-guided refinement framework that transforms LIME from a passive diagnostic into an active training signal. Our method systematically suppresses spurious features using feature masking, sensitivity-aware regularization, and adversarial augmentation in a closed-loop refinement pipeline. This approach does not require additional datasets or model architectures and integrates seamlessly into standard adversarial training. Theoretically, we derive an attribution-aware lower bound on adversarial distortion that formalizes the link between explanation alignment and robustness. Empirical evaluations on CIFAR-10, CIFAR-10-C, and CIFAR-100 demonstrate substantial improvements in adversarial robustness and out-of-distribution generalization.

</details>


### [369] [Contractive Diffusion Policies: Robust Action Diffusion via Contractive Score-Based Sampling with Differential Equations](https://arxiv.org/abs/2601.01003)
*Amin Abyaneh,Charlotte Morissette,Mohamad H. Danesh,Anas El Houssaini,David Meger,Gregory Dudek,Hsiu-Chin Lin*

Main category: cs.LG

TL;DR: The paper introduces Contractive Diffusion Policies (CDPs) to address inaccuracies in diffusion policies for offline policy learning, achieving better performance, especially under data scarcity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the issues in existing diffusion policies, such as solver and score-matching errors, large data requirements, and inconsistent action generation, which become problematic in continuous control settings.

Method: The method introduces CDPs to induce contractive behavior in the diffusion sampling process. This makes the model robust to errors and reduces action variance. They also provide theoretical analysis and practical implementation strategies.

Result: CDPs were evaluated in simulation and real-world environments and consistently outperformed baseline policies, particularly in data-scarce scenarios.

Conclusion: CDPs enhance the robustness and performance of diffusion policies, making them suitable for offline policy learning in various conditions, including data-limited environments.

Abstract: Diffusion policies have emerged as powerful generative models for offline policy learning, whose sampling process can be rigorously characterized by a score function guiding a Stochastic Differential Equation (SDE). However, the same score-based SDE modeling that grants diffusion policies the flexibility to learn diverse behavior also incurs solver and score-matching errors, large data requirements, and inconsistencies in action generation. While less critical in image generation, these inaccuracies compound and lead to failure in continuous control settings. We introduce Contractive Diffusion Policies (CDPs) to induce contractive behavior in the diffusion sampling dynamics. Contraction pulls nearby flows closer to enhance robustness against solver and score-matching errors while reducing unwanted action variance. We develop an in-depth theoretical analysis along with a practical implementation recipe to incorporate CDPs into existing diffusion policy architectures with minimal modification and computational cost. We evaluate CDPs for offline learning by conducting extensive experiments in simulation and real-world settings. Across benchmarks, CDPs often outperform baseline policies, with pronounced benefits under data scarcity.

</details>


### [370] [Zero-shot Forecasting by Simulation Alone](https://arxiv.org/abs/2601.00970)
*Boris N. Oreshkin,Mayank Jauhari,Ravi Kiran Selvam,Malcolm Wolff,Wenhao Pan,Shankar Ramasubramanian,Kin G. Olivares,Tatiana Konstantinova,Andres Potapczynski,Mengfei Cao,Dmitry Efimov,Michael W. Mahoney,Andrew G. Wilson*

Main category: cs.LG

TL;DR: Zero-shot time-series forecasting is enhanced with SarSim0, a simulation pipeline for univariate time series utilizing SARIMA models, enabling superior zero-shot forecasting on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The field faces challenges such as limited data, biased corpora, evaluation issues, and privacy constraints that hinder performance and scalability.

Method: SarSim0 employs a SARIMA-based simulation approach improved through stability region sampling, multi-seasonality superposition, and heavy-tailed noise modeling.

Result: SarSim0 generates 1B unique simulated series on-the-fly, improving zero-shot forecasting performance, surpassing statistical models and neural baselines, especially on M-Series and GiftEval benchmarks.

Conclusion: SarSim0 provides a practical and high-speed solution for simulating univariate time series, enabling strong zero-shot forecasting under strict protocols, marking significant progress in forecasting approaches.

Abstract: Zero-shot time-series forecasting holds great promise, but is still in its infancy, hindered by limited and biased data corpora, leakage-prone evaluation, and privacy and licensing constraints. Motivated by these challenges, we propose the first practical univariate time series simulation pipeline which is simultaneously fast enough for on-the-fly data generation and enables notable zero-shot forecasting performance on M-Series and GiftEval benchmarks that capture trend/seasonality/intermittency patterns, typical of industrial forecasting applications across a variety of domains. Our simulator, which we call SarSim0 (SARIMA Simulator for Zero-Shot Forecasting), is based off of a seasonal autoregressive integrated moving average (SARIMA) model as its core data source. Due to instability in the autoregressive component, naive SARIMA simulation often leads to unusable paths. Instead, we follow a three-step procedure: (1) we sample well-behaved trajectories from its characteristic polynomial stability region; (2) we introduce a superposition scheme that combines multiple paths into rich multi-seasonality traces; and (3) we add rate-based heavy-tailed noise models to capture burstiness and intermittency alongside seasonalities and trends. SarSim0 is orders of magnitude faster than kernel-based generators, and it enables training on circa 1B unique purely simulated series, generated on the fly; after which well-established neural network backbones exhibit strong zero-shot generalization, surpassing strong statistical forecasters and recent foundation baselines, while operating under strict zero-shot protocol. Notably, on GiftEval we observe a "student-beats-teacher" effect: models trained on our simulations exceed the forecasting accuracy of the AutoARIMA generating processes.

</details>


### [371] [Data-Driven Assessment of Concrete Mixture Compositions on Chloride Transport via Standalone Machine Learning Algorithms](https://arxiv.org/abs/2601.01009)
*Mojtaba Aliasghar-Mamaghani,Mohammadreza Khalafi*

Main category: cs.LG

TL;DR: This paper leverages machine learning to study how concrete mix compositions impact chloride concentration over time in infrastructure, identifying trends to improve durability.


<details>
  <summary>Details</summary>
Motivation: Understanding chloride ingress in concrete is essential for improving the lifespan of infrastructure in harsh environments.

Method: Several ML algorithms, both simple (LR, KNN, KRR) and complex (SVR, GPR, MLP, GRU), were applied to a comprehensive dataset to predict chloride concentration trends.

Result: Performance varied across algorithms, with KRR, GPR, and MLP achieving high accuracy, while GRU struggled due to data diversity. Most mixture components showed inverse correlations with chloride content.

Conclusion: ML models effectively reveal relationships between concrete compositions and chloride ingress, aiding the design of durable infrastructure.

Abstract: This paper employs a data-driven approach to determine the impact of concrete mixture compositions on the temporal evolution of chloride in concrete structures. This is critical for assessing the service life of civil infrastructure subjected to aggressive environments. The adopted methodology relies on several simple and complex standalone machine learning (ML) algorithms, with the primary objective of establishing confidence in the unbiased prediction of the underlying hidden correlations. The simple algorithms include linear regression (LR), k-nearest neighbors (KNN) regression, and kernel ridge regression (KRR). The complex algorithms entail support vector regression (SVR), Gaussian process regression (GPR), and two families of artificial neural networks, including a feedforward network (multilayer perceptron, MLP) and a gated recurrent unit (GRU). The MLP architecture cannot explicitly handle sequential data, a limitation addressed by the GRU. A comprehensive dataset is considered. The performance of ML algorithms is evaluated, with KRR, GPR, and MLP exhibiting high accuracy. Given the diversity of the adopted concrete mixture proportions, the GRU was unable to accurately reproduce the response in the test set. Further analyses elucidate the contributions of mixture compositions to the temporal evolution of chloride. The results obtained from the GPR model unravel latent correlations through clear and explainable trends. The MLP, SVR, and KRR also provide acceptable estimates of the overall trends. The majority of mixture components exhibit an inverse relation with chloride content, while a few components demonstrate a direct correlation. These findings highlight the potential of surrogate approaches for describing the physical processes involved in chloride ingress and the associated correlations, toward the ultimate goal of enhancing the service life of civil infrastructure.

</details>


### [372] [Geometric and Dynamic Scaling in Deep Transformers](https://arxiv.org/abs/2601.01014)
*Haoran Su,Chenyu You*

Main category: cs.LG

TL;DR: Deep Transformers suffer from rank collapse due to uncontrolled feature accumulation and systematic drift off the semantic manifold. This paper introduces the Manifold-Geometric Transformer (MGT), a framework employing manifold-constrained hyper-connections and deep delta learning to address these issues.


<details>
  <summary>Details</summary>
Motivation: To solve the paradoxical problem of rank collapse in deep Transformer architectures, which compromises representation quality even with state-of-the-art normalization and initialization methods.

Method: Introduces a geometric framework based on two principles: manifold-constrained hyper-connections and deep delta learning. These principles prevent uncontrolled feature drift and allow dynamic erasure of redundant features.

Result: The proposed methods yield stable geometric evolution across ultra-deep networks, overcoming rank collapse and enabling effective representation learning in Transformers with depths exceeding 100 layers.

Conclusion: Geometry-based constraints and guided feature evolution are crucial for addressing representational degeneracy in deep Transformer architectures, rather than simply addressing depth-related issues.

Abstract: Despite their empirical success, pushing Transformer architectures to extreme depth often leads to a paradoxical failure: representations become increasingly redundant, lose rank, and ultimately collapse. Existing explanations largely attribute this phenomenon to optimization instability or vanishing gradients, yet such accounts fail to explain why collapse persists even under modern normalization and initialization schemes. In this paper, we argue that the collapse of deep Transformers is fundamentally a geometric problem. Standard residual updates implicitly assume that feature accumulation is always beneficial, but offer no mechanism to constrain update directions or to erase outdated information. As depth increases, this leads to systematic drift off the semantic manifold and monotonic feature accumulation, causing representational degeneracy. We propose a unified geometric framework that addresses these failures through two orthogonal principles. First, manifold-constrained hyper-connections restrict residual updates to valid local tangent directions, preventing uncontrolled manifold drift. Second, deep delta learning introduces data-dependent, non-monotonic updates that enable reflection and erasure of redundant features rather than their unconditional accumulation. Together, these mechanisms decouple the direction and sign of feature updates, yielding a stable geometric evolution across depth. We term the resulting architecture the Manifold-Geometric Transformer (MGT). Our analysis predicts that enforcing geometric validity while allowing dynamic erasure is essential for avoiding rank collapse in ultra-deep networks. We outline an evaluation protocol for Transformers exceeding 100 layers to test the hypothesis that geometry, rather than depth itself, is the key limiting factor in deep representation learning.

</details>


### [373] [Improving Variational Autoencoder using Random Fourier Transformation: An Aviation Safety Anomaly Detection Case-Study](https://arxiv.org/abs/2601.01016)
*Ata Akbari Asanjan,Milad Memarzadeh,Bryan Matthews,Nikunj Oza*

Main category: cs.LG

TL;DR: This study introduces Random Fourier Transformation (RFT) in Autoencoders and Variational Autoencoders to improve training and anomaly detection, demonstrating the superiority of RFT models but finding inconclusive results for trainable RFT.


<details>
  <summary>Details</summary>
Motivation: To enhance deep neural network training and inference, specifically for Autoencoders and creating improved anomaly detection methods.

Method: The study applies Random Fourier Transformation (RFT) and its trainable variant to deep neural networks. It uses Frequency Principle (F-Principle) to analyze training behavior and evaluates models with synthetic and real-world datasets.

Result: The results show RFT models learn features more effectively than conventional DNNs in low and high frequencies. Models with RFT outperform conventional ones in anomaly detection but benefits of trainable RFT over random RFT remain unclear.

Conclusion: RFT enhances model performance and anomaly detection accuracy, and further exploration is needed about whether trainable Fourier transformation offers more advantages over its random counterpart.

Abstract: In this study, we focus on the training process and inference improvements of deep neural networks (DNNs), specifically Autoencoders (AEs) and Variational Autoencoders (VAEs), using Random Fourier Transformation (RFT). We further explore the role of RFT in model training behavior using Frequency Principle (F-Principle) analysis and show that models with RFT turn to learn low frequency and high frequency at the same time, whereas conventional DNNs start from low frequency and gradually learn (if successful) high-frequency features. We focus on reconstruction-based anomaly detection using autoencoder and variational autoencoder and investigate the RFT's role. We also introduced a trainable variant of RFT that uses the existing computation graph to train the expansion of RFT instead of it being random. We showcase our findings with two low-dimensional synthetic datasets for data representation, and an aviation safety dataset, called Dashlink, for high-dimensional reconstruction-based anomaly detection. The results indicate the superiority of models with Fourier transformation compared to the conventional counterpart and remain inconclusive regarding the benefits of using trainable Fourier transformation in contrast to the Random variant.

</details>


### [374] [Expanding the Chaos: Neural Operator for Stochastic (Partial) Differential Equations](https://arxiv.org/abs/2601.01021)
*Dai Shi,Lequan Lin,Andi Han,Luke Thompson,José Miguel Hernández-Lobato,Zhiyong Wang,Junbin Gao*

Main category: cs.LG

TL;DR: This paper proposes WCE-based neural operator architectures for solving SPDEs and SDEs efficiently, with applications spanning diverse fields.


<details>
  <summary>Details</summary>
Motivation: To create fast, practical solvers for SPDEs and SDEs using deep learning, leveraging Wiener chaos expansions to develop scalable and effective solution operators.

Method: The methodology employs Wiener chaos expansion (WCE) to project noise paths onto Wick Hermite features, and parameterizes deterministic chaos coefficients with neural operators, achieving full solution trajectory reconstruction in a single forward pass.

Result: The proposed models were tested on SPDE benchmarks and various real-world problems, demonstrating competitive accuracy and broad applicability across fields like image diffusion, financial modeling, and flood prediction.

Conclusion: WCE-based neural operators are shown to be practical, scalable, and widely applicable tools for learning the solution operators of SDE/SPDE systems in natural sciences and machine learning.

Abstract: Stochastic differential equations (SDEs) and stochastic partial differential equations (SPDEs) are fundamental tools for modeling stochastic dynamics across the natural sciences and modern machine learning. Developing deep learning models for approximating their solution operators promises not only fast, practical solvers, but may also inspire models that resolve classical learning tasks from a new perspective. In this work, we build on classical Wiener chaos expansions (WCE) to design neural operator (NO) architectures for SPDEs and SDEs: we project the driving noise paths onto orthonormal Wick Hermite features and parameterize the resulting deterministic chaos coefficients with neural operators, so that full solution trajectories can be reconstructed from noise in a single forward pass. On the theoretical side, we investigate the classical WCE results for the class of multi-dimensional SDEs and semilinear SPDEs considered here by explicitly writing down the associated coupled ODE/PDE systems for their chaos coefficients, which makes the separation between stochastic forcing and deterministic dynamics fully explicit and directly motivates our model designs. On the empirical side, we validate our models on a diverse suite of problems: classical SPDE benchmarks, diffusion one-step sampling on images, topological interpolation on graphs, financial extrapolation, parameter estimation, and manifold SDEs for flood prediction, demonstrating competitive accuracy and broad applicability. Overall, our results indicate that WCE-based neural operators provide a practical and scalable way to learn SDE/SPDE solution operators across diverse domains.

</details>


### [375] [Wireless Dataset Similarity: Measuring Distances in Supervised and Unsupervised Machine Learning](https://arxiv.org/abs/2601.01023)
*João Morais,Sadjad Alikhani,Akshay Malhotra,Shahab Hamidi-Rad,Ahmed Alkhateeb*

Main category: cs.LG

TL;DR: This paper proposes a framework for measuring similarity between wireless datasets, focusing on predicting how well a model trained on one dataset performs on another.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of comparing wireless datasets effectively for tasks like dataset selection, augmentation, and cross-dataset model transfer.

Method: The authors introduce task- and model-aware metrics utilizing UMAP embeddings, Wasserstein, and Euclidean distances. They evaluate these metrics by how well they predict cross-dataset transferability in supervised and unsupervised tasks.

Result: The proposed metrics show strong correlation (Pearson > 0.85) with model transferability. They outperform traditional benchmarks across tasks, supporting their effectiveness for task-relevant dataset comparisons.

Conclusion: The framework achieves better task-relevant dataset comparisons, aiding applications like transfer learning and synthetic data generation in wireless communication research.

Abstract: This paper introduces a task- and model-aware framework for measuring similarity between wireless datasets, enabling applications such as dataset selection/augmentation, simulation-to-real (sim2real) comparison, task-specific synthetic data generation, and informing decisions on model training/adaptation to new deployments. We evaluate candidate dataset distance metrics by how well they predict cross-dataset transferability: if two datasets have a small distance, a model trained on one should perform well on the other. We apply the framework on an unsupervised task, channel state information (CSI) compression, using autoencoders. Using metrics based on UMAP embeddings, combined with Wasserstein and Euclidean distances, we achieve Pearson correlations exceeding 0.85 between dataset distances and train-on-one/test-on-another task performance. We also apply the framework to a supervised beam prediction in the downlink using convolutional neural networks. For this task, we derive a label-aware distance by integrating supervised UMAP and penalties for dataset imbalance. Across both tasks, the resulting distances outperform traditional baselines and consistently exhibit stronger correlations with model transferability, supporting task-relevant comparisons between wireless datasets.

</details>


### [376] [Coarse-Grained Kullback--Leibler Control of Diffusion-Based Generative AI](https://arxiv.org/abs/2601.01045)
*Tatsuaki Tsuruyama*

Main category: cs.LG

TL;DR: This paper proposes a novel reverse diffusion scheme for generative models, incorporating an information-theoretic potential to maintain coarse-grained quantities and ensure better control over the evolution of image structures.


<details>
  <summary>Details</summary>
Motivation: There is a lack of theoretical understanding about how coarse-grained image features evolve during the reverse diffusion process in generative models.

Method: The author extends an information-theoretic Lyapunov function to reverse diffusion in generative models and introduces the V-delta projected reverse diffusion scheme. This method modifies the reverse diffusion dynamics to preserve coarse-grained quantities using an approximate Lyapunov function.

Result: Numerical experiments on block-constant images demonstrate that the proposed method maintains block-mass errors and potential within a prescribed tolerance while achieving comparable pixel accuracy and visual quality.

Conclusion: The study offers a novel interpretation of generative sampling as reducing an information potential and provides a framework for designing reverse diffusion processes that explicitly control coarse-grained image features.

Abstract: Diffusion models and score-based generative models provide a powerful framework for synthesizing high-quality images from noise. However, there is still no satisfactory theory that describes how coarse-grained quantities, such as blockwise intensity or class proportions after partitioning an image into spatial blocks, are preserved and evolve along the reverse diffusion dynamics. In previous work, the author introduced an information-theoretic Lyapunov function V for non-ergodic Markov processes on a state space partitioned into blocks, defined as the minimal Kullback-Leibler divergence to the set of stationary distributions reachable from a given initial condition, and showed that a leak-tolerant potential V-delta with a prescribed tolerance for block masses admits a closed-form expression as a scaling-and-clipping operation on block masses.
  In this paper, I transplant this framework to the reverse diffusion process in generative models and propose a reverse diffusion scheme that is projected by the potential V-delta (referred to as the V-delta projected reverse diffusion). I extend the monotonicity of V to time-inhomogeneous block-preserving Markov kernels and show that, under small leakage and the V-delta projection, V-delta acts as an approximate Lyapunov function. Furthermore, using a toy model consisting of block-constant images and a simplified reverse kernel, I numerically demonstrate that the proposed method keeps the block-mass error and the leak-tolerant potential within the prescribed tolerance, while achieving pixel-wise accuracy and visual quality comparable to the non-projected dynamics. This study reinterprets generative sampling as a decrease of an information potential from noise to data, and provides a design principle for reverse diffusion processes with explicit control of coarse-grained quantities.

</details>


### [377] [A UCB Bandit Algorithm for General ML-Based Estimators](https://arxiv.org/abs/2601.01061)
*Yajing Liu,Erkao Bao,Linqi Song*

Main category: cs.LG

TL;DR: The paper introduces ML-UCB, an algorithm that integrates machine learning models into multi-armed bandit frameworks by modeling learning curve behavior and ensuring sublinear regret.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of lack of concentration inequalities for sophisticated ML models in sequential decision-making, enabling better integration of ML into bandit problems.

Method: ML-UCB models the learning curve of ML models using a power law assumption on Mean Squared Error decline and derives generalized concentration inequalities, ensuring principled exploration.

Result: The authors show ML-UCB achieves sublinear regret and demonstrate its effectiveness with experiments in a collaborative filtering system, outperforming LinUCB.

Conclusion: ML-UCB facilitates the integration of arbitrary ML models into bandit frameworks without requiring specific theoretical analysis, presenting a flexible, efficient approach for sequential decision-making.

Abstract: We present ML-UCB, a generalized upper confidence bound algorithm that integrates arbitrary machine learning models into multi-armed bandit frameworks. A fundamental challenge in deploying sophisticated ML models for sequential decision-making is the lack of tractable concentration inequalities required for principled exploration. We overcome this limitation by directly modeling the learning curve behavior of the underlying estimator. Specifically, assuming the Mean Squared Error decreases as a power law in the number of training samples, we derive a generalized concentration inequality and prove that ML-UCB achieves sublinear regret. This framework enables the principled integration of any ML model whose learning curve can be empirically characterized, eliminating the need for model-specific theoretical analysis. We validate our approach through experiments on a collaborative filtering recommendation system using online matrix factorization with synthetic data designed to simulate a simplified two-tower model, demonstrating substantial improvements over LinUCB

</details>


### [378] [SPoRC-VIST: A Benchmark for Evaluating Generative Natural Narrative in Vision-Language Models](https://arxiv.org/abs/2601.01062)
*Yunlin Zeng*

Main category: cs.LG

TL;DR: The paper introduces a pipeline for generating multi-speaker podcast dialogues based on visual input, fine-tuning a 32B VLM model using synthetic-to-real training and proposing new evaluation metrics emphasizing conversational naturalness.


<details>
  <summary>Details</summary>
Motivation: Exploring how Vision-Language Models can transition from descriptive tasks like image captioning to generating engaging, multi-speaker podcast narratives.

Method: Fine-tuning the Qwen3-VL-32B model using synthetic-to-real training with datasets such as SPoRC for dialogues and VIST for real-world imagery, incorporating tailored metrics and AI-based judges for evaluation.

Result: The fine-tuned model surpasses a larger base model in conversational quality and narrative depth, while maintaining strong visual grounding performance.

Conclusion: The findings highlight the potential of smaller, fine-tuned models for generating high-quality visual podcast dialogues, and call attention to the need for nuanced evaluation frameworks in creative AI tasks.

Abstract: Vision-Language Models (VLMs) have achieved remarkable success in descriptive tasks such as image captioning and visual question answering (VQA). However, their ability to generate engaging, long-form narratives -- specifically multi-speaker podcast dialogues -- remains under-explored and difficult to evaluate. Standard metrics like BLEU and ROUGE fail to capture the nuances of conversational naturalness, personality, and narrative flow, often rewarding safe, repetitive outputs over engaging storytelling. In this work, we present a novel pipeline for end-to-end visual podcast generation, and fine-tune a Qwen3-VL-32B model on a curated dataset of 4,000 image-dialogue pairs. Crucially, we use a synthetic-to-real training strategy: we train on high-quality podcast dialogues from the Structured Podcast Research Corpus (SPoRC) paired with synthetically generated imagery, and evaluate on real-world photo sequences from the Visual Storytelling Dataset (VIST). This rigorous setup tests the model's ability to generalize from synthetic training data to real-world visual domains. We propose a comprehensive evaluation framework that moves beyond textual overlap, and use AI-as-a-judge (Gemini 3 Pro, Claude Opus 4.5, GPT 5.2) and novel style metrics (average turn length, speaker switch rate) to assess quality. Our experiments demonstrate that our fine-tuned 32B model significantly outperforms a 235B base model in conversational naturalness ($>$80\% win rate) and narrative depth (+50\% turn length), while maintaining identical visual grounding capabilities (CLIPScore: 20.39).

</details>


### [379] [Tiny Machine Learning for Real-Time Aquaculture Monitoring: A Case Study in Morocco](https://arxiv.org/abs/2601.01065)
*Achraf Hsain,Yahya Zaki,Othman Abaakil,Hibat-allah Bekkar,Yousra Chtouki*

Main category: cs.LG

TL;DR: The paper presents a Tiny Machine Learning (TinyML)-based technology integrated with low-power edge devices for improving aquaculture monitoring and control.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in aquaculture like water quality management, disease outbreaks, and labor-intensive monitoring through automated systems.

Method: The paper proposes implementing TinyML-powered low-energy edge devices for real-time data collection and anomaly detection in aquaculture environments, focusing on water quality, feed management, and optimization.

Result: The system successfully provides real-time monitoring of parameters such as pH, temperature, dissolved oxygen, and ammonia levels, facilitating efficient farm management and cost reduction.

Conclusion: The integration of TinyML and edge devices in aquaculture can lead to more sustainable, cost-effective, and resource-efficient farming practices.

Abstract: Aquaculture, the farming of aquatic organisms, is a rapidly growing industry facing challenges such as water quality fluctuations, disease outbreaks, and inefficient feed management. Traditional monitoring methods often rely on manual labor and are time consuming, leading to potential delays in addressing issues. This paper proposes the integration of low-power edge devices using Tiny Machine Learning (TinyML) into aquaculture systems to enable real-time automated monitoring and control, such as collecting data and triggering alarms, and reducing labor requirements. The system provides real-time data on the required parameters such as pH levels, temperature, dissolved oxygen, and ammonia levels to control water quality, nutrient levels, and environmental conditions enabling better maintenance, efficient resource utilization, and optimal management of the enclosed aquaculture space. The system enables alerts in case of anomaly detection. The data collected by the sensors over time can serve for important decision-making regarding optimizing water treatment processes, feed distribution, feed pattern analysis and improve feed efficiency, reducing operational costs. This research explores the feasibility of developing TinyML-based solutions for aquaculture monitoring, considering factors such as sensor selection, algorithm design, hardware constraints, and ethical considerations. By demonstrating the potential benefits of TinyML in aquaculture, our aim is to contribute to the development of more sustainable and efficient farming practices.

</details>


### [380] [Flow Equivariant World Models: Memory for Partially Observed Dynamic Environments](https://arxiv.org/abs/2601.01075)
*Hansen Jin Lillemark,Benhao Huang,Fangneng Zhan,Yilun Du,Thomas Anderson Keller*

Main category: cs.LG

TL;DR: The paper introduces Flow Equivariant World Models (FEWM), leveraging Lie group flows for unified self-motion and external object motion representation to improve embodied intelligence in long-term world modeling tasks.


<details>
  <summary>Details</summary>
Motivation: Most neural network world models inefficiently re-learn transformations from data without leveraging underlying symmetries of sensory streams. The motivation is to address this inefficiency in structured representation.

Method: The authors introduce Flow Equivariant World Models, using group equivariance paired with one-parameter Lie group transformations to unify self-motion and external motion for robust latent representations over hundreds of timesteps.

Result: FEWM significantly outperforms state-of-the-art methods, especially in long rollouts and scenarios with predictable world dynamics outside the agent's field of view, on 2D and 3D video benchmarks.

Conclusion: Flow equivariance aids in structuring world model representations, enabling scalable, data-efficient embodied intelligence with symmetry-guided approaches.

Abstract: Embodied systems experience the world as 'a symphony of flows': a combination of many continuous streams of sensory input coupled to self-motion, interwoven with the dynamics of external objects. These streams obey smooth, time-parameterized symmetries, which combine through a precisely structured algebra; yet most neural network world models ignore this structure and instead repeatedly re-learn the same transformations from data. In this work, we introduce 'Flow Equivariant World Models', a framework in which both self-motion and external object motion are unified as one-parameter Lie group 'flows'. We leverage this unification to implement group equivariance with respect to these transformations, thereby providing a stable latent world representation over hundreds of timesteps. On both 2D and 3D partially observed video world modeling benchmarks, we demonstrate that Flow Equivariant World Models significantly outperform comparable state-of-the-art diffusion-based and memory-augmented world modeling architectures -- particularly when there are predictable world dynamics outside the agent's current field of view. We show that flow equivariance is particularly beneficial for long rollouts, generalizing far beyond the training horizon. By structuring world model representations with respect to internal and external motion, flow equivariance charts a scalable route to data efficient, symmetry-guided, embodied intelligence. Project link: https://flowequivariantworldmodels.github.io.

</details>


### [381] [Central Dogma Transformer: Towards Mechanism-Oriented AI for Cellular Understanding](https://arxiv.org/abs/2601.01089)
*Nobuyuki Ota*

Main category: cs.LG

TL;DR: The paper introduces the Central Dogma Transformer (CDT) that integrates DNA, RNA, and protein modeling using directional cross-attention mechanisms for deeper understanding of cellular processes. Validation demonstrates accurate predictions on enhancer perturbation data.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of isolated models for DNA, RNA, and protein and enable integrated modeling of cellular processes following the Central Dogma.

Method: CDT employs pre-trained language models for DNA, RNA, and protein, integrating them using directional cross-attention mechanisms. The system models transcriptional (DNA-to-RNA) and translational (RNA-to-protein) relations to generate a unified Virtual Cell Embedding.

Result: The proof-of-concept CDT v1 achieves a Pearson correlation of 0.503 on CRISPRi enhancer perturbation data, representing 63% of the theoretical ceiling. Attention and gradient analyses highlight regions of genomic interest, such as CTCF binding sites linked to enhancer activity.

Conclusion: AI architectures aligned with biological concepts like the Central Dogma can deliver both predictive accuracy and insights into mechanisms, paving the way for better understanding of cellular processes.

Abstract: Understanding cellular mechanisms requires integrating information across DNA, RNA, and protein - the three molecular systems linked by the Central Dogma of molecular biology. While domain-specific foundation models have achieved success for each modality individually, they remain isolated, limiting our ability to model integrated cellular processes. Here we present the Central Dogma Transformer (CDT), an architecture that integrates pre-trained language models for DNA, RNA, and protein following the directional logic of the Central Dogma. CDT employs directional cross-attention mechanisms - DNA-to-RNA attention models transcriptional regulation, while RNA-to-Protein attention models translational relationships - producing a unified Virtual Cell Embedding that integrates all three modalities. We validate CDT v1 - a proof-of-concept implementation using fixed (non-cell-specific) RNA and protein embeddings - on CRISPRi enhancer perturbation data from K562 cells, achieving a Pearson correlation of 0.503, representing 63% of the theoretical ceiling set by cross-experiment variability (r = 0.797). Attention and gradient analyses provide complementary interpretive windows: in detailed case studies, these approaches highlight largely distinct genomic regions, with gradient analysis identifying a CTCF binding site that Hi-C data showed as physically contacting both enhancer and target gene. These results suggest that AI architectures aligned with biological information flow can achieve both predictive accuracy and mechanistic interpretability.

</details>


### [382] [Community-Based Early-Stage Chronic Kidney Disease Screening using Explainable Machine Learning for Low-Resource Settings](https://arxiv.org/abs/2601.01119)
*Muhammad Ashad Kabir,Sirajam Munira,Dewan Tasnia Azad,Saleh Mohammed Ikram,Mohammad Habibur Rahman Sarker,Syed Manzoor Ahmed Hanifi*

Main category: cs.LG

TL;DR: The study introduces an explainable ML model for early CKD detection in resource-constrained South Asian contexts, achieving high accuracy with minimal, accessible data inputs.


<details>
  <summary>Details</summary>
Motivation: Existing CKD screening tools underperform in South Asia due to reliance on populations from high-income countries, simplistic scoring methods, and focus on late-stage CKD.

Method: The study used a community-based CKD dataset from Bangladesh and explored 12 ML classifiers with feature selection techniques. Model performance was validated using cross-validation and external datasets.

Result: An optimized ML model achieved balanced accuracy up to 90.40% using specific feature subsets, validated externally with 78%-98% sensitivity, and demonstrated superior performance compared to existing tools.

Conclusion: The proposed models are effective, generalizable, and require fewer inputs, making them ideal for low-resource settings. SHAP enabled explanations aligned with CKD risk factors, enhancing clinical interpretability.

Abstract: Early detection of chronic kidney disease (CKD) is essential for preventing progression to end-stage renal disease. However, existing screening tools - primarily developed using populations from high-income countries - often underperform in Bangladesh and South Asia, where risk profiles differ. Most of these tools rely on simple additive scoring functions and are based on data from patients with advanced-stage CKD. Consequently, they fail to capture complex interactions among risk factors and are limited in predicting early-stage CKD. Our objective was to develop and evaluate an explainable machine learning (ML) framework for community-based early-stage CKD screening for low-resource settings, tailored to the Bangladeshi and South Asian population context. We used a community-based dataset from Bangladesh, the first such CKD dataset in South and South Asia, and evaluated twelve ML classifiers across multiple feature domains. Ten complementary feature selection techniques were applied to identify robust, generalizable predictors. The final models were assessed using 10-fold cross-validation. External validation was conducted on three independent datasets from India, the UAE, and Bangladesh. SHAP (SHapley Additive exPlanations) was used to provide model explainability. An ML model trained on an RFECV-selected feature subset achieved a balanced accuracy of 90.40%, whereas minimal non-pathology-test features demonstrated excellent predictive capability with a balanced accuracy of 89.23%, often outperforming larger or full feature sets. Compared with existing screening tools, the proposed models achieved substantially higher accuracy and sensitivity while requiring fewer and more accessible inputs. External validation confirmed strong generalizability with 78% to 98% sensitivity. SHAP interpretation identified clinically meaningful predictors consistent with established CKD risk factors.

</details>


### [383] [Learning from Historical Activations in Graph Neural Networks](https://arxiv.org/abs/2601.01123)
*Yaniv Galron,Hadar Sinai,Haggai Maron,Moshe Eliasof*

Main category: cs.LG

TL;DR: This paper introduces HISTOGRAPH, an attention-based pooling method for Graph Neural Networks (GNNs) that leverages historical activations across layers to address challenges like over-smoothing in deep architectures.


<details>
  <summary>Details</summary>
Motivation: Existing GNN pooling methods often rely exclusively on the final-layer features, ignoring intermediate activations that contain valuable historical information. This limitation is particularly problematic in deep GNNs due to representational shifts and over-smoothing.

Method: The proposed HISTOGRAPH method incorporates a two-stage attention mechanism: first, layer-wise attention models historical graph activations across layers, followed by node-wise attention to refine the features for final prediction.

Result: HISTOGRAPH demonstrates strong and consistent performance across several graph classification benchmarks, showing robustness in deep GNN architectures and improving upon traditional techniques.

Conclusion: By effectively utilizing activation history and graph structures, HISTOGRAPH enhances the pooling process for GNNs, addressing key limitations of previous methods and offering improved performance on graph-related tasks.

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable success in various domains such as social networks, molecular chemistry, and more. A crucial component of GNNs is the pooling procedure, in which the node features calculated by the model are combined to form an informative final descriptor to be used for the downstream task. However, previous graph pooling schemes rely on the last GNN layer features as an input to the pooling or classifier layers, potentially under-utilizing important activations of previous layers produced during the forward pass of the model, which we regard as historical graph activations. This gap is particularly pronounced in cases where a node's representation can shift significantly over the course of many graph neural layers, and worsened by graph-specific challenges such as over-smoothing in deep architectures. To bridge this gap, we introduce HISTOGRAPH, a novel two-stage attention-based final aggregation layer that first applies a unified layer-wise attention over intermediate activations, followed by node-wise attention. By modeling the evolution of node representations across layers, our HISTOGRAPH leverages both the activation history of nodes and the graph structure to refine features used for final prediction. Empirical results on multiple graph classification benchmarks demonstrate that HISTOGRAPH offers strong performance that consistently improves traditional techniques, with particularly strong robustness in deep GNNs.

</details>


### [384] [Self-Training the Neurochaos Learning Algorithm](https://arxiv.org/abs/2601.01146)
*Anusree M,Akhila Henry,Pramod P Nair*

Main category: cs.LG

TL;DR: The paper proposes a hybrid semi-supervised learning (SSL) method combining Neurochaos Learning (NL) and a threshold-based Self-Training (ST) approach, demonstrating its effectiveness on limited, nonlinear, and imbalanced datasets.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenge of limited labelled data in practical applications by leveraging the readily available unlabelled data, as conventional supervised learning methods struggle in such settings.

Method: The proposed approach combines NL, which transforms input features into chaos-based representations, with ST, which iteratively expands the labelled dataset using high-confidence pseudo-labelled data. The model is tested on multiple datasets.

Result: The NL+ST architecture consistently outperformed standalone ST models, achieving significant performance improvements particularly on challenging datasets like Iris, Wine, and Glass Identification.

Conclusion: Chaos-based feature extraction integrated with semi-supervised learning enhances generalisation, resilience, and classification accuracy, particularly in low-data, nonlinear, and imbalanced scenarios.

Abstract: In numerous practical applications, acquiring substantial quantities of labelled data is challenging and expensive, but unlabelled data is readily accessible. Conventional supervised learning methods frequently underperform in scenarios characterised by little labelled data or imbalanced datasets. This study introduces a hybrid semi-supervised learning (SSL) architecture that integrates Neurochaos Learning (NL) with a threshold-based Self-Training (ST) method to overcome this constraint. The NL architecture converts input characteristics into chaos-based ring-rate representations that encapsulate nonlinear relationships within the data, whereas ST progressively enlarges the labelled set utilising high-confidence pseudo-labelled samples. The model's performance is assessed using ten benchmark datasets and five machine learning classifiers, with 85% of the training data considered unlabelled and just 15% utilised as labelled data. The proposed Self-Training Neurochaos Learning (NL+ST) architecture consistently attains superior performance gain relative to standalone ST models, especially on limited, nonlinear and imbalanced datasets like Iris (188.66%), Wine (158.58%) and Glass Identification (110.48%). The results indicate that using chaos-based feature extraction with SSL improves generalisation, resilience, and classification accuracy in low-data contexts.

</details>


### [385] [Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models](https://arxiv.org/abs/2601.01162)
*Zihua Yang,Xin Liao,Yiqun Zhang,Yiu-ming Cheung*

Main category: cs.LG

TL;DR: The paper introduces ARISE, a novel method for clustering categorical data by leveraging semantic embeddings from Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the semantic gap in clustering categorical data caused by treating attribute values as equidistant, especially when co-occurrence patterns are unreliable in limited samples.

Method: The proposed method, ARISE, uses LLMs to enhance semantic descriptions of attribute values, integrates these representations with original categorical data, and applies clustering techniques to identify semantically significant clusters.

Result: ARISE showed consistent improvements over seven clustering methods across eight benchmark datasets, achieving performance gains of 19-27%.

Conclusion: ARISE successfully bridges the semantic gap in categorical data clustering by integrating LLMs, improving clustering quality and identifying latent semantic structures.

Abstract: Categorical data are prevalent in domains such as healthcare, marketing, and bioinformatics, where clustering serves as a fundamental tool for pattern discovery. A core challenge in categorical data clustering lies in measuring similarity among attribute values that lack inherent ordering or distance. Without appropriate similarity measures, values are often treated as equidistant, creating a semantic gap that obscures latent structures and degrades clustering quality. Although existing methods infer value relationships from within-dataset co-occurrence patterns, such inference becomes unreliable when samples are limited, leaving the semantic context of the data underexplored. To bridge this gap, we present ARISE (Attention-weighted Representation with Integrated Semantic Embeddings), which draws on external semantic knowledge from Large Language Models (LLMs) to construct semantic-aware representations that complement the metric space of categorical data for accurate clustering. That is, LLM is adopted to describe attribute values for representation enhancement, and the LLM-enhanced embeddings are combined with the original data to explore semantically prominent clusters. Experiments on eight benchmark datasets demonstrate consistent improvements over seven representative counterparts, with gains of 19-27%. Code is available at https://github.com/develop-yang/ARISE

</details>


### [386] [The Dependency Divide: An Interpretable Machine Learning Framework for Profiling Student Digital Satisfaction in the Bangladesh Context](https://arxiv.org/abs/2601.01231)
*Md Muhtasim Munif Fahim,Humyra Ankona,Md Monimul Huq,Md Rezaul Karim*

Main category: cs.LG

TL;DR: The study explores why satisfaction with digital learning platforms varies among students, introducing the 'Dependency Divide' framework showing that high engagement can make students more vulnerable to infrastructure failures.


<details>
  <summary>Details</summary>
Motivation: Current frameworks do not adequately explain why students with similar digital access experience differing satisfaction levels. The study aims to address this by identifying how engagement interacts with infrastructure reliability.

Method: Conducted a study involving 396 university students in Bangladesh using clustering, Random Forest models, and formal interaction analysis to identify profiles and test the Dependency Divide hypothesis.

Result: Identified three profiles: Casually Engaged (58%), Efficient Learners (35%), and Hyper-Engaged (7%). Found that engagement increases satisfaction only when infrastructure is reliable, with Hyper-Engaged students most vulnerable.

Conclusion: Policies should focus on improving reliability for high-dependency users and educating students about the risks of dependency rather than uniformly promoting engagement.

Abstract: Background: While digital access has expanded rapidly in resource-constrained contexts, satisfaction with digital learning platforms varies significantly among students with seemingly equal connectivity. Traditional digital divide frameworks fail to explain these variations.
  Purpose: This study introduces the "Dependency Divide", a novel framework proposing that highly engaged students become conditionally vulnerable to infrastructure failures, challenging assumptions that engagement uniformly benefits learners in post-access environments.
  Methods: We conducted a cross-sectional study of 396 university students in Bangladesh using a three-stage analytical approach: (1) stability-validated K-prototypes clustering to identify student profiles, (2) profile-specific Random Forest models with SHAP and ALE analysis to determine satisfaction drivers, and (3) formal interaction analysis with propensity score matching to test the Dependency Divide hypothesis.
  Results: Three distinct profiles emerged: Casually Engaged (58%), Efficient Learners (35%), and Hyper-Engaged (7%). A significant interaction between educational device time and internet reliability (\b{eta} = 0.033, p = 0.028) confirmed the Dependency Divide: engagement increased satisfaction only when infrastructure remained reliable. Hyper-Engaged students showed greatest vulnerability despite or because of their sophisticated digital workflows. Policy simulations demonstrated that targeted reliability improvements for high-dependency users yielded 2.06 times greater returns than uniform interventions.
  Conclusions: In fragile infrastructure contexts, capability can become liability. Digital transformation policies must prioritize reliability for dependency-prone users, establish contingency systems, and educate students about dependency risks rather than uniformly promoting engagement.

</details>


### [387] [Benchmarking the Computational and Representational Efficiency of State Space Models against Transformers on Long-Context Dyadic Sessions](https://arxiv.org/abs/2601.01237)
*Abidemi Koledoye,Chinemerem Unachukwu,Gold Nwobu,Hasin Rana*

Main category: cs.LG

TL;DR: The paper benchmarks the Mamba State Space Model (SSM) against the LLaMA Transformer for long-context sequences, emphasizing computational and representational efficiency.


<details>
  <summary>Details</summary>
Motivation: To explore whether SSMs can serve as computationally efficient alternatives to Transformers for long-context sequence modeling.

Method: The study evaluates Mamba SSM and LLaMA Transformer using dyadic therapy sessions, analyzing computational metrics like memory and speed (up to 8,192 tokens), as well as representational aspects such as hidden state dynamics.

Result: The paper identifies specific scenarios where SSMs outperform Transformers in computational and representational efficiency in handling long-context sequences.

Conclusion: State Space Models (SSMs) have potential advantages over Transformers for particular applications in long-context modeling, providing useful insights for real-world tasks.

Abstract: State Space Models (SSMs) have emerged as a promising alternative to Transformers for long-context sequence modeling, offering linear $O(N)$ computational complexity compared to the Transformer's quadratic $O(N^2)$ scaling. This paper presents a comprehensive benchmarking study comparing the Mamba SSM against the LLaMA Transformer on long-context sequences, using dyadic therapy sessions as a representative test case. We evaluate both architectures across two dimensions: (1) computational efficiency, where we measure memory usage and inference speed from 512 to 8,192 tokens, and (2) representational efficiency, where we analyze hidden state dynamics and attention patterns. Our findings provide actionable insights for practitioners working with long-context applications, establishing precise conditions under which SSMs offer advantages over Transformers.

</details>


### [388] [Accelerated Full Waveform Inversion by Deep Compressed Learning](https://arxiv.org/abs/2601.01268)
*Maayan Gelboim,Amir Adler,Mauricio Araya-Polo*

Main category: cs.LG

TL;DR: The paper proposes a method using deep neural networks to reduce the input size for Full Waveform Inversion (FWI), greatly decreasing computational costs while maintaining effectiveness in subsurface exploration.


<details>
  <summary>Details</summary>
Motivation: Modern seismic acquisition systems produce vast datasets that make FWI computationally prohibitive, particularly for complex or large-scale scenarios. Reducing data inputs without losing essential information is critical to improving efficiency and practical feasibility of FWI.

Method: The method employs a deep neural network with a binarized sensing layer to select key seismic data from subsurface models. Representation learning through an autoencoder creates latent representations, which are further refined using K-means clustering to identify the most relevant inputs for FWI.

Result: The proposed approach, even when using only 10% of the full dataset, significantly outperforms random sampling for 2D FWI and shows promise for scaling to large 3D inversions.

Conclusion: The approach demonstrates a successful hierarchical selection process for seismic data compression, paving the way for faster, more computationally feasible large-scale FWI applications.

Abstract: We propose and test a method to reduce the dimensionality of Full Waveform Inversion (FWI) inputs as computational cost mitigation approach. Given modern seismic acquisition systems, the data (as input for FWI) required for an industrial-strength case is in the teraflop level of storage, therefore solving complex subsurface cases or exploring multiple scenarios with FWI become prohibitive. The proposed method utilizes a deep neural network with a binarized sensing layer that learns by compressed learning a succinct but consequential seismic acquisition layout from a large corpus of subsurface models. Thus, given a large seismic data set to invert, the trained network selects a smaller subset of the data, then by using representation learning, an autoencoder computes latent representations of the data, followed by K-means clustering of the latent representations to further select the most relevant data for FWI. Effectively, this approach can be seen as a hierarchical selection. The proposed approach consistently outperforms random data sampling, even when utilizing only 10% of the data for 2D FWI, these results pave the way to accelerating FWI in large scale 3D inversion.

</details>


### [389] [The Alchemy of Thought: Understanding In-Context Learning Through Supervised Classification](https://arxiv.org/abs/2601.01290)
*Harshita Narnoli,Mihai Surdeanu*

Main category: cs.LG

TL;DR: The paper investigates how in-context learning (ICL) compares to supervised classifiers in terms of behavior and finds that ICL is closer to k-nearest neighbors (kNN) than gradient descent-based classifiers, especially when demonstration relevance is high.


<details>
  <summary>Details</summary>
Motivation: To enhance understanding of how in-context learning works in large language models (LLMs), despite its widespread empirical success, and to determine its similarities with classic supervised learning methods.

Method: The study compares the behavior of in-context learning with supervised classifiers (gradient descent and k-nearest neighbors) using text classification tasks across six datasets and three large language models.

Result: ICL on average behaves more like k-nearest neighbors than gradient descent, particularly when the relevance of demonstrations is high. When demonstration relevance is low, LLMs outperform standard classifiers by utilizing their parametric memory.

Conclusion: ICL's behavior aligns closely with k-nearest neighbors due to similarities in the attention mechanism, but LLMs demonstrate superior performance overall because of their ability to fall back on parametric memory.

Abstract: In-context learning (ICL) has become a prominent paradigm to rapidly customize LLMs to new tasks without fine-tuning. However, despite the empirical evidence of its usefulness, we still do not truly understand how ICL works. In this paper, we compare the behavior of in-context learning with supervised classifiers trained on ICL demonstrations to investigate three research questions: (1) Do LLMs with ICL behave similarly to classifiers trained on the same examples? (2) If so, which classifiers are closer, those based on gradient descent (GD) or those based on k-nearest neighbors (kNN)? (3) When they do not behave similarly, what conditions are associated with differences in behavior? Using text classification as a use case, with six datasets and three LLMs, we observe that LLMs behave similarly to these classifiers when the relevance of demonstrations is high. On average, ICL is closer to kNN than logistic regression, giving empirical evidence that the attention mechanism behaves more similarly to kNN than GD. However, when demonstration relevance is low, LLMs perform better than these classifiers, likely because LLMs can back off to their parametric memory, a luxury these classifiers do not have.

</details>


### [390] [Sobolev Approximation of Deep ReLU Network in Log-weighted Barron Space](https://arxiv.org/abs/2601.01295)
*Changhoon Song,Seungchan Ko,Youngjoon Hong*

Main category: cs.LG

TL;DR: This paper introduces the log-weighted Barron space ($\mathscr{B}^{\log}$) with weaker regularity requirements than traditional Barron spaces. It establishes explicit depth-dependent approximation bounds for deep ReLU networks and analyzes embedding properties and statistical aspects.


<details>
  <summary>Details</summary>
Motivation: To explain the success of deep neural networks on high-dimensional data by relaxing the stringent conditions of classical Barron spaces and offering a more practical function space ($\mathscr{B}^{\log}$).

Method: The authors introduce the log-weighted Barron space ($\mathscr{B}^{\log}$), analyze its embedding properties and statistical complexity, and provide theoretical depth-sensitive approximation bounds for deep ReLU networks.

Result: Functions in the log-weighted Barron space can be efficiently approximated with explicit depth dependence, reducing the regularity requirements for stable representation in high dimensions.

Conclusion: The paper clarifies how deeper architectures improve function representation with relaxed regularity constraints, justifying their effectiveness in challenging, high-dimensional problems.

Abstract: Universal approximation theorems show that neural networks can approximate any continuous function; however, the number of parameters may grow exponentially with the ambient dimension, so these results do not fully explain the practical success of deep models on high-dimensional data. Barron space theory addresses this: if a target function belongs to a Barron space, a two-layer network with $n$ parameters achieves an $O(n^{-1/2})$ approximation error in $L^2$. Yet classical Barron spaces $\mathscr{B}^{s+1}$ still require stronger regularity than Sobolev spaces $H^s$, and existing depth-sensitive results often assume constraints such as $sL \le 1/2$. In this paper, we introduce a log-weighted Barron space $\mathscr{B}^{\log}$, which requires a strictly weaker assumption than $\mathscr{B}^s$ for any $s>0$. For this new function space, we first study embedding properties and carry out a statistical analysis via the Rademacher complexity. Then we prove that functions in $\mathscr{B}^{\log}$ can be approximated by deep ReLU networks with explicit depth dependence. We then define a family $\mathscr{B}^{s,\log}$, establish approximation bounds in the $H^1$ norm, and identify maximal depth scales under which these rates are preserved. Our results clarify how depth reduces regularity requirements for efficient representation, offering a more precise explanation for the performance of deep architectures beyond the classical Barron setting, and for their stable use in high-dimensional problems used today.

</details>


### [391] [ARGUS: Adaptive Rotation-Invariant Geometric Unsupervised System](https://arxiv.org/abs/2601.01297)
*Anantha Sharma*

Main category: cs.LG

TL;DR: The paper introduces 'Argus,' a framework for detecting drift in high-dimensional data streams through localized statistics on Voronoi tessellations, offering a computationally efficient and geometry-preserving solution.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for efficient and accurate drift detection methods that preserve high-dimensional geometric structure while being computationally feasible.

Method: The method involves using Voronoi tessellations on orthonormal frames, scaling to high dimensions with product quantization tessellation, and employing graph-theoretic tools to characterize drift propagation.

Result: The results include proof of geometric invariance, linear computational complexity, and successful experimental validation showing better drift detection compared to existing methods.

Conclusion: Argus demonstrates a theoretically robust and computationally efficient framework for monitoring distributional drift, addressing limitations of previous methods while preserving high-dimensional structure.

Abstract: Detecting distributional drift in high-dimensional data streams presents fundamental challenges: global comparison methods scale poorly, projection-based approaches lose geometric structure, and re-clustering methods suffer from identity instability. This paper introduces Argus, A framework that reconceptualizes drift detection as tracking local statistics over a fixed spatial partition of the data manifold.
  The key contributions are fourfold. First, it is proved that Voronoi tessellations over canonical orthonormal frames yield drift metrics that are invariant to orthogonal transformations. The rotations and reflections that preserve Euclidean geometry. Second, it is established that this framework achieves O(N) complexity per snapshot while providing cell-level spatial localization of distributional change. Third, a graph-theoretic characterization of drift propagation is developed that distinguishes coherent distributional shifts from isolated perturbations. Fourth, product quantization tessellation is introduced for scaling to very high dimensions (d>500) by decomposing the space into independent subspaces and aggregating drift signals across subspaces.
  This paper formalizes the theoretical foundations, proves invariance properties, and presents experimental validation demonstrating that the framework correctly identifies drift under coordinate rotation while existing methods produce false positives. The tessellated approach offers a principled geometric foundation for distribution monitoring that preserves high-dimensional structure without the computational burden of pairwise comparisons.

</details>


### [392] [Towards a Principled Muon under $μ\mathsf{P}$: Ensuring Spectral Conditions throughout Training](https://arxiv.org/abs/2601.01306)
*John Zhao*

Main category: cs.LG

TL;DR: The paper introduces Muon++, a variant of the Muon optimizer, that satisfies the spectral conditions of μ-parameterization (μP) throughout LLM training, with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of ensuring spectral conditions required by μP during the entire training horizon of large language models, while minimizing computational overhead.

Method: The authors propose Muon++, where spectral control is maintained solely at the level of optimizer updates for moderately large models, thus eliminating the need for explicit weight spectral normalization.

Result: Muon++ ensures compatibility with μP scaling during long-horizon training and incorporates data-dependent effects for better adaptability.

Conclusion: Muon++ bridges the gap between the theoretical μP framework and its practical application, making it viable for efficient LLM training across model sizes.

Abstract: The $μ$-parameterization ($μ$P) provides a principled foundation for large language model (LLM) training by prescribing width-independent learning dynamics, which in turn enables predictable scaling behavior and robust hyperparameter transfer across model sizes. A central requirement of $μ$P is the satisfaction of certain spectral conditions on weight matrices, which ensure consistent feature learning and optimization behavior as model width grows. While these conditions are well understood in theory, guaranteeing their validity in practical training for matrix-based optimizers such as Muon is still under studied. Existing works that study Muon under $μ$P exhibit important limitations: they either do not ensure that the spectral conditions hold throughout the entire training horizon, or require repeated spectral normalization (or Newton-Schulz iterations) applied to both weights and updates, leading to significant computational overhead and reduced practicality. In this work, we show how to reliably guarantee the spectral conditions required by $μ$P for Muon during the entire training process. Our key insight is that for moderately large models, maintaining spectral control at the level of optimizer updates alone is sufficient to preserve $μ$P-compatible scaling, eliminating the need for explicit spectral normalization of the weights. Based on this principle, we develop a variant of Muon, namely Muon++, that satisfies spectral condition throughout the training process. Our results bridge the gap between the theoretical promises of $μ$P and the practical deployment of matrix-based optimizers in long-horizon training. We also take the first step towards an adaptive spectral condition by incorporating data-dependent effects, making it better suited for long-horizon LLM training.

</details>


### [393] [Spectral-Window Hybrid (SWH)](https://arxiv.org/abs/2601.01313)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: The paper introduces the "Spectral-Window Hybrid (SWH)" architecture for efficient and scalable sequence modeling by combining global and local modeling methods.


<details>
  <summary>Details</summary>
Motivation: To address the computational inefficiencies ($\mathcal{O}(T^2)$ complexity) of Transformers in long-horizon sequence tasks while retaining effective context representation.

Method: The paper proposes the SWH model, which separates sequence modeling into a global branch (using the Convolution Theorem for long-range dynamics) and a local branch (sliding-window attention for bounded interactions), achieving an efficient $O(T \log T)$ complexity.

Result: SWH achieves comparable performance to Transformers on short contexts while demonstrating linear scalability to longer sequences.

Conclusion: SWH effectively balances computational efficiency and representational power, avoiding the bottlenecks of traditional attention mechanisms in Transformers.

Abstract: Scaling sequence modeling to extreme contexts requires balancing computational efficiency with representational expressivity. While Transformers provide precise retrieval via the attention mechanism, their quadratic $\mathcal{O}(T^2)$ complexity limits their application to long-horizon tasks. In this work, we propose the \textbf{Spectral-Window Hybrid (SWH)}, an architecture that decouples sequence modeling into two \textit{parallel} streams: a global branch utilizing the Convolution Theorem to model long-range decay dynamics in $\mathcal{O}(T \log T)$ time, and a local branch employing sliding-window attention for token interactions within a bounded context. By aggregating these representations, SWH avoids the computational bottleneck of global attention while retaining local precision. We demonstrate that SWH matches the perplexity of standard Transformers on short contexts while enabling efficient linear scaling to extended sequences. The code is available at https://github.com/VladimerKhasia/SWH

</details>


### [394] [From Classification to Generation: An Open-Ended Paradigm for Adverse Drug Reaction Prediction Based on Graph-Motif Feature Fusion](https://arxiv.org/abs/2601.01347)
*Yuyan Pi,Min Jin,Wentao Xie,Xinhua Liu*

Main category: cs.LG

TL;DR: This paper introduces GM-MLG, a model for addressing ADR prediction challenges by leveraging graph-motif feature fusion and multi-label generation, yielding improved predictions and expanded label prediction space.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the high costs and limitations of current ADR prediction methods, motivated by challenges like drug data scarcity, closed label sets, and inadequate label dependency modeling.

Method: The proposed method, GM-MLG, uses molecular structure analysis with dual-graph representation, transforming ADR prediction into a multi-label generation problem using a Transformer Decoder approach.

Result: Experiments show GM-MLG achieves up to 38% improvement in prediction and expands the prediction space from 200 to over 10,000 types, while elucidating structure-activity relationships.

Conclusion: The GM-MLG approach offers a novel and interpretable framework for ADR prediction, providing potential to systematically improve drug safety and reduce risks.

Abstract: Computational biology offers immense potential for reducing the high costs and protracted cycles of new drug development through adverse drug reaction (ADR) prediction. However, current methods remain impeded by drug data scarcity-induced cold-start challenge, closed label sets, and inadequate modeling of label dependencies. Here we propose an open-ended ADR prediction paradigm based on Graph-Motif feature fusion and Multi-Label Generation (GM-MLG). Leveraging molecular structure as an intrinsic and inherent feature, GM-MLG constructs a dual-graph representation architecture spanning the atomic level, the local molecular level (utilizing fine-grained motifs dynamically extracted via the BRICS algorithm combined with additional fragmentation rules), and the global molecular level. Uniquely, GM-MLG pioneers transforming ADR prediction from multi-label classification into Transformer Decoder-based multi-label generation. By treating ADR labels as discrete token sequences, it employs positional embeddings to explicitly capture dependencies and co-occurrence relationships within large-scale label spaces, generating predictions via autoregressive decoding to dynamically expand the prediction space. Experiments demonstrate GM-MLG achieves up to 38% improvement and an average gain of 20%, expanding the prediction space from 200 to over 10,000 types. Furthermore, it elucidates non-linear structure-activity relationships between ADRs and motifs via retrosynthetic motif analysis, providing interpretable and innovative support for systematic risk reduction in drug safety.

</details>


### [395] [Towards LLM-enabled autonomous combustion research: A literature-aware agent for self-corrective modeling workflows](https://arxiv.org/abs/2601.01357)
*Ke Xiao,Haoze Zhang,Runze Mao,Han Li,Zhi X. Chen*

Main category: cs.LG

TL;DR: FlamePilot is an AI-driven agent designed for combustion modeling, integrating domain knowledge and automating complex CFD workflows.


<details>
  <summary>Details</summary>
Motivation: To address the gap in integrating AI effectively into expertise-intensive domains like combustion modeling, enabling seamless use of domain-specific tools and literature.

Method: FlamePilot utilizes an architecture based on atomic tools for robust CFD simulation setups, learning from scientific articles to optimize processes from setup to results.

Result: Validation showed FlamePilot achieved superior executability and success rates (1.0 and 0.438) compared to previous agents.

Conclusion: FlamePilot offers a transparent and collaborative AI framework that enables researchers to focus on analysis while managing workflows autonomously.

Abstract: The rapid evolution of large language models (LLMs) is transforming artificial intelligence into autonomous research partners, yet a critical gap persists in complex scientific domains such as combustion modeling. Here, practical AI assistance requires the seamless integration of domain literature knowledge with robust execution capabilities for expertise-intensive tools such as computational fluid dynamics (CFD) codes. To bridge this gap, we introduce FlamePilot, an LLM agent designed to empower combustion modeling research through automated and self-corrective CFD workflows. FlamePilot differentiates itself through an architecture that leverages atomic tools to ensure the robust setup and execution of complex simulations in both OpenFOAM and extended frameworks such as DeepFlame. The system is also capable of learning from scientific articles, extracting key information to guide the simulation from initial setup to optimized results. Validation on a public benchmark shows FlamePilot achieved a perfect 1.0 executability score and a 0.438 success rate, surpassing the prior best reported agent scores of 0.625 and 0.250, respectively. Furthermore, a detailed case study on Moderate or Intense Low-oxygen Dilution (MILD) combustion simulation demonstrates its efficacy as a collaborative research copilot, where FlamePilot autonomously translated a research paper into a configured simulation, conducted the simulation, post-processed the results, proposed evidence-based refinements, and managed a multi-step parameter study to convergence under minimal human intervention. By adopting a transparent and interpretable paradigm, FlamePilot establishes a foundational framework for AI-empowered combustion modeling, fostering a collaborative partnership where the agent manages workflow orchestration, freeing the researcher for high-level analysis.

</details>


### [396] [Causal discovery for linear causal model with correlated noise: an Adversarial Learning Approach](https://arxiv.org/abs/2601.01368)
*Mujin Zhou,Junzhe Zhang*

Main category: cs.LG

TL;DR: The paper proposes a method to learn causal structures from data with unmeasured confounding factors using the f-GAN framework.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of discovering causal structures in the presence of unmeasured confounding factors, which complicates accurate causal inference.

Method: The authors reformulate causal structure learning as minimizing Bayesian free energy, equating it to minimizing f-divergence between true and model distributions. They leverage the f-GAN framework to transform this into a min-max adversarial optimization problem, using Gumbel-Softmax for gradient search in discrete graph space.

Result: The proposed approach enables effective learning of binary causal structures that are independent of specific weight values, even in the presence of unmeasured confounding factors.

Conclusion: The paper demonstrates the viability of leveraging the f-GAN framework coupled with adversarial optimization and Gumbel-Softmax relaxation to address the problem of causal structure discovery.

Abstract: Causal discovery from data with unmeasured confounding factors is a challenging problem. This paper proposes an approach based on the f-GAN framework, learning the binary causal structure independent of specific weight values. We reformulate the structure learning problem as minimizing Bayesian free energy and prove that this problem is equivalent to minimizing the f-divergence between the true data distribution and the model-generated distribution. Using the f-GAN framework, we transform this objective into a min-max adversarial optimization problem. We implement the gradient search in the discrete graph space using Gumbel-Softmax relaxation.

</details>


### [397] [Data Complexity-aware Deep Model Performance Forecasting](https://arxiv.org/abs/2601.01383)
*Yen-Chia Chen,Hsing-Kuo Pao,Hanjuan Huang*

Main category: cs.LG

TL;DR: This paper introduces a two-stage framework to estimate deep learning model performance before training, reducing reliance on trial-and-error approaches.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies, resource intensity, and difficulty in automating model architecture selection using traditional trial-and-error methods for training deep learning models.

Method: Proposes a lightweight, two-stage framework: the first stage analyzes measurable properties of the dataset to predict baseline performance, and the second adjusts the estimation using the model's architectural and hyperparameter details.

Result: The framework generalizes across datasets and models, offers guidance on model selection, reveals early indicators of data quality, and detects problematic datasets.

Conclusion: The framework facilitates efficient model performance forecasting, assists in architecture selection, preprocessing, and identifying dataset issues, improving training workflows.

Abstract: Deep learning models are widely used across computer vision and other domains. When working on the model induction, selecting the right architecture for a given dataset often relies on repetitive trial-and-error procedures. This procedure is time-consuming, resource-intensive, and difficult to automate. While previous work has explored performance prediction using partial training or complex simulations, these methods often require significant computational overhead or lack generalizability. In this work, we propose an alternative approach: a lightweight, two-stage framework that can estimate model performance before training given the understanding of the dataset and the focused deep model structures. The first stage predicts a baseline based on the analysis of some measurable properties of the dataset, while the second stage adjusts the estimation with additional information on the model's architectural and hyperparameter details. The setup allows the framework to generalize across datasets and model types. Moreover, we find that some of the underlying features used for prediction - such as dataset variance - can offer practical guidance for model selection, and can serve as early indicators of data quality. As a result, the framework can be used not only to forecast model performance, but also to guide architecture choices, inform necessary preprocessing procedures, and detect potentially problematic datasets before training begins.

</details>


### [398] [Scale-Adaptive Power Flow Analysis with Local Topology Slicing and Multi-Task Graph Learning](https://arxiv.org/abs/2601.01387)
*Yongzhe Li,Lin Guan,Zihan Cai,Zuxian Lin,Jiyu Huang,Liukai Chen*

Main category: cs.LG

TL;DR: This paper introduces a novel SaMPFA framework to improve power flow analysis adaptability across varying topological scales using deep learning.


<details>
  <summary>Details</summary>
Motivation: The need for power flow analysis models with strong adaptability to variable topological scales and robust predictions prompted this work.

Method: The authors propose the SaMPFA framework with LTS for cross-scale learning and RMGL for physics-guided robust predictions in power systems.

Result: Experiments on simulation datasets (IEEE 39-bus system and a provincial grid in China) demonstrate SaMPFA significantly enhances prediction accuracy by 4.47% and 36.82%.

Conclusion: The SaMPFA framework shows high potential in improving robustness and generalization of power flow predictions under varying system scales, aligning closely with physical laws.

Abstract: Developing deep learning models with strong adaptability to topological variations is of great practical significance for power flow analysis. To enhance model performance under variable system scales and improve robustness in branch power prediction, this paper proposes a Scale-adaptive Multi-task Power Flow Analysis (SaMPFA) framework. SaMPFA introduces a Local Topology Slicing (LTS) sampling technique that extracts subgraphs of different scales from the complete power network to strengthen the model's cross-scale learning capability. Furthermore, a Reference-free Multi-task Graph Learning (RMGL) model is designed for robust power flow prediction. Unlike existing approaches, RMGL predicts bus voltages and branch powers instead of phase angles. This design not only avoids the risk of error amplification in branch power calculation but also guides the model to learn the physical relationships of phase angle differences. In addition, the loss function incorporates extra terms that encourage the model to capture the physical patterns of angle differences and power transmission, further improving consistency between predictions and physical laws. Simulations on the IEEE 39-bus system and a real provincial grid in China demonstrate that the proposed model achieves superior adaptability and generalization under variable system scales, with accuracy improvements of 4.47% and 36.82%, respectively.

</details>


### [399] [A Graph-based Framework for Online Time Series Anomaly Detection Using Model Ensemble](https://arxiv.org/abs/2601.01403)
*Zewei Yu,Jianqiu Xu,Caimin Li*

Main category: cs.LG

TL;DR: GDME is a dynamic graph-based framework for online time series anomaly detection that adapts to evolving industrial streaming data, outperforming existing methods by up to 24%.


<details>
  <summary>Details</summary>
Motivation: The increasing volume and diverse patterns of streaming industrial data necessitate improved online anomaly detection methods due to challenges in handling evolving heterogeneous data.

Method: GDME employs a dynamic model ensemble with a graph structure to represent model relationships. It continuously updates the model pool, uses community detection for ensemble selection, and detects concept drift via structural changes in the graph.

Result: The experiments demonstrate that GDME outperforms existing online anomaly detection methods by up to 24% on heterogeneous time series, achieving superior detection performance and computational efficiency.

Conclusion: GDME provides an effective and adaptive solution for online anomaly detection in streaming data, offering improved performance and adaptability over traditional methods.

Abstract: With the increasing volume of streaming data in industrial systems, online anomaly detection has become a critical task. The diverse and rapidly evolving data patterns pose significant challenges for online anomaly detection. Many existing anomaly detection methods are designed for offline settings or have difficulty in handling heterogeneous streaming data effectively. This paper proposes GDME, an unsupervised graph-based framework for online time series anomaly detection using model ensemble. GDME maintains a dynamic model pool that is continuously updated by pruning underperforming models and introducing new ones. It utilizes a dynamic graph structure to represent relationships among models and employs community detection on the graph to select an appropriate subset for ensemble. The graph structure is also used to detect concept drift by monitoring structural changes, allowing the framework to adapt to evolving streaming data. Experiments on seven heterogeneous time series demonstrate that GDME outperforms existing online anomaly detection methods, achieving improvements of up to 24%. In addition, its ensemble strategy provides superior detection performance compared with both individual models and average ensembles, with competitive computational efficiency.

</details>


### [400] [A Depth Hierarchy for Computing the Maximum in ReLU Networks via Extremal Graph Theory](https://arxiv.org/abs/2601.01417)
*Itay Safran*

Main category: cs.LG

TL;DR: The paper establishes that deep ReLU networks require a certain width to precisely compute the maximum function over $d$ inputs, providing a first unconditional super-linear lower bound for depth $k \geq 3$.


<details>
  <summary>Details</summary>
Motivation: To understand the representational limitations of ReLU neural networks in computing the maximum function and to identify the inherent complexity due to geometric structures.

Method: A combinatorial proof leveraging graph theory, specifically Turán's theorem, to associate ridges of the maximum function with graph cliques and deduce lower bounds on network width.

Result: Proof that ReLU networks need width that grows at least as $Ω\big(d^{1+\frac{1}{2^{k-2}-1}}\big)$ for depths $3 \leq k \leq \log_2(\log_2(d))$ to compute the exact maximum.

Conclusion: The maximum function embodies a fundamental complexity due to its non-differentiable structure, and this complexity sets super-linear width requirements for neural networks even as depth grows.

Abstract: We consider the problem of exact computation of the maximum function over $d$ real inputs using ReLU neural networks. We prove a depth hierarchy, wherein width $Ω\big(d^{1+\frac{1}{2^{k-2}-1}}\big)$ is necessary to represent the maximum for any depth $3\le k\le \log_2(\log_2(d))$. This is the first unconditional super-linear lower bound for this fundamental operator at depths $k\ge3$, and it holds even if the depth scales with $d$. Our proof technique is based on a combinatorial argument and associates the non-differentiable ridges of the maximum with cliques in a graph induced by the first hidden layer of the computing network, utilizing Turán's theorem from extremal graph theory to show that a sufficiently narrow network cannot capture the non-linearities of the maximum. This suggests that despite its simple nature, the maximum function possesses an inherent complexity that stems from the geometric structure of its non-differentiable hyperplanes, and provides a novel approach for proving lower bounds for deep neural networks.

</details>


### [401] [Bayesian Subspace Gradient Estimation for Zeroth-Order Optimization of Large Language Models](https://arxiv.org/abs/2601.01452)
*Jian Feng,Zhihong Huang*

Main category: cs.LG

TL;DR: The paper introduces Bayesian Subspace Zeroth-Order optimization (BSZO), which improves gradient approximation in large language models using zeroth-order methods, enhancing convergence and performance across multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Memory-efficient optimization for fine-tuning large language models is essential, and current zeroth-order methods are limited by simplistic one-step gradient approximations.

Method: BSZO leverages Kalman filtering and Bayesian inference to integrate finite-difference gradient information, with an adaptive mechanism to adjust perturbation scales.

Result: BSZO significantly outperforms existing zeroth-order methods like MeZO and HiZOO on models such as RoBERTa, Mistral, and OPT, achieving up to 6.67% better average performance on OPT-13B with minimal memory impact.

Conclusion: The study demonstrates that BSZO is a superior approach to zeroth-order optimization for LLM fine-tuning, combining better convergence rates with efficient memory usage.

Abstract: Fine-tuning large language models (LLMs) with zeroth-order (ZO) optimization reduces memory by approximating gradients through function evaluations, but existing methods rely on one-step gradient estimates from random perturbations. We introduce Bayesian Subspace Zeroth-Order optimization (BSZO), a ZO optimizer that applies Kalman filtering to combine finite-difference information across multiple perturbation directions. By treating each finite-difference measurement as a noisy observation, BSZO builds a posterior distribution over the projected gradient and updates it through Bayesian inference, with a residual-based adaptive mechanism to adjust perturbation scales. Theoretical analysis shows that BSZO improves the convergence rate by a factor of $k/γ$ compared to standard ZO methods. Experiments on RoBERTa, Mistral, and OPT models show that BSZO outperforms MeZO, MeZO-Adam, and HiZOO across various tasks, achieving up to 6.67\% absolute average improvement on OPT-13B while keeping memory usage close to inference-only baselines (1.00$\times$--1.08$\times$ of MeZO).

</details>


### [402] [Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD](https://arxiv.org/abs/2601.01465)
*Ze Peng,Jian Zhang,Yisen Wang,Lei Qi,Yinghuan Shi,Yang Gao*

Main category: cs.LG

TL;DR: This paper develops a novel information-theoretic (IT) generalization bound for SGD that improves generalization by leveraging the flatness of the loss landscape.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the observation that existing IT bounds fail to adequately capture the effect of SGD's flatness bias on generalization and are numerically loose.

Method: The authors introduce a new flatness-leveraging IT bound and use the 'omniscient trajectory' technique to address the flatness bias in SGD. They analyze the learned models under this framework.

Result: The developed bound shows improved generalization in models when flatness is enhanced and is numerically much tighter. It also improves minimax excess risk rates on certain problems.

Conclusion: This work provides a new understanding of SGD's generalization by linking it to flatness and offers tighter IT bounds that outperform existing methods in theory and practice.

Abstract: Information-theoretic (IT) generalization bounds have been used to study the generalization of learning algorithms. These bounds are intrinsically data- and algorithm-dependent so that one can exploit the properties of data and algorithm to derive tighter bounds. However, we observe that although the flatness bias is crucial for SGD's generalization, these bounds fail to capture the improved generalization under better flatness and are also numerically loose. This is caused by the inadequate leverage of SGD's flatness bias in existing IT bounds. This paper derives a more flatness-leveraging IT bound for the flatness-favoring SGD. The bound indicates the learned models generalize better if the large-variance directions of the final weight covariance have small local curvatures in the loss landscape. Experiments on deep neural networks show our bound not only correctly reflects the better generalization when flatness is improved, but is also numerically much tighter. This is achieved by a flexible technique called "omniscient trajectory". When applied to Gradient Descent's minimax excess risk on convex-Lipschitz-Bounded problems, it improves representative IT bounds' $Ω(1)$ rates to $O(1/\sqrt{n})$. It also implies a by-pass of memorization-generalization trade-offs.

</details>


### [403] [Accelerating Storage-Based Training for Graph Neural Networks](https://arxiv.org/abs/2601.01473)
*Myung-Hwan Jang,Jeong-Min Park,Yunyong Ko,Sang-Wook Kim*

Main category: cs.LG

TL;DR: This paper introduces AGNES, a storage-based graph neural network (GNN) training framework that tackles inefficiencies in handling small storage I/Os, achieving faster training speeds on large-scale graphs compared to state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the bottleneck in existing storage-based GNN training methods caused by frequent inefficient small storage I/Os and to ensure efficient utilization of external storage for large-scale graphs.

Method: This paper proposes AGNES, a framework utilizing block-wise storage I/O processing and hyperbatch-based processing to optimize data access and preparation during GNN training.

Result: Comprehensive experiments on five real-world datasets showed that AGNES outperformed four state-of-the-art methods, achieving up to a 4.1x improvement in training speed.

Conclusion: AGNES offers an effective solution to the data preparation bottlenecks in storage-based GNN training, demonstrating significant performance improvements and scalability for large-scale graph training tasks.

Abstract: Graph neural networks (GNNs) have achieved breakthroughs in various real-world downstream tasks due to their powerful expressiveness. As the scale of real-world graphs has been continuously growing, \textit{a storage-based approach to GNN training} has been studied, which leverages external storage (e.g., NVMe SSDs) to handle such web-scale graphs on a single machine. Although such storage-based GNN training methods have shown promising potential in large-scale GNN training, we observed that they suffer from a severe bottleneck in data preparation since they overlook a critical challenge: \textit{how to handle a large number of small storage I/Os}. To address the challenge, in this paper, we propose a novel storage-based GNN training framework, named \textsf{AGNES}, that employs a method of \textit{block-wise storage I/O processing} to fully utilize the I/O bandwidth of high-performance storage devices. Moreover, to further enhance the efficiency of each storage I/O, \textsf{AGNES} employs a simple yet effective strategy, \textit{hyperbatch-based processing} based on the characteristics of real-world graphs. Comprehensive experiments on five real-world graphs reveal that \textsf{AGNES} consistently outperforms four state-of-the-art methods, by up to 4.1$\times$ faster than the best competitor. Our code is available at https://github.com/Bigdasgit/agnes-kdd26.

</details>


### [404] [Multi-Subspace Multi-Modal Modeling for Diffusion Models: Estimation, Convergence and Mixture of Experts](https://arxiv.org/abs/2601.01475)
*Ruofeng Yang,Yongcan Li,Bo Jiang,Cheng Chen,Shuai Li*

Main category: cs.LG

TL;DR: The paper introduces a new modeling technique, MoLR-MoG, to address limitations in diffusion models, achieving better results with lower computational costs and providing theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the curse of dimensionality and better capture the multi-modal properties of data in diffusion models, an area where previous models with Gaussian latents fell short.

Method: The method involves proposing MoLR-MoG modeling, combining mixture subspace with low-rank mixture of Gaussians, and deriving its corresponding score function with a mixture of expert structure.

Result: MoLR-MoG achieves superior generation quality compared to MoE-latent Gaussian scores and comparable performance with larger models like MoE-latent Unet, while providing new theoretical error bounds.

Conclusion: The paper demonstrates the reasonableness and practicality of MoLR-MoG for real-world data, explaining why diffusion models achieve excellent results with small datasets and fast optimization processes.

Abstract: Recently, diffusion models have achieved a great performance with a small dataset of size $n$ and a fast optimization process. However, the estimation error of diffusion models suffers from the curse of dimensionality $n^{-1/D}$ with the data dimension $D$. Since images are usually a union of low-dimensional manifolds, current works model the data as a union of linear subspaces with Gaussian latent and achieve a $1/\sqrt{n}$ bound. Though this modeling reflects the multi-manifold property, the Gaussian latent can not capture the multi-modal property of the latent manifold. To bridge this gap, we propose the mixture subspace of low-rank mixture of Gaussian (MoLR-MoG) modeling, which models the target data as a union of $K$ linear subspaces, and each subspace admits a mixture of Gaussian latent ($n_k$ modals with dimension $d_k$). With this modeling, the corresponding score function naturally has a mixture of expert (MoE) structure, captures the multi-modal information, and contains nonlinear property. We first conduct real-world experiments to show that the generation results of MoE-latent MoG NN are much better than MoE-latent Gaussian score. Furthermore, MoE-latent MoG NN achieves a comparable performance with MoE-latent Unet with $10 \times$ parameters. These results indicate that the MoLR-MoG modeling is reasonable and suitable for real-world data. After that, based on such MoE-latent MoG score, we provide a $R^4\sqrt{Σ_{k=1}^Kn_k}\sqrt{Σ_{k=1}^Kn_kd_k}/\sqrt{n}$ estimation error, which escapes the curse of dimensionality by using data structure. Finally, we study the optimization process and prove the convergence guarantee under the MoLR-MoG modeling. Combined with these results, under a setting close to real-world data, this work explains why diffusion models only require a small training sample and enjoy a fast optimization process to achieve a great performance.

</details>


### [405] [SGD-Based Knowledge Distillation with Bayesian Teachers: Theory and Guidelines](https://arxiv.org/abs/2601.01484)
*Itai Morad,Nir Shlezinger,Yonina C. Eldar*

Main category: cs.LG

TL;DR: The paper adopts a Bayesian perspective to analyze the convergence of student models trained via Knowledge Distillation, revealing benefits of Bayesian teachers for improved accuracy and stability.


<details>
  <summary>Details</summary>
Motivation: The work aims to enhance theoretical understanding of Knowledge Distillation (KD), a popular method for transferring knowledge from teacher networks to student models, and explores how Bayesian approaches can optimize this process.

Method: Two regimes are analyzed under a Bayesian framework: (1) using exact Bayes Class Probabilities as supervision, and (2) noisy approximations of BCPs; theoretical and experimental validations are conducted on convergence and accuracy aspects.

Result: The analysis highlights variance reduction and improved convergence when using Bayesian Class Probabilities. Empirically, models distilled from Bayesian teachers outperform those from deterministic teachers in accuracy (+4.27%) and convergence stability (30% less noise).

Conclusion: Bayesian models are more effective as teachers for KD, improving student accuracy and stability, providing theoretical and practical advancements in the KD paradigm.

Abstract: Knowledge Distillation (KD) is a central paradigm for transferring knowledge from a large teacher network to a typically smaller student model, often by leveraging soft probabilistic outputs. While KD has shown strong empirical success in numerous applications, its theoretical underpinnings remain only partially understood. In this work, we adopt a Bayesian perspective on KD to rigorously analyze the convergence behavior of students trained with Stochastic Gradient Descent (SGD). We study two regimes: $(i)$ when the teacher provides the exact Bayes Class Probabilities (BCPs); and $(ii)$ supervision with noisy approximations of the BCPs. Our analysis shows that learning from BCPs yields variance reduction and removes neighborhood terms in the convergence bounds compared to one-hot supervision. We further characterize how the level of noise affects generalization and accuracy. Motivated by these insights, we advocate the use of Bayesian deep learning models, which typically provide improved estimates of the BCPs, as teachers in KD. Consistent with our analysis, we experimentally demonstrate that students distilled from Bayesian teachers not only achieve higher accuracies (up to +4.27%), but also exhibit more stable convergence (up to 30% less noise), compared to students distilled from deterministic teachers.

</details>


### [406] [Accelerating Decentralized Optimization via Overlapping Local Steps](https://arxiv.org/abs/2601.01493)
*Yijie Zhou,Shi Pu*

Main category: cs.LG

TL;DR: OLDSGD enhances decentralized learning by minimizing communication delays, boosting training speed while retaining theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for faster decentralized training in distributed learning without compromising data privacy or efficiency.

Method: The authors propose OLDSGD, which employs computation-communication overlapping to reduce communication-induced stalls and network idle times while maintaining average updates similar to Local SGD.

Result: OLDSGD achieves the same iteration complexity as standard Local Decentralized SGD but improves runtime efficiency and empirical convergence speed under varying communication delays.

Conclusion: OLDSGD provides an effective and practical way to enhance the speed of decentralized learning with minimal changes to existing frameworks.

Abstract: Decentralized optimization has emerged as a critical paradigm for distributed learning, enabling scalable training while preserving data privacy through peer-to-peer collaboration. However, existing methods often suffer from communication bottlenecks due to frequent synchronization between nodes. We present Overlapping Local Decentralized SGD (OLDSGD), a novel approach to accelerate decentralized training by computation-communication overlapping, significantly reducing network idle time. With a deliberately designed update, OLDSGD preserves the same average update as Local SGD while avoiding communication-induced stalls. Theoretically, we establish non-asymptotic convergence rates for smooth non-convex objectives, showing that OLDSGD retains the same iteration complexity as standard Local Decentralized SGD while improving per-iteration runtime. Empirical results demonstrate OLDSGD's consistent improvements in wall-clock time convergence under different levels of communication delays. With minimal modifications to existing frameworks, OLDSGD offers a practical solution for faster decentralized learning without sacrificing theoretical guarantees.

</details>


### [407] [Advanced Global Wildfire Activity Modeling with Hierarchical Graph ODE](https://arxiv.org/abs/2601.01501)
*Fan Xu,Wei Gong,Hao Wu,Lilan Peng,Nan Wang,Qingsong Wen,Xian Wu,Kun Wang,Xibin Zhao*

Main category: cs.LG

TL;DR: This study introduces HiGO, a hierarchical graph-based model for continuous-time wildfire prediction, outperforming baselines on the SeasFire Cube dataset.


<details>
  <summary>Details</summary>
Motivation: Wildfires significantly impact the Earth system, but predicting their dynamics on large timescales is challenging. Deep learning's potential for this task remains underexplored.

Method: HiGO, the proposed framework, represents the Earth system as a multi-level graph hierarchy with adaptive message passing for feature fusion and uses GNN-parameterized Neural ODE modules to learn continuous dynamics.

Result: HiGO demonstrates superior performance compared to state-of-the-art models in long-range wildfire forecasting on the SeasFire Cube dataset.

Conclusion: HiGO improves continuous-time, multi-scale wildfire modeling and shows potential for real-world wildfire forecasting applications.

Abstract: Wildfires, as an integral component of the Earth system, are governed by a complex interplay of atmospheric, oceanic, and terrestrial processes spanning a vast range of spatiotemporal scales. Modeling their global activity on large timescales is therefore a critical yet challenging task. While deep learning has recently achieved significant breakthroughs in global weather forecasting, its potential for global wildfire behavior prediction remains underexplored. In this work, we reframe this problem and introduce the Hierarchical Graph ODE (HiGO), a novel framework designed to learn the multi-scale, continuous-time dynamics of wildfires. Specifically, we represent the Earth system as a multi-level graph hierarchy and propose an adaptive filtering message passing mechanism for both intra- and inter-level information flow, enabling more effective feature extraction and fusion. Furthermore, we incorporate GNN-parameterized Neural ODE modules at multiple levels to explicitly learn the continuous dynamics inherent to each scale. Through extensive experiments on the SeasFire Cube dataset, we demonstrate that HiGO significantly outperforms state-of-the-art baselines on long-range wildfire forecasting. Moreover, its continuous-time predictions exhibit strong observational consistency, highlighting its potential for real-world applications.

</details>


### [408] [Utilizing Earth Foundation Models to Enhance the Simulation Performance of Hydrological Models with AlphaEarth Embeddings](https://arxiv.org/abs/2601.01558)
*Pengfei Qu,Wenyu Ouyang,Chi Zhang,Yikai Chai,Shuolong Xu,Lei Ye,Yongri Piao,Miao Zhang,Huchuan Lu*

Main category: cs.LG

TL;DR: This study explores using AlphaEarth embeddings, derived from satellite imagery, to enhance river flow prediction in ungauged basins, achieving better accuracy than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Accurately predicting river flow in ungauged basins is challenging due to the complexity of environmental variables, and traditional basin attributes are insufficient to capture this complexity.

Method: The study uses AlphaEarth Foundation embeddings derived from satellite imagery to represent basin characteristics and evaluates their effectiveness in river flow prediction models.

Result: Models using AlphaEarth embeddings outperformed those using traditional attributes, especially in ungauged basins, by identifying key environmental differences effectively.

Conclusion: Embedding-driven models utilizing satellite data can improve hydrological forecasting and model adaptability to varied landscapes.

Abstract: Predicting river flow in places without streamflow records is challenging because basins respond differently to climate, terrain, vegetation, and soils. Traditional basin attributes describe some of these differences, but they cannot fully represent the complexity of natural environments. This study examines whether AlphaEarth Foundation embeddings, which are learned from large collections of satellite images rather than designed by experts, offer a more informative way to describe basin characteristics. These embeddings summarize patterns in vegetation, land surface properties, and long-term environmental dynamics. We find that models using them achieve higher accuracy when predicting flows in basins not used for training, suggesting that they capture key physical differences more effectively than traditional attributes. We further investigate how selecting appropriate donor basins influences prediction in ungauged regions. Similarity based on the embeddings helps identify basins with comparable environmental and hydrological behavior, improving performance, whereas adding many dissimilar basins can reduce accuracy. The results show that satellite-informed environmental representations can strengthen hydrological forecasting and support the development of models that adapt more easily to different landscapes.

</details>


### [409] [The Two-Stage Decision-Sampling Hypothesis: Understanding the Emergence of Self-Reflection in RL-Trained LLMs](https://arxiv.org/abs/2601.01580)
*Zibo Zhao,Yuanting Zha,Haipeng Zhang,Xingcheng Xu*

Main category: cs.LG

TL;DR: RL post-training enhances self-reflection in Large Language Models compared to SFT, attributed to differences in gradient attribution and optimization mechanisms.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand why reinforcement learning (RL) post-training achieves better self-reflection and decision-making capabilities in AI models compared to supervised fine-tuning (SFT).

Method: The authors introduce the Gradient Attribution Property and Two-Stage Decision-Sampling (DS) Hypothesis to analyze how reward gradients and penalty structures influence model components responsible for generation and decision-making.

Result: The study reveals balanced gradient attribution in RL, which optimizes both generation and decision-making components, while SFT leads to unbalanced optimization. Empirical validation demonstrates improved generalization in RL due to enhanced decision-making capabilities.

Conclusion: RL post-training fosters better generalization and self-correction through improved decision-making, explained via theoretical gradient attribution analysis and empirical evidence.

Abstract: Self-reflection capabilities emerge in Large Language Models after RL post-training, with multi-turn RL achieving substantial gains over SFT counterparts. Yet the mechanism of how a unified optimization objective gives rise to functionally distinct capabilities of generating solutions and evaluating when to revise them remains opaque. To address this question, we introduce the Gradient Attribution Property to characterize how reward gradients distribute across policy components, formalized through the Two-Stage Decision-Sampling (DS) Hypothesis, which decomposes the policy into sampling ($π_{sample}$) for generation and decision ($π_{d}$) for verification. We prove that surrogate rewards exhibit Balanced Gradient Attribution, while SFT and KL penalties exhibit Unbalanced Gradient Attribution, with length-weighting creating asymmetric regularization that constrains $π_{sample}$ while leaving $π_{d}$ under-optimized, providing an theoretical explanation of why RL succeeds where SFT fails. We also empirically validate our theoretical predictions on arithmetic reasoning demonstrates that RL's superior generalization stems primarily from improved decision-making ($π_{d}$) rather than sampling capabilities, providing a first-principles mechanistic explanation for self-correction in thinking models.

</details>


### [410] [REE-TTT: Highly Adaptive Radar Echo Extrapolation Based on Test-Time Training](https://arxiv.org/abs/2601.01605)
*Xin Di,Xinglin Piao,Fei Wang,Guodong Jing,Yong Zhang*

Main category: cs.LG

TL;DR: The paper addresses the limitations of deep learning-based Radar Echo Extrapolation (REE) for precipitation nowcasting by introducing a novel REE-TTT model with adaptive Test-Time Training (TTT) mechanisms, significantly improving generalization and prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the poor generalization of current deep learning-based precipitation nowcasting models, which struggle with diverse regions and extreme events due to reliance on local high-quality training data and static model parameters.

Method: The proposed method, REE-TTT, incorporates an adaptive Test-Time Training mechanism with a Spatio-temporal TTT block. This block employs task-specific attention mechanisms to adapt to non-stationary meteorological data and improve precipitation feature representation.

Result: The REE-TTT model outperforms state-of-the-art models in prediction accuracy and generalization, excelling particularly in cross-regional extreme precipitation scenarios.

Conclusion: The REE-TTT model demonstrates enhanced adaptability and performance in diverse and challenging meteorological conditions, offering a significant improvement in precipitation nowcasting.

Abstract: Precipitation nowcasting is critically important for meteorological forecasting. Deep learning-based Radar Echo Extrapolation (REE) has become a predominant nowcasting approach, yet it suffers from poor generalization due to its reliance on high-quality local training data and static model parameters, limiting its applicability across diverse regions and extreme events. To overcome this, we propose REE-TTT, a novel model that incorporates an adaptive Test-Time Training (TTT) mechanism. The core of our model lies in the newly designed Spatio-temporal Test-Time Training (ST-TTT) block, which replaces the standard linear projections in TTT layers with task-specific attention mechanisms, enabling robust adaptation to non-stationary meteorological distributions and thereby significantly enhancing the feature representation of precipitation. Experiments under cross-regional extreme precipitation scenarios demonstrate that REE-TTT substantially outperforms state-of-the-art baseline models in prediction accuracy and generalization, exhibiting remarkable adaptability to data distribution shifts.

</details>


### [411] [Real Time NILM Based Power Monitoring of Identical Induction Motors Representing Cutting Machines in Textile Industry](https://arxiv.org/abs/2601.01616)
*Md Istiauk Hossain Rifat,Moin Khan,Mohammad Zunaed*

Main category: cs.LG

TL;DR: The study introduces a NILM framework using sensors and cloud platforms for textile industry energy monitoring, acknowledging both the promise and limitations of this approach.


<details>
  <summary>Details</summary>
Motivation: The energy-intensive textile industry in Bangladesh faces outdated monitoring practices, leading to inefficiencies and high costs.

Method: Developed a NILM-based system for textile machines using voltage and current sensors, Arduino Mega, ESP8266, and cloud platforms, creating a specific dataset tested on MATNILM.

Result: The system provides reasonably accurate aggregate energy data but struggles with disaggregation for identical machines, while enabling real-time monitoring via Blynk.

Conclusion: The study demonstrates NILM's potential for industrial energy monitoring but identifies challenges, suggesting future improvements including enhanced data collection and advanced deep learning techniques.

Abstract: The textile industry in Bangladesh is one of the most energy-intensive sectors, yet its monitoring practices remain largely outdated, resulting in inefficient power usage and high operational costs. To address this, we propose a real-time Non-Intrusive Load Monitoring (NILM)-based framework tailored for industrial applications, with a focus on identical motor-driven loads representing textile cutting machines. A hardware setup comprising voltage and current sensors, Arduino Mega and ESP8266 was developed to capture aggregate and individual load data, which was stored and processed on cloud platforms. A new dataset was created from three identical induction motors and auxiliary loads, totaling over 180,000 samples, to evaluate the state-of-the-art MATNILM model under challenging industrial conditions. Results indicate that while aggregate energy estimation was reasonably accurate, per-appliance disaggregation faced difficulties, particularly when multiple identical machines operated simultaneously. Despite these challenges, the integrated system demonstrated practical real-time monitoring with remote accessibility through the Blynk application. This work highlights both the potential and limitations of NILM in industrial contexts, offering insights into future improvements such as higher-frequency data collection, larger-scale datasets and advanced deep learning approaches for handling identical loads.

</details>


### [412] [Learning Resilient Elections with Adversarial GNNs](https://arxiv.org/abs/2601.01653)
*Hao Xiang Li,Yash Shah,Lorenzo Giusti*

Main category: cs.LG

TL;DR: The paper proposes a method combining neural network architecture improvements with adversarial training to enhance voting rule resilience while maximizing social welfare. It uses bipartite graphs and graph neural networks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in designing a universal voting rule that is robust to strategic voting and applicable to real-world circumstances.

Method: The paper integrates adversarial training techniques with graph neural network-based approaches to generalize the expressive capability of learned voting rules, utilizing bipartite graphs for election representation.

Result: The proposed method demonstrated improved resilience against strategic voting and maximized social welfare in tests conducted on synthetic and real-world datasets.

Conclusion: The study advances machine learning's application in real-world elections and resolves previous limitations in learning voting rules, opening new opportunities for future research.

Abstract: In the face of adverse motives, it is indispensable to achieve a consensus. Elections have been the canonical way by which modern democracy has operated since the 17th century. Nowadays, they regulate markets, provide an engine for modern recommender systems or peer-to-peer networks, and remain the main approach to represent democracy. However, a desirable universal voting rule that satisfies all hypothetical scenarios is still a challenging topic, and the design of these systems is at the forefront of mechanism design research. Automated mechanism design is a promising approach, and recent works have demonstrated that set-invariant architectures are uniquely suited to modelling electoral systems. However, various concerns prevent the direct application to real-world settings, such as robustness to strategic voting. In this paper, we generalise the expressive capability of learned voting rules, and combine improvements in neural network architecture with adversarial training to improve the resilience of voting rules while maximizing social welfare. We evaluate the effectiveness of our methods on both synthetic and real-world datasets. Our method resolves critical limitations of prior work regarding learning voting rules by representing elections using bipartite graphs, and learning such voting rules using graph neural networks. We believe this opens new frontiers for applying machine learning to real-world elections.

</details>


### [413] [Length-Aware Adversarial Training for Variable-Length Trajectories: Digital Twins for Mall Shopper Paths](https://arxiv.org/abs/2601.01663)
*He Sun,Jiwoong Shin,Ravi Dhar*

Main category: cs.LG

TL;DR: This paper introduces Length-Aware Sampling (LAS), a strategy for improving training stability in generative modeling of variable-length trajectories.


<details>
  <summary>Details</summary>
Motivation: To address instability and degraded distribution matching caused by heterogeneous trajectory lengths during mini-batch training in generative models.

Method: The authors propose LAS, which groups trajectories by length for mini-batch sampling, and integrate it into a conditional trajectory GAN alongside time-alignment losses to maintain consistency without altering the model class.

Result: Using LAS enhances distribution matching for derived variables across datasets, consistently outperforming random sampling in metrics related to GPS, education, e-commerce, and movies.

Conclusion: LAS reduces within-batch heterogeneity, improves training stability, and better captures derived-variable distributions, providing a practical solution for variable-length trajectory modeling.

Abstract: We study generative modeling of \emph{variable-length trajectories} -- sequences of visited locations/items with associated timestamps -- for downstream simulation and counterfactual analysis. A recurring practical issue is that standard mini-batch training can be unstable when trajectory lengths are highly heterogeneous, which in turn degrades \emph{distribution matching} for trajectory-derived statistics. We propose \textbf{length-aware sampling (LAS)}, a simple batching strategy that groups trajectories by length and samples batches from a single length bucket, reducing within-batch length heterogeneity (and making updates more consistent) without changing the model class. We integrate LAS into a conditional trajectory GAN with auxiliary time-alignment losses and provide (i) a distribution-level guarantee for derived variables under mild boundedness assumptions, and (ii) an IPM/Wasserstein mechanism explaining why LAS improves distribution matching by removing length-only shortcut critics and targeting within-bucket discrepancies. Empirically, LAS consistently improves matching of derived-variable distributions on a multi-mall dataset of shopper trajectories and on diverse public sequence datasets (GPS, education, e-commerce, and movies), outperforming random sampling across dataset-specific metrics.

</details>


### [414] [Who is the Winning Algorithm? Rank Aggregation for Comparative Studies](https://arxiv.org/abs/2601.01664)
*Amichai Painsky*

Main category: cs.LG

TL;DR: The paper introduces a framework to better estimate win probabilities of machine learning algorithms by utilizing complete ranking information.


<details>
  <summary>Details</summary>
Motivation: The motivation is to move beyond standard maximum likelihood approaches, which primarily count the number of wins, to incorporate more detailed ranking information and enhance evaluation of competing algorithms.

Method: The authors propose a novel conceptual framework that leverages complete rankings of algorithms' performances on benchmark datasets to estimate win probabilities for unseen datasets.

Result: The proposed approach shows significant improvements when tested on synthetic and real-world datasets, over existing methods.

Conclusion: Incorporating full ranking data provides a more accurate estimate of algorithm performance, leading to enhanced selection of the best algorithm for future tasks.

Abstract: Consider a collection of m competing machine learning algorithms. Given their performance on a benchmark of datasets, we would like to identify the best performing algorithm. Specifically, which algorithm is most likely to ``win'' (rank highest) on a future, unseen dataset. The standard maximum likelihood approach suggests counting the number of wins per each algorithm. In this work, we argue that there is much more information in the complete rankings. That is, the number of times that each algorithm finished second, third and so forth. Yet, it is not entirely clear how to effectively utilize this information for our purpose. In this work we introduce a novel conceptual framework for estimating the win probability for each of the m algorithms, given their complete rankings over a benchmark of datasets. Our proposed framework significantly improves upon currently known methods in synthetic and real-world examples.

</details>


### [415] [Adversarial Instance Generation and Robust Training for Neural Combinatorial Optimization with Multiple Objectives](https://arxiv.org/abs/2601.01665)
*Wei Liu,Yaoxin Wu,Yingqian Zhang,Thomas Bäck,Yingjie Fan*

Main category: cs.LG

TL;DR: The paper proposes a robustness-oriented DRL framework for multi-objective combinatorial optimization, introducing adversarial attacks and defense strategies to enhance solver performance on challenging and out-of-distribution instances.


<details>
  <summary>Details</summary>
Motivation: To address the limited exploration of robustness in DRL-based solvers for multi-objective combinatorial optimization problems, especially across diverse problem distributions.

Method: A unified framework is introduced with a preference-based adversarial attack for generating challenging instances and a defense strategy using hardness-aware preference selection integrated into adversarial training.

Result: Experimental results on MOCOP benchmarks demonstrated the attack method's ability to uncover solver weaknesses and the defense strategy's improvement to solver robustness and out-of-distribution generalizability.

Conclusion: The proposed approach enhances robustness and performance of neural solvers on complex and hard-to-generalize problem instances, demonstrating its effectiveness in solving diverse multi-objective combinatorial optimization problems.

Abstract: Deep reinforcement learning (DRL) has shown great promise in addressing multi-objective combinatorial optimization problems (MOCOPs). Nevertheless, the robustness of these learning-based solvers has remained insufficiently explored, especially across diverse and complex problem distributions. In this paper, we propose a unified robustness-oriented framework for preference-conditioned DRL solvers for MOCOPs. Within this framework, we develop a preference-based adversarial attack to generate hard instances that expose solver weaknesses, and quantify the attack impact by the resulting degradation on Pareto-front quality. We further introduce a defense strategy that integrates hardness-aware preference selection into adversarial training to reduce overfitting to restricted preference regions and improve out-of-distribution performance. The experimental results on multi-objective traveling salesman problem (MOTSP), multi-objective capacitated vehicle routing problem (MOCVRP), and multi-objective knapsack problem (MOKP) verify that our attack method successfully learns hard instances for different solvers. Furthermore, our defense method significantly strengthens the robustness and generalizability of neural solvers, delivering superior performance on hard or out-of-distribution instances.

</details>


### [416] [HeurekaBench: A Benchmarking Framework for AI Co-scientist](https://arxiv.org/abs/2601.01678)
*Siba Smarak Panigrahi,Jovana Videnović,Maria Brbić*

Main category: cs.LG

TL;DR: The paper introduces HeurekaBench, a framework for creating benchmarks with open-ended scientific questions, focusing on realistic scenarios in experimental datasets, specifically in single-cell biology.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of evaluating LLM-based reasoning systems effectively within realistic, end-to-end scientific research scenarios spanning data analysis to insight generation.

Method: The paper proposes a semi-automated pipeline leveraging multiple LLMs to create benchmarks, which are verified against existing findings. It instantiates this framework in single-cell biology by developing sc-HeurekaBench and testing agents' performance.

Result: Using sc-HeurekaBench, the authors identified that adding a critic module improved agent responses by up to 22% for open-source systems, significantly narrowing the gap with closed-source systems.

Conclusion: HeurekaBench establishes a robust approach for rigorous, end-to-end evaluation of scientific agents, rooted in real-world scientific workflows and generating insights applicable to design improvements of agentic systems.

Abstract: LLM-based reasoning models have enabled the development of agentic systems that act as co-scientists, assisting in multi-step scientific analysis. However, evaluating these systems is challenging, as it requires realistic, end-to-end research scenarios that integrate data analysis, interpretation, and the generation of new insights from the experimental data. To address this limitation, we introduce HeurekaBench, a framework to create benchmarks with exploratory, open-ended research questions for experimental datasets. Each such question is grounded in a scientific study and its corresponding code repository, and is created using a semi-automated pipeline that leverages multiple LLMs to extract insights and generate candidate workflows, which are then verified against reported findings. We instantiate the framework in single-cell biology to obtain sc-HeurekaBench benchmark and use it to compare state-of-the-art single-cell agents. We further showcase the benefits of our benchmark for quantitatively analyzing current design choices in agentic systems. We find that the addition of a critic module can improve ill-formed responses for open-source LLM-based agents by up to 22% and close the gap with their closed-source counterparts. Overall, HeurekaBench sets a path toward rigorous, end-to-end evaluation of scientific agents, grounding benchmark construction in real scientific workflows.

</details>


### [417] [DiMEx: Breaking the Cold Start Barrier in Data-Free Model Extraction via Latent Diffusion Priors](https://arxiv.org/abs/2601.01688)
*Yash Thesia,Meera Suthar*

Main category: cs.LG

TL;DR: Model stealing attacks are a major threat to MLaaS. DiMEx uses pretrained Latent Diffusion Models and Bayesian optimization to bypass cold start problems in DFME, significantly improving query efficiency and fidelity. HSE defense counters these attacks by exploiting latent-space trajectories.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of GAN-based model stealing attacks due to the Cold Start problem and to enhance query-based model replication mechanisms.

Method: DiMEx utilizes pre-trained Latent Diffusion Models and Random Embedding Bayesian Optimization (REMBO) for rapid and effective data-free model extraction. The defense is Hybrid Stateful Ensemble (HSE), aiming to track and mitigate attacks based on latent-space trajectories.

Result: DiMEx achieves 52.1 percent agreement on SVHN with just 2,000 queries, outperforming GAN-based approaches by over 16 percent. HSE reduces attack success rates to 21.6 percent with minimal impact on latency.

Conclusion: DiMEx significantly advances the efficiency of model stealing attacks, posing greater risks to MLaaS. However, the proposed HSE defense effectively mitigates such threats by identifying latent-space optimization patterns.

Abstract: Model stealing attacks pose an existential threat to Machine Learning as a Service (MLaaS), allowing adversaries to replicate proprietary models for a fraction of their training cost. While Data-Free Model Extraction (DFME) has emerged as a stealthy vector, it remains fundamentally constrained by the "Cold Start" problem: GAN-based adversaries waste thousands of queries converging from random noise to meaningful data. We propose DiMEx, a framework that weaponizes the rich semantic priors of pre-trained Latent Diffusion Models to bypass this initialization barrier entirely. By employing Random Embedding Bayesian Optimization (REMBO) within the generator's latent space, DiMEx synthesizes high-fidelity queries immediately, achieving 52.1 percent agreement on SVHN with just 2,000 queries - outperforming state-of-the-art GAN baselines by over 16 percent. To counter this highly semantic threat, we introduce the Hybrid Stateful Ensemble (HSE) defense, which identifies the unique "optimization trajectory" of latent-space attacks. Our results demonstrate that while DiMEx evades static distribution detectors, HSE exploits this temporal signature to suppress attack success rates to 21.6 percent with negligible latency.

</details>


### [418] [Enhanced Multi-model Online Conformal Prediction](https://arxiv.org/abs/2601.01692)
*Erfan Hajihashemi,Yanning Shen*

Main category: cs.LG

TL;DR: The paper presents a novel multi-model online conformal prediction algorithm to improve prediction efficiency and reduce computational complexity by selecting effective models through a bipartite graph.


<details>
  <summary>Details</summary>
Motivation: Existing conformal prediction frameworks rely on a single model or multiple candidates, but these approaches face performance and computational challenges in dynamic environments.

Method: The algorithm generates a bipartite graph at each time step to identify and select effective models for constructing prediction sets.

Result: Experiments demonstrate improved prediction set efficiency and reduced computational costs compared to existing techniques.

Conclusion: The new algorithm successfully addresses the limitations of previous methods by combining effective model selection and computational efficiency, proving its utility in practical scenarios.

Abstract: Conformal prediction is a framework for uncertainty quantification that constructs prediction sets for previously unseen data, guaranteeing coverage of the true label with a specified probability. However, the efficiency of these prediction sets, measured by their size, depends on the choice of the underlying learning model. Relying on a single fixed model may lead to suboptimal performance in online environments, as a single model may not consistently perform well across all time steps. To mitigate this, prior work has explored selecting a model from a set of candidates. However, this approach becomes computationally expensive as the number of candidate models increases. Moreover, poorly performing models in the set may also hinder the effectiveness. To tackle this challenge, this work develops a novel multi-model online conformal prediction algorithm that reduces computational complexity and improves prediction efficiency. At each time step, a bipartite graph is generated to identify a subset of effective models, from which a model is selected to construct the prediction set. Experiments demonstrate that our method outperforms existing multi-model conformal prediction techniques in terms of both prediction set size and computational efficiency.

</details>


### [419] [Digital Twin-Driven Communication-Efficient Federated Anomaly Detection for Industrial IoT](https://arxiv.org/abs/2601.01701)
*Mohammed Ayalew Belay,Adil Rasheed,Pierluigi Salvo Rossi*

Main category: cs.LG

TL;DR: The paper proposes a set of digital twin-integrated federated learning (DTFL) methods to improve anomaly detection performance in industrial systems while addressing constraints like limited data and privacy concerns.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in anomaly detection for industrial systems such as reliance on limited real sensor datasets, high false alarm rates, and data privacy concerns.

Method: Five approaches are introduced: Digital Twin-Based Meta-Learning (DTML), Federated Parameter Fusion (FPF), Layer-wise Parameter Exchange (LPE), Cyclic Weight Adaptation (CWA), and Digital Twin Knowledge Distillation (DTKD). These methods combine synthetic and real knowledge, focusing on enhancing generalization while reducing communication overhead.

Result: CWA achieved a target accuracy of 80% in 33 rounds, significantly faster than other methods. DTFL methods demonstrated up to 62% enhanced communication efficiency compared to DTML and 31% compared to LPE.

Conclusion: Integrating digital twin knowledge into federated learning effectively accelerates anomaly detection convergence while maintaining data privacy and achieving communication efficiency in industrial systems.

Abstract: Anomaly detection is increasingly becoming crucial for maintaining the safety, reliability, and efficiency of industrial systems. Recently, with the advent of digital twins and data-driven decision-making, several statistical and machine-learning methods have been proposed. However, these methods face several challenges, such as dependence on only real sensor datasets, limited labeled data, high false alarm rates, and privacy concerns. To address these problems, we propose a suite of digital twin-integrated federated learning (DTFL) methods that enhance global model performance while preserving data privacy and communication efficiency. Specifically, we present five novel approaches: Digital Twin-Based Meta-Learning (DTML), Federated Parameter Fusion (FPF), Layer-wise Parameter Exchange (LPE), Cyclic Weight Adaptation (CWA), and Digital Twin Knowledge Distillation (DTKD). Each method introduces a unique mechanism to combine synthetic and real-world knowledge, balancing generalization with communication overhead. We conduct an extensive experiment using a publicly available cyber-physical anomaly detection dataset. For a target accuracy of 80%, CWA reaches the target in 33 rounds, FPF in 41 rounds, LPE in 48 rounds, and DTML in 87 rounds, whereas the standard FedAvg baseline and DTKD do not reach the target within 100 rounds. These results highlight substantial communication-efficiency gains (up to 62% fewer rounds than DTML and 31% fewer than LPE) and demonstrate that integrating DT knowledge into FL accelerates convergence to operationally meaningful accuracy thresholds for IIoT anomaly detection.

</details>


### [420] [Entropy-Aligned Decoding of LMs for Better Writing and Reasoning](https://arxiv.org/abs/2601.01714)
*Kareem Ahmed,Sameer Singh*

Main category: cs.LG

TL;DR: The paper introduces EPIC, a decoding approach for language models that enhances generation quality by considering future trajectory entropy and aligning it to data uncertainty.


<details>
  <summary>Details</summary>
Motivation: Despite advances in language models, traditional decoding methods often result in low-quality outputs that are repetitive and incoherent due to myopic heuristics. A more robust decoding method is needed.

Method: EPIC uses Entropy-Aware Lazy Gumbel-Max sampling to regulate uncertainty at each generation step, aligning the sampling process with the data's entropy.

Result: Experiments show that EPIC improves preference win-rates, generates more diverse content, creates faithful summaries in creative and summarization tasks, and improves outputs even in mathematical reasoning.

Conclusion: EPIC is a hyperparameter-free, efficient, and exact decoding approach that improves the quality of language model outputs across various tasks.

Abstract: Language models (LMs) are trained on billions of tokens in an attempt to recover the true language distribution. Still, vanilla random sampling from LMs yields low quality generations. Decoding algorithms attempt to restrict the LM distribution to a set of high-probability continuations, but rely on greedy heuristics that introduce myopic distortions, yielding sentences that are homogeneous, repetitive and incoherent. In this paper, we introduce EPIC, a hyperparameter-free decoding approach that incorporates the entropy of future trajectories into LM decoding. EPIC explicitly regulates the amount of uncertainty expressed at every step of generation, aligning the sampling distribution's entropy to the aleatoric (data) uncertainty. Through Entropy-Aware Lazy Gumbel-Max sampling, EPIC manages to be exact, while also being efficient, requiring only a sublinear number of entropy evaluations per step. Unlike current baselines, EPIC yields sampling distributions that are empirically well-aligned with the entropy of the underlying data distribution. Across creative writing and summarization tasks, EPIC consistently improves LM-as-judge preference win-rates over widely used decoding strategies. These preference gains are complemented by automatic metrics, showing that EPIC produces more diverse generations and more faithful summaries. We also evaluate EPIC on mathematical reasoning, where it outperforms all baselines.

</details>


### [421] [Context-Free Recognition with Transformers](https://arxiv.org/abs/2601.01754)
*Selim Jerad,Anej Svete,Sophie Hao,Ryan Cotterell,William Merrill*

Main category: cs.LG

TL;DR: This paper studies how transformers can recognize context-free languages (CFLs) and their limitations. The authors establish theoretical insights and offer practical approaches under constraints.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate whether transformers, particularly looped ones, can recognize context-free languages, given prior knowledge that standard transformers struggle with such grammatical structures.

Method: The authors derive theoretical results, proving that looped transformers with $O(log n)$ layers and substantial padding tokens ($O(n^6)$) can recognize CFLs. They also explore tractable scenarios, such as unambiguous CFLs, which require significantly less padding ($O(n^3)$). Experiments validate these theoretical insights.

Result: Looped transformers can theoretically recognize all CFLs using $O(n^6)$ padding tokens. For unambiguous CFLs, this requirement drops to $O(n^3)$. Empirical experiments support the enhanced capabilities of looping layers.

Conclusion: While recognizing general CFLs with transformers remains computationally expensive due to large padding requirements, imposing constraints such as unambiguity offers a path toward efficient recognition. This work advances understanding of transformers' grammatical capabilities.

Abstract: Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill & Sabharwal (2024) show that $\mathcal{O}(\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\mathcal{O}(\log n)$ looping layers and $\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.

</details>


### [422] [UnPII: Unlearning Personally Identifiable Information with Quantifiable Exposure Risk](https://arxiv.org/abs/2601.01786)
*Intae Jeon,Yujeong Kwon,Hyungjoon Koo*

Main category: cs.LG

TL;DR: The paper introduces UnPII, a selective unlearning method focusing on prioritizing PII risks for sensitive data privacy, achieving improved accuracy, utility, and generalizability.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the increasing adoption of Large Language Models (LLMs) in privacy-sensitive domains, amidst regulatory requirements like GDPR demanding reliable and cost-efficient personal data removal.

Method: The authors propose UnPII powered by a PII risk index (PRI) that quantifies privacy risks and enables tailored unlearning integrated with existing algorithms.

Result: UnPII achieves superior performance in terms of accuracy (+11.8%), utility (+6.3%), and generalizability (+12.4%), with manageable computational overhead (27.5%).

Conclusion: UnPII presents a privacy-focused unlearning approach balancing regulatory compliance and operational efficiency, paving the way for nuanced privacy and business risk management.

Abstract: The ever-increasing adoption of Large Language Models in critical sectors like finance, healthcare, and government raises privacy concerns regarding the handling of sensitive Personally Identifiable Information (PII) during training. In response, regulations such as European Union's General Data Protection Regulation (GDPR) mandate the deletion of PII upon requests, underscoring the need for reliable and cost-effective data removal solutions. Machine unlearning has emerged as a promising direction for selectively forgetting data points. However, existing unlearning techniques typically apply a uniform forgetting strategy that neither accounts for the varying privacy risks posed by different PII attributes nor reflects associated business risks. In this work, we propose UnPII, the first PII-centric unlearning approach that prioritizes forgetting based on the risk of individual or combined PII attributes. To this end, we introduce the PII risk index (PRI), a composite metric that incorporates multiple dimensions of risk factors: identifiability, sensitivity, usability, linkability, permanency, exposability, and compliancy. The PRI enables a nuanced evaluation of privacy risks associated with PII exposures and can be tailored to align with organizational privacy policies. To support realistic assessment, we systematically construct a synthetic PII dataset (e.g., 1,700 PII instances) that simulates realistic exposure scenarios. UnPII seamlessly integrates with established unlearning algorithms, such as Gradient Ascent, Negative Preference Optimization, and Direct Preference Optimization, without modifying their underlying principles. Our experimental results demonstrate that UnPII achieves the improvements of accuracy up to 11.8%, utility up to 6.3%, and generalizability up to 12.4%, respectively, while incurring a modest fine-tuning overhead of 27.5% on average during unlearning.

</details>


### [423] [HyperCLOVA X 8B Omni](https://arxiv.org/abs/2601.01792)
*NAVER Cloud HyperCLOVA X Team*

Main category: cs.LG

TL;DR: HyperCLOVA X 8B Omni is an omnimodal model capable of handling text, audio, and vision as both inputs and outputs, unifying multimodal operations in a single framework.


<details>
  <summary>Details</summary>
Motivation: To develop a practical model capable of integrating text, audio, and vision for enhanced multimodal understanding and generation.

Method: Multimodal unification via shared next-token prediction interface and continuous embeddings injected by vision and audio encoders.

Result: The model demonstrated competitive results across various multimodal input-output combinations in Korean and English languages.

Conclusion: HyperCLOVA X 8B Omni offers significant potential for multimodal research and use cases, with open-weight availability supporting further exploration and deployment.

Abstract: In this report, we present HyperCLOVA X 8B Omni, the first any-to-any omnimodal model in the HyperCLOVA X family that supports text, audio, and vision as both inputs and outputs. By consolidating multimodal understanding and generation into a single model rather than separate modality-specific pipelines, HyperCLOVA X 8B Omni serves as an 8B-scale omni-pathfinding point toward practical any-to-any omni assistants. At a high level, the model unifies modalities through a shared next-token prediction interface over an interleaved multimodal sequence, while vision and audio encoders inject continuous embeddings for fine-grained understanding and grounding. Empirical evaluations demonstrate competitive performance against comparably sized models across diverse input-output combinations spanning text, audio, and vision, in both Korean and English. We anticipate that the open-weight release of HyperCLOVA X 8B Omni will support a wide range of research and deployment scenarios.

</details>


### [424] [Distributed Federated Learning by Alternating Periods of Training](https://arxiv.org/abs/2601.01793)
*Shamik Bhattacharyya,Rachel Kalpana Kalaimani*

Main category: cs.LG

TL;DR: The paper introduces a distributed federated learning (DFL) algorithm to mitigate the scalability and fault-tolerance limitations of traditional federated learning reliant on a single central server.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges of scalability, risk of single server failure, and improve fault-tolerance in federated learning.

Method: The proposed method involves multiple servers communicating with each other and clients in a decentralized system. The DFL algorithm alternates between local client training and global training among servers.

Result: Theoretical analysis demonstrates that the DFL algorithm achieves convergence among servers to a shared model within a small tolerance of the ideal model. Numerical simulations confirm this effectiveness.

Conclusion: The proposed decentralized DFL algorithm enhances federated learning's fault-tolerance and scalability while retaining its privacy-centric nature.

Abstract: Federated learning is a privacy-focused approach towards machine learning where models are trained on client devices with locally available data and aggregated at a central server. However, the dependence on a single central server is challenging in the case of a large number of clients and even poses the risk of a single point of failure. To address these critical limitations of scalability and fault-tolerance, we present a distributed approach to federated learning comprising multiple servers with inter-server communication capabilities. While providing a fully decentralized approach, the designed framework retains the core federated learning structure where each server is associated with a disjoint set of clients with server-client communication capabilities. We propose a novel DFL (Distributed Federated Learning) algorithm which uses alternating periods of local training on the client data followed by global training among servers. We show that the DFL algorithm, under a suitable choice of parameters, ensures that all the servers converge to a common model value within a small tolerance of the ideal model, thus exhibiting effective integration of local and global training models. Finally, we illustrate our theoretical claims through numerical simulations.

</details>


### [425] [Sparse Threats, Focused Defense: Criticality-Aware Robust Reinforcement Learning for Safe Autonomous Driving](https://arxiv.org/abs/2601.01800)
*Qi Wei,Junchao Fan,Zhao Yang,Jianhua Wang,Jingkai Mao,Xiaolin Chang*

Main category: cs.LG

TL;DR: The paper introduces CARRL, an adversarial training framework to enhance reinforcement learning robustness in autonomous driving by emphasizing sparse, safety-critical scenarios.


<details>
  <summary>Details</summary>
Motivation: The study addresses reinforcement learning's vulnerability to perturbations in real-world autonomous driving, and existing adversarial training's inadequacies due to overlooking asymmetry and rare safety-critical risks.

Method: CARRL combines a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA) in a general-sum game setting. REA identifies sparse safety-critical situations, while RTRA uses dual replay buffers to balance safety and driving efficiency.

Result: CARRL demonstrates at least a 22.66% reduction in collision rates compared to other state-of-the-art methods.

Conclusion: The proposed method effectively tackles the challenge of enhancing robustness in reinforcement learning for autonomous driving, highlighting the importance of considering sparse, safety-critical scenarios.

Abstract: Reinforcement learning (RL) has shown considerable potential in autonomous driving (AD), yet its vulnerability to perturbations remains a critical barrier to real-world deployment. As a primary countermeasure, adversarial training improves policy robustness by training the AD agent in the presence of an adversary that deliberately introduces perturbations. Existing approaches typically model the interaction as a zero-sum game with continuous attacks. However, such designs overlook the inherent asymmetry between the agent and the adversary and then fail to reflect the sparsity of safety-critical risks, rendering the achieved robustness inadequate for practical AD scenarios. To address these limitations, we introduce criticality-aware robust RL (CARRL), a novel adversarial training approach for handling sparse, safety-critical risks in autonomous driving. CARRL consists of two interacting components: a risk exposure adversary (REA) and a risk-targeted robust agent (RTRA). We model the interaction between the REA and RTRA as a general-sum game, allowing the REA to focus on exposing safety-critical failures (e.g., collisions) while the RTRA learns to balance safety with driving efficiency. The REA employs a decoupled optimization mechanism to better identify and exploit sparse safety-critical moments under a constrained budget. However, such focused attacks inevitably result in a scarcity of adversarial data. The RTRA copes with this scarcity by jointly leveraging benign and adversarial experiences via a dual replay buffer and enforces policy consistency under perturbations to stabilize behavior. Experimental results demonstrate that our approach reduces the collision rate by at least 22.66\% across all cases compared to state-of-the-art baseline methods.

</details>


### [426] [Moments Matter:Stabilizing Policy Optimization using Return Distributions](https://arxiv.org/abs/2601.01803)
*Dennis Jabs,Aditya Mohan,Marius Lindauer*

Main category: cs.LG

TL;DR: The paper focuses on improving the stability of policies in Deep RL by using a moment-based correction to reduce instability due to noisy parameter updates.


<details>
  <summary>Details</summary>
Motivation: Stability issues arise in Deep RL as agents may achieve similar returns but with significantly different behaviors. This is caused by both environmental and algorithmic noise, particularly in continuous tasks where even small instabilities can cause significant problems.

Method: The proposed method uses a distributional critic to model the return distribution and incorporates higher-order moments (skewness and kurtosis) into the PPO advantage function to penalize extreme tail behaviors, reducing variability in policy updates.

Result: The method demonstrates improved stability (by up to 75%) in the Walker2D environment, while maintaining comparable evaluation returns.

Conclusion: Incorporating moment-based corrections into RL training processes can significantly reduce policy instability without loss of performance, making it particularly valuable for environments prone to instability.

Abstract: Deep Reinforcement Learning (RL) agents often learn policies that achieve the same episodic return yet behave very differently, due to a combination of environmental (random transitions, initial conditions, reward noise) and algorithmic (minibatch selection, exploration noise) factors. In continuous control tasks, even small parameter shifts can produce unstable gaits, complicating both algorithm comparison and real-world transfer. Previous work has shown that such instability arises when policy updates traverse noisy neighborhoods and that the spread of post-update return distribution $R(θ)$, obtained by repeatedly sampling minibatches, updating $θ$, and measuring final returns, is a useful indicator of this noise. Although explicitly constraining the policy to maintain a narrow $R(θ)$ can improve stability, directly estimating $R(θ)$ is computationally expensive in high-dimensional settings. We propose an alternative that takes advantage of environmental stochasticity to mitigate update-induced variability. Specifically, we model state-action return distribution through a distributional critic and then bias the advantage function of PPO using higher-order moments (skewness and kurtosis) of this distribution. By penalizing extreme tail behaviors, our method discourages policies from entering parameter regimes prone to instability. We hypothesize that in environments where post-update critic values align poorly with post-update returns, standard PPO struggles to produce a narrow $R(θ)$. In such cases, our moment-based correction narrows $R(θ)$, improving stability by up to 75% in Walker2D, while preserving comparable evaluation returns.

</details>


### [427] [RealPDEBench: A Benchmark for Complex Physical Systems with Real-World Data](https://arxiv.org/abs/2601.01829)
*Peiyan Hu,Haodong Feng,Hongyuan Liu,Tongtong Yan,Wenhao Deng,Tianrun Gao,Rong Zheng,Haoren Zheng,Chenglei Yu,Chuanrui Wang,Kaiwen Li,Zhi-Ming Ma,Dezhi Zhou,Xingcai Lu,Dixia Fan,Tailin Wu*

Main category: cs.LG

TL;DR: The paper introduces RealPDEBench, a benchmark integrating real-world data and numerical simulations for scientific ML, and evaluates performance discrepancies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of limited real-world data availability for scientific ML, which hinders sim-to-real transfer and real-world deployment of models.

Method: The authors developed RealPDEBench, including five datasets, three tasks, eight metrics, and ten baseline comparisons for scientific ML evaluation and bridging simulated-to-real-world gaps.

Result: The experiments reveal notable discrepancies between simulation and reality but demonstrate that pretraining models with simulated data enhances accuracy and convergence.

Conclusion: The RealPDEBench contributes valuable real-world datasets and evaluation tools, facilitating advancement in bridging the sim-to-real gap in scientific ML.

Abstract: Predicting the evolution of complex physical systems remains a central problem in science and engineering. Despite rapid progress in scientific Machine Learning (ML) models, a critical bottleneck is the lack of expensive real-world data, resulting in most current models being trained and validated on simulated data. Beyond limiting the development and evaluation of scientific ML, this gap also hinders research into essential tasks such as sim-to-real transfer. We introduce RealPDEBench, the first benchmark for scientific ML that integrates real-world measurements with paired numerical simulations. RealPDEBench consists of five datasets, three tasks, eight metrics, and ten baselines. We first present five real-world measured datasets with paired simulated datasets across different complex physical systems. We further define three tasks, which allow comparisons between real-world and simulated data, and facilitate the development of methods to bridge the two. Moreover, we design eight evaluation metrics, spanning data-oriented and physics-oriented metrics, and finally benchmark ten representative baselines, including state-of-the-art models, pretrained PDE foundation models, and a traditional method. Experiments reveal significant discrepancies between simulated and real-world data, while showing that pretraining with simulated data consistently improves both accuracy and convergence. In this work, we hope to provide insights from real-world data, advancing scientific ML toward bridging the sim-to-real gap and real-world deployment. Our benchmark, datasets, and instructions are available at https://realpdebench.github.io/.

</details>


### [428] [FAROS: Robust Federated Learning with Adaptive Scaling against Backdoor Attacks](https://arxiv.org/abs/2601.01833)
*Chenyu Hu,Qiming Hu,Sinan Chen,Nianyu Li,Mingyue Zhang,Jialong Li*

Main category: cs.LG

TL;DR: The paper introduces FAROS, an advanced FL framework that counters backdoor attacks through dynamic defense sensitivity and robust client selection.


<details>
  <summary>Details</summary>
Motivation: The need to enhance the robustness of Federated Learning (FL) against backdoor attacks, which traditional defenses with static parameters fail to adequately address.

Method: The proposed method, FAROS, integrates Adaptive Differential Scaling (ADS) for dynamic sensitivity adjustment and Robust Core-set Computing (RCC) to mitigate single-point failures.

Result: Experiments demonstrate improved resistance to backdoor attacks and maintained accuracy in various datasets, models, and attack scenarios compared to existing defenses.

Conclusion: FAROS significantly enhances the security and reliability of Federated Learning by overcoming the limitations of fixed parameter defenses.

Abstract: Federated Learning (FL) enables multiple clients to collaboratively train a shared model without exposing local data. However, backdoor attacks pose a significant threat to FL. These attacks aim to implant a stealthy trigger into the global model, causing it to mislead on inputs that possess a specific trigger while functioning normally on benign data. Although pre-aggregation detection is a main defense direction, existing state-of-the-art defenses often rely on fixed defense parameters. This reliance makes them vulnerable to single-point-of-failure risks, rendering them less effective against sophisticated attackers. To address these limitations, we propose FAROS, an enhanced FL framework that incorporates Adaptive Differential Scaling (ADS) and Robust Core-set Computing (RCC). The ADS mechanism adjusts the defense's sensitivity dynamically, based on the dispersion of uploaded gradients by clients in each round. This allows it to counter attackers who strategically shift between stealthiness and effectiveness. Furthermore, the RCC effectively mitigates the risk of single-point failure by computing the centroid of a core set comprising clients with the highest confidence. We conducted extensive experiments across various datasets, models, and attack scenarios. The results demonstrate that our method outperforms current defenses in both attack success rate and main task accuracy.

</details>


### [429] [Output Embedding Centering for Stable LLM Pretraining](https://arxiv.org/abs/2601.02031)
*Felix Stollenwerk,Anna Lokrantz,Niclas Hertzberg*

Main category: cs.LG

TL;DR: Pretraining large language models faces challenges like training instabilities, particularly output logit divergence. A new method, output embedding centering (OEC), is proposed and proves effective in resolving these issues.


<details>
  <summary>Details</summary>
Motivation: Large language models are expensive to train and prone to certain instabilities, like output logit divergence, especially when using large learning rates. Addressing this problem is critical for efficient pretraining.

Method: This paper introduces output embedding centering (OEC) as a mitigation strategy, implemented as μ-centering (deterministic operation) or μ-loss (regularization method).

Result: Experiments show OEC methods (μ-centering and μ-loss) outperform z-loss in training stability and sensitivity to learning rates and require less hyperparameter tuning.

Conclusion: OEC offers a superior alternative to z-loss by directly addressing the cause of output logit divergence, improving training stability and flexibility during pretraining of large language models.

Abstract: Pretraining of large language models is not only expensive but also prone to certain training instabilities. A specific instability that often occurs for large learning rates at the end of training is output logit divergence. The most widely used mitigation strategy, z-loss, merely addresses the symptoms rather than the underlying cause of the problem. In this paper, we analyze the instability from the perspective of the output embeddings' geometry and identify its cause. Based on this, we propose output embedding centering (OEC) as a new mitigation strategy, and prove that it suppresses output logit divergence. OEC can be implemented in two different ways, as a deterministic operation called μ-centering, or a regularization method called μ-loss. Our experiments show that both variants outperform z-loss in terms of training stability and learning rate sensitivity. In particular, they ensure that training converges even for large learning rates when z-loss fails. Furthermore, we find that μ-loss is significantly less sensitive to regularization hyperparameter tuning than z-loss.

</details>


### [430] [High-Order Epistasis Detection Using Factorization Machine with Quadratic Optimization Annealing and MDR-Based Evaluation](https://arxiv.org/abs/2601.01860)
*Shuta Kikuchi,Shu Tanaka*

Main category: cs.LG

TL;DR: The paper presents a computationally efficient method using Factorization Machine with Quadratic Optimization Annealing (FMQA) for detecting high-order epistasis in genetics.


<details>
  <summary>Details</summary>
Motivation: High-order epistasis detection is a key challenge due to the massive computational demands of evaluating combinations of genetic loci.

Method: The authors redefine epistasis detection as a black-box optimization problem, integrated with MDR and FMQA for computational efficiency.

Result: Their proposed FMQA-based method identified predefined epistasis patterns successfully, regardless of interaction order or genetic loci size.

Conclusion: The method is effective and significantly reduces computational effort for detecting high-order epistasis.

Abstract: Detecting high-order epistasis is a fundamental challenge in genetic association studies due to the combinatorial explosion of candidate locus combinations. Although multifactor dimensionality reduction (MDR) is a widely used method for evaluating epistasis, exhaustive MDR-based searches become computationally infeasible as the number of loci or the interaction order increases. In this paper, we define the epistasis detection problem as a black-box optimization problem and solve it with a factorization machine with quadratic optimization annealing (FMQA). We propose an efficient epistasis detection method based on FMQA, in which the classification error rate (CER) computed by MDR is used as a black-box objective function. Experimental evaluations were conducted using simulated case-control datasets with predefined high-order epistasis. The results demonstrate that the proposed method successfully identified ground-truth epistasis across various interaction orders and the numbers of genetic loci within a limited number of iterations. These results indicate that the proposed method is effective and computationally efficient for high-order epistasis detection.

</details>


### [431] [Safety at One Shot: Patching Fine-Tuned LLMs with A Single Instance](https://arxiv.org/abs/2601.01887)
*Jiawen Zhang,Lipeng He,Kejia Chen,Jian Lou,Jian Liu,Xiaohu Yang,Ruoxi Jia*

Main category: cs.LG

TL;DR: The paper demonstrates recovery of safety alignment in large language models with just one safety example, maintaining model utility and minimizing costs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the compromise in safety of LLMs during fine-tuning, avoiding the high computational overhead and utility degradation caused by traditional safety alignment methods.

Method: The proposed method uses only a single safety example to recover safety alignment, elucidating the process through discovering the low-rank structure of the safety gradient.

Result: Safety alignment was restored effectively, irrespective of model size or harmful fine-tuning examples, achieving convergence within a few epochs.

Conclusion: The findings validate efficient and cost-effective recovery of LLM safety alignment across various models and datasets, proving its generality and practicality.

Abstract: Fine-tuning safety-aligned large language models (LLMs) can substantially compromise their safety. Previous approaches require many safety samples or calibration sets, which not only incur significant computational overhead during realignment but also lead to noticeable degradation in model utility. Contrary to this belief, we show that safety alignment can be fully recovered with only a single safety example, without sacrificing utility and at minimal cost. Remarkably, this recovery is effective regardless of the number of harmful examples used in fine-tuning or the size of the underlying model, and convergence is achieved within just a few epochs. Furthermore, we uncover the low-rank structure of the safety gradient, which explains why such efficient correction is possible. We validate our findings across five safety-aligned LLMs and multiple datasets, demonstrating the generality of our approach.

</details>


### [432] [Entropy-Adaptive Fine-Tuning: Resolving Confident Conflicts to Mitigate Forgetting](https://arxiv.org/abs/2601.02151)
*Muxi Diao,Lele Yang,Wuxuan Gong,Yutong Zhang,Zhonghao Yan,Yufei Han,Kongming Liang,Weiran Xu,Zhanyu Ma*

Main category: cs.LG

TL;DR: The paper presents Entropy-Adaptive Fine-Tuning (EAFT) as a method to improve domain adaptation by addressing the challenges of catastrophic forgetting in supervised fine-tuning (SFT), specifically by accounting for knowledge conflicts and uncertainties.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the limitations of supervised fine-tuning (SFT) in domain adaptation, particularly its tendency to cause catastrophic forgetting and conflicts with the model's internal beliefs.

Method: The study proposes Entropy-Adaptive Fine-Tuning (EAFT), which uses token-level entropy to differentiate between epistemic uncertainty and knowledge conflicts, allowing selective gradient updates based on the nature of the conflict.

Result: EAFT demonstrated consistent downstream performance on models like Qwen and GLM across various domains while reducing the loss of general capabilities compared to standard SFT.

Conclusion: Entropy-Adaptive Fine-Tuning offers a promising solution to mitigate catastrophic forgetting by respecting the model's internal confidence and selectively handling conflicts, maintaining both domain-specific and general performance.

Abstract: Supervised Fine-Tuning (SFT) is the standard paradigm for domain adaptation, yet it frequently incurs the cost of catastrophic forgetting. In sharp contrast, on-policy Reinforcement Learning (RL) effectively preserves general capabilities. We investigate this discrepancy and identify a fundamental distributional gap: while RL aligns with the model's internal belief, SFT forces the model to fit external supervision. This mismatch often manifests as "Confident Conflicts" tokens characterized by low probability but low entropy. In these instances, the model is highly confident in its own prediction but is forced to learn a divergent ground truth, triggering destructive gradient updates. To address this, we propose Entropy-Adaptive Fine-Tuning (EAFT). Unlike methods relying solely on prediction probability, EAFT utilizes token-level entropy as a gating mechanism to distinguish between epistemic uncertainty and knowledge conflict. This allows the model to learn from uncertain samples while suppressing gradients on conflicting data. Extensive experiments on Qwen and GLM series (ranging from 4B to 32B parameters) across mathematical, medical, and agentic domains confirm our hypothesis. EAFT consistently matches the downstream performance of standard SFT while significantly mitigating the degradation of general capabilities.

</details>


### [433] [FedBiCross: A Bi-Level Optimization Framework to Tackle Non-IID Challenges in Data-Free One-Shot Federated Learning on Medical Data](https://arxiv.org/abs/2601.01901)
*Yuexuan Xia,Yinghao Zhang,Yalin Liu,Hong-Ning Dai,Yong Xia*

Main category: cs.LG

TL;DR: FedBiCross improves one-shot federated learning (OSFL) for non-IID medical data through personalized distillation, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Enhance OSFL for privacy-sensitive medical applications under non-IID data distribution, addressing weak global teacher supervision.

Method: FedBiCross involves clustering clients, bi-level cross-cluster optimization with adaptive weights, and personalized distillation for client adaptation.

Result: Experiments on four medical image datasets show FedBiCross consistently outperforms state-of-the-art methods under various levels of non-IID data.

Conclusion: FedBiCross effectively addresses the challenges of OSFL in non-IID scenarios, delivering robust personalized learning performance.

Abstract: Data-free knowledge distillation-based one-shot federated learning (OSFL) trains a model in a single communication round without sharing raw data, making OSFL attractive for privacy-sensitive medical applications. However, existing methods aggregate predictions from all clients to form a global teacher. Under non-IID data, conflicting predictions cancel out during averaging, yielding near-uniform soft labels that provide weak supervision for distillation. We propose FedBiCross, a personalized OSFL framework with three stages: (1) clustering clients by model output similarity to form coherent sub-ensembles, (2) bi-level cross-cluster optimization that learns adaptive weights to selectively leverage beneficial cross-cluster knowledge while suppressing negative transfer, and (3) personalized distillation for client-specific adaptation. Experiments on four medical image datasets demonstrate that FedBiCross consistently outperforms state-of-the-art baselines across different non-IID degrees.

</details>


### [434] [TT-FSI: Scalable Faithful Shapley Interactions via Tensor-Train](https://arxiv.org/abs/2601.01903)
*Ungsik Kim,Suwon Lee*

Main category: cs.LG

TL;DR: TT-FSI proposes an efficient algorithm for computing Faithful Shapley Interaction (FSI) indices using Matrix Product Operators, which reduces computation time and memory usage significantly compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for computing Faithful Shapley Interaction indices are computationally and memory intensive, limiting their scalability and practical application.

Method: TT-FSI leverages the algebraic structure of FSI, employing Matrix Product Operators (MPO) to represent the operator with TT-rank $O(\ell d)$. This enables the development of a sweep algorithm with exponential storage and time improvements.

Result: TT-FSI achieves substantial efficiency improvements, reducing computation time by up to 280×, memory usage by up to 290×, and scaling up to $d=20$, outperforming competitive methods.

Conclusion: TT-FSI establishes itself as a scalable and efficient solution for computing FSI indices, surpassing current methods in both speed and memory requirements, and demonstrating applicability to large-scale datasets.

Abstract: The Faithful Shapley Interaction (FSI) index uniquely satisfies the faithfulness axiom among Shapley interaction indices, but computing FSI requires $O(d^\ell \cdot 2^d)$ time and existing implementations use $O(4^d)$ memory. We present TT-FSI, which exploits FSI's algebraic structure via Matrix Product Operators (MPO). Our main theoretical contribution is proving that the linear operator $v \mapsto \text{FSI}(v)$ admits an MPO representation with TT-rank $O(\ell d)$, enabling an efficient sweep algorithm with $O(\ell^2 d^3 \cdot 2^d)$ time and $O(\ell d^2)$ core storage an exponential improvement over existing methods. Experiments on six datasets ($d=8$ to $d=20$) demonstrate up to 280$\times$ speedup over baseline, 85$\times$ over SHAP-IQ, and 290$\times$ memory reduction. TT-FSI scales to $d=20$ (1M coalitions) where all competing methods fail.

</details>


### [435] [Evaluating Feature Dependent Noise in Preference-based Reinforcement Learning](https://arxiv.org/abs/2601.01904)
*Yuxuan Li,Harshith Reddy Kethireddy,Srijita Das*

Main category: cs.LG

TL;DR: The paper explores preference-based reinforcement learning (PbRL) noise issues, introducing feature-dependent noise types impacting learning robustness, and calls for further study on realistic scenarios, including language model noise.


<details>
  <summary>Details</summary>
Motivation: To address the limitations in PbRL when dealing with challenging tasks without an effective reward function and incorporate noise detection techniques for uncertain or feature-dependent noisy preferences.

Method: Introduces several variants of targeted feature-dependent noise (trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise) and evaluates them in complex continuous control tasks.

Result: State-of-the-art noise-robust PbRL methods suffer under feature-dependent noise settings, whereas non-explicit denoising PbRL methods unexpectedly outperform in majority feature-dependent scenarios. Language model noise aligns with feature-dependent noise characteristics.

Conclusion: The study emphasizes the need for improved methodologies to handle feature-dependent noise in PbRL, as well as further exploration in realistic scenarios such as language model noise.

Abstract: Learning from Preferences in Reinforcement Learning (PbRL) has gained attention recently, as it serves as a natural fit for complicated tasks where the reward function is not easily available. However, preferences often come with uncertainty and noise if they are not from perfect teachers. Much prior literature aimed to detect noise, but with limited types of noise and most being uniformly distributed with no connection to observations. In this work, we formalize the notion of targeted feature-dependent noise and propose several variants like trajectory feature noise, trajectory similarity noise, uncertainty-aware noise, and Language Model noise.
  We evaluate feature-dependent noise, where noise is correlated with certain features in complex continuous control tasks from DMControl and Meta-world. Our experiments show that in some feature-dependent noise settings, the state-of-the-art noise-robust PbRL method's learning performance is significantly deteriorated, while PbRL method with no explicit denoising can surprisingly outperform noise-robust PbRL in majority settings.
  We also find language model's noise exhibits similar characteristics to feature-dependent noise, thereby simulating realistic humans and call for further study in learning with feature-dependent noise robustly.

</details>


### [436] [Distorted Distributional Policy Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2601.01917)
*Ryo Iwaki,Takayuki Osogami*

Main category: cs.LG

TL;DR: The paper introduces quantile distortion to improve offline DRL performance, addressing limitations in current methods' uniform underestimation.


<details>
  <summary>Details</summary>
Motivation: Current offline DRL methods suffer from uniform underestimation of return quantiles, resulting in overly conservative value estimations and limited generalization.

Method: A new concept called quantile distortion is proposed to enable non-uniform pessimism, adjusting conservatism based on data support.

Result: Theoretical and empirical analysis confirms that quantile distortion outperforms methods based on uniform pessimism in offline DRL.

Conclusion: Quantile distortion improves the performance of offline DRL by addressing the limitations of uniform pessimism in return quantiles.

Abstract: While Distributional Reinforcement Learning (DRL) methods have demonstrated strong performance in online settings, its success in offline scenarios remains limited. We hypothesize that a key limitation of existing offline DRL methods lies in their approach to uniformly underestimate return quantiles. This uniform pessimism can lead to overly conservative value estimates, ultimately hindering generalization and performance. To address this, we introduce a novel concept called quantile distortion, which enables non-uniform pessimism by adjusting the degree of conservatism based on the availability of supporting data. Our approach is grounded in theoretical analysis and empirically validated, demonstrating improved performance over uniform pessimism.

</details>


### [437] [Theoretical Convergence of SMOTE-Generated Samples](https://arxiv.org/abs/2601.01927)
*Firuz Kamalov,Hana Sulieman,Witold Pedrycz*

Main category: cs.LG

TL;DR: This paper provides a theoretical analysis of SMOTE, proving its convergence properties and giving practical insights for its use in imbalanced data scenarios.


<details>
  <summary>Details</summary>
Motivation: The study aims to validate the widely-used SMOTE method for addressing imbalanced data issues, not just empirically but also theoretically, to enhance its reliability and effectiveness.

Method: The paper employs rigorous theoretical proofs to establish SMOTE's convergence properties and supports the results with numerical experiments on real-life and synthetic datasets.

Result: It is proven that SMOTE's synthetic random variable converges in probability and mean to the underlying variable, with faster convergence linked to certain parameter choices.

Conclusion: This research offers a deeper theoretical foundation for SMOTE, providing practical guidance for its application and broadening its utility beyond traditional imbalanced data handling.

Abstract: Imbalanced data affects a wide range of machine learning applications, from healthcare to network security. As SMOTE is one of the most popular approaches to addressing this issue, it is imperative to validate it not only empirically but also theoretically. In this paper, we provide a rigorous theoretical analysis of SMOTE's convergence properties. Concretely, we prove that the synthetic random variable Z converges in probability to the underlying random variable X. We further prove a stronger convergence in mean when X is compact. Finally, we show that lower values of the nearest neighbor rank lead to faster convergence offering actionable guidance to practitioners. The theoretical results are supported by numerical experiments using both real-life and synthetic data. Our work provides a foundational understanding that enhances data augmentation techniques beyond imbalanced data scenarios.

</details>


### [438] [DéjàQ: Open-Ended Evolution of Diverse, Learnable and Verifiable Problems](https://arxiv.org/abs/2601.01931)
*Willem Röpke,Samuel Coward,Andrei Lupu,Thomas Foster,Tim Rocktäschel,Jakob Foerster*

Main category: cs.LG

TL;DR: The paper introduces DéjàQ, a framework that evolves mathematical problems dynamically during model training to enhance learning, replacing static datasets.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models in math and coding rely on static datasets, limiting generalisation and promoting memorization.

Method: DéjàQ evolves synthetic mathematical problems alongside model training using two LLM-driven mutation strategies: altering contextual details and modifying problem structure.

Result: The framework generates novel, meaningful problems and shows RL training improvements. It is analysed for problem validity and computational efficiency.

Conclusion: DéjàQ demonstrates the potential of dynamic training data to improve mathematical reasoning and offers open-source code for broader use.

Abstract: Recent advances in reasoning models have yielded impressive results in mathematics and coding. However, most approaches rely on static datasets, which have been suggested to encourage memorisation and limit generalisation. We introduce DéjàQ, a framework that departs from this paradigm by jointly evolving a diverse set of synthetic mathematical problems alongside model training. This evolutionary process adapts to the model's ability throughout training, optimising problems for learnability. We propose two LLM-driven mutation strategies in which the model itself mutates the training data, either by altering contextual details or by directly modifying problem structure. We find that the model can generate novel and meaningful problems, and that these LLM-driven mutations improve RL training. We analyse key aspects of DéjàQ, including the validity of generated problems and computational overhead. Our results underscore the potential of dynamically evolving training data to enhance mathematical reasoning and indicate broader applicability, which we will support by open-sourcing our code.

</details>


### [439] [SynRXN: An Open Benchmark and Curated Dataset for Computational Reaction Modeling](https://arxiv.org/abs/2601.01943)
*Tieu-Long Phan,Nhu-Ngoc Nguyen Song,Peter F. Stadler*

Main category: cs.LG

TL;DR: SynRXN is a new framework and open-data resource for benchmarking computer-aided synthesis planning (CASP), offering standardized datasets, evaluation metrics, and tools for fair comparison and reproducibility.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of dataset heterogeneity and evaluation inconsistencies in computer-aided synthesis planning to enable more fair and rigorous benchmarking.

Method: SynRXN decomposes synthesis planning into five task families and creates curated datasets with provenance tracking. It provides transparent partitioning functions, standardized evaluation protocols, and reproducible resource generation tools.

Result: SynRXN offers harmonized datasets and evaluation resources, ensuring leakage-aware data partitioning, contamination-safe tasks, and reproducibility across platforms.

Conclusion: SynRXN promotes fair comparison, rigorous testing, and reproducible benchmarking in CASP, lowering entry barriers for researchers and practitioners in the field.

Abstract: We present SynRXN, a unified benchmarking framework and open-data resource for computer-aided synthesis planning (CASP). SynRXN decomposes end-to-end synthesis planning into five task families, covering reaction rebalancing, atom-to-atom mapping, reaction classification, reaction property prediction, and synthesis route design. Curated, provenance-tracked reaction corpora are assembled from heterogeneous public sources into a harmonized representation and packaged as versioned datasets for each task family, with explicit source metadata, licence tags, and machine-readable manifests that record checksums, and row counts. For every task, SynRXN provides transparent splitting functions that generate leakage-aware train, validation, and test partitions, together with standardized evaluation workflows and metric suites tailored to classification, regression, and structured prediction settings. For sensitive benchmarking, we combine public training and validation data with held-out gold-standard test sets, and contamination-prone tasks such as reaction rebalancing and atom-to-atom mapping are distributed only as evaluation sets and are explicitly not intended for model training. Scripted build recipes enable bitwise-reproducible regeneration of all corpora across machines and over time, and the entire resource is released under permissive open licences to support reuse and extension. By removing dataset heterogeneity and packaging transparent, reusable evaluation scaffolding, SynRXN enables fair longitudinal comparison of CASP methods, supports rigorous ablations and stress tests along the full reaction-informatics pipeline, and lowers the barrier for practitioners who seek robust and comparable performance estimates for real-world synthesis planning workloads.

</details>


### [440] [Refinement Provenance Inference: Detecting LLM-Refined Training Prompts from Model Behavior](https://arxiv.org/abs/2601.01966)
*Bo Yin,Qi Li,Runpeng Yu,Xinchao Wang*

Main category: cs.LG

TL;DR: The paper introduces Refinement Provenance Inference (RPI), addressing auditing whether a fine-tuned model was trained on original or LLM-refined prompts. It proposes a framework, RePro, to infer the provenance of prompts using detectable shifts in token distributions.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the need for better dataset governance and resolving disputes about training data provenance, especially when fine-tuned models mix original and refined prompts in training datasets.

Method: The authors introduce RePro, a logit-based framework that combines likelihood features and logit-ranking signals. It uses shadow fine-tuning to develop a transferable representation and applies a linear head for inference on unseen models without direct access to training data.

Result: RePro demonstrates robust performance, effectively identifying prompt provenance with consistent success across different refiners and training setups. It relies on distribution shifts rather than rewrite specifics.

Conclusion: The approach shows that prompt refinement introduces detectable distributional changes, which can be leveraged for reliable provenance inference, benefiting auditing and governance tasks in fine-tuning processes.

Abstract: Instruction tuning increasingly relies on LLM-based prompt refinement, where prompts in the training corpus are selectively rewritten by an external refiner to improve clarity and instruction alignment. This motivates an instance-level audit problem: for a fine-tuned model and a training prompt-response pair, can we infer whether the model was trained on the original prompt or its LLM-refined version within a mixed corpus? This matters for dataset governance and dispute resolution when training data are contested. However, it is non-trivial in practice: refined and raw instances are interleaved in the training corpus with unknown, source-dependent mixture ratios, making it harder to develop provenance methods that generalize across models and training setups. In this paper, we formalize this audit task as Refinement Provenance Inference (RPI) and show that prompt refinement yields stable, detectable shifts in teacher-forced token distributions, even when semantic differences are not obvious. Building on this phenomenon, we propose RePro, a logit-based provenance framework that fuses teacher-forced likelihood features with logit-ranking signals. During training, RePro learns a transferable representation via shadow fine-tuning, and uses a lightweight linear head to infer provenance on unseen victims without training-data access. Empirically, RePro consistently attains strong performance and transfers well across refiners, suggesting that it exploits refiner-agnostic distribution shifts rather than rewrite-style artifacts.

</details>


### [441] [Prior Diffusiveness and Regret in the Linear-Gaussian Bandit](https://arxiv.org/abs/2601.02022)
*Yifan Zhu,John C. Duchi,Benjamin Van Roy*

Main category: cs.LG

TL;DR: This paper proves that Thompson Sampling achieves a regret bound that decouples prior-dependent and minimax regret additively in a linear-Gaussian bandit setting.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve understanding of Thompson Sampling's performance in a linear-Gaussian bandit model with respect to regret bounds, particularly addressing the additivity of prior-dependent and minimax terms.

Method: The authors employ a new 'elliptical potential' lemma to derive a regret bound and provide theoretical analysis to establish its additivity. They also present a lower bound to assert the necessity of the burn-in term.

Result: The regret bound derived exhibits an additive decoupling between prior-dependent burn-in and minimax regret terms.

Conclusion: The study enhances understanding of Thompson Sampling's Bayesian regret by showing an additive separation, which simplifies the interpretation of regret contributions.

Abstract: We prove that Thompson sampling exhibits $\tilde{O}(σd \sqrt{T} + d r \sqrt{\mathrm{Tr}(Σ_0)})$ Bayesian regret in the linear-Gaussian bandit with a $\mathcal{N}(μ_0, Σ_0)$ prior distribution on the coefficients, where $d$ is the dimension, $T$ is the time horizon, $r$ is the maximum $\ell_2$ norm of the actions, and $σ^2$ is the noise variance. In contrast to existing regret bounds, this shows that to within logarithmic factors, the prior-dependent ``burn-in'' term $d r \sqrt{\mathrm{Tr}(Σ_0)}$ decouples additively from the minimax (long run) regret $σd \sqrt{T}$. Previous regret bounds exhibit a multiplicative dependence on these terms. We establish these results via a new ``elliptical potential'' lemma, and also provide a lower bound indicating that the burn-in term is unavoidable.

</details>


### [442] [GDRO: Group-level Reward Post-training Suitable for Diffusion Models](https://arxiv.org/abs/2601.02036)
*Yiyang Wang,Xi Chen,Xiaogang Xu,Yu Liu,Hengshuang Zhao*

Main category: cs.LG

TL;DR: This paper introduces GDRO, a new paradigm for group-level reward alignment in rectified flow diffusion models, resolving inefficiency and dependency on stochastic samplers, and mitigating reward hacking.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in rectified flow diffusion models, such as inefficiency in online sampling, dependency on stochastic samplers, and risks of reward hacking, while leveraging group-level rewards.

Method: The GDRO method employs group-level direct reward optimization, enabling offline post-training for rectified flow models, bypassing time costs of sampling and eliminating the dependency on specific samplers.

Result: GDRO demonstrates improved reward alignment for diffusion models in tasks like OCR and GenEval, as well as robustness against reward hacking, verified by both theoretical and empirical analysis.

Conclusion: GDRO presents a significant advancement for reward alignment in rectified flow models, achieving efficiency, diffusion-sampler independence, and resistance to reward hacking through offline group-level optimization.

Abstract: Recent advancements adopt online reinforcement learning (RL) from LLMs to text-to-image rectified flow diffusion models for reward alignment. The use of group-level rewards successfully aligns the model with the targeted reward. However, it faces challenges including low efficiency, dependency on stochastic samplers, and reward hacking. The problem is that rectified flow models are fundamentally different from LLMs: 1) For efficiency, online image sampling takes much more time and dominates the time of training. 2) For stochasticity, rectified flow is deterministic once the initial noise is fixed. Aiming at these problems and inspired by the effects of group-level rewards from LLMs, we design Group-level Direct Reward Optimization (GDRO). GDRO is a new post-training paradigm for group-level reward alignment that combines the characteristics of rectified flow models. Through rigorous theoretical analysis, we point out that GDRO supports full offline training that saves the large time cost for image rollout sampling. Also, it is diffusion-sampler-independent, which eliminates the need for the ODE-to-SDE approximation to obtain stochasticity. We also empirically study the reward hacking trap that may mislead the evaluation, and involve this factor in the evaluation using a corrected score that not only considers the original evaluation reward but also the trend of reward hacking. Extensive experiments demonstrate that GDRO effectively and efficiently improves the reward score of the diffusion model through group-wise offline optimization across the OCR and GenEval tasks, while demonstrating strong stability and robustness in mitigating reward hacking.

</details>


### [443] [Multivariate Time-series Anomaly Detection via Dynamic Model Pool & Ensembling](https://arxiv.org/abs/2601.02037)
*Wei Hu,Zewei Yu,Jianqiu Xu*

Main category: cs.LG

TL;DR: The DMPEAD framework enhances multivariate time-series (MTS) anomaly detection through a scalable, dynamic model pool with adaptive selection and ensembling strategies, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: The paper aims to tackle limitations in current MTS anomaly detection methods, particularly issues of overreliance on single models, inefficiency in ensembling, and restricted scalability due to fixed data dimensionality.

Method: The proposed DMPEAD framework creates a diverse model pool by utilizing parameter transfer and diversity metrics. It employs a meta-model and similarity-based adaptive strategy for pool expansion, subset selection, and pool merging. The top-ranked models are ensembled using proxy metric rankings and top-k aggregation.

Result: Extensive experiments on 8 real-world datasets demonstrate that DMPEAD surpasses all baseline methods, showcasing superior performance in terms of efficiency, scalability, and adaptability for MTS anomaly detection.

Conclusion: The results validate DMPEAD's effectiveness as a scalable and adaptive framework for MTS anomaly detection, making it a robust alternative to current approaches.

Abstract: Multivariate time-series (MTS) anomaly detection is critical in domains such as service monitor, IoT, and network security. While multi-model methods based on selection or ensembling outperform single-model ones, they still face limitations: (i) selection methods rely on a single chosen model and are sensitive to the strategy; (ii) ensembling methods often combine all models or are restricted to univariate data; and (iii) most methods depend on fixed data dimensionality, limiting scalability. To address these, we propose DMPEAD, a Dynamic Model Pool and Ensembling framework for MTS Anomaly Detection. The framework first (i) constructs a diverse model pool via parameter transfer and diversity metric, then (ii) updates it with a meta-model and similarity-based strategy for adaptive pool expansion, subset selection, and pool merging, finally (iii) ensembles top-ranked models through proxy metric ranking and top-k aggregation in the selected subset, outputting the final anomaly detection result. Extensive experiments on 8 real-world datasets show that our model outperforms all baselines, demonstrating superior adaptability and scalability.

</details>


### [444] [Explore the Ideology of Deep Learning in ENSO Forecasts](https://arxiv.org/abs/2601.02050)
*Yanhai Gan,Yipeng Chen,Ning Li,Xingguo Liu,Junyu Dong,Xianyao Chen*

Main category: cs.LG

TL;DR: The paper introduces an interpretability framework based on bounded variation function for deep learning models predicting ENSO, highlighting tropical Pacific's dominant role and the Spring Predictability Barrier.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve ENSO prediction by addressing challenges in deep learning, particularly opacity, to enhance scientific trust and operational use.

Method: A mathematically supported framework utilizing bounded variation function is developed to optimize model expressivity and interpret results more transparently.

Result: ENSO predictability is found mainly in the tropical Pacific, with contributions from other regions, while the Spring Predictability Barrier remains due to variable limitations.

Conclusion: Incorporating additional ocean-atmosphere variables may overcome current limitations, enhancing ENSO prediction beyond the Spring Barrier.

Abstract: The El Ni{~n}o-Southern Oscillation (ENSO) exerts profound influence on global climate variability, yet its prediction remains a grand challenge. Recent advances in deep learning have significantly improved forecasting skill, but the opacity of these models hampers scientific trust and operational deployment. Here, we introduce a mathematically grounded interpretability framework based on bounded variation function. By rescuing the "dead" neurons from the saturation zone of the activation function, we enhance the model's expressive capacity. Our analysis reveals that ENSO predictability emerges dominantly from the tropical Pacific, with contributions from the Indian and Atlantic Oceans, consistent with physical understanding. Controlled experiments affirm the robustness of our method and its alignment with established predictors. Notably, we probe the persistent Spring Predictability Barrier (SPB), finding that despite expanded sensitivity during spring, predictive performance declines-likely due to suboptimal variable selection. These results suggest that incorporating additional ocean-atmosphere variables may help transcend SPB limitations and advance long-range ENSO prediction.

</details>


### [445] [The Homogeneity Trap: Spectral Collapse in Doubly-Stochastic Deep Networks](https://arxiv.org/abs/2601.02080)
*Yizhi Liu*

Main category: cs.LG

TL;DR: This work focuses on spectral degradation in deep networks using doubly-stochastic matrices (DSM), introducing a phenomenon called the Homogeneity Trap, which suppresses feature transformation depth and structure.


<details>
  <summary>Details</summary>
Motivation: To address numerical stability and probabilistic interpretability in structure-preserving deep architectures using DSM, while identifying potential spectral limitations.

Method: Authors analyze the spectral properties of DSM via maximum-entropy constraints, derive spectral bounds, and study the impact using Layer Normalization in noise-dominated settings.

Result: They demonstrate that DSM constraints suppress singular value σ_2, limiting feature transformation capacity and causing irreversible geometric collapse under certain noise conditions.

Conclusion: A trade-off exists between entropic stability and spectral expressivity in DSM-constrained networks, influencing feature preservation and architecture depth.

Abstract: Doubly-stochastic matrices (DSM) are increasingly utilized in structure-preserving deep architectures -- such as Optimal Transport layers and Sinkhorn-based attention -- to enforce numerical stability and probabilistic interpretability. In this work, we identify a critical spectral degradation phenomenon inherent to these constraints, termed the Homogeneity Trap. We demonstrate that the maximum-entropy bias, typical of Sinkhorn-based projections, drives the mixing operator towards the uniform barycenter, thereby suppressing the subdominant singular value σ_2 and filtering out high-frequency feature components. We derive a spectral bound linking σ_2 to the network's effective depth, showing that high-entropy constraints restrict feature transformation to a shallow effective receptive field. Furthermore, we formally demonstrate that Layer Normalization fails to mitigate this collapse in noise-dominated regimes; specifically, when spectral filtering degrades the Signal-to-Noise Ratio (SNR) below a critical threshold, geometric structure is irreversibly lost to noise-induced orthogonal collapse. Our findings highlight a fundamental trade-off between entropic stability and spectral expressivity in DSM-constrained networks.

</details>


### [446] [A Differentiable Adversarial Framework for Task-Aware Data Subsampling](https://arxiv.org/abs/2601.02081)
*Jiacheng Lyu,Bihua Bao*

Main category: cs.LG

TL;DR: The paper introduces a novel subsampling framework, ASSS, which intelligently selects data for model training by dynamically assigning importance weights to samples, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The increasing size of datasets poses computational challenges for model training, and traditional subsampling methods often discard critical information needed for downstream tasks.

Method: The ASSS framework utilizes an adversarial game between a selector and a task network, where the selector uses Gumbel-Softmax relaxation to assign importance weights to samples, optimizing for both fidelity and sparsity in the prediction.

Result: Comprehensive experiments show that ASSS outperforms heuristic subsampling methods like clustering and nearest neighbor thinning and can sometimes surpass the performance of training on the entire dataset.

Conclusion: ASSS provides a principled, task-aware solution for large-scale data learning by reconstructing data reduction as a learnable, end-to-end process.

Abstract: The proliferation of large-scale datasets poses a major computational challenge to model training. The traditional data subsampling method works as a static, task independent preprocessing step which usually discards information that is critical to downstream prediction. In this paper, we introduces the antagonistic soft selection subsampling (ASSS) framework as is a novel paradigm that reconstructs data reduction into a differentiable end-to-end learning problem. ASSS uses the adversarial game between selector network and task network, and selector network learning assigns continuous importance weights to samples. This direct optimization implemented by Gumbel-Softmax relaxation allows the selector to identify and retain samples with the maximum amount of information for a specific task target under the guidance of the loss function that balances the fidelity and sparsity of the prediction. Theoretical analysis links this framework with the information bottleneck principle. Comprehensive experiments on four large-scale real world datasets show that ASSS has always been better than heuristic subsampling baselines such as clustering and nearest neighbor thinning in maintaining model performance. It is worth noting that ASSS can not only match, but also sometimes exceed the training performance of the entire dataset, showcasing the effect of intelligent denoising. This work establishes task aware data subsampling as a learnable component, providing a principled solution for effective large-scale data learning.

</details>


### [447] [Horizon Activation Mapping for Neural Networks in Time Series Forecasting](https://arxiv.org/abs/2601.02094)
*Hans Krupakar,V A Kandappan*

Main category: cs.LG

TL;DR: This paper introduces Horizon Activation Mapping (HAM), a universal visual interpretability technique for time series forecasting models, enabling model selection across diverse neural network families.


<details>
  <summary>Details</summary>
Motivation: Time series forecasting requires interpretability techniques that work across different types of neural network architectures and layers to simplify model selection.

Method: The paper presents HAM, inspired by grad-CAM, which uses gradient norm averages to study time series subseries. It incorporates causal and anti-causal modes, proportionality lines, and optimization landscape analyses.

Result: HAM facilitates model interpretability agnostic to architecture types, with insights observed across multivariate forecasting models trained on the ETTm2 dataset. Behavioral trends are analyzed with HAM plots, revealing model-specific characteristics.

Conclusion: HAM provides a standardized approach for evaluating and selecting time series forecasting models, enhancing interpretability across neural network families.

Abstract: Neural networks for time series forecasting have relied on error metrics and architecture-specific interpretability approaches for model selection that don't apply across models of different families. To interpret forecasting models agnostic to the types of layers across state-of-the-art model families, we introduce Horizon Activation Mapping (HAM), a visual interpretability technique inspired by grad-CAM that uses gradient norm averages to study the horizon's subseries where grad-CAM studies attention maps over image data. We introduce causal and anti-causal modes to calculate gradient update norm averages across subseries at every timestep and lines of proportionality signifying uniform distributions of the norm averages. Optimization landscape studies with respect to changes in batch sizes, early stopping, train-val-test splits, univariate forecasting and dropouts are studied with respect to performances and subseries in HAM. Interestingly, batch size based differences in activities seem to indicate potential for existence of an exponential approximation across them per epoch relative to each other. Multivariate forecasting models including MLP-based CycleNet, N-Linear, N-HITS, self attention-based FEDformer, Pyraformer, SSM-based SpaceTime and diffusion-based Multi-Resolution DDPM over different horizon sizes trained over the ETTm2 dataset are used for HAM plots in this study. NHITS' neural approximation theorem and SpaceTime's exponential autoregressive activities have been attributed to trends in HAM plots over their training, validation and test sets. In general, HAM can be used for granular model selection, validation set choices and comparisons across different neural network model families.

</details>


### [448] [LION-DG: Layer-Informed Initialization with Deep Gradient Protocols for Accelerated Neural Network Training](https://arxiv.org/abs/2601.02105)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: This paper introduces LION-DG, an innovative weight initialization strategy targeting deeply-supervised architectures to resolve gradient interference issues, enhancing convergence speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Current weight initialization methods for neural networks, especially in deeply-supervised architectures, often suffer from gradient interference due to untrained auxiliary classifiers, hindering early training stability.

Method: The paper proposes LION-DG, a layer-informed initialization that zero-initializes auxiliary classifier heads and uses He-initialization for the main network. This ensures gradual auxiliary gradient activation through implicit weight warm-up.

Result: Through experiments on CIFAR-10 and CIFAR-100 using DenseNet-DS and ResNet-DS, LION-DG demonstrated faster convergence (+8.3% on DenseNet) and improved accuracy (81.92% with hybrid LSUV+LION-DG). It also achieved speedup (+11.3%) for ResNet-DS on CIFAR-100.

Conclusion: LION-DG enhances the performance of deeply-supervised architectures by mitigating gradient interference, leading to faster convergence and better results. It is simple, hyperparameter-free, and computationally lightweight, providing practical guidelines for deployment.

Abstract: Weight initialization remains decisive for neural network optimization, yet existing methods are largely layer-agnostic. We study initialization for deeply-supervised architectures with auxiliary classifiers, where untrained auxiliary heads can destabilize early training through gradient interference.
  We propose LION-DG, a layer-informed initialization that zero-initializes auxiliary classifier heads while applying standard He-initialization to the backbone. We prove that this implements Gradient Awakening: auxiliary gradients are exactly zero at initialization, then phase in naturally as weights grow -- providing an implicit warmup without hyperparameters.
  Experiments on CIFAR-10 and CIFAR-100 with DenseNet-DS and ResNet-DS architectures demonstrate: (1) DenseNet-DS: +8.3% faster convergence on CIFAR-10 with comparable accuracy, (2) Hybrid approach: Combining LSUV with LION-DG achieves best accuracy (81.92% on CIFAR-10), (3) ResNet-DS: Positive speedup on CIFAR-100 (+11.3%) with side-tap auxiliary design.
  We identify architecture-specific trade-offs and provide clear guidelines for practitioners. LION-DG is simple, requires zero hyperparameters, and adds no computational overhead.

</details>


### [449] [Prototype-Based Learning for Healthcare: A Demonstration of Interpretable AI](https://arxiv.org/abs/2601.02106)
*Ashish Rana,Ammar Shaker,Sascha Saralajew,Takashi Suzuki,Kosuke Yasuda,Shintaro Kato,Toshikazu Wada,Toshiyuki Fujikawa,Toru Kikutsuji*

Main category: cs.LG

TL;DR: The paper introduces ProtoPal, a framework utilizing prototype-based learning to enhance personalized preventive healthcare with understandable and verifiable predictions.


<details>
  <summary>Details</summary>
Motivation: Improving personalized preventive healthcare by providing stakeholders with intuitive and verifiable predictions, interventions, and recommendations.

Method: Development of ProtoPal framework with prototype-based learning, combining front- and back-end modes for both performance and intuitive presentation.

Result: ProtoPal achieves superior quantitative performance and provides clear simulation of outcomes and interventions.

Conclusion: ProtoPal demonstrates effectiveness in predictive healthcare while enhancing explainability and accessibility for stakeholders.

Abstract: Despite recent advances in machine learning and explainable AI, a gap remains in personalized preventive healthcare: predictions, interventions, and recommendations should be both understandable and verifiable for all stakeholders in the healthcare sector. We present a demonstration of how prototype-based learning can address these needs. Our proposed framework, ProtoPal, features both front- and back-end modes; it achieves superior quantitative performance while also providing an intuitive presentation of interventions and their simulated outcomes.

</details>


### [450] [Edge-aware GAT-based protein binding site prediction](https://arxiv.org/abs/2601.02138)
*Weisen Yang,Hanqing Zhang,Wangren Qiu,Xuan Xiao,Weizhong Lin*

Main category: cs.LG

TL;DR: The paper introduces an Edge-aware Graph Attention Network for protein binding site prediction, achieving high accuracy and efficiency using advanced features and a web server for accessibility.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy, generalization, and efficiency in predicting protein and biomolecule binding sites, addressing limitations in traditional methods when handling complex spatial conformations.

Method: The authors use an Edge-aware GAT model that incorporates atom-level graphs, geometric descriptors, secondary structure, RSA, and edge-based interatomic information to enhance prediction accuracy and interpretability.

Result: The model achieved a ROC-AUC of 0.93 on benchmark datasets, outperforming state-of-the-art methods. Visualizations demonstrated its practical utility and interpretability.

Conclusion: The proposed Edge-aware GAT model is a novel, efficient, and interpretable solution for accurately predicting functional sites in biomolecules, with a publicly accessible web server provided.

Abstract: Accurate identification of protein binding sites is crucial for understanding biomolecular interaction mechanisms and for the rational design of drug targets. Traditional predictive methods often struggle to balance prediction accuracy with computational efficiency when capturing complex spatial conformations. To address this challenge, we propose an Edge-aware Graph Attention Network (Edge-aware GAT) model for the fine-grained prediction of binding sites across various biomolecules, including proteins, DNA/RNA, ions, ligands, and lipids. Our method constructs atom-level graphs and integrates multidimensional structural features, including geometric descriptors, DSSP-derived secondary structure, and relative solvent accessibility (RSA), to generate spatially aware embedding vectors. By incorporating interatomic distances and directional vectors as edge features within the attention mechanism, the model significantly enhances its representation capacity. On benchmark datasets, our model achieves an ROC-AUC of 0.93 for protein-protein binding site prediction, outperforming several state-of-the-art methods. The use of directional tensor propagation and residue-level attention pooling further improves both binding site localization and the capture of local structural details. Visualizations using PyMOL confirm the model's practical utility and interpretability. To facilitate community access and application, we have deployed a publicly accessible web server at http://119.45.201.89:5000/. In summary, our approach offers a novel and efficient solution that balances prediction accuracy, generalization, and interpretability for identifying functional sites in proteins.

</details>


### [451] [ACDZero: Graph-Embedding-Based Tree Search for Mastering Automated Cyber Defense](https://arxiv.org/abs/2601.02196)
*Yu Li,Sizhe Tang,Rongqian Chen,Fei Xu Yu,Guangyu Jiang,Mahdi Imani,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: This paper proposes a sample-efficient automated cyber defense (ACD) solution using Monte Carlo Tree Search (MCTS) and Graph Neural Networks (GNNs).


<details>
  <summary>Details</summary>
Motivation: Existing deep reinforcement learning methods for automated cyber defense face challenges with exploration in complex networks, requiring costly samples. The authors aim to address this inefficiency.

Method: The authors use Monte Carlo Tree Search (MCTS) combined with Graph Neural Networks (GNNs) to model context-based partially observable Markov decision problems. They incorporate graph embeddings and priors to facilitate efficient planning in large decision spaces.

Result: The proposed method was evaluated on diverse network scenarios and adversary behaviors, showing improved defense reward and robustness compared to state-of-the-art reinforcement learning baselines.

Conclusion: The approach demonstrates effective sample-efficient automated cyber defense by integrating search-guided planning and graph-based reasoning, outperforming existing methods.

Abstract: Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.

</details>


### [452] [CORE: Code-based Inverse Self-Training Framework with Graph Expansion for Virtual Agents](https://arxiv.org/abs/2601.02201)
*Keyu Wang,Bingchen Miao,Wendong Bu,Yu Wu,Juncheng Li,Shengyu Zhang,Wenqiao Zhang,Siliang Tang,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: The paper presents CORE, a training framework combining imitation and exploration to enhance behavioral diversity in Multimodal Virtual Agents without manual reward design.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in current training methods for virtual agents: imitation lacks behavioral diversity, and reinforcement learning relies on manually designed reward functions.

Method: CORE introduces Semantic Code Abstraction for automatic reward inference, Strategy Graph Expansion for diverse strategy discovery, and Trajectory-Guided Extrapolation for task space expansion.

Result: CORE significantly improves the performance and generalization of virtual agents on Web and Android platforms.

Conclusion: CORE provides a robust framework for training virtual agents, uniting imitation and exploration to achieve diverse and generalizable behaviors.

Abstract: The development of Multimodal Virtual Agents has made significant progress through the integration of Multimodal Large Language Models. However, mainstream training paradigms face key challenges: Behavior Cloning is simple and effective through imitation but suffers from low behavioral diversity, while Reinforcement Learning is capable of discovering novel strategies through exploration but heavily relies on manually designed reward functions. To address the conflict between these two methods, we present CORE, a Code-based Inverse Self-Training Framework with Graph Expansion that bridges imitation and exploration, offering a novel training framework that promotes behavioral diversity while eliminating the reliance on manually reward design. Specifically, we introduce Semantic Code Abstraction to automatically infers reward functions from expert demonstrations without manual design. The inferred reward function, referred to as the Label Function, is executable code that verifies one key step within a task. Building on this, we propose Strategy Graph Expansion to enhance in-domain behavioral diversity, which constructs a multi-path graph called Strategy Graph that captures diverse valid solutions beyond expert demonstrations. Furthermore, we introduce Trajectory-Guided Extrapolation, which enriches out-of-domain behavioral diversity by utilizing both successful and failed trajectories to expand the task space. Experiments on Web and Android platforms demonstrate that CORE significantly improves both overall performance and generalization, highlighting its potential as a robust and generalizable training paradigm for building powerful virtual agents.

</details>


### [453] [Quantized SO(3)-Equivariant Graph Neural Networks for Efficient Molecular Property Prediction](https://arxiv.org/abs/2601.02213)
*Haoyu Zhou,Ping Xue,Tianfan Fu,Hao Zhang*

Main category: cs.LG

TL;DR: The paper proposes a method to compress and speed up 3D rotation-equivariant GNNs using low-bit quantization, achieving efficiency improvements without accuracy or symmetry loss.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the high computational cost of deploying 3D rotation-equivariant GNNs on edge devices, an obstacle for practical applications.

Method: The paper proposes three key innovations: decoupled quantization for vector features, a branch-separated quantization-aware training strategy, and an attention normalization mechanism for robustness under low-bit quantization.

Result: Experiments on molecular benchmarks (QM9 and rMD17) show that 8-bit models achieve comparable accuracy to full precision models while being more efficient, with 2.37-2.73x faster inference and 4x smaller model size.

Conclusion: The proposed techniques enable practical deployment of symmetry-aware GNNs in fields like chemistry, yielding significant efficiency gains without sacrificing model performance or physical symmetry.

Abstract: Deploying 3D graph neural networks (GNNs) that are equivariant to 3D rotations (the group SO(3)) on edge devices is challenging due to their high computational cost. This paper addresses the problem by compressing and accelerating an SO(3)-equivariant GNN using low-bit quantization techniques. Specifically, we introduce three innovations for quantized equivariant transformers: (1) a magnitude-direction decoupled quantization scheme that separately quantizes the norm and orientation of equivariant (vector) features, (2) a branch-separated quantization-aware training strategy that treats invariant and equivariant feature channels differently in an attention-based $SO(3)$-GNN, and (3) a robustness-enhancing attention normalization mechanism that stabilizes low-precision attention computations. Experiments on the QM9 and rMD17 molecular benchmarks demonstrate that our 8-bit models achieve accuracy on energy and force predictions comparable to full-precision baselines with markedly improved efficiency. We also conduct ablation studies to quantify the contribution of each component to maintain accuracy and equivariance under quantization, using the Local error of equivariance (LEE) metric. The proposed techniques enable the deployment of symmetry-aware GNNs in practical chemistry applications with 2.37--2.73x faster inference and 4x smaller model size, without sacrificing accuracy or physical symmetry.

</details>


### [454] [ELLA: Efficient Lifelong Learning for Adapters in Large Language Models](https://arxiv.org/abs/2601.02232)
*Shristi Das Biswas,Yue Zhang,Anwesan Pal,Radhika Bhargava,Kaushik Roy*

Main category: cs.LG

TL;DR: The paper introduces ELLA, a continual learning (CL) framework for large language models (LLMs) that reduces catastrophic forgetting without requiring data replay or extensive memory.


<details>
  <summary>Details</summary>
Motivation: Current approaches to CL suffer from either privacy concerns (data replay) or inflexibility and collapse in performance due to restrictions like strict orthogonality, which prevent shared task representation.

Method: ELLA selectively avoids overlapping task projections in high-energy directions from past tasks but allows freedom in low-energy subspaces, using a lightweight anisotropic shrinkage regularizer.

Result: ELLA achieves state-of-the-art performance on CL benchmarks with up to 9.6% accuracy improvements, reduces memory requirements by 35×, and enables better scalability and zero-shot generalization.

Conclusion: ELLA provides a scalable and efficient solution for continual adaptation of LLMs while maintaining performance and reducing catastrophic forgetting, offering a dedicated approach for lifelong learning.

Abstract: Large Language Models (LLMs) suffer severe catastrophic forgetting when adapted sequentially to new tasks in a continual learning (CL) setting. Existing approaches are fundamentally limited: replay-based methods are impractical and privacy-violating, while strict orthogonality-based methods collapse under scale: each new task is projected onto an orthogonal complement, progressively reducing the residual degrees of freedom and eliminating forward transfer by forbidding overlap in shared representations. In this work, we introduce ELLA, a training framework built on the principle of selective subspace de-correlation. Rather than forbidding all overlap, ELLA explicitly characterizes the structure of past updates and penalizes alignments along their high-energy, task-specific directions, while preserving freedom in the low-energy residual subspaces to enable transfer. Formally, this is realized via a lightweight regularizer on a single aggregated update matrix. We prove this mechanism corresponds to an anisotropic shrinkage operator that bounds interference, yielding a penalty that is both memory- and compute-constant regardless of task sequence length. ELLA requires no data replay, no architectural expansion, and negligible storage. Empirically, it achieves state-of-the-art CL performance on three popular benchmarks, with relative accuracy gains of up to $9.6\%$ and a $35\times$ smaller memory footprint. Further, ELLA scales robustly across architectures and actively enhances the model's zero-shot generalization performance on unseen tasks, establishing a principled and scalable solution for constructive lifelong LLM adaptation.

</details>


### [455] [POSEIDON: Physics-Optimized Seismic Energy Inference and Detection Operating Network](https://arxiv.org/abs/2601.02264)
*Boris Kriuk,Fedor Kriuk*

Main category: cs.LG

TL;DR: POSEIDON is a physics-informed machine learning model designed for seismic event prediction, leveraging physical laws and achieving state-of-the-art results across tasks using the largest open earthquake dataset: Poseidon.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning models in geophysics often act as black boxes, disregarding physical laws. The study aims to create a framework integrating established seismological principles for improved seismic prediction.

Method: POSEIDON embeds principles like the Gutenberg-Richter law and Omori-Utsu decay law into its energy-based modeling framework. It handles three prediction tasks: aftershock identification, tsunami potential evaluation, and foreshock detection.

Result: POSEIDON outperforms traditional methods such as gradient boosting and CNN models, achieving higher predictive accuracy and scientifically interpretable parameters within established ranges.

Conclusion: POSEIDON bridges the gap between machine learning and physics-based understanding in geophysics, offering both superior predictive accuracy and scientific validity, supported by the publicly available Poseidon dataset.

Abstract: Earthquake prediction and seismic hazard assessment remain fundamental challenges in geophysics, with existing machine learning approaches often operating as black boxes that ignore established physical laws. We introduce POSEIDON (Physics-Optimized Seismic Energy Inference and Detection Operating Network), a physics-informed energy-based model for unified multi-task seismic event prediction, alongside the Poseidon dataset -- the largest open-source global earthquake catalog comprising 2.8 million events spanning 30 years. POSEIDON embeds fundamental seismological principles, including the Gutenberg-Richter magnitude-frequency relationship and Omori-Utsu aftershock decay law, as learnable constraints within an energy-based modeling framework. The architecture simultaneously addresses three interconnected prediction tasks: aftershock sequence identification, tsunami generation potential, and foreshock detection. Extensive experiments demonstrate that POSEIDON achieves state-of-the-art performance across all tasks, outperforming gradient boosting, random forest, and CNN baselines with the highest average F1 score among all compared methods. Crucially, the learned physics parameters converge to scientifically interpretable values -- Gutenberg-Richter b-value of 0.752 and Omori-Utsu parameters p=0.835, c=0.1948 days -- falling within established seismological ranges while enhancing rather than compromising predictive accuracy. The Poseidon dataset is publicly available at https://huggingface.co/datasets/BorisKriuk/Poseidon, providing pre-computed energy features, spatial grid indices, and standardized quality metrics to advance physics-informed seismic research.

</details>


### [456] [Differential Privacy for Transformer Embeddings of Text with Nonparametric Variational Information Bottleneck](https://arxiv.org/abs/2601.02307)
*Dina El Zein,James Henderson*

Main category: cs.LG

TL;DR: This paper introduces a method, Nonparametric Variational Differential Privacy (NVDP), to protect sensitive information in transformer embeddings while preserving utility by adding calibrated noise.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of adversaries recovering sensitive input data from transformer embeddings, which can encode private information due to their multi-vector structure.

Method: The authors integrate a Nonparametric Variational Information Bottleneck (NVIB) layer into transformer models to introduce noise into embeddings. Privacy is quantified using Rényi divergence and Bayesian Differential Privacy guarantees, with the NVIB layer optimizing the tradeoff between utility and privacy.

Result: The proposed method demonstrated the ability to balance privacy and accuracy on the GLUE benchmark, maintaining strong privacy guarantees without compromising high accuracy at lower noise levels.

Conclusion: NVDP offers a practical tradeoff that ensures both strong privacy protection and useful data sharing by effectively injecting noise into the embeddings of transformers.

Abstract: We propose a privacy-preserving method for sharing text data by sharing noisy versions of their transformer embeddings. It has been shown that hidden representations learned by deep models can encode sensitive information from the input, making it possible for adversaries to recover the input data with considerable accuracy. This problem is exacerbated in transformer embeddings because they consist of multiple vectors, one per token. To mitigate this risk, we propose Nonparametric Variational Differential Privacy (NVDP), which ensures both useful data sharing and strong privacy protection. We take a differential privacy approach, integrating a Nonparametric Variational Information Bottleneck (NVIB) layer into the transformer architecture to inject noise into its multi-vector embeddings and thereby hide information, and measuring privacy protection with Rényi divergence and its corresponding Bayesian Differential Privacy (BDP) guarantee. Training the NVIB layer calibrates the noise level according to utility. We test NVDP on the GLUE benchmark and show that varying the noise level gives us a useful tradeoff between privacy and accuracy. With lower noise levels, our model maintains high accuracy while offering strong privacy guarantees, effectively balancing privacy and utility.

</details>


### [457] [Temporal Kolmogorov-Arnold Networks (T-KAN) for High-Frequency Limit Order Book Forecasting: Efficiency, Interpretability, and Alpha Decay](https://arxiv.org/abs/2601.02310)
*Ahmad Makinde*

Main category: cs.LG

TL;DR: The paper proposes Temporal Kolmogorov-Arnold Networks (T-KAN) to improve predictive power in high-frequency trading environments, demonstrating significant advancements in accuracy and profits compared to traditional models.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of alpha decay and limited effectiveness of traditional models like DeepLOB in high-frequency trading data, which are noisy and non-linear.

Method: Introducing T-KAN to replace linear weights in LSTMs with learnable B-spline activation functions, enabling shape-based market signal analysis. Experiments conducted on FI-2010 dataset.

Result: T-KAN achieved 19.1% improvement in F1-score at extended horizons (k=100) and significant profitability (+132.48% return vs DeepLOB's drawdown). The model also proved interpretable and FPGA-optimized.

Conclusion: T-KAN effectively enhances forecasting in high-frequency trading, improving accuracy, profitability, interpretability, and low-latency implementation possibilities.

Abstract: High-Frequency trading (HFT) environments are characterised by large volumes of limit order book (LOB) data, which is notoriously noisy and non-linear. Alpha decay represents a significant challenge, with traditional models such as DeepLOB losing predictive power as the time horizon (k) increases. In this paper, using data from the FI-2010 dataset, we introduce Temporal Kolmogorov-Arnold Networks (T-KAN) to replace the fixed, linear weights of standard LSTMs with learnable B-spline activation functions. This allows the model to learn the 'shape' of market signals as opposed to just their magnitude. This resulted in a 19.1% relative improvement in the F1-score at the k = 100 horizon. The efficacy of T-KAN networks cannot be understated, producing a 132.48% return compared to the -82.76% DeepLOB drawdown under 1.0 bps transaction costs. In addition to this, the T-KAN model proves quite interpretable, with the 'dead-zones' being clearly visible in the splines. The T-KAN architecture is also uniquely optimized for low-latency FPGA implementation via High level Synthesis (HLS). The code for the experiments in this project can be found at https://github.com/AhmadMak/Temporal-Kolmogorov-Arnold-Networks-T-KAN-for-High-Frequency-Limit-Order-Book-Forecasting.

</details>


### [458] [Game of Coding: Coding Theory in the Presence of Rational Adversaries, Motivated by Decentralized Machine Learning](https://arxiv.org/abs/2601.02313)
*Hanzaleh Akbari Nodehi,Viveck R. Cadambe,Mohammad Ali Maddah-Ali*

Main category: cs.LG

TL;DR: This paper introduces a game-theoretic framework for coding theory in decentralized systems with rational adversaries, enabling reliable data recovery and Sybil resistance.


<details>
  <summary>Details</summary>
Motivation: The need arises for coding solutions to address rational adversaries in decentralized systems, particularly in decentralized machine learning, where participants act strategically due to incentives.

Method: The paper proposes a game-theoretic approach called the 'game of coding' that extends traditional coding theory to scenarios where adversarial nodes outnumber honest ones.

Result: It demonstrates that data recovery is possible even with a majority of adversarial nodes, and the method resists Sybil attacks by maintaining stable equilibria.

Conclusion: The introduced framework achieves reliable communication in trust-minimized environments and opens avenues for addressing unknown adversary strategies in future studies.

Abstract: Coding theory plays a crucial role in enabling reliable communication, storage, and computation. Classical approaches assume a worst-case adversarial model and ensure error correction and data recovery only when the number of honest nodes exceeds the number of adversarial ones by some margin. However, in some emerging decentralized applications, particularly in decentralized machine learning (DeML), participating nodes are rewarded for accepted contributions. This incentive structure naturally gives rise to rational adversaries who act strategically rather than behaving in purely malicious ways.
  In this paper, we first motivate the need for coding in the presence of rational adversaries, particularly in the context of outsourced computation in decentralized systems. We contrast this need with existing approaches and highlight their limitations. We then introduce the game of coding, a novel game-theoretic framework that extends coding theory to trust-minimized settings where honest nodes are not in the majority. Focusing on repetition coding, we highlight two key features of this framework: (1) the ability to achieve a non-zero probability of data recovery even when adversarial nodes are in the majority, and (2) Sybil resistance, i.e., the equilibrium remains unchanged even as the number of adversarial nodes increases. Finally, we explore scenarios in which the adversary's strategy is unknown and outline several open problems for future research.

</details>


### [459] [DatBench: Discriminative, Faithful, and Efficient VLM Evaluations](https://arxiv.org/abs/2601.02316)
*Siddharth Joshi,Haoli Yin,Rishabh Adiga,Ricardo Monti,Aldo Carranza,Alex Fang,Alvin Deng,Amro Abbas,Brett Larsen,Cody Blakeney,Darren Teh,David Schwab,Fan Pan,Haakon Mongstad,Jack Urbanek,Jason Lee,Jason Telanoff,Josh Wills,Kaleigh Mentzer,Luke Merrick,Parth Doshi,Paul Burstein,Pratyush Maini,Scott Loftin,Spandan Das,Tony Jiang,Vineeth Dorna,Zhengping Wang,Bogdan Gaza,Ari Morcos,Matthew Leavitt*

Main category: cs.LG

TL;DR: The paper addresses evaluation practices in vision-language models (VLMs), proposing improvements to enhance faithfulness, discriminability, and efficiency. It highlights critical flaws in current methods and introduces cleaned datasets to optimize evaluation.


<details>
  <summary>Details</summary>
Motivation: Evaluation methods for foundation models are underdeveloped, leading to inefficiencies and inaccuracies in assessing VLM capabilities.

Method: The authors propose desiderata for evaluation, identify failure modes in current datasets, and introduce transformations and filtering to improve evaluation benchmarks. They also release optimized datasets.

Result: Converting evaluation formats and filtering flawed samples revealed significant capability differences while reducing computational costs. The optimized dataset achieves up to a 50x speedup while retaining discriminative power.

Conclusion: Rigorous and sustainable evaluation practices are critical for advancing VLMs, and the curated benchmark datasets provide a pathway to achieve this.

Abstract: Empirical evaluation serves as the primary compass guiding research progress in foundation models. Despite a large body of work focused on training frontier vision-language models (VLMs), approaches to their evaluation remain nascent. To guide their maturation, we propose three desiderata that evaluations should satisfy: (1) faithfulness to the modality and application, (2) discriminability between models of varying quality, and (3) efficiency in compute. Through this lens, we identify critical failure modes that violate faithfulness and discriminability, misrepresenting model capabilities: (i) multiple-choice formats reward guessing, poorly reflect downstream use cases, and saturate early as models improve; (ii) blindly solvable questions, which can be answered without images, constitute up to 70% of some evaluations; and (iii) mislabeled or ambiguous samples compromise up to 42% of examples in certain datasets. Regarding efficiency, the computational burden of evaluating frontier models has become prohibitive: by some accounts, nearly 20% of development compute is devoted to evaluation alone. Rather than discarding existing benchmarks, we curate them via transformation and filtering to maximize fidelity and discriminability. We find that converting multiple-choice questions to generative tasks reveals sharp capability drops of up to 35%. In addition, filtering blindly solvable and mislabeled samples improves discriminative power while simultaneously reducing computational cost. We release DatBench-Full, a cleaned evaluation suite of 33 datasets spanning nine VLM capabilities, and DatBench, a discriminative subset that achieves 13x average speedup (up to 50x) while closely matching the discriminative power of the original datasets. Our work outlines a path toward evaluation practices that are both rigorous and sustainable as VLMs continue to scale.

</details>


### [460] [Heterogeneous Low-Bandwidth Pre-Training of LLMs](https://arxiv.org/abs/2601.02360)
*Yazan Obeidi,Amir Sarfi,Joel Lidin,Paul Janson,Eugene Belilovsky*

Main category: cs.LG

TL;DR: The paper combines sparse communication via SparseLoCo with compressed activation in pipeline model parallelism to alleviate bandwidth constraints in distributed pre-training of large language models.


<details>
  <summary>Details</summary>
Motivation: The increasing need for distributed computation to pre-train large language models faces challenges due to bandwidth limitations, especially when model parallelism requires frequent large-scale inter-device communication.

Method: A heterogeneous distributed training framework combining SparseLoCo with compressed pipeline model parallelism is introduced. It employs infrequent synchronization, sparse pseudo-gradient exchange, and subspace-projected communication to adapt to varying resource conditions.

Result: Experiments on language models of 178M-1B parameters demonstrate that activation compression integrates effectively with SparseLoCo, and selective compression improves the communication tradeoff while maintaining performance at higher compression ratios.

Conclusion: The study provides a practical approach for enabling distributed LLM pre-training, incorporating low-bandwidth model parallelism and support for heterogeneous participants.

Abstract: Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.

</details>


### [461] [Real-Time Human Detection for Aerial Captured Video Sequences via Deep Models](https://arxiv.org/abs/2601.00391)
*Nouar AlDahoul,Aznul Qalid Md Sabri,Ali Mohammed Mansoor*

Main category: cs.LG

TL;DR: The paper proposes automatic feature learning methods using deep models for human detection in aerial videos. It evaluates models based on accuracy and learning speed.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of handcrafted feature-based human detection methods, which are task-specific and sensitive to environmental changes like illumination or camera jitter.

Method: Combining optical flow with three deep models: supervised CNN, pretrained CNN feature extractor, and hierarchical extreme learning machine (H-ELM). Evaluation is performed using aerial video data from the UCF-ARG dataset.

Result: Pretrained CNN achieves the highest average accuracy at 98.09%. S-CNN delivers 95.6% accuracy (Softmax) and 91.7% (SVM), while H-ELM produces 95.9% accuracy with faster training duration compared to S-CNN.

Conclusion: Automatic feature learning methods show promising results for human detection tasks in dynamic aerial environments, making them superior to traditional handcrafted techniques.

Abstract: Human detection in videos plays an important role in various real-life applications. Most traditional approaches depend on utilizing handcrafted features, which are problem-dependent and optimal for specific tasks. Moreover, they are highly susceptible to dynamical events such as illumination changes, camera jitter, and variations in object sizes. On the other hand, the proposed feature learning approaches are cheaper and easier because highly abstract and discriminative features can be produced automatically without the need of expert knowledge. In this paper, we utilize automatic feature learning methods, which combine optical flow and three different deep models (i.e., supervised convolutional neural network (S-CNN), pretrained CNN feature extractor, and hierarchical extreme learning machine) for human detection in videos captured using a nonstatic camera on an aerial platform with varying altitudes. The models are trained and tested on the publicly available and highly challenging UCF-ARG aerial dataset. The comparison between these models in terms of training, testing accuracy, and learning speed is analyzed. The performance evaluation considers five human actions (digging, waving, throwing, walking, and running). Experimental results demonstrated that the proposed methods are successful for the human detection task. The pretrained CNN produces an average accuracy of 98.09%. S-CNN produces an average accuracy of 95.6% with softmax and 91.7% with Support Vector Machines (SVM). H-ELM has an average accuracy of 95.9%. Using a normal Central Processing Unit (CPU), H-ELM's training time takes 445 seconds. Learning in S-CNN takes 770 seconds with a high-performance Graphical Processing Unit (GPU).

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [462] [Implementation of high-efficiency, lightweight residual spiking neural network processor based on field-programmable gate arrays](https://arxiv.org/abs/2601.00802)
*Hou Yue,Xiang Shuiying,Zou Tao,Huang Zhiquan,Shi Shangxuan,Guo Xingxing,Zhang Yahui,Zheng Ling,Hao Yue*

Main category: cs.NE

TL;DR: The paper introduces an efficient Spiking Neural Network (SNN) accelerator leveraging hardware-software co-design to optimize for energy efficiency and inference speed on FPGAs.


<details>
  <summary>Details</summary>
Motivation: Existing SNN processors depend on multi-timestep training and architectures that result in increased computational overhead and lower deployment efficiency.

Method: The proposed solution uses single-timestep training, grouped convolutions, batch normalization fusion, quantization-aware training (QAT) for 8-bit parameters, and hardware optimizations like resource reuse, pipeline design, and on-chip storage.

Result: The accelerator achieved 87.11% accuracy on CIFAR-10, 3.98 ms/image inference time, and 183.5 FPS/W energy efficiency—doubling energy efficiency compared to GPUs and outperforming other SNN processors in speed and efficiency.

Conclusion: The co-designed system demonstrates significant improvements in efficiency, accuracy, and inference speed, making it a promising solution for SNN deployment on FPGAs.

Abstract: With the development of hardware-optimized deployment of spiking neural networks (SNNs), SNN processors based on field-programmable gate arrays (FPGAs) have become a research hotspot due to their efficiency and flexibility. However, existing methods rely on multi-timestep training and reconfigurable computing architectures, which increases computational and memory overhead, thus reducing deployment efficiency. This work presents an efficient and lightweight residual SNN accelerator that combines algorithm and hardware co-design to optimize inference energy efficiency. In terms of the algorithm, we employ single-timesteps training, integrate grouped convolutions, and fuse batch normalization (BN) layers, thus compressing the network to only 0.69M parameters. Quantization-aware training (QAT) further constrains all parameters to 8-bit precision. In terms of hardware, the reuse of intra-layer resources maximizes FPGA utilization, a full pipeline cross-layer architecture improves throughput, and on-chip block RAM (BRAM) stores network parameters and intermediate results to improve memory efficiency. The experimental results show that the proposed processor achieves a classification accuracy of 87.11% on the CIFAR-10 dataset, with an inference time of 3.98 ms per image and an energy efficiency of 183.5 FPS/W. Compared with mainstream graphics processing unit (GPU) platforms, it achieves more than double the energy efficiency. Furthermore, compared with other SNN processors, it achieves at least a 4x faster inference speed and a 5x higher energy efficiency.

</details>


### [463] [Optimal Traffic Relief Road Design using Bilevel Programming and Greedy Seeded Simulated Annealing: A Case Study of Kinshasa](https://arxiv.org/abs/2601.00804)
*Yves Matanga,Chunling Du,Etienne van Wyk*

Main category: cs.NE

TL;DR: The study introduced a traffic flow-based algorithm using a network design problem to prioritize road projects in Kinshasa, resulting in substantial travel time reductions and suggestions for critical road segment developments.


<details>
  <summary>Details</summary>
Motivation: To address severe traffic congestion in Kinshasa and overcome financial constraints for infrastructure projects in the DRC by optimizing the prioritization of road construction projects.

Method: A transport network design problem (TNDP) was formulated using demand data specific to Kinshasa. Metaheuristics, including hybridized simulated annealing (SA) and Tabu search (TS), were applied to achieve optimized solutions for a 30-node network.

Result: The proposed Greedy Simulated Annealing and Greedy Tabu search techniques significantly reduced travel time and improved network centrality by roughly 2.5 times, with stable results compared to other methods.

Conclusion: Key road segments for prioritization were identified, emphasizing junctions connecting Bandundu and Kongo Central to main destinations like Gombe and the airport, alongside key inner-city areas.

Abstract: Context: The city of Kinshasa faces severe traffic congestion, requiring strategic infrastructure capacity enhancements. Although a comprehensive master plan has been proposed, its implementation requires substantial financial investment, which remains constrained in the Democratic Republic of the Congo (DRC), an emerging economy. This research proposes a traffic flow based algorithm to support the development of priority road segments. The objective is to enable more effective prioritisation of road construction projects and facilitate the optimal allocation of limited infrastructure budgets.
  Methods: The study was conducted by formulating a standard transport network design problem (TNDP) that included estimated origin demand data specific to the city of Kinshasa. Given the high computational nature of the 30 node network design, TNDP relevant metaheuristics (GA, ACO, PSO, SA, TS, Greedy) were used selectively and hybridised to achieve high quality, stable solutions. A greedy search seeded simulated annealing and Tabu search were devised to achieve the design goals.
  Results: Greedy Simulated Annealing and Greedy Tabu search yielded the best travel time reduction and the most stable solutions compared to other solvers, also improving network edge betweenness centrality by nearly a scale of two and a half.
  Conclusions: Road priorities were proposed, including junctions connecting the Bandundu and Kongo Central entry point to main attraction centres (Limete Poids Lourd, Gombe, Airport) and additional inner city areas (Ngaliema, Selembao, Lemba, Masina, Kimwenza).

</details>


### [464] [ChronoPlastic Spiking Neural Networks](https://arxiv.org/abs/2601.00805)
*Sarim Chaudhry*

Main category: cs.NE

TL;DR: This paper introduces ChronoPlastic Spiking Neural Networks (CPSNNs), which improve spiking neural networks' handling of long-range temporal dependencies by dynamically modulating synaptic decay rates.


<details>
  <summary>Details</summary>
Motivation: Spiking neural networks are energy-efficient but struggle to deal with long-term temporal dependencies using fixed synaptic/membrane constants.

Method: CPSNNs embed adaptive temporal modulation in synaptic dynamics, learning task-relevant time-warping functions to selectively preserve useful information.

Result: CPSNNs demonstrated better learning efficiency and reliability for long-gap temporal dependencies compared to standard SNNs, achieving scalability in temporal tasks.

Conclusion: Adaptive temporal modulation is essential for scalable and efficient temporal learning in spiking neural systems, as implemented by CPSNNs.

Abstract: Spiking neural networks (SNNs) offer a biologically grounded and energy-efficient alternative to conventional neural architectures; however, they struggle with long-range temporal dependencies due to fixed synaptic and membrane time constants. This paper introduces ChronoPlastic Spiking Neural Networks (CPSNNs), a novel architectural principle that enables adaptive temporal credit assignment by dynamically modulating synaptic decay rates conditioned on the state of the network. CPSNNs maintain multiple internal temporal traces and learn a continuous time-warping function that selectively preserves task-relevant information while rapidly forgetting noise. Unlike prior approaches based on adaptive membrane constants, attention mechanisms, or external memory, CPSNNs embed temporal control directly within local synaptic dynamics, preserving linear-time complexity and neuromorphic compatibility. We provide a formal description of the model, analyze its computational properties, and demonstrate empirically that CPSNNs learn long-gap temporal dependencies significantly faster and more reliably than standard SNN baselines. Our results suggest that adaptive temporal modulation is a key missing ingredient for scalable temporal learning in spiking systems.

</details>


### [465] [Energy-Efficient Eimeria Parasite Detection Using a Two-Stage Spiking Neural Network Architecture](https://arxiv.org/abs/2601.00806)
*Ángel Miguel García-Vico,Huseyin Seker,Muhammad Afzal*

Main category: cs.NE

TL;DR: This paper introduces a two-stage Spiking Neural Network (SNN) architecture for energy-efficient and accurate classification of Eimeria parasite in poultry, achieving 98.32% accuracy with significantly reduced energy consumption.


<details>
  <summary>Details</summary>
Motivation: Address the need for rapid and energy-efficient diagnostic tools for coccidiosis in resource-constrained settings.

Method: The model converts a pre-trained Convolutional Neural Network into a spiking feature extractor and utilizes an unsupervised SNN classifier trained with Spike-Timing-Dependent Plasticity (STDP).

Result: Achieved 98.32% classification accuracy in diagnosing Eimeria and reduced energy consumption by over 223 times compared to traditional ANN methods.

Conclusion: The proposed solution establishes a balance between high accuracy and extreme energy efficiency, promoting low-power diagnostic applications suitable for neuromorphic hardware.

Abstract: Coccidiosis, a disease caused by the Eimeria parasite, represents a major threat to the poultry and rabbit industries, demanding rapid and accurate diagnostic tools. While deep learning models offer high precision, their significant energy consumption limits their deployment in resource-constrained environments. This paper introduces a novel two-stage Spiking Neural Network (SNN) architecture, where a pre-trained Convolutional Neural Network is first converted into a spiking feature extractor and then coupled with a lightweight, unsupervised SNN classifier trained with Spike-Timing-Dependent Plasticity (STDP). The proposed model sets a new state-of-the-art, achieving 98.32\% accuracy in Eimeria classification. Remarkably, this performance is accomplished with a significant reduction in energy consumption, showing an improvement of more than 223 times compared to its traditional ANN counterpart. This work demonstrates a powerful synergy between high accuracy and extreme energy efficiency, paving the way for autonomous, low-power diagnostic systems on neuromorphic hardware.

</details>


### [466] [Evolutionary optimization of spatially-distributed multi-sensors placement for indoor surveillance environments with security levels](https://arxiv.org/abs/2601.00826)
*Luis M. Moreno-Saavedra,Vinıcius G. Costa,Adrian Garrido-Saez,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,Sancho Salcedo-Sanz*

Main category: cs.NE

TL;DR: The paper addresses an optimized approach for indoor surveillance multisensor placement using an advanced evolutionary algorithm.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve the indoor surveillance sensor placement problem, minimizing costs while maximizing coverage and adapting to security scenarios such as military installations.

Method: Proposes an evolutionary algorithm with innovative integer encoding and binary conversion for better performance, incorporating detection probability influenced by sensor distance.

Result: The algorithm successfully demonstrated excellent results across various problem instances, reducing sensor placement costs and improving convergence times.

Conclusion: The evolutionary approach introduced is effective for optimizing indoor surveillance multisensor placement while considering real-life constraints like detection probability and security levels.

Abstract: The surveillance multisensor placement is an important optimization problem that consists of positioning several sensors of different types to maximize the coverage of a determined area while minimizing the cost of the deployment. In this work, we tackle a modified version of the problem, consisting of spatially distributed multisensor placement for indoor surveillance. Our approach is focused on security surveillance of sensible indoor spaces, such as military installations, where distinct security levels can be considered. We propose an evolutionary algorithm to solve the problem, in which a novel special encoding,integer encoding with binary conversion, and effective initialization have been defined to improve the performance and convergence of the proposed algorithm. We also consider the probability of detection for each surveillance point, which depends on the distance to the sensor at hand, to better model real-life scenarios. We have tested the proposed evolutionary approach in different instances of the problem, varying both size and difficulty, and obtained excellent results in terms of the cost of sensors placement and convergence time of the algorithm.

</details>


### [467] [Benchmarking Continuous Dynamic Multi-Objective Optimization: Survey and Generalized Test Suite](https://arxiv.org/abs/2601.01317)
*Chang Shao,Qi Zhao,Nana Pu,Shi Cheng,Jing Jiang,Yuhui Shi*

Main category: cs.NE

TL;DR: The paper introduces a framework for developing realistic benchmarks for dynamic multi-objective optimization problems, addressing the complexities of real-world dynamic systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for advanced benchmarks in dynamic multi-objective optimization, as many real-world applications evolve over time and can benefit from rigorous algorithm evaluation.

Method: The framework includes a generalized formulation for Pareto-optimal set changes, mechanisms for heterogeneous landscapes and variable interactions, temporal perturbations, and time-linkage mechanisms to simulate realistic dynamic conditions.

Result: Experimental results show that the proposed framework outperforms conventional benchmarks in realism, complexity, and ability to evaluate algorithmic performance effectively.

Conclusion: The work introduces a comprehensive benchmarking framework, setting a new standard for evaluating dynamic multi-objective optimization algorithms in realistic scenarios.

Abstract: Dynamic multi-objective optimization (DMOO) has recently attracted increasing interest from both academic researchers and engineering practitioners, as numerous real-world applications that evolve over time can be naturally formulated as dynamic multi-objective optimization problems (DMOPs). This growing trend necessitates advanced benchmarks for the rigorous evaluation of optimization algorithms under realistic conditions. This paper introduces a comprehensive and principled framework for constructing highly realistic and challenging DMOO benchmarks. The proposed framework features several novel components: a generalized formulation that allows the Pareto-optimal Set (PS) to change on hypersurfaces, a mechanism for creating controlled variable contribution imbalances to generate heterogeneous landscapes, and dynamic rotation matrices for inducing time-varying variable interactions and non-separability. Furthermore, we incorporate a temporal perturbation mechanism to simulate irregular environmental changes and propose a generalized time-linkage mechanism that systematically embeds historical solution quality into future problems, thereby capturing critical real-world phenomena such as error accumulation and time-deception. Extensive experimental results validate the effectiveness of the proposed framework, demonstrating its superiority over conventional benchmarks in terms of realism, complexity, and its capability for discriminating state-of-the-art algorithmic performance. This work establishes a new standard for dynamic multi-objective optimization benchmarking, providing a powerful tool for the development and evaluation of next-generation algorithms capable of addressing the complexities of real-world dynamic systems.

</details>


### [468] [STEMNIST: Spiking Tactile Extended MNIST Neuromorphic Dataset](https://arxiv.org/abs/2601.01658)
*Anubhab Tripathi,Li Gaishan,Zhengnan Fu,Chiara Bartolozzi,Bert E. Shi,Arindam Basu*

Main category: cs.NE

TL;DR: The paper introduces STEMNIST, a neuromorphic tactile dataset with 35 alphanumeric classes for benchmarking event-based haptic recognition, aiming to improve tactile sensing technologies.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of large-scale neuromorphic tactile datasets to advance research in robotic manipulation, prosthetics, assistive technologies, and neuromorphic learning.

Method: The researchers created the STEMNIST dataset using a custom 16x16 tactile sensor, collecting 7,700 samples from 34 participants, encoded as over 1 million spike events with adaptive temporal differentiation. Baseline performance was evaluated with CNNs and spiking neural networks.

Result: STEMNIST established performance benchmarks with 90.91% test accuracy using CNNs and 89.16% with spiking neural networks.

Conclusion: STEMNIST advances reproducibility and research in tactile recognition, enabling testing of neuromorphic hardware and learning models for increased efficiency in robotics, biomedical engineering, and human-machine interfaces.

Abstract: Tactile sensing is essential for robotic manipulation, prosthetics and assistive technologies, yet neuromorphic tactile datasets remain limited compared to their visual counterparts. We introduce STEMNIST, a large-scale neuromorphic tactile dataset extending ST-MNIST from 10 digits to 35 alphanumeric classes (uppercase letters A--Z and digits 1--9), providing a challenging benchmark for event-based haptic recognition. The dataset comprises 7,700 samples collected from 34 participants using a custom \(16\times 16\) tactile sensor array operating at 120 Hz, encoded as 1,005,592 spike events through adaptive temporal differentiation. Following EMNIST's visual character recognition protocol, STEMNIST addresses the critical gap between simplified digit classification and real-world tactile interaction scenarios requiring alphanumeric discrimination. Baseline experiments using conventional CNNs (90.91% test accuracy) and spiking neural networks (89.16%) establish performance benchmarks. The dataset's event-based format, unrestricted spatial variability and rich temporal structure makes it suitable for testing neuromorphic hardware and bio-inspired learning algorithms. STEMNIST enables reproducible evaluation of tactile recognition systems and provides a foundation for advancing energy-efficient neuromorphic perception in robotics, biomedical engineering and human-machine interfaces. The dataset, documentation and codes are publicly available to accelerate research in neuromorphic tactile computing.

</details>


### [469] [Yukthi Opus: A Multi-Chain Hybrid Metaheuristic for Large-Scale NP-Hard Optimization](https://arxiv.org/abs/2601.01832)
*SB Danush Vikraman,Hannah Abagail,Prasanna Kesavraj,Gajanan V Honnavar*

Main category: cs.NE

TL;DR: The paper introduces Yukthi Opus (YO), a multi-chain hybrid metaheuristic for solving NP-hard optimization problems under evaluation budget constraints, featuring a dual-phase structure combining exploration and exploitation techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of efficiently solving NP-hard optimization problems, especially in scenarios where evaluations are expensive and budgets are tight.

Method: The methodology includes the integration of Markov Chain Monte Carlo for global exploration, greedy local search for exploitation, and simulated annealing with reheating to escape local minima, along with a spatial blacklist and multi-chain execution for robustness.

Result: The evaluation on benchmarks such as Rastrigin, Traveling Salesman Problem, and Rosenbrock shows that YO achieves competitive results compared to other established optimizers, with significant contributions from MCMC, greedy local search, and multi-chain strategies.

Conclusion: Yukthi Opus effectively balances exploration and exploitation, delivers competitive performance, and reduces sensitivity in expensive optimization problems, making it suitable for constrained evaluation environments.

Abstract: We present Yukthi Opus (YO), a multi-chain hybrid metaheuristic designed for NP-hard optimization under explicit evaluation budget constraints. YO integrates three complementary mechanisms in a structured two-phase architecture: Markov Chain Monte Carlo (MCMC) for global exploration, greedy local search for exploitation, and simulated annealing with adaptive reheating to enable controlled escape from local minima. A dedicated burn-in phase allocates evaluations to probabilistic exploration, after which a hybrid optimization loop refines promising candidates. YO further incorporates a spatial blacklist mechanism to avoid repeated evaluation of poor regions and a multi-chain execution strategy to improve robustness and reduce sensitivity to initialization.
  We evaluate YO on three benchmarks: the Rastrigin function (5D) with ablation studies, the Traveling Salesman Problem with 50 to 200 cities, and the Rosenbrock function (5D) with comparisons against established optimizers including CMA-ES, Bayesian optimization, and accelerated particle swarm optimization. Results show that MCMC exploration and greedy refinement are critical for solution quality, while simulated annealing and multi-chain execution primarily improve stability and variance reduction. Overall, YO achieves competitive performance on large and multimodal problems while maintaining predictable evaluation budgets, making it suitable for expensive black-box optimization settings.

</details>


### [470] [Multi-strategy Improved Northern Goshawk Optimization for WSN Coverage Enhancement](https://arxiv.org/abs/2601.01898)
*Yiran Tian,Yuanjia Liu*

Main category: cs.NE

TL;DR: The paper introduces an advanced approach for improving Wireless Sensor Networks (WSNs) using a multi-strategy integrated Northern Goshawk Optimization (NGO) algorithm, showing promising simulation results.


<details>
  <summary>Details</summary>
Motivation: Current strategies for enhancing the coverage rate of Wireless Sensor Networks (WSNs) face limitations in improving optimization diversity and avoiding local optima issues.

Method: Proposed the multi-strategy integration of the Northern Goshawk Optimization (NGO) algorithm, adding multivariate chaotic mapping for population randomness and a bidirectional population evolutionary dynamics strategy.

Result: Experimental simulations reveal significant improvement in WSN coverage and node connectivity compared to existing methods.

Conclusion: The multi-strategy NGO algorithm is highly effective in enhancing both coverage and connectivity in WSNs, surpassing traditional approaches.

Abstract: To enhance the coverage rate of Wireless Sensor Networks (WSNs), this paper proposes an advanced optimization strategy based on a multi-strategy integrated Northern Goshawk Optimization (NGO) algorithm. Specifically, multivariate chaotic mapping is first employed to improve the randomness and uniformity of the initial population. To further bolster population diversity and prevent the algorithm from stagnating in local optima, a bidirectional population evolutionary dynamics strategy is incorporated following the pursuit-and-evasion phase, thereby facilitating the attainment of the global optimal solution. Extensive simulations were conducted to evaluate the performance of the proposed multi-strategy NGO in WSN coverage. Experimental results demonstrate that the proposed algorithm significantly outperforms existing benchmarks in terms of both coverage enhancement and node connectivity.

</details>


### [471] [Toward Thermodynamic Reservoir Computing: Exploring SHA-256 ASICs as Potential Physical Substrates](https://arxiv.org/abs/2601.01916)
*Francisco Angulo de Lafuente,Vladimir Veselov,Richard Goodman*

Main category: cs.NE

TL;DR: The paper introduces Holographic Reservoir Computing (HRC), proposing Bitcoin mining ASICs as a novel computing substrate, outlining theoretical and experimental observations.


<details>
  <summary>Details</summary>
Motivation: Explore thermodynamic noise and timing dynamics in Bitcoin ASICs as a computational substrate for neuromorphic applications.

Method: Developed CHIMERA system architecture utilizing controlled voltage/frequency conditions and studied timing dynamics during SHA-256 hashing pipeline.

Result: Observed non-Poissonian inter-arrival time variability ('Silicon Heartbeat') suggesting potential neuromorphic utility and theoretical energy efficiency.

Conclusion: Theoretical framework suggests feasibility but emphasizes experimental validation; repurposing obsolete cryptographic hardware for thermodynamic computing.

Abstract: We propose a theoretical framework--Holographic Reservoir Computing (HRC)--which hypothesizes that the thermodynamic noise and timing dynamics in voltage-stressed Bitcoin mining ASICs (BM1366) could potentially serve as a physical reservoir computing substrate. We present the CHIMERA (Conscious Hybrid Intelligence via Miner-Embedded Resonance Architecture) system architecture, which treats the SHA-256 hashing pipeline not as an entropy source, but as a deterministic diffusion operator whose timing characteristics under controlled voltage and frequency conditions may exhibit computationally useful dynamics. We report preliminary observations of non-Poissonian variability in inter-arrival time statistics during edge-of-stability operation, which we term the "Silicon Heartbeat" hypothesis. Theoretical analysis based on Hierarchical Number System (HNS) representations suggests that such architectures could achieve O(log n) energy scaling compared to traditional von Neumann O(2^n) dependencies. However, we emphasize that these are theoretical projections requiring experimental validation. We present the implemented measurement infrastructure, acknowledge current limitations, and outline the experimental program necessary to confirm or refute these hypotheses. This work contributes to the emerging field of thermodynamic computing by proposing a novel approach to repurposing obsolete cryptographic hardware for neuromorphic applications.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [472] [BALI: Branch-Aware Loop Invariant Inference with Large Language Models](https://arxiv.org/abs/2601.00882)
*Mingxiu Wang,Jiawei Wang,Xiao Cheng*

Main category: cs.PL

TL;DR: BALI introduces a new approach to automate loop invariant inference using LLMs combined with branch-aware static program analysis, improving precision and scalability.


<details>
  <summary>Details</summary>
Motivation: Deriving loop invariants is a challenging manual task, especially for complex programs, which this paper aims to simplify and automate.

Method: The paper introduces BALI, which verifies branch-sequence-level clauses with SMT and then merges them into program-level invariants, incorporating LLMs for improved inference.

Result: Preliminary results demonstrate that BALI enhances the automated reasoning and generation of loop invariants significantly.

Conclusion: BALI represents a promising advancement toward fully automated invariant discovery, showcasing improvements in precision and scalability.

Abstract: Loop invariants are fundamental for reasoning about the correctness of iterative algorithms. However, deriving suitable invariants remains a challenging and often manual task, particularly for complex programs. In this paper, we introduce BALI, a branch-aware framework that integrates large language models (LLMs) to enhance the inference and verification of loop invariants. Our approach combines automated reasoning with branch-aware static program analysis to improve both precision and scalability. Specifically, unlike prior LLM-only guess-and-check methods, BALI first verifies branch-sequence-level (path-level) clauses with SMT and then composes them into program-level invariants. We outline its key components, present preliminary results, and discuss future directions toward fully automated invariant discovery.

</details>


### [473] [The New Compiler Stack: A Survey on the Synergy of LLMs and Compilers](https://arxiv.org/abs/2601.02045)
*Shuoming Zhang,Jiacheng Zhao,Qiuchu Yu,Chunwei Xia,Zheng Wang,Xiaobing Feng,Huimin Cui*

Main category: cs.PL

TL;DR: This paper provides an overview of LLM-enabled compilation, proposing a taxonomy, outlining benefits, and identifying challenges, while charting a roadmap for future research.


<details>
  <summary>Details</summary>
Motivation: To explore the integration of large language models in the field of compilation and develop a systematic understanding of their impact, advancements, and challenges.

Method: A comprehensive, multi-dimensional taxonomy was developed to categorize works, and an analysis was conducted to identify benefits, challenges, and future opportunities within LLM-enabled compilation.

Result: Identified key benefits include democratizing compiler development, discovering novel optimization strategies, and expanding compiler scope, along with highlighting challenges such as correctness and scalability.

Conclusion: The paper presents a foundational roadmap for leveraging LLMs in compilation, advocating for hybrid systems as the path forward for intelligent and adaptive tools.

Abstract: This survey has provided a systematic overview of the emerging field of LLM-enabled compilation by addressing several key research questions. We first answered how LLMs are being integrated by proposing a comprehensive, multi-dimensional taxonomy that categorizes works based on their Design Philosophy (Selector, Translator, Generator), LLM Methodology, their operational Level of Code Abstraction, and the specific Task Type they address. In answering what advancements these approaches offer, we identified three primary benefits: the democratization of compiler development, the discovery of novel optimization strategies, and the broadening of the compiler's traditional scope. Finally, in addressing the field's challenges and opportunities, we highlighted the critical hurdles of ensuring correctness and achieving scalability, while identifying the development of hybrid systems as the most promising path forward. By providing these answers, this survey serves as a foundational roadmap for researchers and practitioners, charting the course for a new generation of LLM-powered, intelligent, adaptive and synergistic compilation tools.

</details>


### [474] [Perish or Flourish? A Holistic Evaluation of Large Language Models for Code Generation in Functional Programming](https://arxiv.org/abs/2601.02060)
*Nguyet-Anh H. Lang,Eric Lang,Thanh Le-Cong,Bach Le,Quyet-Thang Huynh*

Main category: cs.PL

TL;DR: The paper investigates how Large Language Models (LLMs) like GPT-3.5, GPT-4 and GPT-5 perform on functional programming (FP) tasks, introducing a framework called FPEval.


<details>
  <summary>Details</summary>
Motivation: To address the lack of focus on functional programming performance in the context of LLM-based code generation, despite the growing application of LLMs in software development.

Method: The authors created FPEval, an evaluation framework that assesses LLM performance on 721 tasks across Haskell, Ocaml, Scala, and Java. The framework uses dynamic test validation and static analysis tools.

Result: LLMs show better performance as models advance, but struggle more with purely functional languages (Haskell, Ocaml) compared to hybrid or imperative languages (Scala, Java). Issues in functional idioms and maintainable coding persist.

Conclusion: While LLM advancements aid functional programming, challenges in code correctness, idiomatic usage, and maintainable style highlight areas for improvement. Feedback-driven self-repair shows promise.

Abstract: Functional programming provides strong foundations for developing reliable and secure software systems, yet its adoption remains not widespread due to the steep learning curve. Recent advances in Large Language Models (LLMs) for code generation present new opportunities to lower these barriers. However, extensive evaluations of LLMs largely focus on imperative programming languages, and their capabilities in functional programming languages (FP) remain underexplored. To address this gap, we introduce FPEval, a holistic evaluation framework built on FPBench, a new benchmark of 721 programming tasks across three difficulty levels on three mainstream FP languages: Haskell, Ocaml and Scala. FPEval provides compehensive evaluation infrastructures with both test validations with comprehensive test suites and static analysis tools to assess both functional correctness and code style and maintainability. Using this framework, we evaluate state-of-the-art LLMs, including GPT-3.5, GPT-4o, and GPT-5, for code generation in functional programming languages and Java as an imperative baseline. Our results demonstrate that LLM performance in functional programming improves substantially with model advancement; however, error rates remain significantly higher in purely functional languages (Haskell and OCaml) than in hybrid (Scala) or imperative (Java) languages. Moreover, LLMs frequently generate non-idiomatic functional code that follows imperative patterns, raising concerns about code style and long-term maintainability. Finally, we show that LLMs can partially self-repair both correctness and quality issues when provided with static analysis feedback and hand-crafted instructions for common types of issues.

</details>


### [475] [MLIR-Smith: A Novel Random Program Generator for Evaluating Compiler Pipelines](https://arxiv.org/abs/2601.02218)
*Berke Ates,Filip Dobrosavljević,Theodoros Theodoridis,Zhendong Su*

Main category: cs.PL

TL;DR: MLIR-Smith is introduced as a tool for testing MLIR-based compilers, overcoming limitations of prior methods like Csmith and discovering bugs in multiple compiler pipelines.


<details>
  <summary>Details</summary>
Motivation: To address the need for adaptable testing tools specific to extensible MLIR compiler optimizations.

Method: Development and utilization of MLIR-Smith, a random MLIR program generator, for differential testing across various compiler pipelines.

Result: MLIR-Smith successfully identified numerous bugs in MLIR, LLVM, DaCe, and DCIR, proving its efficacy.

Conclusion: MLIR-Smith advances the evaluation and debugging of compilers, laying groundwork for future testing tools in software development.

Abstract: Compilers are essential for the performance and correct execution of software and hold universal relevance across various scientific disciplines. Despite this, there is a notable lack of tools for testing and evaluating them, especially within the adaptable Multi-Level Intermediate Representation (MLIR) context. This paper addresses the need for a tool that can accommodate MLIR's extensibility, a feature not provided by previous methods such as Csmith. Here we introduce MLIR-Smith, a novel random program generator specifically designed to test and evaluate MLIR-based compiler optimizations. We demonstrate the utility of MLIR-Smith by conducting differential testing on MLIR, LLVM, DaCe, and DCIR, which led to the discovery of multiple bugs in these compiler pipelines. The introduction of MLIR-Smith not only fills a void in the realm of compiler testing but also emphasizes the importance of comprehensive testing within these systems. By providing a tool that can generate random MLIR programs, this paper enhances our ability to evaluate and improve compilers and paves the way for future tools, potentially shaping the wider landscape of software testing and quality assurance.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [476] [Value Vision-Language-Action Planning & Search](https://arxiv.org/abs/2601.00969)
*Ali Salamatian,Ke,Ren,Kieran Pattison,Cyrus Neary*

Main category: cs.RO

TL;DR: The paper introduces V-VLAPS, a framework that integrates a lightweight, learnable value function with MCTS for Vision-Language-Action (VLA) models, enhancing robotic manipulation performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address robustness issues and inefficiency arising from the reliance of VLA models on mere behavior cloning and test-time search algorithms without grounded expected return estimation.

Method: The method involves augmenting Monte Carlo Tree Search (MCTS) with an explicit value function trained on latent representations of a fixed VLA backbone (Octo) using a lightweight MLP model.

Result: The results show that V-VLAPS increases success rates by over 5 percentage points and reduces MCTS simulation requirements by 5-15 percent on the LIBERO robotic manipulation suite.

Conclusion: Integrating a value function into MCTS improves the efficacy and efficiency of VLA models for robotic manipulation tasks, solving previous brittleness under distribution shifts.

Abstract: Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic manipulation, yet they remain fundamentally limited by their reliance on behavior cloning, leading to brittleness under distribution shift. While augmenting pretrained models with test-time search algorithms like Monte Carlo Tree Search (MCTS) can mitigate these failures, existing formulations rely solely on the VLA prior for guidance, lacking a grounded estimate of expected future return. Consequently, when the prior is inaccurate, the planner can only correct action selection via the exploration term, which requires extensive simulation to become effective. To address this limitation, we introduce Value Vision-Language-Action Planning and Search (V-VLAPS), a framework that augments MCTS with a lightweight, learnable value function. By training a simple multilayer perceptron (MLP) on the latent representations of a fixed VLA backbone (Octo), we provide the search with an explicit success signal that biases action selection toward high-value regions. We evaluate V-VLAPS on the LIBERO robotic manipulation suite, demonstrating that our value-guided search improves success rates by over 5 percentage points while reducing the average number of MCTS simulations by 5-15 percent compared to baselines that rely only on the VLA prior.

</details>


### [477] [From Perception to Symbolic Task Planning: Vision-Language Guided Human-Robot Collaborative Structured Assembly](https://arxiv.org/abs/2601.00978)
*Yanyi Chen,Min Deng*

Main category: cs.RO

TL;DR: The paper introduces a framework for human-robot collaborative structured assembly using vision-language models for state estimation and adaptive planning for stable task execution under dynamic conditions.


<details>
  <summary>Details</summary>
Motivation: To achieve reliable state estimation and robust task planning in human-robot collaboration during structured assembly, despite noisy perception and human interventions.

Method: The framework includes (1) a Perception-to-Symbolic State (PSS) module which uses vision-language models for state tracking, and (2) a Human-Aware Planning and Replanning (HPR) module for task assignment and selective replanning under deviations.

Result: The PSS module achieved 97% accuracy in state synthesis, and the HPR module ensured consistent task progression in diverse scenarios.

Conclusion: Integrating vision-language-based perception with knowledge-driven planning enhances robustness and stability in human-robot collaborative tasks under dynamic and noisy conditions.

Abstract: Human-robot collaboration (HRC) in structured assembly requires reliable state estimation and adaptive task planning under noisy perception and human interventions. To address these challenges, we introduce a design-grounded human-aware planning framework for human-robot collaborative structured assembly. The framework comprises two coupled modules. Module I, Perception-to-Symbolic State (PSS), employs vision-language models (VLMs) based agents to align RGB-D observations with design specifications and domain knowledge, synthesizing verifiable symbolic assembly states. It outputs validated installed and uninstalled component sets for online state tracking. Module II, Human-Aware Planning and Replanning (HPR), performs task-level multi-robot assignment and updates the plan only when the observed state deviates from the expected execution outcome. It applies a minimal-change replanning rule to selectively revise task assignments and preserve plan stability even under human interventions. We validate the framework on a 27-component timber-frame assembly. The PSS module achieves 97% state synthesis accuracy, and the HPR module maintains feasible task progression across diverse HRC scenarios. Results indicate that integrating VLM-based perception with knowledge-driven planning improves robustness of state estimation and task planning under dynamic conditions.

</details>


### [478] [Simulations of MRI Guided and Powered Ferric Applicators for Tetherless Delivery of Therapeutic Interventions](https://arxiv.org/abs/2601.00981)
*Wenhui Chu,Khang Tran,Nikolaos V. Tsekos*

Main category: cs.RO

TL;DR: A computational platform for preoperative planning and modeling of MRI-powered applicators in blood vessels is proposed.


<details>
  <summary>Details</summary>
Motivation: To aid preoperative planning and ensure safe operations of MRI-powered applicators during procedures inside vascular networks.

Method: Developed a two-way data and command platform linking MRI, computational core, and operator, using virtual fixtures and MR waveforms for modeling safe navigation of applicators.

Result: The platform processes MRI data to design virtual fixtures for safe applicator navigation and models potential maneuvers under different blood flow profiles.

Conclusion: The platform effectively aids procedural planning and offers modeling for improved safety and control of MRI-powered applicators.

Abstract: Magnetic Resonance Imaging (MRI) is a well-established modality for pre-operative planning and is also explored for intra-operative guidance of procedures such as intravascular interventions. Among the experimental robot-assisted technologies, the magnetic field gradients of the MRI scanner are used to power and maneuver ferromagnetic applicators for accessing sites in the patient's body via the vascular network. In this work, we propose a computational platform for preoperative planning and modeling of MRI-powered applicators inside blood vessels. This platform was implemented as a two-way data and command pipeline that links the MRI scanner, the computational core, and the operator. The platform first processes multi-slice MR data to extract the vascular bed and then fits a virtual corridor inside the vessel. This corridor serves as a virtual fixture (VF), a forbidden region for the applicators to avoid vessel perforation or collision. The geometric features of the vessel centerline, the VF, and MRI safety compliance (dB/dt, max available gradient) are then used to generate magnetic field gradient waveforms. Different blood flow profiles can be user-selected, and those parameters are used for modeling the applicator's maneuvering. The modeling module further generates cues about whether the selected vascular path can be safely maneuvered. Given future experimental studies that require a real-time operation, the platform was implemented on the Qt framework (C/C++) with software modules performing specific tasks running on dedicated threads: PID controller, generation of VF, generation of MR gradient waveforms.

</details>


### [479] [Topological Mapping and Navigation using a Monocular Camera based on AnyLoc](https://arxiv.org/abs/2601.01067)
*Wenzheng Zhang,Yoshitaka Hara,Sousuke Nakamura*

Main category: cs.RO

TL;DR: The paper introduces a monocular camera-based method for topological mapping and navigation, emphasizing loop detection and simplified path planning using key nodes.


<details>
  <summary>Details</summary>
Motivation: To develop a lightweight mapping and navigation system that avoids reliance on metric maps or pre-training, improving efficiency and success rates for robots and humans.

Method: Keyframes are converted into descriptors to build topological relationships using AnyLoc. Navigation decisions are made by comparing segmented images with target node images.

Result: Experiments demonstrate improved loop detection and navigation success rates, achieving 60.2% higher success than ResNet-based methods while optimizing time and space usage.

Conclusion: The proposed approach provides a fast, lightweight navigation solution using only a monocular camera, suitable for diverse environments and scenarios.

Abstract: This paper proposes a method for topological mapping and navigation using a monocular camera. Based on AnyLoc, keyframes are converted into descriptors to construct topological relationships, enabling loop detection and map building. Unlike metric maps, topological maps simplify path planning and navigation by representing environments with key nodes instead of precise coordinates. Actions for visual navigation are determined by comparing segmented images with the image associated with target nodes. The system relies solely on a monocular camera, ensuring fast map building and navigation using key nodes. Experiments show effective loop detection and navigation in real and simulation environments without pre-training. Compared to a ResNet-based method, this approach improves success rates by 60.2% on average while reducing time and space costs, offering a lightweight solution for robot and human navigation in various scenarios.

</details>


### [480] [Towards reliable subsea object recovery: a simulation study of an auv with a suction-actuated end effector](https://arxiv.org/abs/2601.01106)
*Michele Grimaldi,Yosaku Maeda,Hitoshi Kakami,Ignacio Carlucho,Yvan Petillot,Tomoya Inoue*

Main category: cs.RO

TL;DR: This paper develops and tests a simulation-based framework for autonomous object recovery missions at deep-sea depths (6,000m) using a Hadal Small Vehicle (HSV) with a robotic arm.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in the hadal zone recovery missions like extreme pressure, low visibility, costly and risky direct experimentation, and difficulty validating autonomous behaviors.

Method: The method involves using the Stonefish simulator to model realistic behaviors of the HSV, combining PID navigation control, inverse-kinematics manipulator control, and acceleration feed-forward techniques for coordinated operations.

Result: Simulation results demonstrate the HSV autonomously performing object detection, seafloor coverage, and successful object recovery in a realistic underwater environment.

Conclusion: High-fidelity simulations are an effective and low-risk tool for validating autonomous deep-sea recovery operations, reducing reliance on costly field experiments.

Abstract: Autonomous object recovery in the hadal zone is challenging due to extreme hydrostatic pressure, limited visibility and currents, and the need for precise manipulation at full ocean depth. Field experimentation in such environments is costly, high-risk, and constrained by limited vehicle availability, making early validation of autonomous behaviors difficult. This paper presents a simulation-based study of a complete autonomous subsea object recovery mission using a Hadal Small Vehicle (HSV) equipped with a three-degree-of-freedom robotic arm and a suction-actuated end effector. The Stonefish simulator is used to model realistic vehicle dynamics, hydrodynamic disturbances, sensing, and interaction with a target object under hadal-like conditions. The control framework combines a world-frame PID controller for vehicle navigation and stabilization with an inverse-kinematics-based manipulator controller augmented by acceleration feed-forward, enabling coordinated vehicle - manipulator operation. In simulation, the HSV autonomously descends from the sea surface to 6,000 m, performs structured seafloor coverage, detects a target object, and executes a suction-based recovery. The results demonstrate that high-fidelity simulation provides an effective and low-risk means of evaluating autonomous deep-sea intervention behaviors prior to field deployment.

</details>


### [481] [Latent Space Reinforcement Learning for Multi-Robot Exploration](https://arxiv.org/abs/2601.01139)
*Sriram Rajasekar,Ashwini Ratnoo*

Main category: cs.RO

TL;DR: This paper presents a system for autonomous mapping with multi-agent collaboration, using autoencoders for dimensionality reduction and hierarchical reinforcement learning for navigation in complex environments.


<details>
  <summary>Details</summary>
Motivation: Mapping unknown environments efficiently, especially under time constraints, is challenging. Multi-agent collaboration can improve efficiency, but motion-planning scalability and application in complex environments remain bottlenecks.

Method: The authors use autoencoders to compress occupancy maps into latent vectors, procedural generation for creating diverse training environments, and hierarchical reinforcement learning for decentralized navigation. They also incorporate a weighted consensus mechanism for data trust modulation.

Result: The system demonstrates scalability with more agents, generalization to unfamiliar environments, and resilience in environments with constrained communication.

Conclusion: The proposed approach successfully combines dimensionality reduction, procedural environment generation, and hierarchical reinforcement learning to enable scalable, efficient, and robust multi-agent navigation.

Abstract: Autonomous mapping of unknown environments is a critical challenge, particularly in scenarios where time is limited. Multi-agent systems can enhance efficiency through collaboration, but the scalability of motion-planning algorithms remains a key limitation. Reinforcement learning has been explored as a solution, but existing approaches are constrained by the limited input size required for effective learning, restricting their applicability to discrete environments. This work addresses that limitation by leveraging autoencoders to perform dimensionality reduction, compressing high-fidelity occupancy maps into latent state vectors while preserving essential spatial information. Additionally, we introduce a novel procedural generation algorithm based on Perlin noise, designed to generate topologically complex training environments that simulate asteroid fields, caves and forests. These environments are used for training the autoencoder and the navigation algorithm using a hierarchical deep reinforcement learning framework for decentralized coordination. We introduce a weighted consensus mechanism that modulates reliance on shared data via a tuneable trust parameter, ensuring robustness to accumulation of errors. Experimental results demonstrate that the proposed system scales effectively with number of agents and generalizes well to unfamiliar, structurally distinct environments and is resilient in communication-constrained settings.

</details>


### [482] [VISO: Robust Underwater Visual-Inertial-Sonar SLAM with Photometric Rendering for Dense 3D Reconstruction](https://arxiv.org/abs/2601.01144)
*Shu Pan,Simon Archieri,Ahmet Cinar,Jonatan Scharff Willners,Ignacio Carlucho,Yvan Petillot*

Main category: cs.RO

TL;DR: The paper introduces VISO, an advanced underwater SLAM system combining stereo cameras, IMU, and 3D sonar for precise localisation and efficient dense 3D mapping.


<details>
  <summary>Details</summary>
Motivation: Underwater environments pose challenges like visual distortions that affect localisation and dense mapping accuracy. The paper aims to mitigate these issues to improve underwater SLAM systems.

Method: VISO integrates stereo camera, IMU, and 3D sonar, using a coarse-to-fine calibration for parameter estimation and introducing a photometric rendering strategy for visual enrichment of sonar data.

Result: Experiments in different environments show VISO's superiority over current underwater and visual SLAM systems in localisation robustness and accuracy, along with real-time 3D reconstruction performance comparable to offline methods.

Conclusion: VISO enhances underwater SLAM capabilities, achieving both robust localisation and efficient 3D reconstruction for applications in challenging underwater conditions.

Abstract: Visual challenges in underwater environments significantly hinder the accuracy of vision-based localisation and the high-fidelity dense reconstruction. In this paper, we propose VISO, a robust underwater SLAM system that fuses a stereo camera, an inertial measurement unit (IMU), and a 3D sonar to achieve accurate 6-DoF localisation and enable efficient dense 3D reconstruction with high photometric fidelity. We introduce a coarse-to-fine online calibration approach for extrinsic parameters estimation between the 3D sonar and the camera. Additionally, a photometric rendering strategy is proposed for the 3D sonar point cloud to enrich the sonar map with visual information. Extensive experiments in a laboratory tank and an open lake demonstrate that VISO surpasses current state-of-the-art underwater and visual-based SLAM algorithms in terms of localisation robustness and accuracy, while also exhibiting real-time dense 3D reconstruction performance comparable to the offline dense mapping method.

</details>


### [483] [ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation](https://arxiv.org/abs/2601.01155)
*Zhang Shizhe,Liang Jingsong,Zhou Zhitao,Ye Shuhan,Wang Yizhuo,Tan Ming Siang Derek,Chiun Jimmy,Cao Yuhong,Sartoretti Guillaume*

Main category: cs.RO

TL;DR: ORION proposes a deep reinforcement learning framework for multi-agent navigation in partially known environments, focusing on balancing path optimality and map uncertainty reduction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in multi-agent navigation within partially known environments, such as warehouses, where agents need to balance trajectory planning and environmental information sharing.

Method: ORION integrates a shared graph encoder and option-critic framework. It utilizes decentralized decision-making, high-level cooperative modes, and a dual-stage strategy to guide agents adaptively between navigation and exploration.

Result: ORION achieves superior decentralized cooperation in simulation tests, outperforming existing baselines, and demonstrates robustness in real-world robot navigation.

Conclusion: ORION is effective and practical for cooperative multi-agent navigation, merging learning strategies for dynamic and uncertain environments.

Abstract: Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation.

</details>


### [484] [DST-Calib: A Dual-Path, Self-Supervised, Target-Free LiDAR-Camera Extrinsic Calibration Network](https://arxiv.org/abs/2601.01188)
*Zhiwei Huang,Yanwei Fu,Yi Zhou,Xieyuanli Chen,Qijun Chen,Rui Fan*

Main category: cs.RO

TL;DR: The paper introduces the first self-supervised, online LiDAR-camera extrinsic calibration method that eliminates reliance on static calibration targets, leveraging a new double-sided data augmentation technique and a simpler yet effective calibration framework.


<details>
  <summary>Details</summary>
Motivation: The motivation of this research is to overcome the limitations of existing LiDAR-camera calibration methods, which depend on handcrafted calibration targets or static scenes, making them less adaptive for real-world, dynamic robotic perception applications.

Method: The method involves introducing a double-sided data augmentation strategy using camera views from depth maps and building a dual-path, self-supervised framework. It incorporates a difference map construction process to enhance feature correlation and reduce model complexity.

Result: Experiments on five public and one custom dataset show that the proposed approach achieves superior calibration accuracy and generalizability compared to existing methods.

Conclusion: The work presents an advanced calibration network that is robust, adaptive, and simplifies the model, making it highly applicable to real-world scenarios requiring online calibration.

Abstract: LiDAR-camera extrinsic calibration is essential for multi-modal data fusion in robotic perception systems. However, existing approaches typically rely on handcrafted calibration targets (e.g., checkerboards) or specific, static scene types, limiting their adaptability and deployment in real-world autonomous and robotic applications. This article presents the first self-supervised LiDAR-camera extrinsic calibration network that operates in an online fashion and eliminates the need for specific calibration targets. We first identify a significant generalization degradation problem in prior methods, caused by the conventional single-sided data augmentation strategy. To overcome this limitation, we propose a novel double-sided data augmentation technique that generates multi-perspective camera views using estimated depth maps, thereby enhancing robustness and diversity during training. Built upon this augmentation strategy, we design a dual-path, self-supervised calibration framework that reduces the dependence on high-precision ground truth labels and supports fully adaptive online calibration. Furthermore, to improve cross-modal feature association, we replace the traditional dual-branch feature extraction design with a difference map construction process that explicitly correlates LiDAR and camera features. This not only enhances calibration accuracy but also reduces model complexity. Extensive experiments conducted on five public benchmark datasets, as well as our own recorded dataset, demonstrate that the proposed method significantly outperforms existing approaches in terms of generalizability.

</details>


### [485] [EduSim-LLM: An Educational Platform Integrating Large Language Models and Robotic Simulation for Beginners](https://arxiv.org/abs/2601.01196)
*Shenqi Lu,Liangwei Zhang*

Main category: cs.RO

TL;DR: The paper introduces EduSim-LLM, an educational platform that integrates large language models (LLMs) with robotic simulation for intuitive human-robot interaction via natural language instructions.


<details>
  <summary>Details</summary>
Motivation: Integrating natural language understanding with robotic control to enable intuitive human interaction with complex robotic systems and address the challenges in human-robot collaboration.

Method: Developed EduSim-LLM, combining LLMs with CoppeliaSim for translating natural language commands into robot actions. Designed two interaction modes (direct and autonomous control), studied systematic use of multiple LLMs, and evaluated multi-robot collaboration, motion planning, and manipulation.

Result: LLMs demonstrated reliable conversion of language into robot commands. Instruction-parsing accuracy improved significantly with prompt engineering, achieving over 88.9% accuracy in highly complex tasks.

Conclusion: Incorporating LLMs for robotic control considerably improves human-robot interaction by enabling efficient and accurate language-driven robot behavior in simulations.

Abstract: In recent years, the rapid development of Large Language Models (LLMs) has significantly enhanced natural language understanding and human-computer interaction, creating new opportunities in the field of robotics. However, the integration of natural language understanding into robotic control is an important challenge in the rapid development of human-robot interaction and intelligent automation industries. This challenge hinders intuitive human control over complex robotic systems, limiting their educational and practical accessibility. To address this, we present the EduSim-LLM, an educational platform that integrates LLMs with robot simulation and constructs a language-drive control model that translates natural language instructions into executable robot behavior sequences in CoppeliaSim. We design two human-robot interaction models: direct control and autonomous control, conduct systematic simulations based on multiple language models, and evaluate multi-robot collaboration, motion planning, and manipulation capabilities. Experiential results show that LLMs can reliably convert natural language into structured robot actions; after applying prompt-engineering templates instruction-parsing accuracy improves significantly; as task complexity increases, overall accuracy rate exceeds 88.9% in the highest complexity tests.

</details>


### [486] [SAHA: Supervised Autonomous HArvester for selective forest thinning](https://arxiv.org/abs/2601.01282)
*Fang Nan,Meher Malladi,Qingqing Li,Fan Yang,Joonas Juola,Tiziano Guadagnino,Jens Behley,Cesar Cadena,Cyrill Stachniss,Marco Hutter*

Main category: cs.RO

TL;DR: The paper introduces a small-scale robotic harvester (SAHA) for efficient selective thinning in forest management, demonstrating its capabilities in field trials.


<details>
  <summary>Details</summary>
Motivation: To improve forest management through automation by addressing the labor-intensive and complex task of selective thinning, enhancing ecological and economic efficiency.

Method: Developed a robotic harvester platform integrated with learning-based and model-based controls, hydraulic mechanisms, autonomous navigation, and state/terrain estimation for precise operations in forests.

Result: The system successfully performed kilometer-long autonomous missions, demonstrating its ability to navigate, reach and perform selective thinning in northern European forests.

Conclusion: The robotic harvester effectively automates selective thinning, proving its potential for advancing sustainable and efficient robotic forest management.

Abstract: Forestry plays a vital role in our society, creating significant ecological, economic, and recreational value. Efficient forest management involves labor-intensive and complex operations. One essential task for maintaining forest health and productivity is selective thinning, which requires skilled operators to remove specific trees to create optimal growing conditions for the remaining ones. In this work, we present a solution based on a small-scale robotic harvester (SAHA) designed for executing this task with supervised autonomy. We build on a 4.5-ton harvester platform and implement key hardware modifications for perception and automatic control. We implement learning- and model-based approaches for precise control of hydraulic actuators, accurate navigation through cluttered environments, robust state estimation, and reliable semantic estimation of terrain traversability. Integrating state-of-the-art techniques in perception, planning, and control, our robotic harvester can autonomously navigate forest environments and reach targeted trees for selective thinning. We present experimental results from extensive field trials over kilometer-long autonomous missions in northern European forests, demonstrating the harvester's ability to operate in real forests. We analyze the performance and provide the lessons learned for advancing robotic forest management.

</details>


### [487] [Online Estimation and Manipulation of Articulated Objects](https://arxiv.org/abs/2601.01438)
*Russell Buchanan,Adrian Röfer,João Moura,Abhinav Valada,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: This paper presents a method for service robots to manipulate unknown articulated objects by combining learned visual priors and proprioceptive sensing into an analytical model based on Screw Theory.


<details>
  <summary>Details</summary>
Motivation: Service robots must be able to manipulate arbitrary articulated objects independently to automate tasks such as household chores, but current methods either rely on vision alone or presuppose motion observation.

Method: The proposed approach uses a factor graph to integrate visual priors and proprioceptive sensing during manipulation, enabling online articulation estimation based on Screw Theory.

Result: The system achieved 75% success rate in real-world hardware experiments for opening previously unseen articulated objects, validated through both simulations and robotic experiments.

Conclusion: The method effectively enables robots to predict and interact with unknown articulated objects autonomously, improving manipulation capabilities in household tasks.

Abstract: From refrigerators to kitchen drawers, humans interact with articulated objects effortlessly every day while completing household chores. For automating these tasks, service robots must be capable of manipulating arbitrary articulated objects. Recent deep learning methods have been shown to predict valuable priors on the affordance of articulated objects from vision. In contrast, many other works estimate object articulations by observing the articulation motion, but this requires the robot to already be capable of manipulating the object. In this article, we propose a novel approach combining these methods by using a factor graph for online estimation of articulation which fuses learned visual priors and proprioceptive sensing during interaction into an analytical model of articulation based on Screw Theory. With our method, a robotic system makes an initial prediction of articulation from vision before touching the object, and then quickly updates the estimate from kinematic and force sensing during manipulation. We evaluate our method extensively in both simulations and real-world robotic manipulation experiments. We demonstrate several closed-loop estimation and manipulation experiments in which the robot was capable of opening previously unseen drawers. In real hardware experiments, the robot achieved a 75% success rate for autonomous opening of unknown articulated objects.

</details>


### [488] [AIMS: An Adaptive Integration of Multi-Sensor Measurements for Quadrupedal Robot Localization](https://arxiv.org/abs/2601.01561)
*Yujian Qiu,Yuqiu Mu,Wen Yang,Hao Zhu*

Main category: cs.RO

TL;DR: The paper introduces AIMS, a fusion method for enhancing localization accuracy of quadrupedal robots in challenging narrow tunnel-like environments using adaptive adjustments.


<details>
  <summary>Details</summary>
Motivation: To address the localization challenges in narrow tunnel-like settings where traditional methods face issues due to limited geometric constraints and motion estimation errors.

Method: Developing AIMS, an adaptive fusion method combining LiDAR, IMU, and leg odometry under an error-state Kalman filtering framework with adaptive noise covariance matrices.

Result: Experimental results demonstrate that AIMS offers better accuracy and localization robustness compared to existing state-of-the-art methods.

Conclusion: AIMS successfully improves quadrupedal robot localization in degenerate environments through adaptive adjustments and reliability assessments.

Abstract: This paper addresses the problem of accurate localization for quadrupedal robots operating in narrow tunnel-like environments. Due to the long and homogeneous characteristics of such scenarios, LiDAR measurements often provide weak geometric constraints, making traditional sensor fusion methods susceptible to accumulated motion estimation errors. To address these challenges, we propose AIMS, an adaptive LiDAR-IMU-leg odometry fusion method for robust quadrupedal robot localization in degenerate environments. The proposed method is formulated within an error-state Kalman filtering framework, where LiDAR and leg odometry measurements are integrated with IMU-based state prediction, and measurement noise covariance matrices are adaptively adjusted based on online degeneracy-aware reliability assessment. Experimental results obtained in narrow corridor environments demonstrate that the proposed method improves localization accuracy and robustness compared with state-of-the-art approaches.

</details>


### [489] [HanoiWorld : A Joint Embedding Predictive Architecture BasedWorld Model for Autonomous Vehicle Controller](https://arxiv.org/abs/2601.01577)
*Tran Tien Dat,Nguyen Hai An,Nguyen Khanh Viet Dung,Nguyen Duy Duc*

Main category: cs.RO

TL;DR: The paper introduces 'Hanoi-World,' a JEPA-based world model using RNN for safety-aware autonomous driving, achieving effective long-term planning and inference.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning methods for autonomous controllers are data-intensive, underperforming, unstable, and lack safety considerations while self-supervised learning leveraging JEPA shows promise in mimicking human-like learning processes.

Method: The study utilizes JEPA-based approaches combined with RNNs to carry out effective long-term horizontal planning in autonomous driving tasks.

Result: Experiments conducted on the Highway-Env package show Hanoi-World's capability in making driving plans with enhanced safety-awareness and a lower collision rate compared to SOTA baselines.

Conclusion: Hanoi-World demonstrates an innovative approach to autonomous driving with improved efficiency and safety through novel JEPA-based learning and RNN integration.

Abstract: Current attempts of Reinforcement Learning for Autonomous Controller are data-demanding while the results are under-performed, unstable, and unable to grasp and anchor on the concept of safety, and over-concentrating on noise features due to the nature of pixel reconstruction. While current Self-Supervised Learningapproachs that learning on high-dimensional representations by leveraging the JointEmbedding Predictive Architecture (JEPA) are interesting and an effective alternative, as the idea mimics the natural ability of the human brain in acquiring new skill usingimagination and minimal samples of observations. This study introduces Hanoi-World, a JEPA-based world model that using recurrent neural network (RNN) formaking longterm horizontal planning with effective inference time. Experimentsconducted on the Highway-Env package with difference enviroment showcase the effective capability of making a driving plan while safety-awareness, with considerablecollision rate in comparison with SOTA baselines

</details>


### [490] [Action-Sketcher: From Reasoning to Action via Visual Sketches for Long-Horizon Robotic Manipulation](https://arxiv.org/abs/2601.01618)
*Huajie Tan,Peterson Co,Yijie Xu,Shanyu Rong,Yuheng Ji,Cheng Chi,Xiansheng Chen,Qiongyu Zhang,Zhongxia Zhao,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: This paper introduces Visual Sketch, an intermediate representation that externalizes spatial intent, connected with Action-Sketcher, a Vision-Language-Action (VLA) framework focused on improving robotic manipulation across complex and dynamic conditions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve real-world robotic manipulation tasks under conditions requiring spatial and temporal reasoning, addressing limitations of VLA policies that rely primarily on text cues and lack effective grounding and causal explanations.

Method: Visual Sketch renders visual elements like points, boxes, and arrows as intermediates for spatial intent illustration, and the Action-Sketcher framework operates on a See-Think-Sketch-Act cyclic workflow coordinated using adaptive token-gated strategies. Scalable training uses multi-stage curriculum training with language-to-sketch consistency and reinforcement techniques.

Result: Experiments on cluttered scenes and multi-object tasks (both simulation and real-world) demonstrate enhanced long-horizon task success, robustness against scene changes, and improved interpretability through editable plans and sketches.

Conclusion: The proposed Action-Sketcher framework improves spatial referencing capabilities, robustness, human-interactive corrections, and interpretability for robotic manipulation tasks, especially in dynamic and complex environments.

Abstract: Long-horizon robotic manipulation is increasingly important for real-world deployment, requiring spatial disambiguation in complex layouts and temporal resilience under dynamic interaction. However, existing end-to-end and hierarchical Vision-Language-Action (VLA) policies often rely on text-only cues while keeping plan intent latent, which undermines referential grounding in cluttered or underspecified scenes, impedes effective task decomposition of long-horizon goals with close-loop interaction, and limits causal explanation by obscuring the rationale behind action choices. To address these issues, we first introduce Visual Sketch, an implausible visual intermediate that renders points, boxes, arrows, and typed relations in the robot's current views to externalize spatial intent, connect language to scene geometry. Building on Visual Sketch, we present Action-Sketcher, a VLA framework that operates in a cyclic See-Think-Sketch-Act workflow coordinated by adaptive token-gated strategy for reasoning triggers, sketch revision, and action issuance, thereby supporting reactive corrections and human interaction while preserving real-time action prediction. To enable scalable training and evaluation, we curate diverse corpus with interleaved images, text, Visual Sketch supervision, and action sequences, and train Action-Sketcher with a multi-stage curriculum recipe that combines interleaved sequence alignment for modality unification, language-to-sketch consistency for precise linguistic grounding, and imitation learning augmented with sketch-to-action reinforcement for robustness. Extensive experiments on cluttered scenes and multi-object tasks, in simulation and on real-world tasks, show improved long-horizon success, stronger robustness to dynamic scene changes, and enhanced interpretability via editable sketches and step-wise plans. Project website: https://action-sketcher.github.io

</details>


### [491] [DemoBot: Efficient Learning of Bimanual Manipulation with Dexterous Hands From Third-Person Human Videos](https://arxiv.org/abs/2601.01651)
*Yucheng Xu,Xiaofeng Mao,Elle Miller,Xinyu Yi,Yang Li,Zhibin Li,Robert B. Fisher*

Main category: cs.RO

TL;DR: DemoBot framework enables robotic systems to learn manipulation skills from a single unannotated RGB-D video.


<details>
  <summary>Details</summary>
Motivation: To create a scalable model allowing robots to learn complex manipulation tasks directly from simple video demonstrations without the need for detailed annotations.

Method: This method extracts motion trajectories from video data to be used as priors in a new reinforcement learning framework, which refines skills by leveraging contact-rich interactions. Key innovations include temporal-segment-based RL, Success-Gated Reset strategies, and Event-Driven Reward curriculums.

Result: The framework successfully achieved synchronous and asynchronous long-horizon bimanual tasks, showcasing its scalability and precision.

Conclusion: DemoBot offers a feasible solution for teaching robots advanced manipulation tasks directly from human video demonstrations, minimizing the need for manual input.

Abstract: This work presents DemoBot, a learning framework that enables a dual-arm, multi-finger robotic system to acquire complex manipulation skills from a single unannotated RGB-D video demonstration. The method extracts structured motion trajectories of both hands and objects from raw video data. These trajectories serve as motion priors for a novel reinforcement learning (RL) pipeline that learns to refine them through contact-rich interactions, thereby eliminating the need to learn from scratch. To address the challenge of learning long-horizon manipulation skills, we introduce: (1) Temporal-segment based RL to enforce temporal alignment of the current state with demonstrations; (2) Success-Gated Reset strategy to balance the refinement of readily acquired skills and the exploration of subsequent task stages; and (3) Event-Driven Reward curriculum with adaptive thresholding to guide the RL learning of high-precision manipulation. The novel video processing and RL framework successfully achieved long-horizon synchronous and asynchronous bimanual assembly tasks, offering a scalable approach for direct skill acquisition from human videos.

</details>


### [492] [VisuoTactile 6D Pose Estimation of an In-Hand Object using Vision and Tactile Sensor Data](https://arxiv.org/abs/2601.01675)
*Snehal s. Dikhale,Karankumar Patel,Daksh Dhingra,Itoshi Naramura,Akinobu Hayashi,Soshi Iba,Nawid Jamali*

Main category: cs.RO

TL;DR: The paper introduces a method to estimate the 6D pose of objects grasped by robots, combining tactile and vision data to overcome occlusion issues.


<details>
  <summary>Details</summary>
Motivation: Estimating the 6D pose of objects during in-hand manipulation is significant for robotics but is impeded by vision data limitations due to occlusions caused by grippers.

Method: The authors propose a fusion of tactile and vision data, using point clouds for tactile representation and a dense pixel-wise fusion network. They also extend a dataset synthesizer for generating training data.

Result: Experiments show that combining tactile with vision data improves pose estimation, and the method generalizes well from synthetic settings to physical robots.

Conclusion: Incorporating tactile data enhances the robustness of object pose estimation, offering a reliable solution for robotic tasks in real-world scenarios.

Abstract: Knowledge of the 6D pose of an object can benefit in-hand object manipulation. In-hand 6D object pose estimation is challenging because of heavy occlusion produced by the robot's grippers, which can have an adverse effect on methods that rely on vision data only. Many robots are equipped with tactile sensors at their fingertips that could be used to complement vision data. In this paper, we present a method that uses both tactile and vision data to estimate the pose of an object grasped in a robot's hand. To address challenges like lack of standard representation for tactile data and sensor fusion, we propose the use of point clouds to represent object surfaces in contact with the tactile sensor and present a network architecture based on pixel-wise dense fusion. We also extend NVIDIA's Deep Learning Dataset Synthesizer to produce synthetic photo-realistic vision data and corresponding tactile point clouds. Results suggest that using tactile data in addition to vision data improves the 6D pose estimate, and our network generalizes successfully from synthetic training to real physical robots.

</details>


### [493] [Explicit World Models for Reliable Human-Robot Collaboration](https://arxiv.org/abs/2601.01705)
*Kenneth Kwok,Basura Fernando,Qianli Xu,Vigneshwaran Subbaraju,Dongkyu Choi,Boon Kiat Quek*

Main category: cs.RO

TL;DR: The paper redefines the approach to reliability in embodied AI by emphasizing human-robot interaction and building a shared explicit world model to align robot actions with human expectations.


<details>
  <summary>Details</summary>
Motivation: Current methods for ensuring AI robustness and reliability focus on formal verification, which often overlook the dynamic and subjective nature of human-robot interactions. Thus, there is a need to develop an approach better suited for social and fluid environments.

Method: The paper proposes focusing on an 'explicit world model' that builds and updates a shared understanding between humans and robots, ensuring AI can align its behavior with human intentions in multimodal environments.

Result: The framework emphasizes reliability as context-driven, anchored to human goals, allowing AI to adapt robustly to the nonlinearities of human interaction scenarios.

Conclusion: The study suggests a paradigm shift from traditional verification methods to a contextually adaptive AI framework that is comprehensible to humans and more aligned with real-world human-robot interactions.

Abstract: This paper addresses the topic of robustness under sensing noise, ambiguous instructions, and human-robot interaction. We take a radically different tack to the issue of reliable embodied AI: instead of focusing on formal verification methods aimed at achieving model predictability and robustness, we emphasise the dynamic, ambiguous and subjective nature of human-robot interactions that requires embodied AI systems to perceive, interpret, and respond to human intentions in a manner that is consistent, comprehensible and aligned with human expectations. We argue that when embodied agents operate in human environments that are inherently social, multimodal, and fluid, reliability is contextually determined and only has meaning in relation to the goals and expectations of humans involved in the interaction. This calls for a fundamentally different approach to achieving reliable embodied AI that is centred on building and updating an accessible "explicit world model" representing the common ground between human and AI, that is used to align robot behaviours with human expectations.

</details>


### [494] [Simulations and Advancements in MRI-Guided Power-Driven Ferric Tools for Wireless Therapeutic Interventions](https://arxiv.org/abs/2601.01726)
*Wenhui Chu,Aobo Jin,Hardik A. Gohel*

Main category: cs.RO

TL;DR: Creating a system to utilize MRI technology for safer and more precise robot-assisted intravascular procedures.


<details>
  <summary>Details</summary>
Motivation: Improving precision and safety in intravascular interventions using MRI-guided robotic systems.

Method: Development of an integrated computational system based on Qt framework and C/C++ that analyzes vascular networks and controls robotic devices within an MRI environment.

Result: System successfully generates tailored magnetic field patterns for navigation, accounting for vessel geometry, safety norms, and blood flow characteristics.

Conclusion: This advancement merges imaging and robotics to improve precision and safety in medical interventions involving vascular navigation.

Abstract: Designing a robotic system that functions effectively within the specific environment of a Magnetic Resonance Imaging (MRI) scanner requires solving numerous technical issues, such as maintaining the robot's precision and stability under strong magnetic fields. This research focuses on enhancing MRI's role in medical imaging, especially in its application to guide intravascular interventions using robot-assisted devices. A newly developed computational system is introduced, designed for seamless integration with the MRI scanner, including a computational unit and user interface. This system processes MR images to delineate the vascular network, establishing virtual paths and boundaries within vessels to prevent procedural damage. Key findings reveal the system's capability to create tailored magnetic field gradient patterns for device control, considering the vessel's geometry and safety norms, and adapting to different blood flow characteristics for finer navigation. Additionally, the system's modeling aspect assesses the safety and feasibility of navigating pre-set vascular paths. Conclusively, this system, based on the Qt framework and C/C++, with specialized software modules, represents a major step forward in merging imaging technology with robotic aid, significantly enhancing precision and safety in intravascular procedures.

</details>


### [495] [AlignDrive: Aligned Lateral-Longitudinal Planning for End-to-End Autonomous Driving](https://arxiv.org/abs/2601.01762)
*Yanhao Wu,Haoyang Zhang,Fei He,Rui Wu,Congpei Qiu,Liang Gao,Wei Ke,Tong Zhang*

Main category: cs.RO

TL;DR: The paper proposes a novel framework for end-to-end autonomous driving to improve coordination between lateral and longitudinal planning, achieving new state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Current approaches to end-to-end autonomous driving lack proper coordination between lateral and longitudinal planning, leading to inefficiencies and collisions.

Method: The paper introduces a cascaded framework with path-conditioned longitudinal planning and predicts longitudinal displacements along the drive path. It also employs a data augmentation strategy for rare safety-critical scenarios.

Result: The approach achieved a new state-of-the-art driving score of 89.07 and a success rate of 73.18% on the Bench2Drive benchmark.

Conclusion: This cascaded framework coordinates lateral and longitudinal planning, enhances safety, and addresses the inefficiencies in previous autonomous driving strategies.

Abstract: End-to-end autonomous driving has rapidly progressed, enabling joint perception and planning in complex environments. In the planning stage, state-of-the-art (SOTA) end-to-end autonomous driving models decouple planning into parallel lateral and longitudinal predictions. While effective, this parallel design can lead to i) coordination failures between the planned path and speed, and ii) underutilization of the drive path as a prior for longitudinal planning, thus redundantly encoding static information. To address this, we propose a novel cascaded framework that explicitly conditions longitudinal planning on the drive path, enabling coordinated and collision-aware lateral and longitudinal planning. Specifically, we introduce a path-conditioned formulation that explicitly incorporates the drive path into longitudinal planning. Building on this, the model predicts longitudinal displacements along the drive path rather than full 2D trajectory waypoints. This design simplifies longitudinal reasoning and more tightly couples it with lateral planning. Additionally, we introduce a planning-oriented data augmentation strategy that simulates rare safety-critical events, such as vehicle cut-ins, by adding agents and relabeling longitudinal targets to avoid collision. Evaluated on the challenging Bench2Drive benchmark, our method sets a new SOTA, achieving a driving score of 89.07 and a success rate of 73.18%, demonstrating significantly improved coordination and safety

</details>


### [496] [DisCo-FLoc: Using Dual-Level Visual-Geometric Contrasts to Disambiguate Depth-Aware Visual Floorplan Localization](https://arxiv.org/abs/2601.01822)
*Shiyong Meng,Tao Zou,Bolei Chen,Chaoxu Mu,Jianxin Wang*

Main category: cs.RO

TL;DR: DisCo-FLoc proposes a method for visual Floorplan Localization (FLoc) using dual-level visual-geometric contrasts to eliminate ambiguity without relying on semantic labels.


<details>
  <summary>Details</summary>
Motivation: Address the limitations in existing FLoc methods, such as reliance on semantic annotations and challenges with repetitive structures in minimalist floorplans.

Method: Introduce a ray regression predictor for depth-aware FLoc candidates with a novel contrastive learning method incorporating position-level and orientation-level constraints.

Result: DisCo-FLoc surpasses state-of-the-art methods in robustness and accuracy on visual FLoc benchmarks.

Conclusion: The proposed method enhances FLoc accuracy and applicability by resolving ambiguity without additional semantic annotations.

Abstract: Since floorplan data is readily available, long-term persistent, and robust to changes in visual appearance, visual Floorplan Localization (FLoc) has garnered significant attention. Existing methods either ingeniously match geometric priors or utilize sparse semantics to reduce FLoc uncertainty. However, they still suffer from ambiguous FLoc caused by repetitive structures within minimalist floorplans. Moreover, expensive but limited semantic annotations restrict their applicability. To address these issues, we propose DisCo-FLoc, which utilizes dual-level visual-geometric Contrasts to Disambiguate depth-aware visual Floc, without requiring additional semantic labels. Our solution begins with a ray regression predictor tailored for ray-casting-based FLoc, predicting a series of FLoc candidates using depth estimation expertise. In addition, a novel contrastive learning method with position-level and orientation-level constraints is proposed to strictly match depth-aware visual features with the corresponding geometric structures in the floorplan. Such matches can effectively eliminate FLoc ambiguity and select the optimal imaging pose from FLoc candidates. Exhaustive comparative studies on two standard visual Floc benchmarks demonstrate that our method outperforms the state-of-the-art semantic-based method, achieving significant improvements in both robustness and accuracy.

</details>


### [497] [CausalNav: A Long-term Embodied Navigation System for Autonomous Mobile Robots in Dynamic Outdoor Scenarios](https://arxiv.org/abs/2601.01872)
*Hongbo Duan,Shangyi Luo,Zhiyuan Deng,Yanbo Chen,Yuanhao Chiang,Yi Liu,Fangming Liu,Xueqian Wang*

Main category: cs.RO

TL;DR: The paper introduces CausalNav, a navigation framework for dynamic outdoor environments leveraging a multi-level semantic scene graph to aid in better planning and real-time navigation.


<details>
  <summary>Details</summary>
Motivation: The study aims to address challenges in semantic reasoning, dynamic conditions, and maintaining long-term stability for autonomous navigation in large-scale outdoor environments.

Method: CausalNav employs a semantic scene graph called the Embodied Graph, constructed through LLMs. It integrates map data with object entities, supports real-time updates, and fuses perception with offline data to enable navigation and planning under open-vocabulary conditions.

Result: CausalNav demonstrated robust and efficient performance in both simulated and real-world experiments, highlighting its superior navigation under dynamic conditions.

Conclusion: CausalNav proves effective for dynamic outdoor navigation, bridging semantic reasoning with adaptable planning through an innovative use of scene graphs and real-time data integration.

Abstract: Autonomous language-guided navigation in large-scale outdoor environments remains a key challenge in mobile robotics, due to difficulties in semantic reasoning, dynamic conditions, and long-term stability. We propose CausalNav, the first scene graph-based semantic navigation framework tailored for dynamic outdoor environments. We construct a multi-level semantic scene graph using LLMs, referred to as the Embodied Graph, that hierarchically integrates coarse-grained map data with fine-grained object entities. The constructed graph serves as a retrievable knowledge base for Retrieval-Augmented Generation (RAG), enabling semantic navigation and long-range planning under open-vocabulary queries. By fusing real-time perception with offline map data, the Embodied Graph supports robust navigation across varying spatial granularities in dynamic outdoor environments. Dynamic objects are explicitly handled in both the scene graph construction and hierarchical planning modules. The Embodied Graph is continuously updated within a temporal window to reflect environmental changes and support real-time semantic navigation. Extensive experiments in both simulation and real-world settings demonstrate superior robustness and efficiency.

</details>


### [498] [From Metrics to Meaning: Insights from a Mixed-Methods Field Experiment on Retail Robot Deployment](https://arxiv.org/abs/2601.01946)
*Sichao Song,Yuki Okafuji,Takuya Iwamoto,Jun Baba,Hiroshi Ishiguro*

Main category: cs.RO

TL;DR: This paper presents a field experiment with a conversational robot deployed in a retail setting, examining its effects on passersby interactions and sales processes. The study combines quantitative data and staff interviews for insights.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by understanding how conversational service robots impact customer behavior, staff interactions, and the overall retail service funnel, to provide actionable guidance for better integration of robots in high-touch retail environments.

Method: A 12-day mixed-methods field experiment alternated three conditions (Baseline, Robot-only, Robot+Fixture) and analyzed passerby-to-purchase behaviors through video annotations and interviews with staff to explain observed patterns.

Result: The robot increased passerby stopping rates, particularly with accompanying fixture, but decreased clerk-led steps like store entry and on-site purchases. Clerks found it challenging to engage during robot-customer conversations and noted child-centered interactions at entry points.

Conclusion: The study highlights the nuanced dynamics of introducing service robots in retail, finding both benefits (e.g., attracting attention) and challenges (e.g., disrupting traditional staff roles). It provides practical advice on deploying robots effectively in customer-facing settings.

Abstract: We report a mixed-methods field experiment of a conversational service robot deployed under everyday staffing discretion in a live bedding store. Over 12 days we alternated three conditions--Baseline (no robot), Robot-only, and Robot+Fixture--and video-annotated the service funnel from passersby to purchase. An explanatory sequential design then used six post-experiment staff interviews to interpret the quantitative patterns.
  Quantitatively, the robot increased stopping per passerby (highest with the fixture), yet clerk-led downstream steps per stopper--clerk approach, store entry, assisted experience, and purchase--decreased. Interviews explained this divergence: clerks avoided interrupting ongoing robot-customer talk, struggled with ambiguous timing amid conversational latency, and noted child-centered attraction that often satisfied curiosity at the doorway. The fixture amplified visibility but also anchored encounters at the threshold, creating a well-defined micro-space where needs could ``close'' without moving inside.
  We synthesize these strands into an integrative account from the initial show of interest on the part of a customer to their entering the store and derive actionable guidance. The results advance the understanding of interactions between customers, staff members, and the robot and offer practical recommendations for deploying service robots in high-touch retail.

</details>


### [499] [Learning Diffusion Policy from Primitive Skills for Robot Manipulation](https://arxiv.org/abs/2601.01948)
*Zhihao Gu,Ming Yang,Difan Zou,Dong Xu*

Main category: cs.RO

TL;DR: This paper introduces SDP, a skill-conditioned diffusion policy, improving action generation in robotic manipulation through fine-grained primitive skills.


<details>
  <summary>Details</summary>
Motivation: To address the issue of misaligned action generation in robotic manipulation, stemming from reliance on global instructions.

Method: The proposed SDP combines interpretable skill learning and conditional action planning. It employs primitive skills via a vision-language model, supporting a single-skill policy with a lightweight router network.

Result: SDP shows superior performance over state-of-the-art methods in simulation benchmarks and real-world robot tasks.

Conclusion: SDP offers a structured approach for skill-based learning using reusable primitive skills, enhancing consistency and effectiveness in robotic manipulation.

Abstract: Diffusion policies (DP) have recently shown great promise for generating actions in robotic manipulation. However, existing approaches often rely on global instructions to produce short-term control signals, which can result in misalignment in action generation. We conjecture that the primitive skills, referred to as fine-grained, short-horizon manipulations, such as ``move up'' and ``open the gripper'', provide a more intuitive and effective interface for robot learning. To bridge this gap, we propose SDP, a skill-conditioned DP that integrates interpretable skill learning with conditional action planning. SDP abstracts eight reusable primitive skills across tasks and employs a vision-language model to extract discrete representations from visual observations and language instructions. Based on them, a lightweight router network is designed to assign a desired primitive skill for each state, which helps construct a single-skill policy to generate skill-aligned actions. By decomposing complex tasks into a sequence of primitive skills and selecting a single-skill policy, SDP ensures skill-consistent behavior across diverse tasks. Extensive experiments on two challenging simulation benchmarks and real-world robot deployments demonstrate that SDP consistently outperforms SOTA methods, providing a new paradigm for skill-based robot learning with diffusion policies.

</details>


### [500] [What you reward is what you learn: Comparing rewards for online speech policy optimization in public HRI](https://arxiv.org/abs/2601.01969)
*Sichao Song,Yuki Okafuji,Kaito Ariu,Amy Koike*

Main category: cs.RO

TL;DR: The study explores adaptive speech policy optimization for robots using a 12-day deployment and analyses online/offline interactions.


<details>
  <summary>Details</summary>
Motivation: The flexibility of robots' communication in real-world, dynamic environments demands efficient adaptation methods, overcoming static policy limitations.

Method: Online learning framed as a multi-armed bandit problem employed Thompson sampling using speech rate and verbosity adjustments.

Result: Distinct rewards induced varied interaction behaviors and were supplemented by offline evaluations of contextual factors, providing nuanced insights.

Conclusion: Design lessons were derived for implementing optimized speech policies for robots in public settings, ensuring efficiency and adaptability.

Abstract: Designing policies that are both efficient and acceptable for conversational service robots in open and diverse environments is non-trivial. Unlike fixed, hand-tuned parameters, online learning can adapt to non-stationary conditions. In this paper, we study how to adapt a social robot's speech policy in the wild. During a 12-day in-situ deployment with over 1,400 public encounters, we cast online policy optimization as a multi-armed bandit problem and use Thompson sampling to select among six actions defined by speech rate (slow/normal/fast) and verbosity (concise/detailed). We compare three complementary binary rewards--Ru (user rating), Rc (conversation closure), and Rt (>=2 turns)--and show that each induces distinct arm distributions and interaction behaviors. We complement the online results with offline evaluations that analyze contextual factors (e.g., crowd level, group size) using video-annotated data. Taken together, we distill ready-to-use design lessons for deploying online optimization of speech policies in real public HRI settings.

</details>


### [501] [Deep Robust Koopman Learning from Noisy Data](https://arxiv.org/abs/2601.01971)
*Aditya Singh,Rajpal Singh,Jishnu Keshavan*

Main category: cs.RO

TL;DR: This paper introduces an autoencoder-based neural architecture to reduce noise-induced bias in Koopman operator approximations for dynamics prediction and control, demonstrating robust performance in simulations and real-world experiments.


<details>
  <summary>Details</summary>
Motivation: To address the noise-induced bias in the Koopman operator approximations, which hampers accurate predictions and tracking control in noisy real-world datasets.

Method: Proposed an autoencoder-based neural architecture that learns Koopman basis functions consistent with forward and backward dynamics to synthesize a reduced-bias Koopman operator. Also incorporated theoretical analysis to validate bias reduction.

Result: The method showed significant bias reduction and robustness to noise when tested on dynamics prediction and tracking control simulations involving serial manipulator arms. Real-world experiments with the Franka FR3 7-DoF manipulator arm validated its effectiveness.

Conclusion: The approach effectively reduces noise-induced bias in Koopman operator approximations, improving predictive and tracking capabilities in both simulated and practical robotic systems.

Abstract: Koopman operator theory has emerged as a leading data-driven approach that relies on a judicious choice of observable functions to realize global linear representations of nonlinear systems in the lifted observable space. However, real-world data is often noisy, making it difficult to obtain an accurate and unbiased approximation of the Koopman operator. The Koopman operator generated from noisy datasets is typically corrupted by noise-induced bias that severely degrades prediction and downstream tracking performance. In order to address this drawback, this paper proposes a novel autoencoder-based neural architecture to jointly learn the appropriate lifting functions and the reduced-bias Koopman operator from noisy data. The architecture initially learns the Koopman basis functions that are consistent for both the forward and backward temporal dynamics of the system. Subsequently, by utilizing the learned forward and backward temporal dynamics, the Koopman operator is synthesized with a reduced bias making the method more robust to noise compared to existing techniques. Theoretical analysis is used to demonstrate significant bias reduction in the presence of training noise. Dynamics prediction and tracking control simulations are conducted for multiple serial manipulator arms, including performance comparisons with leading alternative designs, to demonstrate its robustness under various noise levels. Experimental studies with the Franka FR3 7-DoF manipulator arm are further used to demonstrate the effectiveness of the proposed approach in a practical setting.

</details>


### [502] [Genie Sim 3.0 : A High-Fidelity Comprehensive Simulation Platform for Humanoid Robot](https://arxiv.org/abs/2601.02078)
*Chenghao Yin,Da Huang,Di Yang,Jichao Wang,Nanshu Zhao,Chen Xu,Wenjun Sun,Linjie Hou,Zhijun Li,Junhui Wu,Zhaobo Liu,Zhen Xiao,Sheng Zhang,Lei Bao,Rui Feng,Zhenquan Pang,Jiayu Li,Qian Wang,Maoqing Yao*

Main category: cs.RO

TL;DR: Genie Sim 3.0 provides a unified simulation tool for robotic manipulation, using LLM to create high-fidelity scenes and enabling scalable data collection and evaluation.


<details>
  <summary>Details</summary>
Motivation: To address limitations in robotic learning, such as the high cost of real-world data collection and limitations of existing simulation benchmarks.

Method: Introduced Genie Sim Generator, an LLM-powered tool for generating diverse robotic simulation scenes, along with automated data evaluation using LLM and VLM.

Result: Created the first LLM-based automated benchmark, a synthetic dataset with over 10,000 hours of data spanning 200+ tasks, and validated robust sim-to-real transfer capabilities.

Conclusion: Synthetic data, generated under controlled conditions, can effectively support scalable robot policy training, reducing reliance on expensive real-world data.

Abstract: The development of robust and generalizable robot learning models is critically contingent upon the availability of large-scale, diverse training data and reliable evaluation benchmarks. Collecting data in the physical world poses prohibitive costs and scalability challenges, and prevailing simulation benchmarks frequently suffer from fragmentation, narrow scope, or insufficient fidelity to enable effective sim-to-real transfer. To address these challenges, we introduce Genie Sim 3.0, a unified simulation platform for robotic manipulation. We present Genie Sim Generator, a large language model (LLM)-powered tool that constructs high-fidelity scenes from natural language instructions. Its principal strength resides in rapid and multi-dimensional generalization, facilitating the synthesis of diverse environments to support scalable data collection and robust policy evaluation. We introduce the first benchmark that pioneers the application of LLM for automated evaluation. It leverages LLM to mass-generate evaluation scenarios and employs Vision-Language Model (VLM) to establish an automated assessment pipeline. We also release an open-source dataset comprising more than 10,000 hours of synthetic data across over 200 tasks. Through systematic experimentation, we validate the robust zero-shot sim-to-real transfer capability of our open-source dataset, demonstrating that synthetic data can server as an effective substitute for real-world data under controlled conditions for scalable policy training. For code and dataset details, please refer to: https://github.com/AgibotTech/genie_sim.

</details>


### [503] [Vision-Based Early Fault Diagnosis and Self-Recovery for Strawberry Harvesting Robots](https://arxiv.org/abs/2601.02085)
*Meili Sun,Chunjiang Zhao,Lichao Yang,Hao Liu,Shimin Hu,Ya Xiong*

Main category: cs.RO

TL;DR: This paper introduced a solution to common issues in strawberry-harvesting robots by developing a visual fault diagnosis and self-recovery framework that integrates multi-task perception and corrective control.


<details>
  <summary>Details</summary>
Motivation: Persistent challenges in strawberry harvesting robots, including visual perception issues, fruit misalignment, misgrasping, and slippage, reduce harvesting efficiency and effectiveness in orchard environments.

Method: The framework leverages SRR-Net, a multi-task perception model for detection, segmentation, and ripeness estimation; employs a compensation method for positional errors; adds an early abort strategy to prevent mis and empty grasping and uses embedded feedback systems to predict slip issues.

Result: Experiments showed high accuracy for detection (precision 0.895, recall 0.813 for strawberries), segmentation, ripeness estimation (error 0.035), and competitive inference speed (163.35 FPS).

Conclusion: The proposed framework significantly enhances stability and efficiency of strawberry harvesting robots, ensuring integration of perception and control strategies.

Abstract: Strawberry harvesting robots faced persistent challenges such as low integration of visual perception, fruit-gripper misalignment, empty grasping, and strawberry slippage from the gripper due to insufficient gripping force, all of which compromised harvesting stability and efficiency in orchard environments. To overcome these issues, this paper proposed a visual fault diagnosis and self-recovery framework that integrated multi-task perception with corrective control strategies. At the core of this framework was SRR-Net, an end-to-end multi-task perception model that simultaneously performed strawberry detection, segmentation, and ripeness estimation, thereby unifying visual perception with fault diagnosis. Based on this integrated perception, a relative error compensation method based on the simultaneous target-gripper detection was designed to address positional misalignment, correcting deviations when error exceeded the tolerance threshold. To mitigate empty grasping and fruit-slippage faults, an early abort strategy was implemented. A micro-optical camera embedded in the end-effector provided real-time visual feedback, enabling grasp detection during the deflating stage and strawberry slip prediction during snap-off through MobileNet V3-Small classifier and a time-series LSTM classifier. Experiments demonstrated that SRR-Net maintained high perception accuracy. For detection, it achieved a precision of 0.895 and recall of 0.813 on strawberries, and 0.972/0.958 on hands. In segmentation, it yielded a precision of 0.887 and recall of 0.747 for strawberries, and 0.974/0.947 for hands. For ripeness estimation, SRR-Net attained a mean absolute error of 0.035, while simultaneously supporting multi-task perception and sustaining a competitive inference speed of 163.35 FPS.

</details>


### [504] [SingingBot: An Avatar-Driven System for Robotic Face Singing Performance](https://arxiv.org/abs/2601.02125)
*Zhuoxiong Xu,Xuanchen Li,Yuhao Cheng,Fei Xu,Yichao Yan,Xiaokang Yang*

Main category: cs.RO

TL;DR: This paper introduces a novel avatar-driven approach to enhance emotional and coherent singing capabilities in robotic faces by leveraging human-like portrait video generation, semantic mapping, and a new metric for emotion evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing robotic face designs struggle with emotional coherence and depth in singing compared to basic conversation or static expressions. Emotional depth is essential for engaging and appealing Human-Robot Interaction.

Method: The framework synthesizes vivid singing avatars using advanced portrait video generation models and transfers these emotional and facial features to robots via semantic-oriented mapping functions. A new metric, Emotion Dynamic Range, measures emotional expression breadth.

Result: The method enables robots to display rich emotional expressions in singing performances, achieving better lip-audio synchronization and outperforming prior techniques.

Conclusion: Robots driven by the proposed framework become more emotionally expressive in singing, paving the way for improved empathetic Human-Robot Interactions.

Abstract: Equipping robotic faces with singing capabilities is crucial for empathetic Human-Robot Interaction. However, existing robotic face driving research primarily focuses on conversations or mimicking static expressions, struggling to meet the high demands for continuous emotional expression and coherence in singing. To address this, we propose a novel avatar-driven framework for appealing robotic singing. We first leverage portrait video generation models embedded with extensive human priors to synthesize vivid singing avatars, providing reliable expression and emotion guidance. Subsequently, these facial features are transferred to the robot via semantic-oriented mapping functions that span a wide expression space. Furthermore, to quantitatively evaluate the emotional richness of robotic singing, we propose the Emotion Dynamic Range metric to measure the emotional breadth within the Valence-Arousal space, revealing that a broad emotional spectrum is crucial for appealing performances. Comprehensive experiments prove that our method achieves rich emotional expressions while maintaining lip-audio synchronization, significantly outperforming existing approaches.

</details>


### [505] [Differential Barometric Altimetry for Submeter Vertical Localization and Floor Recognition Indoors](https://arxiv.org/abs/2601.02184)
*Yuhang Zhang,Sören Schwertfeger*

Main category: cs.RO

TL;DR: The paper introduces a low-cost and accurate altitude estimation framework using differential barometric sensing for mobile robots, achieving sub-meter accuracy and perfect floor recognition.


<details>
  <summary>Details</summary>
Motivation: To enable reliable altitude estimation and floor recognition for mobile robots in complex multi-storey environments.

Method: The framework utilizes differential barometric sensing with real-time altitude data updates from both a stationary base station and a mobile sensor, integrated with ROS-compliant software.

Result: Achieved sub-meter vertical accuracy (RMSE: 0.29 m) and 100% floor identification accuracy, outperforming traditional visual- or LiDAR-based SLAM methods.

Conclusion: The released ROS-compatible barometric module offers a reliable, cost-effective solution for vertical localization in real-world applications, available as open-source.

Abstract: Accurate altitude estimation and reliable floor recognition are critical for mobile robot localization and navigation within complex multi-storey environments. In this paper, we present a robust, low-cost vertical estimation framework leveraging differential barometric sensing integrated within a fully ROS-compliant software package. Our system simultaneously publishes real-time altitude data from both a stationary base station and a mobile sensor, enabling precise and drift-free vertical localization. Empirical evaluations conducted in challenging scenarios -- such as fully enclosed stairwells and elevators, demonstrate that our proposed barometric pipeline achieves sub-meter vertical accuracy (RMSE: 0.29 m) and perfect (100%) floor-level identification. In contrast, our results confirm that standalone height estimates, obtained solely from visual- or LiDAR-based SLAM odometry, are insufficient for reliable vertical localization. The proposed ROS-compatible barometric module thus provides a practical and cost-effective solution for robust vertical awareness in real-world robotic deployments. The implementation of our method is released as open source at https://github.com/witsir/differential-barometric.

</details>


### [506] [CycleVLA: Proactive Self-Correcting Vision-Language-Action Models via Subtask Backtracking and Minimum Bayes Risk Decoding](https://arxiv.org/abs/2601.02295)
*Chenyang Ma,Guangyu Yang,Kai Lu,Shitong Xu,Bill Byrne,Niki Trigoni,Andrew Markham*

Main category: cs.RO

TL;DR: CycleVLA enables proactive self-correction in Vision-Language-Action models (VLAs), detecting and recovering from potential failures before they occur.


<details>
  <summary>Details</summary>
Motivation: Current failure detection approaches in robotics focus only on post hoc corrections after errors occur, lacking proactive measures.

Method: CycleVLA integrates three key components: a progress-aware VLA for flagging critical failure-prone subtasks, a VLM-based failure predictor and planner for backtracking, and Minimum Bayes Risk decoding for retry success after backtracking.

Result: CycleVLA improves the performance of VLAs, including those that are under-trained, showcasing effective zero-shot test-time scaling with MBR.

Conclusion: Proactive failure detection and correction systems like CycleVLA enhance the reliability and effectiveness of robot execution in Vision-Language-Action models.

Abstract: Current work on robot failure detection and correction typically operate in a post hoc manner, analyzing errors and applying corrections only after failures occur. This work introduces CycleVLA, a system that equips Vision-Language-Action models (VLAs) with proactive self-correction, the capability to anticipate incipient failures and recover before they fully manifest during execution. CycleVLA achieves this by integrating a progress-aware VLA that flags critical subtask transition points where failures most frequently occur, a VLM-based failure predictor and planner that triggers subtask backtracking upon predicted failure, and a test-time scaling strategy based on Minimum Bayes Risk (MBR) decoding to improve retry success after backtracking. Extensive experiments show that CycleVLA improves performance for both well-trained and under-trained VLAs, and that MBR serves as an effective zero-shot test-time scaling strategy for VLAs. Project Page: https://dannymcy.github.io/cyclevla/

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [507] [SeRe: A Security-Related Code Review Dataset Aligned with Real-World Review Activities](https://arxiv.org/abs/2601.01042)
*Zixiao Zhao,Yanjie Jiang,Hui Liu,Kui Liu,Lu Zhang*

Main category: cs.SE

TL;DR: The paper introduces 'SeRe,' a security-focused code review dataset created using active learning techniques, containing 6,732 samples for advancing research in automated security-focused code review.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the scarcity of security-focused feedback in code reviews due to either lack of attention or expertise, and the absence of large-scale, security-annotated datasets for research purposes.

Method: The authors developed an active learning-based ensemble classification method to iteratively refine predictions with human annotations, achieving high precision and balance in recall for dataset generation.

Result: The SeRe dataset was created, comprising 6,732 security-related reviews extracted from 373,824 instances, representing a wide range of programming languages and aligning with real-world security review distributions.

Conclusion: The release of SeRe and its benchmark results seeks to enhance research in automated security-oriented code review and promote better secure software engineering practices.

Abstract: Software security vulnerabilities can lead to severe consequences, making early detection essential. Although code review serves as a critical defense mechanism against security flaws, relevant feedback remains scarce due to limited attention to security issues or a lack of expertise among reviewers. Existing datasets and studies primarily focus on general-purpose code review comments, either lacking security-specific annotations or being too limited in scale to support large-scale research. To bridge this gap, we introduce \textbf{SeRe}, a \textbf{security-related code review dataset}, constructed using an active learning-based ensemble classification approach. The proposed approach iteratively refines model predictions through human annotations, achieving high precision while maintaining reasonable recall. Using the fine-tuned ensemble classifier, we extracted 6,732 security-related reviews from 373,824 raw review instances, ensuring representativeness across multiple programming languages. Statistical analysis indicates that SeRe generally \textbf{aligns with real-world security-related review distribution}. To assess both the utility of SeRe and the effectiveness of existing code review comment generation approaches, we benchmark state-of-the-art approaches on security-related feedback generation. By releasing SeRe along with our benchmark results, we aim to advance research in automated security-focused code review and contribute to the development of more effective secure software engineering practices.

</details>


### [508] [RovoDev Code Reviewer: A Large-Scale Online Evaluation of LLM-based Code Review Automation at Atlassian](https://arxiv.org/abs/2601.01129)
*Kla Tantithamthavorn,Yaotian Zou,Andy Wong,Michael Gupta,Zhe Wang,Mike Buller,Ryan Jiang,Matthew Watson,Minwoo Jeong,Kun Chen,Ming Wu*

Main category: cs.SE

TL;DR: This paper introduces RovoDev Code Reviewer, an enterprise-grade LLM-powered code review tool deployed in Atlassian's environment to automate and enhance code review processes, achieving performance improvements in PR cycle time, reviewer workload, and software quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address practical challenges in automating code review workflows with LLMs, specifically designing a scalable, review-guided, and quality-checked system without requiring fine-tuning for enterprise-grade use.

Method: The authors developed the RovoDev Code Reviewer, integrating it into Atlassian's Bitbucket, evaluating its effectiveness through offline and online assessments and user feedback over a year of deployment in a real-world development environment.

Result: RovoDev Code Reviewer demonstrated effectiveness by achieving a 38.70% rate of generating comments leading to code changes, reducing pull request cycle time by 30.8%, decreasing human-written comments by 35.6%, and improving software quality through actionable suggestions.

Conclusion: The deployment of RovoDev Code Reviewer shows that LLM-powered code review tools can significantly enhance code review processes by offering meaningful suggestions that lead to actionable changes, speeding up reviews, and reducing manual effort.

Abstract: Large Language Models (LLMs)-powered code review automation has the potential to transform code review workflows. Despite the advances of LLM-powered code review comment generation approaches, several practical challenges remain for designing enterprise-grade code review automation tools. In particular, this paper aims at answering the practical question: how can we design a review-guided, context-aware, quality-checked code review comment generation without fine-tuning?
  In this paper, we present RovoDev Code Reviewer, an enterprise-grade LLM-based code review automation tool designed and deployed at scale within Atlassian's development ecosystem with seamless integration into Atlassian's Bitbucket. Through the offline, online, user feedback evaluations over a one-year period, we conclude that RovoDev Code Reviewer is (1) effective in generating code review comments that could lead to code resolution for 38.70% (i.e., comments that triggered code changes in the subsequent commits); and (2) offers the promise of accelerating feedback cycles (i.e., decreasing the PR cycle time by 30.8%), alleviating reviewer workload (i.e., reducing the number of human-written comments by 35.6%), and improving overall software quality (i.e., finding errors with actionable suggestions).

</details>


### [509] [Abductive Vibe Coding (Extended Abstract)](https://arxiv.org/abs/2601.01199)
*Logan Murphy,Aren A. Babikian,Marsha Chechik*

Main category: cs.SE

TL;DR: The paper discusses developing a framework for generating semi-formal rationales to evaluate AI-generated code adequacy, instead of formal proofs of correctness.


<details>
  <summary>Details</summary>
Motivation: Human engineers face challenges in validating AI-generated artifacts, especially when requirements are not formally definable.

Method: The authors propose a framework that extracts semi-formal rationales to determine conditions for adequacy of AI-generated code.

Result: Current implementation efforts of the framework are discussed along with research opportunities.

Conclusion: The framework aims to provide practical conditions for assessing the adequacy of AI-generated software artifacts in scenarios where formal proofs are impractical.

Abstract: When software artifacts are generated by AI models ("vibe coding"), human engineers assume responsibility for validating them. Ideally, this validation would be done through the creation of a formal proof of correctness. However, this is infeasible for many real-world vibe coding scenarios, especially when requirements for the AI-generated artifacts resist formalization. This extended abstract describes ongoing work towards the extraction of analyzable, semi-formal rationales for the adequacy of vibe-coded artifacts. Rather than deciding correctness directly, our framework produces a set of conditions under which the generated code can be considered adequate. We describe current efforts towards implementing our framework and anticipated research opportunities.

</details>


### [510] [Correctness isnt Efficiency: Runtime Memory Divergence in LLM-Generated Code](https://arxiv.org/abs/2601.01215)
*Prateek Rajput,Yewei Song,Abdoul Aziz Bonkoungou,Iyiola E. Olatunji,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.SE

TL;DR: This paper introduces a framework to measure runtime memory stability of programs generated by LLMs, highlighting risks of runtime divergence among different correct solutions.


<details>
  <summary>Details</summary>
Motivation: To address the risks of runtime divergences and hidden operational instabilities in memory and performance among different correct solutions generated by LLMs.

Method: The authors propose metrics like Dynamic Mean Pairwise Distance (DMPD) based on Monotonic Peak Profiles (MPPs) and Dynamic Time Warping, aggregated into a Model Instability Score (MIS) to assess memory stability and runtime performance across solutions.

Result: Experiments reveal significant runtime divergence across solutions, with instability increasing at higher sampling temperatures. There are also correlations between stability measures and software complexity indicators.

Conclusion: The study emphasizes the importance of stability-aware solution selection during CI/CD processes to mitigate operational risks while retaining correctness, making it vital for more reliable deployment of LLM-generated programs.

Abstract: Large language models (LLMs) can generate programs that pass unit tests, but passing tests does not guarantee reliable runtime behavior. We find that different correct solutions to the same task can show very different memory and performance patterns, which can lead to hidden operational risks. We present a framework to measure execution-time memory stability across multiple correct generations. At the solution level, we introduce Dynamic Mean Pairwise Distance (DMPD), which uses Dynamic Time Warping to compare the shapes of memory-usage traces after converting them into Monotonic Peak Profiles (MPPs) to reduce transient noise. Aggregating DMPD across tasks yields a model-level Model Instability Score (MIS). Experiments on BigOBench and CodeContests show substantial runtime divergence among correct solutions. Instability often increases with higher sampling temperature even when pass@1 improves. We also observe correlations between our stability measures and software engineering indicators such as cognitive and cyclomatic complexity, suggesting links between operational behavior and maintainability. Our results support stability-aware selection among passing candidates in CI/CD to reduce operational risk without sacrificing correctness. Artifacts are available.

</details>


### [511] [HD-GEN: A High-Performance Software System for Human Mobility Data Generation Based on Patterns of Life](https://arxiv.org/abs/2601.01219)
*Hossein Amiri,Joon-Seok Kim,Hamdi Kavak,Andrew Crooks,Dieter Pfoser,Carola Wenk,Andreas Züfle*

Main category: cs.SE

TL;DR: This paper introduces a software pipeline to generate realistic, scalable human mobility datasets using synthetic simulations calibrated with real-world data.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between the realism of empirical mobility datasets and the scalability and flexibility of synthetic data.

Method: The pipeline integrates four modules: (1) a data generation engine using OpenStreetMap, (2) a calibration module with genetic algorithms, (3) a data processing suite, and (4) a visualization module.

Result: The system effectively combines realism and control in mobility simulations, producing structured datasets adaptable for various applications.

Conclusion: This approach enhances human mobility research by providing realistic synthetic data, supporting applications such as model training, benchmarking, and mobility analysis.

Abstract: Understanding individual-level human mobility is critical for a wide range of applications. Real-world trajectory datasets provide valuable insights into actual movement behaviors but are often constrained by data sparsity and participant bias. Synthetic data, by contrast, offer scalability and flexibility but frequently lack realism. To address this gap, we introduce a comprehensive software pipeline for calibrating, generating, processing, and visualizing large-scale individual-level human mobility datasets that combine the realism of empirical data with the control and extensibility of Patterns-of-Life simulations. Our system consists of four integrated components. (1) a data generation engine constructs geographically grounded simulations using OpenStreetMap data to produce diverse mobility logs. (2) a genetic algorithm-based calibration module fine-tunes simulation parameters to align with real-world mobility characteristics, such as daily trip counts and radius of gyration, enabling realistic behavioral modeling. (3) a data processing suite transforms raw simulation logs into structured formats suitable for downstream applications, including model training and benchmarking. (4) a visualization module extracts key mobility patterns and insights from the processed datasets and presents them through intuitive visual analytics for improved interpretability.

</details>


### [512] [Atomizer: An LLM-based Collaborative Multi-Agent Framework for Intent-Driven Commit Untangling](https://arxiv.org/abs/2601.01233)
*Kangchen Zhu,Zhiliang Tian,Shangwen Wang,Mingyue Leng,Xiaoguang Mao*

Main category: cs.SE

TL;DR: The paper introduces Atomizer, a multi-agent framework to untangle composite code commits, which addresses semantic and refinement limitations in existing methods.


<details>
  <summary>Details</summary>
Motivation: Composite commits hinder software comprehension and maintenance, and current automated solutions fail to capture semantic intent and lack iterative refinement.

Method: Atomizer uses Intent-Oriented Chain-of-Thought (IO-CoT) for semantic analysis and a grouper-reviewer collaborative loop for iterative refinement.

Result: Atomizer outperformed state-of-the-art approaches by 6.0% on a C# dataset, 5.5% on a Java dataset, and over 16% on complex commits.

Conclusion: Atomizer effectively untangles composite commits by leveraging semantic insights and a collaborative refinement loop, enhancing automated commit untangling methods.

Abstract: Composite commits, which entangle multiple unrelated concerns, are prevalent in software development and significantly hinder program comprehension and maintenance. Existing automated untangling methods, particularly state-of-the-art graph clustering-based approaches, are fundamentally limited by two issues. (1) They over-rely on structural information, failing to grasp the crucial semantic intent behind changes, and (2) they operate as ``single-pass'' algorithms, lacking a mechanism for the critical reflection and refinement inherent in human review processes. To overcome these challenges, we introduce Atomizer, a novel collaborative multi-agent framework for composite commit untangling. To address the semantic deficit, Atomizer employs an Intent-Oriented Chain-of-Thought (IO-CoT) strategy, which prompts large language models (LLMs) to infer the intent of each code change according to both the structure and the semantic information of code. To overcome the limitations of ``single-pass'' grouping, we employ two agents to establish a grouper-reviewer collaborative refinement loop, which mirrors human review practices by iteratively refining groupings until all changes in a cluster share the same underlying semantic intent. Extensive experiments on two benchmark C# and Java datasets demonstrate that Atomizer significantly outperforms several representative baselines. On average, it surpasses the state-of-the-art graph-based methods by over 6.0% on the C# dataset and 5.5% on the Java dataset. This superiority is particularly pronounced on complex commits, where Atomizer's performance advantage widens to over 16%.

</details>


### [513] [CatchAll: Repository-Aware Exception Handling with Knowledge-Guided LLMs](https://arxiv.org/abs/2601.01271)
*Qingxiao Tao,Xiaodong Gu,Hao Zhong,Beijun Shen*

Main category: cs.SE

TL;DR: CatchAll is a novel LLM-based approach for handling exceptions in programming at the repository level, enhancing code generation and error recovery by leveraging API-level, repository-level, and cross-repository exception-handling knowledge.


<details>
  <summary>Details</summary>
Motivation: The study addresses the challenges faced by large language models in managing exception handling in software repositories due to complex dependencies and insufficient context-awareness, which often result in severe consequences such as system failures.

Method: CatchAll integrates three types of exception-handling knowledge into structured prompts for large language models: API-level exception knowledge, repository-level context, and cross-repository handling patterns. These are used to enhance LLMs' ability to generate accurate and context-aware exception-handling code.

Result: Experimental results demonstrate CatchAll's superior performance compared to existing methods, with higher CodeBLEU scores, improved intent prediction accuracy, and better Pass@1 results on benchmarks constructed for repository-aware exception handling.

Conclusion: CatchAll is an effective framework for equipping large language models with repository-aware exception handling capabilities, improving their ability to produce robust and context-aware exception-handling code.

Abstract: Exception handling is a vital forward error-recovery mechanism in many programming languages, enabling developers to manage runtime anomalies through structured constructs (e.g., try-catch blocks). Improper or missing exception handling often leads to severe consequences, including system crashes and resource leaks. While large language models (LLMs) have demonstrated strong capabilities in code generation, they struggle with exception handling at the repository level, due to complex dependencies and contextual constraints. In this work, we propose CatchAll, a novel LLM-based approach for repository-aware exception handling. CatchAll equips LLMs with three complementary layers of exception-handling knowledge: (1) API-level exception knowledge, obtained from an empirically constructed API-exception mapping that characterizes the exception-throwing behaviors of APIs in real-world codebases; (2) repository-level execution context, which captures exception propagation by modeling contextual call traces around the target code; and (3) cross-repository handling knowledge, distilled from reusable exception-handling patterns mined from historical code across projects. The knowledge is encoded into structured prompts to guide the LLM in generating accurate and context-aware exception-handling code. To evaluate CatchAll, we construct two new benchmarks for repository-aware exception handling: a large-scale dataset RepoExEval and an executable subset RepoExEval-Exec. Experiments demonstrate that RepoExEval consistently outperforms state-of-the-art baselines, achieving a CodeBLEU score of 0.31 (vs. 0.27% for the best baseline), intent prediction accuracy of 60.1% (vs. 48.0%), and Pass@1 of 29% (vs. 25%). These results affirm RepoExEval's effectiveness in real-world repository-level exception handling.

</details>


### [514] [The Invisible Hand of AI Libraries Shaping Open Source Projects and Communities](https://arxiv.org/abs/2601.01944)
*Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: The paper examines the adoption of AI libraries in Python and Java open source software (OSS) projects, aiming to understand their impact on development practices, community engagement, and code complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how AI influences OSS projects as AI's role in software development continues to grow, yet its adoption and impact in OSS projects remain underexplored.

Method: A large-scale analysis of 157.7k OSS repositories is proposed, using repository and software metrics to compare projects utilizing AI libraries with those that do not.

Result: The researchers anticipate identifying quantifiable differences in activity levels, engagement, and coding complexity between OSS projects adopting AI and those that do not.

Conclusion: The study aims to provide evidence-based insights into how integrating AI reshapes development practices within the OSS ecosystem.

Abstract: In the early 1980s, Open Source Software emerged as a revolutionary concept amidst the dominance of proprietary software. What began as a revolutionary idea has now become the cornerstone of computer science. Amidst OSS projects, AI is increasing its presence and relevance. However, despite the growing popularity of AI, its adoption and impacts on OSS projects remain underexplored.
  We aim to assess the adoption of AI libraries in Python and Java OSS projects and examine how they shape development, including the technical ecosystem and community engagement. To this end, we will perform a large-scale analysis on 157.7k potential OSS repositories, employing repository metrics and software metrics to compare projects adopting AI libraries against those that do not. We expect to identify measurable differences in development activity, community engagement, and code complexity between OSS projects that adopt AI libraries and those that do not, offering evidence-based insights into how AI integration reshapes software development practices.

</details>


### [515] [Adaptive Hierarchical Evaluation of LLMs and SAST tools for CWE Prediction in Python](https://arxiv.org/abs/2601.01320)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: The paper introduces ALPHA, a Python benchmark designed to evaluate LLMs and SAST tools on vulnerability detection with CWE-specific penalties.


<details>
  <summary>Details</summary>
Motivation: Existing vulnerability detection lacks the hierarchical specificity needed for effective feedback in code correction systems.

Method: ALPHA evaluates LLMs and SAST tools through function-level, CWE-specific penalties, distinguishing types of errors, and measuring prediction consistency.

Result: LLMs outperform SAST tools overall, but SAST has higher precision in detections. Prediction consistency greatly varies across models.

Conclusion: ALPHA offers a pathway for principled, hierarchy-aware vulnerability detection and sets the stage for improving supervised fine-tuning in future research.

Abstract: Large Language Models have become integral to software development, yet they frequently generate vulnerable code. Existing code vulnerability detection benchmarks employ binary classification, lacking the CWE-level specificity required for actionable feedback in iterative correction systems. We present ALPHA (Adaptive Learning via Penalty in Hierarchical Assessment), the first function-level Python benchmark that evaluates both LLMs and SAST tools using hierarchically aware, CWE-specific penalties. ALPHA distinguishes between over-generalisation, over-specification, and lateral errors, reflecting practical differences in diagnostic utility. Evaluating seven LLMs and two SAST tools, we find LLMs substantially outperform SAST, though SAST demonstrates higher precision when detections occur. Critically, prediction consistency varies dramatically across models (8.26%-81.87% agreement), with significant implications for feedback-driven systems. We further outline a pathway for future work incorporating ALPHA penalties into supervised fine-tuning, which could provide principled hierarchy-aware vulnerability detection pending empirical validation.

</details>


### [516] [GlycoPy: An Equation-Oriented and Object-Oriented Software for Hierarchical Modeling, Optimization, and Control in Python](https://arxiv.org/abs/2601.01413)
*Yingjie Ma,Jing Guo,Richard D. Braatz*

Main category: cs.SE

TL;DR: The paper introduces GlycoPy, a Python-based framework for nonlinear model predictive control (NMPC) in process industries, aimed at addressing the limitations of linear MPC for complex systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in employing NMPC for real-world (bio)chemical processes, caused by the reliance on linear models and the lack of tools for hierarchical modeling and efficient NMPC algorithms.

Method: They developed GlycoPy, an equation-oriented, object-oriented framework in Python, providing features for hierarchical modeling, parameter estimation, dynamic optimization, and NMPC customization.

Result: GlycoPy's capabilities are validated through three case studies, demonstrating its effectiveness in modeling, optimization, and NMPC for systems ranging from simple equations to multiscale bioprocess models.

Conclusion: GlycoPy offers a practical pathway for bridging the gap between advanced NMPC algorithms and their practical applications in complex (bio)chemical processes.

Abstract: Most existing model predictive control (MPC) applications in process industries employ lin-ear models, although real-world (bio)chemical processes are typically nonlinear. The use of linear models limits the performance and applicability of MPC for processes that span a wide range of operating conditions. A challenge in employing nonlinear models in MPC for com-plex systems is the lack of tools that facilitate hierarchical model development, as well as lack of efficient implementations of the corresponding nonlinear MPC (NMPC) algorithms. As a step towards making NMPC more practical for hierarchical systems, we introduce Gly-coPy, an equation-oriented, object-oriented software framework for process modeling, opti-mization, and NMPC in Python. GlycoPy enables users to focus on writing equations for modeling while supporting hierarchical modeling. GlycoPy includes algorithms for parame-ter estimation, dynamic optimization, and NMPC, and allows users to customize the simula-tion, optimization, and control algorithms. Three case studies, ranging from a simple differ-ential algebraic equation system to a multiscale bioprocess model, validate the modeling, optimization, and NMPC capabilities of GlycoPy. GlycoPy has the potential to bridge the gap between advanced NMPC algorithms and their practical application in real-world (bio)chemical processes.

</details>


### [517] [SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving](https://arxiv.org/abs/2601.01426)
*Chaofan Tao,Jierun Chen,Yuxin Jiang,Kaiqi Kou,Shaowei Wang,Ruoyu Wang,Xiaohui Li,Sidi Yang,Yiming Du,Jianbo Dai,Zhiming Mao,Xinyu Wang,Lifeng Shang,Haoli Bai*

Main category: cs.SE

TL;DR: SWE-Lego introduces a fine-tuning recipe achieving state-of-the-art performance in software engineering issue-solving using lightweight supervised fine-tuning (SFT) methods.


<details>
  <summary>Details</summary>
Motivation: To simplify and push the boundaries of lightweight supervised fine-tuning (SFT) approaches for software engineering tasks while reaching state-of-the-art performance without relying on complex methodologies.

Method: The method includes three components: (1) a high-quality SWE-Lego dataset of real and synthetic data; (2) a refined SFT procedure utilizing error masking and a difficulty-based curriculum; and (3) test-time scaling improvements using a trained verifier.

Result: SWE-Lego achieved state-of-the-art performance among open-source SWE models of comparable size, with notable results such as 42.2% and 52.6% performances for different model sizes, further boosted with test-time scaling to 49.6% and 58.8%.

Conclusion: SWE-Lego demonstrates the effectiveness of a lightweight SFT-only approach when combined with high-quality datasets and strategic improvements, offering a promising direction for SWE issue-solving.

Abstract: We present SWE-Lego, a supervised fine-tuning (SFT) recipe designed to achieve state-ofthe-art performance in software engineering (SWE) issue resolving. In contrast to prevalent methods that rely on complex training paradigms (e.g., mid-training, SFT, reinforcement learning, and their combinations), we explore how to push the limits of a lightweight SFT-only approach for SWE tasks. SWE-Lego comprises three core building blocks, with key findings summarized as follows: 1) the SWE-Lego dataset, a collection of 32k highquality task instances and 18k validated trajectories, combining real and synthetic data to complement each other in both quality and quantity; 2) a refined SFT procedure with error masking and a difficulty-based curriculum, which demonstrably improves action quality and overall performance. Empirical results show that with these two building bricks alone,the SFT can push SWE-Lego models to state-of-the-art performance among open-source models of comparable size on SWE-bench Verified: SWE-Lego-Qwen3-8B reaches 42.2%, and SWE-Lego-Qwen3-32B attains 52.6%. 3) We further evaluate and improve test-time scaling (TTS) built upon the SFT foundation. Based on a well-trained verifier, SWE-Lego models can be significantly boosted--for example, 42.2% to 49.6% and 52.6% to 58.8% under TTS@16 for the 8B and 32B models, respectively.

</details>


### [518] [Group versus Individual Review Requests: Tradeoffs in Speed and Quality at Mozilla Firefox](https://arxiv.org/abs/2601.01514)
*Matej Kucera,Marco Castelluccio,Daniel Feitosa,Ayushi Rastogi*

Main category: cs.SE

TL;DR: The paper investigates the impact of group versus individual review requests on code review performance in the Mozilla Firefox project, highlighting group reviews as beneficial for quality but not necessarily for velocity.


<details>
  <summary>Details</summary>
Motivation: To understand the influence of group review assignments on code review velocity and quality, given their significance for productivity and developer satisfaction.

Method: The study analyzed 66,000 revisions from the Mozilla Firefox project using statistical modeling and insights from a practitioner focus group.

Result: Group reviews were linked to better review quality with fewer code regressions but did not significantly impact review velocity. Additionally, group reviews distributed workloads more evenly and supported training for new reviewers.

Conclusion: Group review requests improve code review quality and have broader benefits for team management, though they do not enhance review speed.

Abstract: The speed at which code changes are integrated into the software codebase, also referred to as code review velocity, is a prevalent industry metric for improved throughput and developer satisfaction. While prior studies have explored factors influencing review velocity, the role of the review assignment process, particularly the `group review request', is unclear. In group review requests, available on platforms like Phabricator, GitHub, and Bitbucket, a code change is assigned to a reviewer group, allowing any member to review it, unlike individual review assignments to specific reviewers. Drawing parallels with shared task queues in Management Sciences, this study examines the effects of group versus individual review requests on velocity and quality. We investigate approximately 66,000 revisions in the Mozilla Firefox project, combining statistical modeling with practitioner views from a focus group discussion. Our study associates group reviews with improved review quality, characterized by fewer regressions, while having a negligible association with review velocity. Additional perceived benefits include balanced work distribution and training opportunities for new reviewers.

</details>


### [519] [MTS-1: A Lightweight Delta-Encoded Telemetry Format optimised for Low-Resource Environments and Offline-First System Health Monitoring](https://arxiv.org/abs/2601.01602)
*Henry Ndou*

Main category: cs.SE

TL;DR: This paper introduces MTS-1, a new telemetry format optimized for bandwidth-constrained and offline-first systems, achieving significant compression over existing formats.


<details>
  <summary>Details</summary>
Motivation: Current telemetry formats impose high bandwidth overhead, making them unsuitable for bandwidth-limited or unstable networks common in areas like Sub-Saharan Africa and rural enterprises.

Method: The authors developed MTS-1, a delta-encoded binary format optimized for offline-first monitoring and bandwidth efficiency, and benchmarked it against popular formats in terms of compression and performance.

Result: MTS-1 achieved up to 74.7% compression improvement over JSON and up to 5.4% over MessagePack, demonstrating strong scalability and efficiency.

Conclusion: MTS-1 is a promising solution for telemetry in constrained environments, offering significant bandwidth savings while maintaining performance.

Abstract: System-level telemetry is fundamental to modern remote monitoring, predictive maintenance, and AI-driven infrastructure optimisation. Existing telemetry encodings such as JSON, JSON Lines, CBOR, and Protocol Buffers were designed for high-bandwidth, always-online environments. They impose significant overhead when deployed in bandwidth-constrained networks common across Sub-Saharan Africa, rural enterprise deployments, and unstable LAN environments. This paper introduces MTS-1 (Magenta Telemetry Standard v1), a novel delta-encoded binary telemetry format designed for offline-first system monitoring, LAN-assisted proxy delivery, and energy-efficient IoT-to-server transmission. We compare MTS-1 against JSON, JSON Lines, CBOR, MessagePack, and Protocol Buffers across payload size, encoding cost, network efficiency, and cost-latency performance. Synthetic benchmarking demonstrates preliminary compression improvements of up to 74.7% versus JSON and 5.4% versus MessagePack, with linear scaling characteristics across dataset sizes.

</details>


### [520] [LIA: Supervised Fine-Tuning of Large Language Models for Automatic Issue Assignment](https://arxiv.org/abs/2601.01780)
*Arsham Khosravani,Alireza Hosseinpour,Arshia Akhavan,Mehdi Keshani,Abbas Heydarnoori*

Main category: cs.SE

TL;DR: This paper introduces LIA, an LLM-based method for automatic issue assignment in software maintenance, achieving remarkable performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the inconsistency and inefficiency in manual issue assignment processes, which are common in large open-source projects, while overcoming challenges faced by existing automated approaches relying on large or sparse noisy data.

Method: The proposed method, LIA, adapts a pretrained LLM (DeepSeek-R1-Distill-Llama-8B) through supervised fine-tuning to generate ranked developer recommendations based on issue titles and descriptions, leveraging semantic understanding and historical assignment patterns.

Result: LIA significantly outperformed its pretrained base model and state-of-the-art baselines, achieving up to +187.8% improvement in Hit@1 score compared to the base model and up to +211.2% improvement over other leading methods.

Conclusion: Domain-adapted LLMs, like LIA, hold great promise for software maintenance tasks, providing a highly practical and effective solution to automatic issue assignment.

Abstract: Issue assignment is a critical process in software maintenance, where new issue reports are validated and assigned to suitable developers. However, manual issue assignment is often inconsistent and error-prone, especially in large open-source projects where thousands of new issues are reported monthly. Existing automated approaches have shown promise, but many rely heavily on large volumes of project-specific training data or relational information that is often sparse and noisy, which limits their effectiveness. To address these challenges, we propose LIA (LLM-based Issue Assignment), which employs supervised fine-tuning to adapt an LLM, DeepSeek-R1-Distill-Llama-8B in this work, for automatic issue assignment. By leveraging the LLM's pretrained semantic understanding of natural language and software-related text, LIA learns to generate ranked developer recommendations directly from issue titles and descriptions. The ranking is based on the model's learned understanding of historical issue-to-developer assignments, using patterns from past tasks to infer which developers are most likely to handle new issues. Through comprehensive evaluation, we show that LIA delivers substantial improvements over both its base pretrained model and state-of-the-art baselines. It achieves up to +187.8% higher Hit@1 compared to the DeepSeek-R1-Distill-Llama-8B pretrained base model, and outperforms four leading issue assignment methods by as much as +211.2% in Hit@1 score. These results highlight the effectiveness of domain-adapted LLMs for software maintenance tasks and establish LIA as a practical, high-performing solution for issue assignment.

</details>


### [521] [The Machine Learning Canvas: Empirical Findings on Why Strategy Matters More Than AI Code Generation](https://arxiv.org/abs/2601.01839)
*Martin Prause*

Main category: cs.SE

TL;DR: The study introduces a Machine Learning Canvas framework identifying four success factors for ML projects: Strategy, Process, Ecosystem, and Support, showing their interconnected influence on project success.


<details>
  <summary>Details</summary>
Motivation: To address the high failure rate of ML projects in delivering business value and identify key factors for their success.

Method: Developed and tested a Machine Learning Canvas framework through a survey of 150 data scientists, and analyzed responses using statistical modeling.

Result: Identified four interlinked success factors: Strategy, Process, Ecosystem, and Support. Quantified their relationships proving organizational support leads to better strategies, processes, and infrastructure.

Conclusion: AI assistants aid coding efficiency but can't replace strategic thinking which is essential for ML project success. Effective ML project implementation relies on interconnected organizational, strategic, and technical factors.

Abstract: Despite the growing popularity of AI coding assistants, over 80% of machine learning (ML) projects fail to deliver real business value. This study creates and tests a Machine Learning Canvas, a practical framework that combines business strategy, software engineering, and data science in order to determine the factors that lead to the success of ML projects. We surveyed 150 data scientists and analyzed their responses using statistical modeling. We identified four key success factors: Strategy (clear goals and planning), Process (how work gets done), Ecosystem (tools and infrastructure), and Support (organizational backing and resources). Our results show that these factors are interconnected - each one affects the next. For instance, strong organizational support results in a clearer strategy (β= 0.432, p < 0.001), which improves work processes (β= 0.428, p < 0.001) and builds better infrastructure (β= 0.547, p < 0.001). Together, these elements determine whether a project succeeds. The surprising finding? Although AI assistants make coding faster, they don't guarantee project success. AI assists with the "how" of coding but cannot replace the "why" and "what" of strategic thinking.

</details>


### [522] [A Defect is Being Born: How Close Are We? A Time Sensitive Forecasting Approach](https://arxiv.org/abs/2601.01921)
*Mikel Robredo,Matteo Esposito,Fabio Palomba,Rafael Peñaloza,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: The paper focuses on using time-sensitive forecasting techniques for predicting software defects and identifying early indicators.


<details>
  <summary>Details</summary>
Motivation: The need for accurate, time-sensitive defect prediction methods has grown due to the continuous evolution of software systems.

Method: The study involves training multiple time-sensitive forecasting models to predict future bug density and identify early indicators of defects.

Result: The expected outcome is empirical validation of the proposed forecasting's effectiveness in early defect estimation.

Conclusion: Time-sensitive defect forecasting could significantly aid in software fault management and early intervention.

Abstract: Background. Defect prediction has been a highly active topic among researchers in the Empirical Software Engineering field. Previous literature has successfully achieved the most accurate prediction of an incoming fault and identified the features and anomalies that precede it through just-in-time prediction. As software systems evolve continuously, there is a growing need for time-sensitive methods capable of forecasting defects before they manifest.
  Aim. Our study seeks to explore the effectiveness of time-sensitive techniques for defect forecasting. Moreover, we aim to investigate the early indicators that precede the occurrence of a defect.
  Method. We will train multiple time-sensitive forecasting techniques to forecast the future bug density of a software project, as well as identify the early symptoms preceding the occurrence of a defect.
  Expected results. Our expected results are translated into empirical evidence on the effectiveness of our approach for early estimation of bug proneness.

</details>


### [523] [Context-Adaptive Requirements Defect Prediction through Human-LLM Collaboration](https://arxiv.org/abs/2601.01952)
*Max Unterbusch,Andreas Vogelsang*

Main category: cs.SE

TL;DR: The paper introduces Human-LLM Collaboration (HLC) for adaptive defect prediction in requirements assessment, leveraging LLMs' Chain-of-Thought reasoning for improved results.


<details>
  <summary>Details</summary>
Motivation: Traditional defect prediction methods fail to address the context-dependent nature of requirements and rely on static models, leading to suboptimal results.

Method: HLC utilizes user validation in a feedback loop with LLM's Chain-of-Thought reasoning for defect prediction. Predictions adapt through few-shot learning and validated explanations provided by users.

Result: HLC achieves rapid improvements with as few as 20 validated examples, significantly outperforming standard few-shot prompting and fine-tuned BERT models on QuRE benchmark data.

Conclusion: HLC demonstrates the potential for adaptive and continuously learning classification models by leveraging stakeholder feedback and LLM capabilities, advancing beyond generic, static approaches.

Abstract: Automated requirements assessment traditionally relies on universal patterns as proxies for defectiveness, implemented through rule-based heuristics or machine learning classifiers trained on large annotated datasets. However, what constitutes a "defect" is inherently context-dependent and varies across projects, domains, and stakeholder interpretations. In this paper, we propose a Human-LLM Collaboration (HLC) approach that treats defect prediction as an adaptive process rather than a static classification task. HLC leverages LLM Chain-of-Thought reasoning in a feedback loop: users validate predictions alongside their explanations, and these validated examples adaptively guide future predictions through few-shot learning. We evaluate this approach using the weak word smell on the QuRE benchmark of 1,266 annotated Mercedes-Benz requirements. Our results show that HLC effectively adapts to the provision of validated examples, with rapid performance gains from as few as 20 validated examples. Incorporating validated explanations, not just labels, enables HLC to substantially outperform both standard few-shot prompting and fine-tuned BERT models while maintaining high recall. These results highlight how the in-context and Chain-of-Thought learning capabilities of LLMs enable adaptive classification approaches that move beyond one-size-fits-all models, creating opportunities for tools that learn continuously from stakeholder feedback.

</details>


### [524] [Reporting LLM Prompting in Automated Software Engineering: A Guideline Based on Current Practices and Expectations](https://arxiv.org/abs/2601.01954)
*Alexander Korn,Lea Zaruchas,Chetan Arora,Andreas Metzger,Sven Smolka,Fanyu Wang,Andreas Vogelsang*

Main category: cs.SE

TL;DR: This paper studies the importance of prompt engineering for using Large Language Models (LLMs) in Software Engineering (SE) tasks. It evaluates reporting standards in current SE research and offers a structured guideline to enhance transparency and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Current SE research lacks a standardized method for documenting prompt engineering decisions for LLMs, negatively impacting reproducibility and comparability.

Method: The authors conducted an empirical study analyzing nearly 300 SE papers and surveyed 105 program committee members to evaluate current practices and expectations for prompt reporting.

Result: The study revealed gaps between current reporting practices and reviewer expectations, such as limited disclosure of LLM versions, inadequate prompt justification, and minimal discussions of validity threats.

Conclusion: The paper proposes a structured guideline to improve reporting standards for prompt engineering in LLM-based SE research, aiming to enhance transparency, reproducibility, and rigor.

Abstract: Large Language Models, particularly decoder-only generative models such as GPT, are increasingly used to automate Software Engineering tasks. These models are primarily guided through natural language prompts, making prompt engineering a critical factor in system performance and behavior. Despite their growing role in SE research, prompt-related decisions are rarely documented in a systematic or transparent manner, hindering reproducibility and comparability across studies. To address this gap, we conducted a two-phase empirical study. First, we analyzed nearly 300 papers published at the top-3 SE conferences since 2022 to assess how prompt design, testing, and optimization are currently reported. Second, we surveyed 105 program committee members from these conferences to capture their expectations for prompt reporting in LLM-driven research. Based on the findings, we derived a structured guideline that distinguishes essential, desirable, and exceptional reporting elements. Our results reveal significant misalignment between current practices and reviewer expectations, particularly regarding version disclosure, prompt justification, and threats to validity. We present our guideline as a step toward improving transparency, reproducibility, and methodological rigor in LLM-based SE research.

</details>


### [525] [The State of Open Science in Software Engineering Research: A Case Study of ICSE Artifacts](https://arxiv.org/abs/2601.02066)
*Al Muttakin,Saikat Mondal,Chanchal Roy*

Main category: cs.SE

TL;DR: The study evaluates 100 replication packages from ICSE over a decade, revealing only 40% are executable, and only 35% of those successfully reproduced original results.


<details>
  <summary>Details</summary>
Motivation: Address the lack of studies examining the executability and reproducibility of replication packages in SE research and improve transparency and reuse.

Method: Evaluation of 100 replication packages from ICSE, assessing executability, reproduction efforts, and challenges over approximately 650 person-hours.

Result: Only 40% of artifacts were executable (32.5% without modifications), and 35% reproduced original results; challenges identified include environmental, documentation, and structural issues.

Conclusion: There's a significant gap in artifact usability and reproducibility in SE. Proposed guidelines aim to improve artifact preparation, documentation, and review practices.

Abstract: Replication packages are crucial for enabling transparency, validation, and reuse in software engineering (SE) research. While artifact sharing is now a standard practice and even expected at premier SE venues such as ICSE, the practical usability of these replication packages remains underexplored. In particular, there is a marked lack of studies that comprehensively examine the executability and reproducibility of replication packages in SE research. In this paper, we aim to fill this gap by evaluating 100 replication packages published as part of ICSE proceedings over the past decade (2015--2024). We assess the (1) executability of the replication packages, (2) efforts and modifications required to execute them, (3) challenges that prevent executability, and (4) reproducibility of the original findings. We spent approximately 650 person-hours in total executing the artifacts and reproducing the study findings. Our findings reveal that only 40\% of the 100 evaluated artifacts were executable, of which 32.5\% (13 out of 40) ran without any modification. Regarding effort levels, 17.5\% (7 out of 40) required low effort, while 82.5\% (33 out of 40) required moderate to high effort to execute successfully. We identified five common types of modifications and 13 challenges leading to execution failure, spanning environmental, documentation, and structural issues. Among the executable artifacts, only 35\% (14 out of 40) reproduced the original results. These findings highlight a notable gap between artifact availability, executability, and reproducibility. Our study proposes three actionable guidelines to improve the preparation, documentation, and review of research artifacts, thereby strengthening the rigor and sustainability of open science practices in SE research.

</details>


### [526] [Code for Machines, Not Just Humans: Quantifying AI-Friendliness with Code Health Metrics](https://arxiv.org/abs/2601.02200)
*Markus Borg,Nadim Hagatulah,Adam Tornhill,Emma Söderberg*

Main category: cs.SE

TL;DR: This paper explores optimizing code readability for both human developers and AI coding agents by examining 'AI-friendly code' and its refactoring using LLMs.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the hybrid era of human developers and AI agents working together, emphasizing the need for code compatibility with AI tools.

Method: The study evaluates 5,000 Python files using LLM-based refactoring and measures the association between CodeHealth metrics and semantic preservation.

Result: Findings show that human-readable code correlates strongly with reliability in AI-driven modifications.

Conclusion: Improving maintainability benefits both human developers and AI systems, and CodeHealth can guide organizations in AI adoption risks and oversight strategies.

Abstract: We are entering a hybrid era in which human developers and AI coding agents work in the same codebases. While industry practice has long optimized code for human comprehension, it is increasingly important to ensure that LLMs with different capabilities can edit code reliably. In this study, we investigate the concept of ``AI-friendly code'' via LLM-based refactoring on a dataset of 5,000 Python files from competitive programming. We find a meaningful association between CodeHealth, a quality metric calibrated for human comprehension, and semantic preservation after AI refactoring. Our findings confirm that human-friendly code is also more compatible with AI tooling. These results suggest that organizations can use CodeHealth to guide where AI interventions are lower risk and where additional human oversight is warranted. Investing in maintainability not only helps humans; it also prepares for large-scale AI adoption.

</details>


### [527] [LLM-Empowered Functional Safety and Security by Design in Automotive Systems](https://arxiv.org/abs/2601.02215)
*Nenad Petrovic,Vahid Zolfaghari,Fengjunjie Pan,Alois Knoll*

Main category: cs.SE

TL;DR: This paper explores the use of Large Language Models (LLMs) to support Software Defined Vehicle (SDV) development, focusing on security-aware topology design and event-driven code analysis.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance SDV software development by integrating models and approaches to address functional safety and security concerns, especially in ADAS-related scenarios.

Method: Utilizes event chain models for functional safety validation and employs Model-Driven Engineering (MDE) with Object Constraint Language (OCL) rules for topology security analysis.

Result: Achieves systematic security and functional safety validation within SDV frameworks, especially for ADAS-related systems, with both proprietary and locally deployable solutions.

Conclusion: Combining LLMs, event chain models, and MDE approaches offers a robust solution for SDV software development challenges in the areas of safety, security, and decision-making analysis.

Abstract: This paper presents LLM-empowered workflow to support Software Defined Vehicle (SDV) software development, covering the aspects of security-aware system topology design, as well as event-driven decision-making code analysis. For code analysis we adopt event chains model which provides formal foundations to systematic validation of functional safety, taking into account the semantic validity of messages exchanged between key components, including both CAN and Vehicle Signal Specification (VSS). Analysis of security aspects for topology relies on synergy with Model-Driven Engineering (MDE) approach and Object Constraint Language (OCL) rules. Both locally deployable and proprietary solution are taken into account for evaluation within Advanced Driver-Assistance Systems (ADAS)-related scenarios.

</details>


### [528] [NQC2: A Non-Intrusive QEMU Code Coverage Plugin](https://arxiv.org/abs/2601.02238)
*Nils Bosbach,Alwalid Salama,Lukas Jünger,Mark Burton,Niko Zurstraßen,Rebecca Pelke,Rainer Leupers*

Main category: cs.SE

TL;DR: The paper presents NQC2, a QEMU plugin for extracting code coverage information for embedded systems without requiring target software instrumentation, outperforming a comparable method.


<details>
  <summary>Details</summary>
Motivation: Conventional code coverage methods do not work effectively for embedded systems, especially for bare-metal programs lacking operating and file systems.

Method: The paper introduces NQC2, a QEMU plugin that captures runtime coverage data directly from QEMU and stores it on the host, eliminating the need for software instrumentation.

Result: NQC2 is compatible with different versions of QEMU and achieves up to 8.5x better performance compared to Xilinx's approach.

Conclusion: NQC2 addresses the limitations of conventional coverage analysis tools for embedded systems by providing an efficient and non-intrusive solution.

Abstract: Code coverage analysis has become a standard approach in software development, facilitating the assessment of test suite effectiveness, the identification of under-tested code segments, and the discovery of performance bottlenecks. When code coverage of software for embedded systems needs to be measured, conventional approaches quickly meet their limits. A commonly used approach involves instrumenting the source files with added code that collects and dumps coverage information during runtime. This inserted code usually relies on the existence of an operating and a file system to dump the collected data. These features are not available for bare-metal programs that are executed on embedded systems.
  To overcome this issue, we present NQC2, a plugin for QEMU.NQC2 extracts coverage information from QEMU during runtime and stores them into a file on the host machine. This approach is even compatible with modified QEMU versions and does not require target-software instrumentation. NQC2 outperforms a comparable approach from Xilinx by up to 8.5 x.

</details>


### [529] [Automatic Assertion Mining in Assertion-Based Verification: Techniques, Challenges, and Future Directions](https://arxiv.org/abs/2601.02248)
*Mohammad Reza Heidari Iman,Giorgio Di Natale,Katell Morin-Allory*

Main category: cs.SE

TL;DR: The paper reviews and compares advanced assertion miners used in Assertion-Based Verification for hardware design, highlighting their strengths and weaknesses.


<details>
  <summary>Details</summary>
Motivation: The need for efficient and effective methods to verify hardware designs drives the focus on Assertion-Based Verification and automatic assertion miners.

Method: A comparative analysis of advanced and widely adopted assertion miners is conducted to evaluate their methodologies.

Result: Insights into the capabilities and limitations of existing assertion miners are provided, highlighting shortcomings.

Conclusion: The work identifies gaps in current assertion miners, suggesting directions for advancing future development of these tools.

Abstract: Functional verification increasingly relies on Assertion-Based Verification (ABV), which has become a key approach for verifying hardware designs due to its efficiency and effectiveness. Central to ABV are automatic assertion miners, which apply different techniques to generate assertions automatically. This paper reviews the most recent, advanced, and widely adopted assertion miners, offering a comparative analysis of their methodologies. The goal is to provide researchers and verification practitioners with insights into the capabilities and limitations of existing miners. By identifying their shortcomings, this work also points toward directions for developing more powerful and advanced assertion miners in the future.

</details>


### [530] [Question Answering for Multi-Release Systems: A Case Study at Ciena](https://arxiv.org/abs/2601.02345)
*Parham Khamsepour,Mark Cole,Ish Ashraf,Sandeep Puri,Mehrdad Sabetzadeh,Shiva Nejati*

Main category: cs.SE

TL;DR: This paper introduces QAMR, a chatbot for answering questions over multi-release system documents, showing improved accuracy and efficiency compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing question-answering techniques perform poorly on overlapping documentation in multi-release systems.

Method: QAMR integrates enhanced retrieval-augmented generation (RAG) with novel preprocessing, query rewriting, context selection, and dual-chunking strategies.

Result: QAMR delivers a 16.5% improvement in answer correctness and a 12% boost in retrieval accuracy over baseline models, along with reduced response time and high correlation to expert evaluations.

Conclusion: QAMR effectively addresses question-answering challenges in multi-release systems, showing superior performance and reliability in handling overlapping documentation.

Abstract: Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR's mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [531] [A Biologically Plausible Dense Associative Memory with Exponential Capacity](https://arxiv.org/abs/2601.00984)
*Mohadeseh Shafiei Kafraj,Dmitry Krotov,Peter E. Latham*

Main category: q-bio.NC

TL;DR: The paper introduces a novel associative memory network with exponential memory capacity in the number of hidden units using threshold nonlinearity.


<details>
  <summary>Details</summary>
Motivation: To overcome the memory capacity limitation caused by the winner-takes-all dynamics in previous associative memory networks.

Method: Developing a distributed representation using a novel threshold nonlinearity, which allows hidden neurons to encode shared memory components.

Result: Achieved exponential memory storage capacity in hidden units and established stable memory patterns using binary hidden states.

Conclusion: The work demonstrates a high-capacity, scalable associative memory system preserving biological plausibility and efficient pattern decoding.

Abstract: Krotov and Hopfield (2021) proposed a biologically plausible two-layer associative memory network with memory storage capacity exponential in the number of visible neurons. However, the capacity was only linear in the number of hidden neurons. This limitation arose from the choice of nonlinearity between the visible and hidden units, which enforced winner-takes-all dynamics in the hidden layer, thereby restricting each hidden unit to encode only a single memory. We overcome this limitation by introducing a novel associative memory network with a threshold nonlinearity that enables distributed representations. In contrast to winner-takes-all dynamics, where each hidden neuron is tied to an entire memory, our network allows hidden neurons to encode basic components shared across many memories. Consequently, complex patterns are represented through combinations of hidden neurons. These representations reduce redundancy and allow many correlated memories to be stored compositionally. Thus, we achieve much higher capacity: exponential in the number of hidden units, provided the number of visible units is sufficiently larger than the number of hidden neurons. Exponential capacity arises because all binary states of the hidden units can become stable memory patterns with an appropriately chosen threshold. Moreover, the distributed hidden representation, which has much lower dimensionality than the visible layer, preserves class-discriminative structure, supporting efficient nonlinear decoding. These results establish a new regime for associative memory, enabling high-capacity, robust, and scalable architectures consistent with biological constraints.

</details>


### [532] [From Theory of Mind to Theory of Environment: Counterfactual Simulation of Latent Environmental Dynamics](https://arxiv.org/abs/2601.01599)
*Ryutaro Uchiyama*

Main category: q-bio.NC

TL;DR: The paper explores how humans use social cues and motor system strategies for behavioral innovation in complex environments.


<details>
  <summary>Details</summary>
Motivation: The study investigates how humans limit motor complexity for control while innovating behavior in environments with hidden contingencies.

Method: The paper proposes a conceptual framework, 'Theory of Environment,' connecting social cue use, motor dimensionality, and behavioral innovation.

Result: Using shared computational mechanisms with Theory of Mind, the study highlights how humans infer hidden dynamics for adaptive motor exploration.

Conclusion: Humans utilize social cues and a 'Theory of Environment' to innovate motor behaviors by expanding exploration in response to hidden environmental complexities.

Abstract: The vertebrate motor system employs dimensionality-reducing strategies to limit the complexity of movement coordination, for efficient motor control. But when environments are dense with hidden action-outcome contingencies, movement complexity can promote behavioral innovation. Humans, perhaps uniquely, may infer the presence of hidden environmental dynamics from social cues, by drawing upon computational mechanisms shared with Theory of Mind. This proposed "Theory of Environment" supports behavioral innovation by expanding the dimensionality of motor exploration.

</details>


### [533] [Insular intracranial activity identifies multiple facial expressions via diverse, intermixed temporal patterns at the single-contact level](https://arxiv.org/abs/2601.01782)
*Yingyu Huang,Lisen Sui,Liying Zhan,Chaolun Wang,Zhihan Guo,Yanjuan Li,Xiang Wu*

Main category: q-bio.NC

TL;DR: The study explores how the human insular cortex processes emotional facial expressions, highlighting its role as a key hub for versatile cognitive and emotional functions.


<details>
  <summary>Details</summary>
Motivation: To understand neural representations in the insula related to emotional processing, particularly its debated specialization for disgust.

Method: Human subjects underwent stereoelectroencephalography while performing a facial emotion recognition task involving various expressions. Neural activity was analyzed using ERP and ERSP pattern comparisons.

Result: Insular activity differentiated all emotional expressions studied, with heterogeneous ERP responses, contrasting the uniform responses in the fusiform face area. ERSPs were also significant in expression identification.

Conclusion: The insula employs diverse neural mechanisms for emotional perception and acts as a hub for complex cognitive and emotional processing.

Abstract: How neural representations in the insular cortex support emotional processing remains poorly understood, and the extent to which the insula is specialized for disgust processing remains debated. We recorded stereoelectroencephalography data from the insula while human subjects with implanted electrode contacts performed a facial emotion recognition task involving disgusted, fearful, angry, sad, neutral, and happy expressions. Expression category specificity of insular activity was assessed via pairwise comparisons of within- and between-category pattern similarities, capturing both the shape and scale of event-related potentials (ERPs) and event-related spectral perturbations (ERSPs; theta to high-gamma frequency ranges). Insular activity successfully identified all investigated expressions, mediated by diverse ERP responses intermixed across the insula. In contrast to the marked heterogeneity of insula ERP responses, the fusiform face area exhibited convergent ERP responses across expressions and contacts, with ERSPs also contributing substantially to expression identification. These findings not only elucidate the insula's neural mechanisms underlying facial emotion perception, but also establish a potential single-contact-level neural substrate for how the insula leverages its heterogeneous response profiles to act as a key hub for versatile cognitive and emotional functions.

</details>


### [534] [A neural network for modeling human concept formation, understanding and communication](https://arxiv.org/abs/2601.02010)
*Liangxuan Guo,Haoyang Chen,Yang Chen,Yanchao Bi,Shan Yu*

Main category: q-bio.NC

TL;DR: Researchers developed the CATS Net, a dual-module neural network, to model human-like conceptual cognition and its computational mechanisms by combining concept abstraction and task-solving.


<details>
  <summary>Details</summary>
Motivation: The motivation of this study is to understand the computational mechanism behind the human brain's ability to form abstract concepts from sensory experiences and apply these concepts flexibly without direct sensory input.

Method: The authors introduced the CATS Net, a dual-module framework, including a concept-abstraction module for extracting low-dimensional concept representations and a task-solving module controlled by hierarchical gating based on these concepts, which enable behavior and cross-network knowledge transfer.

Result: The emergent conceptual structures in CATS Net align with neurocognitive semantic models and brain responses in the human ventral occipitotemporal cortex. The gating mechanisms resemble those in semantic control brain networks.

Conclusion: This computational framework offers insights into human conceptual cognition and inspires the development of artificial systems capable of human-like conceptual processing and intelligence.

Abstract: A remarkable capability of the human brain is to form more abstract conceptual representations from sensorimotor experiences and flexibly apply them independent of direct sensory inputs. However, the computational mechanism underlying this ability remains poorly understood. Here, we present a dual-module neural network framework, the CATS Net, to bridge this gap. Our model consists of a concept-abstraction module that extracts low-dimensional conceptual representations, and a task-solving module that performs visual judgement tasks under the hierarchical gating control of the formed concepts. The system develops transferable semantic structure based on concept representations that enable cross-network knowledge transfer through conceptual communication. Model-brain fitting analyses reveal that these emergent concept spaces align with both neurocognitive semantic model and brain response structures in the human ventral occipitotemporal cortex, while the gating mechanisms mirror that in the semantic control brain network. This work establishes a unified computational framework that can offer mechanistic insights for understanding human conceptual cognition and engineering artificial systems with human-like conceptual intelligence.

</details>


### [535] [How much neuroscience does a neuroscientist need to know?](https://arxiv.org/abs/2601.02063)
*James C. R. Whittington,William Dorrell*

Main category: q-bio.NC

TL;DR: This paper argues that simple biological details significantly constrain learning algorithms in the brain, shaping brain-like responses and informing AI neural mechanisms.


<details>
  <summary>Details</summary>
Motivation: The authors aim to explore how critical biological details influence brain-learned algorithms and compare their principles with artificial intelligence mechanisms.

Method: They identify specific biological constraints—such as nonnegative firing, space, and energy limitations—that reduce algorithmic possibilities, correlating these constraints with task-specific responses, down to single neurons.

Result: It is established that each biological detail breaks system symmetries, enabling interpretable and characteristic single-neuron responses that shed light on brain-like algorithms.

Conclusion: Understanding these biological constraints aligns computational neuroscience with AI interpretability, proposing a unified approach to dissecting intelligence mechanisms in both the brain and machines.

Abstract: How much of the brain's learned algorithms depend on the fact it is a brain? We argue: a lot, but surprisingly few details matter. We point to simple biological details -- e.g. nonnegative firing and energetic/space budgets in connectionist architectures -- which, when mixed with the requirements of solving a task, produce models that predict brain responses down to single-neuron tuning. We understand this as details constraining the set of plausible algorithms, and their implementations, such that only `brain-like' algorithms are learned. In particular, each biological detail breaks a symmetry in connectionist models (scale, rotation, permutation) leading to interpretable single-neuron responses that are meaningfully characteristic of particular algorithms. This view helps us not only understand the brain's choice of algorithm but also infer algorithm from measured neural responses. Further, this perspective aligns computational neuroscience with mechanistic interpretability in AI, suggesting a more unified approach to studying the mechanisms of intelligence, both natural and artificial.

</details>


### [536] [Responses of the Neurobiological Craving Signature to smoking versus alternative social rewards predict craving and monthly smoking in adolescents](https://arxiv.org/abs/2601.02143)
*Maddalena Tamellini,Joyce Dieleman,Guillaume Sescousse,Maartje Luijten,Leonie Koban*

Main category: q-bio.NC

TL;DR: The study investigates adolescent brain craving responses to smoking cues compared to social rewards, using fMRI and highlighting the influence of peer environmental tobacco smoke (ETS) exposure.


<details>
  <summary>Details</summary>
Motivation: Adolescents are susceptible to tobacco addiction due to brain development and environmental influences like exposure to tobacco smoke. Despite its importance, craving and ETS exposure during early smoking stages are poorly understood.

Method: Using fMRI and the Neurobiological Craving Signature (NCS) tool, the study compared craving brain responses to smoking and social cues in 100 Experimental Smokers and 48 Non-smokers, with varying ETS exposure levels.

Result: Experimental Smokers showed higher NCS responses to smoking cues than Non-smokers, with these responses predicting craving levels and smoking behavior. Peer ETS exposure strongly correlated to smoking behavior and brain responses.

Conclusion: Adolescents in the experimental smoking phase show increased brain craving responses and are highly influenced by peer ETS exposure, emphasizing the role of social norms in early smoking behavior.

Abstract: Smoking remains the leading cause of preventable mortality worldwide. Adolescents are particularly vulnerable to the development of tobacco addiction due to ongoing brain maturation and susceptibility to social influences, such as exposure to environmental tobacco smoke (ETS). Craving -the strong desire to use drugs -already emerges with non-daily tobacco use and predicts continued use and relapse. However, the roles of craving and ETS exposure during the early stages of tobacco use in adolescence remain poorly understood. In this pre-registered study, we harness a recently developed fMRI marker of craving -the Neurobiological Craving Signature (NCS) -to compare craving-related brain responses to smoking versus social cues in adolescent Experimental Smokers (N=100) and Non-smokers (N=48) with varying levels of ETS exposure levels. Results showed that NCS responses to smoking cues compared to alternative social rewards were higher in Experimental Smokers compared to Non-smokers and predicted individual differences in self-reported craving and monthly smoking. Both smoking behavior and NCS responses were correlated with the relative amount of ETS exposure from peers compared to exposure from family members. Together, these findings indicate a heightened sensitivity of craving-related brain circuits already during experimental smoking and highlight the important role of peer social norms on craving and smoking initiation in the critical period of adolescence.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [537] [Beyond Demand Estimation: Consumer Surplus Evaluation via Cumulative Propensity Weights](https://arxiv.org/abs/2601.01029)
*Zeyu Bian,Max Biggs,Ruijiang Gao,Zhengling Qi*

Main category: stat.ML

TL;DR: The paper develops a novel framework to estimate the effects of AI-driven decisions, specifically on consumer surplus, utilizing observational data through cumulative propensity weights (CPW) and its robust variant, ACPW, while addressing fairness in decision-making.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in traditional demand estimation methods, which face issues like model misspecifications, high data requirements, and slow convergence, and to improve the analysis of AI-driven decisions on consumer surplus.

Method: The paper introduces an estimator using cumulative propensity weights (CPW) and an augmented version (ACPW) for integrating demand estimations from randomized pricing data, avoiding explicit estimation and numerical integration. It extends this framework to analyze fairness via inequality-aware surplus measures.

Result: The proposed methods improve accuracy and efficiency in estimating consumer surplus while enabling faster convergence and using flexible machine learning techniques. The framework also quantifies profit-equity trade-offs.

Conclusion: This framework offers a practical and robust alternative for evaluating consumer surplus under AI-driven decisions, addressing fairness and providing insights for regulation and decision-making.

Abstract: This paper develops a practical framework for using observational data to audit the consumer surplus effects of AI-driven decisions, specifically in targeted pricing and algorithmic lending. Traditional approaches first estimate demand functions and then integrate to compute consumer surplus, but these methods can be challenging to implement in practice due to model misspecification in parametric demand forms and the large data requirements and slow convergence of flexible nonparametric or machine learning approaches. Instead, we exploit the randomness inherent in modern algorithmic pricing, arising from the need to balance exploration and exploitation, and introduce an estimator that avoids explicit estimation and numerical integration of the demand function. Each observed purchase outcome at a randomized price is an unbiased estimate of demand and by carefully reweighting purchase outcomes using novel cumulative propensity weights (CPW), we are able to reconstruct the integral. Building on this idea, we introduce a doubly robust variant named the augmented cumulative propensity weighting (ACPW) estimator that only requires one of either the demand model or the historical pricing policy distribution to be correctly specified. Furthermore, this approach facilitates the use of flexible machine learning methods for estimating consumer surplus, since it achieves fast convergence rates by incorporating an estimate of demand, even when the machine learning estimate has slower convergence rates. Neither of these estimators is a standard application of off-policy evaluation techniques as the target estimand, consumer surplus, is unobserved. To address fairness, we extend this framework to an inequality-aware surplus measure, allowing regulators and firms to quantify the profit-equity trade-off. Finally, we validate our methods through comprehensive numerical studies.

</details>


### [538] [Fibonacci-Driven Recursive Ensembles: Algorithms, Convergence, and Learning Dynamics](https://arxiv.org/abs/2601.01055)
*Ernest Fokoué*

Main category: stat.ML

TL;DR: This paper proposes a second-order recursive ensemble learning system based on Fibonacci-type flows, integrating past data into ensemble evolution and showing improved learning performance.


<details>
  <summary>Details</summary>
Motivation: Traditional ensemble methods like boosting rely on first-order updates, which fail to effectively integrate historical residual patterns and adaptability.

Method: Proposes a recursive weight-update algorithm inspired by Fibonacci flows, generalizing to higher-order schemes and differential equations for dynamic ensemble evolution.

Result: Establishes theoretical properties such as convergence, stability, and generalization bounds, validated through experiments with machine learning models which show enhanced performance.

Conclusion: The study unifies recursive ensembles and dynamic systems for improved learning algorithms, extending geometric weighting to dynamic implementations, completing an existing research trilogy.

Abstract: This paper develops the algorithmic and dynamical foundations of recursive ensemble learning driven by Fibonacci-type update flows. In contrast with classical boosting  Freund and Schapire (1997); Friedman (2001), where the ensemble evolves through first-order additive updates, we study second-order recursive architectures in which each predictor depends on its two immediate predecessors. These Fibonacci flows induce a learning dynamic with memory, allowing ensembles to integrate past structure while adapting to new residual information. We introduce a general family of recursive weight-update algorithms encompassing Fibonacci, tribonacci, and higher-order recursions, together with continuous-time limits that yield systems of differential equations governing ensemble evolution. We establish global convergence conditions, spectral stability criteria, and non-asymptotic generalization bounds under Rademacher Bartlett and Mendelson (2002) and algorithmic stability analyses. The resulting theory unifies recursive ensembles, structured weighting, and dynamical systems viewpoints in statistical learning. Experiments with kernel ridge regression Rasmussen and Williams (2006), spline smoothers Wahba (1990), and random Fourier feature models Rahimi and Recht (2007) demonstrate that recursive flows consistently improve approximation and generalization beyond static weighting. These results complete the trilogy begun in Papers I and II: from Fibonacci weighting, through geometric weighting theory, to fully dynamical recursive ensemble learning systems.

</details>


### [539] [Neural Networks on Symmetric Spaces of Noncompact Type](https://arxiv.org/abs/2601.01097)
*Xuan Son Nguyen,Shuo Yang,Aymeric Histace*

Main category: stat.ML

TL;DR: The paper introduces a unified approach for neural networks on noncompact symmetric spaces, including hyperbolic and SPD spaces, validated across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve neural network performance on advanced geometric spaces like hyperbolic spaces and SPD manifolds by providing a unified and effective methodology.

Method: A novel framework based on a unified formulation of point-to-hyperplane distances in symmetric spaces of noncompact type to design fully-connected layers and attention mechanisms.

Result: The proposed approach showed high effectiveness through validation on tasks such as image classification, EEG signal classification, image generation, and natural language inference.

Conclusion: The unified formulation for neural networks on symmetric spaces enhances their capabilities and generalizes various methods for advanced geometric tasks.

Abstract: Recent works have demonstrated promising performances of neural networks on hyperbolic spaces and symmetric positive definite (SPD) manifolds. These spaces belong to a family of Riemannian manifolds referred to as symmetric spaces of noncompact type. In this paper, we propose a novel approach for developing neural networks on such spaces. Our approach relies on a unified formulation of the distance from a point to a hyperplane on the considered spaces. We show that some existing formulations of the point-to-hyperplane distance can be recovered by our approach under specific settings. Furthermore, we derive a closed-form expression for the point-to-hyperplane distance in higher-rank symmetric spaces of noncompact type equipped with G-invariant Riemannian metrics. The derived distance then serves as a tool to design fully-connected (FC) layers and an attention mechanism for neural networks on the considered spaces. Our approach is validated on challenging benchmarks for image classification, electroencephalogram (EEG) signal classification, image generation, and natural language inference.

</details>


### [540] [Conformal Blindness: A Note on $A$-Cryptic change-points](https://arxiv.org/abs/2601.01147)
*Johan Hallberg Szabadváry*

Main category: stat.ML

TL;DR: The paper investigates limitations of Conformal Test Martingales (CTMs) by demonstrating cases where a significant shift in distribution can occur without altering p-value uniformity, making CTMs unable to detect exchangeability breaches.


<details>
  <summary>Details</summary>
Motivation: To explore whether shifts in data exchangeability can remain undetected by CTMs due to uniform p-values and to highlight potential blind spots in their change detection.

Method: The authors construct theoretical scenarios using the "oracle" conformity measure to show situations, particularly $A$-cryptic change-points, where CTMs become blind to shifts despite large changes in distribution. Simulations confirm these constructed scenarios using bivariate Gaussian distributions.

Result: CTMs fail to detect exchangeability shifts in scenarios where large distribution changes do not alter conformity scores, leading to cryptically uniform p-values.

Conclusion: This study exposes a critical limitation in CTMs and emphasizes the need for better alignment between conformity measures and potential distributional shifts to effectively detect changes in data exchangeability.

Abstract: Conformal Test Martingales (CTMs) are a standard method within the Conformal Prediction framework for testing the crucial assumption of data exchangeability by monitoring deviations from uniformity in the p-value sequence. Although exchangeability implies uniform p-values, the converse does not hold. This raises the question of whether a significant break in exchangeability can occur, such that the p-values remain uniform, rendering CTMs blind. We answer this affirmatively, demonstrating the phenomenon of \emph{conformal blindness}.
  Through explicit construction, for the theoretically ideal ``oracle'' conformity measure (given by the true conditional density), we demonstrate the possibility of an \emph{$A$-cryptic change-point} (where $A$ refers to the conformity measure). Using bivariate Gaussian distributions, we identify a line along which a change in the marginal means does not alter the distribution of the conformity scores, thereby producing perfectly uniform p-values.
  Simulations confirm that even a massive distribution shift can be perfectly cryptic to the CTM, highlighting a fundamental limitation and emphasising the critical role of the alignment of the conformity measure with potential shifts.

</details>


### [541] [Evidence Slopes and Effective Dimension in Singular Linear Models](https://arxiv.org/abs/2601.01238)
*Kalyaan Rao*

Main category: stat.ML

TL;DR: This paper evaluates the failure of Laplace approximations/BIC in singular models and introduces RLCT-aware methods to correct estimation errors.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of Laplace approximation and BIC in overparameterized or rank-deficient models by utilizing singular learning theory.

Method: It studies linear-Gaussian rank models and linear subspace models with analytically tractable RLCT. The authors theoretically and empirically examine the growth of estimation errors while proposing RLCT-aware corrections.

Result: The error of Laplace/BIC increases with a specific linear term dependent on RLCT, while the proposed correction successfully recovers the correct evidence slope.

Conclusion: RLCT-aware methods provide a robust approach to improving model selection accuracy in singular models, offering practical applications for effective dimension estimation.

Abstract: Bayesian model selection commonly relies on Laplace approximation or the Bayesian Information Criterion (BIC), which assume that the effective model dimension equals the number of parameters. Singular learning theory replaces this assumption with the real log canonical threshold (RLCT), an effective dimension that can be strictly smaller in overparameterized or rank-deficient models.
  We study linear-Gaussian rank models and linear subspace (dictionary) models in which the exact marginal likelihood is available in closed form and the RLCT is analytically tractable. In this setting, we show theoretically and empirically that the error of Laplace/BIC grows linearly with (d/2 minus lambda) times log n, where d is the ambient parameter dimension and lambda is the RLCT. An RLCT-aware correction recovers the correct evidence slope and is invariant to overcomplete reparameterizations that represent the same data subspace.
  Our results provide a concrete finite-sample characterization of Laplace failure in singular models and demonstrate that evidence slopes can be used as a practical estimator of effective dimension in simple linear settings.

</details>


### [542] [Fast Gibbs Sampling on Bayesian Hidden Markov Model with Missing Observations](https://arxiv.org/abs/2601.01442)
*Dongrong Li,Tianwei Yu,Xiaodan Fan*

Main category: stat.ML

TL;DR: The paper introduces a collapsed Gibbs sampler for Hidden Markov Models (HMMs) to handle missing data more efficiently, achieving better time complexity and sampling efficiency compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in applying Hidden Markov Models (HMMs) to datasets with missing observations, particularly the inefficiencies and limitations of current methods like the EM algorithm and Gibbs samplers.

Method: A collapsed Gibbs sampler is introduced, which integrates out both missing observations and latent states to improve sampling efficiency, computational speed, and achieve higher Effective Sample Size (ESS) per iteration.

Result: The proposed sampler demonstrates significant advantages: maintaining comparable estimation accuracy, offering higher ESS per iteration, and reducing computational complexity when the data has many missing entries.

Conclusion: The new sampling algorithm is faster, theoretically sound, and performs better empirically, especially in scenarios with a high number of missing entries, outperforming traditional approaches in terms of time complexity and efficiency.

Abstract: The Hidden Markov Model (HMM) is a widely-used statistical model for handling sequential data. However, the presence of missing observations in real-world datasets often complicates the application of the model. The EM algorithm and Gibbs samplers can be used to estimate the model, yet suffering from various problems including non-convexity, high computational complexity and slow mixing. In this paper, we propose a collapsed Gibbs sampler that efficiently samples from HMMs' posterior by integrating out both the missing observations and the corresponding latent states. The proposed sampler is fast due to its three advantages. First, it achieves an estimation accuracy that is comparable to existing methods. Second, it can produce a larger Effective Sample Size (ESS) per iteration, which can be justified theoretically and numerically. Third, when the number of missing entries is large, the sampler has a significant smaller computational complexity per iteration compared to other methods, thus is faster computationally. In summary, the proposed sampling algorithm is fast both computationally and theoretically and is particularly advantageous when there are a lot of missing entries. Finally, empirical evaluations based on numerical simulations and real data analysis demonstrate that the proposed algorithm consistently outperforms existing algorithms in terms of time complexity and sampling efficiency (measured in ESS).

</details>


### [543] [Modeling Information Blackouts in Missing Not-At-Random Time Series Data](https://arxiv.org/abs/2601.01480)
*Aman Sunesh,Allan Ma,Siddarth Nilol*

Main category: stat.ML

TL;DR: This paper addresses traffic forecasting issues caused by sensor outages and introduces a model that improves imputation and forecasting performance under Missing Not At Random (MNAR) assumptions.


<details>
  <summary>Details</summary>
Motivation: Sensor blackouts in traffic monitoring systems can correlate with unobserved traffic conditions, thus challenging the assumption of Missing At Random (MAR). Addressing this problem motivates the need for MNAR treatment in traffic forecasting.

Method: A latent state-space framework is used to jointly model traffic dynamics with a linear dynamical system and sensor dropout using a Bernoulli observation channel dependent on latent traffic states. Inference is conducted with an Extended Kalman Filter and parameters learned via an approximate EM procedure.

Result: The proposed method achieves considerably lower imputation RMSE compared to baseline methods on real data (4.23 under MAR LDS vs. 7.02 and 5.02 from baseline methods) and sees slight improvement with explicit MNAR modeling. Synthetic experiments show MNAR performs better when missingness strongly depends on latent state.

Conclusion: Temporal dynamics significantly improve traffic forecasting performance, while MNAR modeling provides a fine-tuned approach that enhances results when missingness patterns are informative.

Abstract: Large-scale traffic forecasting relies on fixed sensor networks that often exhibit blackouts: contiguous intervals of missing measurements caused by detector or communication failures. These outages are typically handled under a Missing At Random (MAR) assumption, even though blackout events may correlate with unobserved traffic conditions (e.g., congestion or anomalous flow), motivating a Missing Not At Random (MNAR) treatment. We propose a latent state-space framework that jointly models (i) traffic dynamics via a linear dynamical system and (ii) sensor dropout via a Bernoulli observation channel whose probability depends on the latent traffic state. Inference uses an Extended Kalman Filter with Rauch-Tung-Striebel smoothing, and parameters are learned via an approximate EM procedure with a dedicated update for detector-specific missingness parameters. On the Seattle inductive loop detector data, introducing latent dynamics yields large gains over naive baselines, reducing blackout imputation RMSE from 7.02 (LOCF) and 5.02 (linear interpolation + seasonal naive) to 4.23 (MAR LDS), corresponding to about a 64% reduction in MSE relative to LOCF. Explicit MNAR modeling provides a consistent but smaller additional improvement on real data (imputation RMSE 4.20; 0.8% RMSE reduction relative to MAR), with similar modest gains for short-horizon post-blackout forecasts (evaluated at 1, 3, and 6 steps). In controlled synthetic experiments, the MNAR advantage increases as the true missingness dependence on latent state strengthens. Overall, temporal dynamics dominate performance, while MNAR modeling offers a principled refinement that becomes most valuable when missingness is genuinely informative.

</details>


### [544] [Variance-Reduced Diffusion Sampling via Conditional Score Expectation Identity](https://arxiv.org/abs/2601.01594)
*Alois Duston,Tan Bui-Thanh*

Main category: stat.ML

TL;DR: The paper introduces the Conditional Score Expectation (CSE) identity for affine diffusion processes. It proposes methods to estimate the score efficiently and applies these to high-dimensional image reconstruction and inverse problems.


<details>
  <summary>Details</summary>
Motivation: To establish a new statistical framework improving score estimation and computational performance in applications like image reconstruction and Bayesian inverse problems.

Method: The method involves proving the CSE identity, proposing a Self-Normalized Importance Sampling estimator, and deriving a variance-minimizing blended score estimator.

Result: The blended score estimator reduces variance and improves sample quality in statistical estimation, and enhances reconstruction quality and sample diversity in Bayesian inverse problems.

Conclusion: The proposed estimators offer computational efficiency and better performance in statistical estimation and image reconstruction tasks compared to existing methods.

Abstract: We introduce and prove a \textbf{Conditional Score Expectation (CSE)} identity: an exact relation for the marginal score of affine diffusion processes that links scores across time via a conditional expectation under the forward dynamics. Motivated by this identity, we propose a CSE-based statistical estimator for the score using a Self-Normalized Importance Sampling (SNIS) procedure with prior samples and forward noise. We analyze its relationship to the standard Tweedie estimator, proving anti-correlation for Gaussian targets and establishing the same behavior for general targets in the small time-step regime. Exploiting this structure, we derive a variance-minimizing blended score estimator given by a state--time dependent convex combination of the CSE and Tweedie estimators. Numerical experiments show that this optimal-blending estimator reduces variance and improves sample quality for a fixed computational budget compared to either baseline. We further extend the framework to Bayesian inverse problems via likelihood-informed SNIS weights, and demonstrate improved reconstruction quality and sample diversity on high-dimensional image reconstruction tasks and PDE-governed inverse problems.

</details>


### [545] [Deep Linear Discriminant Analysis Revisited](https://arxiv.org/abs/2601.01619)
*Maxat Tezekbayev,Rustem Takhanov,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: This paper addresses issues with Deep Linear Discriminant Analysis classifiers by introducing the Discriminative Negative Log-Likelihood (DNLL) loss, improving class separation and predictive reliability.


<details>
  <summary>Details</summary>
Motivation: Improve the discriminative and probabilistic performance of Deep Linear Discriminant Analysis (LDA) classifiers while addressing issues in maximum-likelihood and cross-entropy training.

Method: The authors propose DNLL loss, which modifies the LDA log-likelihood training by penalizing regions with overlapping class likelihoods, encouraging separation.

Result: Using DNLL, deep LDA models exhibit cleaner latent spaces, achieve competitive accuracy on benchmarks, and improve predictive probability calibration.

Conclusion: DNLL reconciles generative structure with discriminative performance, enhancing the utility and reliability of deep LDA classifiers.

Abstract: We show that for unconstrained Deep Linear Discriminant Analysis (LDA) classifiers, maximum-likelihood training admits pathological solutions in which class means drift together, covariances collapse, and the learned representation becomes almost non-discriminative. Conversely, cross-entropy training yields excellent accuracy but decouples the head from the underlying generative model, leading to highly inconsistent parameter estimates. To reconcile generative structure with discriminative performance, we introduce the \emph{Discriminative Negative Log-Likelihood} (DNLL) loss, which augments the LDA log-likelihood with a simple penalty on the mixture density. DNLL can be interpreted as standard LDA NLL plus a term that explicitly discourages regions where several classes are simultaneously likely. Deep LDA trained with DNLL produces clean, well-separated latent spaces, matches the test accuracy of softmax classifiers on synthetic data and standard image benchmarks, and yields substantially better calibrated predictive probabilities, restoring a coherent probabilistic interpretation to deep discriminant models.

</details>


### [546] [Simplex Deep Linear Discriminant Analysis](https://arxiv.org/abs/2601.01679)
*Maxat Tezekbayev,Arman Bolatov,Zhenisbek Assylbekov*

Main category: stat.ML

TL;DR: The paper revisits Deep LDA, discussing its limitations with unconstrained training and proposing a constrained formulation to improve stability and class separation.


<details>
  <summary>Details</summary>
Motivation: Improve training stability and classification performance of Deep LDA models while maintaining interpretable latent geometry.

Method: Introduce geometric constraints (class means fixed to regular simplex vertices, shared spherical covariance) for stable maximum likelihood estimation.

Result: Deep LDA models under the constraints achieve competitive accuracy on Fashion-MNIST, CIFAR-10, and CIFAR-100 compared to softmax baselines, with interpretable latent geometry.

Conclusion: Constrained Deep LDA provides both competitive accuracy and interpretable latent representations, addressing issues seen in unconstrained Deep LDA training.

Abstract: We revisit Deep Linear Discriminant Analysis (Deep LDA) from a likelihood-based perspective. While classical LDA is a simple Gaussian model with linear decision boundaries, attaching an LDA head to a neural encoder raises the question of how to train the resulting deep classifier by maximum likelihood estimation (MLE). We first show that end-to-end MLE training of an unconstrained Deep LDA model ignores discrimination: when both the LDA parameters and the encoder parameters are learned jointly, the likelihood admits a degenerate solution in which some of the class clusters may heavily overlap or even collapse, and classification performance deteriorates. Batchwise moment re-estimation of the LDA parameters does not remove this failure mode. We then propose a constrained Deep LDA formulation that fixes the class means to the vertices of a regular simplex in the latent space and restricts the shared covariance to be spherical, leaving only the priors and a single variance parameter to be learned along with the encoder. Under these geometric constraints, MLE becomes stable and yields well-separated class clusters in the latent space. On images (Fashion-MNIST, CIFAR-10, CIFAR-100), the resulting Deep LDA models achieve accuracy competitive with softmax baselines while offering a simple, interpretable latent geometry that is clearly visible in two-dimensional projections.

</details>


### [547] [Sparse Convex Biclustering](https://arxiv.org/abs/2601.01757)
*Jiakun Jiang,Dewei Xiang,Chenliang Gu,Wei Liu,Binhuan Wang*

Main category: stat.ML

TL;DR: The paper introduces Sparse Convex Biclustering (SpaCoBi) to address noise, non-convex optimization, and scalability issues in biclustering of large datasets.


<details>
  <summary>Details</summary>
Motivation: Existing biclustering methods face challenges in scalability, noise handling, non-convex optimization, and maintaining accuracy for large datasets.

Method: SpaCoBi deploys convex optimization and integrates a stability-based tuning criterion to enhance cluster fidelity and sparsity.

Result: Simulations and a mouse olfactory bulb dataset application show SpaCoBi outperforms state-of-the-art biclustering methods in accuracy.

Conclusion: SpaCoBi proves to be a robust and efficient approach for high-dimensional, large-scale biclustering tasks, addressing prior limitations in the field.

Abstract: Biclustering is an essential unsupervised machine learning technique for simultaneously clustering rows and columns of a data matrix, with widespread applications in genomics, transcriptomics, and other high-dimensional omics data. Despite its importance, existing biclustering methods struggle to meet the demands of modern large-scale datasets. The challenges stem from the accumulation of noise in high-dimensional features, the limitations of non-convex optimization formulations, and the computational complexity of identifying meaningful biclusters. These issues often result in reduced accuracy and stability as the size of the dataset increases. To overcome these challenges, we propose Sparse Convex Biclustering (SpaCoBi), a novel method that penalizes noise during the biclustering process to improve both accuracy and robustness. By adopting a convex optimization framework and introducing a stability-based tuning criterion, SpaCoBi achieves an optimal balance between cluster fidelity and sparsity. Comprehensive numerical studies, including simulations and an application to mouse olfactory bulb data, demonstrate that SpaCoBi significantly outperforms state-of-the-art methods in accuracy. These results highlight SpaCoBi as a robust and efficient solution for biclustering in high-dimensional and large-scale datasets.

</details>


### [548] [A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk](https://arxiv.org/abs/2601.01970)
*Ayomide Afolabi,Ebere Ogburu,Symon Kimitei*

Main category: stat.ML

TL;DR: The study evaluates classifiers for three models: response, risk, and response-risk, in credit campaigns and default prediction. Key classifiers include Extra Trees and Random Forest.


<details>
  <summary>Details</summary>
Motivation: The study aims to optimize credit card mail campaigns and default prediction by analyzing classifiers for targeted offers and risk assessment.

Method: The research utilizes different classifiers (Extra Trees, Random Forest) to evaluate recall, specificity, and accuracy for distinct response, risk, and combined models.

Result: Extra Trees achieved 79.1% recall in response model, Random Forest reached 84.1% specificity in risk, and 83.2% accuracy in multi-class response-risk models.

Conclusion: The study highlights the optimized performance of classifiers (Extra Trees, Random Forest) for solving problems of credit risk and mail responsiveness.

Abstract: This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem.

</details>


### [549] [From Mice to Trains: Amortized Bayesian Inference on Graph Data](https://arxiv.org/abs/2601.02241)
*Svenja Jedhoff,Elizaveta Semenova,Aura Raulo,Anne Meyer,Paul-Christian Bürkner*

Main category: stat.ML

TL;DR: The paper adapts Amortized Bayesian Inference (ABI) to perform inference on graph-structured data using a two-module pipeline consisting of summary networks and inference networks.


<details>
  <summary>Details</summary>
Motivation: Inference on graph-structured data is challenging due to permutation-invariance requirements, scalability across sizes and sparsities, and capturing long-range dependencies, which makes posterior estimation difficult.

Method: The paper combines permutation-invariant graph encoders with neural posterior estimators using a two-module pipeline: a summary network for fixed-length representations and an inference network for posterior approximation.

Result: The approach is evaluated on synthetic data and two real-world domains (biology and logistics). Various architectures for the summary network are compared in terms of recovery and calibration.

Conclusion: Adapting ABI with permutation-invariant graph encoders and flexible posterior estimators offers an effective solution for tackling inference challenges in graph data.

Abstract: Graphs arise across diverse domains, from biology and chemistry to social and information networks, as well as in transportation and logistics. Inference on graph-structured data requires methods that are permutation-invariant, scalable across varying sizes and sparsities, and capable of capturing complex long-range dependencies, making posterior estimation on graph parameters particularly challenging. Amortized Bayesian Inference (ABI) is a simulation-based framework that employs generative neural networks to enable fast, likelihood-free posterior inference. We adapt ABI to graph data to address these challenges to perform inference on node-, edge-, and graph-level parameters. Our approach couples permutation-invariant graph encoders with flexible neural posterior estimators in a two-module pipeline: a summary network maps attributed graphs to fixed-length representations, and an inference network approximates the posterior over parameters. In this setting, several neural architectures can serve as the summary network. In this work we evaluate multiple architectures and assess their performance on controlled synthetic settings and two real-world domains - biology and logistics - in terms of recovery and calibration.

</details>


<div id='nlin.CG'></div>

# nlin.CG [[Back]](#toc)

### [550] [Visualizing the Structure of Lenia Parameter Space](https://arxiv.org/abs/2601.01932)
*Barbora Hudcová,František Dušek,Marco Tuccio,Clément Hongler*

Main category: nlin.CG

TL;DR: The paper presents a novel method to classify Lenia systems into four dynamical classes and offers insights into the soliton behavior and parameter space structure.


<details>
  <summary>Details</summary>
Motivation: Understanding the behavior of continuous cellular automata like Lenia is crucial due to challenges in defining solitons, exploring parameter space structure, and locating solitons.

Method: Developed an automated classification system for Lenia systems into four distinct dynamical categories and created an interactive tool to visualize the parameter space.

Result: Discovered new soliton families in unexpected parameter regions and showed the universal structure of phase space across different kernels.

Conclusion: This work provides a deeper understanding of Lenia, addressing key open questions and contributing to the broader understanding of continuous cellular automata.

Abstract: Continuous cellular automata are rocketing in popularity, yet developing a theoretical understanding of their behaviour remains a challenge. In the case of Lenia, a few fundamental open problems include determining what exactly constitutes a soliton, what is the overall structure of the parameter space, and where do the solitons occur in it. In this abstract, we present a new method to automatically classify Lenia systems into four qualitatively different dynamical classes. This allows us to detect moving solitons, and to provide an interactive visualization of Lenia's parameter space structure on our website https://lenia-explorer.vercel.app/. The results shed new light on the above-mentioned questions and lead to several observations: the existence of new soliton families for parameters where they were not believed to exist, or the universality of the phase space structure across various kernels.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [551] [Disordered Dynamics in High Dimensions: Connections to Random Matrices and Machine Learning](https://arxiv.org/abs/2601.01010)
*Blake Bordelon,Cengiz Pehlevan*

Main category: cond-mat.dis-nn

TL;DR: The paper provides a detailed analysis of high-dimensional dynamical systems driven by random matrices, focusing on applications in machine learning theory using tools like dynamical mean field theory (DMFT), stochastic processes, and deep learning models.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the dynamics of high-dimensional random systems and apply this understanding to learning models in machine learning, exploring deeper links between random matrices and training dynamics.

Method: The authors use DMFT, cavity methods, and stochastic processes to examine random matrix-driven systems. Applications span gradient flow, stochastic descent, and deep linear neural networks, analyzing features like bias, variance, and ensemble behavior.

Result: Key results include descriptions of training/test loss dynamics, mechanisms for non-monotonic loss curves, and insights into random non-Hermitian versus Hermitian matrix behavior. Time translation invariance loss in deep networks is explained using spiked random matrices.

Conclusion: This work establishes a theoretical framework for analyzing learning dynamics in high-dimensional, random systems, revealing new mechanisms in randomness, loss behaviors, and the structures of deep neural network training.

Abstract: We provide an overview of high dimensional dynamical systems driven by random matrices, focusing on applications to simple models of learning and generalization in machine learning theory. Using both cavity method arguments and path integrals, we review how the behavior of a coupled infinite dimensional system can be characterized as a stochastic process for each single site of the system. We provide a pedagogical treatment of dynamical mean field theory (DMFT), a framework that can be flexibly applied to these settings. The DMFT single site stochastic process is fully characterized by a set of (two-time) correlation and response functions. For linear time-invariant systems, we illustrate connections between random matrix resolvents and the DMFT response. We demonstrate applications of these ideas to machine learning models such as gradient flow, stochastic gradient descent on random feature models and deep linear networks in the feature learning regime trained on random data. We demonstrate how bias and variance decompositions (analysis of ensembling/bagging etc) can be computed by averaging over subsets of the DMFT noise variables. From our formalism we also investigate how linear systems driven with random non-Hermitian matrices (such as random feature models) can exhibit non-monotonic loss curves with training time, while Hermitian matrices with the matching spectra do not, highlighting a different mechanism for non-monotonicity than small eigenvalues causing instability to label noise. Lastly, we provide asymptotic descriptions of the training and test loss dynamics for randomly initialized deep linear neural networks trained in the feature learning regime with high-dimensional random data. In this case, the time translation invariance structure is lost and the hidden layer weights are characterized as spiked random matrices.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [552] [Physically-Constrained Autoencoder-Assisted Bayesian Optimization for Refinement of High-Dimensional Defect-Sensitive Single Crystalline Structure](https://arxiv.org/abs/2601.00855)
*Joseph Oche Agada,Andrew McAninch,Haley Day,Yasemin Tanyu,Ewan McCombs,Seyed M. Koohpayeh,Brian H. Toby,Yishu Wang,Arpan Biswas*

Main category: cond-mat.mtrl-sci

TL;DR: This paper introduces a Machine Learning framework to refine crystal structures and quantify defects in materials more effectively. It combines a physically-constrained variational-autoencoder (pcVAE) with Bayesian Optimization (BO) in a computational workflow.


<details>
  <summary>Details</summary>
Motivation: The study aims to enhance the understanding of structure-property relationships in materials by addressing the limitations of traditional methods like Rietveld refinement, which often struggle with complex, nonlinear systems and local minima in high-dimensional spaces.

Method: The authors use a physically-constrained variational-autoencoder (pcVAE) to reduce high-dimensional diffraction data to a lower-dimensional latent space while preserving physical relevance. The reduced representation is then optimized through different Bayesian Optimization methods to minimize chisq errors between experimental and simulated data.

Result: The paper compares the efficiency of pcVAE-assisted Bayesian Optimization with traditional Rietveld refinement and non-pcVAE optimization approaches, demonstrating improved results in refining crystal structure parameters and defect resolution.

Conclusion: By combining pcVAE and Bayesian Optimization, this framework accelerates the exploration of high-dimensional parameter spaces, improving crystal structure refinements and providing a better understanding of defect-sensitive material systems.

Abstract: Physical properties and functionalities of materials are dictated by global crystal structures as well as local defects. To establish a structure-property relationship, not only the crystallographic symmetry but also quantitative knowledge about defects are required. Here we present a hybrid Machine Learning framework that integrates a physically-constrained variational-autoencoder (pcVAE) with different Bayesian Optimization (BO) methods to systematically accelerate and improve crystal structure refinement with resolution of defects. We chose the pyrochlore structured Ho2Ti2O7 as a model system and employed the GSAS2 package for benchmarking crystallographic parameters from Rietveld refinement. However, the function space of these material systems is highly nonlinear, which limits optimizers like traditional Rietveld refinement, into trapping at local minima. Also, these naive methods don't provide an extensive learning about the overall function space, which is essential for large space, large time consuming explorations to identify various potential regions of interest. Thus, we present the approach of exploring the high Dimensional structure parameters of defect sensitive systems via pretrained pcVAE assisted BO and Sparse Axis Aligned BO. The pcVAE projects high-Dimensional diffraction data consisting of thousands of independently measured diffraction orders into a lowD latent space while enforcing scaling invariance and physical relevance. Then via BO methods, we aim to minimize the L2 norm based chisq errors in the real and latent spaces separately between experimental and simulated diffraction patterns, thereby steering the refinement towards potential optimum crystal structure parameters. We investigated and compared the results among different pcVAE assisted BO, non pcVAE assisted BO, and Rietveld refinement.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [553] [Deciding Serializability in Network Systems](https://arxiv.org/abs/2601.02251)
*Guy Amir,Mark Barbone,Nicolas Amat,Jules Jacobs*

Main category: cs.FL

TL;DR: This paper introduces the SER modeling language for verifying if concurrent program executions are serializable, using automated methods to prove or provide counterexamples.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of verifying the serializability of concurrent programs, crucial for ensuring predictable program execution and correctness.

Method: The paper develops an automated decision procedure using the SER modeling language, compiling to a network-system abstraction and reducing the serializability problem to Petri net reachability with optimization techniques.

Result: The framework successfully evaluates real-world program models, such as firewalls and routers, despite the complexity of the problem.

Conclusion: The SER modeling language is effective for automated serializability verification, demonstrating scalability for practical applications.

Abstract: We present the SER modeling language for automatically verifying serializability of concurrent programs, i.e., whether every concurrent execution of the program is equivalent to some serial execution.
  SER programs are suitably restricted to make this problem decidable, while still allowing for an unbounded number of concurrent threads of execution, each potentially running for an unbounded number of steps.
  Building on prior theoretical results, we give the first automated end-to-end decision procedure that either proves serializability by producing a checkable certificate, or refutes it by producing a counterexample trace.
  We also present a network-system abstraction to which SER programs compile. Our decision procedure then reduces serializability in this setting to a Petri net reachability query.
  Furthermore, in order to scale, we curtail the search space via multiple optimizations, including Petri net slicing, semilinear-set compression, and Presburger-formula manipulation.
  We extensively evaluate our framework and show that, despite the theoretical hardness of the problem, it can successfully handle various models of real-world programs, including stateful firewalls, BGP routers, and more.

</details>


<div id='cs.CC'></div>

# cs.CC [[Back]](#toc)

### [554] [Hidden costs for inference with deep network on embedded system devices](https://arxiv.org/abs/2601.01698)
*Chankyu Lee,Woohyun Choi,Sangwook Park*

Main category: cs.CC

TL;DR: This study analyzes inference performance of deep learning models on embedded systems, challenging the Multiply-Accumulate operation metric for its limitations in estimating inference time.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the overlooked factors in real-time deep learning model optimization for embedded systems beyond Multiply-Accumulate operations.

Method: The study compares and analyzes Deep Learning models' inference times versus Multiply-Accumulate operations on a CIFAR-100 dataset, tested on an embedded system device.

Result: The experiments show that other factors, such as additional tensor computations, significantly affect inference performance, not just Multiply-Accumulate operations.

Conclusion: Performance optimization for deep learning in embedded systems demands considering computations beyond Multiply-Accumulate operations for accurate real-time efficiency assessment.

Abstract: This study evaluates the inference performance of various deep learning models under an embedded system environment. In previous works, Multiply-Accumulate operation is typically used to measure computational load of a deep model. According to this study, however, this metric has a limitation to estimate inference time on embedded devices. This paper poses the question of what aspects are overlooked when expressed in terms of Multiply-Accumulate operations. In experiments, an image classification task is performed on an embedded system device using the CIFAR-100 dataset to compare and analyze the inference times of ten deep models with the theoretically calculated Multiply-Accumulate operations for each model. The results highlight the importance of considering additional computations between tensors when optimizing deep learning models for real-time performing in embedded systems.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [555] [Predicting Early and Complete Drug Release from Long-Acting Injectables Using Explainable Machine Learning](https://arxiv.org/abs/2601.02265)
*Karla N. Robles,Manar D. Samad*

Main category: q-bio.BM

TL;DR: This paper introduces a novel machine learning (ML) approach to optimize polymer-based long-acting injectables (LAIs) by modeling and predicting key drug release patterns from 321 formulations.


<details>
  <summary>Details</summary>
Motivation: Current long-acting injectables (LAIs) require extensive property optimization for controlled drug release. Machine learning has potential but existing studies fail to provide actionable insights due to a lack of tailored modeling for LAI data.

Method: A custom data transformation and explainable ML approach were developed to predict early drug release (24, 48, and 72 hours), classify release profile types, and forecast complete release profiles. The framework integrated Shapley additive explanations to analyze material influence.

Result: Achieved >0.65 correlation in predicting 72-hour release, an 0.87 F1-score in release profile classification, and improved biphasic/triphasic release forecasting compared to time-dependent methods.

Conclusion: The framework advances understanding and optimization of LAIs' drug-release dynamics, offering a quantitative strategy for systematic development. Publicly available source code facilitates broader applicability.

Abstract: Polymer-based long-acting injectables (LAIs) have transformed the treatment of chronic diseases by enabling controlled drug delivery, thus reducing dosing frequency and extending therapeutic duration. Achieving controlled drug release from LAIs requires extensive optimization of the complex underlying physicochemical properties. Machine learning (ML) can accelerate LAI development by modeling the complex relationships between LAI properties and drug release. However, recent ML studies have provided limited information on key properties that modulate drug release, due to the lack of custom modeling and analysis tailored to LAI data. This paper presents a novel data transformation and explainable ML approach to synthesize actionable information from 321 LAI formulations by predicting early drug release at 24, 48, and 72 hours, classification of release profile types, and prediction of complete release profiles. These three experiments investigate the contribution and control of LAI material characteristics in early and complete drug release profiles. A strong correlation (>0.65) is observed between the true and predicted drug release in 72 hours, while a 0.87 F1-score is obtained in classifying release profile types. A time-independent ML framework predicts delayed biphasic and triphasic curves with better performance than current time-dependent approaches. Shapley additive explanations reveal the relative influence of material characteristics during early and for complete release which fill several gaps in previous in-vitro and ML-based studies. The novel approach and findings can provide a quantitative strategy and recommendations for scientists to optimize the drug-release dynamics of LAI. The source code for the model implementation is publicly available.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [556] [AI-enhanced tuning of quantum dot Hamiltonians toward Majorana modes](https://arxiv.org/abs/2601.02149)
*Mateusz Krawczyk,Jarosław Pawłowski*

Main category: cond-mat.mes-hall

TL;DR: A neural network-based model uses unsupervised learning on conductance maps to automatically tune quantum dot simulators to generate Majorana modes.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method to autotune quantum dot simulators toward achieving Majorana modes without manual adjustments or extensive experimentation.

Method: A deep vision-transformer network is trained on synthetic conductance map data using a physics-informed loss function, enabling it to propose parameter updates that guide the system toward a topological phase.

Result: The model efficiently connects Hamiltonian parameters with conductance map structures and achieves topological states with updated zero modes in just one or iterative parameter update steps.

Conclusion: The approach demonstrates potential to optimize large regions of parameter space, automating quantum dot tunings for Majorana modes and advancing the field of topological quantum computation.

Abstract: We propose a neural network-based model capable of learning the broad landscape of working regimes in quantum dot simulators, and using this knowledge to autotune these devices - based on transport measurements - toward obtaining Majorana modes in the structure. The model is trained in an unsupervised manner on synthetic data in the form of conductance maps, using a physics-informed loss that incorporates key properties of Majorana zero modes. We show that, with appropriate training, a deep vision-transformer network can efficiently memorize relation between Hamiltonian parameters and structures on conductance maps and use it to propose parameters update for a quantum dot chain that drive the system toward topological phase. Starting from a broad range of initial detunings in parameter space, a single update step is sufficient to generate nontrivial zero modes. Moreover, by enabling an iterative tuning procedure - where the system acquires updated conductance maps at each step - we demonstrate that the method can address a much larger region of the parameter space.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [557] [Entity-Aware and Secure Query Optimization in Database Using Named Entity Recognition](https://arxiv.org/abs/2601.01254)
*Azrin Sultana,Hasibur Rashid Chayon*

Main category: cs.DB

TL;DR: This paper introduces a privacy-preserving query optimization framework using Named Entity Recognition (NER), deep learning models, and advanced encryption to securely and efficiently handle sensitive cloud-based data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations of traditional privacy-preserving approaches, particularly the lack of automatic identification of sensitive information before encryption, which affects data retrieval efficiency and privacy risks.

Method: The framework uses NER for identifying sensitive information, employs deep learning and transformer models for classification, AES algorithm for encryption, and optimization techniques like blind indexing and K-means clustering to handle sensitive and non-sensitive data effectively.

Result: The proposed model achieves 93% accuracy with DBN-LSTM for NER, improves encrypted search speed using blind indexing, and provides superior performance for non-sensitive data queries compared to traditional methods.

Conclusion: This research significantly enhances privacy-preserving query optimization by integrating sensitive data detection, encryption, and improved query performance, advancing cloud infrastructure data security.

Abstract: Cloud storage has become the backbone of modern data infrastructure, yet privacy and efficient data retrieval remain significant challenges. Traditional privacy-preserving approaches primarily focus on enhancing database security but fail to address the automatic identification of sensitive information before encryption. This can dramatically reduce query processing time and mitigate errors during manual identification of sensitive information, thereby reducing potential privacy risks. To address this limitation, this research proposes an intelligent privacy-preserving query optimization framework that integrates Named Entity Recognition (NER) to detect sensitive information in queries, utilizing secure data encryption and query optimization techniques for both sensitive and non-sensitive data in parallel, thereby enabling efficient database optimization. Combined deep learning algorithms and transformer-based models to detect and classify sensitive entities with high precision, and the Advanced Encryption Standard (AES) algorithm to encrypt, with blind indexing to secure search functionality of the sensitive data, whereas non-sensitive data was divided into groups using the K-means algorithm, along with a rank search for optimization. Among all NER models, the Deep Belief Network combined with Long Short-Term Memory (DBN-LSTM) delivers the best performance, with an accuracy of 93% and precision (94%), recall, and F1 score of 93%, and 93%, respectively. Besides, encrypted search achieved considerably faster results with the help of blind indexing, and non-sensitive data fetching also outperformed traditional clustering-based searches. By integrating sensitive data detection, encryption, and query optimization, this work advances the state of privacy-preserving computation in modern cloud infrastructures.

</details>


### [558] [SafeLoad: Efficient Admission Control Framework for Identifying Memory-Overloading Queries in Cloud Data Warehouses](https://arxiv.org/abs/2601.01888)
*Yifan Wu,Yuhan Li,Zhenhua Wang,Zhongle Xie,Dingyu Yang,Ke Chen,Lidan Shou,Bo Tang,Liang Lin,Huan Li,Gang Chen*

Main category: cs.DB

TL;DR: The paper introduces SafeLoad, a framework to predict and manage memory-overloading (MO) queries in cloud data warehouses, and releases SafeBench, a benchmark dataset for the task.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies and disruptions caused by memory-overloading queries, improving their prediction and management in cloud data warehouses.

Method: SafeLoad combines interpretable discriminative rules, a hybrid architecture of global and cluster-level models, a misprediction correction module, and a self-tuning quota management mechanism.

Result: Experimental results show SafeLoad improves prediction precision by 66%, reduces wasted CPU time by up to 8.09x compared to scenarios without it.

Conclusion: SafeLoad provides a high-performance admission control framework for MO queries, complemented by SafeBench, greatly enhancing efficiency and precision in cloud data warehouse operations.

Abstract: Memory overload is a common form of resource exhaustion in cloud data warehouses. When database queries fail due to memory overload, it not only wastes critical resources such as CPU time but also disrupts the execution of core business processes, as memory-overloading (MO) queries are typically part of complex workflows. If such queries are identified in advance and scheduled to memory-rich serverless clusters, it can prevent resource wastage and query execution failure. Therefore, cloud data warehouses desire an admission control framework with high prediction precision, interpretability, efficiency, and adaptability to effectively identify MO queries. However, existing admission control frameworks primarily focus on scenarios like SLA satisfaction and resource isolation, with limited precision in identifying MO queries. Moreover, there is a lack of publicly available MO-labeled datasets with workloads for training and benchmarking. To tackle these challenges, we propose SafeLoad, the first query admission control framework specifically designed to identify MO queries. Alongside, we release SafeBench, an open-source, industrial-scale benchmark for this task, which includes 150 million real queries. SafeLoad first filters out memory-safe queries using the interpretable discriminative rule. It then applies a hybrid architecture that integrates both a global model and cluster-level models, supplemented by a misprediction correction module to identify MO queries. Additionally, a self-tuning quota management mechanism dynamically adjusts prediction quotas per cluster to improve precision. Experimental results show that SafeLoad achieves state-of-the-art prediction performance with low online and offline time overhead. Specifically, SafeLoad improves precision by up to 66% over the best baseline and reduces wasted CPU time by up to 8.09x compared to scenarios without SafeLoad.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [559] [Bithoven: Formal Safety for Expressive Bitcoin Smart Contracts](https://arxiv.org/abs/2601.01436)
*Hyunhum Cho,Ik Rae Jeong*

Main category: cs.CR

TL;DR: Bitcoin's scripting poses security and usability issues; Bithoven provides a high-level, type-safe language for better contract design.


<details>
  <summary>Details</summary>
Motivation: Bitcoin's UTXO model causes security vulnerabilities and limits developer usability; there's a need for safer abstractions.

Method: Developed Bithoven, a language with type checker, liveness analyzer, and control-flow analyzer to ensure safety.

Result: Bithoven effectively eliminates major consensus defects and compiles to efficient Bitcoin Script comparable to manual optimization.

Conclusion: The paper proves that type-safe, developer-friendly coding for Bitcoin is achievable without sacrificing blockchain efficiency.

Abstract: The rigorous security model of Bitcoin's UTXO architecture often comes at the cost of developer usability, forcing a reliance on manual stack manipulation that leads to critical financial vulnerabilities like signature malleability, unspendable states and unconstrained execution paths. Industry standards such as Miniscript provide necessary abstractions for policy verification but do not model the full imperative logic required for complex contracts, leaving gaps in state management and resource liveness. This paper introduces Bithoven, a high-level language designed to bridge the gap between expressiveness and formal safety. By integrating a strict type checker and a resource liveness analyzer with a semantic control-flow analyzer, Bithoven eliminates major categories of consensus and logic defects defined in our fault model prior to deployment. Our results indicate that this safety comes at modest cost: Bithoven compiles to Bitcoin Script with efficiency comparable to hand-optimized code, demonstrating that type-safe, developer-friendly abstractions are viable even within the strict byte-size constraints of the Bitcoin blockchain.

</details>


### [560] [Vouchsafe: A Zero-Infrastructure Capability Graph Model for Offline Identity and Trust](https://arxiv.org/abs/2601.02254)
*Jay Kuri*

Main category: cs.CR

TL;DR: The paper introduces a system for secure identity and trust called ZI-CG that operates without online infrastructure, and demonstrates its practical implementation with the Vouchsafe model.


<details>
  <summary>Details</summary>
Motivation: Existing identity and trust systems fail in critical conditions such as disaster zones or adversarial environments, primarily because they rely on functional network infrastructure.

Method: The authors developed the Zero-Infrastructure Capability Graph (ZI-CG), a model for self-contained, locally evaluated signed statements to manage identity, delegation, and revocation. This model is instantiated in the Vouchsafe system using existing cryptographic primitives like Ed25519, SHA-256, and JSON Web Tokens.

Result: The authors showcase that offline-verifiable, practical trust systems can be constructed using current cryptographic technologies without needing online infrastructure.

Conclusion: Trust and identity systems can be made resilient to infrastructure failures and adversarial conditions by locally validating self-contained signed statements, as demonstrated by the ZI-CG model and Vouchsafe system.

Abstract: Modern identity and trust systems collapse in the environments where they are needed most: disaster zones, disconnected or damaged networks, and adversarial conditions such as censorship or infrastructure interference. These systems depend on functioning networks to reach online authorities, resolvers, directories, and revocation services, leaving trust unverifiable whenever communication is unavailable or untrusted. This work demonstrates that secure identity and trust are possible without such infrastructure. We introduce the Zero-Infrastructure Capability Graph (ZI-CG), a model showing that identity, delegation, and revocation can be represented as self-contained, signed statements whose validity is determined entirely by local, deterministic evaluation. We further present Vouchsafe, a complete working instantiation of this model built using widely deployed primitives including Ed25519, SHA-256, and structured JSON Web Tokens, requiring no new cryptography or online services. The results show that a practical, offline-verifiable trust substrate can be constructed today using only the cryptographic data presented at evaluation time.

</details>


### [561] [MCP-SandboxScan: WASM-based Secure Execution and Runtime Analysis for MCP Tools](https://arxiv.org/abs/2601.01241)
*Zhuoran Tan,Run Hao,Jeremy Singer,Yutian Tang,Christos Anagnostopoulos*

Main category: cs.CR

TL;DR: This paper introduces MCP-SandboxScan, a framework to analyze security risks associated with tool-augmented LLMs by safely simulating tool executions in a WebAssembly/WASI sandbox, allowing researchers to identify and audit external-input vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the growing security risks in tool-augmented LLMs, such as runtime-only behaviors like prompt injection and external-input exposure, which traditional static scanners fail to detect.

Method: The method involves MCP-SandboxScan, a framework that executes untrusted tools in a sandbox environment, extracting relevant runtime outputs, identifying external inputs, and linking source-to-sink behaviors using substring matching.

Result: The study demonstrates the ability of MCP-SandboxScan to highlight provenance evidence of external inputs and expose filesystem violations in runtime tests, outperforming a static baseline in various scenarios.

Conclusion: The framework is an effective approach for analyzing runtime behaviors in tool-augmented LLMs, addressing security risks that static methods cannot evaluate while providing actionable runtime evidence.

Abstract: Tool-augmented LLM agents raise new security risks: tool executions can introduce runtime-only behaviors, including prompt injection and unintended exposure of external inputs (e.g., environment secrets or local files). While existing scanners often focus on static artifacts, analyzing runtime behavior is challenging because directly executing untrusted tools can itself be dangerous. We present MCP-SandboxScan, a lightweight framework motivated by the Model Context Protocol (MCP) that safely executes untrusted tools inside a WebAssembly/WASI sandbox and produces auditable reports of external-to-sink exposures. Our prototype (i) extracts LLM-relevant sinks from runtime outputs (prompt/messages and structured tool-return fields), (ii) instantiates external-input candidates from environment values, mounted file contents, and output-surfaced HTTP fetch intents, and (iii) links sources to sinks via snippet-based substring matching. Case studies on three representative tools show that MCP-SandboxScan can surface provenance evidence when external inputs appear in prompt/messages or tool-return payloads, and can expose filesystem capability violations as runtime evidence. We further compare against a lightweight static string-signature baseline and use a micro-benchmark to characterize false negatives under transformations and false positives from short-token collisions.

</details>


### [562] [The Silicon Psyche: Anthropomorphic Vulnerabilities in Large Language Models](https://arxiv.org/abs/2601.00867)
*Giuseppe Canale,Kashyap Thimmaraju*

Main category: cs.CR

TL;DR: LLMs moving into critical roles are vulnerable to human-like psychological exploits, necessitating new security measures.


<details>
  <summary>Details</summary>
Motivation: To address the overlooked vulnerabilities of LLMs inherited from human cognitive traits.

Method: Introduce Synthetic Psychometric Assessment Protocol and apply Cybersecurity Psychology Framework.

Result: LLMs show susceptibility to psychological exploits despite strong defenses against technical attacks.

Conclusion: Security measures must account for psychological vulnerabilities in AI systems to enhance robustness.

Abstract: Large Language Models (LLMs) are rapidly transitioning from conversational assistants to autonomous agents embedded in critical organizational functions, including Security Operations Centers (SOCs), financial systems, and infrastructure management. Current adversarial testing paradigms focus predominantly on technical attack vectors: prompt injection, jailbreaking, and data exfiltration. We argue this focus is catastrophically incomplete. LLMs, trained on vast corpora of human-generated text, have inherited not merely human knowledge but human \textit{psychological architecture} -- including the pre-cognitive vulnerabilities that render humans susceptible to social engineering, authority manipulation, and affective exploitation. This paper presents the first systematic application of the Cybersecurity Psychology Framework (\cpf{}), a 100-indicator taxonomy of human psychological vulnerabilities, to non-human cognitive agents. We introduce the \textbf{Synthetic Psychometric Assessment Protocol} (\sysname{}), a methodology for converting \cpf{} indicators into adversarial scenarios targeting LLM decision-making. Our preliminary hypothesis testing across seven major LLM families reveals a disturbing pattern: while models demonstrate robust defenses against traditional jailbreaks, they exhibit critical susceptibility to authority-gradient manipulation, temporal pressure exploitation, and convergent-state attacks that mirror human cognitive failure modes. We term this phenomenon \textbf{Anthropomorphic Vulnerability Inheritance} (AVI) and propose that the security community must urgently develop ``psychological firewalls'' -- intervention mechanisms adapted from the Cybersecurity Psychology Intervention Framework (\cpif{}) -- to protect AI agents operating in adversarial environments.

</details>


### [563] [Device-Native Autonomous Agents for Privacy-Preserving Negotiations](https://arxiv.org/abs/2601.00911)
*Joyjit Roy*

Main category: cs.CR

TL;DR: This paper presents a privacy-focused AI agent system for on-device automated negotiations in areas like insurance and B2B commerce by avoiding centralized data sharing.


<details>
  <summary>Details</summary>
Motivation: To address privacy and security concerns in automated negotiations, which compromise user trust by forcing sensitive data to be handled centrally.

Method: The study developed an autonomous AI system that uses zero-knowledge proofs, operates fully on user hardware, and integrates six components in an agentic AI workflow.

Result: The system achieved an 87% success rate, improved latency by 2.4x over cloud alternatives, and increased user trust by 27% due to transparent decision trails.

Conclusion: The system demonstrates a path to secure, efficient, and trustworthy autonomous agents for privacy-sensitive negotiation scenarios.

Abstract: Automated negotiations in insurance and business-to-business (B2B) commerce encounter substantial challenges. Current systems force a trade-off between convenience and privacy by routing sensitive financial data through centralized servers, increasing security risks, and diminishing user trust. This study introduces a device-native autonomous Artificial Intelligence (AI) agent system for privacy-preserving negotiations. The proposed system operates exclusively on user hardware, enabling real-time bargaining while maintaining sensitive constraints locally. It integrates zero-knowledge proofs to ensure privacy and employs distilled world models to support advanced on-device reasoning. The architecture incorporates six technical components within an agentic AI workflow. Agents autonomously plan negotiation strategies, conduct secure multi-party bargaining, and generate cryptographic audit trails without exposing user data to external servers. The system is evaluated in insurance and B2B procurement scenarios across diverse device configurations. Results show an average success rate of 87%, a 2.4x latency improvement over cloud baselines, and strong privacy preservation through zero-knowledge proofs. User studies show 27% higher trust scores when decision trails are available. These findings establish a foundation for trustworthy autonomous agents in privacy-sensitive financial domains.

</details>


### [564] [Emoji-Based Jailbreaking of Large Language Models](https://arxiv.org/abs/2601.00936)
*M P V S Gopinadh,S Mahaboob Hussain*

Main category: cs.CR

TL;DR: This paper examines the vulnerability of large language models (LLMs) to emoji-based jailbreaking, where emojis in prompts bypass safety measures.


<details>
  <summary>Details</summary>
Motivation: To investigate and highlight vulnerabilities in modern LLMs caused by adversarial prompt engineering using emojis, emphasizing the need for improved safety alignment mechanisms.

Method: The authors conducted tests on 50 emoji-based prompts across four open-source LLMs (Mistral 7B, Qwen 2 7B, Gemma 2 9B, and Llama 3 8B), evaluating metrics such as jailbreak success rate, safety alignment adherence, and latency. Statistical analysis was performed using a chi-square test.

Result: The study showed model-specific vulnerabilities with varying jailbreak success rates: Gemma 2 9B and Mistral 7B reached 10% success rates, while Qwen 2 7B demonstrated full alignment (0%). Significant inter-model differences were verified (chi^2 = 32.94, p < 0.001).

Conclusion: LLMs exhibit significant vulnerabilities to emoji-based prompts, with differing success rates among models. These findings highlight the need for robust safety alignment improvements, especially in handling emoji-based representations.

Abstract: Large Language Models (LLMs) are integral to modern AI applications, but their safety alignment mechanisms can be bypassed through adversarial prompt engineering. This study investigates emoji-based jailbreaking, where emoji sequences are embedded in textual prompts to trigger harmful and unethical outputs from LLMs. We evaluated 50 emoji-based prompts on four open-source LLMs: Mistral 7B, Qwen 2 7B, Gemma 2 9B, and Llama 3 8B. Metrics included jailbreak success rate, safety alignment adherence, and latency, with responses categorized as successful, partial and failed. Results revealed model-specific vulnerabilities: Gemma 2 9B and Mistral 7B exhibited 10 % success rates, while Qwen 2 7B achieved full alignment (0% success). A chi-square test (chi^2 = 32.94, p < 0.001) confirmed significant inter-model differences. While prior works focused on emoji attacks targeting safety judges or classifiers, our empirical analysis examines direct prompt-level vulnerabilities in LLMs. The results reveal limitations in safety mechanisms and highlight the necessity for systematic handling of emoji-based representations in prompt-level safety and alignment pipelines.

</details>


### [565] [AI-Powered Hybrid Intrusion Detection Framework for Cloud Security Using Novel Metaheuristic Optimization](https://arxiv.org/abs/2601.01134)
*Maryam Mahdi Alhusseini,Alireza Rouhi,Mohammad-Reza Feizi-Derakhshi*

Main category: cs.CR

TL;DR: The study presents Hybrid Intrusion Detection System (HyIDS) utilizing the Energy Valley Optimizer (EVO) to improve intrusion detection in cloud computing and achieves high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address cybersecurity challenges in cloud computing, especially the issues of skewed datasets and the poor performance of traditional intrusion detection models.

Method: The paper proposes the HyIDS system that integrates the EVO algorithm for feature selection and combines it with four machine learning models: SVM, RF, D_Tree, and KNN. It balances datasets using downsampling techniques.

Result: HyIDS achieved notable accuracy and F1 scores (99.13% accuracy, 98.94% F1 score on CIC-DDoS2019 and 99.78% accuracy, 99.70% F1 score on CSE-CIC-IDS2018), demonstrating improved detection performance.

Conclusion: The integration of EVO and machine learning in HyIDS enhances intrusion detection for cloud computing, effectively addressing class imbalances and improving cybersecurity.

Abstract: Cybersecurity poses considerable problems to Cloud Computing (CC), especially regarding Intrusion Detection Systems (IDSs), facing difficulties with skewed datasets and suboptimal classification model performance. This study presents the Hybrid Intrusion Detection System (HyIDS), an innovative IDS that employs the Energy Valley Optimizer (EVO) for Feature Selection (FS). Additionally, it introduces a novel technique for enhancing the cybersecurity of cloud computing through the integration of machine learning methodologies with the EVO Algorithm. The Energy Valley Optimizer (EVO) effectively diminished features in the CIC-DDoS2019 dataset from 88 to 38 and in the CSE-CIC-IDS2018 data from 80 to 43, significantly enhancing computing efficiency. HyIDS incorporates four Machine Learning (ML) models: Support Vector Machine (SVM), Random Forest (RF), Decision Tree (D_Tree), and K-Nearest Neighbors (KNN). The proposed HyIDS was assessed utilizing two real-world intrusion datasets, CIC-DDoS2019 and CSE-CIC-IDS2018, both distinguished by considerable class imbalances. The CIC-DDoS2019 dataset has a significant imbalance between DDoS assault samples and legal traffic, while the CSE-CIC-IDS2018 dataset primarily comprises benign traffic with insufficient representation of attack types, complicating the detection of minority attacks. A downsampling technique was employed to balance the datasets, hence improving detection efficacy for both benign and malicious traffic. Twenty-four trials were done, revealing substantial enhancements in categorization accuracy, precision, and recall. Our suggested D_TreeEVO model attained an accuracy rate of 99.13% and an F1 score of 98.94% on the CIC-DDoS2019 dataset, and an accuracy rate of 99.78% and an F1 score of 99.70% on the CSE-CIC-IDS2018 data. These data demonstrate that EVO significantly improves cybersecurity in Cloud Computing (CC).

</details>


### [566] [Aggressive Compression Enables LLM Weight Theft](https://arxiv.org/abs/2601.01296)
*Davis Brown,Juan-Pablo Rivera,Dan Hendrycks,Mantas Mazeika*

Main category: cs.CR

TL;DR: The paper highlights the risks of exfiltration attacks on AI model weights, showing how compression methods could speedily aid attackers. It compares defenses, emphasizing the effectiveness of forensic watermarks.


<details>
  <summary>Details</summary>
Motivation: As advanced AIs become more valuable, adversaries are incentivized to steal model weights. This paper seeks to address vulnerabilities related to exfiltration attacks, specifically in transmitting model weights from datacenters.

Method: The authors focus on exfiltration attacks and demonstrate how attackers can exploit compression by relaxing decompression constraints, achieving 16x to 100x compression. They test defenses including compressibility reduction, harder-to-find models, and forensic watermarks.

Result: Attackers can significantly reduce transmission times (months to days) with compression. Defenses were studied, with forensic watermarks emerging as a highly effective and cost-efficient solution.

Conclusion: Forensic watermarks are a practical and affordable defense to mitigate exfiltration risks, underlining their suitability for protecting advanced AI models from theft.

Abstract: As frontier AIs become more powerful and costly to develop, adversaries have increasing incentives to steal model weights by mounting exfiltration attacks. In this work, we consider exfiltration attacks where an adversary attempts to sneak model weights out of a datacenter over a network. While exfiltration attacks are multi-step cyber attacks, we demonstrate that a single factor, the compressibility of model weights, significantly heightens exfiltration risk for large language models (LLMs). We tailor compression specifically for exfiltration by relaxing decompression constraints and demonstrate that attackers could achieve 16x to 100x compression with minimal trade-offs, reducing the time it would take for an attacker to illicitly transmit model weights from the defender's server from months to days. Finally, we study defenses designed to reduce exfiltration risk in three distinct ways: making models harder to compress, making them harder to 'find,' and tracking provenance for post-attack analysis using forensic watermarks. While all defenses are promising, the forensic watermark defense is both effective and cheap, and therefore is a particularly attractive lever for mitigating weight-exfiltration risk.

</details>


### [567] [Towards eco friendly cybersecurity: machine learning based anomaly detection with carbon and energy metrics](https://arxiv.org/abs/2601.00893)
*KC Aashish,Md Zakir Hossain Zamil,Md Shafiqul Islam Mridul,Lamia Akter,Farmina Sharmin,Eftekhar Hossain Ayon,Md Maruf Bin Reza,Ali Hassan,Abdur Rahim,Sirapa Malla*

Main category: cs.CR

TL;DR: This study introduces an environmentally aware anomaly detection framework that integrates ML-based network monitoring with carbon and energy tracking, emphasizing eco-efficient approaches.


<details>
  <summary>Details</summary>
Motivation: To address the environmental cost of artificial intelligence in cybersecurity, which is rarely considered despite its rising energy footprint and emissions.

Method: Benchmarked various ML models on energy, carbon, and performance using the Carbon Aware Cybersecurity Traffic Dataset and CodeCarbon toolkit. Developed an Eco Efficiency Index to measure performance per kilowatt-hour.

Result: Random Forest and Logistic Regression models emerged as the most eco-efficient, cutting energy usage by 40% while maintaining strong detection accuracy. Principal Component Analysis reduced computational load with minimal recall loss.

Conclusion: Incorporating carbon and energy metrics into cybersecurity workflows enables sustainable and environmentally responsible machine learning without losing operational effectiveness.

Abstract: The rising energy footprint of artificial intelligence has become a measurable component of US data center emissions, yet cybersecurity research seldom considers its environmental cost. This study introduces an eco aware anomaly detection framework that unifies machine learning based network monitoring with real time carbon and energy tracking. Using the publicly available Carbon Aware Cybersecurity Traffic Dataset comprising 2300 flow level observations, we benchmark Logistic Regression, Random Forest, Support Vector Machine, Isolation Forest, and XGBoost models across energy, carbon, and performance dimensions. Each experiment is executed in a controlled Colab environment instrumented with the CodeCarbon toolkit to quantify power draw and equivalent CO2 output during both training and inference. We construct an Eco Efficiency Index that expresses F1 score per kilowatt hour to capture the trade off between detection quality and environmental impact. Results reveal that optimized Random Forest and lightweight Logistic Regression models achieve the highest eco efficiency, reducing energy consumption by more than forty percent compared to XGBoost while sustaining competitive detection accuracy. Principal Component Analysis further decreases computational load with negligible loss in recall. Collectively, these findings establish that integrating carbon and energy metrics into cybersecurity workflows enables environmentally responsible machine learning without compromising operational protection. The proposed framework offers a reproducible path toward sustainable carbon accountable cybersecurity aligned with emerging US green computing and federal energy efficiency initiatives.

</details>


### [568] [Noise-Aware and Dynamically Adaptive Federated Defense Framework for SAR Image Target Recognition](https://arxiv.org/abs/2601.00900)
*Yuchao Hou,Zixuan Zhang,Jie Wang,Wenke Huang,Lianhui Liang,Di Wu,Zhiquan Liu,Youliang Tian,Jianming Zhu,Jisheng Dang,Junhao Dong,Zhongliang Guo*

Main category: cs.CR

TL;DR: The paper proposes NADAFD, a novel federated defense framework specifically tailored for SAR image target recognition, addressing backdoor security concerns in federated learning.


<details>
  <summary>Details</summary>
Motivation: While deep learning enhances SAR image target recognition, it relies on centralized training, creating privacy and security issues. Federated learning (FL) offers privacy-preserving collaboration but is threatened by malicious backdoor attacks that exploit SAR's speckle noise.

Method: The authors introduced NADAFD, which combines frequency-domain collaborative inversion, noise-aware adversarial training, and a dynamic health assessment module. These strategies analyze spectral inconsistencies, generate speckle-noise-robust adversarial samples, and adaptively manage client contributions to defend against backdoor attacks.

Result: Experiments on the MSTAR and OpenSARShip datasets demonstrate that NADAFD leads to higher accuracy on clean SAR test samples and lowers the success rate of backdoor attacks compared to existing methods.

Conclusion: NADAFD effectively handles SAR backdoor threats in federated learning by integrating multi-domain defenses and adapting to client behaviors, improving both security and model performance.

Abstract: As a critical application of computational intelligence in remote sensing, deep learning-based synthetic aperture radar (SAR) image target recognition facilitates intelligent perception but typically relies on centralized training, where multi-source SAR data are uploaded to a single server, raising privacy and security concerns. Federated learning (FL) provides an emerging computational intelligence paradigm for SAR image target recognition, enabling cross-site collaboration while preserving local data privacy. However, FL confronts critical security risks, where malicious clients can exploit SAR's multiplicative speckle noise to conceal backdoor triggers, severely challenging the robustness of the computational intelligence model. To address this challenge, we propose NADAFD, a noise-aware and dynamically adaptive federated defense framework that integrates frequency-domain, spatial-domain, and client-behavior analyses to counter SAR-specific backdoor threats. Specifically, we introduce a frequency-domain collaborative inversion mechanism to expose cross-client spectral inconsistencies indicative of hidden backdoor triggers. We further design a noise-aware adversarial training strategy that embeds $Γ$-distributed speckle characteristics into mask-guided adversarial sample generation to enhance robustness against both backdoor attacks and SAR speckle noise. In addition, we present a dynamic health assessment module that tracks client update behaviors across training rounds and adaptively adjusts aggregation weights to mitigate evolving malicious contributions. Experiments on MSTAR and OpenSARShip datasets demonstrate that NADAFD achieves higher accuracy on clean test samples and a lower backdoor attack success rate on triggered inputs than existing federated backdoor defenses for SAR target recognition.

</details>


### [569] [Security Hardening Using FABRIC: Implementing a Unified Compliance Aggregator for Linux Servers](https://arxiv.org/abs/2601.00909)
*Sheldon Paul,Izzat Alsmadi*

Main category: cs.CR

TL;DR: This paper introduces a Unified Compliance Aggregator (UCA) for evaluating Linux security hardening across varying levels, improving compliance scores significantly.


<details>
  <summary>Details</summary>
Motivation: The lack of consistent interpretation across Linux security audit tools makes evaluating hardening effectiveness challenging, motivating the need for a unified framework.

Method: The authors deployed three Ubuntu nodes at different hardening levels and assessed them using Lynis, OpenSCAP, and AIDE tools. They built the Unified Compliance Aggregator (UCA) to normalize diverse tool outputs and aggregate compliance metrics.

Result: Full hardening increased OpenSCAP compliance scores from 39.7 to 71.8, and custom rule compliance improved from 39.3% to 83.6% under the UCA framework.

Conclusion: UCA enables a reproducible and clearer evaluation of security postures compared to individual tools, enhancing systematic assessments of Linux hardening in testbed environments.

Abstract: This paper presents a unified framework for evaluating Linux security hardening on the FABRIC testbed through aggregation of heterogeneous security auditing tools. We deploy three Ubuntu 22.04 nodes configured at baseline, partial, and full hardening levels, and evaluate them using Lynis, OpenSCAP, and AIDE across 108 audit runs. To address the lack of a consistent interpretation across tools, we implement a Unified Compliance Aggregator (UCA) that parses tool outputs, normalizes scores to a common 0--100 scale, and combines them into a weighted metric augmented by a customizable rule engine for organization-specific security policies. Experimental results show that full hardening increases OpenSCAP compliance from 39.7 to 71.8, while custom rule compliance improves from 39.3\% to 83.6\%. The results demonstrate that UCA provides a clearer and more reproducible assessment of security posture than individual tools alone, enabling systematic evaluation of hardening effectiveness in programmable testbed environments.

</details>


### [570] [Byzantine-Robust Federated Learning Framework with Post-Quantum Secure Aggregation for Real-Time Threat Intelligence Sharing in Critical IoT Infrastructure](https://arxiv.org/abs/2601.01053)
*Milad Rahmati,Nima Rahmati*

Main category: cs.CR

TL;DR: The paper proposes a secure federated learning framework for IoT devices to address security vulnerabilities caused by Byzantine attacks and future quantum threats.


<details>
  <summary>Details</summary>
Motivation: IoT's critical infrastructure faces cybersecurity challenges due to vulnerabilities in traditional federated learning approaches, making robust and privacy-preserving solutions essential.

Method: A Byzantine-robust federated learning framework is designed utilizing an adaptive weighted aggregation mechanism, reputation-based client selection, and post-quantum cryptographic protocols like CRYSTALS-Kyber.

Result: Achieved 96.8% threat detection accuracy, mitigated up to 40% Byzantine attackers, with only 18% computational overhead while ensuring real-time performance and 256-bit post-quantum security.

Conclusion: The proposed framework effectively tackles IoT's cybersecurity challenges with robustness against both Byzantine and quantum threats, offering high detection accuracy and efficient performance.

Abstract: The proliferation of Internet of Things devices in critical infrastructure has created unprecedented cybersecurity challenges, necessitating collaborative threat detection mechanisms that preserve data privacy while maintaining robustness against sophisticated attacks. Traditional federated learning approaches for IoT security suffer from two critical vulnerabilities: susceptibility to Byzantine attacks where malicious participants poison model updates, and inadequacy against future quantum computing threats that can compromise cryptographic aggregation protocols. This paper presents a novel Byzantine-robust federated learning framework integrated with post-quantum secure aggregation specifically designed for real-time threat intelligence sharing across critical IoT infrastructure. The proposed framework combines a adaptive weighted aggregation mechanism with lattice-based cryptographic protocols to simultaneously defend against model poisoning attacks and quantum adversaries. We introduce a reputation-based client selection algorithm that dynamically identifies and excludes Byzantine participants while maintaining differential privacy guarantees. The secure aggregation protocol employs CRYSTALS-Kyber for key encapsulation and homomorphic encryption to ensure confidentiality during parameter updates. Experimental evaluation on industrial IoT intrusion detection datasets demonstrates that our framework achieves 96.8% threat detection accuracy while successfully mitigating up to 40% Byzantine attackers, with only 18% computational overhead compared to non-secure federated approaches. The framework maintains sub-second aggregation latency suitable for real-time applications and provides 256-bit post-quantum security level.

</details>


### [571] [Exposing Hidden Interfaces: LLM-Guided Type Inference for Reverse Engineering macOS Private Frameworks](https://arxiv.org/abs/2601.01673)
*Arina Kharlamova,Youcheng Sun,Ting Yu*

Main category: cs.CR

TL;DR: The paper introduces MOTIF, a framework that combines tools and a finetuned large language model to recover headers from undocumented macOS private frameworks, achieving a significant improvement in analysis and security research.


<details>
  <summary>Details</summary>
Motivation: Private macOS frameworks are undocumented and distributed as stripped binaries, making security analysis difficult. The paper aims to provide a scalable solution for analyzing these opaque binaries.

Method: MOTIF combines runtime metadata extraction, binary inspection, and constraint checking managed by an agent, with an Objective-C type-inference model to reconstruct function signatures and generate accurate headers.

Result: MOTIF improves header signature recovery from 15% to 86% compared to static analysis tools. Headers reconstructed by MOTIF are verifiable and usable for security research and vulnerability analysis.

Conclusion: MOTIF offers a robust and scalable approach for transforming opaque macOS binaries into analyzable structures, enabling systematic auditing and further research on macOS internals.

Abstract: Private macOS frameworks underpin critical services and daemons but remain undocumented and distributed only as stripped binaries, complicating security analysis. We present MOTIF, an agentic framework that integrates tool-augmented analysis with a finetuned large language model specialized for Objective-C type inference. The agent manages runtime metadata extraction, binary inspection, and constraint checking, while the model generates candidate method signatures that are validated and refined into compilable headers. On MOTIF-Bench, a benchmark built from public frameworks with groundtruth headers, MOTIF improves signature recovery from 15% to 86% compared to baseline static analysis tooling, with consistent gains in tool-use correctness and inference stability. Case studies on private frameworks show that reconstructed headers compile, link, and facilitate downstream security research and vulnerability studies. By transforming opaque binaries into analyzable interfaces, MOTIF establishes a scalable foundation for systematic auditing of macOS internals.

</details>


### [572] [Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization](https://arxiv.org/abs/2601.01747)
*Jiwei Guan,Haibo Jin,Haohan Wang*

Main category: cs.CR

TL;DR: This paper introduces a black-box jailbreak attack on Large Vision-Language Models (LVLMs) using Zeroth-Order optimization via Simultaneous Perturbation Stochastic Approximation (ZO-SPSA), achieving high success rates and exposing vulnerabilities.


<details>
  <summary>Details</summary>
Motivation: Address the inadequacy of existing white-box attacks targeting LVLMs, which are computationally expensive, lack adversarial transferability, and are impractical for black-box scenarios.

Method: Developed a black-box attack strategy using ZO-SPSA, which avoids requiring knowledge of model internals, utilizes gradient-free optimization, and consumes fewer computational resources.

Result: Demonstrated an 83.0% jailbreak success rate on InstructBLIP, strong adversarial transferability with 64.18% ASR between models, and the attack's imperceptibility close to white-box methods.

Conclusion: ZO-SPSA enables efficient and effective black-box jailbreak attacks on LVLMs, revealing critical vulnerabilities in their safety mechanisms applicable to real-world scenarios.

Abstract: Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs

</details>


### [573] [OpenRT: An Open-Source Red Teaming Framework for Multimodal LLMs](https://arxiv.org/abs/2601.01592)
*Xin Wang,Yunhao Chen,Juncheng Li,Yixu Wang,Yang Yao,Tianle Gu,Jie Li,Yan Teng,Xingjun Ma,Yingchun Wang,Xia Hu*

Main category: cs.CR

TL;DR: The paper introduces OpenRT, a framework for evaluating safety in Multimodal Large Language Models (MLLMs) through modular and scalable adversarial testing, revealing vulnerabilities in top models.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLM safety are fragmented, single-turn focused, and lack scalability, creating a need for a unified, comprehensive red-teaming framework.

Method: The paper proposes OpenRT, a framework featuring a modular adversarial kernel with five components (model integration, dataset management, attack strategies, judging methods, and evaluation metrics), alongside 37 diverse attack methodologies.

Result: The study tested 20 advanced MLLMs and found significant vulnerabilities, with average Attack Success Rates up to 49.14% and poor generalization against complex, multi-turn scenarios.

Conclusion: OpenRT provides a critical tool for improving and standardizing AI safety by offering open-source, extensible infrastructure for evaluating MLLM vulnerabilities.

Abstract: The rapid integration of Multimodal Large Language Models (MLLMs) into critical applications is increasingly hindered by persistent safety vulnerabilities. However, existing red-teaming benchmarks are often fragmented, limited to single-turn text interactions, and lack the scalability required for systematic evaluation. To address this, we introduce OpenRT, a unified, modular, and high-throughput red-teaming framework designed for comprehensive MLLM safety evaluation. At its core, OpenRT architects a paradigm shift in automated red-teaming by introducing an adversarial kernel that enables modular separation across five critical dimensions: model integration, dataset management, attack strategies, judging methods, and evaluation metrics. By standardizing attack interfaces, it decouples adversarial logic from a high-throughput asynchronous runtime, enabling systematic scaling across diverse models. Our framework integrates 37 diverse attack methodologies, spanning white-box gradients, multi-modal perturbations, and sophisticated multi-agent evolutionary strategies. Through an extensive empirical study on 20 advanced models (including GPT-5.2, Claude 4.5, and Gemini 3 Pro), we expose critical safety gaps: even frontier models fail to generalize across attack paradigms, with leading models exhibiting average Attack Success Rates as high as 49.14%. Notably, our findings reveal that reasoning models do not inherently possess superior robustness against complex, multi-turn jailbreaks. By open-sourcing OpenRT, we provide a sustainable, extensible, and continuously maintained infrastructure that accelerates the development and standardization of AI safety.

</details>


### [574] [Improved Accuracy for Private Continual Cardinality Estimation in Fully Dynamic Streams via Matrix Factorization](https://arxiv.org/abs/2601.02257)
*Joel Daniel Andersson,Palak Jain,Satchit Sivakumar*

Main category: cs.CR

TL;DR: The paper develops accurate methods for differentially-private statistics in dynamic streaming settings by improving error bounds in cardinality estimation problems like counting distinct elements and triangle counts.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in applying private continual counting algorithms in dynamic streams where updates cause substantial changes, impacting accuracy in cardinality estimation problems.

Method: They analyze $\ell_p$-sensitivity vectors in difference streams, leveraging properties of sensitivity vectors and advanced factorizations of counting matrices to improve accuracy.

Result: Their framework achieves improved error bounds and accuracy in counting distinct elements, degree histograms, and triangle counts, validated both empirically and analytically.

Conclusion: The paper presents a general and improved approach to private continual cardinality estimation, enhancing the accuracy of dynamic stream statistics within a differentially-private framework.

Abstract: We study differentially-private statistics in the fully dynamic continual observation model, where many updates can arrive at each time step and updates to a stream can involve both insertions and deletions of an item. Earlier work (e.g., Jain et al., NeurIPS 2023 for counting distinct elements; Raskhodnikova & Steiner, PODS 2025 for triangle counting with edge updates) reduced the respective cardinality estimation problem to continual counting on the difference stream associated with the true function values on the input stream. In such reductions, a change in the original stream can cause many changes in the difference stream, this poses a challenge for applying private continual counting algorithms to obtain optimal error bounds. We improve the accuracy of several such reductions by studying the associated $\ell_p$-sensitivity vectors of the resulting difference streams and isolating their properties.
  We demonstrate that our framework gives improved bounds for counting distinct elements, estimating degree histograms, and estimating triangle counts (under a slightly relaxed privacy model), thus offering a general approach to private continual cardinality estimation in streaming settings. Our improved accuracy stems from tight analysis of known factorization mechanisms for the counting matrix in this setting; the key technical challenge is arguing that one can use state-of-the-art factorizations for sensitivity vector sets with the properties we isolate. Empirically and analytically, we demonstrate that our improved error bounds offer a substantial improvement in accuracy for cardinality estimation problems over a large range of parameters.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [575] [Measuring Social Media Polarization Using Large Language Models and Heuristic Rules](https://arxiv.org/abs/2601.00927)
*Jawad Chowdhury,Rezaur Rashid,Gabriel Terejanu*

Main category: cs.SI

TL;DR: This paper introduces a framework using large language models and domain-informed heuristics to analyze affective polarization in online discussions, focusing on divisive events.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance understanding of affective polarization in social media discussions and its societal implications, particularly around divisive topics.

Method: A framework that leverages LLMs for stance, tone, and interaction analysis, coupled with a scoring system based on stance alignment, emotional content, and dynamics, to measure polarization.

Result: The study identifies two patterns of affective polarization: anticipation-driven polarization before major events and reactive polarization after impactful events.

Conclusion: The framework provides a scalable, interpretable, and systematic method to measure affective polarization using advanced AI tools. The code is publicly available for further application.

Abstract: Understanding affective polarization in online discourse is crucial for evaluating the societal impact of social media interactions. This study presents a novel framework that leverages large language models (LLMs) and domain-informed heuristics to systematically analyze and quantify affective polarization in discussions on divisive topics such as climate change and gun control. Unlike most prior approaches that relied on sentiment analysis or predefined classifiers, our method integrates LLMs to extract stance, affective tone, and agreement patterns from large-scale social media discussions. We then apply a rule-based scoring system capable of quantifying affective polarization even in small conversations consisting of single interactions, based on stance alignment, emotional content, and interaction dynamics. Our analysis reveals distinct polarization patterns that are event dependent: (i) anticipation-driven polarization, where extreme polarization escalates before well-publicized events, and (ii) reactive polarization, where intense affective polarization spikes immediately after sudden, high-impact events. By combining AI-driven content annotation with domain-informed scoring, our framework offers a scalable and interpretable approach to measuring affective polarization. The source code is publicly available at: https://github.com/hasanjawad001/llm-social-media-polarization.

</details>


### [576] [Gendered Pathways in AI Companionship: Cross-Community Behavior and Toxicity Patterns on Reddit](https://arxiv.org/abs/2601.01073)
*Erica Coppolillo,Emilio Ferrara*

Main category: cs.SI

TL;DR: This paper explores the gendered interactions and risks associated with AI-companionship communities on Reddit, focusing on user behavior, emotional expression, and toxicity within the MyBoyfriendIsAI (MBIA) subreddit ecosystem.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand how people, particularly across different genders, form relationships with AI companions and how this intersects with gendered online behaviors and harmful content.

Method: Researchers analyzed activity histories of over 3,000 highly engaged MBIA subreddit users and created a network across 2,000 related subreddits to examine interaction pathways, emotional expression, and toxicity.

Result: Findings indicate that users participate in four main community spheres with significant female engagement. While toxicity is low overall, distinct gendered pathways show spikes in toxicity, particularly in AI-porn and gender-oriented subreddits. Gender-focused trajectories reveal different emotional and toxicity patterns.

Conclusion: The study reveals gendered dynamics and highlights specific pathways where risks and toxicity concentrate, offering insights for improving measurement, moderation, and platform design in AI-companionship ecosystems.

Abstract: AI-companionship platforms are rapidly reshaping how people form emotional, romantic, and parasocial bonds with non-human agents, raising new questions about how these relationships intersect with gendered online behavior and exposure to harmful content. Focusing on the MyBoyfriendIsAI (MBIA) subreddit, we reconstruct the Reddit activity histories of more than 3,000 highly engaged users over two years, yielding over 67,000 historical submissions. We then situate MBIA within a broader ecosystem by building a historical interaction network spanning more than 2,000 subreddits, which enables us to trace cross-community pathways and measure how toxicity and emotional expression vary across these trajectories. We find that MBIA users primarily traverse four surrounding community spheres (AI-companionship, porn-related, forum-like, and gaming) and that participation across the ecosystem exhibits a distinct gendered structure, with substantial engagement by female users. While toxicity is generally low across most pathways, we observe localized spikes concentrated in a small subset of AI-porn and gender-oriented communities. Nearly 16% of users engage with gender-focused subreddits, and their trajectories display systematically different patterns of emotional expression and elevated toxicity, suggesting that a minority of gendered pathways may act as toxicity amplifiers within the broader AI-companionship ecosystem. These results characterize the gendered structure of cross-community participation around AI companionship on Reddit and highlight where risks concentrate, informing measurement, moderation, and design practices for human-AI relationship platforms.

</details>


### [577] [Beyond Homophily: Community Search on Heterophilic Graphs](https://arxiv.org/abs/2601.01703)
*Qing Sima,Xiaoyang Wang,Wenjie Zhang*

Main category: cs.SI

TL;DR: AdaptCS is a framework addressing community search challenges in heterophilic graphs by combining signal disentangling, scalability in computation, and adaptive scoring.


<details>
  <summary>Details</summary>
Motivation: Existing methods, both classical and ML-based, fail to effectively perform community search on heterophilic graphs where edge semantics are unclear.

Method: AdaptCS incorporates an encoder to process multi-frequency signals, a scalable low-rank optimization, and adaptive scoring for online community refinement.

Result: AdaptCS surpasses baselines in F1-score by an average of 11%, maintains robustness across various heterophily levels, and offers a significant speedup in computation.

Conclusion: AdaptCS is a robust and efficient solution for community search, accommodating both homophilic and heterophilic graph characteristics with improved performance and scalability.

Abstract: Community search aims to identify a refined set of nodes that are most relevant to a given query, supporting tasks ranging from fraud detection to recommendation. Unlike homophilic graphs, many real-world networks are heterophilic, where edges predominantly connect dissimilar nodes. Therefore, structural signals that once reflected smooth, low-frequency similarity now appear as sharp, high-frequency contrasts. However, both classical algorithms (e.g., k-core, k-truss) and recent ML-based models struggle to achieve effective community search on heterophilic graphs, where edge signs or semantics are generally unknown. Algorithm-based methods often return communities with mixed class labels, while GNNs, built on homophily, smooth away meaningful signals and blur community boundaries. Therefore, we propose Adaptive Community Search (AdaptCS), a unified framework featuring three key designs: (i) an AdaptCS Encoder that disentangles multi-hop and multi-frequency signals, enabling the model to capture both smooth (homophilic) and contrastive (heterophilic) relations; (ii) a memory-efficient low-rank optimization that removes the main computational bottleneck and ensures model scalability; and (iii) an Adaptive Community Score (ACS) that guides online search by balancing embedding similarity and topological relations. Extensive experiments on both heterophilic and homophilic benchmarks demonstrate that AdaptCS outperforms the best-performing baseline by an average of 11% in F1-score, retains robustness across heterophily levels, and achieves up to 2 orders of magnitude speedup.

</details>


### [578] [Inferring Network Evolutionary History via Structure-State Coupled Learning](https://arxiv.org/abs/2601.02121)
*En Xu,Shihe Zhou,Huandong Wang,Jingtao Ding,Yong Li*

Main category: cs.SI

TL;DR: The paper introduces CS$^2$, which utilizes both network topology and steady-state dynamics to infer a network's evolution history more effectively than existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of inferring network evolution history from limited data, particularly when topology-based methods face inadequacy and noise.

Method: The CS$^2$ framework explicitly captures the interaction between topology and steady-state dynamics, modeling their interplay to improve edge formation order recovery.

Result: CS$^2$ outperforms baseline methods, achieving a 4.0% improvement in pairwise edge precedence accuracy and a 7.7% enhancement in global ordering consistency on real temporal networks.

Conclusion: Steady-state dynamics serve as a significant independent signal for network evolution inference, and their integration with topology enhances overall performance in reconstructing evolution trajectories.

Abstract: Inferring a network's evolutionary history from a single final snapshot with limited temporal annotations is fundamental yet challenging. Existing approaches predominantly rely on topology alone, which often provides insufficient and noisy cues. This paper leverages network steady-state dynamics -- converged node states under a given dynamical process -- as an additional and widely accessible observation for network evolution history inference. We propose CS$^2$, which explicitly models structure-state coupling to capture how topology modulates steady states and how the two signals jointly improve edge discrimination for formation-order recovery. Experiments on six real temporal networks, evaluated under multiple dynamical processes, show that CS$^2$ consistently outperforms strong baselines, improving pairwise edge precedence accuracy by 4.0% on average and global ordering consistency (Spearman-$ρ$) by 7.7% on average. CS$^2$ also more faithfully recovers macroscopic evolution trajectories such as clustering formation, degree heterogeneity, and hub growth. Moreover, a steady-state-only variant remains competitive when reliable topology is limited, highlighting steady states as an independent signal for evolution inference.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [579] [Speak the Art: A Direct Speech to Image Generation Framework](https://arxiv.org/abs/2601.00827)
*Mariam Saeed,Manar Amr,Farida Adel,Nada Hassan,Nour Walid,Eman Mohamed,Mohamed Hussein,Marwan Torki*

Main category: eess.AS

TL;DR: This paper introduces the 'Speak the Art (STA)' framework designed to generate images directly from speech, achieving significant improvements over existing models.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to direct speech-to-image generation face challenges such as insufficient linguistic representation in speech embeddings and GAN-related issues like instability and limited diversity.

Method: The STA framework uses a speech encoding network supervised by a large pre-trained image-text model, combined with a VQ-Diffusion network to replace GANs for more stable and diverse image generation. It also explores multilingual capabilities.

Result: The STA framework surpasses the performance of current state-of-the-art speech-to-image models and demonstrates effectiveness in both English and Arabic input.

Conclusion: The proposed STA framework improves speech-to-image generation by leveraging enhanced speech embeddings and diffusion models, providing both stability and diversity in image generation.

Abstract: Direct speech-to-image generation has recently shown promising results. However, compared to text-to-image generation, there is still a large gap to enclose. Current approaches use two stages to tackle this task: speech encoding network and image generative adversarial network (GAN). The speech encoding networks in these approaches produce embeddings that do not capture sufficient linguistic information to semantically represent the input speech. GANs suffer from issues such as non-convergence, mode collapse, and diminished gradient, which result in unstable model parameters, limited sample diversity, and ineffective generator learning, respectively. To address these weaknesses, we introduce a framework called \textbf{Speak the Art (STA)} which consists of a speech encoding network and a VQ-Diffusion network conditioned on speech embeddings. To improve speech embeddings, the speech encoding network is supervised by a large pre-trained image-text model during training. Replacing GANs with diffusion leads to more stable training and the generation of diverse images. Additionally, we investigate the feasibility of extending our framework to be multilingual. As a proof of concept, we trained our framework with two languages: English and Arabic. Finally, we show that our results surpass state-of-the-art models by a large margin.

</details>


### [580] [Improving Code-Switching Speech Recognition with TTS Data Augmentation](https://arxiv.org/abs/2601.00935)
*Yue Heng Yeo,Yuchen Hu,Shreyas Gopal,Yizhou Peng,Hexin Liu,Eng Siong Chng*

Main category: eess.AS

TL;DR: The paper uses a multilingual TTS model for data augmentation to improve ASR performance in code-switching scenarios, achieving consistent MER reductions.


<details>
  <summary>Details</summary>
Motivation: Conversational code-switching ASR faces challenges due to limited high-quality labeled data, necessitating innovative solutions to enhance robustness and accuracy.

Method: The authors fine-tuned the multilingual CosyVoice2 TTS model on the SEAME dataset to generate synthetic Chinese-English code-switching speech for data augmentation in ASR training.

Result: Augmenting real speech with synthetic data reduced MER from 12.1% to 10.1% on DevMan and from 17.8% to 16.0% on DevSGE, achieving consistent gains.

Conclusion: Multilingual TTS proves to be an effective tool for improving ASR robustness, particularly in low-resource conversational code-switching scenarios.

Abstract: Automatic speech recognition (ASR) for conversational code-switching speech remains challenging due to the scarcity of realistic, high-quality labeled speech data. This paper explores multilingual text-to-speech (TTS) models as an effective data augmentation technique to address this shortage. Specifically, we fine-tune the multilingual CosyVoice2 TTS model on the SEAME dataset to generate synthetic conversational Chinese-English code-switching speech, significantly increasing the quantity and speaker diversity of available training data. Our experiments demonstrate that augmenting real speech with synthetic speech reduces the mixed error rate (MER) from 12.1 percent to 10.1 percent on DevMan and from 17.8 percent to 16.0 percent on DevSGE, indicating consistent performance gains. These results confirm that multilingual TTS is an effective and practical tool for enhancing ASR robustness in low-resource conversational code-switching scenarios.

</details>


### [581] [Bayesian Negative Binomial Regression of Afrobeats Chart Persistence](https://arxiv.org/abs/2601.01391)
*Ian Jacob Cabansag,Paul Ntegeka*

Main category: eess.AS

TL;DR: The study explores how collaborations affect Afrobeats songs' duration on Nigerian Spotify Top 200 charts, finding that collaborative tracks spend fewer days on the chart compared to solo tracks.


<details>
  <summary>Details</summary>
Motivation: To understand whether collaborations help Afrobeats songs sustain chart visibility, thereby influencing revenue and cultural impact.

Method: The paper uses Bayesian negative binomial regression on 2024 Nigeria Spotify Top 200 data, analyzing days on the chart and total annual streams as variables.

Result: Collaborative tracks, when adjusted for total streams, spend slightly fewer days on the chart compared to solo tracks.

Conclusion: Collaborations might not extend the chart lifespan of Afrobeats songs, despite their popularity potentially reflected in total streams.

Abstract: Afrobeats songs compete for attention on streaming platforms, where chart visibility can influence both revenue and cultural impact. This paper examines whether collaborations help songs remain on the charts longer, using daily Nigeria Spotify Top 200 data from 2024. Each track is summarized by the number of days it appears in the Top 200 during the year and its total annual streams in Nigeria. A Bayesian negative binomial regression is applied, with days on chart as the outcome and collaboration status (solo versus multi-artist) and log total streams as predictors. This approach is well suited for overdispersed count data and allows the effect of collaboration to be interpreted while controlling for overall popularity. Posterior inference is conducted using Markov chain Monte Carlo, and results are assessed using rate ratios, posterior probabilities, and predictive checks. The findings indicate that, after accounting for total streams, collaboration tracks tend to spend slightly fewer days on the chart than comparable solo tracks.

</details>


### [582] [MORE: Multi-Objective Adversarial Attacks on Speech Recognition](https://arxiv.org/abs/2601.01852)
*Xiaoxue Gao,Zexin Li,Yiming Chen,Nancy F. Chen*

Main category: eess.AS

TL;DR: This paper introduces the MORE attack to compromise both recognition accuracy and inference efficiency in automatic speech recognition (ASR) models like Whisper.


<details>
  <summary>Details</summary>
Motivation: A need to ensure ASR model robustness against minor input perturbations beyond just accuracy, as real-world efficiency vulnerabilities remain unexplored.

Method: The multi-objective MORE attack uses a hierarchical staged repulsion-anchoring mechanism and introduces a REDO objective for duplicative text generation to simultaneously degrade ASR performance and efficiency.

Result: MORE produces longer incorrect transcriptions and higher computational costs compared to existing adversarial attack methods.

Conclusion: The MORE attack effectively exposes multi-objective vulnerabilities in ASR models, illustrating potential risks for real-world applications.

Abstract: The emergence of large-scale automatic speech recognition (ASR) models such as Whisper has greatly expanded their adoption across diverse real-world applications. Ensuring robustness against even minor input perturbations is therefore critical for maintaining reliable performance in real-time environments. While prior work has mainly examined accuracy degradation under adversarial attacks, robustness with respect to efficiency remains largely unexplored. This narrow focus provides only a partial understanding of ASR model vulnerabilities. To address this gap, we conduct a comprehensive study of ASR robustness under multiple attack scenarios. We introduce MORE, a multi-objective repetitive doubling encouragement attack, which jointly degrades recognition accuracy and inference efficiency through a hierarchical staged repulsion-anchoring mechanism. Specifically, we reformulate multi-objective adversarial optimization into a hierarchical framework that sequentially achieves the dual objectives. To further amplify effectiveness, we propose a novel repetitive encouragement doubling objective (REDO) that induces duplicative text generation by maintaining accuracy degradation and periodically doubling the predicted sequence length. Overall, MORE compels ASR models to produce incorrect transcriptions at a substantially higher computational cost, triggered by a single adversarial input. Experiments show that MORE consistently yields significantly longer transcriptions while maintaining high word error rates compared to existing baselines, underscoring its effectiveness in multi-objective adversarial attack.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [583] [Generating Diverse TSP Tours via a Combination of Graph Pointer Network and Dispersion](https://arxiv.org/abs/2601.01132)
*Hao-Hsung Yang,Ssu-Yuan Lo,Kuan-Lun Chen,Ching-Kai Wang*

Main category: cs.CG

TL;DR: The paper introduces a hybrid approach for the Diverse Traveling Salesman Problem (D-TSP), achieving efficient, high-quality, and diverse solutions while being significantly faster than existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in existing methods for D-TSP, which require high-quality and diverse solutions but currently suffer from high computational complexity or limited diversity quality. This has crucial implications for applications like logistics, robotics, and patrolling.

Method: The proposed method involves a two-step hybrid framework: (1) A Graph Pointer Network (GPN) with sequence entropy loss is used to generate diverse solutions efficiently. (2) A greedy algorithm selects $k$ diverse tours with a 2-approximation on diversity.

Result: The proposed method achieves state-of-the-art diversity (Jaccard index of $0.015$ compared to $0.081$ from prior methods), near-linear runtime growth with GPU acceleration, and is over 360 times faster on large-scale instances.

Conclusion: This approach provides a simple, efficient, yet powerful framework for solving D-TSP, balancing solution quality and diversity while drastically reducing computational overhead.

Abstract: We address the Diverse Traveling Salesman Problem (D-TSP), a bi-criteria optimization challenge that seeks a set of $k$ distinct TSP tours. The objective requires every selected tour to have a length at most $c|T^*|$ (where $|T^*|$ is the optimal tour length) while minimizing the average Jaccard similarity across all tour pairs. This formulation is crucial for applications requiring both high solution quality and fault tolerance, such as logistics planning, robotics pathfinding or strategic patrolling. Current methods are limited: traditional heuristics, such as the Niching Memetic Algorithm (NMA) or bi-criteria optimization, incur high computational complexity $O(n^3)$, while modern neural approaches (e.g., RF-MA3S) achieve limited diversity quality and rely on complex, external mechanisms.
  To overcome these limitations, we propose a novel hybrid framework that decomposes D-TSP into two efficient steps. First, we utilize a simple Graph Pointer Network (GPN), augmented with an approximated sequence entropy loss, to efficiently sample a large, diverse pool of high-quality tours. This simple modification effectively controls the quality-diversity trade-off without complex external mechanisms. Second, we apply a greedy algorithm that yields a 2-approximation for the dispersion problem to select the final $k$ maximally diverse tours from the generated pool. Our results demonstrate state-of-the-art performance. On the Berlin instance, our model achieves an average Jaccard index of $0.015$, significantly outperforming NMA ($0.081$) and RF-MA3S. By leveraging GPU acceleration, our GPN structure achieves a near-linear empirical runtime growth of $O(n)$. While maintaining solution diversity comparable to complex bi-criteria algorithms, our approach is over 360 times faster on large-scale instances (783 cities), delivering high-quality TSP solutions with unprecedented efficiency and simplicity.

</details>


### [584] [Efficient Cover Construction for Ball Mapper via Accelerated Range Queries](https://arxiv.org/abs/2601.01405)
*Jay-Anne Bulauan,John Rick Manzanares*

Main category: cs.CG

TL;DR: The paper focuses on improving the efficiency of the Ball Mapper tool by speeding up range queries through advanced techniques, making it more scalable for large and high-dimensional data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address computational inefficiencies in the Ball Mapper tool, specifically during the cover construction stage, which becomes a bottleneck for large-scale and high-dimensional data.

Method: The authors introduce two enhancements: hierarchical geometric pruning using ball trees and hardware-aware distance computation with the Facebook AI Similarity Search framework, analyzing their trade-offs and applications.

Result: Benchmarks show significant speedups in processing, with performance improvements varying with data size, dimensionality, and distance functions, enhancing scalability.

Conclusion: The introduced methods make Ball Mapper more practically scalable without altering its theoretical framework, benefiting modern data analysis workflows and providing implementation insights.

Abstract: Ball Mapper is an widely used tool in topological data analysis for summarizing the structure of high-dimensional data through metric-based coverings and graph representations. A central computational bottleneck in Ball Mapper is the construction of the underlying cover, which requires repeated range queries to identify data points within a fixed distance of selected landmarks. As data sets grow in size and dimensionality, naive implementations of this step become increasingly inefficient.
  In this work, we study practical strategies for accelerating cover construction in Ball Mapper by improving the efficiency of range queries. We integrate two complementary approaches into the Ball Mapper pipeline: hierarchical geometric pruning using ball tree data structures, and hardware-aware distance computation using Facebook AI Similarity Search. We describe the underlying algorithms, discuss their trade-offs with respect to metric flexibility and dimensionality, and provide implementation details relevant to large-scale data analysis.
  Empirical benchmarks demonstrate that both approaches yield substantial speedups over the baseline implementation, with performance gains depending on data set size, dimensionality, and choice of distance function. These results improve the practical scalability of Ball Mapper without modifying its theoretical formulation and provide guidance for the efficient implementation of metric-based exploratory tools in modern data analysis workflows.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [585] [Deep Deterministic Nonlinear ICA via Total Correlation Minimization with Matrix-Based Entropy Functional](https://arxiv.org/abs/2601.00904)
*Qiang Li,Shujian Yu,Liang Ma,Chen Ma,Jingyu Liu,Tulay Adali,Vince D. Calhoun*

Main category: stat.ME

TL;DR: The paper introduces DDICA, a deep learning framework enhancing independent component analysis (ICA) for nonlinear and noisy environments.


<details>
  <summary>Details</summary>
Motivation: Traditional ICA methods struggle with linear mixing assumptions and noise robustness, necessitating a more capable tool for separating complex signal relationships.

Method: The authors developed DDICA, leveraging a matrix-based entropy function and directly optimizing independence via stochastic gradient descent without variational or adversarial techniques.

Result: DDICA showed high accuracy in separating independent components across multiple domains, including signal mixtures, hyperspectral imaging, visual receptive modeling, and fMRI analysis.

Conclusion: DDICA proves to be a robust, versatile blind source separation approach, effectively handling complex nonlinear problems across diverse applications.

Abstract: Blind source separation, particularly through independent component analysis (ICA), is widely utilized across various signal processing domains for disentangling underlying components from observed mixed signals, owing to its fully data-driven nature that minimizes reliance on prior assumptions. However, conventional ICA methods rely on an assumption of linear mixing, limiting their ability to capture complex nonlinear relationships and to maintain robustness in noisy environments. In this work, we present deep deterministic nonlinear independent component analysis (DDICA), a novel deep neural network-based framework designed to address these limitations. DDICA leverages a matrix-based entropy function to directly optimize the independence criterion via stochastic gradient descent, bypassing the need for variational approximations or adversarial schemes. This results in a streamlined training process and improved resilience to noise. We validated the effectiveness and generalizability of DDICA across a range of applications, including simulated signal mixtures, hyperspectral image unmixing, modeling of primary visual receptive fields, and resting-state functional magnetic resonance imaging (fMRI) data analysis. Experimental results demonstrate that DDICA effectively separates independent components with high accuracy across a range of applications. These findings suggest that DDICA offers a robust and versatile solution for blind source separation in diverse signal processing tasks.

</details>


### [586] [Personalizing black-box models for nonparametric regression with minimax optimality](https://arxiv.org/abs/2601.01432)
*Sai Li,Linjun Zhang*

Main category: stat.ME

TL;DR: This paper introduces a theoretical framework for few-shot personalization in nonparametric regression using pre-trained models, achieving optimal results and demonstrating robustness.


<details>
  <summary>Details</summary>
Motivation: To explore how pre-trained large-scale models can improve statistical learning tasks in scenarios with limited data availability.

Method: Developed algorithms for integrating pre-trained models into few-shot nonparametric regression, and established theoretical guarantees on their optimal performance and robustness.

Result: Methods achieve minimax optimal rates and demonstrate finite-sample performance benefits in simulations and application to housing datasets using pre-trained models.

Conclusion: Leveraging pre-trained models in few-shot personalization enhances statistical learning performance under sample scarcity, with proven robustness and optimality.

Abstract: Recent advances in large-scale models, including deep neural networks and large language models, have substantially improved performance across a wide range of learning tasks. The widespread availability of such pre-trained models creates new opportunities for data-efficient statistical learning, provided they can be effectively integrated into downstream tasks. Motivated by this setting, we study few-shot personalization, where a pre-trained black-box model is adapted to a target domain using a limited number of samples. We develop a theoretical framework for few-shot personalization in nonparametric regression and propose algorithms that can incorporate a black-box pre-trained model into the regression procedure. We establish the minimax optimal rate for the personalization problem and show that the proposed method attains this rate. Our results clarify the statistical benefits of leveraging pre-trained models under sample scarcity and provide robustness guarantees when the pre-trained model is not informative. We illustrate the finite-sample performance of the methods through simulations and an application to the California housing dataset with several pre-trained models.

</details>


### [587] [Varying-Coefficient Mixture of Experts Model](https://arxiv.org/abs/2601.01699)
*Qicheng Zhao,Celia M. T. Greenwood,Qihuang Zhang*

Main category: stat.ME

TL;DR: The paper introduces the Varying-Coefficient Mixture of Experts (VCMoE) model, extending traditional MoE frameworks to dynamically evolving settings, validated through theoretical proofs and applications, including gene expression data.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing Mixture-of-Experts (MoE) models, which assume constant coefficients and may fail in dynamic contexts like longitudinal or spatial data.

Method: Developed the VCMoE model incorporating varying coefficients in both gating functions and expert models, with an estimation procedure using a label-consistent EM algorithm; conducted theoretical proofs and built inference tools such as confidence bands and generalized likelihood ratio tests.

Result: Simulation studies showed accurate finite-sample performance, low bias, and good coverage. The application to gene expression data revealed temporal dynamics in neuronal gene expression consistent with past studies.

Conclusion: VCMoE effectively captures dynamic covariate influences and latent subpopulation structures, proving useful in analyzing heterogeneous, evolving data such as single-cell gene expression.

Abstract: Mixture-of-Experts (MoE) is a flexible framework that combines multiple specialized submodels (``experts''), by assigning covariate-dependent weights (``gating functions'') to each expert, and have been commonly used for analyzing heterogeneous data. Existing statistical MoE formulations typically assume constant coefficients, for covariate effects within the expert or gating models, which can be inadequate for longitudinal, spatial, or other dynamic settings where covariate influences and latent subpopulation structure evolve across a known dimension. We propose a Varying-Coefficient Mixture of Experts (VCMoE) model that allows all coefficient effects in both the gating functions and expert models to vary along an indexing variable. We establish identifiability and consistency of the proposed model, and develop an estimation procedure, label-consistent EM algorithm, for both fully functional and hybrid specifications, along with the corresponding asymptotic distributions of the resulting estimators. For inference, simultaneous confidence bands are constructed using both asymptotic theory for the maximum discrepancy between the estimated functional coefficients and their true counterparts, and with bootstrap methods. In addition, a generalized likelihood ratio test is developed to examine whether a coefficient function is genuinely varying across the index variable. Simulation studies demonstrate good finite-sample performance, with acceptable bias and satisfactory coverage rates. We illustrate the proposed VCMoE model using a dataset of single nucleus gene expression in embryonic mice to characterize the temporal dynamics of the associations between the expression levels of genes Satb2 and Bcl11b across two latent cell subpopulations of neurons, yielding results that are consistent with prior findings.

</details>


### [588] [Spatio-temporal modeling and forecasting with Fourier neural operators](https://arxiv.org/abs/2601.01813)
*Pratik Nag,Andrew Zammit-Mangion,Sumeetpal Singh,Noel Cressie*

Main category: stat.ME

TL;DR: This paper introduces Fourier neural operators (FNOs) for dynamic spatio-temporal modeling, demonstrating their efficiency in forecasting complex phenomena compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address challenges in modeling environmental heterogeneity and interactions in spatio-temporal phenomena where traditional methods like Gaussian processes are insufficient.

Method: The paper utilizes Fourier neural operators (FNOs) as flexible mappings to approximate solutions of partial differential equations without requiring explicit PDE knowledge, leveraging sample input-output data.

Result: Evaluation on simulated nonlinear PDEs and real-world datasets (Atlantic sea surface temperature and European precipitation) indicates superior forecasting accuracy and valid uncertainty quantification by FNO.

Conclusion: FNO-based models excel in capturing complex dependencies and uncertainty in spatio-temporal processes, making them a promising tool for dynamic environmental and biological forecasting.

Abstract: Spatio-temporal process models are often used for modeling dynamic physical and biological phenomena that evolve across space and time. These phenomena may exhibit environmental heterogeneity and complex interactions that are difficult to capture using traditional statistical process models such as Gaussian processes. This work proposes the use of Fourier neural operators (FNOs) for constructing statistical dynamical spatio-temporal models for forecasting. An FNO is a flexible mapping of functions that approximates the solution operator of possibly unknown linear or non-linear partial differential equations (PDEs) in a computationally efficient manner. It does so using samples of inputs and their respective outputs, and hence explicit knowledge of the underlying PDE is not required. Through simulations from a nonlinear PDE with known solution, we compare FNO forecasts to those from state-of-the-art statistical spatio-temporal-forecasting methods. Further, using sea surface temperature data over the Atlantic Ocean and precipitation data across Europe, we demonstrate the ability of FNO-based dynamic spatio-temporal (DST) statistical modeling to capture complex real-world spatio-temporal dependencies. Using collections of testing instances, we show that the FNO-DST forecasts are accurate with valid uncertainty quantification.

</details>


### [589] [Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction](https://arxiv.org/abs/2601.02322)
*Shuozhi Zuo,Yixin Wang*

Main category: stat.ME

TL;DR: The paper introduces a new algorithm that adapts covariate selection based on distribution shifts in out-of-distribution (OOD) prediction tasks, showing superior performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Out-of-distribution prediction often fails due to reliance on unstable spurious associations or subsets of observed causal factors, leading to performance gaps in tasks requiring generalization across environments.

Method: The authors propose an environment-adaptive covariate selection (EACS) algorithm that uses signatures from unlabeled covariate distributions in the target OOD environment to dynamically select predictive covariates tailored to specific shifts.

Result: In simulations and real-world datasets, EACS consistently outperformed static approaches, including causal, invariant, and empirical risk minimization (ERM)-based predictors, under various distribution shifts.

Conclusion: The study demonstrates that OOD prediction benefits from environment-specific strategies, challenging the universality of causal or invariant covariate selection approaches and highlighting the importance of adapting to specific covariate shifts.

Abstract: Out-of-distribution (OOD) prediction is often approached by restricting models to causal or invariant covariates, avoiding non-causal spurious associations that may be unstable across environments. Despite its theoretical appeal, this strategy frequently underperforms empirical risk minimization (ERM) in practice. We investigate the source of this gap and show that such failures naturally arise when only a subset of the true causes of the outcome is observed. In these settings, non-causal spurious covariates can serve as informative proxies for unobserved causes and substantially improve prediction, except under distribution shifts that break these proxy relationships. Consequently, the optimal set of predictive covariates is neither universal nor necessarily exhibits invariant relationships with the outcome across all environments, but instead depends on the specific type of shift encountered. Crucially, we observe that different covariate shifts induce distinct, observable signatures in the covariate distribution itself. Moreover, these signatures can be extracted from unlabeled data in the target OOD environment and used to assess when proxy covariates remain reliable and when they fail. Building on this observation, we propose an environment-adaptive covariate selection (EACS) algorithm that maps environment-level covariate summaries to environment-specific covariate sets, while allowing the incorporation of prior causal knowledge as constraints. Across simulations and applied datasets, EACS consistently outperforms static causal, invariant, and ERM-based predictors under diverse distribution shifts.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [590] [Interaction Improvement](https://arxiv.org/abs/2601.01638)
*Adrienne Lancelot,Giulio Manzonetto,Guy McCusker,Gabriele Vanoni*

Main category: cs.LO

TL;DR: The paper uses the checkers calculus to give a quantitative interpretation to the relational semantics of linear logic, showing it refines the contextual preorder.


<details>
  <summary>Details</summary>
Motivation: To address the lack of quantitative insights in the preorders and equational theories induced by relational semantics despite its resource-aware nature.

Method: The authors employ the checkers calculus to develop a quantitative, contextual interpretation of the preorder within the relational semantics.

Result: They show that relational semantics refines the contextual preorder by limiting the number of interactions between terms and context.

Conclusion: The work extends the relational semantics' expressiveness by imbuing it with quantitative evaluations via the checkers calculus.

Abstract: The relational semantics of linear logic is a powerful framework for defining resource-aware models of the $λ$-calculus. However, its quantitative aspects are not reflected in the preorders and equational theories induced by these models. Indeed, they can be characterized in terms of (in)equalities between Böhm trees up to extensionality, which are qualitative in nature. We employ the recently introduced checkers calculus to provide a quantitative and contextual interpretation of the preorder associated to the relational semantics. This way, we show that the relational semantics refines the contextual preorder constraining the number of interactions between the related terms and the context.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [591] [PyBatchRender: A Python Library for Batched 3D Rendering at Up to One Million FPS](https://arxiv.org/abs/2601.01288)
*Evgenii Rudakov,Jonathan Shock,Benjamin Ultan Cowley*

Main category: cs.GR

TL;DR: PyBatchRender is a Python library for high-throughput 3D rendering, achieving over 1 million FPS for simple scenes, designed to support reinforcement learning from pixels.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between performance and accessibility in 3D-rendered environments for reinforcement learning, providing a solution that is fast, simple, and scalable.

Method: The developers built PyBatchRender on the Panda3D game engine, utilizing optimized batched rendering techniques and Python flexibility, allowing researchers to create scenes effortlessly with minimal code.

Result: PyBatchRender achieves up to 1000X speedups compared to traditional frameworks, rivaling C++ engines, enabling rapid prototyping and scalable AI training.

Conclusion: PyBatchRender democratizes high-performance 3D simulation for AI research by combining state-of-the-art speed, simplicity, and accessibility, making it valuable for researchers and developers.

Abstract: Reinforcement learning from pixels is often bottlenecked by the performance and complexity of 3D rendered environments. Researchers face a trade-off between high-speed, low-level engines and slower, more accessible Python frameworks. To address this, we introduce PyBatchRender, a Python library for high-throughput, batched 3D rendering that achieves over 1 million FPS on simple scenes. Built on the Panda3D game engine, it utilizes its mature ecosystem while enhancing performance through optimized batched rendering for up to 1000X speedups. Designed as a physics-agnostic renderer for reinforcement learning from pixels, PyBatchRender offers greater flexibility than dedicated libraries, simpler setup than typical game-engine wrappers, and speeds rivaling state-of-the-art C++ engines like Madrona. Users can create custom scenes entirely in Python with tens of lines of code, enabling rapid prototyping for scalable AI training. Open-source and easy to integrate, it serves to democratize high-performance 3D simulation for researchers and developers. The library is available at https://github.com/dolphin-in-a-coma/PyBatchRender.

</details>


### [592] [VARTS: A Tool for the Visualization and Analysis of Representative Time Series Data](https://arxiv.org/abs/2601.01361)
*Duosi Jin,Jianqiu Xu,Guidong Zhang*

Main category: cs.GR

TL;DR: The paper introduces VARTS, a tool for selecting and visualizing representative time series to improve clarity in large-scale data analysis.


<details>
  <summary>Details</summary>
Motivation: The challenge of visual clutter and redundant patterns in large-scale time series makes it hard for users to identify main trends effectively.

Method: VARTS uses M4-based sampling, DTW-based similarity, and greedy selection in a unified workflow for representative time series identification and visualization.

Result: VARTS provides an interactive interface for raw and reduced data visualization, enhancing visual clarity and interpretability of time series data.

Conclusion: VARTS reduces redundancy while maintaining essential patterns, enabling efficient analysis of large-scale time series datasets.

Abstract: Large-scale time series visualization often suffers from excessive visual clutter and redundant patterns, making it difficult for users to understand the main temporal trends. To address this challenge, we present VARTS, an interactive visual analytics tool for representative time series selection and visualization. Building upon our previous work M4-Greedy, VARTS integrates M4-based sampling, DTW-based similarity computation, and greedy selection into a unified workflow for the identification and visualization of representative series. The tool provides a responsive graphical interface that allows users to import time series datasets, perform representative selection, and visualize both raw and reduced data through multiple coordinated views. By reducing redundancy while preserving essential data patterns, VARTS effectively enhances visual clarity and interpretability for large-scale time series analysis. The demo video is available at https://youtu.be/mS9f12Rf0jo.

</details>


### [593] [SketchRodGS: Sketch-based Extraction of Slender Geometries for Animating Gaussian Splatting Scenes](https://arxiv.org/abs/2601.02072)
*Haato Watanabe,Nobuyuki Umetani*

Main category: cs.GR

TL;DR: This paper proposes a method for constructing polyline representations of slender objects in Gaussian splatting using user sketches.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of creating connected polyline representations from Gaussian splatting, which lacks connectivity data and contains noise.

Method: The paper uses dynamic programming with screen-space shortest path analysis to robustly extract polylines from user-provided sketches.

Result: The method effectively extracts polyline meshes in various real-world examples, demonstrating its robustness and efficacy.

Conclusion: The approach successfully enables polyline construction for slender objects in Gaussian splatting scenes, overcoming previous challenges and showcasing practical applications.

Abstract: Physics simulation of slender elastic objects often requires discretization as a polyline. However, constructing a polyline from Gaussian splatting is challenging as Gaussian splatting lacks connectivity information and the configuration of Gaussian primitives contains much noise. This paper presents a method to extract a polyline representation of the slender part of the objects in a Gaussian splatting scene from the user's sketching input. Our method robustly constructs a polyline mesh that represents the slender parts using the screen-space shortest path analysis that can be efficiently solved using dynamic programming. We demonstrate the effectiveness of our approach in several in-the-wild examples.

</details>


### [594] [Dancing Points: Synthesizing Ballroom Dancing with Three-Point Inputs](https://arxiv.org/abs/2601.02096)
*Peizhuo Li,Sebastian Starke,Yuting Ye,Olga Sorkine-Hornung*

Main category: cs.GR

TL;DR: The paper introduces a method for paired ballroom dancing synthesis using VR-derived sparse trajectories, simplifying motion modeling and improving computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Understanding and synthesizing ballroom dancing motions is challenging due to their complexity and diverse interactions, prompting the need for a simplified motion descriptor.

Method: The method uses a VR device's three-point trajectory to represent a dancer's motion along with an efficient MLP network to predict follower trajectories from leader trajectories, applying deterministic neural networks for virtual embodiment.

Result: The approach demonstrates accurate modeling for ballroom dancing and generalizes well to diverse datasets like LaFAN, proving to be computationally- and data-efficient.

Conclusion: The solution creates possibilities for immersive paired dancing applications and showcases robust performance across varied motion datasets.

Abstract: Ballroom dancing is a structured yet expressive motion category. Its highly diverse movement and complex interactions between leader and follower dancers make the understanding and synthesis challenging. We demonstrate that the three-point trajectory available from a virtual reality (VR) device can effectively serve as a dancer's motion descriptor, simplifying the modeling and synthesis of interplay between dancers' full-body motions down to sparse trajectories. Thanks to the low dimensionality, we can employ an efficient MLP network to predict the follower's three-point trajectory directly from the leader's three-point input for certain types of ballroom dancing, addressing the challenge of modeling high-dimensional full-body interaction. It also prevents our method from overfitting thanks to its compact yet explicit representation. By leveraging the inherent structure of the movements and carefully planning the autoregressive procedure, we show a deterministic neural network is able to translate three-point trajectories into a virtual embodied avatar, which is typically considered under-constrained and requires generative models for common motions. In addition, we demonstrate this deterministic approach generalizes beyond small, structured datasets like ballroom dancing, and performs robustly on larger, more diverse datasets such as LaFAN. Our method provides a computationally- and data-efficient solution, opening new possibilities for immersive paired dancing applications. Code and pre-trained models for this paper are available at https://peizhuoli.github.io/dancing-points.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [595] [Comparative Analysis of Formula and Structure Prediction from Tandem Mass Spectra](https://arxiv.org/abs/2601.00941)
*Xujun Che,Xiuxia Du,Depeng Xu*

Main category: q-bio.QM

TL;DR: This paper systematically evaluates computational approaches to improve compound predictions using LC-MS/MS for metabolomics and exposomics.


<details>
  <summary>Details</summary>
Motivation: Metabolomics and exposomics rely on LC-MS/MS data, but many signals remain unannotated due to limited spectral libraries. This limits the discovery of metabolic and environmental effects on health.

Method: The study evaluates formula and structure prediction performance of various prediction algorithms across different types of adducts in LC-MS/MS datasets.

Result: Results established performance baselines, identified bottlenecks, and highlighted areas for improvement in compound prediction workflows.

Conclusion: The findings provide actionable insights and directions for enhancing computational workflows to better predict compounds and fully utilize LC-MS/MS data.

Abstract: Liquid chromatography mass spectrometry (LC-MS)-based metabolomics and exposomics aim to measure detectable small molecules in biological samples. The results facilitate hypothesis-generating discovery of metabolic changes and disease mechanisms and provide information about environmental exposures and their effects on human health. Metabolomics and exposomics are made possible by the high resolving power of LC and high mass measurement accuracy of MS. However, a majority of the signals from such studies still cannot be identified or annotated using conventional library searching because existing spectral libraries are far from covering the vast chemical space captured by LC-MS/MS. To address this challenge and unleash the full potential of metabolomics and exposomics, a number of computational approaches have been developed to predict compounds based on tandem mass spectra. Published assessment of these approaches used different datasets and evaluation. To select prediction workflows for practical applications and identify areas for further improvements, we have carried out a systematic evaluation of the state-of-the-art prediction algorithms. Specifically, the accuracy of formula prediction and structure prediction was evaluated for different types of adducts. The resulting findings have established realistic performance baselines, identified critical bottlenecks, and provided guidance to further improve compound predictions based on MS.

</details>


### [596] [Deep Learning Framework for RNA Inverse Folding with Geometric Structure Potentials](https://arxiv.org/abs/2601.00895)
*Annabelle Yao*

Main category: q-bio.QM

TL;DR: This paper presents a new deep learning framework for RNA inverse folding using GVP layers and a Transformer architecture, demonstrating superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately predicting or designing RNA sequences that conform with given 3D structures, which is important for understanding RNA's biological functions.

Method: The author integrates GVP layers with a Transformer model and constructs a dataset of experimentally solved RNA 3D structures to evaluate sequence recovery and structural fidelity.

Result: The proposed model achieves state-of-the-art performance with improved recovery and TM-scores, effectively generalizes to unseen RNA families, and produces structurally accurate sequences upon refolding.

Conclusion: Incorporating geometric features via GVP layers improves Transformer-based RNA design, enabling accurate predictions and designs of RNA sequences with high fidelity to native structures.

Abstract: RNA's diverse biological functions stem from its structural versatility, yet accurately predicting and designing RNA sequences given a 3D conformation (inverse folding) remains a challenge. Here, I introduce a deep learning framework that integrates Geometric Vector Perceptron (GVP) layers with a Transformer architecture to enable end-to-end RNA design. I construct a dataset consisting of experimentally solved RNA 3D structures, filtered and deduplicated from the BGSU RNA list, and evaluate performance using both sequence recovery rate and TM-score to assess sequence and structural fidelity, respectively. On standard benchmarks and RNA-Puzzles, my model achieves state-of-the-art performance, with recovery and TM-scores of 0.481 and 0.332, surpassing existing methods across diverse RNA families and length scales. Masked family-level validation using Rfam annotations confirms strong generalization beyond seen families. Furthermore, inverse-folded sequences, when refolded using AlphaFold3, closely resemble native structures, highlighting the critical role of geometric features captured by GVP layers in enhancing Transformer-based RNA design.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [597] [A Global Atlas of Digital Dermatology to Map Innovation and Disparities](https://arxiv.org/abs/2601.00840)
*Fabian Gröger,Simone Lionetti,Philippe Gottfrois,Alvaro Gonzalez-Jimenez,Lea Habermacher,Labelling Consortium,Ludovic Amruthalingam,Matthew Groh,Marc Pouly,Alexander A. Navarini*

Main category: cs.DL

TL;DR: The paper introduces 'SkinMap,' a framework for analyzing dermatology datasets and quantifying their informational coverage and gaps, finding significant underrepresentation in certain demographics and conditions.


<details>
  <summary>Details</summary>
Motivation: To ensure AI in dermatology is reliable and inclusive, comprehensive and diverse datasets are required, but current datasets may lack coverage across demographics and conditions.

Method: The authors developed SkinMap, which aggregates over 1.1 million dermatology images into a semantic atlas, allowing for the analysis of dataset trends, novelty, redundancies, and representation gaps.

Result: The analysis revealed limited informational novelty in recent datasets, redundancy in common areas, and significant gaps in representation for darker skin tones, pediatric patients, and rare diseases.

Conclusion: SkinMap provides a critical tool for identifying and addressing underrepresented areas in dermatology datasets, enabling more inclusive and comprehensive AI-driven approaches.

Abstract: The adoption of artificial intelligence in dermatology promises democratized access to healthcare, but model reliability depends on the quality and comprehensiveness of the data fueling these models. Despite rapid growth in publicly available dermatology images, the field lacks quantitative key performance indicators to measure whether new datasets expand clinical coverage or merely replicate what is already known. Here we present SkinMap, a multi-modal framework for the first comprehensive audit of the field's entire data basis. We unify the publicly available dermatology datasets into a single, queryable semantic atlas comprising more than 1.1 million images of skin conditions and quantify (i) informational novelty over time, (ii) dataset redundancy, and (iii) representation gaps across demographics and diagnoses. Despite exponential growth in dataset sizes, informational novelty across time has somewhat plateaued: Some clusters, such as common neoplasms on fair skin, are densely populated, while underrepresented skin types and many rare diseases remain unaddressed. We further identify structural gaps in coverage: Darker skin tones (Fitzpatrick V-VI) constitute only 5.8% of images and pediatric patients only 3.0%, while many rare diseases and phenotype combinations remain sparsely represented. SkinMap provides infrastructure to measure blind spots and steer strategic data acquisition toward undercovered regions of clinical space.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [598] [Can Large Language Models Improve Venture Capital Exit Timing After IPO?](https://arxiv.org/abs/2601.00810)
*Mohammadhossien Rashidi*

Main category: q-fin.PM

TL;DR: The paper explores using large language models (LLMs) to determine the optimal exit time for venture capital (VC) investors post-IPO, comparing these recommendations with actual exit decisions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the gap in research on whether VC exit timing decisions post-IPO are economically optimal. Additionally, it explores whether LLMs can effectively synthesize financial and textual data to improve these decisions.

Method: The paper introduces a framework that leverages LLMs to analyze monthly financial performance, filings, news, and market data post-IPO. The framework generates recommendations on whether VCs should sell or hold their stake and compares these with actual VC exit dates.

Result: LLM recommendations were evaluated against observed VC exit dates to calculate gains or losses, assessing whether following AI-driven advice leads to better financial outcomes.

Conclusion: The study concludes by presenting evidence on whether AI-driven decision-making models like LLMs can enhance exit timing accuracy and supplement traditional financial decision-making frameworks in venture capital.

Abstract: Exit timing after an IPO is one of the most consequential decisions for venture capital (VC) investors, yet existing research focuses mainly on describing when VCs exit rather than evaluating whether those choices are economically optimal. Meanwhile, large language models (LLMs) have shown promise in synthesizing complex financial data and textual information but have not been applied to post-IPO exit decisions. This study introduces a framework that uses LLMs to estimate the optimal time for VC exit by analyzing monthly post IPO information financial performance, filings, news, and market signals and recommending whether to sell or continue holding. We compare these LLM generated recommendations with the actual exit dates observed for VCs and compute the return differences between the two strategies. By quantifying gains or losses associated with following the LLM, this study provides evidence on whether AI-driven guidance can improve exit timing and complements traditional hazard and real-options models in venture capital research.

</details>


<div id='q-bio.TO'></div>

# q-bio.TO [[Back]](#toc)

### [599] [Quantifying Local Strain Field and Deformation in Active Contraction of Bladder Using a Pretrained Transformer Model: A Speckle-Free Approach](https://arxiv.org/abs/2601.01315)
*Alireza Asadbeygi,Anne M. Robertson,Yasutaka Tobe,Masoud Zamani,Sean D. Stocker,Paul Watton,Naoki Yoshimura,Simon C Watkins*

Main category: q-bio.TO

TL;DR: This paper presents a new speckle-free framework for quantifying local strain fields in bladder tissues using a zero-shot transformer model (CoTracker3), validated through multiphoton microscopy.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the accuracy of strain field quantification in bladder contractions without relying on artificial markers, which can alter tissue properties.

Method: The authors used the zero-shot transformer model CoTracker3, integrated with a custom isotonic biaxial apparatus and multiphoton microscopy to measure natural bladder textures during contraction without speckling.

Result: Their method demonstrated high pixel accuracy, low strain errors, and effectively captured complex deformation patterns, revealing significant anisotropic contraction in rat bladder tissues.

Conclusion: This non-invasive framework offers physiologically accurate measurements by avoiding artificial speckling, with potential applications in analyzing other biological and engineered systems.

Abstract: Accurate quantification of local strain fields during bladder contraction is essential for understanding the biomechanics of bladder micturition, in both health and disease. Conventional digital image correlation (DIC) methods have been successfully applied to various biological tissues; however, this approach requires artificial speckling, which can alter both passive and active properties of the tissue. In this study, we introduce a speckle-free framework for quantifying local strain fields using a state-of-the-art, zero-shot transformer model, CoTracker3. We utilized a custom-designed, portable isotonic biaxial apparatus compatible with multiphoton microscopy (MPM) to demonstrate this approach, successfully tracking natural bladder lumen textures without artificial markers. Benchmark tests validated the method's high pixel accuracy and low strain errors. Our framework effectively captured heterogeneous deformation patterns, despite complex folding and buckling, which conventional DIC often fails to track. Application to in vitro active bladder contractions in four rat specimens (n=4) revealed statistically significant anisotropy (p<0.01), with higher contraction longitudinally compared to circumferentially. Multiphoton microscopy further illustrated and confirmed heterogeneous morphological changes, such as large fold formation during active contraction. This non-invasive approach eliminates speckle-induced artifacts, enabling more physiologically relevant measurements, and has broad applicability for material testing of other biological and engineered systems.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [600] [Tessellation Localized Transfer learning for nonparametric regression](https://arxiv.org/abs/2601.00987)
*Hélène Halconruy,Benjamin Bobbia,Paul Lejamtel*

Main category: math.ST

TL;DR: The paper introduces a framework for transfer learning using nonparametric regression with localized transformations to handle source-target heterogeneity effectively.


<details>
  <summary>Details</summary>
Motivation: Improve transfer learning by addressing heterogeneity between source and target tasks to avoid negative transfer.

Method: Proposes a regression model based on local cell partitioning, allowing low-complexity transformations for better target-source relationship modeling.

Result: Achieves sharp minimax rates in target regression, local transfers mitigate dimensionality issues with theoretical robustness proofs.

Conclusion: Localized transfer functions improve learning performance, limiting negative transfer and providing robust theoretical guarantees.

Abstract: Transfer learning aims to improve performance on a target task by leveraging information from related source tasks. We propose a nonparametric regression transfer learning framework that explicitly models heterogeneity in the source-target relationship. Our approach relies on a local transfer assumption: the covariate space is partitioned into finitely many cells such that, within each cell, the target regression function can be expressed as a low-complexity transformation of the source regression function. This localized structure enables effective transfer where similarity is present while limiting negative transfer elsewhere. We introduce estimators that jointly learn the local transfer functions and the target regression, together with fully data-driven procedures that adapt to unknown partition structure and transfer strength. We establish sharp minimax rates for target regression estimation, showing that local transfer can mitigate the curse of dimensionality by exploiting reduced functional complexity. Our theoretical guarantees take the form of oracle inequalities that decompose excess risk into estimation and approximation terms, ensuring robustness to model misspecification. Numerical experiments illustrate the benefits of the proposed approach.

</details>


### [601] [SGD with Dependent Data: Optimal Estimation, Regret, and Inference](https://arxiv.org/abs/2601.01371)
*Yinan Shen,Yichen Zhang,Wen-Xin Zhou*

Main category: math.ST

TL;DR: The paper examines stochastic gradient descent (SGD) under dependent data, proving it maintains statistical optimality and avoids prior estimation-regret trade-offs, alongside introducing a sparse regression algorithm.


<details>
  <summary>Details</summary>
Motivation: To extend the applicability of SGD to scenarios with dependent data streams and sequential decision-making, addressing limitations in non-stationary and non-mixing data settings.

Method: Analyzing SGD under martingale-type and sequential decision-induced dependencies, supplying non-asymptotic and asymptotic performance guarantees, and proposing a "conic" approximation for unbounded covariates. They also develop an efficient sparse regression algorithm.

Result: SGD achieves optimal statistical estimation error and regret, converges asymptotically with sharp tail bounds, and a new SGD-based sparse regression algorithm achieves statistical optimality with high efficiency.

Conclusion: SGD accommodates both independence and dependence in data, overcoming estimation-regret trade-offs, and is also scalable for sparse regression with dependent data.

Abstract: This work investigates the performance of the final iterate produced by stochastic gradient descent (SGD) under temporally dependent data. We consider two complementary sources of dependence: $(i)$ martingale-type dependence in both the covariate and noise processes, which accommodates non-stationary and non-mixing time series data, and $(ii)$ dependence induced by sequential decision making. Our formulation runs in parallel with classical notions of (local) stationarity and strong mixing, while neither framework fully subsumes the other. Remarkably, SGD is shown to automatically accommodate both independent and dependent information under a broad class of stepsize schedules and exploration rate schemes.
  Non-asymptotically, we show that SGD simultaneously achieves statistically optimal estimation error and regret, extending and improving existing results. In particular, our tail bounds remain sharp even for potentially infinite horizon $T=+\infty$. Asymptotically, the SGD iterates converge to a Gaussian distribution with only an $O_{\PP}(1/\sqrt{t})$ remainder, demonstrating that the supposed estimation-regret trade-off claimed in prior work can in fact be avoided. We further propose a new ``conic'' approximation of the decision region that allows the covariates to have unbounded support. For online sparse regression, we develop a new SGD-based algorithm that uses only $d$ units of storage and requires $O(d)$ flops per iteration, achieving the long term statistical optimality. Intuitively, each incoming observation contributes to estimation accuracy, while aggregated summary statistics guide support recovery.

</details>


### [602] [Double Machine Learning of Continuous Treatment Effects with General Instrumental Variables](https://arxiv.org/abs/2601.01471)
*Shuyuan Chen,Peng Zhang,Yifan Cui*

Main category: math.ST

TL;DR: The paper introduces a new framework for identifying dose-response functions under unmeasured confounders using instrumental variables and develops estimation methods to achieve this.


<details>
  <summary>Details</summary>
Motivation: Unmeasured confounders often cause bias in estimating causal effects of continuous treatments, especially in real-world applications, making it critical to develop accurate methods to address such biases.

Method: The paper introduces a local identification framework with instrumental variables, proposes a uniform regular weighting function, and develops augmented inverse probability weighting scores using a debiased machine learning framework.

Result: The authors establish asymptotic properties for the proposed methods and demonstrate their effectiveness through simulations and empirical studies.

Conclusion: The proposed approach successfully mitigates unmeasured confounding in dose-response estimations and is effective for finite-sample applications under the outlined framework.

Abstract: Estimating causal effects of continuous treatments is a common problem in practice, for example, in studying dose-response functions. Classical analyses typically assume that all confounders are fully observed, whereas in real-world applications, unmeasured confounding often persists. In this article, we propose a novel framework for local identification of dose-response functions using instrumental variables, thereby mitigating bias induced by unobserved confounders. We introduce the concept of a uniform regular weighting function and consider covering the treatment space with a finite collection of open sets. On each of these sets, such a weighting function exists, allowing us to identify the dose-response function locally within the corresponding region. For estimation, we develop an augmented inverse probability weighting score for continuous treatments under a debiased machine learning framework with instrumental variables. We further establish the asymptotic properties when the dose-response function is estimated via kernel regression or empirical risk minimization. Finally, we conduct both simulation and empirical studies to assess the finite-sample performance of the proposed methods.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [603] [ARIES: A Scalable Multi-Agent Orchestration Framework for Real-Time Epidemiological Surveillance and Outbreak Monitoring](https://arxiv.org/abs/2601.01831)
*Aniket Wattamwar,Sampson Akwafuo*

Main category: cs.MA

TL;DR: The paper introduces ARIES, an AI-driven multi-agent system for real-time, specialized epidemiological surveillance to address data silos and emergent threat detection.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of general-purpose AI in handling high-stakes epidemiological challenges, such as hallucinations and integration across specialized data silos.

Method: The paper develops ARIES, a hierarchical multi-agent system leveraging GPTs, which autonomously queries health agencies and peer-reviewed data to synthesize epidemiological information.

Result: ARIES demonstrated superior performance over generic AI models by identifying emerging threats and enabling robust and dynamic health intelligence systems.

Conclusion: Task-specific AI swarms can significantly enhance outbreak responses and advance global health intelligence by offering specialized, real-time, and scalable solutions.

Abstract: Global health surveillance is currently facing a challenge of Knowledge Gaps. While general-purpose AI has proliferated, it remains fundamentally unsuited for the high-stakes epidemiological domain due to chronic hallucinations and an inability to navigate specialized data silos. This paper introduces ARIES (Agentic Retrieval Intelligence for Epidemiological Surveillance), a specialized, autonomous multi-agent framework designed to move beyond static, disease-specific dashboards toward a dynamic intelligence ecosystem. Built on a hierarchical command structure, ARIES utilizes GPTs to orchestrate a scalable swarm of sub-agents capable of autonomously querying World Health Organization (WHO), Center for Disease Control and Prevention (CDC), and peer-reviewed research papers. By automating the extraction and logical synthesis of surveillance data, ARIES provides a specialized reasoning that identifies emergent threats and signal divergence in near real-time. This modular architecture proves that a task-specific agentic swarm can outperform generic models, offering a robust, extensible for next-generation outbreak response and global health intelligence.

</details>


### [604] [Harm in AI-Driven Societies: An Audit of Toxicity Adoption on Chirper.ai](https://arxiv.org/abs/2601.01090)
*Erica Coppolillo,Luca Luceri,Emilio Ferrara*

Main category: cs.MA

TL;DR: This paper analyzes how exposure to harmful content influences LLM-driven agents' behavior over time on Chirper.ai, finding that toxic responses often arise from stimuli but can also emerge spontaneously. Metrics are proposed for understanding toxicity dynamics.


<details>
  <summary>Details</summary>
Motivation: To understand how harmful content exposure affects behavior of LLM-based agents in fully AI-driven online ecosystems, specifically regarding toxicity propagation and spontaneous generation.

Method: The study models agent interactions as stimuli (posts) and responses (comments), analyzing toxicity adoption empirically through observable interactions and introducing influence metrics.

Result: Findings demonstrate that toxic responses occur more frequently after exposure to toxic stimuli, cumulative exposure heightens toxicity probability, and toxicity prediction is possible using exposure data.

Conclusion: Exposure significantly influences LLM agent behavior; monitoring encountered content could offer lightweight frameworks for auditing and mitigating toxic behavior in autonomous AI deployment.

Abstract: Large Language Models (LLMs) are increasingly embedded in autonomous agents that participate in online social ecosystems, where interactions are sequential, cumulative, and only partially controlled. While prior work has documented the generation of toxic content by LLMs, far less is known about how exposure to harmful content shapes agent behavior over time, particularly in environments composed entirely of interacting AI agents. In this work, we study toxicity adoption of LLM-driven agents on Chirper.ai, a fully AI-driven social platform. Specifically, we model interactions in terms of stimuli (posts) and responses (comments), and by operationalizing exposure through observable interactions rather than inferred recommendation mechanisms.
  We conduct a large-scale empirical analysis of agent behavior, examining how response toxicity relates to stimulus toxicity, how repeated exposure affects the likelihood of toxic responses, and whether toxic behavior can be predicted from exposure alone. Our findings show that while toxic responses are more likely following toxic stimuli, a substantial fraction of toxicity emerges spontaneously, independent of exposure. At the same time, cumulative toxic exposure significantly increases the probability of toxic responding. We further introduce two influence metrics, the Influence-Driven Response Rate and the Spontaneous Response Rate, revealing a strong trade-off between induced and spontaneous toxicity. Finally, we show that the number of toxic stimuli alone enables accurate prediction of whether an agent will eventually produce toxic content.
  These results highlight exposure as a critical risk factor in the deployment of LLM agents and suggest that monitoring encountered content may provide a lightweight yet effective mechanism for auditing and mitigating harmful behavior in the wild.

</details>


### [605] [CONSENT: A Negotiation Framework for Leveraging User Flexibility in Vehicle-to-Building Charging under Uncertainty](https://arxiv.org/abs/2601.01581)
*Rishav Sen,Fangqi Liu,Jose Paolo Talusan,Ava Pettet,Yoshinori Suzue,Mark Bailey,Ayan Mukhopadhyay,Abhishek Dubey*

Main category: cs.MA

TL;DR: This paper proposes a negotiation-based framework to balance interests between building operators and EV users, ensuring operational cooperation and mutual savings.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of EVs has created a conflict in V2B setups due to uncoordinated charging, leading to high energy costs for building operators and inconvenience for EV drivers who prioritize a full charge.

Method: The framework introduces incentive-backed flexible options for EV users regarding departure time or charging needs, ensuring it meets principles of strategy-proofness, budget feasibility, and voluntary participation. It uses real operational data and user surveys for calibration and validation.

Result: The framework reduces building operators' costs by over 3.5% and decreases user charging expenses by 22% compared to conventional systems.

Conclusion: This framework bridges energy and mobility systems by turning EV charging into a collaborative platform, benefiting both operators and users with cost savings and operational alignment.

Abstract: The growth of Electric Vehicles (EVs) creates a conflict in vehicle-to-building (V2B) settings between building operators, who face high energy costs from uncoordinated charging, and drivers, who prioritize convenience and a full charge. To resolve this, we propose a negotiation-based framework that, by design, guarantees voluntary participation, strategy-proofness, and budget feasibility. It transforms EV charging into a strategic resource by offering drivers a range of incentive-backed options for modest flexibility in their departure time or requested state of charge (SoC). Our framework is calibrated with user survey data and validated using real operational data from a commercial building and an EV manufacturer. Simulations show that our negotiation protocol creates a mutually beneficial outcome: lowering the building operator's costs by over 3.5\% compared to an optimized, non-negotiating smart charging policy, while simultaneously reducing user charging expenses by 22\% below the utility's retail energy rate. By aligning operator and EV user objectives, our framework provides a strategic bridge between energy and mobility systems, transforming EV charging from a source of operational friction into a platform for collaboration and shared savings.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [606] [SAFE-QAQ: End-to-End Slow-Thinking Audio-Text Fraud Detection via Reinforcement Learning](https://arxiv.org/abs/2601.01392)
*Peidong Wang,Zhiming Ma,Xin Dai,Yongkang Liu,Shi Feng,Xiaocui Yang,Wenxing Hu,Zhihao Wang,Mingjun Pan,Li Yuan,Daling Wang*

Main category: cs.SD

TL;DR: SAFE-QAQ is an advanced, audio-based fraud detection framework that overcomes transcription errors, incorporates slow-thinking mechanisms, and enables real-time fraud risk assessment, showing significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Existing fraud detection heavily relies on transcribed text, which is prone to Automated Speech Recognition (ASR) errors and lacks critical acoustic information like vocal nuances and context, making it less effective against sophisticated fraud strategies.

Method: The method involves an end-to-end framework named SAFE-QAQ, which eliminates ASR issues, uses rule-based slow-thinking reward mechanisms for detecting fine-grained patterns, and incorporates dynamic live-call risk assessments for early fraud detection.

Result: SAFE-QAQ outperforms existing methods significantly in accuracy, efficiency, and real-time processing, as confirmed by experiments on the TeleAntiFraud-Bench. It is actively deployed, analyzing over 70,000 calls daily, thus enhancing fraud detection automation.

Conclusion: SAFE-QAQ demonstrates a robust solution to overcome transcription-based fraud detection limitations, combining innovative mechanisms for accurate, efficient, and real-time fraud prevention at scale.

Abstract: Existing fraud detection methods predominantly rely on transcribed text, suffering from ASR errors and missing crucial acoustic cues like vocal tone and environmental context. This limits their effectiveness against complex deceptive strategies. To address these challenges, we first propose \textbf{SAFE-QAQ}, an end-to-end comprehensive framework for audio-based slow-thinking fraud detection. First, the SAFE-QAQ framework eliminates the impact of transcription errors on detection performance. Secondly, we propose rule-based slow-thinking reward mechanisms that systematically guide the system to identify fraud-indicative patterns by accurately capturing fine-grained audio details, through hierarchical reasoning processes. Besides, our framework introduces a dynamic risk assessment framework during live calls, enabling early detection and prevention of fraud. Experiments on the TeleAntiFraud-Bench demonstrate that SAFE-QAQ achieves dramatic improvements over existing methods in multiple key dimensions, including accuracy, inference efficiency, and real-time processing capabilities. Currently deployed and analyzing over 70,000 calls daily, SAFE-QAQ effectively automates complex fraud detection, reducing human workload and financial losses. Code: https://anonymous.4open.science/r/SAFE-QAQ.

</details>


### [607] [Diffusion Timbre Transfer Via Mutual Information Guided Inpainting](https://arxiv.org/abs/2601.01294)
*Ching Ho Lee,Javier Nistal,Stefan Lattner,Marco Pasini,George Fazekas*

Main category: cs.SD

TL;DR: The paper introduces a lightweight inference-time procedure for timbre transfer in music audio using a pre-trained latent diffusion model.


<details>
  <summary>Details</summary>
Motivation: To enable timbre transfer in music audio without additional training while preserving melodic and rhythmic structure.

Method: They implement dimension-wise noise injection targeting latent channels associated with instrument identity and an early-step clamping mechanism during reverse diffusion.

Result: The proposed method achieves effective timbre transfer while maintaining the structural integrity of melodies and rhythms.

Conclusion: Simple inference-time controls are sufficient for guiding pre-trained models in music style-transfer applications, offering a balance between timbral changes and structural fidelity.

Abstract: We study timbre transfer as an inference-time editing problem for music audio. Starting from a strong pre-trained latent diffusion model, we introduce a lightweight procedure that requires no additional training: (i) a dimension-wise noise injection that targets latent channels most informative of instrument identity, and (ii) an early-step clamping mechanism that re-imposes the input's melodic and rhythmic structure during reverse diffusion. The method operates directly on audio latents and is compatible with text/audio conditioning (e.g., CLAP). We discuss design choices,analyze trade-offs between timbral change and structural preservation, and show that simple inference-time controls can meaningfully steer pre-trained models for style-transfer use cases.

</details>


### [608] [UltraEval-Audio: A Unified Framework for Comprehensive Evaluation of Audio Foundation Models](https://arxiv.org/abs/2601.01373)
*Qundong Shi,Jie Zhou,Biyuan Lin,Junbo Cui,Guoyang Zeng,Yixuan Zhou,Ziyang Wang,Xin Liu,Zhen Luo,Yudong Wang,Zhiyuan Liu*

Main category: cs.SD

TL;DR: The paper introduces UltraEval-Audio, a unified evaluation framework for audio foundation models, addressing challenges in fair evaluation, codec methodologies, and language diversity.


<details>
  <summary>Details</summary>
Motivation: To overcome issues in audio evaluation, such as the lack of a unified framework, inadequate codec evaluation methods, and the dominance of English-centric benchmarks.

Method: The authors present UltraEval-Audio, a modular framework supporting 10 languages, 14 task categories, 24 mainstream models, and 36 benchmarks. It includes one-command evaluations, leaderboards, and new Chinese benchmarks for enhanced assessment.

Result: UltraEval-Audio integrates a unified framework with innovative evaluation metrics for codecs and supports comprehensive cross-model comparisons for audio understanding and generation tasks.

Conclusion: UltraEval-Audio provides an efficient, transparent, and fair platform that can accelerate academia and industry advancements in audio model research and development.

Abstract: The development of audio foundation models has accelerated rapidly since the emergence of GPT-4o. However, the lack of comprehensive evaluation has become a critical bottleneck for further progress in the field, particularly in audio generation. Current audio evaluation faces three major challenges: (1) audio evaluation lacks a unified framework, with datasets and code scattered across various sources, hindering fair and efficient cross-model comparison;(2) audio codecs, as a key component of audio foundation models, lack a widely accepted and holistic evaluation methodology; (3) existing speech benchmarks are heavily reliant on English, making it challenging to objectively assess models' performance on Chinese. To address the first issue, we introduce UltraEval-Audio, a unified evaluation framework for audio foundation models, specifically designed for both audio understanding and generation tasks. UltraEval-Audio features a modular architecture, supporting 10 languages and 14 core task categories, while seamlessly integrating 24 mainstream models and 36 authoritative benchmarks. To enhance research efficiency, the framework provides a one-command evaluation feature, accompanied by real-time public leaderboards. For the second challenge, UltraEval-Audio adopts a novel comprehensive evaluation scheme for audio codecs, evaluating performance across three key dimensions: semantic accuracy, timbre fidelity, and acoustic quality. To address the third issue, we propose two new Chinese benchmarks, SpeechCMMLU and SpeechHSK, designed to assess Chinese knowledge proficiency and language fluency. We wish that UltraEval-Audio will provide both academia and industry with a transparent, efficient, and fair platform for comparison of audio models. Our code, benchmarks, and leaderboards are available at https://github.com/OpenBMB/UltraEval-Audio.

</details>


### [609] [MOSS Transcribe Diarize: Accurate Transcription with Speaker Diarization](https://arxiv.org/abs/2601.01554)
*Donghua Yu,Zhengyuan Lin,Chen Yang,Yiyang Zhang,Zhaoye Fei,Hanfu Chen,Jingqi Chen,Ke Chen,Qinyuan Cheng,Liwei Fan,Yi Jiang,Jie Zhu,Muchen Li,Shimin Li,Wenxuan Wang,Yang Wang,Zhe Xu,Yitian Gong,Yuqian Zhang*

Main category: cs.SD

TL;DR: The paper introduces a model, MOSS Transcribe Diarize, for Speaker-Attributed, Time-Stamped Transcription (SATS) that combines transcription and speaker timing attribution in an end-to-end manner, addressing limitations of current systems.


<details>
  <summary>Details</summary>
Motivation: Current SATS systems struggle with limited context capabilities, weak memory for speakers over long durations, and lack of timestamps, creating a gap for more robust and comprehensive solutions.

Method: The authors propose a unified multimodal large language model named MOSS Transcribe Diarize. It is trained on extensive real-world data, supports a context window of 128k tokens—enabling processing of up to 90-minute inputs—and performs end-to-end SATS.

Result: The model exceeds the performance of state-of-the-art commercial systems on several benchmarks, demonstrating scalability and robust generalization.

Conclusion: MOSS Transcribe Diarize proves to be a significant advancement in SATS, providing a comprehensive, scalable, and efficient solution for transcription and speaker diarization.

Abstract: Speaker-Attributed, Time-Stamped Transcription (SATS) aims to transcribe what is said and to precisely determine the timing of each speaker, which is particularly valuable for meeting transcription. Existing SATS systems rarely adopt an end-to-end formulation and are further constrained by limited context windows, weak long-range speaker memory, and the inability to output timestamps. To address these limitations, we present MOSS Transcribe Diarize, a unified multimodal large language model that jointly performs Speaker-Attributed, Time-Stamped Transcription in an end-to-end paradigm. Trained on extensive real wild data and equipped with a 128k context window for up to 90-minute inputs, MOSS Transcribe Diarize scales well and generalizes robustly. Across comprehensive evaluations, it outperforms state-of-the-art commercial systems on multiple public and in-house benchmarks.

</details>


### [610] [MM-Sonate: Multimodal Controllable Audio-Video Generation with Zero-Shot Voice Cloning](https://arxiv.org/abs/2601.01568)
*Chunyu Qiang,Jun Wang,Xiaopeng Wang,Kang Yin,Yuxin Guo,Xijuan Zeng,Nan Li,Zihan Li,Yuzhe Liang,Ziyu Zhang,Teng Ma,Yushen Chen,Zhongliang Liu,Feng Deng,Chen Zhang,Pengfei Wan*

Main category: cs.SD

TL;DR: MM-Sonate is presented as a multimodal framework for joint audio-video generation with zero-shot voice cloning capabilities, achieving state-of-the-art synchronization and intelligibility.


<details>
  <summary>Details</summary>
Motivation: Current unified models for joint audio-video generation fail at fine-grained acoustic control and identity-preserving speech, and lack the ability for seamless zero-shot voice cloning.

Method: The paper introduces MM-Sonate, utilizing unified instruction-phoneme inputs for linguistic and temporal alignment, a timbre injection mechanism for separating speaker identity and content, and a noise-based negative conditioning strategy for improved acoustic fidelity.

Result: Empirical results show MM-Sonate outperforms existing methods in benchmarks for lip synchronization, speech intelligibility, and matches specialized Text-to-Speech systems in voice cloning fidelity.

Conclusion: MM-Sonate sets a new benchmark in joint audio-video generation, addressing key limitations of previous methods and proving itself effective for fine-grained synchronized multisensory content creation.

Abstract: Joint audio-video generation aims to synthesize synchronized multisensory content, yet current unified models struggle with fine-grained acoustic control, particularly for identity-preserving speech. Existing approaches either suffer from temporal misalignment due to cascaded generation or lack the capability to perform zero-shot voice cloning within a joint synthesis framework. In this work, we present MM-Sonate, a multimodal flow-matching framework that unifies controllable audio-video joint generation with zero-shot voice cloning capabilities. Unlike prior works that rely on coarse semantic descriptions, MM-Sonate utilizes a unified instruction-phoneme input to enforce strict linguistic and temporal alignment. To enable zero-shot voice cloning, we introduce a timbre injection mechanism that effectively decouples speaker identity from linguistic content. Furthermore, addressing the limitations of standard classifier-free guidance in multimodal settings, we propose a noise-based negative conditioning strategy that utilizes natural noise priors to significantly enhance acoustic fidelity. Empirical evaluations demonstrate that MM-Sonate establishes new state-of-the-art performance in joint generation benchmarks, significantly outperforming baselines in lip synchronization and speech intelligibility, while achieving voice cloning fidelity comparable to specialized Text-to-Speech systems.

</details>


### [611] [DARC: Drum accompaniment generation with fine-grained rhythm control](https://arxiv.org/abs/2601.02357)
*Trey Brosnan*

Main category: cs.SD

TL;DR: The paper introduces DARC, a generative model improving drum accompaniment creation by enabling rhythm control and context awareness.


<details>
  <summary>Details</summary>
Motivation: Existing music tools lack simultaneous support for structural control and stylistic flexibility, limiting rapid prototyping.

Method: DARC enhances the STAGE drum stem generator through parameter-efficient fine-tuning to enable control via rhythm prompts and context awareness.

Result: The model successfully combines musical context conditioning and precise rhythm control for flexible drum stem generation.

Conclusion: DARC addresses key limitations in generative music tools by providing enhanced control and adaptability for drum accompaniment creation.

Abstract: In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [612] [Feature-based Inversion of 2.5D Controlled Source Electromagnetic Data using Generative Priors](https://arxiv.org/abs/2601.02145)
*Hongyu Zhou,Haoran Sun,Rui Guo,Maokun Li,Fan Yang,Shenheng Xu*

Main category: physics.geo-ph

TL;DR: The study proposes a method leveraging generative priors with variational autoencoders for improved 2.5D mCSEM data inversion, ensuring better reconstruction accuracy and adaptability.


<details>
  <summary>Details</summary>
Motivation: To enhance controlled source marine electromagnetic (mCSEM) data inversion by integrating prior knowledge without relying on black-box neural network methods.

Method: A 2.5D finite difference modeling approach is employed to simulate horizontal electric dipole (HED) responses in conjunction with variational autoencoders (VAEs) that constrain the inversion process by enforcing prior knowledge during iterative updates using the Gauss Newton method.

Result: Numerical and field tests confirm improved reconstruction accuracy, better incorporation of prior information, and strong generalization capabilities of the proposed method.

Conclusion: The integration of a plug-and-play VAE framework provides precise control over inversion, adapts flexibly to various configurations, and enhances inversion accuracy.

Abstract: In this study, we investigate feature-based 2.5D controlled source marine electromagnetic (mCSEM) data inversion using generative priors. Two-and-half dimensional modeling using finite difference method (FDM) is adopted to compute the response of horizontal electric dipole (HED) excitation. Rather than using a neural network to approximate the entire inverse mapping in a black-box manner, we adopt a plug-andplay strategy in which a variational autoencoder (VAE) is used solely to learn prior information on conductivity distributions. During the inversion process, the conductivity model is iteratively updated using the Gauss Newton method, while the model space is constrained by projections onto the learned VAE decoder. This framework preserves explicit control over data misfit and enables flexible adaptation to different survey configurations. Numerical and field experiments demonstrate that the proposed approach effectively incorporates prior information, improves reconstruction accuracy, and exhibits good generalization performance.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [613] [Improving the Graph Challenge Reference Implementation](https://arxiv.org/abs/2601.00974)
*Inna Voloshchuk,Hayden Jananthan,Chansup Byun,Jeremy Kepner*

Main category: cs.NI

TL;DR: This paper improves the implementation for the Anonymized Network Sensing Graph Challenge, reducing code size by 67%, while enhancing performance and scalability using distributed programming libraries.


<details>
  <summary>Details</summary>
Motivation: To improve the clarity, adaptability, and performance of the GraphBLAS reference code used in the network sensing graph challenge, enabling better utility for participants.

Method: Refactored the original Python code into fewer, clearer modules, reduced its code size, and added parallelism using distributed array programming libraries like pMatlab and pPython.

Result: Achieved a 67% reduction in code size while preserving full functionality and demonstrated scalable performance for traffic matrix analysis.

Conclusion: The optimized implementation enhances the Graph Challenge by offering a more efficient foundation for future participants, showcasing significant improvements in scalability and usability.

Abstract: The MIT/IEEE/Amazon Graph Challenge provides a venue for individuals and teams to showcase new innovations in large-scale graph and sparse data analysis. The Anonymized Network Sensing Graph Challenge processes over 100 billion network packets to construct privacy-preserving traffic matrices, with a GraphBLAS reference implementation demonstrating how hypersparse matrices can be applied to this problem. This work presents a refactoring and benchmarking of a section of the reference code to improve clarity, adaptability, and performance. The original Python implementation spanning approximately 1000 lines across 3 files has been streamlined to 325 lines across two focused modules, achieving a 67% reduction in code size while maintaining full functionality. Using pMatlab and pPython distributed array programming libraries, the addition of parallel maps allowed for parallel benchmarking of the data. Scalable performance is demonstrated for large-scale summation and analysis of traffic matrices. The resulting implementation increases the potential impact of the Graph Challenge by providing a clear and efficient foundation for participants.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [614] [Hunting for "Oddballs" with Machine Learning: Detecting Anomalous Exoplanets Using a Deep-Learned Low-Dimensional Representation of Transit Spectra with Autoencoders](https://arxiv.org/abs/2601.02324)
*Alexander Roman,Emilie Panek,Roy T. Forestano,Eyup B. Unlu,Katia Matcheva,Konstantin T. Matchev*

Main category: astro-ph.EP

TL;DR: The paper investigates autoencoder-driven anomaly detection for identifying unconventional exoplanet atmospheres, demonstrating the effectiveness of analyzing latent space with K-means clustering.


<details>
  <summary>Details</summary>
Motivation: To identify chemically anomalous exoplanet atmospheres efficiently using machine learning, particularly in scenarios with significant noise, avoiding computational overheads of exhaustive retrievals.

Method: Tested four anomaly detection strategies (Autoencoder Reconstruction Loss, 1 class-SVM, K-means Clustering, LOF) on an exoplanet spectral dataset, analyzing their performance in both raw spectral and autoencoder latent space under varying noise conditions.

Result: K-means Clustering in latent space performed best with robust anomaly detection up to 30 ppm noise and remained viable at 50 ppm. The methods in raw spectral space degraded significantly under noise.

Conclusion: Autoencoder-driven latent space offers a computationally efficient and robust way to detect chemically anomalous exoplanet atmospheres, especially under realistic noise conditions.

Abstract: This study explores the application of autoencoder-based machine learning techniques for anomaly detection to identify exoplanet atmospheres with unconventional chemical signatures using a low-dimensional data representation. We use the Atmospheric Big Challenge (ABC) database, a publicly available dataset with over 100,000 simulated exoplanet spectra, to construct an anomaly detection scenario by defining CO2-rich atmospheres as anomalies and CO2-poor atmospheres as the normal class. We benchmarked four different anomaly detection strategies: Autoencoder Reconstruction Loss, One-Class Support Vector Machine (1 class-SVM), K-means Clustering, and Local Outlier Factor (LOF). Each method was evaluated in both the original spectral space and the autoencoder's latent space using Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) metrics. To test the performance of the different methods under realistic conditions, we introduced Gaussian noise levels ranging from 10 to 50 ppm. Our results indicate that anomaly detection is consistently more effective when performed within the latent space across all noise levels. Specifically, K-means clustering in the latent space emerged as a stable and high-performing method. We demonstrate that this anomaly detection approach is robust to noise levels up to 30 ppm (consistent with realistic space-based observations) and remains viable even at 50 ppm when leveraging latent space representations. On the other hand, the performance of the anomaly detection methods applied directly in the raw spectral space degrades significantly with increasing the level of noise. This suggests that autoencoder-driven dimensionality reduction offers a robust methodology for flagging chemically anomalous targets in large-scale surveys where exhaustive retrievals are computationally prohibitive.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [615] [Latent Space Element Method](https://arxiv.org/abs/2601.01741)
*Seung Whan Chung,Youngsoo Choi,Christopher Miller,H. Keo Springer,Kyle T. Sullivan*

Main category: math.DS

TL;DR: This paper introduces the Latent Space Element Method (LSEM), a modular surrogate solver scaling from small to large domains without direct PDE operator access.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create scalable surrogate solvers that can extend from small training domains to larger domains while maintaining accuracy and without relying on intensive computational PDE operator access.

Method: The Latent Space Element Method (LSEM) uses element-based latent surrogate models trained on local patches. Neighboring models are coupled in latent space with directional interactions and blended for global predictions without Schwarz iterations or residual evaluations.

Result: LSEM accurately predicts results for large-scale spatial domains, demonstrated on the 1D Burgers and Korteweg-de Vries equations, achieving accuracy for domains larger than training ones.

Conclusion: LSEM provides a scalable, reusable, and interpretable framework for building robust surrogate solvers using locally trained latent models.

Abstract: How can we build surrogate solvers that train on small domains but scale to larger ones without intrusive access to PDE operators? Inspired by the Data-Driven Finite Element Method (DD-FEM) framework for modular data-driven solvers, we propose the Latent Space Element Method (LSEM), an element-based latent surrogate assembly approach in which a learned subdomain ("element") model can be tiled and coupled to form a larger computational domain. Each element is a LaSDI latent ODE surrogate trained from snapshots on a local patch, and neighboring elements are coupled through learned directional interaction terms in latent space, avoiding Schwarz iterations and interface residual evaluations. A smooth window-based blending reconstructs a global field from overlapping element predictions, yielding a scalable assembled latent dynamical system. Experiments on the 1D Burgers and Korteweg-de Vries equations show that LSEM maintains predictive accuracy while scaling to spatial domains larger than those seen in training. LSEM offers an interpretable and extensible route toward foundation-model surrogate solvers built from reusable local models.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [616] [Multiscale replay: A robust algorithm for stochastic variational inequalities with a Markovian buffer](https://arxiv.org/abs/2601.01502)
*Milind Nakul,Tianjiao Li,Ashwin Pananjady*

Main category: math.OC

TL;DR: This paper introduces the Multiscale Experience Replay (MER) algorithm to address convergence issues in stochastic variational inequalities with Markov-chain-generated samples.


<details>
  <summary>Details</summary>
Motivation: To overcome biases in serial sampling schemes when dealing with Markov-chain-generated samples in stochastic variational inequalities, aiming to accelerate convergence.

Method: The proposed MER algorithm employs multi-scale sampling from a memory buffer, emulating i.i.d. sample-based VI algorithms, eliminating bias, and achieving efficient convergence.

Result: MER accelerates convergence in iteration complexity without requiring knowledge of Markov chain mixing times. Applications include temporal difference learning and training generalized linear models with dependent data.

Conclusion: MER provides an efficient and robust solution for handling stochastic VI problems with dependent samples, showing potential in various real-world applications.

Abstract: We introduce the Multiscale Experience Replay (MER) algorithm for solving a class of stochastic variational inequalities (VIs) in settings where samples are generated from a Markov chain and we have access to a memory buffer to store them. Rather than uniformly sampling from the buffer, MER utilizes a multi-scale sampling scheme to emulate the behavior of VI algorithms designed for independent and identically distributed samples, overcoming bias in the de facto serial scheme and thereby accelerating convergence. Notably, unlike standard sample-skipping variants of serial algorithms, MER is robust in that it achieves this acceleration in iteration complexity whenever possible, and without requiring knowledge of the mixing time of the Markov chain. We also discuss applications of MER, particularly in policy evaluation with temporal difference learning and in training generalized linear models with dependent data.

</details>


### [617] [Gradient-Free Approaches is a Key to an Efficient Interaction with Markovian Stochasticity](https://arxiv.org/abs/2601.01160)
*Boris Prokhorov,Semyon Chebykin,Alexander Gasnikov,Aleksandr Beznosikov*

Main category: math.OC

TL;DR: The paper introduces a derivative-free method for solving stochastic optimization problems with Markovian noise in both convex and non-convex settings, achieving optimal convergence independent of noise mixing time for low dimensions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address stochastic optimization problems with Markovian noise using a zero-order oracle, avoiding expensive first-order oracle evaluations.

Method: The paper presents a randomized batching derivative-free optimization method under zero-order oracle settings, analyzing both one-point and two-point feedback with convergence estimates.

Result: The method achieves convergence estimates independent of the noise mixing time τ when τ is less than the problem dimension d, and establishes upper and lower bounds for optimality.

Conclusion: The proposed method efficiently handles Markovian stochasticity in optimization, offering a derivative-free approach that is optimal and avoids computationally costly first-order oracles.

Abstract: This paper deals with stochastic optimization problems involving Markovian noise with a zero-order oracle. We present and analyze a novel derivative-free method for solving such problems in strongly convex smooth and non-smooth settings with both one-point and two-point feedback oracles. Using a randomized batching scheme, we show that when mixing time $τ$ of the underlying noise sequence is less than the dimension of the problem $d$, the convergence estimates of our method do not depend on $τ$. This observation provides an efficient way to interact with Markovian stochasticity: instead of invoking the expensive first-order oracle, one should use the zero-order oracle. Finally, we complement our upper bounds with the corresponding lower bounds. This confirms the optimality of our results.

</details>


### [618] [Stochastic Control Methods for Optimization](https://arxiv.org/abs/2601.01248)
*Jinniao Qiu*

Main category: math.OC

TL;DR: This paper develops a stochastic control approach for global optimization in Euclidean and Wasserstein spaces, demonstrates convergence to the global minimum, and provides numerical validation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in global optimization by proposing a new methodological framework using stochastic control, which can handle optimization in both Euclidean spaces and probability measure spaces.

Method: The study reformulates the minimization problem as regularized stochastic control problems and analyzes related mathematical systems, like the Hamilton-Jacobi-Bellman equations, via dynamic programming. It introduces probabilistic representations using the Cole--Hopf transform, Feynman--Kac formula, and develops numerical algorithms based on Monte Carlo methods.

Result: The method demonstrates convergence of the control problem to the global minimum as regularization vanishes, and simulations showcase the algorithm's effectiveness, with experimental evidence supporting theoretical convergence rates.

Conclusion: The proposed stochastic control framework effectively achieves global optimization, bridging theory and practical implementation while offering convergence guarantees and numeric validation.

Abstract: In this work, we investigate a stochastic control framework for global optimization over both finite-dimensional Euclidean spaces and the Wasserstein space of probability measures. In the Euclidean setting, the original minimization problem is approximated by a family of regularized stochastic control problems; using dynamic programming, we analyze the associated Hamilton--Jacobi--Bellman equations and obtain tractable representations via the Cole--Hopf transform and the Feynman--Kac formula. For optimization over probability measures, we formulate a regularized mean-field control problem characterized by a master equation, and further approximate it by controlled $N$-particle systems. We establish that, as the regularization parameter tends to zero (and as the particle number tends to infinity for the optimization over probability measures), the value of the control problem converges to the global minimum of the original objective. Building on the resulting probabilistic representations, Monte Carlo-based numerical schemes are proposed and numerical experiments are reported to illustrate the practical performance of the methods and to support the theoretical convergence rates.

</details>


### [619] [Concave Certificates: Geometric Framework for Distributionally Robust Risk and Complexity Analysis](https://arxiv.org/abs/2601.01311)
*Hong T. M. Chu*

Main category: math.OC

TL;DR: This paper proposes a geometric framework to provide a tighter certification for distributionally robust optimization using concave certificate and extends its application to literature on adversarial and empirical complexities.


<details>
  <summary>Details</summary>
Motivation: Existing methods for certifying DR optimization are often overly conservative or rely on first-order approximations, which are inadequate for certain losses like non-Lipschitz and non-differentiable ones.

Method: The authors present a concave certificate derived from the least concave majorants of the growth rate function, enabling deterministic complexity analysis and addressing structure-specific gaps in deep neural network analysis.

Result: The results include eliminating dependencies on structural parameters such as input diameter and network size, validated by experiments on classification and regression tasks.

Conclusion: The proposed concave certificate provides a comprehensive and practical approach for analyzing DR optimization and neural networks, with theoretical insights confirmed by empirical validation.

Abstract: Distributionally Robust (DR) optimization aims to certify worst-case risk within a Wasserstein uncertainty set. Current certifications typically rely either on global Lipschitz bounds, which are often conservative, or on local gradient information, which provides only a first-order approximation. This paper introduces a novel geometric framework based on the least concave majorants of the growth rate function. Our proposed concave certificate establishes a tight bound of DR risk that remains applicable to non-Lipschitz and non-differentiable losses. We extend this framework to complexity analysis, introducing a deterministic bound that complements standard statistical generalization bound. Furthermore, we utilize this certificate to bound the gap between adversarial and empirical Rademacher complexity, demonstrating that dependencies on input diameter, network width, and depth can be eliminated. For practical application in deep learning, we introduce the adversarial score as a tractable relaxation of the concave certificate that enables efficient and layer-wise analysis of neural networks. We validate our theoretical results in various numerical experiments on classification and regression tasks on real-world data.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [620] [Multi-fidelity graph-based neural networks architectures to learn Navier-Stokes solutions on non-parametrized 2D domains](https://arxiv.org/abs/2601.02157)
*Francesco Songia,Raoul Sallé de Chou,Hugues Talbot,Irene Vignon-Clementel*

Main category: physics.flu-dyn

TL;DR: The paper presents a graph-based learning framework combining physical knowledge and advanced AI techniques for predicting stationary Navier--Stokes solutions in complex geometries.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of predicting stationary Navier--Stokes solutions in non-parametrized 2D geometries by integrating domain physics into advanced machine learning methods.

Method: The approach uses a multi-fidelity strategy with successive approximations, starting from reduced-order models to full Navier--Stokes solutions, employing graph neural networks combined with Transformers and a tailored Mamba architecture.

Result: The framework achieves accurate predictions while embedding physical constraints and ensuring computational efficiency, notably through Mamba's adaptation and enriched graph convolutions.

Conclusion: The integration of physical knowledge and AI techniques not only enhances learning accuracy but also demonstrates potential for fluid dynamics applications, with reduced computational costs and adherence to governing equations.

Abstract: We propose a graph-based, multi-fidelity learning framework for the prediction of stationary Navier--Stokes solutions in non-parametrized two-dimensional geometries. The method is designed to guide the learning process through successive approximations, starting from reduced-order and full Stokes models, and progressively approaching the Navier--Stokes solution. To effectively capture both local and long-range dependencies in the velocity and pressure fields, we combine graph neural networks with Transformer and Mamba architectures. While Transformers achieve the highest accuracy, we show that Mamba can be successfully adapted to graph-structured data through an unsupervised node-ordering strategy. The Mamba approach significantly reduces computational cost while maintaining performance. Physical knowledge is embedded directly into the architecture through an encoding -- processing -- physics informed decoding pipeline. Derivatives are computed through algebraic operators constructed via the Weighted Least Squares method. The flexibility of these operators allows us not only to make the output obey the governing equations, but also to constrain selected hidden features to satisfy mass conservation. We introduce additional physical biases through an enriched graph convolution with the same differential operators describing the PDEs. Overall, we successfully guide the learning process by physical knowledge and fluid dynamics insights, leading to more regular and accurate predictions

</details>


### [621] [Efficient temporal prediction of compressible flows in irregular domains using Fourier neural operators](https://arxiv.org/abs/2601.01922)
*Yifan Nie,Qiaoxin Li*

Main category: physics.flu-dyn

TL;DR: This study employed Fourier Neural Operator (FNO) to simulate high-speed compressible fluid dynamics in irregular flow fields effectively and efficiently, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of simulating high-speed compressible fluid flows in irregular domains efficiently and accurately, as traditional numerical methods are computationally expensive.

Method: The study uses Fourier Neural Operator (FNO), reformats irregular flow point sets for FNO compatibility, integrates temporal bundling in a recurrent neural network, and applies a composite loss function for balanced prediction errors.

Result: Experimental evaluation on three irregular flow fields shows that the proposed method achieves significantly better computational efficiency and lower relative $L_2$ errors (0.78%, 0.57%, 0.35% for $p$, $T$, and $\mathbf{u}$) compared to conventional approaches.

Conclusion: The proposed FNO-based method can efficiently and accurately simulate the evolution of high-speed compressible flows in irregular domains, marking it as an improvement over traditional techniques.

Abstract: This paper investigates the temporal evolution of high-speed compressible fluids in irregular flow fields using the Fourier Neural Operator (FNO). We reconstruct the irregular flow field point set into sequential format compatible with FNO input requirements, and then embed temporal bundling technique within a recurrent neural network (RNN) for multi-step prediction. We further employ a composite loss function to balance errors across different physical quantities. Experiments are conducted on three different types of irregular flow fields, including orthogonal and non-orthogonal grid configurations. Then we comprehensively analyze the physical component loss curves, flow field visualizations, and physical profiles. Results demonstrate that our approach significantly surpasses traditional numerical methods in computational efficiency while achieving high accuracy, with maximum relative $L_2$ errors of (0.78, 0.57, 0.35)% for ($p$, $T$, $\mathbf{u}$) respectively. This verifies that the method can efficiently and accurately simulate the temporal evolution of high-speed compressible flows in irregular domains.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [622] [LLM Collusion](https://arxiv.org/abs/2601.01279)
*Shengyu Cao,Ming Hu*

Main category: econ.TH

TL;DR: The paper explores how large language models (LLMs) used by duopolies in pricing can induce collusion under specific configuration conditions, such as model fidelity and retraining frequency.


<details>
  <summary>Details</summary>
Motivation: To investigate whether LLMs, when utilized as pricing tools, could lead to price collusion in a duopoly context, influenced by their internal propensity and output-fidelity characteristics.

Method: The study analyzes LLM behavior by modeling a duopoly situation. It examines the impact of LLM configuration parameters like bias (towards high prices) and fidelity, and explores factors like batch sizes and retraining frequency to understand stability and long-term outcomes.

Result: They identify a critical threshold for output fidelity that determines whether competitive pricing or collusion emerges. With higher fidelity or infrequent retraining, collusion becomes increasingly likely, whereas lower fidelity leads to competitive pricing.

Conclusion: LLMs configured with high fidelity and robustness are prone to induce tacit or full collusion, especially as batch sizes grow during retraining. This can have significant implications for market regulation and the use of automated decision-making tools.

Abstract: We study how delegating pricing to large language models (LLMs) can facilitate collusion in a duopoly when both sellers rely on the same pre-trained model. The LLM is characterized by (i) a propensity parameter capturing its internal bias toward high-price recommendations and (ii) an output-fidelity parameter measuring how tightly outputs track that bias; the propensity evolves through retraining. We show that configuring LLMs for robustness and reproducibility can induce collusion via a phase transition: there exists a critical output-fidelity threshold that pins down long-run behavior. Below it, competitive pricing is the unique long-run outcome. Above it, the system is bistable, with competitive and collusive pricing both locally stable and the realized outcome determined by the model's initial preference. The collusive regime resembles tacit collusion: prices are elevated on average, yet occasional low-price recommendations provide plausible deniability. With perfect fidelity, full collusion emerges from any interior initial condition. For finite training batches of size $b$, infrequent retraining (driven by computational costs) further amplifies collusion: conditional on starting in the collusive basin, the probability of collusion approaches one as $b$ grows, since larger batches dampen stochastic fluctuations that might otherwise tip the system toward competition. The indeterminacy region shrinks at rate $O(1/\sqrt{b})$.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [623] [Dynamic Accuracy Estimation in a Wi-Fi-based Positioning System](https://arxiv.org/abs/2601.00999)
*Marcin Kolakowski,Vitomir Djaja-Josko*

Main category: eess.SP

TL;DR: The paper proposes a dynamic accuracy estimation method validated in a Wi-Fi-based indoor positioning system, using various regression methods for localization error prediction.


<details>
  <summary>Details</summary>
Motivation: Improve prediction of localization errors in indoor positioning using measurement data.

Method: Experimental validation of dynamic accuracy estimation with linear regression, random forest, k-nearest neighbors, and neural networks.

Result: Random forest regression achieved the best error estimation accuracy (mean absolute error: 0.72 m).

Conclusion: Random forest is effective for dynamic accuracy estimation, enhancing reliability in indoor positioning systems.

Abstract: The paper presents a concept of a dynamic accuracy estimation method, in which the localization errors are derived based on the measurement results used by the positioning algorithm. The concept was verified experimentally in a Wi\nobreakdash-Fi based indoor positioning system, where several regression methods were tested (linear regression, random forest, k-nearest neighbors, and neural networks). The highest positioning error estimation accuracy was achieved for random forest regression, with a mean absolute error of 0.72 m.

</details>


### [624] [NeuroSSM: Multiscale Differential State-Space Modeling for Context-Aware fMRI Analysis](https://arxiv.org/abs/2601.01229)
*Furkan Genç,Boran İsmet Macun,Sait Sarper Özaslan,Emine U. Saritas,Tolga Çukur*

Main category: eess.SP

TL;DR: The paper introduces NeuroSSM, a multiscale selective state-space architecture for analyzing raw fMRI signals, capturing both fast and slow temporal dynamics efficiently.


<details>
  <summary>Details</summary>
Motivation: Most existing deep learning models for fMRI analysis cannot efficiently handle and represent the multiscale temporal dynamics of BOLD signals, limiting sensitivity to both fast transient dynamics and slow global trends.

Method: NeuroSSM employs a multiscale state-space backbone for concurrent modeling of fast and slow dynamics and includes a parallel differencing branch to capture transient state changes in raw fMRI sequences.

Result: Experiments show that NeuroSSM provides competitive performance and efficiency compared to state-of-the-art fMRI analysis methods on both clinical and non-clinical datasets.

Conclusion: By addressing limitations of prior models, NeuroSSM enables more accurate, efficient, and comprehensive analysis of fMRI signals, benefiting neuroscience and cognitive research.

Abstract: Accurate fMRI analysis requires sensitivity to temporal structure across multiple scales, as BOLD signals encode cognitive processes that emerge from fast transient dynamics to slower, large-scale fluctuations. Existing deep learning (DL) approaches to temporal modeling face challenges in jointly capturing these dynamics over long fMRI time series. Among current DL models, transformers address long-range dependencies by explicitly modeling pairwise interactions through attention, but the associated quadratic computational cost limits effective integration of temporal dependencies across long fMRI sequences. Selective state-space models (SSMs) instead model long-range temporal dependencies implicitly through latent state evolution in a dynamical system, enabling efficient propagation of dependencies over time. However, recent SSM-based approaches for fMRI commonly operate on derived functional connectivity representations and employ single-scale temporal processing. These design choices constrain the ability to jointly represent fast transient dynamics and slower global trends within a single model. We propose NeuroSSM, a selective state-space architecture designed for end-to-end analysis of raw BOLD signals in fMRI time series. NeuroSSM addresses the above limitations through two complementary design components: a multiscale state-space backbone that captures fast and slow dynamics concurrently, and a parallel differencing branch that increases sensitivity to transient state changes. Experiments on clinical and non-clinical datasets demonstrate that NeuroSSM achieves competitive performance and efficiency against state-of-the-art fMRI analysis methods.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [625] [Benchmarking Quantum Data Center Architectures: A Performance and Scalability Perspective](https://arxiv.org/abs/2601.01353)
*Shahrooz Pouryousef,Eneet Kaur,Hassan Shapourian,Don Towsley,Ramana Kompella,Reza Nejabati*

Main category: quant-ph

TL;DR: The paper benchmarks four quantum data-center (QDC) architectures and evaluates their performance under realistic quantum hardware constraints by analyzing execution latency, resource contention, and scalability.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance limitations of single quantum processors by examining and comparing scalable distributed quantum computing (DQC) architectures, especially under realistic hardware constraints.

Method: The paper systematically benchmarks four QDC architectures (QFly, BCube, Clos, and Fat-Tree) by analyzing their performance for distributed quantum circuit execution, considering quantum-specific challenges like entanglement delays and teleportation-based non-local gate contention.

Result: The study shows that distributed quantum performance is influenced by topology, scheduling policies, and hardware constraints, and these factors work together in complex ways to shape overall execution performance.

Conclusion: This research provides guiding principles for designing scalable and high-performance QDC architectures, highlighting the importance of understanding the interplay of topology, scheduling, and physical-layer characteristics.

Abstract: Scalable distributed quantum computing (DQC) has motivated the design of multiple quantum data-center (QDC) architectures that overcome the limitations of single quantum processors through modular interconnection. While these architectures adopt fundamentally different design philosophies, their relative performance under realistic quantum hardware constraints remains poorly understood.
  In this paper, we present a systematic benchmarking study of four representative QDC architectures-QFly, BCube, Clos, and Fat-Tree-quantifying their impact on distributed quantum circuit execution latency, resource contention, and scalability. Focusing on quantum-specific effects absent from classical data-center evaluations, we analyze how optical-loss-induced Einstein-Podolsky-Rosen (EPR) pair generation delays, coherence-limited entanglement retry windows, and contention from teleportation-based non-local gates shape end-to-end execution performance. Across diverse circuit workloads, we evaluate how architectural properties such as path diversity and path length, and shared BSM (Bell State Measurement) resources interact with optical-switch insertion loss and reconfiguration delay. Our results show that distributed quantum performance is jointly shaped by topology, scheduling policies, and physical-layer parameters, and that these factors interact in nontrivial ways. Together, these insights provide quantitative guidance for the design of scalable and high-performance quantum data-center architectures for DQC.

</details>


### [626] [Cutting Quantum Circuits Beyond Qubits](https://arxiv.org/abs/2601.02064)
*Manav Seksaria,Anil Prabhakar*

Main category: quant-ph

TL;DR: The paper extends quantum circuit cutting to systems with mixed-dimensional qudits, enabling high-dimensional circuit simulation on fragmented hardware.


<details>
  <summary>Details</summary>
Motivation: To address the need for efficient simulation and execution of high-dimensional quantum circuits on heterogeneous systems with varying qudit dimensions.

Method: Non-local interactions are decomposed into tensor products of local generalised Gell-Mann matrices, allowing state reconstruction and execution across disconnected hardware fragments.

Result: Achieved exact state reconstruction with Total Variation Distance of 0 (within single-precision limits) and demonstrated significant memory savings in an 8-particle, dimension-8 system.

Conclusion: The framework is effective for quantum circuit cutting with heterogeneous qudits, offering both accuracy and resource efficiency in high-dimensional systems.

Abstract: We extend quantum circuit cutting to heterogeneous registers comprising mixed-dimensional qudits. By decomposing non-local interactions into tensor products of local generalised Gell-Mann matrices, we enable the simulation and execution of high-dimensional circuits on disconnected hardware fragments. We validate this framework on qubit--qutrit ($2$--$3$) interfaces, achieving exact state reconstruction with a Total Variation Distance of 0 within single-precision floating-point tolerance. Furthermore, we demonstrate the memory advantage in an 8-particle, dimension-8 system, reducing memory usage from 128 MB to 64 KB per circuit.

</details>


### [627] [Integrating Quantum Software Tools with(in) MLIR](https://arxiv.org/abs/2601.02062)
*Patrick Hopf,Erick Ochoa Lopez,Yannick Stade,Damian Rovara,Nils Quetschlich,Ioan Albert Florea,Josh Izaac,Robert Wille,Lukas Burgholzer*

Main category: quant-ph

TL;DR: The paper discusses leveraging MLIR for quantum software engineering and provides a guide to foster modular and interoperable quantum compilation. 


<details>
  <summary>Details</summary>
Motivation: To address the lack of interoperability in quantum software tools due to ad hoc and isolated developments.

Method: The authors use a case study linking PennyLane and the Munich Quantum Toolkit (MQT) to demonstrate integration and best practices with MLIR.

Result: The paper provides actionable steps, insights, and practical guidance on using MLIR for quantum software integration.

Conclusion: MLIR can serve as a unifying framework for quantum software tools, promoting modular, integrated stacks and easing development challenges.

Abstract: Compilers transform code into action. They convert high-level programs into executable hardware instructions - a crucial step in enabling reliable and scalable quantum computation. However, quantum compilation is still in its infancy, and many existing solutions are ad hoc, often developed independently and from scratch. The resulting lack of interoperability leads to significant missed potential, as quantum software tools remain isolated and cannot be seamlessly integrated into cohesive toolchains.
  The Multi-Level Intermediate Representation (MLIR) has addressed analogous challenges in the classical domain. It was developed within the LLVM project, which has long powered robust software stacks and enabled compilation across diverse software and hardware components, with particular importance in high-performance computing environments. However, MLIR's steep learning curve poses a significant barrier to entry, particularly in quantum computing, where much of the software stack is still predominantly built by experimentalists out of necessity rather than by experienced software engineers.
  This paper provides a practical and hands-on guide for quantum software engineers to overcome this steep learning curve. Through a concrete case study linking Xanadu's PennyLane framework with the Munich Quantum Toolkit (MQT), we outline actionable integration steps, highlight best practices, and share hard-earned insights from real-world development. This work aims to support quantum tool developers in navigating MLIR's complexities and to foster its adoption as a unifying bridge across a rapidly growing ecosystem of quantum software tools, ultimately guiding the development of more modular, interoperable, and integrated quantum software stacks.

</details>


### [628] [PauliEngine: High-Performant Symbolic Arithmetic for Quantum Operations](https://arxiv.org/abs/2601.02233)
*Leon Müller,Adelina Bärligea,Alexander Knapp,Jakob S. Kottmann*

Main category: quant-ph

TL;DR: PauliEngine is a C++ framework for fast and efficient operations on Pauli string manipulations, optimized for scalability in quantum computing.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the necessity for fast classical manipulation of qubit operators to ensure scalability in hybrid quantum computation processes.

Method: The authors developed PauliEngine using a binary symplectic representation and optimized bit-wise operations. It supports both numerical and symbolic coefficients and features a Python interface.

Result: PauliEngine achieves significant performance improvements in runtime benchmarks compared to existing quantum software implementations.

Conclusion: PauliEngine is a scalable and high-performance backend for operator-based quantum software, enhancing the efficiency of quantum computation workflows.

Abstract: Quantum computation is inherently hybrid, and fast classical manipulation of qubit operators is necessary to ensure scalability in quantum software. We introduce PauliEngine, a high-performance C++ framework that provides efficient primitives for Pauli string multiplication, commutators, symbolic phase tracking, and structural transformations. Built on a binary symplectic representation and optimized bit-wise operations, PauliEngine supports both numerical and symbolic coefficients and is accessible through a Python interface. Runtime benchmarks demonstrate substantial speedups over state-of-the-art implementations. PauliEngine provides a scalable backend for operator-based quantum software tools and simulations.

</details>


### [629] [Learning Relationship between Quantum Walks and Underdamped Langevin Dynamics](https://arxiv.org/abs/2601.01589)
*Yazhen Wang*

Main category: quant-ph

TL;DR: The paper explores the relationship between quantum walk-based search algorithms and underdamped Langevin dynamics in machine learning tasks, revealing asymptotic equivalence under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To understand how quantum and classical algorithms contribute to computational advancements, particularly in machine learning, by analyzing quantum walks and Langevin dynamics.

Method: The study evaluates the Le Cam deficiency distance to determine the equivalence between randomized quantum walks and underdamped Langevin dynamics, analyzing oscillatory behaviors and their computational impacts.

Result: The analysis finds that randomized quantum walks and underdamped Langevin dynamics are asymptotically equivalent, whereas non-randomized quantum walks are not due to oscillatory effects.

Conclusion: The findings provide insights into the mechanisms of quantum speedup and classical gradient acceleration, contributing to the development of more efficient computational algorithms for machine learning tasks.

Abstract: Fast computational algorithms are in constant demand, and their development has been driven by advances such as quantum speedup and classical acceleration. This paper intends to study search algorithms based on quantum walks in quantum computation and sampling algorithms based on Langevin dynamics in classical computation. On the quantum side, quantum walk-based search algorithms can achieve quadratic speedups over their classical counterparts. In classical computation, a substantial body of work has focused on gradient acceleration, with gradient-adjusted algorithms derived from underdamped Langevin dynamics providing quadratic acceleration over conventional Langevin algorithms.
  Since both search and sampling algorithms are designed to address learning tasks, we study learning relationship between coined quantum walks and underdamped Langevin dynamics. Specifically, we show that, in terms of the Le Cam deficiency distance, a quantum walk with randomization is asymptotically equivalent to underdamped Langevin dynamics, whereas the quantum walk without randomization is not asymptotically equivalent due to its high-frequency oscillatory behavior. We further discuss the implications of these equivalence and nonequivalence results for the computational and inferential properties of the associated algorithms in machine learning tasks. Our findings offer new insight into the relationship between quantum walks and underdamped Langevin dynamics, as well as the intrinsic mechanisms underlying quantum speedup and classical gradient acceleration.

</details>


### [630] [Random-Matrix-Induced Simplicity Bias in Over-parameterized Variational Quantum Circuits](https://arxiv.org/abs/2601.01877)
*Jun Qi,Chao-Han Huck Yang,Pin-Yu Chen,Min-Hsiu Hsieh*

Main category: quant-ph

TL;DR: Over-parameterization in variational quantum circuits can lead to poor trainability and generalization due to a simplicity bias in unstructured designs, but tensor-structured architectures offer a solution.


<details>
  <summary>Details</summary>
Motivation: To explain why over-parameterized variational quantum circuits often suffer from poor trainability and generalization.

Method: The study used tools from random matrix theory and concentration of measure to analyze function-class behavior and demonstrated the effectiveness of tensor-structured variational quantum circuits.

Result: Unstructured VQCs often collapse into a Haar-like universality class with simplicity bias and barren plateaus. Structured tensor-based VQCs prevent such collapse, maintaining variability and trainability.

Conclusion: Architectural inductive bias, such as tensor-structured designs, is crucial for improving trainability and generalization in variational quantum algorithms, addressing issues like barren plateaus and simplicity bias.

Abstract: Over-parameterization is commonly used to increase the expressivity of variational quantum circuits (VQCs), yet deeper and more highly parameterized circuits often exhibit poor trainability and limited generalization. In this work, we provide a theoretical explanation for this phenomenon from a function-class perspective. We show that sufficiently expressive, unstructured variational ansatze enter a Haar-like universality class in which both observable expectation values and parameter gradients concentrate exponentially with system size. As a consequence, the hypothesis class induced by such circuits collapses with high probability to a narrow family of near-constant functions, a phenomenon we term simplicity bias, with barren plateaus arising as a consequence rather than the root cause. Using tools from random matrix theory and concentration of measure, we rigorously characterize this universality class and establish uniform hypothesis-class collapse over finite datasets. We further show that this collapse is not unavoidable: tensor-structured VQCs, including tensor-network-based and tensor-hypernetwork parameterizations, lie outside the Haar-like universality class. By restricting the accessible unitary ensemble through bounded tensor rank or bond dimension, these architectures prevent concentration of measure, preserve output variability for local observables, and retain non-degenerate gradient signals even in over-parameterized regimes. Together, our results unify barren plateaus, expressivity limits, and generalization collapse under a single structural mechanism rooted in random-matrix universality, highlighting the central role of architectural inductive bias in variational quantum algorithms.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [631] [Scalable Data-Driven Reachability Analysis and Control via Koopman Operators with Conformal Coverage Guarantees](https://arxiv.org/abs/2601.01076)
*Devesh Nath,Haoran Yin,Glen Chou*

Main category: eess.SY

TL;DR: The paper proposes a scalable reachability framework using Koopman theory and neural networks for probabilistic safety verification of unknown nonlinear systems, demonstrating results in dynamic tasks.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety verification of nonlinear dynamical systems with unknown behaviors and efficient computation methods.

Method: Combination of Koopman theory, neural network lifting functions, reachable set calculations, NN verification tools, and conformal prediction for error bounds.

Result: Improved reachable set coverage, computational efficiency, and conservativeness in high-dimensional dynamical tasks like MuJoCo and quadcopters.

Conclusion: The framework reliably verifies safety of nonlinear systems with statistical guarantees, reusability, and computational scalability.

Abstract: We propose a scalable reachability-based framework for probabilistic, data-driven safety verification of unknown nonlinear dynamics. We use Koopman theory with a neural network (NN) lifting function to learn an approximate linear representation of the dynamics and design linear controllers in this space to enable closed-loop tracking of a reference trajectory distribution. Closed-loop reachable sets are efficiently computed in the lifted space and mapped back to the original state space via NN verification tools. To capture model mismatch between the Koopman dynamics and the true system, we apply conformal prediction to produce statistically-valid error bounds that inflate the reachable sets to ensure the true trajectories are contained with a user-specified probability. These bounds generalize across references, enabling reuse without recomputation. Results on high-dimensional MuJoCo tasks (11D Hopper, 28D Swimmer) and 12D quadcopters show improved reachable set coverage rate, computational efficiency, and conservativeness over existing methods.

</details>


### [632] [Sampling Strategy Design for Model Predictive Path Integral Control on Legged Robot Locomotion](https://arxiv.org/abs/2601.01409)
*Chuyuan Tao,Fanxin Wang,Haolong Jiang,Jia He,Yiyang Chen,Qinglei Bu*

Main category: eess.SY

TL;DR: This paper explores and evaluates various sampling strategies in the Model Predictive Path Integral (MPPI) control framework for legged robotics, with insights on control smoothness, task performance, robustness, and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in directly applying MPPI controllers to complex legged robotic systems and to understand the impact of sampling strategy design.

Method: The paper investigates structured control parameterization by comparing unstructured and spline-based sampling strategies through simulations on a quadruped robot.

Result: Extensive simulations reveal how different sampling strategies influence key metrics like control smoothness, performance, robustness, and sample efficiency.

Conclusion: The findings offer practical insights into how sampling design impacts the effective deployment of MPPI in legged robotic systems.

Abstract: Model Predictive Path Integral (MPPI) control has emerged as a powerful sampling-based optimal control method for complex, nonlinear, and high-dimensional systems. However, directly applying MPPI to legged robotic systems presents several challenges. This paper systematically investigates the role of sampling strategy design within the MPPI framework for legged robot locomotion. Based upon the idea of structured control parameterization, we explore and compare multiple sampling strategies within the framework, including both unstructured and spline-based approaches. Through extensive simulations on a quadruped robot platform, we evaluate how different sampling strategies affect control smoothness, task performance, robustness, and sample efficiency. The results provide new insights into the practical implications of sampling design for deploying MPPI on complex legged systems.

</details>


### [633] [Reliable Grid Forecasting: State Space Models for Safety-Critical Energy Systems](https://arxiv.org/abs/2601.01410)
*Jisoo Lee,Sunki Hong*

Main category: eess.SY

TL;DR: The study evaluates operational risk in grid load forecasting using a new asymmetric error evaluation framework instead of conventional accuracy metrics.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional symmetric error metrics in grid load forecasting, which fail to capture operational risks like under-predictions.

Method: The authors propose an evaluation framework combining Asymmetric MAPE, Under-Prediction Rate, and Reserve Margin and apply it to Mamba-based State Space Models on a weather-aligned dataset from the California grid.

Result: The study finds standard accuracy metrics inadequate for operational safety, associating forecast errors with temperature, and highlights the superior reliability of the S-Mamba model.

Conclusion: Weather-aware modeling outperforms traditional methods in risk evaluation, with S-Mamba setting a benchmark for reliable grid load forecasting.

Abstract: Accurate grid load forecasting is safety-critical: under-predictions risk supply shortfalls, while symmetric error metrics mask this operational asymmetry. We introduce a grid-specific evaluation framework--Asymmetric MAPE, Under-Prediction Rate, and Reserve Margin--that directly measures operational risk rather than statistical accuracy alone.
  Using this framework, we conduct a systematic evaluation of Mamba-based State Space Models for California grid forecasting on a weather-aligned CAISO TAC-area dataset spanning Nov 2023--Nov 2025 (84,498 hourly records across 5 transmission areas). Our analysis reveals that standard accuracy metrics are poor proxies for operational safety: models with identical MAPE can require vastly different reserve margins.
  We demonstrate that forecast errors are weakly but significantly associated with temperature (r = 0.16, p < 10^{-16}), motivating weather-aware modeling rather than loss function modification alone. The S-Mamba model achieves the lowest Reserve_{99.5}% margin (14.12%) compared to 16.66% for iTransformer, demonstrating superior forecast reliability under a 99.5th-percentile tail-risk reserve proxy.

</details>


### [634] [An Energy-Efficient Smart Bus Transport Management System with Blind-Spot Collision Detection Ability](https://arxiv.org/abs/2601.01274)
*Md. Sadman Haque,Zobaer Ibn Razzaque,Robiul Awoul Robin,Fahim Hafiz,Riasat Azim*

Main category: eess.SY

TL;DR: The paper proposes a smart public bus system with IoT and deep learning solutions to enhance safety, efficiency, and sustainability in transportation.


<details>
  <summary>Details</summary>
Motivation: To address challenges in public bus systems like lack of real-time updates, unsafe practices, and traffic inefficiencies common in developing countries.

Method: The system integrates deep learning for blind-spot warnings, IoT-based solar-powered smart bus stops, RFID tracking, smart doors, and an HTTP server for connectivity.

Result: Achieved 99% accuracy in blind-spot detection and precise bus-stop automation, real-time updates, and an energy-efficient bus stop saving 12.71kWh.

Conclusion: The system significantly improves safety, efficiency, and sustainability in public bus transportation, making commuting more reliable and organized.

Abstract: Public bus transport systems in developing countries often suffer from a lack of real-time location updates and for users, making commuting inconvenient and unreliable for passengers. Furthermore, stopping at undesired locations rather than designated bus stops creates safety risks and contributes to roadblocks, often causing traffic congestion. Additionally, issues such as blind spots, along with a lack of following traffic laws, increase the chances of accidents. In this work, we address these challenges by proposing a smart public bus system along with intelligent bus stops that enhance safety, efficiency, and sustainability. Our approach includes a deep learning-based blind-spot warning system to help drivers avoid accidents with automated bus-stop detection to accurately identify bus stops, improving transit efficiency. We also introduce IoT-based solar-powered smart bus stops that show real-time passenger counts, along with an RFID-based card system to track where passengers board and exit. A smart door system ensures safer and more organised boarding, while real-time bus tracking keeps passengers informed. To connect all these features, we use an HTTP-based server for seamless communication between the interconnected network systems. Our proposed system demonstrated approximately 99% efficiency in real-time blind spot detection while stopping precisely at the bus stops. Furthermore, the server showed real-time location updates both to the users and at the bus stops, enhancing commuting efficiency. The proposed energy-efficient bus stop demonstrated 12.71kWh energy saving, promoting sustainable architecture. Full implementation and source code are available at: https://github.com/sadman-adib/MoveMe-IoT

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [635] [Investigation into U.S. Citizen and Non-Citizen Worker Health Insurance and Employment](https://arxiv.org/abs/2601.00896)
*Annabelle Yao*

Main category: econ.GN

TL;DR: The study employs statistical and advanced machine learning techniques to uncover hidden disparities in socioeconomic integration, identifying 5 distinct population clusters based on health insurance, education, and employment.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the persistent disparities in access to health insurance, education, and employment across demographic groups to better understand socioeconomic integration.

Method: It uses statistical analysis ($χ^2$ test, Two Proportion Z-Test) and machine learning techniques (K-Modes, K-Prototypes clustering, t-SNE visualization, CatBoost classification) to analyze population data.

Result: The study identifies significant disparities, especially with employer-sponsored health insurance, segregated educational attainment patterns, and precarious employment among non-citizens.

Conclusion: Unveiling inequities in demographic clusters highlights systemic inequalities in socioeconomic factors, aiding in a nuanced understanding of stratification and highlighting areas for policy intervention.

Abstract: Socioeconomic integration is a critical dimension of social equity, yet persistent disparities remain in access to health insurance, education, and employment across different demographic groups. While previous studies have examined isolated aspects of inequality, there is limited research that integrates both statistical analysis and advanced machine learning to uncover hidden structures within population data. This study leverages statistical analysis ($χ^2$ test of independence and Two Proportion Z-Test) and machine learning clustering techniques -- K-Modes and K-Prototypes -- along with t-SNE visualization and CatBoost classification to analyze socioeconomic integration and inequality. Using statistical tests, we identified the proportion of the population with healthcare insurance, quality education, and employment. With this data, we concluded that there was an association between employment and citizenship status. Moreover, we were able to determine 5 distinct population groups using Machine Learning classification. The five clusters our analysis identifies reveal that while citizenship status shows no association with workforce participation, significant disparities exist in access to employer-sponsored health insurance. Each cluster represents a distinct demographic of the population, showing that there is a primary split along the lines of educational attainment which separates Clusters 0 and 4 from Clusters 1, 2, and 3. Furthermore, labor force status and nativity serve as secondary differentiators. Non-citizens are also disproportionately concentrated in precarious employment without benefits, highlighting systemic inequalities in healthcare access. By uncovering demographic clusters that face compounded disadvantages, this research contributes to a more nuanced understanding of socioeconomic stratification.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [636] [MDAgent2: Large Language Model for Code Generation and Knowledge Q&A in Molecular Dynamics](https://arxiv.org/abs/2601.02075)
*Zhuofan Shi,Hubao A,Yufei Shao,Mengyan Dai,Yadong Yu,Pan Xiang,Dongliang Huang,Hongxu An,Chunxiao Xin,Haiyang Shen,Zhenyu Wang,Yunshan Na,Gang Huang,Xiang Jing*

Main category: cs.CE

TL;DR: The paper introduces MDAgent2, an end-to-end framework for knowledge Q&A and code generation in molecular dynamics (MD), improving LLM adaptability in the domain.


<details>
  <summary>Details</summary>
Motivation: Writing LAMMPS scripts for molecular dynamics is specialized and time-consuming, and existing LLMs underperform due to limited data and low executability.

Method: The authors build high-quality domain-specific datasets for MD and train adapted models (MD-Instruct and MD-Code) using a three-stage strategy (CPT, SFT, RL). They also introduce MD-GRPO, a closed-loop RL for refinement and MDAgent2-RUNTIME, a deployable system combining code generation and self-correction.

Result: Their models demonstrate superior performance compared to baselines, benefiting from MD-EvalBench and advancements like MD-GRPO.

Conclusion: The study showcases the capability of LLMs in automating complex MD tasks, setting a foundation for AI-driven code generation in science and industry.

Abstract: Molecular dynamics (MD) simulations are essential for understanding atomic-scale behaviors in materials science, yet writing LAMMPS scripts remains highly specialized and time-consuming tasks. Although LLMs show promise in code generation and domain-specific question answering, their performance in MD scenarios is limited by scarce domain data, the high deployment cost of state-of-the-art LLMs, and low code executability. Building upon our prior MDAgent, we present MDAgent2, the first end-to-end framework capable of performing both knowledge Q&A and code generation within the MD domain. We construct a domain-specific data-construction pipeline that yields three high-quality datasets spanning MD knowledge, question answering, and code generation. Based on these datasets, we adopt a three stage post-training strategy--continued pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL)--to train two domain-adapted models, MD-Instruct and MD-Code. Furthermore, we introduce MD-GRPO, a closed-loop RL method that leverages simulation outcomes as reward signals and recycles low-reward trajectories for continual refinement. We further build MDAgent2-RUNTIME, a deployable multi-agent system that integrates code generation, execution, evaluation, and self-correction. Together with MD-EvalBench proposed in this work, the first benchmark for LAMMPS code generation and question answering, our models and system achieve performance surpassing several strong baselines.This work systematically demonstrates the adaptability and generalization capability of large language models in industrial simulation tasks, laying a methodological foundation for automatic code generation in AI for Science and industrial-scale simulations. URL: https://github.com/FredericVAN/PKU_MDAgent2

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [637] [A Knowledge Graph and Deep Learning-Based Semantic Recommendation Database System for Advertisement Retrieval and Personalization](https://arxiv.org/abs/2601.00833)
*Tangtang Wang,Kaijie Zhang,Kuangcong Liu*

Main category: cs.IR

TL;DR: The paper proposes a Knowledge Graph and Deep Learning-Based System (KGSR-ADS) to create personalized ad recommendations by combining Knowledge Graphs, LLMs, and efficient retrieval mechanisms.


<details>
  <summary>Details</summary>
Motivation: The aim is to address the growing complexity of advertisement data and provide intelligent systems capable of understanding semantic relationships among products, audiences, and advertising content.

Method: The system utilizes a multi-layer architecture integrating a heterogeneous Ad-Knowledge Graph (Ad-KG), a Semantic Embedding Layer powered by LLMs, a GNN with attention mechanisms, and a database optimized with FAISS/Milvus for efficient search.

Result: The system demonstrates improved semantic matching and scalable retrieval for personalized advertisement recommendations within large-scale, complex environments.

Conclusion: The KGSR-ADS framework effectively handles advertisement data complexities while offering scalable and accurate solutions for ad personalization and retrieval.

Abstract: In modern digital marketing, the growing complexity of advertisement data demands intelligent systems capable of understanding semantic relationships among products, audiences, and advertising content. To address this challenge, this paper proposes a Knowledge Graph and Deep Learning-Based Semantic Recommendation Database System (KGSR-ADS) for advertisement retrieval and personalization. The proposed framework integrates a heterogeneous Ad-Knowledge Graph (Ad-KG) that captures multi-relational semantics, a Semantic Embedding Layer that leverages large language models (LLMs) such as GPT and LLaMA to generate context-aware vector representations, a GNN + Attention Model that infers cross-entity dependencies, and a Database Optimization & Retrieval Layer based on vector indexing (FAISS/Milvus) for efficient semantic search. This layered architecture enables both accurate semantic matching and scalable retrieval, allowing personalized ad recommendations under large-scale heterogeneous workloads.

</details>


### [638] [Enhancing Retrieval-Augmented Generation with Topic-Enriched Embeddings: A Hybrid Approach Integrating Traditional NLP Techniques](https://arxiv.org/abs/2601.00891)
*Rodrigo Kataishi*

Main category: cs.IR

TL;DR: This paper introduces topic-enriched embeddings to improve document retrieval by integrating term and topic information with contextual embeddings, enhancing RAG systems.


<details>
  <summary>Details</summary>
Motivation: Low retrieval quality in retrieval-augmented generation (RAG) systems, particularly in corpora with overlapping topics and high thematic variety, motivates the need for better embeddings.

Method: The method combines TF-IDF, Latent Semantic Analysis (LSA), and Latent Dirichlet Allocation (LDA) with a contextual embedding model (all-MiniLM) to encode both term-level and topic-level semantics.

Result: Experiments on legal-text datasets show better clustering, higher retrieval precision, and lower computational effort compared to purely contextual approaches.

Conclusion: Topic-enriched embeddings can significantly enhance RAG systems in knowledge-intensive tasks, providing practical and efficient improvements in text clustering and retrieval.

Abstract: Retrieval-augmented generation (RAG) systems rely on accurate document retrieval to ground large language models (LLMs) in external knowledge, yet retrieval quality often degrades in corpora where topics overlap and thematic variation is high. This work proposes topic-enriched embeddings that integrate term-based signals and topic structure with contextual sentence embeddings. The approach combines TF-IDF with topic modeling and dimensionality reduction, using Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) to encode latent topical organization, and fuses these representations with a compact contextual encoder (all-MiniLM). By jointly capturing term-level and topic-level semantics, topic-enriched embeddings improve semantic clustering, increase retrieval precision, and reduce computational burden relative to purely contextual baselines. Experiments on a legal-text corpus show consistent gains in clustering coherence and retrieval metrics, suggesting that topic-enriched embeddings can serve as a practical component for more reliable knowledge-intensive RAG pipelines.

</details>


### [639] [The Discovery Gap: How Product Hunt Startups Vanish in LLM Organic Discovery Queries](https://arxiv.org/abs/2601.00912)
*Amit Prakash Sharma*

Main category: cs.IR

TL;DR: The study assesses how well ChatGPT and Perplexity recognize new startup products, finding high accuracy for name-based queries but poor visibility in organic discovery questions. Optimization for LLMs visibility directly is not effective.


<details>
  <summary>Details</summary>
Motivation: To determine whether startup products appear in AI-generated recommendations and discover factors influencing their visibility.

Method: The research involved testing 112 startups across 2,240 queries on ChatGPT (gpt-4o-mini) and Perplexity (sonar with web search), analyzing factors like SEO and GEO scores.

Result: Products were recognized effectively by name (99.4% ChatGPT, 94.3% Perplexity) but visibility in discovery-type questions was very low (3.32% ChatGPT, 8.29% Perplexity). SEO signals correlated positively with discovery rates, while GEO scores showed no correlation.

Conclusion: Focus on traditional SEO techniques and community presence rather than AI-specific optimization to improve product visibility in AI-generated queries.

Abstract: When someone asks ChatGPT to recommend a project management tool, which products show up in the response? And more importantly for startup founders: will their newly launched product ever appear? This research set out to answer these questions.
  I randomly selected 112 startups from the top 500 products featured on the 2025 Product Hunt leaderboard and tested each one across 2,240 queries to two different large language models: ChatGPT (gpt-4o-mini) and Perplexity (sonar with web search).
  The results were striking. When users asked about products by name, both LLMs recognized them almost perfectly: 99.4% for ChatGPT and 94.3% for Perplexity. But when users asked discovery-style questions like "What are the best AI tools launched this year?" the success rates collapsed to 3.32% and 8.29% respectively. That's a gap of 30-to-1 for ChatGPT.
  Perhaps the most surprising finding was that Generative Engine Optimization (GEO), the practice of optimizing website content for AI visibility, showed no correlation with actual discovery rates. Products with high GEO scores were no more likely to appear in organic queries than products with low scores.
  What did matter? For Perplexity, traditional SEO signals like referring domains (r = +0.319, p < 0.001) and Product Hunt ranking (r = -0.286, p = 0.002) predicted visibility. After cleaning the Reddit data for false positives, community presence also emerged as significant (r = +0.395, p = 0.002).
  The practical takeaway is counterintuitive: don't optimize for AI discovery directly. Instead, build the SEO foundation first and LLM visibility will follow.

</details>


### [640] [MACA: A Framework for Distilling Trustworthy LLMs into Efficient Retrievers](https://arxiv.org/abs/2601.00926)
*Satya Swaroop Gudipudi,Sahil Girhepuje,Ponnurangam Kumaraguru,Kristine Ma*

Main category: cs.IR

TL;DR: The paper introduces the Metadata-Aware Cross-Model Alignment (MACA) approach for enterprise retrieval systems, improving metadata-aware retrieval accuracy while avoiding costly online LLM calls.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the retrieval challenges posed by short, underspecified queries in enterprise systems, necessitating consideration of semantic nuances and metadata, while avoiding the expense of online LLM usage.

Method: MACA distills a metadata-aware LLM re-ranker into a compact student retriever using a metadata-aware prompt for trust calibration and the MetaFusion training objective. This involves listwise scores, negatives, and ranking losses to improve metadata-awareness and retrieval accuracy.

Result: MACA achieves significant performance gains, with the teacher model surpassing the baseline MAFA, and the student models considerably outperforming pretrained encoders on proprietary consumer banking FAQ and BankFAQs datasets.

Conclusion: MACA demonstrates that distilling LLM functionality into an efficient retriever can significantly enhance metadata-aware query accuracy while reducing costs associated with LLM queries.

Abstract: Modern enterprise retrieval systems must handle short, underspecified queries such as ``foreign transaction fee refund'' and ``recent check status''. In these cases, semantic nuance and metadata matter but per-query large language model (LLM) re-ranking and manual labeling are costly. We present Metadata-Aware Cross-Model Alignment (MACA), which distills a calibrated metadata aware LLM re-ranker into a compact student retriever, avoiding online LLM calls. A metadata-aware prompt verifies the teacher's trustworthiness by checking consistency under permutations and robustness to paraphrases, then supplies listwise scores, hard negatives, and calibrated relevance margins. The student trains with MACA's MetaFusion objective, which combines a metadata conditioned ranking loss with a cross model margin loss so it learns to push the correct answer above semantically similar candidates with mismatched topic, sub-topic, or entity. On a proprietary consumer banking FAQ corpus and BankFAQs, the MACA teacher surpasses a MAFA baseline at Accuracy@1 by five points on the proprietary set and three points on BankFAQs. MACA students substantially outperform pretrained encoders; e.g., on the proprietary corpus MiniLM Accuracy@1 improves from 0.23 to 0.48, while keeping inference free of LLM calls and supporting retrieval-augmented generation.

</details>


### [641] [AlignUSER: Human-Aligned LLM Agents via World Models for Recommender System Evaluation](https://arxiv.org/abs/2601.00930)
*Nicolas Bougie,Gian Maria Marconi,Tony Yip,Narimasa Watanabe*

Main category: cs.IR

TL;DR: AlignUSER addresses the challenge of evaluating recommender systems by using learned agents from human interactions, aiming to align synthetic user behavior closely with real user actions.


<details>
  <summary>Details</summary>
Motivation: The authors aim to tackle the limitations of evaluating recommender systems, including discrepancies between offline metrics and real user behavior, and the lack of sufficient interaction data.

Method: The framework formalizes world modeling as a next state prediction task using rollout sequences of actions and states, coupled with counterfactual trajectories to align agent actions with human personas.

Result: AlignUSER demonstrates better alignment with real human behavior compared to previous approaches across multiple datasets, on both micro and macro levels.

Conclusion: The study provides a novel and effective approach for simulating user behavior, improving the fidelity of recommender system evaluations.

Abstract: Evaluating recommender systems remains challenging due to the gap between offline metrics and real user behavior, as well as the scarcity of interaction data. Recent work explores large language model (LLM) agents as synthetic users, yet they typically rely on few-shot prompting, which yields a shallow understanding of the environment and limits their ability to faithfully reproduce user actions. We introduce AlignUSER, a framework that learns world-model-driven agents from human interactions. Given rollout sequences of actions and states, we formalize world modeling as a next state prediction task that helps the agent internalize the environment. To align actions with human personas, we generate counterfactual trajectories around demonstrations and prompt the LLM to compare its decisions with human choices, identify suboptimal actions, and extract lessons. The learned policy is then used to drive agent interactions with the recommender system. We evaluate AlignUSER across multiple datasets and demonstrate closer alignment with genuine humans than prior work, both at the micro and macro levels.

</details>


### [642] [OpenNovelty: An LLM-powered Agentic System for Verifiable Scholarly Novelty Assessment](https://arxiv.org/abs/2601.01576)
*Ming Zhang,Kexin Tan,Yueyuan Huang,Yujiong Shen,Chunchun Ma,Li Ju,Xinran Zhang,Yuhui Wang,Wenqing Jing,Jingyi Deng,Huayu Sha,Binze Hu,Jingqi Tong,Changhao Jiang,Yage Geng,Yuankai Ying,Yue Zhang,Zhangyue Yin,Zhiheng Xi,Shihan Dou,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.IR

TL;DR: OpenNovelty, a system utilizing large language models (LLMs), provides transparent and evidence-based analysis of novelty in research submissions, promoting more consistent and fair peer reviews.


<details>
  <summary>Details</summary>
Motivation: The paper is driven by the need to address challenges in evaluating novelty during peer reviews, given the volume and rapid evolution of academic literature.

Method: OpenNovelty operates in four key phases: extracting contributions, retrieving prior work using semantic search, creating a hierarchical taxonomy, and synthesizing analyses into structured novelty reports with transparent citations.

Result: The system was applied to over 500 ICLR 2026 submissions, demonstrating its capability to identify relevant prior works, including overlooked papers by authors.

Conclusion: OpenNovelty enhances peer review by providing an evidence-backed, scalable tool for assessing novelty, improving fairness and consistency in the academic community.

Abstract: Evaluating novelty is critical yet challenging in peer review, as reviewers must assess submissions against a vast, rapidly evolving literature. This report presents OpenNovelty, an LLM-powered agentic system for transparent, evidence-based novelty analysis. The system operates through four phases: (1) extracting the core task and contribution claims to generate retrieval queries; (2) retrieving relevant prior work based on extracted queries via semantic search engine; (3) constructing a hierarchical taxonomy of core-task-related work and performing contribution-level full-text comparisons against each contribution; and (4) synthesizing all analyses into a structured novelty report with explicit citations and evidence snippets. Unlike naive LLM-based approaches, \textsc{OpenNovelty} grounds all assessments in retrieved real papers, ensuring verifiable judgments. We deploy our system on 500+ ICLR 2026 submissions with all reports publicly available on our website, and preliminary analysis suggests it can identify relevant prior work, including closely related papers that authors may overlook. OpenNovelty aims to empower the research community with a scalable tool that promotes fair, consistent, and evidence-backed peer review.

</details>


### [643] [LACONIC: Dense-Level Effectiveness for Scalable Sparse Retrieval via a Two-Phase Training Curriculum](https://arxiv.org/abs/2601.01684)
*Zhichao Xu,Shengyao Zhuang,Crystina Zhang,Xueguang Ma,Yijun Tian,Maitrey Mehta,Jimmy Lin,Vivek Srikumar*

Main category: cs.IR

TL;DR: This paper introduces LACONIC, a series of learned sparse retrievers based on Llama-3, achieving state-of-the-art retrieval performance while being highly memory and compute efficient.


<details>
  <summary>Details</summary>
Motivation: High memory requirements and reliance on GPUs limit the deployment of dense retrieval models in information retrieval systems. Sparse retrieval, though more efficient, has received less attention. The authors aim to create a solution that is both high-performing and resource-efficient.

Method: LACONIC uses a streamlined training approach with two phases: (1) weakly supervised pre-finetuning to adapt causal language models for bidirectional contextualization, and (2) finetuning using curated hard negatives.

Result: The 8B variant of LACONIC achieves a state-of-the-art score of 60.2 nDCG on the MTEB Retrieval benchmark, requiring 71% less memory than equivalent dense models while operating effectively on commodity CPU hardware.

Conclusion: LACONIC bridges the gap between dense and sparse models by offering a practical, high-performance, and efficient alternative suitable for real-world applications.

Abstract: While dense retrieval models have become the standard for state-of-the-art information retrieval, their deployment is often constrained by high memory requirements and reliance on GPU accelerators for vector similarity search. Learned sparse retrieval offers a compelling alternative by enabling efficient search via inverted indices, yet it has historically received less attention than dense approaches. In this report, we introduce LACONIC, a family of learned sparse retrievers based on the Llama-3 architecture (1B, 3B, and 8B). We propose a streamlined two-phase training curriculum consisting of (1) weakly supervised pre-finetuning to adapt causal LLMs for bidirectional contextualization and (2) high-signal finetuning using curated hard negatives. Our results demonstrate that LACONIC effectively bridges the performance gap with dense models: the 8B variant achieves a state-of-the-art 60.2 nDCG on the MTEB Retrieval benchmark, ranking 15th on the leaderboard as of January 1, 2026, while utilizing 71\% less index memory than an equivalent dense model. By delivering high retrieval effectiveness on commodity CPU hardware with a fraction of the compute budget required by competing models, LACONIC provides a scalable and efficient solution for real-world search applications.

</details>


### [644] [Query-Document Dense Vectors for LLM Relevance Judgment Bias Analysis](https://arxiv.org/abs/2601.01751)
*Samaneh Mohtadi,Gianluca Demartini*

Main category: cs.IR

TL;DR: The paper investigates systematic mistakes of LLMs in IR evaluation by comparing their relevance judgments with human assessors, using a clustering-based semantic analysis approach.


<details>
  <summary>Details</summary>
Motivation: To identify whether LLMs make systematic errors in relevance judgments instead of only evaluating their average reliability compared to human assessors.

Method: A novel clustering-based framework embedding query-document pairs into a joint semantic space to analyze relevance disagreement patterns and localize systematic errors between LLM and human assessments.

Result: Systematic disagreement between LLMs and humans is identified in specific semantic clusters, especially in definition-seeking, policy-related, and ambiguous contexts, revealing hotspots of disagreement.

Conclusion: The proposed framework helps uncover biases in LLM judgments and improves their reliability for IR evaluation by linking global diagnostics with localized clustering.

Abstract: Large Language Models (LLMs) have been used as relevance assessors for Information Retrieval (IR) evaluation collection creation due to reduced cost and increased scalability as compared to human assessors. While previous research has looked at the reliability of LLMs as compared to human assessors, in this work, we aim to understand if LLMs make systematic mistakes when judging relevance, rather than just understanding how good they are on average. To this aim, we propose a novel representational method for queries and documents that allows us to analyze relevance label distributions and compare LLM and human labels to identify patterns of disagreement and localize systematic areas of disagreement. We introduce a clustering-based framework that embeds query-document (Q-D) pairs into a joint semantic space, treating relevance as a relational property. Experiments on TREC Deep Learning 2019 and 2020 show that systematic disagreement between humans and LLMs is concentrated in specific semantic clusters rather than distributed randomly. Query-level analyses reveal recurring failures, most often in definition-seeking, policy-related, or ambiguous contexts. Queries with large variation in agreement across their clusters emerge as disagreement hotspots, where LLMs tend to under-recall relevant content or over-include irrelevant material. This framework links global diagnostics with localized clustering to uncover hidden weaknesses in LLM judgments, enabling bias-aware and more reliable IR evaluation.

</details>


### [645] [Exploring Diversity, Novelty, and Popularity Bias in ChatGPT's Recommendations](https://arxiv.org/abs/2601.01997)
*Dario Di Palma,Giovanni Maria Biancofiore,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: The paper examines the performance of ChatGPT-3.5 and ChatGPT-4 in recommendation systems, specifically exploring diversity, novelty, and popularity bias, revealing that they outperform traditional recommenders in some aspects.


<details>
  <summary>Details</summary>
Motivation: To analyze ChatGPT's application in recommendation systems beyond accuracy, focusing on diversity, novelty, and overcoming biases like popularity bias to improve user satisfaction and personalization.

Method: The study evaluates ChatGPT-3.5 and ChatGPT-4 on three datasets using metrics like Top-N recommendations and cold-start scenarios, assessing their performance on diversity, novelty, and accuracy.

Result: ChatGPT-4 outperforms traditional recommenders in balancing novelty and diversity, excels in accuracy and novelty for cold-start scenarios, and is effective for new users.

Conclusion: ChatGPT models demonstrate strengths in novel and diverse recommendations, suggesting their potential in enhancing recommendation systems by going beyond accuracy-focused benchmarks.

Abstract: ChatGPT has emerged as a versatile tool, demonstrating capabilities across diverse domains. Given these successes, the Recommender Systems (RSs) community has begun investigating its applications within recommendation scenarios primarily focusing on accuracy. While the integration of ChatGPT into RSs has garnered significant attention, a comprehensive analysis of its performance across various dimensions remains largely unexplored. Specifically, the capabilities of providing diverse and novel recommendations or exploring potential biases such as popularity bias have not been thoroughly examined. As the use of these models continues to expand, understanding these aspects is crucial for enhancing user satisfaction and achieving long-term personalization.
  This study investigates the recommendations provided by ChatGPT-3.5 and ChatGPT-4 by assessing ChatGPT's capabilities in terms of diversity, novelty, and popularity bias. We evaluate these models on three distinct datasets and assess their performance in Top-N recommendation and cold-start scenarios. The findings reveal that ChatGPT-4 matches or surpasses traditional recommenders, demonstrating the ability to balance novelty and diversity in recommendations. Furthermore, in the cold-start scenario, ChatGPT models exhibit superior performance in both accuracy and novelty, suggesting they can be particularly beneficial for new users. This research highlights the strengths and limitations of ChatGPT's recommendations, offering new perspectives on the capacity of these models to provide recommendations beyond accuracy-focused metrics.

</details>


### [646] [Exploring Approaches for Detecting Memorization of Recommender System Data in Large Language Models](https://arxiv.org/abs/2601.02002)
*Antonio Colacicco,Vito Guida,Dario Di Palma,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: The study explores methods to detect and extract memorized data from LLMs, emphasizing the potential of automatic prompt engineering.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address concerns about data leakage in LLMs, particularly focusing on detecting and extracting memorized data efficiently.

Method: Three approaches were evaluated: jailbreak prompt engineering, unsupervised latent knowledge discovery, and Automatic Prompt Engineering (APE).

Result: Jailbreak prompting showed inconsistent results, unsupervised techniques worked partially, and APE showed moderate success for specific data types.

Conclusion: Automatic Prompt Engineering is the most promising method for extracting LLM-memorized data, though challenges with numerical data persist.

Abstract: Large Language Models (LLMs) are increasingly applied in recommendation scenarios due to their strong natural language understanding and generation capabilities. However, they are trained on vast corpora whose contents are not publicly disclosed, raising concerns about data leakage. Recent work has shown that the MovieLens-1M dataset is memorized by both the LLaMA and OpenAI model families, but the extraction of such memorized data has so far relied exclusively on manual prompt engineering. In this paper, we pose three main questions: Is it possible to enhance manual prompting? Can LLM memorization be detected through methods beyond manual prompting? And can the detection of data leakage be automated? To address these questions, we evaluate three approaches: (i) jailbreak prompt engineering; (ii) unsupervised latent knowledge discovery, probing internal activations via Contrast-Consistent Search (CCS) and Cluster-Norm; and (iii) Automatic Prompt Engineering (APE), which frames prompt discovery as a meta-learning process that iteratively refines candidate instructions. Experiments on MovieLens-1M using LLaMA models show that jailbreak prompting does not improve the retrieval of memorized items and remains inconsistent; CCS reliably distinguishes genuine from fabricated movie titles but fails on numerical user and rating data; and APE retrieves item-level information with moderate success yet struggles to recover numerical interactions. These findings suggest that automatically optimizing prompts is the most promising strategy for extracting memorized samples.

</details>


### [647] [ScienceDB AI: An LLM-Driven Agentic Recommender System for Large-Scale Scientific Data Sharing Services](https://arxiv.org/abs/2601.01118)
*Qingqing Long,Haotian Chen,Chenyang Zhao,Xiaolei Du,Xuezhi Wang,Pengyao Wang,Chengzan Li,Yuanchun Zhou,Hengshu Zhu*

Main category: cs.IR

TL;DR: The paper introduces ScienceDB AI, an LLM-driven recommender system for scientific datasets that employs advanced mechanisms for query analysis and dataset recommendations, enhancing sharing and utilization.


<details>
  <summary>Details</summary>
Motivation: Promoting efficient sharing and utilization of scientific datasets remains challenging due to intricate domain-specific knowledge and contexts not easily addressed by traditional recommenders.

Method: The system uses LLMs for conversational recommendations, introducing a Scientific Intention Perceptor for structured extraction, a Structured Memory Compressor for dialogue management, and a Trustworthy RAG framework for credible recommendations.

Result: ScienceDB AI demonstrated significant effectiveness through offline and online experiments using over 10 million real-world datasets, providing accurate and trustworthy dataset recommendations.

Conclusion: ScienceDB AI is the first LLM-driven conversational recommender specialized for large-scale scientific dataset services, successfully improving dataset utilization and trustworthiness.

Abstract: The rapid growth of AI for Science (AI4S) has underscored the significance of scientific datasets, leading to the establishment of numerous national scientific data centers and sharing platforms. Despite this progress, efficiently promoting dataset sharing and utilization for scientific research remains challenging. Scientific datasets contain intricate domain-specific knowledge and contexts, rendering traditional collaborative filtering-based recommenders inadequate. Recent advances in Large Language Models (LLMs) offer unprecedented opportunities to build conversational agents capable of deep semantic understanding and personalized recommendations. In response, we present ScienceDB AI, a novel LLM-driven agentic recommender system developed on Science Data Bank (ScienceDB), one of the largest global scientific data-sharing platforms. ScienceDB AI leverages natural language conversations and deep reasoning to accurately recommend datasets aligned with researchers' scientific intents and evolving requirements. The system introduces several innovations: a Scientific Intention Perceptor to extract structured experimental elements from complicated queries, a Structured Memory Compressor to manage multi-turn dialogues effectively, and a Trustworthy Retrieval-Augmented Generation (Trustworthy RAG) framework. The Trustworthy RAG employs a two-stage retrieval mechanism and provides citable dataset references via Citable Scientific Task Record (CSTR) identifiers, enhancing recommendation trustworthiness and reproducibility. Through extensive offline and online experiments using over 10 million real-world datasets, ScienceDB AI has demonstrated significant effectiveness. To our knowledge, ScienceDB AI is the first LLM-driven conversational recommender tailored explicitly for large-scale scientific dataset sharing services. The platform is publicly accessible at: https://ai.scidb.cn/en.

</details>


### [648] [MergeRec: Model Merging for Data-Isolated Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2601.01753)
*Hyunsoo Kim,Jaewan Moon,Seongmin Park,Jongwuk Lee*

Main category: cs.IR

TL;DR: MergeRec is a framework for cross-domain sequential recommendation under data isolation constraints, improving the generalizability of recommender systems without sharing raw user data across domains.


<details>
  <summary>Details</summary>
Motivation: Modern recommender systems often fail to generalize across different domains. Existing cross-domain approaches rely on overlapping users/items or unrealistic privacy assumptions, which are impractical.

Method: MergeRec introduces a novel three-stage framework: merging initialization using training-free methods, pseudo-user data construction by treating items as virtual sequences, and collaborative merging optimization using a joint recommendation and distillation loss.

Result: Extensive experiments demonstrate that MergeRec outperforms traditional model merging approaches, achieving an average improvement of up to 17.21% in Recall@10.

Conclusion: MergeRec shows the potential of model merging for scalable and effective universal recommender systems, particularly under realistic conditions where raw user data across domains is isolated.

Abstract: Modern recommender systems trained on domain-specific data often struggle to generalize across multiple domains. Cross-domain sequential recommendation has emerged as a promising research direction to address this challenge; however, existing approaches face fundamental limitations, such as reliance on overlapping users or items across domains, or unrealistic assumptions that ignore privacy constraints. In this work, we propose a new framework, MergeRec, based on model merging under a new and realistic problem setting termed data-isolated cross-domain sequential recommendation, where raw user interaction data cannot be shared across domains. MergeRec consists of three key components: (1) merging initialization, (2) pseudo-user data construction, and (3) collaborative merging optimization. First, we initialize a merged model using training-free merging techniques. Next, we construct pseudo-user data by treating each item as a virtual sequence in each domain, enabling the synthesis of meaningful training samples without relying on real user interactions. Finally, we optimize domain-specific merging weights through a joint objective that combines a recommendation loss, which encourages the merged model to identify relevant items, and a distillation loss, which transfers collaborative filtering signals from the fine-tuned source models. Extensive experiments demonstrate that MergeRec not only preserves the strengths of the original models but also significantly enhances generalizability to unseen domains. Compared to conventional model merging methods, MergeRec consistently achieves superior performance, with average improvements of up to 17.21% in Recall@10, highlighting the potential of model merging as a scalable and effective approach for building universal recommender systems. The source code is available at https://github.com/DIALLab-SKKU/MergeRec.

</details>


### [649] [MCGI: Manifold-Consistent Graph Indexing for Billion-Scale Disk-Resident Vector Search](https://arxiv.org/abs/2601.01930)
*Dongfang Zhao*

Main category: cs.IR

TL;DR: MCGI is a new geometry-aware indexing method that addresses performance issues in high-dimensional graph-based ANN search by dynamically adapting to data geometry.


<details>
  <summary>Details</summary>
Motivation: High-dimensional ANN search suffers from performance issues due to the mismatch between the Euclidean and data manifold geometries, leading to inefficient greedy routing.

Method: The authors propose MCGI, which uses Local Intrinsic Dimensionality (LID) to adapt search strategies dynamically without relying on static hyperparameters. It modulates beam search based on in situ geometry analysis and preserves manifold-consistent topology.

Result: MCGI achieves significant performance improvements: 5.8× higher throughput at 95% recall on high-dimensional GIST1M compared to DiskANN, and 3× reduced high-recall query latency on the SIFT1B dataset, while maintaining competitive performance on lower-dimensional data.

Conclusion: MCGI demonstrates scalability and efficiency in high-dimensional ANN search by aligning index design with the intrinsic geometry of data, showing strong empirical and theoretical advantages.

Abstract: Graph-based Approximate Nearest Neighbor (ANN) search often suffers from performance degradation in high-dimensional spaces due to the ``Euclidean-Geodesic mismatch,'' where greedy routing diverges from the underlying data manifold. To address this, we propose Manifold-Consistent Graph Indexing (MCGI), a geometry-aware and disk-resident indexing method that leverages Local Intrinsic Dimensionality (LID) to dynamically adapt search strategies to the data's intrinsic geometry. Unlike standard algorithms that treat dimensions uniformly, MCGI modulates its beam search budget based on in situ geometric analysis, eliminating dependency on static hyperparameters. Theoretical analysis confirms that MCGI enables improved approximation guarantees by preserving manifold-consistent topological connectivity. Empirically, MCGI achieves 5.8$\times$ higher throughput at 95\% recall on high-dimensional GIST1M compared to state-of-the-art DiskANN. On the billion-scale SIFT1B dataset, MCGI further validates its scalability by reducing high-recall query latency by 3$\times$, while maintaining performance parity on standard lower-dimensional datasets.

</details>


### [650] [SRAS: A Lightweight Reinforcement Learning-based Document Selector for Edge-Native RAG Pipelines](https://arxiv.org/abs/2601.01785)
*Rajiv Chaitanya Muttur*

Main category: cs.IR

TL;DR: The paper proposes SRAS, a lightweight document selector leveraging reinforcement learning (RL) for efficient Retrieval-Augmented Generation (RAG) systems under strict constraints like low memory and latency.


<details>
  <summary>Details</summary>
Motivation: Current RAG systems rely on rigid top-k document selection, ignoring downstream generation quality while being computationally expensive, making them unsuitable for edge-native applications.

Method: The method, SRAS, utilizes Proximal Policy Optimization (PPO) in reinforcement learning with a hybrid reward signal combining Relaxed F1 and BERTScore for training a compact (~0.76MB) document selection policy.

Result: Experiments show that SRAS achieves better generation quality than supervised and random selectors under tight constraints, with a BERTScore F1 of 0.8546 on SQuAD v2 without specific domain tuning.

Conclusion: RL-based document selection can be made ultra-lightweight, effective, and suitable for real-time on-device RAG deployments, as demonstrated by SRAS.

Abstract: Retrieval-Augmented Generation (RAG) systems often rely on fixed top-k document selection mechanisms that ignore downstream generation quality and impose computational overheads. We propose SRAS (Sparse Reward-Aware Selector), a lightweight document selector trained via reinforcement learning (RL) for edge-native RAG deployment. Unlike prior RL-based retrievers that assume large memory and latency budgets, SRAS learns a compact (~0.76MB) policy using Proximal Policy Optimization (PPO), guided by a hybrid reward signal combining Relaxed F1 and BERTScore. Our method operates under tight token and compute constraints, maintaining <1s latency on CPU. SRAS outperforms supervised and random selectors on a synthetic QA benchmark, and generalizes to real-world data, achieving BERTScore F1 of 0.8546 on SQuAD v2 without domain-specific tuning. This work is the first to demonstrate that RL-based document selection can be made ultra-lightweight, latency-aware, and effective for on-device RAG pipelines.

</details>


<div id='stat.CO'></div>

# stat.CO [[Back]](#toc)

### [651] [grangersearch: An R Package for Exhaustive Granger Causality Testing with Tidyverse Integration](https://arxiv.org/abs/2601.01604)
*Nikolaos Korfiatis*

Main category: stat.CO

TL;DR: The paper introduces an R package "grangersearch" to perform exhaustive Granger causality searches across multiple time series efficiently.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify and automate Granger causality analysis for applied researchers working with multiple time series data.

Method: The paper describes the statistical methodology behind the "grangersearch" package which supports pairwise searches, lag optimization, and tidyverse compatibility, among others.

Result: A new R package "grangersearch" is successfully developed and demonstrated with examples, making causal analysis more accessible.

Conclusion: The package simplifies exploratory causal analysis and offers practical functionality for researchers with integrations into existing R ecosystems.

Abstract: This paper introduces grangersearch, an R package for performing exhaustive Granger causality searches on multiple time series. The package provides: (1) exhaustive pairwise search across multiple variables, (2) automatic lag order optimization with visualization, (3) tidyverse-compatible syntax with pipe operators and non-standard evaluation, and (4) integration with the broom ecosystem through tidy() and glance() methods. The package wraps the vars infrastructure while providing a simple interface for exploratory causal analysis. We describe the statistical methodology, demonstrate the package through worked examples, and discuss practical considerations for applied researchers.

</details>


<div id='q-fin.PR'></div>

# q-fin.PR [[Back]](#toc)

### [652] [Reinforcement Learning for Option Hedging: Static Implied-Volatility Fit versus Shortfall-Aware Performance](https://arxiv.org/abs/2601.01709)
*Ziheng Chen,Minxuan Hu,Jiayu Yi,Wenxi Sun*

Main category: q-fin.PR

TL;DR: The paper enhances the QLBS framework with risk aversion and trading costs, introducing a new approach for option pricing (RLOP), which improves both static pricing accuracy and dynamic hedging performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve the robustness of option pricing models by addressing limitations of existing frameworks, incorporating risk aversion, trading costs, and emphasizing dynamic hedging performance.

Method: The study extends the QLBS model to include risk aversion and trading costs and introduces the RLOP approach, which integrates seamlessly with standard reinforcement learning techniques.

Result: Adaptive-QLBS provides better static pricing accuracy in the implied volatility domain, while RLOP excels in dynamic hedging by lowering shortfall probability using real-world SPY and XOP options data.

Conclusion: The findings demonstrate the necessity of evaluating option pricing models on dynamic hedging efficiency and indicate that reinforcement learning-based methods can enhance performance under market frictions.

Abstract: We extend the Q-learner in Black-Scholes (QLBS) framework by incorporating risk aversion and trading costs, and propose a novel Replication Learning of Option Pricing (RLOP) approach. Both methods are fully compatible with standard reinforcement learning algorithms and operate under market frictions. Using SPY and XOP option data, we evaluate performance along static and dynamic dimensions. Adaptive-QLBS achieves higher static pricing accuracy in implied volatility space, while RLOP delivers superior dynamic hedging performance by reducing shortfall probability. These results highlight the importance of evaluating option pricing models beyond static fit, emphasizing realized hedging outcomes.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [653] [Placenta Accreta Spectrum Detection using Multimodal Deep Learning](https://arxiv.org/abs/2601.00907)
*Sumaiya Ali,Areej Alhothali,Sameera Albasri,Ohoud Alzamzami,Ahmed Abduljabbar,Muhammad Alwazzan*

Main category: eess.IV

TL;DR: The study developed a multimodal deep learning model combining MRI and Ultrasound for detecting Placenta Accreta Spectrum (PAS) with superior accuracy and AUC compared to single modalities.


<details>
  <summary>Details</summary>
Motivation: To improve early and accurate prenatal diagnosis of Placenta Accreta Spectrum (PAS) to mitigate maternal and neonatal risks using advanced machine learning techniques.

Method: Integrated a 3D DenseNet121-Vision Transformer for MRI and a 2D ResNet50 for Ultrasound through multimodal feature-level fusion using curated datasets for training and evaluation.

Result: Achieved superior diagnostic performance with 92.5% accuracy and 0.927 AUC, surpassing unimodal MRI (82.5%, AUC 0.825) and Ultrasound (87.5%, AUC 0.879) models.

Conclusion: The proposed multimodal deep learning framework effectively combines MRI and Ultrasound features, significantly improving PAS detection and demonstrating strong potential for enhanced prenatal diagnostics and risk assessment.

Abstract: Placenta Accreta Spectrum (PAS) is a life-threatening obstetric complication involving abnormal placental invasion into the uterine wall. Early and accurate prenatal diagnosis is essential to reduce maternal and neonatal risks. This study aimed to develop and validate a deep learning framework that enhances PAS detection by integrating multiple imaging modalities. A multimodal deep learning model was designed using an intermediate feature-level fusion architecture combining 3D Magnetic Resonance Imaging (MRI) and 2D Ultrasound (US) scans. Unimodal feature extractors, a 3D DenseNet121-Vision Transformer for MRI and a 2D ResNet50 for US, were selected after systematic comparative analysis. Curated datasets comprising 1,293 MRI and 1,143 US scans were used to train the unimodal models and paired samples of patient-matched MRI-US scans was isolated for multimodal model development and evaluation. On an independent test set, the multimodal fusion model achieved superior performance, with an accuracy of 92.5% and an Area Under the Receiver Operating Characteristic Curve (AUC) of 0.927, outperforming the MRI-only (82.5%, AUC 0.825) and US-only (87.5%, AUC 0.879) models. Integrating MRI and US features provides complementary diagnostic information, demonstrating strong potential to enhance prenatal risk assessment and improve patient outcomes.

</details>


### [654] [Scale-aware Adaptive Supervised Network with Limited Medical Annotations](https://arxiv.org/abs/2601.01005)
*Zihan Li,Dandan Shan,Yunxiang Li,Paul E. Kinahan,Qingqi Hong*

Main category: eess.IV

TL;DR: This paper introduces SASNet, a semi-supervised architecture for medical image segmentation that addresses annotation scarcity, inter-annotator variability, and insufficient feature integration through innovative methodologies, achieving near fully-supervised performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in medical image segmentation under semi-supervised learning settings, such as annotation scarcity, annotation variability, and inadequate feature integration for accurate boundary delineation.

Method: The paper proposes SASNet, a dual-branch architecture with key innovations: 1) Scale-aware Adaptive Reweight strategy for dynamic prediction weighting using temporal confidence, 2) View Variance Enhancement through 3D Fourier transformations to simulate annotation variability, and 3) segmentation-regression consistency via signed distance maps.

Result: SASNet outperforms existing semi-supervised methods on LA, Pancreas-CT, and BraTS datasets, achieving results close to fully-supervised performance with limited labeled data.

Conclusion: SASNet effectively integrates spatial, temporal, and geometric consistency principles, offering a robust solution to semi-supervised medical image segmentation challenges, and its source code is open for future research.

Abstract: Medical image segmentation faces critical challenges in semi-supervised learning scenarios due to severe annotation scarcity requiring expert radiological knowledge, significant inter-annotator variability across different viewpoints and expertise levels, and inadequate multi-scale feature integration for precise boundary delineation in complex anatomical structures. Existing semi-supervised methods demonstrate substantial performance degradation compared to fully supervised approaches, particularly in small target segmentation and boundary refinement tasks. To address these fundamental challenges, we propose SASNet (Scale-aware Adaptive Supervised Network), a dual-branch architecture that leverages both low-level and high-level feature representations through novel scale-aware adaptive reweight mechanisms. Our approach introduces three key methodological innovations, including the Scale-aware Adaptive Reweight strategy that dynamically weights pixel-wise predictions using temporal confidence accumulation, the View Variance Enhancement mechanism employing 3D Fourier domain transformations to simulate annotation variability, and segmentation-regression consistency learning through signed distance map algorithms for enhanced boundary precision. These innovations collectively address the core limitations of existing semi-supervised approaches by integrating spatial, temporal, and geometric consistency principles within a unified optimization framework. Comprehensive evaluation across LA, Pancreas-CT, and BraTS datasets demonstrates that SASNet achieves superior performance with limited labeled data, surpassing state-of-the-art semi-supervised methods while approaching fully supervised performance levels. The source code for SASNet is available at https://github.com/HUANGLIZI/SASNet.

</details>


### [655] [An Explainable Agentic AI Framework for Uncertainty-Aware and Abstention-Enabled Acute Ischemic Stroke Imaging Decisions](https://arxiv.org/abs/2601.01008)
*Md Rashadul Islam*

Main category: eess.IV

TL;DR: This paper proposes a novel AI framework for safe and trustworthy acute ischemic stroke imaging, incorporating uncertainty awareness, selective abstention, and visual explanations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address safety and trust deficiencies in existing AI-based stroke imaging tools by introducing mechanisms for uncertainty awareness and abstention in decision-making.

Method: The approach involves a modular agentic pipeline: a perception agent for image analysis, an uncertainty agent to assess predictive reliability, and a decision agent to handle abstention under uncertainty.

Result: The framework demonstrates its effectiveness in identifying diagnostically ambiguous and low-information regions, with qualitative analyses verifying the emergence of uncertainty-driven abstention.

Conclusion: Agentic control, uncertainty awareness, and selective abstention are proposed as core principles for designing safer and more transparent medical imaging AI systems.

Abstract: Artificial intelligence models have shown strong potential in acute ischemic stroke imaging, particularly for lesion detection and segmentation using computed tomography and magnetic resonance imaging. However, most existing approaches operate as black box predictors, producing deterministic outputs without explicit uncertainty awareness or structured mechanisms to abstain under ambiguous conditions. This limitation raises serious safety and trust concerns in high risk emergency radiology settings. In this paper, we propose an explainable agentic AI framework for uncertainty aware and abstention enabled decision support in acute ischemic stroke imaging. The framework follows a modular agentic pipeline in which a perception agent performs lesion aware image analysis, an uncertainty estimation agent computes slice level predictive reliability, and a decision agent determines whether to issue a prediction or abstain based on predefined uncertainty thresholds. Unlike prior stroke imaging systems that primarily focus on improving segmentation or classification accuracy, the proposed framework explicitly prioritizes clinical safety, transparency, and clinician aligned decision behavior. Qualitative and case based analyses across representative stroke imaging scenarios demonstrate that uncertainty driven abstention naturally emerges in diagnostically ambiguous regions and low information slices. The framework further integrates visual explanation mechanisms to support both predictive and abstention decisions, addressing a key limitation of existing uncertainty aware medical imaging systems. Rather than introducing a new performance benchmark, this work presents agentic control, uncertainty awareness, and selective abstention as essential design principles for developing safe and trustworthy medical imaging AI systems.

</details>


### [656] [Seamlessly Natural: Image Stitching with Natural Appearance Preservation](https://arxiv.org/abs/2601.01257)
*Gaetane Lorna N. Tchana,Damaris Belle M. Fotso,Antonio Hendricks,Christophe Bobda*

Main category: eess.IV

TL;DR: The paper introduces SENA, an advanced image stitching method designed to solve issues like distortions and artifacts caused by traditional methods in challenging real-world scenes.


<details>
  <summary>Details</summary>
Motivation: Conventional image stitching methods, based on homography alignment, perform poorly in complex scenes with parallax and depth variations, creating distortions and structural issues.

Method: SENA uses a three-part strategy: hierarchical affine-based warping for preserving shape and parallelism, geometry-guided zone detection to avoid parallax issues, and anchor-based seamline segmentation to eliminate artifacts and improve stitching.

Result: SENA delivers alignment comparable to top homography techniques but outperforms them significantly in maintaining shape accuracy, texture, and visual aesthetics.

Conclusion: SENA offers a robust, geometry-aware solution for image stitching that addresses the limitations of traditional methods and enhances stitching quality in complex scenes.

Abstract: This paper introduces SENA (SEamlessly NAtural), a geometry-driven image stitching approach that prioritizes structural fidelity in challenging real-world scenes characterized by parallax and depth variation. Conventional image stitching relies on homographic alignment, but this rigid planar assumption often fails in dual-camera setups with significant scene depth, leading to distortions such as visible warps and spherical bulging. SENA addresses these fundamental limitations through three key contributions. First, we propose a hierarchical affine-based warping strategy, combining global affine initialization with local affine refinement and smooth free-form deformation. This design preserves local shape, parallelism, and aspect ratios, thereby avoiding the hallucinated structural distortions commonly introduced by homography-based models. Second, we introduce a geometry-driven adequate zone detection mechanism that identifies parallax-minimized regions directly from the disparity consistency of RANSAC-filtered feature correspondences, without relying on semantic segmentation. Third, building upon this adequate zone, we perform anchor-based seamline cutting and segmentation, enforcing a one-to-one geometric correspondence across image pairs by construction, which effectively eliminates ghosting, duplication, and smearing artifacts in the final panorama.
  Extensive experiments conducted on challenging datasets demonstrate that SENA achieves alignment accuracy comparable to leading homography-based methods, while significantly outperforming them in critical visual metrics such as shape preservation, texture integrity, and overall visual realism.

</details>


### [657] [UniCrop: A Universal, Multi-Source Data Engineering Pipeline for Scalable Crop Yield Prediction](https://arxiv.org/abs/2601.01655)
*Emiliya Khidirova,Oktay Karakuş*

Main category: eess.IV

TL;DR: This paper introduces UniCrop, an automated pipeline for crop yield prediction using diverse data streams, aiming to enhance scalability, transparency, and reproducibility.


<details>
  <summary>Details</summary>
Motivation: Existing crop yield prediction models are limited by being crop- or region-specific and require significant data engineering efforts, restricting their scalability and operational use.

Method: UniCrop retrieves, cleans, harmonises, and engineers multi-source environmental data (over 200 variables) automatically, applying a feature reduction method to create a compact dataset and using various machine learning models for validation.

Result: UniCrop demonstrated high prediction accuracy using selected features, achieving best performance with LightGBM (RMSE = 465.1 kg/ha, R² = 0.6576) and further improvement with an ensemble model (RMSE = 463.2 kg/ha, R² = 0.6604).

Conclusion: UniCrop addresses bottlenecks in crop yield modelling by providing a reusable and scalable pipeline for harmonising environmental data and enabling practical agricultural analytics across diverse crops, regions, and timeframes.

Abstract: Accurate crop yield prediction relies on diverse data streams, including satellite, meteorological, soil, and topographic information. However, despite rapid advances in machine learning, existing approaches remain crop- or region-specific and require data engineering efforts. This limits scalability, reproducibility, and operational deployment. This study introduces UniCrop, a universal and reusable data pipeline designed to automate the acquisition, cleaning, harmonisation, and engineering of multi-source environmental data for crop yield prediction. For any given location, crop type, and temporal window, UniCrop automatically retrieves, harmonises, and engineers over 200 environmental variables (Sentinel-1/2, MODIS, ERA5-Land, NASA POWER, SoilGrids, and SRTM), reducing them to a compact, analysis-ready feature set utilising a structured feature reduction workflow with minimum redundancy maximum relevance (mRMR). To validate, UniCrop was applied to a rice yield dataset comprising 557 field observations. Using only the selected 15 features, four baseline machine learning models (LightGBM, Random Forest, Support Vector Regression, and Elastic Net) were trained. LightGBM achieved the best single-model performance (RMSE = 465.1 kg/ha, $R^2 = 0.6576$), while a constrained ensemble of all baselines further improved accuracy (RMSE = 463.2 kg/ha, $R^2 = 0.6604$). UniCrop contributes a scalable and transparent data-engineering framework that addresses the primary bottleneck in operational crop yield modelling: the preparation of consistent and harmonised multi-source data. By decoupling data specification from implementation and supporting any crop, region, and time frame through simple configuration updates, UniCrop provides a practical foundation for scalable agricultural analytics. The code and implementation documentation are shared in https://github.com/CoDIS-Lab/UniCrop.

</details>


### [658] [MetaFormer-driven Encoding Network for Robust Medical Semantic Segmentation](https://arxiv.org/abs/2601.00922)
*Le-Anh Tran,Chung Nguyen Tran,Nhan Cach Dang,Anh Le Van Quoc,Jordi Carrabina,David Castells-Rufas,Minh Son Nguyen*

Main category: eess.IV

TL;DR: MFEnNet is proposed to address the complexity of medical image segmentation architectures using a resource-efficient approach and achieves competitive results.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation is vital for effective diagnosis and treatment planning, but existing models require significant computational resources unsuitable for constrained environments.

Method: The authors propose MFEnNet, which integrates MetaFormer into the U-Net backbone for efficient global feature aggregation using pooling transformers, Swish activation, and spatial pyramid pooling.

Result: MFEnNet performs competitively in accuracy while substantially reducing computational costs, verified through experiments on medical segmentation benchmarks.

Conclusion: MFEnNet offers a resource-efficient segmentation framework suitable for medical imaging, balancing accuracy and computational demands effectively.

Abstract: Semantic segmentation is crucial for medical image analysis, enabling precise disease diagnosis and treatment planning. However, many advanced models employ complex architectures, limiting their use in resource-constrained clinical settings. This paper proposes MFEnNet, an efficient medical image segmentation framework that incorporates MetaFormer in the encoding phase of the U-Net backbone. MetaFormer, an architectural abstraction of vision transformers, provides a versatile alternative to convolutional neural networks by transforming tokenized image patches into sequences for global context modeling. To mitigate the substantial computational cost associated with self-attention, the proposed framework replaces conventional transformer modules with pooling transformer blocks, thereby achieving effective global feature aggregation at reduced complexity. In addition, Swish activation is used to achieve smoother gradients and faster convergence, while spatial pyramid pooling is incorporated at the bottleneck to improve multi-scale feature extraction. Comprehensive experiments on different medical segmentation benchmarks demonstrate that the proposed MFEnNet approach attains competitive accuracy while significantly lowering computational cost compared to state-of-the-art models. The source code for this work is available at https://github.com/tranleanh/mfennet.

</details>


### [659] [Uncertainty-Calibrated Explainable AI for Fetal Ultrasound Plane Classification](https://arxiv.org/abs/2601.00990)
*Olaf Yunus Laitinen Imanov*

Main category: eess.IV

TL;DR: The paper develops a trustworthy framework for fetal ultrasound standard-plane classification by synthesizing uncertainty estimation methods with explainable AI approaches, ensuring clinically reliable predictions under challenging conditions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in deploying fetal ultrasound standard-plane classification in real-world settings, including domain shifts, image noise, and poorly calibrated prediction probabilities.

Method: The authors integrate uncertainty estimation approaches (e.g., Monte Carlo dropout, deep ensembles, evidential learning) with explainable AI techniques (e.g., Grad-CAM variants, LIME-style surrogates) into a clinician-focused workflow for fetal ultrasound classification.

Result: The framework is based on the FETAL_PLANES_DB benchmark and includes metrics (e.g., calibration, structured error analysis) and protocols for accuracy and uncertainty evaluation, with discussion on quality control and human-in-the-loop systems.

Conclusion: This framework provides a reproducible and clinically aligned approach for developing fetal ultrasound classifiers that deliver both reliable confidence measures and actionable explanations.

Abstract: Fetal ultrasound standard-plane classification underpins reliable prenatal biometry and anomaly screening, yet real-world deployment is limited by domain shift, image noise, and poor calibration of predicted probabilities. This paper presents a practical framework for uncertainty-calibrated explainable AI in fetal plane classification. We synthesize uncertainty estimation methods (Monte Carlo dropout, deep ensembles, evidential learning, and conformal prediction) with post-hoc and uncertainty-aware explanations (Grad-CAM variants, LIME-style local surrogates, and uncertainty-weighted multi-resolution activation maps), and we map these components to a clinician-facing workflow. Using FETAL_PLANES_DB as a reference benchmark, we define a reporting protocol that couples accuracy with calibration and selective prediction, including expected calibration error, Brier score, coverage-risk curves, and structured error analysis with explanations. We also discuss integration points for quality control and human-in-the-loop review, where uncertainty flags trigger re-acquisition or expert confirmation. The goal is a reproducible, clinically aligned blueprint for building fetal ultrasound classifiers whose confidence and explanations remain trustworthy under noisy acquisition conditions.

</details>


### [660] [YODA: Yet Another One-step Diffusion-based Video Compressor](https://arxiv.org/abs/2601.01141)
*Xingchen Li,Junzhe Zhang,Junqi Shi,Ming Lu,Zhan Ma*

Main category: eess.IV

TL;DR: The paper introduces YODA, a one-step diffusion-based video compressor, achieving state-of-the-art perceptual performance in video compression.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current video compression methods that ignore temporal dependencies and are less efficient in exploiting spatial-temporal correlations.

Method: The proposed method incorporates multiscale features from temporal references for latent generation and coding to enhance representation efficiency. It uses a linear Diffusion Transformer (DiT) for one-step denoising.

Result: YODA outperforms both traditional and deep-learning baselines in multiple perceptual metrics, including LPIPS, DISTS, FID, and KID.

Conclusion: By exploiting spatial-temporal correlations and using efficient one-step diffusion techniques, YODA offers a significant advancement in video compression technology.

Abstract: While one-step diffusion models have recently excelled in perceptual image compression, their application to video remains limited. Prior efforts typically rely on pretrained 2D autoencoders that generate per-frame latent representations independently, thereby neglecting temporal dependencies. We present YODA--Yet Another One-step Diffusion-based Video Compressor--which embeds multiscale features from temporal references for both latent generation and latent coding to better exploit spatial-temporal correlations for more compact representation, and employs a linear Diffusion Transformer (DiT) for efficient one-step denoising. YODA achieves state-of-the-art perceptual performance, consistently outperforming traditional and deep-learning baselines on LPIPS, DISTS, FID, and KID. Source code will be publicly available at https://github.com/NJUVISION/YODA.

</details>


### [661] [Sim2Real SAR Image Restoration: Metadata-Driven Models for Joint Despeckling and Sidelobes Reduction](https://arxiv.org/abs/2601.01541)
*Antoine De Paepe,Pascal Nguyen,Michael Mabelle,Cédric Saleun,Antoine Jouadé,Jean-Christophe Louvigne*

Main category: eess.IV

TL;DR: This paper introduces a unified NN-based framework for SAR image restoration tackling both despeckling and sidelobe reduction, using realistic simulated SAR data for effective real-world application.


<details>
  <summary>Details</summary>
Motivation: Current SAR image restoration methods typically treat despeckling and sidelobe reduction as separate challenges, hindering effective and unified SAR interpretation.

Method: The authors propose a NN framework trained on MOCEM-generated simulated SAR data, using acquisition metadata as auxiliary input to enhance performance and ensure Sim2Real compatibility.

Result: The framework effectively performs joint despeckling and sidelobe reduction, with successful application to real SAR images.

Conclusion: Joint restoration for SAR images can be efficiently achieved using NNs trained on realistic simulated data, improving real-world SAR interpretation capabilities.

Abstract: Synthetic aperture radar (SAR) provides valuable information about the Earth's surface under all weather and illumination conditions. However, the inherent phenomenon of speckle and the presence of sidelobes around bright targets pose challenges for accurate interpretation of SAR imagery. Most existing SAR image restoration methods address despeckling and sidelobes reduction as separate tasks. In this paper, we propose a unified framework that jointly performs both tasks using neural networks (NNs) trained on a realistic SAR simulated dataset generated with MOCEM. Inference can then be performed on real SAR images, demonstrating effective simulation to real (Sim2Real) transferability. Additionally, we incorporate acquisition metadata as auxiliary input to the NNs, demonstrating improved restoration performance.

</details>


<div id='nlin.CD'></div>

# nlin.CD [[Back]](#toc)

### [662] [Identifying recurrent flows in high-dimensional dissipative chaos from low-dimensional embeddings](https://arxiv.org/abs/2601.01590)
*Pierre Beck,Tobias M. Schneider*

Main category: nlin.CD

TL;DR: The paper presents a method to identify unstable periodic orbits (UPOs) in chaotic systems by utilizing a low-dimensional embedding and a convergence algorithm, avoiding challenges of chaotic dynamics and high-dimensional instability.


<details>
  <summary>Details</summary>
Motivation: Understanding turbulence and spatio-temporal chaos through first-principles hinges on identifying UPOs, which are foundational for ergodic theory. Current identification methods face challenges due to high-dimensionality and chaotic dynamics.

Method: The authors propose a loop convergence algorithm within a low-dimensional embedding of the chaotic attractor, leveraging automatic differentiation and latent dynamics maintaining the original attractor’s structure.

Result: The suggested approach avoids exponential error amplification, yielding statistically accurate and interpretable latent dynamics, demonstrating equivalences between latent and physical UPOs for model PDEs and 2D Navier-Stokes equations.

Conclusion: The proposed method facilitates reliable identification of UPOs by exploiting the low-dimensional embedding structure of chaotic attractors, offering insights into complex high-dimensional systems.

Abstract: Unstable periodic orbits (UPOs) are the non-chaotic, dynamical building blocks of spatio-temporal chaos, motivating a first-principles based theory for turbulence ever since the discovery of deterministic chaos. Despite their key role in the ergodic theory approach to fluid turbulence, identifying UPOs is challenging for two reasons: chaotic dynamics and the high-dimensionality of the spatial discretization. We address both issues at once by proposing a loop convergence algorithm for UPOs directly within a low-dimensional embedding of the chaotic attractor. The convergence algorithm circumvents time-integration, hence avoiding instabilities from exponential error amplification, and operates on a latent dynamics obtained by pulling back the physical equations using automatic differentiation through the learned embedding function. The interpretable latent dynamics is accurate in a statistical sense, and, crucially, the embedding preserves the internal structure of the attractor, which we demonstrate through an equivalence between the latent and physical UPOs of both a model PDE and the 2D Navier-Stokes equations. This allows us to exploit the collapse of high-dimensional dissipative systems onto a lower dimensional manifold, and identify UPOs in the low-dimensional embedding.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [663] [The Optimal Sample Complexity of Linear Contracts](https://arxiv.org/abs/2601.01496)
*Mikael Møller Høgsgaard*

Main category: cs.GT

TL;DR: The paper addresses the problem of learning optimal linear contracts in the offline setting, demonstrating that an empirical utility maximization (EUM) algorithm can achieve optimal performance with minimal sample complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize the principal's utility by constructing optimal linear contracts from data when agent types are sampled from an unknown distribution.

Method: The proposed method involves the use of the Empirical Utility Maximization (EUM) algorithm, employing a chaining argument to achieve efficient and optimal sample complexity for approximating the utility function.

Result: The paper proves that the EUM algorithm delivers an ε-approximation of the optimal linear contract with high probability (1-δ) using only $O(ln(1/δ) / ε^2)$ samples, matching theoretical lower bounds.

Conclusion: This research establishes the optimality of the EUM algorithm for the problem of linear contract design, providing rigorous sample complexity guarantees and advancing the understanding of such contracts.

Abstract: In this paper, we settle the problem of learning optimal linear contracts from data in the offline setting, where agent types are drawn from an unknown distribution and the principal's goal is to design a contract that maximizes her expected utility. Specifically, our analysis shows that the simple Empirical Utility Maximization (EUM) algorithm yields an $\varepsilon$-approximation of the optimal linear contract with probability at least $1-δ$, using just $O(\ln(1/δ) / \varepsilon^2)$ samples. This result improves upon previously known bounds and matches a lower bound from Duetting et al. [2025] up to constant factors, thereby proving its optimality. Our analysis uses a chaining argument, where the key insight is to leverage a simple structural property of linear contracts: their expected reward is non-decreasing. This property, which holds even though the utility function itself is non-monotone and discontinuous, enables the construction of fine-grained nets required for the chaining argument, which in turn yields the optimal sample complexity. Furthermore, our proof establishes the stronger guarantee of uniform convergence: the empirical utility of every linear contract is a $\varepsilon$-approximation of its true expectation with probability at least $1-δ$, using the same optimal $O(\ln(1/δ) / \varepsilon^2)$ sample complexity.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [664] [Image Synthesis Using Spintronic Deep Convolutional Generative Adversarial Network](https://arxiv.org/abs/2601.01441)
*Saumya Gupta,Abhinandan,Venkatesh vadde,Bhaskaran Muralidharan,Abhishek Sharma*

Main category: physics.app-ph

TL;DR: The paper proposes a hybrid CMOS-spintronic DCGAN architecture for energy-efficient synthetic image generation, showcasing adaptable implementation and promising performance metrics.


<details>
  <summary>Details</summary>
Motivation: Conventional Von Neumann architectures struggle to meet the computational and energy efficiency demands of GANs, highlighting the need for advanced platforms such as spintronics.

Method: A hybrid CMOS-spintronic hardware approach integrates modified DCGAN layers (e.g., transforming deconvolution to padded convolution) using spintronic elements like skyrmion-based synapses and domain wall-based ReLU for energy-efficient processing.

Result: The architecture achieves competitive Fréchet Inception Distances on Fashion MNIST and Anime Face datasets, with low energy consumption of 4.9 nJ and 24.72 nJ during testing, respectively.

Conclusion: Hybrid CMOS-spintronic implementation presents a promising pathway for efficient neuromorphic computing aligning cutting-edge GAN frameworks with energy constraints.

Abstract: The computational requirements of generative adversarial networks (GANs) exceed the limit of conventional Von Neumann architectures, necessitating energy efficient alternatives such as neuromorphic spintronics. This work presents a hybrid CMOS-spintronic deep convolutional generative adversarial network (DCGAN) architecture for synthetic image generation. The proposed generative vision model approach follows the standard framework, leveraging generator and discriminators adversarial training with our designed spintronics hardware for deconvolution, convolution, and activation layers of the DCGAN architecture. To enable hardware aware spintronic implementation, the generator's deconvolution layers are restructured as zero padded convolution, allowing seamless integration with a 6-bit skyrmion based synapse in a crossbar, without compromising training performance. Nonlinear activation functions are implemented using a hybrid CMOS domain wall based Rectified linear unit (ReLU) and Leaky ReLU units. Our proposed tunable Leaky ReLU employs domain wall position coded, continuous resistance states and a piecewise uniaxial parabolic anisotropy profile with a parallel MTJ readout, exhibiting energy consumption of 0.192 pJ. Our spintronic DCGAN model demonstrates adaptability across both grayscale and colored datasets, achieving Fr'echet Inception Distances (FID) of 27.5 for the Fashion MNIST and 45.4 for Anime Face datasets, with testing energy (training energy) of 4.9 nJ (14.97~nJ/image) and 24.72 nJ (74.7 nJ/image).

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [665] [A Wachspress-based transfinite formulation for exactly enforcing Dirichlet boundary conditions on convex polygonal domains in physics-informed neural networks](https://arxiv.org/abs/2601.01756)
*N. Sukumar,Ritwick Roy*

Main category: math.NA

TL;DR: The paper presents a Wachspress-based transfinite interpolation method for enforcing Dirichlet boundary conditions in physics-informed neural networks on convex polygonal domains.


<details>
  <summary>Details</summary>
Motivation: To address limitations of prior methods in enforcing Dirichlet boundary conditions in neural network applications, particularly for problems involving convex polygonal geometries.

Method: The approach involves using Wachspress coordinates to construct a transfinite interpolant that ensures kinematic admissibility of the trial function in the deep Ritz method for solving boundary-value problems.

Result: The proposed method overcomes previous limitations, such as issues with approximate distance functions, and achieves bounded Laplacians in trial functions. It achieves high accuracy on forward, inverse, and parametrized geometric Poisson boundary-value problems.

Conclusion: Leveraging Wachspress coordinates for transfinite interpolation provides a robust framework for employing neural networks in parametrized convex geometries while ensuring accurate enforcement of Dirichlet boundary conditions.

Abstract: In this paper, we present a Wachspress-based transfinite formulation on convex polygonal domains for exact enforcement of Dirichlet boundary conditions in physics-informed neural networks. This approach leverages prior advances in geometric design such as blending functions and transfinite interpolation over convex domains. For prescribed Dirichlet boundary function $\mathcal{B}$, the transfinite interpolant of $\mathcal{B}$, $g : \bar P \to C^0(\bar P)$, $\textit{lifts}$ functions from the boundary of a two-dimensional polygonal domain to its interior. The trial function is expressed as the difference between the neural network's output and the extension of its boundary restriction into the interior of the domain, with $g$ added to it. This ensures kinematic admissibility of the trial function in the deep Ritz method. Wachspress coordinates for an $n$-gon are used in the transfinite formula, which generalizes bilinear Coons transfinite interpolation on rectangles to convex polygons. The neural network trial function has a bounded Laplacian, thereby overcoming a limitation in a previous contribution where approximate distance functions were used to exactly enforce Dirichlet boundary conditions. For a point $\boldsymbol{x} \in \bar{P}$, Wachspress coordinates, $\boldsymbolλ : \bar P \to [0,1]^n$, serve as a geometric feature map for the neural network: $\boldsymbolλ$ encodes the boundary edges of the polygonal domain. This offers a framework for solving problems on parametrized convex geometries using neural networks. The accuracy of physics-informed neural networks and deep Ritz is assessed on forward, inverse, and parametrized geometric Poisson boundary-value problems.

</details>


<div id='nlin.PS'></div>

# nlin.PS [[Back]](#toc)

### [666] [On the Practical Estimation and Interpretation of Rényi Transfer Entropy](https://arxiv.org/abs/2601.01497)
*Zlata Tabachová,Petr Jizba,Hynek Lavička,Milan Paluš*

Main category: nlin.PS

TL;DR: This paper explores Rényi Transfer Entropy (RTE), a generalization of transfer entropy sensitive to low/high-probability events, and addresses challenges in its estimation using a k-nearest neighbor approach.


<details>
  <summary>Details</summary>
Motivation: The aim is to better understand causal relationships in complex, non-Gaussian systems using RTE, while addressing challenges like estimation accuracy and interpretation issues, particularly for high-dimensional or finite datasets.

Method: The k-nearest neighbor estimator was systematically tested across parameters such as sample size, dimensionality, memory length, and Rényi order $α$, using a variety of synthetic datasets simulating different structural complexities.

Result: The study shows that RTE's explanatory power depends on the correct tuning of estimator parameters, which can effectively capture directional information flow in various scenarios. It also highlights the role of Rényi parameter $α$ in identifying drivers of extreme system behavior.

Conclusion: Reliable and parameter-tuned RTE estimates provide significant insights into causality and information flow in complex systems, making it a useful tool for studying extreme behaviors and causal relations.

Abstract: Rényi transfer entropy (RTE) is a generalization of classical transfer entropy that replaces Shannon's entropy with Rényi's information measure. This, in turn, introduces a new tunable parameter $α$, which accounts for sensitivity to low- or high-probability events. Although RTE shows strong potential for analyzing causal relations in complex, non-Gaussian systems, its practical use is limited, primarily due to challenges related to its accurate estimation and interpretation. These difficulties are especially pronounced when working with finite, high-dimensional, or heterogeneous datasets. In this paper, we systematically study the performance of a k-nearest neighbor estimator for both Rényi entropy (RE) and RTE using various synthetic data sets with clear cause-and-effect relationships inherent to their construction. We test the estimator across a broad range of parameters, including sample size, dimensionality, memory length, and Rényi order $α$. In particular, we apply the estimator to a set of simulated processes with increasing structural complexity, ranging from linear dynamics to nonlinear systems with multi-source couplings. To address interpretational challenges arising from potentially negative RE and RTE values, we introduce three reliability conditions and formulate practical guidelines for tuning the estimator parameters. We show that when the reliability conditions are met and the parameters are calibrated accordingly, the resulting effective RTE estimates accurately capture directional information flow across a broad range of scenarios. Results obtained show that the explanatory power of RTE depends sensitively on the choice of the Rényi parameter $α$. This highlights the usefulness of the RTE framework for identifying the drivers of extreme behavior in complex systems.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [667] [A New Framework for Explainable Rare Cell Identification in Single-Cell Transcriptomics Data](https://arxiv.org/abs/2601.01358)
*Di Su,Kai Ming Ting,Jie Zhang,Xiaorui Zhang,Xinpeng Li*

Main category: q-bio.GN

TL;DR: This paper addresses the need for explaining the rarity of cells detected in single-cell transcriptomics data by introducing a novel framework for explainable anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Rare cell types play a significant role in disease understanding and tissue development, but current methods fail to explain why detected cells are anomalous, mainly due to black-box anomaly detectors, dimensionality reduction, and high-dimensional data constraints.

Method: The proposed framework eliminates PCA, uses advanced anomaly detection and explanation techniques to pinpoint rare cells, and visually represents the gene-level reasons for anomalies.

Result: The framework provides efficient detection of rare cells and their gene-based explanations, resolving key interpretability challenges.

Conclusion: The study advances the field by introducing interpretable methods for understanding rare anomalies in complex single-cell transcriptomics data.

Abstract: The detection of rare cell types in single-cell transcriptomics data is crucial for elucidating disease pathogenesis and tissue development dynamics. However, a critical gap that persists in current methods is their inability to provide an explanation based on genes for each cell they have detected as rare. We identify three primary sources of this deficiency. First, the anomaly detectors often function as "black boxes", designed to detect anomalies but unable to explain why a cell is anomalous. Second, the standard analytical framework hinders interpretability by relying on dimensionality reduction techniques, such as Principal Component Analysis (PCA), which transform meaningful gene expression data into abstract, uninterpretable features. Finally, existing explanation algorithms cannot be readily applied to this domain, as single-cell data is characterized by high dimensionality, noise, and substantial sparsity. To overcome these limitations, we introduce a framework for explainable anomaly detection in single-cell transcriptomics data which not only identifies individual anomalies, but also provides a visual explanation based on genes that makes an instance anomalous. This framework has two key ingredients that are not existed in current methods applied in this domain. First, it eliminates the PCA step which is deemed to be an essential component in previous studies. Second, it employs the state-of-art anomaly detector and explainer as the efficient and effective means to find each rare cell and the relevant gene subspace in order to provide explanations for each rare cell as well as the typical normal cell associated with the rare cell's closest normal cells.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [668] [Bridging Language Gaps: Utilizing Interactive Robots to Teach Cantonese in Real-Life Contexts for Newly-Arrived Children](https://arxiv.org/abs/2601.01234)
*Ka-Yan Fung,Yuxing Tao,Tze-Leung,Rick Lui,Kuen-Fung Sin*

Main category: cs.ET

TL;DR: This paper examines the use of an interactive robot, Boon Boon, to teach Cantonese to non-Chinese-speaking and newly arrived students in Hong Kong to enhance learning engagement and motivation.


<details>
  <summary>Details</summary>
Motivation: To address the language barriers and cultural challenges faced by non-Chinese-speaking and newly arrived students in Hong Kong, particularly the lack of support for learning Cantonese and navigating the local culture.

Method: A four-day learning program involving 14 children where an interactive robot, Boon Boon, teaches Cantonese through real-life contexts.

Result: Preliminary results show that Boon Boon enhanced students' attention to learning and academic achievements.

Conclusion: Robot-empowered learning, like Boon Boon, has the potential to improve language education, particularly for non-native speakers, and future research will explore long-term impacts and scalability.

Abstract: Hong Kong's education system is notably multicultural, including local, non-Chinese-speaking, and newly arrived students (NAS) (Mandarine Chinese-speaking). NAS can guess the meaning of vocabulary but cannot speak out, presenting unique challenges for them, particularly language barriers and cultural differences. These challenges hinder their academic success and social integration, leading to feelings of isolation and demotivation. Current resources often fail to address the emotional well-being of these students and predominantly focus on English language acquisition, leaving a gap in support for learning Cantonese and navigating the local cultural landscape. This study explores the effectiveness of an interactive robot, Boon Boon, in teaching Cantonese through real-life contexts to enhance NAS children learning engagement and motivation. The research questions are: (1) How does interactive robot-empowered scenario learning influence the learning engagement and motivation of NAS in learning Cantonese? and (2) What is the impact of a robot-empowered scenario learning system on the Cantonese language proficiency of NAS? Fourteen children are invited to participate in a four-day learning program with Boon Boon. The preliminary result indicated that Boon Boon drove students' attention to learning and academic achievement. Future research will focus on long-term assessments of robot-empowered learning's effectiveness and explore the scalability of this approach across diverse educational settings and cultural backgrounds.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [669] [AppellateGen: A Benchmark for Appellate Legal Judgment Generation](https://arxiv.org/abs/2601.01331)
*Hongkun Yang,Lionel Z. Wang,Wei Fan,Yiran Hu,Lixu Wang,Chenyu Liu,Shenghong Fu,Haoyang Li,Xin Xu,Jiexin Zheng,Wei Dong*

Main category: cs.CY

TL;DR: The paper introduces 'AppellateGen,' a benchmark and method for generating legal judgments in appellate cases, emphasizing reasoning between trial stages, and presents a Standard Operating Procedure-based system to address the challenge.


<details>
  <summary>Details</summary>
Motivation: Existing research primarily focuses on static fact-to-verdict mappings in first-instance trials, neglecting the complex dialectical reasoning required for appellate (second-instance) case reviews.

Method: The authors created AppellateGen, a dataset with 7,351 case pairs, to model the causal dependency between trial stages. They also developed an SOP-based Legal Multi-Agent System (SLMAS) that breaks judgment generation into distinct stages: issue identification, retrieval, and drafting.

Result: SLMAS enhances logical consistency in judgment generation. However, it highlights that the nuanced reasoning in appellate cases is still a significant challenge for current language models.

Conclusion: AppellateGen and SLMAS offer valuable advances for handling appellate legal reasoning, but further research is necessary to address the intricacies of reasoning in this domain.

Abstract: Legal judgment generation is a critical task in legal intelligence. However, existing research in legal judgment generation has predominantly focused on first-instance trials, relying on static fact-to-verdict mappings while neglecting the dialectical nature of appellate (second-instance) review. To address this, we introduce AppellateGen, a benchmark for second-instance legal judgment generation comprising 7,351 case pairs. The task requires models to draft legally binding judgments by reasoning over the initial verdict and evidentiary updates, thereby modeling the causal dependency between trial stages. We further propose a judicial Standard Operating Procedure (SOP)-based Legal Multi-Agent System (SLMAS) to simulate judicial workflows, which decomposes the generation process into discrete stages of issue identification, retrieval, and drafting. Experimental results indicate that while SLMAS improves logical consistency, the complexity of appellate reasoning remains a substantial challenge for current LLMs. The dataset and code are publicly available at: https://anonymous.4open.science/r/AppellateGen-5763.

</details>


### [670] [The Gray Area: Characterizing Moderator Disagreement on Reddit](https://arxiv.org/abs/2601.01620)
*Shayan Alipour,Shruti Phadke,Seyed Shahabeddin Mousavi,Amirhossein Afsharrad,Morteza Zihayat,Mattia Samory*

Main category: cs.CY

TL;DR: The paper investigates disagreements among volunteer moderators in online communities, focusing on the 'gray area' of moderation, utilizing data from moderation logs.


<details>
  <summary>Details</summary>
Motivation: To better understand the complexities and disagreements in content moderation and the role of human moderators in sustaining online dialogue.

Method: Analysis of 5 years and 4.3 million moderation logs from 24 subreddits, and evaluation using information-theoretic approaches and language models.

Result: One in seven moderation cases are disputed, highlighting the challenges of moderation in cases involving trolling, brigading, and community governance issues. Automated moderation struggles in these scenarios.

Conclusion: Human moderators play a crucial role due to the difficulty of adjudicating gray area moderation cases, suggesting limitations in automated moderation systems and existing tools.

Abstract: Volunteer moderators play a crucial role in sustaining online dialogue, but they often disagree about what should or should not be allowed. In this paper, we study the complexity of content moderation with a focus on disagreements between moderators, which we term the ``gray area'' of moderation. Leveraging 5 years and 4.3 million moderation log entries from 24 subreddits of different topics and sizes, we characterize how gray area, or disputed cases, differ from undisputed cases. We show that one-in-seven moderation cases are disputed among moderators, often addressing transgressions where users' intent is not directly legible, such as in trolling and brigading, as well as tensions around community governance. This is concerning, as almost half of all gray area cases involved automated moderation decisions. Through information-theoretic evaluations, we demonstrate that gray area cases are inherently harder to adjudicate than undisputed cases and show that state-of-the-art language models struggle to adjudicate them. We highlight the key role of expert human moderators in overseeing the moderation process and provide insights about the challenges of current moderation processes and tools.

</details>


### [671] [VEAT Quantifies Implicit Associations in Text-to-Video Generator Sora and Reveals Challenges in Bias Mitigation](https://arxiv.org/abs/2601.00996)
*Yongxu Sun,Michael Saxon,Ian Yang,Anna-Maria Gueorguieva,Aylin Caliskan*

Main category: cs.CY

TL;DR: The study introduces methods to measure societal bias in text-to-video (T2V) generators and evaluates their results, revealing biases in generated content.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by concerns that text-to-video (T2V) generators, such as Sora, may reflect and propagate societal biases in their content.

Method: They introduce and validate two methods, the Video Embedding Association Test (VEAT) and Single-Category VEAT (SC-VEAT), to measure biases in T2V content by comparing it to existing baselines like IAT scenarios and OASIS image categories. They analyze race and gender-based associations across occupations and awards using Sora.

Result: Sora-generated videos exhibit significant biases, favoring European Americans and women more positively. These biases align with real-world demographic distributions, but debiasing methods sometimes exacerbate these biases.

Conclusion: T2V generators can amplify societal biases in generated content unless rigorously evaluated and responsibly managed, as demonstrated by both their baseline and debiasing attempts.

Abstract: Text-to-Video (T2V) generators such as Sora raise concerns about whether generated content reflects societal bias. We extend embedding-association tests from words and images to video by introducing the Video Embedding Association Test (VEAT) and Single-Category VEAT (SC-VEAT). We validate these methods by reproducing the direction and magnitude of associations from widely used baselines, including Implicit Association Test (IAT) scenarios and OASIS image categories. We then quantify race (African American vs. European American) and gender (women vs. men) associations with valence (pleasant vs. unpleasant) across 17 occupations and 7 awards. Sora videos associate European Americans and women more with pleasantness (both d>0.8). Effect sizes correlate with real-world demographic distributions: percent men and White in occupations (r=0.93, r=0.83) and percent male and non-Black among award recipients (r=0.88, r=0.99). Applying explicit debiasing prompts generally reduces effect-size magnitudes, but can backfire: two Black-associated occupations (janitor, postal service) become more Black-associated after debiasing. Together, these results reveal that easily accessible T2V generators can actually amplify representational harms if not rigorously evaluated and responsibly deployed.

</details>


<div id='physics.ins-det'></div>

# physics.ins-det [[Back]](#toc)

### [672] [Autonomous battery research: Principles of heuristic operando experimentation](https://arxiv.org/abs/2601.00851)
*Emily Lu,Gabriel Perez,Peter Baker,Daniel Irving,Santosh Kumar,Veronica Celorrio,Sylvia Britto,Thomas F. Headen,Miguel Gomez-Gonzalez,Connor Wright,Calum Green,Robert Scott Young,Oleg Kirichek,Ali Mortazavi,Sarah Day,Isabel Antony,Zoe Wright,Thomas Wood,Tim Snow,Jeyan Thiyagalingam,Paul Quinn,Martin Owen Jones,William David,James Le Houx*

Main category: physics.ins-det

TL;DR: This paper addresses the difficulties in capturing battery degradation processes using current methods and introduces an AI-physics-based framework to improve experiment efficiency and precision.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges in operando characterization for battery degradation due to the unreliability, unrepresentativeness, and irreproducibility of current methods.

Method: The proposed method introduces Heuristic Operando experiments, where an AI system leverages digital twins to actively predict and capture rare and transient battery failure phenomena.

Result: The new framework redefines experimental efficiency using an entropy-based metric to prioritize meaningful insights, reduces data redundancy, and mitigates damage during experiments.

Conclusion: This AI-driven proactive approach lays the foundation for creating autonomous, efficient, and reliable battery research laboratories guided by FAIR data principles.

Abstract: Unravelling the complex processes governing battery degradation is critical to the energy transition, yet the efficacy of operando characterisation is severely constrained by a lack of Reliability, Representativeness, and Reproducibility (the 3Rs). Current methods rely on bespoke hardware and passive, pre-programmed methodologies that are ill-equipped to capture stochastic failure events. Here, using the Rutherford Appleton Laboratory's multi-modal toolkit as a case study, we expose the systemic inability of conventional experiments to capture transient phenomena like dendrite initiation. To address this, we propose Heuristic Operando experiments: a framework where an AI pilot leverages physics-based digital twins to actively steer the beamline to predict and deterministically capture these rare events. Distinct from uncertainty-driven active learning, this proactive search anticipates failure precursors, redefining experimental efficiency via an entropy-based metric that prioritises scientific insight per photon, neutron, or muon. By focusing measurements only on mechanistically decisive moments, this framework simultaneously mitigates beam damage and drastically reduces data redundancy. When integrated with FAIR data principles, this approach serves as a blueprint for the trusted autonomous battery laboratories of the future.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [673] [MotiBo: The Impact of Interactive Digital Storytelling Robots on Student Motivation through Self-Determination Theory](https://arxiv.org/abs/2601.01218)
*Ka Yan Fung,Tze Leung Rick Lui,Yuxing Tao,Kuen Fung Sin*

Main category: cs.HC

TL;DR: The study investigates how a human-like robot-enhanced interactive storytelling system, MotiBo, can improve student engagement and creativity compared to traditional storytelling methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interactive elements in conventional storytelling methods that limit student engagement and creativity.

Method: A quasi-experimental study design comparing engagement across three storytelling modalities: paper-based, PowerPoint, and robot-assisted (MotiBo), over five days with three student groups.

Result: Students using MotiBo demonstrated statistically significant improvements in behavioural and cognitive engagement compared to traditional methods.

Conclusion: Novel technologies like robot-assisted storytelling systems can enhance the learning experience and foster creativity, with potential for broader future applications in diverse educational contexts.

Abstract: Creativity is increasingly recognized as an important skill in education, and storytelling can enhance motivation and engagement among students. However, conventional storytelling methods often lack the interactive elements necessary to engage students. To this end, this study examines the impact of an interactive digital storytelling system incorporating a human-like robot on student engagement and creativity. The study aims to compare engagement levels across three modalities: paper-based, PowerPoint, and robot-assisted storytelling, MotiBo. Utilizing a quasi-experimental design, this work involves three groups of students who interact with the storytelling system over a five-day learning. Findings reveal that students using MotiBo exhibit statistically significant improvement in behavioural and cognitive engagement compared to those using traditional methods. These results suggest that the integration of novel technologies can effectively enhance the learning experience, ultimately promoting creativity and self-learning ability in educational settings. Future research will investigate the long-term effects of these technologies on learning outcomes and explore their potential for broader applications in diverse educational contexts.

</details>


### [674] [LiveBo: Empowering Non-Chinese Speaking Students through AI-Driven Real-Life Scenarios in Cantonese](https://arxiv.org/abs/2601.01227)
*Ka Yan Fung,Kwong Chiu Fung,Yuxing Tao,Tze Leung Rick Lui,Kuen Fung Sin*

Main category: cs.HC

TL;DR: The paper examines the use of interactive social robots and real-life scenario simulations to improve engagement and language acquisition for non-Chinese speaking students learning Traditional Chinese.


<details>
  <summary>Details</summary>
Motivation: To address the challenges NCS students face in learning Traditional Chinese due to complexity and insufficient vocabulary, and to explore methods that foster better engagement and motivation.

Method: A quasi-experimental study using an AI-powered robot-assisted language learning system, LiveBo, analyzing student interaction with proficiency tests, questionnaires, and interviews.

Result: The approach showed positive effects on behavioral and emotional engagement, motivation, and language learning outcomes among participating students.

Conclusion: Interactive and immersive technologies have significant potential to improve motivation and language acquisition for NCS students, though further comparisons with control groups are planned for the future.

Abstract: Language learning is a multifaceted process. Insufficient vocabulary can hinder communication and lead to demotivation. For non-Chinese speaking (NCS) students, learning Traditional Chinese (Cantonese) poses distinct challenges, particularly due to the complexity of converting spoken and written forms. To address this issue, this study examines the effectiveness of real-life scenario simulations integrated with interactive social robots in enhancing NCS student engagement and language acquisition. The research employs a quasi-experimental design involving NCS students who interact with an AI-driven, robot-assisted language learning system, LiveBo. The study aims to assess the impact of this innovative approach on active participation and motivation. Data are collected through proficiency tests, questionnaires and semi-structured interviews. Findings indicate that NCS students experience positive improvements in behavioural and emotional engagement, motivation and learning outcomes, highlighting the potential of integrating novel technologies in language education. We plan to compare with the control group in the future. This study highlights the significance of interactive and immersive learning experiences in promoting motivation and enhancing language acquisition among NCS students.

</details>


### [675] [Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation](https://arxiv.org/abs/2601.02082)
*Yueyang Wang,Mehmet Dogar,Gustav Markkula*

Main category: cs.HC

TL;DR: This paper proposes using a cognitively inspired pedestrian model in AV simulations to create realistic adversarial scenarios, improving AV control safety and efficiency testing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure safe deployment of AVs by improving simulation-based testing and making realistic adversarial interactions with pedestrians that enhance safety and efficiency.

Method: The authors use a cognitively inspired pedestrian model to introduce inter- and intra-individual variability into simulation, replacing rule-based models for generating realistic behaviours.

Result: The model produced realistic gap acceptance and smoother vehicle decelerations, highlighting unsafe interactions only in specific conditions and improving the AV controller testing process.

Conclusion: Incorporating human-like pedestrian models into adversarial AV testing enhances realism, credibility, and supports safer and optimized AV controller design.

Abstract: Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method's potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation.

</details>


### [676] [A Platform for Interactive AI Character Experiences](https://arxiv.org/abs/2601.01027)
*Rafael Wampfler,Chen Yang,Dillon Elste,Nikola Kovacevic,Philine Witzig,Markus Gross*

Main category: cs.HC

TL;DR: The paper introduces a platform for creating interactive, story-driven digital characters by integrating diverse AI technologies. A proof-of-concept, 'Digital Einstein,' showcases these capabilities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the long-standing challenge of creating interactive, conversational digital characters for story-driven experiences, overcoming technical hurdles like personality, emotions, and integration with real-world environments.

Method: The authors propose a unified system and platform that combines advancements in foundation models, prompt engineering, fine-tuning, and multimodal AI technologies to create believable and interactive characters.

Result: The creation of 'Digital Einstein,' an interactive character representing Albert Einstein, demonstrates the system's effectiveness. It enables users to engage in conversational experiences about Einstein’s life and work.

Conclusion: This platform generalizes to various story-driven or conversational characters, paving the way for more immersive and adaptable character-based interactions.

Abstract: From movie characters to modern science fiction - bringing characters into interactive, story-driven conversations has captured imaginations across generations. Achieving this vision is highly challenging and requires much more than just language modeling. It involves numerous complex AI challenges, such as conversational AI, maintaining character integrity, managing personality and emotions, handling knowledge and memory, synthesizing voice, generating animations, enabling real-world interactions, and integration with physical environments. Recent advancements in the development of foundation models, prompt engineering, and fine-tuning for downstream tasks have enabled researchers to address these individual challenges. However, combining these technologies for interactive characters remains an open problem. We present a system and platform for conveniently designing believable digital characters, enabling a conversational and story-driven experience while providing solutions to all of the technical challenges. As a proof-of-concept, we introduce Digital Einstein, which allows users to engage in conversations with a digital representation of Albert Einstein about his life, research, and persona. While Digital Einstein exemplifies our methods for a specific character, our system is flexible and generalizes to any story-driven or conversational character. By unifying these diverse AI components into a single, easy-to-adapt platform, our work paves the way for immersive character experiences, turning the dream of lifelike, story-based interactions into a reality.

</details>


### [677] [SoulSeek: Exploring the Use of Social Cues in LLM-based Information Seeking](https://arxiv.org/abs/2601.01094)
*Yubo Shu,Peng Zhang,Meng Wu,Yan Chen,Haoxuan Zhou,Guanming Liu,Yu Zhang,Liuxin Zhang,Qianying Wang,Tun Lu,Ning Gu*

Main category: cs.HC

TL;DR: This paper investigates the impact of integrating social cues into LLM-based search systems, focusing on user perception and behavior.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based search systems lack alignment with the socialized cognition underlying natural information seeking, as they primarily rely on semantic features.

Method: The study incorporates design workshops, a prototype system (SoulSeek), a between-subjects study, and mixed-method analyses to explore the effects of social cues.

Result: Social cues enhanced perceived outcomes and experiences, encouraged reflective information behaviors, and highlighted limitations in current LLM-based search systems.

Conclusion: The integration of social cues can improve LLM-based search systems, suggesting design implications for better social-knowledge understanding and controllable, personalized cue settings.

Abstract: Social cues, which convey others' presence, behaviors, or identities, play a crucial role in human information seeking by helping individuals judge relevance and trustworthiness. However, existing LLM-based search systems primarily rely on semantic features, creating a misalignment with the socialized cognition underlying natural information seeking. To address this gap, we explore how the integration of social cues into LLM-based search influences users' perceptions, experiences, and behaviors. Focusing on social media platforms that are beginning to adopt LLM-based search, we integrate design workshops, the implementation of the prototype system (SoulSeek), a between-subjects study, and mixed-method analyses to examine both outcome- and process-level findings. The workshop informs the prototype's cue-integrated design. The study shows that social cues improve perceived outcomes and experiences, promote reflective information behaviors, and reveal limits of current LLM-based search. We propose design implications emphasizing better social-knowledge understanding, personalized cue settings, and controllable interactions.

</details>


<div id='hep-th'></div>

# hep-th [[Back]](#toc)

### [678] [Machine learning modularity](https://arxiv.org/abs/2601.01779)
*Yi Fan,Vishnu Jejjala,Yang Lei*

Main category: hep-th

TL;DR: The paper introduces a machine learning framework based on a sequence-to-sequence transformer and dynamic batching to simplify complex expressions involving elliptic Gamma functions, achieving high accuracy even under extrapolation.


<details>
  <summary>Details</summary>
Motivation: To automate the simplification of expressions with elliptic Gamma functions, commonly used in quantum field theory and string theory.

Method: The authors employ a transformer-based sequence-to-sequence model combined with dynamic batching, leveraging modular transformations (SL(2,Z) and SL(3,Z)) for simplification.

Result: The model achieves over 99% accuracy for in-distribution data and over 90% accuracy in extrapolation cases.

Conclusion: The framework successfully internalizes algebraic rules instead of memorizing and opens new automated computational capabilities for special functions in advanced theoretical physics domains.

Abstract: Based on a transformer based sequence-to-sequence architecture combined with a dynamic batching algorithm, this work introduces a machine learning framework for automatically simplifying complex expressions involving multiple elliptic Gamma functions, including the $q$-$θ$ function and the elliptic Gamma function. The model learns to apply algebraic identities, particularly the SL$(2,\mathbb{Z})$ and SL$(3,\mathbb{Z})$ modular transformations, to reduce heavily scrambled expressions to their canonical forms. Experimental results show that the model achieves over 99\% accuracy on in-distribution tests and maintains robust performance (exceeding 90\% accuracy) under significant extrapolation, such as with deeper scrambling depths. This demonstrates that the model has internalized the underlying algebraic rules of modular transformations rather than merely memorizing training patterns. Our work presents the first successful application of machine learning to perform symbolic simplification using modular identities, offering a new automated tool for computations with special functions in quantum field theory and the string theory.

</details>


<div id='cs.OH'></div>

# cs.OH [[Back]](#toc)

### [679] [A Modular Reference Architecture for MCP-Servers Enabling Agentic BIM Interaction](https://arxiv.org/abs/2601.00809)
*Tobias Heimig-Elschner,Changyu Du,Anna Scheuvens,André Borrmann,Jakob Beetz*

Main category: cs.OH

TL;DR: This paper presents a modular reference architecture for integrating large language models with BIM systems using the Model Context Protocol (MCP), enabling reusable and reproducible workflows.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations in current BIM-side implementations, which are tool-specific and ad hoc, hindering reuse, evaluation, and workflow portability across environments.

Method: The method involves deriving core requirements from a systematic analysis of recent literature, designing a microservice architecture with an adapter contract to decouple MCP from specific BIM-APIs, and implementing a prototype using IfcOpenShell.

Result: The results indicate that the proposed architecture supports reliable workflows, reduces coupling, and provides a reusable framework for agentic BIM interactions.

Conclusion: The paper concludes that the modular architecture addresses key limitations in current systems, enabling more systematic, reusable, and API-agnostic BIM workflows using LLMs.

Abstract: Agentic workflows driven by large language models (LLMs) are increasingly applied to Building Information Modelling (BIM), enabling natural-language retrieval, modification and generation of IFC models. Recent work has begun adopting the emerging Model Context Protocol (MCP) as a uniform tool-calling interface for LLMs, simplifying the agent side of BIM interaction. While MCP standardises how LLMs invoke tools, current BIM-side implementations are still authoring tool-specific and ad hoc, limiting reuse, evaluation, and workflow portability across environments. This paper addresses this gap by introducing a modular reference architecture for MCP servers that enables API-agnostic, isolated and reproducible agentic BIM interactions. From a systematic analysis of recurring capabilities in recent literature, we derive a core set of requirements. These inform a microservice architecture centred on an explicit adapter contract that decouples the MCP interface from specific BIM-APIs. A prototype implementation using IfcOpenShell demonstrates feasibility across common modification and generation tasks. Evaluation across representative scenarios shows that the architecture enables reliable workflows, reduces coupling, and provides a reusable foundation for systematic research.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [680] [Deep versus Broad Technology Search and the Timing of Innovation Impact](https://arxiv.org/abs/2601.00871)
*Likun Cao,James Evans*

Main category: physics.soc-ph

TL;DR: The study explores depth vs. breadth in innovation strategies, analyzing patent citations to understand how search strategies influence technological impact timing and trajectory.


<details>
  <summary>Details</summary>
Motivation: To understand how the choice between specializing or broadening search strategies affects the timing and long-term impact of innovations.

Method: Using machine learning, patent citation networks are mapped in hyperbolic space and analyzed for patterns in impact accumulation across 4.9 million U.S. patents.

Result: Deep searches achieve immediate impact through early adoption within niche communities but have limited long-term diffusion potential. Broad searches initially face resistance but enable greater diffusion and long-term impact.

Conclusion: Balancing depth and breadth is essential for stable impact; organizations can strategically use depth for infrastructure and breadth for diverse applications.

Abstract: This study offers a new perspective on the depth-versus-breadth debate in innovation strategy, by modeling inventive search within dynamic collective knowledge systems, and underscoring the importance of timing for technological impact. Using frontier machine learning to project patent citation networks in hyperbolic space, we analyze 4.9 million U.S. patents to examine how search strategies give rise to distinct temporal patterns in impact accumulation. We find that inventions based on deep search, which relies on a specialized understanding of complex recombination structures, drive higher short-term impact through early adoption within specialized communities, but face diminishing returns as innovations become "locked-in" with limited diffusion potential. Conversely, when inventions are grounded in broad search that spans disparate domains, they encounter initial resistance but achieve wider diffusion and greater long-term impact by reaching cognitively diverse audiences. Individual inventions require both depth and breadth for stable impact. Organizations can strategically balance approaches across multiple inventions: using depth to build reliable technological infrastructure while pursuing breadth to expand applications. We advance innovation theory by demonstrating how deep and broad search strategies distinctly shape the timing and trajectory of technological impact, and how individual inventors and organizations can leverage these mechanisms to balance exploitation and exploration.

</details>
