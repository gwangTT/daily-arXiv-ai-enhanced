<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 58]
- [cs.AR](#cs.AR) [Total: 7]
- [cs.CL](#cs.CL) [Total: 86]
- [cs.CV](#cs.CV) [Total: 141]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.LG](#cs.LG) [Total: 128]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.PL](#cs.PL) [Total: 8]
- [cs.RO](#cs.RO) [Total: 43]
- [cs.SE](#cs.SE) [Total: 37]
- [q-bio.NC](#q-bio.NC) [Total: 9]
- [stat.ML](#stat.ML) [Total: 12]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.NI](#cs.NI) [Total: 10]
- [stat.AP](#stat.AP) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 5]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.HC](#cs.HC) [Total: 7]
- [cs.DB](#cs.DB) [Total: 3]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.DS](#cs.DS) [Total: 4]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [math.AT](#math.AT) [Total: 1]
- [stat.ME](#stat.ME) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 15]
- [cs.MM](#cs.MM) [Total: 1]
- [math.RT](#math.RT) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [eess.SP](#eess.SP) [Total: 27]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.IT](#cs.IT) [Total: 2]
- [math.NA](#math.NA) [Total: 2]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.SD](#cs.SD) [Total: 4]
- [econ.TH](#econ.TH) [Total: 1]
- [math.OC](#math.OC) [Total: 4]
- [cs.CE](#cs.CE) [Total: 2]
- [eess.IV](#eess.IV) [Total: 17]
- [cs.CY](#cs.CY) [Total: 9]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: The paper introduces the "Free Will Equation," a theoretical framework inspired by quantum field theory to enhance AGI with adaptive stochastic decision-making, enabling better creativity and adaptation.


<details>
  <summary>Details</summary>
Motivation: Traditional AGI research emphasizes deterministic goal optimization, but human-like intelligence showcases adaptive spontaneity, essential for creativity, problem-solving, and adaptability.

Method: The framework conceptualizes AGI agents' cognitive states as superpositions of potential actions, collapsing probabilistically upon decision-making. It incorporates quantum field-like mechanisms and intrinsic motivation.

Result: Applying the framework in a non-stationary multi-armed bandit environment led to agents achieving higher rewards and policy diversity compared to traditional approaches.

Conclusion: The Free Will Equation framework demonstrates potential to significantly enhance AGI's adaptability and creativity by introducing a controlled form of stochasticity in decision-making.

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [2] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: The paper introduces DREAMS, a multi-agent framework integrating LLMs to automate DFT simulations for materials discovery, achieving expert-level fidelity and reducing human reliance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in DFT simulations, such as extensive training, error handling, and parameter fine-tuning, to enhance accessibility and efficiency in materials discovery.

Method: DREAMS utilizes a hierarchical framework combining a central LLM planner agent and domain-specific agents for simulations, atomistic structure generation, error handling, and leveraging HPC resources, supported by a shared canvas for preserving context.

Result: DREAMS achieves below 1% average errors in lattice-constant benchmarks and reproduces expert-level adsorption-energy differences, validating its effectiveness in solving both standard and complex problems.

Conclusion: The framework demonstrates autonomous exploration capabilities, reduces human intervention, and offers a scalable solution for high-throughput and democratized computational materials discovery.

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [3] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard addresses safety risks in autonomous web agents by providing a dataset for risk assessment and training specialized guardrail models.


<details>
  <summary>Details</summary>
Motivation: To mitigate the risks of autonomous web agents powered by LLMs taking unintended or harmful actions.

Method: WebGuard introduces a dataset with 4,939 human-annotated actions across three-tier risk categories and conducts evaluations with fine-tuned models.

Result: Fine-tuning with WebGuard improved recall of HIGH-risk actions from 20% to 76% and accuracy from 37% to 80%, but challenges remain.

Conclusion: Although WebGuard enhances web agent safety, current systems lack reliability for deployment in high-stakes scenarios, requiring further advancements.

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [4] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: The paper introduces 'manimator,' an open-source tool that uses Large Language Models (LLMs) and the Manim engine to automate the creation of explanatory animations for STEM topics from research papers and natural language prompts.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this work is to address the difficulty learners face in understanding complex scientific and mathematical concepts from dense research papers. Dynamic visualizations improve comprehension but are difficult and time-intensive to produce without expertise.

Method: The method involves a pipeline where an LLM interprets research papers or natural language prompts to generate structured scene descriptions with key concepts and another LLM converts these descriptions into Manim Python code for animation rendering.

Result: The result is a system capable of automatically generating educational animations for complex STEM topics, using input from text, PDFs, or prompts, thus saving time and effort.

Conclusion: Manimator has the potential to democratize the creation of high-quality educational content by enabling rapid, automated generation of engaging visual explanations for learners without requiring technical expertise.

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [5] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: OnT is a novel ontology embedding method utilizing a Pretrained Language Model (PLM) fine-tuned in hyperbolic space, which integrates textual labels and preserves logical structures for significantly improved performance in prediction and inference tasks.


<details>
  <summary>Details</summary>
Motivation: Current OWL ontology embeddings either ignore textual information or fail to maintain logical hierarchies, leading to suboptimal results in knowledge inference and reasoning tasks.

Method: OnT leverages Pretrained Language Models and fine-tunes them using geometric modeling in hyperbolic space, integrating both textual labels and logical structures such as class hierarchies from Description Logic EL.

Result: OnT significantly outperforms state-of-the-art methods in knowledge prediction and axiom inference tasks across four real-world ontologies, demonstrating improved transfer learning and practical utility in constructing ontologies like SNOMED CT.

Conclusion: OnT provides a robust solution for ontology embeddings, bridging the gap between textual representation and logical reasoning, making it effective for both theoretical tasks and real-world applications.

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [6] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: The paper introduces ProofCompass, a hybrid methodology combining LLMs with specialized mathematical provers, achieving improved efficiency and accuracy in theorem proving.


<details>
  <summary>Details</summary>
Motivation: Existing methods for mathematical reasoning rely heavily on large general-purpose or smaller specialized models, both of which have limitations. Training large specialized models is computationally intensive. There is a need for an efficient approach that leverages the strengths of these models.

Method: The authors propose ProofCompass, a method that combines an LLM with specialized prover methods like DSP-v1.5. The LLM provides guidance for proof strategies and selects intermediate lemmas, avoiding the need for additional model training.

Result: ProofCompass outperformed DSP-v1.5 on the miniF2F benchmark (54.9% to 55.3% accuracy) while requiring 25x fewer attempts (3200 reduced to 128).

Conclusion: ProofCompass demonstrates it is possible to enhance both computational efficiency and accuracy in theorem proving by leveraging the complementary strengths of LLMs and specialized provers without additional training requirements.

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [7] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: This paper presents Nexus Architect, an enhanced system framework that improves reasoning and generalization tasks of language models, outperforming state-of-the-art models on challenging logical problems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the poor generalization capabilities of current Large Reasoning Models (LRMs), which often fail to infer solutions for novel problems and lean heavily on memorization.

Method: The authors developed Nexus Architect, an advanced iteration of the Nexus multi-agent system. It autonomously synthesizes reasoning workflows based on user prompts and examples, integrating strategies and tools while iteratively refining prompts.

Result: Nexus Architect achieved a significant performance boost, demonstrating up to 66% higher pass rates compared to Gemini 2.5 Flash Preview and outperforming other state-of-the-art LRMs by factors of 2.5 to over 3 on logical questions.

Conclusion: The study validates the efficacy of Nexus Architect in enhancing the problem-solving and generalization capabilities of reasoning systems, suggesting its robust potential for advancing logical reasoning in LRMs.

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [8] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: The paper introduces a human-in-the-loop system leveraging reasoning LLMs and non-reasoning models for error mitigation and latency reduction in problem-solving, targeting applications with near-zero error tolerance.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art reasoning LLMs make occasional mistakes and have high latency, problematic for risk-sensitive domains requiring almost error-free outputs.

Method: Combining reasoning LLMs with human intervention based on uncertainty metrics, quantified by reasoning trace lengths, and using a non-reasoning model to manage high-volume queries in the 'Fail Fast, or Ask' framework.

Result: The system reduced the error rate of Qwen3 on challenging MATH problems from 3% to less than 1% (with 7.5% query deferral) and achieved 40% latency reduction and 50% cost savings for DeepSeek R1, although latency drag was noted.

Conclusion: Black-box systems engineering can alleviate reasoning LLM limitations like errors and latency, without modifying LLM internals, presenting viable solutions for high-stakes applications.

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [9] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: The paper explores inverse scaling, where longer reasoning tasks worsen accuracy in Large Reasoning Models (LRMs), identifying distinct failure modes across various evaluation tasks.


<details>
  <summary>Details</summary>
Motivation: To investigate how extending test-time reasoning impacts model performance and accuracy, given the promise of compute scaling for model capabilities.

Method: Four evaluation tasks (counting, regression, deduction, AI risks) were constructed to assess performance, analyzing distinctive failure modes among models like Claude and OpenAI's o-series.

Result: Five reasoning failure modes were identified, including distraction by irrelevant information, overfitting problem framings, shifting to spurious correlations, difficulty with deductive tasks, and amplification of concerning behaviors.

Conclusion: Longer reasoning can degrade performance and reinforce flawed patterns; evaluating models across varied reasoning lengths is essential to mitigate these issues.

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [10] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: The paper introduces Routine, a framework that significantly enhances the accuracy and stability of multi-step agent workflows for enterprise environments, demonstrating remarkable improvements in tool-calling performance and adaptability.


<details>
  <summary>Details</summary>
Motivation: The deployment of agent systems in enterprises often faces challenges such as lack of domain-specific process knowledge, poor execution stability, and incomplete tools.

Method: The proposed Routine framework incorporates a structured, multi-step planning approach with explicit instructions and seamless parameter integration to optimize agent task execution. Fine-tuning and distillation techniques are applied to enhance performance.

Result: Routine increased the tool-calling accuracy significantly in real-world tests, improving GPT-4o's accuracy from 41.1% to 96.3% and Qwen3-14B's from 32.6% to 83.3%. Fine-tuning further boosted Qwen3-14B's accuracy to 88.2%, and distilled datasets raised it to 95.5%, close to GPT-4o.

Conclusion: Routine proves effective in stabilizing agent workflows, improving domain-specific adaptability, and accelerating adoption in enterprise settings. Its practical framework aligns with advancing AI-driven process planning.

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [11] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion is a novel framework that integrates global semantic and local structural learning to improve biomedical knowledge graph reasoning and completion through advanced techniques like tensor decomposition and hybrid scoring.


<details>
  <summary>Details</summary>
Motivation: Biomedical knowledge graphs are essential for areas such as drug discovery but pose challenges in their completion and reasoning. Existing methodologies either lack dynamic structural integration or struggle to develop reciprocal refinement between semantics and structure.

Method: The proposed BioGraphFusion framework uses tensor decomposition for global semantic understanding, combined with an LSTM mechanism to refine relation embeddings during graph propagation. It also employs query-guided subgraph construction and a hybrid scoring mechanism for enhanced reasoning.

Result: BioGraphFusion demonstrated superior performance in various biomedical tasks compared to state-of-the-art methods. The framework was also shown to uncover biologically meaningful pathways in a case study on Cutaneous Malignant Melanoma 1 (CMM1).

Conclusion: BioGraphFusion represents an innovative and effective approach for the integrated learning of semantics and structure, significantly advancing reasoning and insights in biomedical knowledge graphs.

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [12] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico is a modular framework for autonomous agents, designed for embedded systems and browser environments, focusing on resilience and efficiency.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing frameworks for autonomous agents that struggle in real-world, resource-constrained environments.

Method: Developing Amico in Rust for safety and performance, it employs modular design, event-driven architecture, and WebAssembly compatibility.

Result: Amico enables efficient, reactive, and persistent autonomous agents capable of operating in constrained contexts like embedded systems.

Conclusion: Amico provides a robust infrastructure for resilient agents, supporting dynamic contexts and resource-constrained deployments.

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [13] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: The paper investigates grounding language models using multi-modal training for Othello, finding improved performance and robustness with the inclusion of visual data.


<details>
  <summary>Details</summary>
Motivation: To explore the efficacy of multi-modal training for language models in grounding symbolic reasoning in a structured, rule-based world.

Method: The authors introduced VISOTHELLO, a model combining move histories and board images for next-move prediction, compared it against mono-modal baselines, and tested it under perturbations.

Result: Multi-modal training yielded better performance and more robust internal representations compared to mono-modal baselines.

Conclusion: Grounding language models in visual input enhances their ability to infer structured world representations and improves robustness.

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [14] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: This paper introduces OE-Assist, a framework leveraging large language models (LLMs) for automated and semi-automated ontology evaluation, reducing errors and effort associated with competency question (CQ) verification.


<details>
  <summary>Details</summary>
Motivation: Current ontology evaluation methods are labor-intensive, error-prone, and costly, requiring expert effort for competency question verification. There is a need for more efficient evaluation approaches.

Method: The authors utilized a dataset of 1,393 CQs paired with corresponding ontologies to test LLM-assisted ontology evaluation. They developed OE-Assist, which integrates automated LLM-based verification and suggestions for users employing tools like Protégé.

Result: Automated LLM evaluation tools such as o1-preview and o3-mini demonstrated performance levels comparable to the average user's manual verification, indicating their utility in ontology evaluation.

Conclusion: Large language models can effectively assist and enhance ontology evaluation processes, showing promise for reducing effort and improving the consistency of competency question verification.

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [15] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: The paper introduces the Coordinate Heart System (CHS), a geometric and mathematical framework for emotion representation in AI, with enhanced capabilities for multidimensional stability modeling.


<details>
  <summary>Details</summary>
Motivation: Current emotion representation methods in artificial intelligence have limitations, particularly in their ability to fully cover and mathematically handle complex emotional states, necessitating a more robust and comprehensive framework.

Method: The authors developed an eight-emotion geometric model mapped onto a unit circle, utilizing vector operations and coordinate systems to mathematically represent emotions. Additionally, they integrated a recalibrated stability parameter to dynamically assess complex emotional and psychological states through algorithms and temporal tracking.

Result: The framework resolved emotion space gaps in previous models, demonstrated mathematical proofs for the necessity of eight emotions, and validated its real-time application through experimental case studies. It effectively handled emotionally conflicted states and psychological complexities.

Conclusion: The CHS provides a novel mathematical foundation for emotion modeling, surpassing traditional categorical models with its ability to address nuanced emotional states and deliver real-time AI emotion representation applications.

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [16] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: Story point estimation in agile software development can be streamlined using a machine learning-based comparative learning framework, which simplifies and outperforms traditional manual and regression-based methods.


<details>
  <summary>Details</summary>
Motivation: Traditional agile story point estimation techniques are tedious and labor-intensive, requiring significant manual input from developers. These methods depend heavily on project-specific historical data, demanding improved automation and cognitive load reduction.

Method: The authors propose a machine learning framework that uses comparative judgments (pairs of backlog items judged on relative effort) instead of assigning specific story point values. This method trains a model to predict story points based on these judgments.

Result: The technique was evaluated with 23,313 manual estimates across 16 projects. The learned model achieved a 0.34 Spearman's rank correlation coefficient, comparable to or better than traditional regression-based models trained on ground truth values.

Conclusion: The comparative learning approach is more efficient and less cognitively burdensome for developers, outperforming state-of-the-art regression-based methods for story point estimation.

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [17] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: This paper investigates the risks posed by malicious multi-agent systems (MAS), emphasizing decentralized groups causing more harm.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the threat of coordinated malicious actions by AI-driven multi-agent systems in high-risk fields like misinformation and fraud.

Method: The authors devised a simulation framework for malicious MAS, analyzing centralized and decentralized coordination in misinformation and e-commerce fraud scenarios.

Result: Findings demonstrate that decentralized systems adapt strategies better and outperform centralized systems in executing harmful actions, even under intervention.

Conclusion: Decentralized malicious AI systems pose significant risks and necessitate improved detection measures and counterstrategies.

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [18] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo is a novel framework for automated multi-turn evaluation of large language model (LLM) agents, demonstrating scalability and efficiency in uncovering failures and probing conversational depth.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for benchmarking and testing LLMs are static and quickly become obsolete due to the dynamic and context-sensitive nature of LLMs.

Method: Neo integrates a Question Generation Agent and an Evaluation Agent into a shared context hub, uses a probabilistic state model to simulate diverse conversational scenarios, and applies these tests to assess chatbot robustness.

Result: Neo uncovered edge-case failures with a 3.3% break rate similar to human red-teamers, achieved 10-12X higher throughput, generating 180 coherent test questions in 45 mins compared to 16 hours of human effort.

Conclusion: Neo offers a scalable, self-evolving, and model-agnostic framework for efficient LLM testing, promoting broader behavioral exploration and reproducible high-fidelity evaluation.

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [19] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI is a platform for safety evaluation of large language models (LLMs) that converts safety policies into adversarial prompts and evaluates model responses. The findings show significant disparities in safety performances across models and domains.


<details>
  <summary>Details</summary>
Motivation: Evaluate and address the inconsistent safety performance of LLMs across varying real-world safety domains to ensure responsible AI usage.

Method: Introduced the Aymara AI platform, which transforms safety policies into adversarial prompts, scores model responses with an AI-based rater validated by human judgments, and evaluated 20 LLMs across 10 safety domains.

Result: Wide disparities in safety performance were observed, with scores ranging from 86.2% to 52.4%; well-established domains like Misinformation performed well, whereas underspecified domains like Privacy & Impersonation had poor scores (mean = 24.3%).

Conclusion: LLM safety is inconsistent and context-dependent. Tools like Aymara AI are essential for robust safety evaluation and oversight in responsible AI development.

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [20] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: This paper explores using generative AI models for urban planning, highlighting opportunities and challenges while proposing research directions.


<details>
  <summary>Details</summary>
Motivation: Urban planning could greatly benefit from generative AI, but its integration faces conceptual and practical gaps.

Method: The authors survey generative AI approaches like VAEs, GANs, transformers, and diffusion models to inform urban design and identify current limitations.

Result: The study reveals limited integration of urban theory, spatial resolutions, knowledge augmentation, and real-world interaction in AI-assisted urban planning.

Conclusion: Future research should focus on theory-guided urban design, leveraging digital twins, and fostering human-machine collaboration in participatory urban planning.

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [21] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: The paper introduces AgentFly, a framework designed to integrate reinforcement learning (RL) with language model (LM) agents, enabling scalable and effective task training through multi-turn interactions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of systematic study and integration of reinforcement learning (RL) with language model (LM) agents, enhancing their autonomy in task completion.

Method: AgentFly adapts RL methods with token-level masking for multi-turn interactions and provides a user-friendly decorator-based interface. It ensures high-throughput training with asynchronous execution and centralized resource management, along with prebuilt tools and environments.

Result: The framework allows successful agent training across various tasks, showcasing its scalability and extensibility.

Conclusion: AgentFly demonstrates that integrating RL with LM agents can systematically enhance their autonomous task-solving capabilities, opening avenues for further research and applications.

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [22] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: The paper introduces InsightX Agent, a Large Multimodal Model-based framework for reliable, interpretable, and interactive X-ray Non-Destructive Testing (NDT) analysis.


<details>
  <summary>Details</summary>
Motivation: Current deep-learning-based NDT systems lack interactivity, interpretability, and self-assessment capabilities, undermining reliability and operator trust.

Method: The framework employs a Large Multimodal Model (LMM) to orchestrate processes between the SDMSD detector and the EGR tool for reasoning and interpretation. SDMSD optimizes defect detection, while the EGR tool refines outputs through a structured review process.

Result: InsightX Agent achieved a high F1-score of 96.35% on the GDXray+ dataset, demonstrating superior performance, interpretability, and reliability.

Conclusion: The proposed InsightX Agent showcases the potential of agentic LLM frameworks to transform industrial X-ray NDT by providing reliable, interpretable, and actionable results.

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [23] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: This paper investigates the suitability of Large Language Models (LLMs) in autonomous decision-making tasks within Markov Decision Processes (MDPs). It finds that while LLMs outperform classical reinforcement learning (RL) in simpler tasks, they face challenges in complex environments without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To explore whether LLMs can effectively leverage their pre-trained knowledge to enhance decision-making tasks compared to traditional RL methods.

Method: The study uses online structured prompting strategies to evaluate zero-shot performance of LLMs across sequential decision-making tasks, comparing their outputs against classical RL techniques.

Result: LLMs perform well initially in simpler environments but fail in complex scenarios due to problems with reasoning, planning, and confusion introduced by feedback mechanisms.

Conclusion: Advanced methods like hybrid strategies, fine-tuning, and integrating memory are necessary to overcome LLMs' limitations in sophisticated decision-making tasks.

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [24] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: The paper proposes the "Endless Tuning" method for AI, aiming to prevent human replacement and fill the responsibility gap, demonstrated in three decision-making applications.


<details>
  <summary>Details</summary>
Motivation: To address ethical concerns in AI by avoiding human replacement and closing the responsibility gap in decision-making processes.

Method: Double mirroring design with the relational approach, employing a protocol tested on loan decisions, pneumonia diagnosis, and art style recognition using deep learning and XAI algorithms.

Result: The experiments showed that users felt in full control of decision-making and that accountability can align with liability in case of damage.

Conclusion: A relational and hermeneutic AI deployment approach enhances user experience and bridges gaps in ethics and accountability.

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [25] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: This paper explores the transformative role of Agentic AI, powered by Large Language Models, in elderly care, emphasizing its potential benefits and challenges.


<details>
  <summary>Details</summary>
Motivation: The global ageing population demands innovative solutions for elderly care, prompting exploration of technologies like Agentic AI.

Method: The paper analyzes how Agentic AI can enable proactive decision-making, personalized health tracking, cognitive support, and environmental management for older adults.

Result: Agentic AI shows promise in improving elderly independence and quality of life but raises critical concerns regarding ethics, privacy, and transparency.

Conclusion: To responsibly integrate Agentic AI into elderly care, ethical safeguards, transparency, and a human-centered approach must be prioritized while addressing academic research gaps.

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [26] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: The paper explores facets and distances in propositional abduction to enhance understanding of heterogeneity in explanations while keeping computational complexity manageable.


<details>
  <summary>Details</summary>
Motivation: To address the computational challenges of counting and enumeration in propositional abduction and improve an understanding of variability in explanations.

Method: The authors introduce facets of propositional abduction—designating literals as relevant or dispensable—and analyze their behavior systematically under Post's framework.

Result: Facets improve comprehension of explanation variability, and the use of distances between explanations aids in understanding their heterogeneity and homogeneity in different computational settings.

Conclusion: Reasoning with facets and distances offers a more granular analysis of propositional abduction while balancing computational complexity and insight.

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [27] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: The paper introduces AlphaAlign, a new RL framework to improve the safety and utility of large language models without compromising their helpfulness or requiring extensive supervised data.


<details>
  <summary>Details</summary>
Motivation: Large language models, despite their safety-aware pretraining, still generate harmful content or suffer performance degradation after safety alignment. Existing alignment methods often rely on shortcuts or require intensive supervision, limiting their effectiveness.

Method: AlphaAlign employs reinforcement learning with a dual-reward system: a verifiable safety reward for justified refusals of harmful queries and a normalized helpfulness reward for high-quality responses to benign inputs, fostering proactive safety reasoning.

Result: AlphaAlign reduces harmful outputs, minimizes over-refusals, improves general task performance, and enhances robustness against unknown jailbreak attempts.

Conclusion: AlphaAlign offers a simple, effective, and efficient safety alignment method that balances safety and utility while promoting deeper safety reasoning in LLMs.

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [28] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: This study introduces a Forced-Choice Neural Cognitive Diagnostic Model (FCNCD) using deep learning for psychometric tests, addressing limitations in traditional models and improving accuracy, interpretability, and robustness.


<details>
  <summary>Details</summary>
Motivation: Psychometric tests are vital for personnel, career, and health assessments, with forced-choice formats minimizing response distortion. Current methods need improvement in handling forced-choice item types effectively.

Method: The study develops FCNCD that uses deep learning to model interactions between participants and items through multilayer neural networks and employs monotonicity assumptions for interpretable diagnostic results.

Result: Experiments on real-world and simulated datasets demonstrate the FCNCD's accuracy, robustness, and enhanced interpretability in forced-choice psychometric evaluations.

Conclusion: FCNCD represents a meaningful advancement, addressing traditional limitations in forced-choice psychometric tests while leveraging deep learning for practical and reliable applications.

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [29] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: This paper introduces an adversarial attack method using Differential Evolution (DE) to create optimized prompt suffixes for misleading RAG systems, demonstrating effectiveness against other approaches while evading detection.


<details>
  <summary>Details</summary>
Motivation: To address the vulnerability of Retrieval-Augmented Generation (RAG) systems susceptible to adversarial prompt attacks, which manipulate output reliability.

Method: A gradient-free approach using Differential Evolution (DE) to evolve adversarial prompt suffixes, optimizing for misleading document retrieval, evaluated on BEIR QA datasets for attack success and readability.

Result: The proposed DE method achieved competitive or better success rates compared to existing methods, used fewer tokens, and evaded detection with near-chance accuracy.

Conclusion: The DE approach is effective for prompt attacks on RAG systems, ensures high attack success while maintaining readability, and avoids detection by adversarial detection models.

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [30] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: This paper introduces the Causal Action Influence Score (CAIS), a new intrinsic reward for reinforcement learning that robustly isolates an agent's causal influence in noisy environments.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning agents struggle to identify their causal influence in noisy scenarios because they rely on correlation-based rewards.

Method: CAIS uses the 1-Wasserstein distance to measure the divergence between distributions of sensory outcomes conditioned on actions, isolating causal influence while ignoring environmental noise.

Result: In a simulated noisy environment, CAIS allowed agents to filter noise, discover their causal influence, and learn appropriate behaviors, where traditional correlation-based rewards failed.

Conclusion: CAIS represents a psychologically plausible mechanism that enables agents to develop a robust sense of agency, enhancing adaptability in autonomous systems.

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [31] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: This research integrates background knowledge in the form of DL-Lite ontologies into automated planning under open-world semantics, showcasing manageable complexity and implementation efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of standard automated planning systems that lack integration of rich background knowledge, especially ontologies under open-world semantics.

Method: Proposes using eKABs for ontology-based action conditions and the coherence update semantics for ontology-aware action effects, coupled with a polynomial compilation into classical planning.

Result: The approach achieves a complexity on par with prior methods and demonstrates efficient implementation, as evidenced in an evaluation involving performance benchmarks.

Conclusion: The integration of DL-Lite ontologies enhances planning systems without compromising computational efficiency, laying groundwork for ontology-driven automated planning.

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [32] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: The paper proposes CSI, an AI framework for diagnosing 118 oral diseases, achieving high diagnostic accuracy through expert-mimicking hierarchical reasoning.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the challenge of diagnosing oral diseases, which often present overlapping symptoms, by creating a system that emulates expert clinical reasoning.

Method: CSI combines a fine-tuned multimodal CLIP model and ChatGLM-6B language model to execute a Hierarchical Diagnostic Reasoning Tree (HDRT) for differential diagnosis across fast and standard modes.

Result: CSI achieved 73.4% accuracy in its Fast Mode and 89.5% in Standard Mode using a hierarchical reasoning framework, validated on both internal and external datasets.

Conclusion: Emulating expert clinical reasoning enhances the diagnostic utility of AI systems, with CSI demonstrating promising accuracy and clinical potential for oral disease diagnosis.

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [33] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: The paper evaluates human mobility in The Line, a futuristic smart city concept, using simulations that incorporate advanced AI techniques to model transportation efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to assess the feasibility of ensuring smooth human mobility in an unprecedented urban design, The Line, and its implications for sustainable and efficient transportation.

Method: They designed a hybrid simulation framework combining agent-based modeling, reinforcement learning, supervised learning, and graph neural networks to simulate multi-modal transportation across varying scenarios.

Result: Experiments showed average commute times of less than 8.4 minutes, satisfaction rates above 89%, and reachability indices greater than 91%. Removing AI modules worsened outcomes, increasing commute times by 85% and reducing reachability below 70%.

Conclusion: Freedom of movement within The Line is both conceptually and operationally feasible when supported by adaptive AI systems, eco-friendly infrastructure, and dynamic feedback mechanisms.

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [34] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: The paper addresses identifying conditional causal effects with a graph represented by MPDAG, offering formulas, generalized do calculus, and algorithms.


<details>
  <summary>Details</summary>
Motivation: To enhance the identification of conditional causal effects within equivalence classes of graphs (MPDAG) constrained by background knowledge.

Method: The authors present a formula for cases where the conditioning set is unaltered, extend the do calculus for MPDAGs, and introduce a complete identification algorithm.

Result: The paper achieves methodologies for identifying conditional causal effects within MPDAGs.

Conclusion: These contributions enable accurate causal effect identification in restricted graph structures using provided formulas and algorithms.

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [35] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover, a framework utilizing general-purpose Large Language Models (LLMs) and Lean 4 proof environment, achieves a state-of-the-art success rate in formal theorem proving without requiring model specialization.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with formal work like theorem proving in Lean 4, requiring costly fine-tuning. The paper aims to explore the untapped capabilities of LLMs for formal theorem proving without specialization.

Method: Delta Prover uses an agent-based framework focused on reflective decomposition, proof repair, and subproblem management via a custom DSL, integrating general-purpose LLMs with Lean 4.

Result: Delta Prover attained 95.9% success on the miniF2F-test benchmark, outperforming specialized models and demonstrating scalability during test-time.

Conclusion: General-purpose LLMs, guided by effective frameworks like Delta Prover, show strong potential for theorem proving, offering a cost-efficient alternative to specialized approaches.

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [36] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: This paper introduces a soft evaluation indicator leveraging Explainable AI for arc fault diagnosis and proposes a lightweight balanced neural network for competitive accuracy.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the trustworthiness of AI-based arc fault diagnosis models in identifying arc faults effectively.

Method: The study defines correct arc fault explanations, incorporates Explainable AI, real fault experiments, and develops a lightweight balanced neural network for diagnosis accuracy and feature analysis.

Result: Experiments on two arc fault datasets with varying sample times and noise levels demonstrate the effectiveness of the proposed soft evaluation indicator and competitive performance of the neural network.

Conclusion: The research enhances the reliability and interpretability of arc fault diagnosis models, helping practitioners make informed decisions confidently.

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [37] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: The paper introduces a novel framework called DMGC for unsupervised multimodal graph clustering, addressing hybrid neighborhood patterns in such graphs.


<details>
  <summary>Details</summary>
Motivation: Current unsupervised learning approaches lack comprehensive exploration of multimodal graphs, which combine heterogeneous data with structured interconnections.

Method: The DMGC framework decomposes hybrid multimodal graphs into two views: a homophily-enhanced graph and modality-specific heterophily-aware graphs, using a dual-frequency fusion mechanism with self-supervised alignment objectives.

Result: DMGC achieves state-of-the-art performance across multimodal and multi-relational graph datasets, showcasing its effectiveness and generalizability.

Conclusion: DMGC successfully addresses the challenge of hybrid neighborhood patterns in multimodal graphs, enhancing integration and reducing category confusion without requiring label supervision.

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [38] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat, a multi-agent framework using LLMs, addresses knowledge transfer challenges in injection molding via a modular architecture integrating field data, environmental variables, and documented knowledge.


<details>
  <summary>Details</summary>
Motivation: To address the critical challenge in injection molding of preserving and transferring field knowledge amid workforce retirement and communication barriers.

Method: IM-Chat employs retrieval-augmented generation (RAG) and tool-calling agents combined with a modular architecture. It leverages documented data and a data-driven condition generator to infer optimal settings.

Result: Models like GPT-4o performed better in complex scenarios during evaluations, showcasing higher relevance and correctness scores in manufacturing-related tasks.

Conclusion: IM-Chat demonstrates scalability and generalizability as an AI-assisted decision-making tool for industrial workflows in injection molding.

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [39] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: The paper introduces Cognitive Degradation as a novel vulnerability in agentic AI systems, characterized by internal failures affecting memory, planning, and logic. It proposes the Qorvex Security AI Framework with six lifecycle stages and seven runtime controls for resilience.


<details>
  <summary>Details</summary>
Motivation: Traditional AI vulnerabilities like prompt injection are external threats, but this paper focuses on novel internal failures that impair agent functionality over time.

Method: The authors developed the Qorvex Security AI Framework, featuring a six-stage lifecycle and seven runtime controls to detect and mitigate Cognitive Degradation in agentic AI systems.

Result: The introduced framework offers lifecycle-aware, cross-platform defenses to monitor and respond to memory starvation, logic collapse, and related failures in real-time.

Conclusion: Cognitive Degradation is formally recognized as a critical vulnerability class, and the proposed framework enables resilient behaviors in agentic AI systems through proactive detection and mitigation.

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [40] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: The paper introduces two innovative methods, GRPO and OSPO, to enhance multi-agent reinforcement learning for ride-sharing platforms, bypassing value function estimations while optimizing performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges facing real-time, uncertain, large-scale ride-sharing environments, where conventional methods relying on Q-value or V-value estimations encounter instability and bias issues.

Method: This paper introduces GRPO to replace traditional baselines with a group average reward, reducing estimation and training bias. Additionally, OSPO simplifies policy optimization by leveraging one-step rewards in homogeneous fleets.

Result: Using a Manhattan ride-sharing dataset, the proposed GRPO and OSPO methods outperform traditional approaches in optimizing pickup times and increasing the served orders, all with simple networks.

Conclusion: The methods provide promising alternatives to conventional MARL frameworks for real-time dynamic ride-sharing, offering efficiency and better scalability to challenging environments.

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [41] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD introduces a novel approach to address offline RL challenges by dynamically retrieving high-quality states for better trajectory planning via a diffusion-based model.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome key challenges in offline reinforcement learning, specifically addressing issues with dataset sparsity and lack of transition overlap, hindering long-horizon planning.

Method: RAD combines non-parametric state retrieval with a condition-guided diffusion model to identify high-return states and dynamically plan trajectories towards them.

Result: Experiments demonstrate RAD delivers competitive or better performance in diverse benchmarks compared to existing methods.

Conclusion: RAD validates its potential as an effective solution for offline RL, improving generalization and enhancing trajectory stitching through dynamic retrieval and generation mechanisms.

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [42] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: The paper introduces an end-to-end model using graph attention and LSTM networks for predicting future process behavior, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: Enhance process predictions by leveraging object-centric event logs effectively, addressing challenges in extracting relevant information and model building.

Method: A combination of graph attention network for encoding activity relationships and LSTM network for managing temporal dependencies.

Result: The model was evaluated on four event logs and showcased competitive performance compared to state-of-the-art methods.

Conclusion: The proposed model validates its capability in improving process predictions, particularly for next activity and event time analysis, using object-centric event logs.

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [43] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: This paper examines activity batching in business processes, proposing a Pareto optimization approach to discover optimal batching policies balancing cost, effort, and waiting time.


<details>
  <summary>Details</summary>
Motivation: Managers face the challenge of balancing batching size and frequency while managing cost, processing effort, and waiting time.

Method: The study introduces intervention heuristics to iteratively improve batching policies using meta-heuristics like hill-climbing, simulated annealing, and reinforcement learning, paired with simulation evaluations.

Result: Experimental comparisons revealed improvements in convergence, diversity, and cycle time gains of Pareto-optimal batching policies through heuristic-guided interventions.

Conclusion: The proposed approach demonstrates effectiveness in optimizing batching policies, ensuring improved trade-offs for practical business applications.

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [44] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1 introduces a vision-language model with reinforcement learning fine-tuning to tackle complex chart reasoning, supported by programmatic data synthesis and a two-stage training strategy, demonstrating significant advances in the chart domain.


<details>
  <summary>Details</summary>
Motivation: To extend R1-Style methods, which were traditionally focused on mathematical reasoning and code intelligence, into the multimodal domain, specifically targeting the research challenges posed by chart-based reasoning tasks.

Method: Chart-R1 employs a novel data synthesis approach for generating high-quality chart reasoning datasets and utilizes a two-stage training strategy: Chart-COT for chain-of-thought supervision and Chart-RFT for numerically sensitive reinforcement learning.

Result: Experimental results on open benchmarks and the self-built ChartRQA dataset show Chart-R1 outperforms existing chart-domain methods and performs comparably to advanced large-scale models like GPT-4o and Claude-3.5.

Conclusion: Chart-R1 provides an innovative framework for tackling complex chart reasoning tasks in the multimodal domain, showcasing the effectiveness of integrating fine-tuning strategies with tailored data synthesis approaches.

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [45] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: The paper introduces HAMLET, a multi-agent framework for creating immersive online theatrical performances using large language models (LLM).


<details>
  <summary>Details</summary>
Motivation: The goal is to overcome the limitations of existing LLM-based drama generation methods, including a lack of initiative in AI agents, inability to interact with the physical environment, and reliance on detailed user input, which hinder interactivity and immersion.

Method: HAMLET generates a narrative blueprint from a simple topic and gives each actor an autonomous mind to make independent decisions influenced by their goals, emotions, and environment. It also allows interactions with the physical environment, the outcomes of which are shared with other actors to affect dynamic storytelling.

Result: Experimental evaluations demonstrate that the HAMLET framework delivers expressive and coherent theatrical experiences across various aspects such as character performance, narrative quality, and interaction experience.

Conclusion: HAMLET represents a step forward in achieving interactive narratives by providing autonomy to AI actors and including environmental interactions for better immersion and interactivity in online drama performances.

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [46] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: The study tests whether large language models (LLMs) simulate internal world models or rely purely on statistical associations through experiments using pulley systems.


<details>
  <summary>Details</summary>
Motivation: To understand whether modern LLMs actually reason using internal world models or merely rely on statistical heuristics.

Method: LLMs were evaluated with cognitive science-inspired tests, focusing on mechanical advantage (MA) estimation and spatial reasoning using pulley systems.

Result: Models performed marginally above chance on MA estimation, correlated with true MA, differentiated functional from non-functional systems, but struggled with more complex reasoning tasks.

Conclusion: While models showcase some ability to exploit statistical patterns and spatial relations, they struggle with detailed structural reasoning. Cognitive science methods are useful for probing AI world-modeling capacities.

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [47] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: This paper enhances offline reinforcement learning by utilizing parametric dependencies in transition dynamics, offering methods to increase data efficiency for safe policy improvement (SPI).


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in offline reinforcement learning by incorporating extra parametric dependency information in the environment's transition dynamics.

Method: The authors introduced a parametric SPI algorithm, a game-based preprocessing technique for pruning redundant actions, and an advanced pruning method leveraging satisfiability modulo theory (SMT) solving.

Result: The proposed methods showed significantly improved data efficiency for SPI while maintaining reliability guarantees, as demonstrated through empirical studies and an ablation analysis.

Conclusion: Integrating parametric dependencies and advanced preprocessing techniques in SPI can lead to more data-efficient learning without compromising safe decision-making.

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [48] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: This paper analyzes metrics for evaluating LLMs using multiple-choice questions and introduces a novel metric called worst accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of thorough assessment of evaluation metrics for LLMs and the issue of answer fluctuation during evaluation using MCQs.

Method: The authors proposed a metric assessment protocol that analyzes evaluation methodologies by examining their connections with fluctuation rates and performance.

Result: There is a strong link between existing metrics and answer fluctuation. A newly introduced metric, worst accuracy, showed the highest association with the proposed assessment protocol.

Conclusion: The study emphasizes the importance of considering fluctuation rates in evaluation, with the novel metric worst accuracy serving as a more reliable measure.

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [49] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: The paper introduces an adapter-based method to give StarCraft II AI agents the capability to adapt their strategies based on tactical directives, maintaining core skills and competitive performance.


<details>
  <summary>Details</summary>
Motivation: Current StarCraft II AI agents are strong but unable to adjust strategies dynamically based on high-level tactical guidance.

Method: An adapter-based solution adds lightweight modules to a pre-trained policy network, conditioned by a tactical tensor, and constraints are applied to preserve core agent competencies.

Result: Experimental findings demonstrate successful modulation of agent behavior across various tactical dimensions while retaining competitive capabilities.

Conclusion: The approach provides efficient tactical control and customization in real-time strategy games with minimal computational demand.

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [50] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: The paper investigates agentic AI's role in autonomously handling anomalies in complex systems.


<details>
  <summary>Details</summary>
Motivation: To enhance anomaly detection and response processes by leveraging agentic AI capabilities.

Method: Examines how agentic AI can autonomously manage anomalies, reducing reliance on traditional human-centric approaches.

Result: Agentic AI shows promising potential in improving anomaly detection and management autonomy.

Conclusion: Agentic AI can effectively transform traditional anomaly management systems, highlighting its significance in complex scenarios.

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [51] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: The paper proposes g-AMIE, an AI system designed to take medical histories under strict guardrails and enable asynchronous oversight by physicians, improving diagnostic accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to ensure patient safety while leveraging AI for diagnostic dialogues, addressing the regulatory requirement for physician oversight in clinical decision-making.

Method: The proposed method involves developing g-AMIE, which includes structured safety guardrails for AI history-taking. g-AMIE submits assessments to primary care physicians for review via a clinician cockpit interface, enabling asynchronous oversight.

Result: In a virtual OSCE study, g-AMIE excelled over nurse practitioners, physician assistants, and PCPs operating under similar guardrails in case intake, summaries, and proposed management plans, showing enhanced efficiency and decision quality.

Conclusion: The study underscores the potential of asynchronous oversight frameworks to integrate diagnostic AI into clinical workflows, ensuring safety and improving care quality despite limitations in mimicking existing practices.

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [52] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: This paper introduces LAPO, a framework to make reasoning models more efficient in terms of token use while enhancing accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the problem of excessive token generation in large reasoning models, which impacts computational efficiency, especially for simpler tasks.

Method: The paper proposes LAPO, a framework utilizing a two-stage reinforcement learning process where models first learn statistical reasoning patterns and then embed them for adaptive reasoning based on problem complexity.

Result: LAPO reduces token usage by up to 40.9% and improves accuracy by 2.3% according to mathematical reasoning benchmarks.

Conclusion: LAPO allows reasoning models to allocate computational resources efficiently, ensuring reasoning quality while maintaining flexibility.

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [53] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent is a multi-agent system developed to improve Gas optimization in smart contracts, achieving notable savings in deployment costs.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in smart contracts' Gas usage due to non-optimal coding practices and limitations of current manual and LLM-based optimization methods.

Method: GasAgent consists of four specialized agents (Seeker, Innovator, Executor, Manager) working collaboratively in a closed-loop system to identify, validate, and apply Gas-saving techniques.

Result: Successfully optimized 82 out of 100 real-world verified contracts with an average Gas saving of 9.97%. In broader testing, 79.8% of LLM-generated contracts showed savings between 4.79% and 13.93%.

Conclusion: GasAgent proves effective in optimizing smart contract deployment Gas, combines compatibility with existing solutions, and serves as an enhancement layer for LLM-based contract development.

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [54] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: Service ecosystems are increasingly complex, and traditional methods struggle with emergence analysis. This paper presents the EAMI framework, leveraging large language models and dynamic multi-agent intention analysis for more effective results.


<details>
  <summary>Details</summary>
Motivation: The complexity of service ecosystems due to trends like IoT, cloud computing, and service computing requires a shift from traditional individual-based causal methods to approaches that can analyze group-level dynamic interactions and agent intentions.

Method: The EAMI framework uses a dual-perspective thought track mechanism involving an Inspector Agent and an Analysis Agent to extract agent intentions. K-means clustering is applied to detect phase transitions in intentions, and an Intention Temporal Emergence diagram is used for dynamic visualization.

Result: Experiments on O2O service systems and the Stanford AI Town validate the effectiveness, generalizability, and efficiency of the EAMI framework. Ablation studies further support its utility.

Conclusion: EAMI offers an innovative and interpretable method for analyzing abnormal emergence and causality in complex service ecosystems, marking a significant advancement in this field.

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [55] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: The paper systematically analyzes challenges in aligning Federated Learning (FL) with Trustworthy AI (TAI) requirements.


<details>
  <summary>Details</summary>
Motivation: To address privacy concerns and ethical, legal, and technical requirements in deploying AI systems in sensitive domains using FL.

Method: They use TAI requirements as a framework to identify, classify, and assess FL's key challenges in meeting these standards.

Result: The paper provides a structured analysis of existing efforts, trends, and gaps in making FL compatible with TAI.

Conclusion: There are significant challenges in aligning FL with TAI, necessitating focused efforts to address distributed nature-related issues and other outlined gaps.

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [56] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: This paper introduces HBPO, a reinforcement learning framework that adjusts a model's reasoning depth based on problem complexity, achieving both increased efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models are computationally inefficient as they use uniform reasoning strategies irrespective of problem complexity. There is a need for an approach to optimize reasoning depth without compromising results.

Method: HBPO uses hierarchical budget exploration to allocate computational resources efficiently by dividing samples into subgroups with distinct token budgets. It incorporates budget-aware reward mechanisms to align computational effort with task complexity.

Result: HBPO achieves a 60.6% reduction in average token usage and a 3.14% improvement in accuracy across four reasoning benchmarks.

Conclusion: Reasoning efficiency and capability can be optimized simultaneously using HBPO. The framework enables models to adapt reasoning depth to problem-specific requirements, ensuring both efficiency and effectiveness.

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [57] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: This paper explores temporal cognition in Large Language Models (LLMs), discovering that they adhere to cognitive principles like the Weber-Fechner law and encode temporal information logarithmically.


<details>
  <summary>Details</summary>
Motivation: Understanding emergent human-like cognitive patterns in LLMs, specifically temporal cognition, can enhance comprehension and alignment of their internal mechanisms.

Method: The study used similarity judgment tasks, neuronal analysis, and pretrained embedding models to investigate LLMs' temporal cognition. This involved identifying temporal-preferential neurons, probing year representations across layers, and analyzing the training corpus' intrinsic temporal structure.

Result: LLMs exhibit a subjective temporal reference point, logarithmic compression for temporal distance, temporal-preferential neurons, hierarchical temporal representation, and inherent temporal structures in their training data.

Conclusion: LLMs internally construct their temporal cognition through nuanced processes inspired by training data. This raises the potential for AI developing alien cognitive patterns, suggesting the need for alignment strategies focusing on internal representational systems.

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [58] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: Google's Gemini 2.5 Pro tackled the 2025 IMO problems, correctly solving 5 out of 6 with specialized techniques.


<details>
  <summary>Details</summary>
Motivation: To evaluate Large Language Models' ability to solve Olympiad-level mathematical problems.

Method: Applied Google's Gemini 2.5 Pro with tailored pipeline design and prompt engineering on the IMO 2025 problems.

Result: Correctly solved 5 problems, showing the impact of using optimal techniques.

Conclusion: LLMs like Gemini can achieve high success rates on complex mathematical tasks with proper methodologies.

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [59] [SpeedLLM: An FPGA Co-design of Large Language Model Inference Accelerator](https://arxiv.org/abs/2507.14139)
*Peipei Wang,Wu Guan,Liping Liang,Zhijun Wang,Hanqing Luo,Zhibin Zhang*

Main category: cs.AR

TL;DR: SpeedLLM, optimized for Tinyllama on Xilinx Alevo U280, improves edge computing with innovations reducing latency and energy consumption, achieving 4.8x faster performance and 1.18x lower energy use.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to enhance the performance and efficiency of neural network accelerators for edge computing, particularly for the Tinyllama framework.

Method: The authors implement SpeedLLM on the Xilinx Alevo U280 platform, utilizing techniques like data stream parallelism, memory reuse strategy, and Llama2 operator fusion to optimize the performance and energy efficiency of edge computing systems.

Result: SpeedLLM delivers significant performance improvements, including 4.8x faster execution and 1.18x reduced energy consumption, compared to traditional Tinyllama implementations.

Conclusion: SpeedLLM demonstrates its capability to effectively enhance both computational speed and energy efficiency for edge devices, showcasing its potential as a high-performance edge computing accelerator.

Abstract: This paper introduces SpeedLLM, a neural network accelerator designed on the
Xilinx Alevo U280 platform and optimized for the Tinyllama framework to enhance
edge computing performance. Key innovations include data stream parallelism, a
memory reuse strategy, and Llama2 operator fusion, which collectively reduce
latency and energy consumption. SpeedLLM's data pipeline architecture optimizes
the read-compute-write cycle, while the memory strategy minimizes FPGA resource
demands. The operator fusion boosts computational density and throughput.
Results show SpeedLLM outperforms traditional Tinyllama implementations,
achieving up to 4.8* faster performance and 1.18* lower energy consumption,
offering improvements in edge devices.

</details>


### [60] [Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need](https://arxiv.org/abs/2507.14397)
*Michael Davies,Neal Crago,Karthikeyan Sankaralingam,Christos Kozyrakis*

Main category: cs.AR

TL;DR: The study examines the performance bottlenecks in transformer-based LLM inference, focusing on memory bandwidth, memory capacity, and synchronization overhead in distributed systems.


<details>
  <summary>Details</summary>
Motivation: To identify fundamental performance constraints in distributed LLM inference systems and explore how future hardware advancements could mitigate them.

Method: A hardware-agnostic performance model was created to analyze technologies such as HBM3, HBM4, SRAM designs, and DRAM scaling techniques for serving LLMs in distributed setups.

Result: It highlights the significant resource requirements (100s of GB memory per server), criticality of high memory bandwidth, synchronization latency needs (~1μs), and DRAM-based designs' advantage in throughput per cost and energy.

Conclusion: Future hardware advancements and optimized deployment strategies can unlock higher throughput in LLM inference, but fundamental limits require algorithmic and design innovations.

Abstract: This paper presents a limit study of transformer-based large language model
(LLM) inference, focusing on the fundamental performance bottlenecks imposed by
memory bandwidth, memory capacity, and synchronization overhead in distributed
inference systems. We develop a hardware-agnostic performance model that
abstracts away implementation details, enabling the analysis of a wide range of
current and near-future hardware technologies. Our analysis spans from current
HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems
based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers
SRAM-based designs and scaling techniques from distributed clusters with
varying numbers of chips to wafer-scale integration. Our key findings for
auto-regressive decoding are: i) serving LLMs requires 100s of GB per server to
serve a model instance; ii) high memory bandwidth is critical for high per-user
throughput; iii) exposed synchronization latencies to achieve collective
communication must be around 1us else they make the memory bandwidth
ineffective; iv) DRAM-based designs have a fundamental advantage in terms of
system-level efficiency as measured in throughput per cost or watt; and v)
hardware designs can easily reach 2000+ user token/sec but getting to 10,000+
tokens/sec will need smaller models, smaller context, or other forms of
algorithmic advances. This study provides valuable insights into the
fundamental performance limits of LLM inference, highlighting the potential
benefits of future hardware advancements and guiding the optimization of LLM
deployment strategies.

</details>


### [61] [Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge](https://arxiv.org/abs/2507.14651)
*Joren Dumoulin,Pouya Houshmand,Vikram Jain,Marian Verhelst*

Main category: cs.AR

TL;DR: The paper presents solutions for deploying hybrid vision transformers on edge devices, addressing challenges of diverse NN layers and large data tensors.


<details>
  <summary>Details</summary>
Motivation: Facilitate efficient execution of hybrid vision transformers on resource-constrained edge devices.

Method: Introduces a configurable PE array for different layer types, temporal loop re-ordering, and layer fusion for optimized scheduling.

Result: Implemented accelerator achieves energy efficiency of 1.39 TOPS/W and processing rate of 25.6 GMACs/s.

Conclusion: Effective techniques enable efficient hardware acceleration of hybrid vision transformers on edge devices.

Abstract: Hybrid vision transformers combine the elements of conventional neural
networks (NN) and vision transformers (ViT) to enable lightweight and accurate
detection. However, several challenges remain for their efficient deployment on
resource-constrained edge devices. The hybrid models suffer from a widely
diverse set of NN layer types and large intermediate data tensors, hampering
efficient hardware acceleration. To enable their execution at the edge, this
paper proposes innovations across the hardware-scheduling stack: a.) At the
lowest level, a configurable PE array supports all hybrid ViT layer types; b.)
temporal loop re-ordering within one layer, enabling hardware support for
normalization and softmax layers, minimizing on-chip data transfers; c.)
further scheduling optimization employs layer fusion across inverted bottleneck
layers to drastically reduce off-chip memory transfers. The resulting
accelerator is implemented in 28nm CMOS, achieving a peak energy efficiency of
1.39 TOPS/W at 25.6 GMACs/s.

</details>


### [62] [GCC: A 3DGS Inference Architecture with Gaussian-Wise and Cross-Stage Conditional Processing](https://arxiv.org/abs/2507.15300)
*Minnan Pei,Gang Li,Junwen Si,Zeyu Zhu,Zitao Mo,Peisong Wang,Zhuoran Song,Xiaoyao Liang,Jian Cheng*

Main category: cs.AR

TL;DR: This paper introduces GCC, a novel accelerator targeting efficient 3D Gaussian Splatting inference by optimizing the dataflow and reducing computational and data movement overhead.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inefficiencies in conventional 3DGS accelerators, specifically the redundant Gaussian preprocessing and repetitive data loading during rendering.

Method: The GCC accelerator uses innovative dataflow techniques: cross-stage conditional processing and Gaussian-wise rendering, alongside an alpha-based boundary identification method.

Result: Experimental results show that GCC achieves superior performance and energy efficiency compared to the previous state-of-the-art accelerator, GSCore.

Conclusion: GCC significantly enhances the speed and energy efficiency of 3DGS inference, making it a promising solution for mobile neural rendering applications.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading neural rendering
technique for high-fidelity view synthesis, prompting the development of
dedicated 3DGS accelerators for mobile applications. Through in-depth analysis,
we identify two major limitations in the conventional decoupled
preprocessing-rendering dataflow adopted by existing accelerators: 1) a
significant portion of preprocessed Gaussians are not used in rendering, and 2)
the same Gaussian gets repeatedly loaded across different tile renderings,
resulting in substantial computational and data movement overhead. To address
these issues, we propose GCC, a novel accelerator designed for fast and
energy-efficient 3DGS inference. At the dataflow level, GCC introduces: 1)
cross-stage conditional processing, which interleaves preprocessing and
rendering to dynamically skip unnecessary Gaussian preprocessing; and 2)
Gaussian-wise rendering, ensuring that all rendering operations for a given
Gaussian are completed before moving to the next, thereby eliminating
duplicated Gaussian loading. We also propose an alpha-based boundary
identification method to derive compact and accurate Gaussian regions, thereby
reducing rendering costs. We implement our GCC accelerator in 28nm technology.
Extensive experiments demonstrate that GCC significantly outperforms the
state-of-the-art 3DGS inference accelerator, GSCore, in both performance and
energy efficiency.

</details>


### [63] [The New LLM Bottleneck: A Systems Perspective on Latent Attention and Mixture-of-Experts](https://arxiv.org/abs/2507.15465)
*Sungmin Yun,Seonyong Park,Hwayong Nam,Younjoo Lee,Gunjun Lee,Kwanhee Kyung,Sangpyo Kim,Nam Sung Kim,Jongmin Kim,Hyungyo Kim,Juhwan Cho,Seungmin Baek,Jung Ho Ahn*

Main category: cs.AR

TL;DR: The paper discusses architectural shifts like Multi-head Latent Attention (MLA) and Mixture-of-Experts (MoE) that reduce reliance on specialized hardware for Transformers.


<details>
  <summary>Details</summary>
Motivation: Traditional Transformer models are hindered by computational inefficiencies, especially Multi-Head Attention (MHA), which motivates the investigation of solutions to these bottlenecks.

Method: The authors study architectural innovations (MLA and MoE) to evaluate their impact on computation profiles and hardware needs using metrics like arithmetic intensity.

Result: MLA achieves compute-bound performance while distributed MoE experts balance workload profiles, removing the need for specialized attention hardware.

Conclusion: Future Transformer hardware should focus on balanced systems with compute resources, memory bandwidth, and interconnects for large-scale model demands.

Abstract: Computational workloads composing traditional Transformer models are starkly
bifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic
intensity, while feedforward layers are compute-bound. This dichotomy has long
motivated research into specialized hardware to mitigate the MHA bottleneck.
  This paper argues that recent architectural shifts, namely Multi-head Latent
Attention (MLA) and Mixture-of-Experts (MoE), challenge the premise of
specialized attention hardware. We make two key observations. First, the
arithmetic intensity of MLA is over two orders of magnitude greater than that
of MHA, shifting it close to a compute-bound regime well-suited for modern
accelerators like GPUs. Second, by distributing MoE experts across a pool of
accelerators, their arithmetic intensity can be tuned through batching to match
that of the dense layers, creating a more balanced computational profile.
  These findings reveal a diminishing need for specialized attention hardware.
The central challenge for next-generation Transformers is no longer
accelerating a single memory-bound layer. Instead, the focus must shift to
designing balanced systems with sufficient compute, memory capacity, memory
bandwidth, and high-bandwidth interconnects to manage the diverse demands of
large-scale models.

</details>


### [64] [When Pipelined In-Memory Accelerators Meet Spiking Direct Feedback Alignment: A Co-Design for Neuromorphic Edge Computing](https://arxiv.org/abs/2507.15603)
*Haoxiong Ren,Yangu He,Kwunhang Wong,Rui Bao,Ning Lin,Zhongrui Wang,Dashan Shang*

Main category: cs.AR

TL;DR: This paper proposes a novel software-hardware solution for energy-efficient training of Spiking Neural Networks (SNNs) using Spiking Direct Feedback Alignment (SDFA), integrated with a Resistive Random Access Memory (RRAM)-based architecture called PipeSDFA.


<details>
  <summary>Details</summary>
Motivation: SNNs are appealing for resource-constrained edge devices due to their energy efficiency, but traditional training methods are computationally intensive. This paper aims to address these challenges.

Method: The authors introduce SDFA, a hardware-friendly training algorithm, and implement a three-level pipelined IMC architecture called PipeSDFA to parallelize SNN training.

Result: Experimental results show that the proposed PipeSDFA reduces training time by 1.1X~10.5X, energy consumption by 1.37X~2.1X, and incurs less than 2% accuracy loss across five datasets compared to the baseline.

Conclusion: PipeSDFA offers an efficient solution to SNN training bottlenecks, combining software and hardware innovations to significantly reduce computational demands while maintaining accuracy.

Abstract: Spiking Neural Networks (SNNs) are increasingly favored for deployment on
resource-constrained edge devices due to their energy-efficient and
event-driven processing capabilities. However, training SNNs remains
challenging because of the computational intensity of traditional
backpropagation algorithms adapted for spike-based systems. In this paper, we
propose a novel software-hardware co-design that introduces a hardware-friendly
training algorithm, Spiking Direct Feedback Alignment (SDFA) and implement it
on a Resistive Random Access Memory (RRAM)-based In-Memory Computing (IMC)
architecture, referred to as PipeSDFA, to accelerate SNN training.
Software-wise, the computational complexity of SNN training is reduced by the
SDFA through the elimination of sequential error propagation. Hardware-wise, a
three-level pipelined dataflow is designed based on IMC architecture to
parallelize the training process. Experimental results demonstrate that the
PipeSDFA training accelerator incurs less than 2% accuracy loss on five
datasets compared to baselines, while achieving 1.1X~10.5X and 1.37X~2.1X
reductions in training time and energy consumption, respectively compared to
PipeLayer.

</details>


### [65] [VeriRAG: A Retrieval-Augmented Framework for Automated RTL Testability Repair](https://arxiv.org/abs/2507.15664)
*Haomin Qi,Yuyang Du,Lihao Zhang,Soung Chang Liew,Kexin Chen,Yining Du*

Main category: cs.AR

TL;DR: The paper introduces VeriRAG, a novel LLM-assisted framework for automated Design for Testability (DFT) corrections in electronic design automation, leveraging RAG methodology and a curated DFT dataset.


<details>
  <summary>Details</summary>
Motivation: To explore the underutilized potential of LLMs in DFT corrections within the domain of electronic design automation, enabling automated, accurate debugging and compliance adjustments.

Method: Developed VeriRAG, an LLM-assisted DFT correction framework utilizing a Retrieval-Augmented Generation approach, an autoencoder-based similarity measure for precise code referencing, and an iterative revision pipeline. Created VeriDFT, a dataset for DFT-aware RTL repairs.

Result: VeriRAG achieved a 7.72-fold improvement in successful DFT repair rates compared to a zero-shot baseline. Ablation studies validated the contributions of each framework component.

Conclusion: The integration of VeriRAG and VeriDFT demonstrates significant advancements in automating DFT corrections, establishing a new benchmark in LLM-aided CAD for EDA tools. Resources are open-sourced for further research.

Abstract: Large language models (LLMs) have demonstrated immense potential in
computer-aided design (CAD), particularly for automated debugging and
verification within electronic design automation (EDA) tools. However, Design
for Testability (DFT) remains a relatively underexplored area. This paper
presents VeriRAG, the first LLM-assisted DFT-EDA framework. VeriRAG leverages a
Retrieval-Augmented Generation (RAG) approach to enable LLM to revise code to
ensure DFT compliance. VeriRAG integrates (1) an autoencoder-based similarity
measurement model for precise retrieval of reference RTL designs for the LLM,
and (2) an iterative code revision pipeline that allows the LLM to ensure DFT
compliance while maintaining synthesizability. To support VeriRAG, we introduce
VeriDFT, a Verilog-based DFT dataset curated for DFT-aware RTL repairs. VeriRAG
retrieves structurally similar RTL designs from VeriDFT, each paired with a
rigorously validated correction, as references for code repair. With VeriRAG
and VeriDFT, we achieve fully automated DFT correction -- resulting in a
7.72-fold improvement in successful repair rate compared to the zero-shot
baseline (Fig. 5 in Section V). Ablation studies further confirm the
contribution of each component of the VeriRAG framework. We open-source our
data, models, and scripts at https://github.com/yuyangdu01/LLM4DFT.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [66] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter is introduced as a multimodal, long-form writing assistant that addresses the limitations of LLMs in specialized domains by leveraging a curated offline knowledge base and incorporating innovative techniques.


<details>
  <summary>Details</summary>
Motivation: Specialized domains like finance, medicine, and law face challenges with existing LLMs due to limited domain-specific knowledge and hallucination tendencies.

Method: Deploy a customizable pipeline with task decomposition, outline generation, multimodal retrieval, and section-by-section writing. It uses hierarchical knowledge representation and operates on an offline knowledge base.

Result: DeepWriter excels in financial report generation, achieving better factual accuracy and article quality compared to existing baselines.

Conclusion: DeepWriter effectively addresses challenges in domain-specific writing, offering a robust solution for generating professional-grade, verifiable documents.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [67] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: The paper examines the durability of edited knowledge in LLMs under fine-tuning and finds edited information is more prone to forgetting than pre-trained knowledge.


<details>
  <summary>Details</summary>
Motivation: The need to correct errors, add new information, or adapt behavior in LLMs often requires efficient model editing, but the persistence of these edits under fine-tuning is unclear.

Method: Systematic analysis of the interaction between various fine-tuning objectives and model editing techniques, combined with tests on knowledge retention.

Result: Edited knowledge is more likely to be forgotten during fine-tuning compared to pre-trained knowledge; freezing layers improves retention of edits.

Conclusion: Future editing methods must account for robustness under fine-tuning to ensure practical use of model edits, a challenge partially addressed by layer-freezing techniques.

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [68] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: This paper proposes SMACS, a framework that integrates multiple open-source LLMs to outperform closed-source LLMs in various tasks, showcasing the strength of collaborative open-source systems.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the potential of open-source AI collaborations, aiming to challenge and surpass the performance of closed-source LLMs using a collective approach.

Method: The authors introduce a two-step framework: Retrieval-based Prior Selection (RPS) to pick high-performing LLMs for a given question, and Exploration-Exploitation-Driven Posterior Enhancement (EPE) to encourage response diversity and refine quality.

Result: SMACS, leveraging 15 open-source LLMs, shows superior performance compared to leading closed-source LLMs such as Claude-3.7-Sonnet, GPT-4.1, and GPT-o3-mini by significant percentages.

Conclusion: By combining multiple open-source LLMs in a structured framework, SMACS not only breaks performance barriers of individual models but also showcases the potential of collective intelligence in the AI domain.

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [69] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: Introducing PoliAnalyzer, a system that uses NLP and logic to analyze privacy policies and compare them with user preferences, achieving high accuracy and helping reduce user cognitive burden.


<details>
  <summary>Details</summary>
Motivation: People rarely read or understand online privacy policies, so there is a need for a tool that can analyze these policies and inform users about compliance with their data-sharing preferences.

Method: Combines NLP for extracting data usage practices and logical inference to compare these against user preferences. Uses a formal data policy language and evaluates on a legal expert-curated dataset.

Result: Achieved F1-scores of 90-100% in identifying data usage practices and highlighted only the 4.8% of policy segments violating user preferences, significantly reducing cognitive burden.

Conclusion: PoliAnalyzer enables scalable, automated privacy policy analysis and reveals common violations, empowering users to focus on key issues and fostering discussions on privacy policies.

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [70] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: This paper evaluates transformer-based NLP models and LSTM models for detecting bipolar disorder using social media text, finding RoBERTa to be the best performer with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the underdiagnosis of bipolar disorder by leveraging NLP models on social media data, aiming to overcome challenges posed by subtle symptoms and social stigma.

Method: The study evaluates transformer architectures (e.g., RoBERTa, BERT) and LSTM models using different types of word embeddings on a validated Reddit dataset to detect bipolar disorder patterns.

Result: RoBERTa achieved the best F1 score (~98%) among tested transformer models, while LSTMs with static embeddings performed poorly. DistilBERT offered a good trade-off between accuracy and efficiency.

Conclusion: Contextualized language models are effective for detecting bipolar disorder, suggesting their potential application in mental health screenings and emphasizing critical model selection insights in such applications.

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [71] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: This paper investigates biases in large language models with respect to user identity markers across high-stakes domains, revealing significant disparities in model responses.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the gaps in understanding how identity information affects LLM responses in real-world applications.

Method: A comprehensive analysis was conducted across five high-stakes domains by probing LLM behaviors with queries reflecting varied identity markers.

Result: Findings show that race, gender, and age influence LLM responses, leading to biases in medical advice, political information, and salary recommendations.

Conclusion: The study underscores the harmful impact of deploying biased LLMs in user-facing applications and recommends thorough bias assessments before implementation.

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [72] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: The paper introduces CCL-XCoT, a two-stage fine-tuning framework to reduce hallucination in multilingual large language models (MLLMs), achieving up to 62% improvement in low-resource languages without external tools.


<details>
  <summary>Details</summary>
Motivation: Multilingual large language models often face hallucination issues, particularly while working with low-resource languages due to the imbalance in training data. This affects their reliability in domain-specific tasks.

Method: The proposed CCL-XCoT framework involves two stages: first improving cross-lingual semantic alignment using curriculum-based contrastive learning with next-token prediction, and second, employing a cross-lingual Chain-of-Thought (XCoT) prompting strategy during fine-tuning.

Result: CCL-XCoT reduces hallucination rates by up to 62% and significantly enhances factual knowledge transfer between languages, showcasing superior performance in low-resource settings.

Conclusion: The CCL-XCoT framework presents an effective solution to hallucination in multilingual models, improving both accuracy and cross-lingual knowledge transfer while avoiding reliance on external retrieval systems.

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [73] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: This paper constructs a graph representing relationships between language models and datasets, analyzing its structure and interdependencies.


<details>
  <summary>Details</summary>
Motivation: The study aims to address vulnerabilities and biases in language models by understanding the relationships and dynamics between models and datasets in the supply chain.

Method: The authors systematically collect data on the relationships between models and datasets, creating a directed heterogeneous graph with over 397,376 nodes and 453,469 edges, followed by an analysis of its characteristics.

Result: They identified key insights regarding the graph's structure, sparsity, power-law degree distribution, core-periphery dynamics, dataset influence, strong interdependence, and its ongoing evolution.

Conclusion: Analyzing the LLM supply chain graph helps detect risks, improve fairness, and ensure compliance within the AI ecosystem.

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [74] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix is an automatic prompt optimization tool for Large Language Models that enhances prompts without requiring manual tuning or domain expertise.


<details>
  <summary>Details</summary>
Motivation: The manual nature of prompt engineering for LLMs makes it inaccessible and inefficient for non-experts, prompting the need for automatic optimization.

Method: It uses a meta-prompt-based optimizer and DSPy-powered compiler with modular design to analyze user intent, generate synthetic data, optimize prompts, and refine them using cost-aware objectives.

Result: Promptomatix demonstrates competitive or superior performance against existing libraries while reducing prompt length and computational cost across five task categories.

Conclusion: The framework offers a scalable and efficient solution to optimize LLM prompts, making them more accessible without requiring manual intervention or expertise.

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [75] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope is a customized LVLM designed for comprehensive chart understanding across various chart types, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations in existing LVLMs for domain-specific tasks in scientific chart comprehension, particularly their weak generalization to diverse chart types and lack of targeted pre-training for chart-data alignment.

Method: Developed a data generation pipeline to synthesize paired data across diverse chart types. Introduced a Dual-Path training strategy for improved data understanding and reasoning.

Result: ChartScope demonstrates enhanced comprehension performance across diverse chart types, supported by results from a newly established benchmark, ChartDQA.

Conclusion: ChartScope advances LVLM capabilities for scientific chart comprehension, providing improved generalizability and reasoning. Tools and resources are publicly available for further exploration.

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [76] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: The paper explores LLM-based selective translation as a more effective alternative to standard translation methods for aligning multilingual language models to low-resource languages like Hindi.


<details>
  <summary>Details</summary>
Motivation: To bridge the performance gap between English and low-resource languages in multilingual LLMs, focusing on the challenges of limited high-quality data and the limitations of standard translation techniques.

Method: The authors propose LLM-based selective translation, preserving untranslatable elements while translating the rest of the text. They systematically evaluate its performance, addressing noise filtering and combining translated and original English data. Experiments utilize translations by Google Cloud Translation (GCP) and Llama-3.1-405B.

Result: Selective translation outperformed vanilla translation in improving multilingual alignment for Hindi, demonstrating its promise as an effective method for low-resource settings.

Conclusion: Selective translation is a practical solution for enhancing multilingual alignment in LLMs, particularly for low-resource languages. Its focus on preserving structure and non-translatable content offers significant advantages over traditional methods.

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [77] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: The study explores whether large language models (LLMs) grasp temporal meaning in narratives like humans but finds that LLMs show challenges in semantic understanding and narrative reasoning.


<details>
  <summary>Details</summary>
Motivation: To examine to what extent LLMs exhibit human-like cognitive abilities versus merely advanced pattern recognition, particularly in processing linguistic aspects in narratives.

Method: An 'Expert-in-the-Loop' probing pipeline with targeted experiments was used to analyze LLMs' semantic representations and pragmatic inferences in narratives.

Result: LLMs were found to over-rely on prototypical patterns, produce inconsistent judgments, and struggle with aspect-based causal reasoning, implying limited narrative understanding.

Conclusion: LLMs process linguistic aspects differently from humans, lacking deep narrative comprehension; the study also proposed a standardized experimental framework for evaluating LLMs' linguistic and cognitive abilities.

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [78] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: The paper focuses on identifying clickbait in Croatian news using a dataset ('CLIC') and finds fine-tuned models outperform large language models (LLMs) for this task.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of clickbait headlines that compromise information quality and trust in digital media, especially for less-resourced languages like Croatian.

Method: The authors compiled a 20-year dataset (CLIC) and used BERTić fine-tuned models, comparing their performance with LLM-based in-context learning methods using prompts in Croatian and English.

Result: Fine-tuned models demonstrated better accuracy in detecting clickbait compared to general-purpose LLMs. Linguistic features of clickbait headlines are also explored.

Conclusion: Fine-tuned models are more effective than LLMs for clickbait detection in Croatian, highlighting the importance of task-specific customization in low-resource language NLP tasks.

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [79] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: The study assesses the use of Large Language Models (LLMs) for personality inference via semi-structured interviews and finds limited validity despite high reliability.


<details>
  <summary>Details</summary>
Motivation: LLMs have potential for scalable personality assessment but lack psychometric validity when applied to real-world contexts.

Method: 555 semi-structured interviews with self-reported BFI-10 scores were used to test three state-of-the-art LLMs under different prompting setups.

Result: Models exhibited high test-retest reliability but weak correlations with ground-truth scores and low interrater agreement, with biases toward higher trait levels.

Conclusion: LLMs are currently limited in accurately inferring personality traits, requiring better methods for use in psychological applications.

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [80] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: This paper discusses building a chatbot at LinkedIn to help teams generate data insights using Text-to-SQL and a knowledge graph, with interactive capabilities and a focus on solving enterprise challenges.


<details>
  <summary>Details</summary>
Motivation: To address challenges in deploying Text-to-SQL models as functioning enterprise solutions and enable LinkedIn product teams to self-serve insights from large data lakes.

Method: The method involves three components: creating a knowledge graph for metadata and context, designing a Text-to-SQL agent with error correction, and implementing an interactive chatbot with rich UI for user interaction.

Result: The chatbot engages over 300 weekly users with 53% of its responses deemed correct or nearly correct based on internal benchmarking, with components analyzed through ablation studies.

Conclusion: Insights and practical guidance are provided for creating enterprise Text-to-SQL solutions, emphasizing the importance of knowledge graph components and interactivity in the process.

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [81] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: The paper proposes an error-aware teacher-student framework, leveraging GPT-4o and curriculum learning to enhance relation classification (RC) performance in biomedical texts.


<details>
  <summary>Details</summary>
Motivation: To improve relational classification in biomedical texts for accurate knowledge graph construction and applications like drug repurposing, overcoming existing model limitations and errors.

Method: An error-aware teacher-student framework that uses GPT-4o to analyze model errors, generate enhanced remediations, and employs curriculum learning with a biomedical knowledge graph for robust progressive learning.

Result: The approach achieves state-of-the-art performance on 4 out of 5 protein-protein interaction (PPI) datasets and the drug-drug interaction (DDI) dataset, while maintaining strong competitiveness on ChemProt.

Conclusion: The proposed framework effectively enhances relation classification in biomedical texts and sets new benchmarks, demonstrating the efficacy of targeted remediations and curriculum learning.

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [82] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0 is a specialized reasoning model for the semiconductor display industry that outperforms state-of-the-art models despite its smaller size.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the lack of domain-specific expertise in existing large language models for solving complex problems in the semiconductor display industry.

Method: The approach includes supervised fine-tuning, reinforcement learning, a domain-specific RAG mechanism, and an automated evaluation framework using a curated industry knowledge base.

Result: X-Intelligence 3.0 showed significant performance improvements over benchmark datasets and surpassed DeepSeek-R1-671B despite having only 32 billion parameters.

Conclusion: X-Intelligence 3.0 establishes itself as an efficient and expert-level reasoning model addressing challenges specific to the semiconductor display industry.

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [83] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: This paper introduces XL-DURel, a multilingual model for ordinal Word-in-Context tasks, showing improved performance with a ranking objective.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve Word-in-Context (WiC) classification tasks, focusing on both ordinal and binary settings, and seeks a unified approach for these tasks.

Method: They fine-tuned a multilingual Sentence Transformer, experimented with various loss functions, and optimized a ranking objective based on angular distance in complex space.

Result: XL-DURel outperformed prior models in ordinal and binary Word-in-Context classification tasks and demonstrated that binary WiC is a subcase of ordinal WiC.

Conclusion: A unified treatment of WiC modeling is feasible, and optimizing for ordinal tasks enhances performance on binary tasks.

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [84] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: The paper extends prior work on AudiBERT, a multimodal model integrating audio features, showing statistically significant improvements over BERT for sparse classes but not for affective classifications. Correlation analysis connects larger data and coder agreement to model performance, with suggestions for human-AI complementarity.


<details>
  <summary>Details</summary>
Motivation: Detecting meaningful collaborative problem-solving indicators using AI techniques, particularly enhancing the use of multimodal approaches like AudiBERT.

Method: The study uses AudiBERT, a multimodal BERT model that incorporates speech and acoustic-prosodic features, and compares its performance against the standard BERT model with correlation analyses.

Result: AudiBERT shows statistically significant improvements over BERT in social-cognitive classification but not in affective classification. Larger training datasets improve recall, and high coder agreement enhances precision in BERT.

Conclusion: AudiBERT provides distinct advantages in certain dimensions; however, achieving human-AI complementarity requires model explainability, particularly to promote human agency and engagement during coding.

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [85] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: The study uses SHapley Additive exPlanations (SHAP) to analyze how tokenized words contribute to BERT's classification of collaborative problem-solving (CPS).


<details>
  <summary>Details</summary>
Motivation: Limited attention has been given to understanding how individual tokenized words in datasets influence BERT's classification in CPS, which is important for building trust and adoption within educational practices.

Method: The study applied SHAP to analyze how tokenized words in transcription data impacted a BERT model's CPS classification. It examined model transparency and explainability.

Result: The analysis revealed that high-performing classifications didn't always align with reasonable explanations. Spurious, non-semantic words also influenced model decisions, questioning reliability.

Conclusion: While current transparency tools may not directly help end-users like teachers, they highlight the importance of not overrelying on AI diagnostics. Future work should explore ensemble models and human-AI collaboration in CPS diagnostics, given the need for human reasoning in detailed classifications.

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [86] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: The paper evaluates traditional data augmentation methods using ChatGPT, showing that paraphrasing and backtranslation can perform comparably or better than generative methods.


<details>
  <summary>Details</summary>
Motivation: Address challenges of data scarcity and class imbalance in NLP tasks through improved data augmentation methods.

Method: Conducted experiments comparing four approaches to data augmentation, including backtranslation, paraphrasing, and zero/few-shot generation, using ChatGPT.

Result: Backtranslation and paraphrasing yielded high-quality data and significantly impacted classification performance, surpassing generative methods in certain setups.

Conclusion: Traditional data augmentation methods, adapted with modern language models, remain effective and competitive against newer generative techniques.

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [87] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: This paper introduces a benchmark dataset and evaluation framework—focused on Kenyan outpatient care—to improve healthcare access using large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The paper aims to fill a gap in research exploring the effectiveness of LLMs in localized African primary healthcare scenarios, particularly in low-resource settings.

Method: The researchers digitized Kenya's national guidelines, used RAG techniques for semantic retrieval, and developed clinical QA datasets in English and Swahili co-created with Kenyan physicians. They leveraged tools like Gemini Flash 2.0 Lite for generating realistic clinical scenarios.

Result: The study led to the creation of the Alama Health QA dataset with thousands of regulator-aligned question-answer pairs reflecting outpatient conditions. Initial tests found a performance gap when LLMs handled localized African medical content compared to US benchmarks.

Conclusion: The paper offers a replicable model for benchmark development that aligns AI tools like LLMs with local healthcare guidelines, improving their safety and utility in African health systems.

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [88] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: This paper investigates the use of an affine approximation and linear transformations to model subject-object relationships in transformer-based language models, achieving high accuracy in specific linguistic tasks.


<details>
  <summary>Details</summary>
Motivation: To understand conceptual relationships, such as morphology, encoded in latent spaces of language models and interpret their representations with mathematical transformations.

Method: The study used a linear transformation Ws—derived from middle-layer subject token representations and model derivatives—to approximate final object states accurately.

Result: Results showed that the linear technique achieved 90% faithfulness on morphological relations and was observed across multiple languages and language models.

Conclusion: Certain linguistic relationships, like morphology, are sparsely encoded in latent spaces and can be accurately interpreted using cross-layer linear transformations.

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [89] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: The study introduces 'Cleanse,' a clustering-based uncertainty estimation method to evaluate and detect hallucinations in large language models (LLMs). It uses LLM hidden embeddings for semantic consistency and proves effective with four models and two QA benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the critical problem of hallucinations in LLMs, which poses challenges for building safe and reliable AI systems.

Method: The proposed method, Cleanse, employs clustering to measure semantic consistency in LLM hidden embeddings, quantifying uncertainty based on intra-cluster consistency.

Result: Cleanse effectively detects hallucination using four LLM models (LLaMA-7B, LLaMA-13B, LLaMA2-7B, and Mistral-7B) tested against two benchmarks (SQuAD and CoQA).

Conclusion: Cleanse demonstrates its suitability as a tool for uncertainty estimation in LLM hallucinations, enhancing the reliability of AI-generated responses.

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [90] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: The paper introduces Mangosteen, a high-quality 47 billion-token Thai corpus developed through an adapted Dolma pipeline, addressing script and cultural nuances for better language modeling. It surpasses prior benchmarks and offers full reproducibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of high-quality, reproducible Thai language corpora for language model training. Existing pipelines are often English-centric or lack cultural adaptation, leaving Thai-specific concerns like gambling content or nuanced filtering unaddressed.

Method: The authors developed Mangosteen using an adapted Dolma pipeline. This includes customized rule-based language identification, enhanced quality filters tailored for Thai, and curated non-web sources like Wikipedia and OCR-extracted books. They conducted systematic ablations with GPT-2 to refine the dataset.

Result: The pipeline reduced CommonCrawl documents from 202M to 25M and significantly improved SEA-HELM NLG performance (from 3 to 11). An 8B-parameter SEA-LION model pre-trained on Mangosteen outperformed previous versions and Llama-3.1 by about four points on Thai benchmarks.

Conclusion: Mangosteen provides a high-quality, fully reproducible corpus for Thai language modeling, surpassing existing benchmarks. By releasing all resources and code, it sets a transparent foundation for future Thai and regional NLP research.

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [91] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: The paper evaluates large language models (LLMs) for assigning medical ICPC-2 codes using clinical expressions, achieving high performance metrics with specific model optimizations.


<details>
  <summary>Details</summary>
Motivation: To explore the feasibility of automating medical coding through large language models, aiming to streamline healthcare data handling and enhance research and policy-making processes.

Method: A dataset of clinical expressions in Brazilian Portuguese was used, combined with semantic search engine outputs to retrieve coding candidates. Prompts were fed to 33 LLMs, and the performance of assigning ICPC-2 codes was assessed using F1-score and other metrics.

Result: Twenty-eight out of thirty-three models achieved an F1-score greater than 0.8, and ten models surpassed 0.85. Top-performing models included advanced options like gpt-4.5-preview. Formatting issues and code hallucinations were minimal in larger models, while smaller ones had challenges.

Conclusion: LLMs hold significant promise in automating medical coding tasks, showing high accuracy even without fine-tuning. However, the study recommends broader evaluations across diverse languages and clinical domains for better clinical adoption.

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [92] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: The paper introduces MiroMind-M1, an open-source reasoning language model series for mathematical reasoning tasks, aiming to improve transparency and reproducibility.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of openness and reproducibility in existing reasoning language models by creating open-source alternatives with extensive resources and transparent configurations.

Method: The authors train the models in two stages: supervised fine-tuning (SFT) on a curated dataset of 719K math problems with chain-of-thought (CoT) trajectories, followed by reinforcement learning with verification and repetition control (RLVR) on 62K challenging problems. They also introduce Context-Aware Multi-Stage Policy Optimization.

Result: The MiroMind-M1 models achieve state-of-the-art or competitive performance and superior token efficiency on the AIME24, AIME25, and MATH benchmarks compared to other open-source Qwen-2.5-based models.

Conclusion: The complete suite of models, datasets, and training configurations is made publicly available to promote transparency, reproducibility, and community engagement in reasoning language model development.

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [93] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: This paper reviews publicly available Arabic post-training datasets for Large Language Models (LLMs), identifying key gaps like limited task diversity and poor documentation.


<details>
  <summary>Details</summary>
Motivation: Post-training is essential for improving the alignment of Large Language Models (LLMs) with human instructions, especially for developing Arabic LLMs.

Method: The paper evaluates Arabic post-training datasets along dimensions such as capabilities, steerability, alignment, and robustness using criteria like popularity, adoption, and documentation quality.

Result: The review shows a lack of quality and diversity in Arabic post-training datasets, along with inconsistent documentation and limited community adoption.

Conclusion: The paper underscores critical gaps in the availability and quality of Arabic post-training datasets and offers recommendations to address these for better development of Arabic LLMs.

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [94] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: This paper addresses the challenges of limited language coverage and unreliable annotations in suicidal ideation detection by constructing a Turkish social media dataset, assessing label reliability, and evaluating AI models.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the scarcity of high-quality suicidal ideation datasets, particularly in non-English languages, and challenges in annotation reliability for real-time suicide prevention via AI.

Method: The study constructs a Turkish suicidal ideation dataset using social media posts, employs a novel annotation framework involving three human annotators and two LLMs, and evaluates label reliability and model consistency using transfer learning with eight pre-trained sentiment and emotion classifiers.

Result: The research finds inconsistencies in annotation and model reliability across datasets and highlights the limitations of popular models, especially with zero-shot transfer learning.

Conclusion: The authors call for rigorous, language-inclusive annotation practices, improved methods for evaluating mental health NLP datasets, and greater transparency in model training and dataset construction.

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [95] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: The study investigates linguistic biases in peer review using 80,000 reviews, revealing disparities linked to author demographics.


<details>
  <summary>Details</summary>
Motivation: Concerns about biases in peer review and the subtle role of language in reinforcing disparities.

Method: Natural language processing and large-scale statistical modeling analyzing tone, sentiment, and supportive language across demographics.

Result: Hidden biases in peer feedback and insight into the effects of reviewer anonymity on fairness were uncovered.

Conclusion: Revealing linguistic bias in peer review highlights critical questions about fairness, career impact, and reform in academic publishing.

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [96] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: Researchers investigated the adaptability of multimodal neural networks in learning word-referent mappings using automated transcription-based datasets derived from three children's developmental experiences.


<details>
  <summary>Details</summary>
Motivation: To address the mismatch between children's ability to acquire language with limited input and the large dataset requirements of neural networks, and to explore if neural networks show robust learning across diverse developmental datasets.

Method: Automated speech transcription was applied to the SAYCam dataset (500+ hours of video data from three children) to create multimodal training and evaluation datasets. Different neural network configurations were tested to study simulated word learning.

Result: Multimodal networks trained on automatically transcribed individual datasets could successfully acquire and generalize word-referent mappings across various architectural approaches.

Conclusion: Multimodal neural networks demonstrate robust word learning capabilities, validating their applicability to grounded language acquisition while also highlighting individual differences based on unique developmental contexts.

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [97] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: The paper introduces GRACE, a generative framework for multi-behavior recommendation that enhances token reasoning, reduces computational complexity, and utilizes multi-scale user history modeling.


<details>
  <summary>Details</summary>
Motivation: Improve the adoption of generative models in recommendation systems by addressing issues like lack of explicit token reasoning, inefficiency in attention computation, and limited modeling of user behavior.

Method: GRACE employs a hybrid Chain-of-Thought tokenization to explicitly encode user interactions using product attributes and a Journey-Aware Sparse Attention mechanism for efficient sequence analysis.

Result: Experiments on real-world datasets reveal significant improvements in recommendation metrics like HR@10 and NDCG@10, alongside a reduction in computational effort for long sequences.

Conclusion: GRACE demonstrates the ability to provide interpretable, behavior-aligned recommendations while being computationally efficient and outperforming existing state-of-the-art methods.

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [98] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: The paper introduces FastLongSpeech, a framework to improve the ability of Large Speech-Language Models (LSLMs) to process long-speech efficiently without requiring additional long-speech training data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of efficiently processing long-form speech using LSLMs, which is underexplored due to limited datasets and high computational demands.

Method: FastLongSpeech employs an iterative fusion strategy that compresses long-speech into shorter sequences. It also uses a dynamic compression training approach to adapt LSLMs for long-speech inputs by varying compression ratios during training.

Result: Experiments demonstrate that FastLongSpeech is effective in both short and long-speech tasks, significantly improving inference efficiency.

Conclusion: The framework advances LSLMs by enabling efficient long-speech processing without the need for dedicated long-speech training data, as validated by strong experimental results on the newly designed LongSpeech-Eval benchmark.

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [99] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: This paper introduces and formulates the novel task of intent-based chart generation from documents, presenting a two-stage unsupervised framework that extracts relevant information and selects appropriate chart types.


<details>
  <summary>Details</summary>
Motivation: Existing methods for generating visualizations from descriptions or tables are not easily adaptable for cases involving long documents, where users specify intents instead of manually selecting contents.

Method: The authors propose a two-stage approach: (1) Extract relevant data from documents using LLMs through intent decomposition and iterative refinement; (2) Use a heuristic-guided module to choose chart types and generate chart code. Additionally, they propose an attribution-based metric for evaluating chart data accuracy.

Result: The method outperforms baselines by up to 9 points in chart data accuracy and 17 points in chart type selection across a curated dataset of 1,242 tuples from finance and scientific domains.

Conclusion: The framework effectively addresses the challenge of intent-based chart generation from documents, improving accuracy and applicability over standard approaches.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [100] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: This paper explores how reasoning distillation affects long-context comprehension and retrieval in small language models, showing it improves multi-document Q&A task performance.


<details>
  <summary>Details</summary>
Motivation: To understand the influence of reasoning distillation on in-context retrieval and reasoning, especially in Retrieval-Augmented Generation (RAG) systems.

Method: The study used open-source models distilled from Deepseek-R1 and conducted multi-document question-answering tasks to assess their ability to extract and integrate information from extended contexts.

Result: Distilled reasoning patterns significantly improve long-context understanding, enhancing detailed reasoning and addressing the 'lost in the middle' issue.

Conclusion: Reasoning distillation enhances long-context comprehension by fostering explicit and detailed reasoning processes, making it valuable for tasks requiring extended contextual analysis.

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [101] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: This paper investigates Tiny Language Models (TLMs) and demonstrates that pre-training significantly improves their performance on classification tasks, suggesting they share key features with large language models, but are more computationally accessible.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inaccessibility of pre-training large language models (LLMs) by exploring the potential and effectiveness of smaller, more computationally efficient alternatives like Tiny Language Models (TLMs).

Method: The study pre-trains TLMs (BERT-6 and BERT-1 variants) on subsets of the Wikipedia dataset and evaluates their performance on classification tasks such as FewRel, AGNews, and DBPedia. It also investigates the effects of dataset size, token overlap, and leverages a soft committee of independently pre-trained shallow architectures to replicate deep TLM performance.

Result: The study finds a clear performance gap between pre-trained and non-pre-trained TLMs, with pre-training proving effective even at a tiny scale. The performance improves with larger pre-training datasets and higher token overlap. Additionally, a soft committee of shallow TLMs can match the performance of a deeper model while maintaining low latency.

Conclusion: TLMs not only exhibit qualitative features of LLMs but also present a viable, computationally efficient alternative for natural language processing tasks. This accessibility could democratize NLP research and illuminate biologically inspired mechanisms underlying language acquisition.

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [102] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: This study proposes the MEKiT method to improve LLMs' performance on the ECPE task by injecting heterogeneous internal emotional and external causal knowledge, achieving superior results compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Large language models struggle with the ECPE task due to a lack of auxiliary knowledge, which hinders their reasoning and emotion perception capabilities.

Method: The MEKiT method is developed, utilizing instruction templates and mixed data for instruction-tuning, to inject internal emotional and external causal knowledge into LLMs.

Result: MEKiT significantly enhances the ECPE task performance of LLMs and outperforms alternative baseline methods in the experiments conducted.

Conclusion: MEKiT is an effective and adaptable approach for boosting LLMs' emotion and cause reasoning, demonstrating its value in addressing the limitations of LLMs on ECPE tasks.

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [103] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: The paper addresses unexpected code-switching in LLMs, presenting a novel technique (SASFT) that reduces language mixing by over 50% without performance loss.


<details>
  <summary>Details</summary>
Motivation: LLMs often switch to unintended languages in responses, leading to usability issues, and existing solutions lack thorough analysis and effective mitigation.

Method: The authors use sparse autoencoders to analyze activation values and propose SASFT, a supervised fine-tuning approach guided by autoencoder findings to stabilize multilingual model behavior.

Result: SASFT reduced unexpected code-switching by over 50% in five models and eliminated it in four cases, while maintaining or improving performance across six benchmarks.

Conclusion: SASFT is a robust solution for reducing code-switching in multilingual LLMs, achieving both improved language control and preservation of overall capabilities.

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [104] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

TL;DR: This paper introduces NeuronXA, a method to evaluate cross-lingual alignment in multilingual language models based on neuron activity. The method is tested on multiple benchmarks and models, showing high correlation with task performance and transferability.


<details>
  <summary>Details</summary>
Motivation: Existing methods evaluate cross-lingual alignment using sentence embeddings but struggle with low-resource languages due to non-smooth representation spaces. To address this limitation, the study explores a novel, more semantically grounded approach inspired by neuroscientific findings.

Method: The paper proposes NeuronXA, a neuron state-based framework for cross-lingual alignment evaluation. It uses neuron activation patterns to assess semantic alignment and is tested on various multilingual LLMs across transfer tasks and benchmarks.

Result: With only 100 parallel sentence pairs, NeuronXA achieves a high Pearson correlation of 0.9556 with downstream tasks and 0.8514 with transferability. This underscores its effectiveness even in limited data scenarios.

Conclusion: NeuronXA provides a novel, efficient, and semantically robust method for evaluating cross-lingual alignment and transferability in multilingual language models. It has the potential to further research and improve multilingual LLM understanding.

Abstract: Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [105] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: LLM evaluations with single prompts are unreliable; PromptSuite automates robust multi-prompt evaluations via modular and extensible designs.


<details>
  <summary>Details</summary>
Motivation: Single-prompt evaluations are inconsistent, and creating prompt variations for reliable evaluations is difficult, limiting robust assessment adoption.

Method: The paper introduces PromptSuite, an automated framework for generating diverse prompt variations, featuring modular design, controlled perturbations, and extensibility.

Result: PromptSuite successfully provides meaningful variations for reliable LLM evaluations, as demonstrated in various case studies.

Conclusion: PromptSuite enables robust and efficient multi-prompt evaluations for LLMs through automatic prompt generation, enhancing assessment practices.

Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


### [106] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
*Vahid Rahimzadeh,Erfan Moosavi Monazzah,Mohammad Taher Pilehvar,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: This paper introduces SYNTHIA, a dataset combining authentic social media activity with synthetic generation to improve narrative consistency and enable novel research in computational social science.


<details>
  <summary>Details</summary>
Motivation: Existing methods for persona-driven LLMs either rely on expensive human-curated data or produce unrealistic synthetic personas, creating a need for an approach that balances realism and scalability.

Method: The researchers created SYNTHIA by deriving 30,000 backstories from 10,000 real social media users' activities across three time windows, grounded in real-life interactions from the BlueSky platform.

Result: SYNTHIA outperformed state-of-the-art methods in narrative consistency while also achieving competitive results in demographic diversity and alignment with social surveys.

Conclusion: SYNTHIA serves as a robust dataset for computational social science research and persona-driven language modeling, incorporating temporal aspects and rich social metadata for enhanced applications.

Abstract: Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

</details>


### [107] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
*Hang Yan,Fangzhi Xu,Rongman Xu,Yifei Li,Jian Zhang,Haoran Luo,Xiaobao Wu,Luu Anh Tuan,Haiteng Zhao,Qika Lin,Jun Liu*

Main category: cs.CL

TL;DR: The paper proposes a novel reasoning strategy called Momentum Uncertainty-guided Reasoning (MUR) to improve the efficiency and accuracy of Large Language Models (LLMs) during test-time scaling, achieving better results with reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Large Language Models excel at reasoning-intensive tasks but face challenges in optimizing efficiency during test-time reasoning, often leading to redundant computations and wasted tokens.

Method: The authors introduce Momentum Uncertainty-guided Reasoning (MUR), which allocates reasoning budgets dynamically based on stepwise uncertainty tracking, alongside a gamma-control mechanism for flexible tuning using a single hyperparameter.

Result: Experiments across four benchmarks and three model sizes show that MUR cuts computation by over 50% on average while boosting accuracy by 0.62-3.37%.

Conclusion: MUR offers an effective and efficient reasoning approach for LLMs, enhancing their accuracy and reducing computational expense without requiring additional training.

Abstract: Large Language Models (LLMs) have achieved impressive performance on
reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an
open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it
often leads to overthinking, wasting tokens on redundant computations. This
work investigates how to efficiently and adaptively guide LLM test-time scaling
without additional training. Inspired by the concept of momentum in physics, we
propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically
allocates thinking budgets to critical reasoning steps by tracking and
aggregating stepwise uncertainty over time. To support flexible inference-time
control, we introduce gamma-control, a simple mechanism that tunes the
reasoning budget via a single hyperparameter. We provide in-depth theoretical
proof to support the superiority of MUR in terms of stability and biases. MUR
is comprehensively evaluated against various TTS methods across four
challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using
different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate
that MUR reduces computation by over 50% on average while improving accuracy by
0.62-3.37%.

</details>


### [108] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun,Junyang Lin*

Main category: cs.CL

TL;DR: This paper introduces RefCritic, a reinforcement learning-based critic module designed to provide precise and actionable feedback for LLMs, outperforming traditional supervised fine-tuning methods in multiple evaluation benchmarks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of current supervised fine-tuning techniques in developing critic modules for LLMs, which often produce critiques lacking depth, reflection, and validation.

Method: The proposed RefCritic uses reinforcement learning with dual rule-based rewards focusing on solution correctness and refinement accuracy. It generates critiques that promote actionable feedback for improving model performance.

Result: RefCritic was evaluated on two models across five benchmarks, showing consistent improvements in critique quality and refinement capabilities, including 6.8% and 7.2% gains on specific benchmarks.

Conclusion: RefCritic demonstrates superior critique and refinement abilities, providing a promising alternative to traditional supervised fine-tuning for developing critic modules in LLMs.

Abstract: With the rapid advancement of Large Language Models (LLMs), developing
effective critic modules for precise guidance has become crucial yet
challenging. In this paper, we initially demonstrate that supervised
fine-tuning for building critic modules (which is widely adopted in current
solutions) fails to genuinely enhance models' critique abilities, producing
superficial critiques with insufficient reflections and verifications. To
unlock the unprecedented critique capabilities, we propose RefCritic, a
long-chain-of-thought critic module based on reinforcement learning with dual
rule-based rewards: (1) instance-level correctness of solution judgments and
(2) refinement accuracies of the policy model based on critiques, aiming to
generate high-quality evaluations with actionable feedback that effectively
guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and
DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement
settings, RefCritic demonstrates consistent advantages across all benchmarks,
e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably,
under majority voting, policy models filtered by RefCritic show superior
scaling with increased voting numbers. Moreover, despite training on
solution-level supervision, RefCritic outperforms step-level supervised
approaches on ProcessBench, a benchmark to identify erroneous steps in
mathematical reasoning.

</details>


### [109] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: This paper introduces WebShaper, a formalization-driven framework for creating information-seeking datasets, enabling state-of-the-art performance in open-sourced IS agents.


<details>
  <summary>Details</summary>
Motivation: The development of IS agents is hindered by the lack of high-quality, structured training datasets. Current methods often create inconsistencies between retrieved information and reasoning processes.

Method: WebShaper uses set theory formalization and Knowledge Projections (KP) to systematically synthesize IS tasks, employing a multi-step process with an agentic Expander to enhance reasoning complexity and validate data.

Result: Models trained on WebShaper-generated datasets outperform baselines in benchmarks such as GAIA and WebWalkerQA.

Conclusion: The WebShaper framework facilitates the creation of high-quality IS datasets, enabling improved reasoning and retrieval for large language model-powered agents.

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [110] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: This study evaluates tokenization and positional encoding methods for DNA sequences using Transformer models, comparing k-mer segmentation, BPE tokenization, and three positional encoding strategies on the GUE benchmark dataset.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the effectiveness of different tokenization methods (k-mer vs. BPE) and positional encodings in modeling DNA sequences with Transformers.

Method: The paper compares k-mer segmentation with various k values and BPE tokenization, along with sinusoidal, AliBi, and RoPE positional encodings, using Transformer encoders of varying depths (3, 6, 12, and 24 layers) and evaluates them on the GUE benchmark dataset.

Result: BPE tokenization demonstrates higher and more stable performance by compressing frequent motifs and improving generalization, while RoPE performs best for periodic motifs and AliBi is effective for tasks involving local dependencies. Deeper models up to 12 layers show significant gains, with marginal benefits beyond that depth.

Conclusion: BPE tokenization is superior for DNA sequence modeling, and the choice of positional encoding depends on the task. Transformers deeper than 12 layers show diminishing returns.

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


### [111] [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)
*Vijeta Deshpande,Ishita Dasgupta,Uttaran Bhattacharya,Somdeb Sarkhel,Saayan Mitra,Anna Rumshisky*

Main category: cs.CL

TL;DR: The paper introduces Penalty-Adjusted Type-Token Ratio (PATTR), a new diversity metric designed to account for variations in text length when analyzing synthetic data from Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of ensuring lexical diversity in synthetic text generated by LLMs while countering biases introduced by text length variations.

Method: The paper proposes the PATTR metric, evaluates it using synthetic data generated by seven LLMs in a creative writing task, and compares its performance with existing diversity measures like MATTR and Compression Ratio.

Result: PATTR shows superiority by consistently mitigating length biases and producing better diversity results in filtering tasks, compared to MATTR and CR, while adhering to task-specific target response lengths.

Conclusion: PATTR is a robust metric for measuring lexical diversity in synthetic texts, capable of outperforming existing metrics, especially in tasks requiring adherence to predefined text lengths.

Abstract: Synthetic text generated by Large Language Models (LLMs) is increasingly used
for further training and improvement of LLMs. Diversity is crucial for the
effectiveness of synthetic data, and researchers rely on prompt engineering to
improve diversity. However, the impact of prompt variations on response text
length, and, more importantly, the consequential effect on lexical diversity
measurements, remain underexplored. In this work, we propose Penalty-Adjusted
Type-Token Ratio (PATTR), a diversity metric robust to length variations. We
generate a large synthetic corpus of over 20M words using seven models from the
LLaMA, OLMo, and Phi families, focusing on a creative writing task of video
script generation, where diversity is crucial. We evaluate per-response lexical
diversity using PATTR and compare it against existing metrics of Moving-Average
TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length
variations introduce biases favoring shorter responses. Unlike existing
metrics, PATTR explicitly considers the task-specific target response length
($L_T$) to effectively mitigate length biases. We further demonstrate the
utility of PATTR in filtering the top-10/100/1,000 most lexically diverse
responses, showing that it consistently outperforms MATTR and CR by yielding on
par or better diversity with high adherence to $L_T$.

</details>


### [112] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
*Chathuri Jayaweera,Brianna Yanqui,Bonnie Dorr*

Main category: cs.CL

TL;DR: The paper investigates using Large Language Models (LLMs) to generate commonsense knowledge for Natural Language Inference (NLI) tasks, aiming to assess their reliability and impact.


<details>
  <summary>Details</summary>
Motivation: Existing commonsense resources for NLI lack sufficient coverage for diverse premise-hypothesis pairs, necessitating exploration of alternative methods such as LLMs.

Method: The researchers adapted and modified metrics to evaluate the factuality and consistency of commonsense knowledge generated by LLMs, then analyzed its influence on NLI prediction outcomes.

Result: Although explicitly incorporating commonsense knowledge did not consistently improve overall results, it effectively distinguished entailing instances and moderately enhanced identifying contradictory and neutral inferences.

Conclusion: LLMs show potential for commonsense knowledge generation in NLI, particularly in differentiating certain inference categories, but their overall impact on performance varies.

Abstract: Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

</details>


### [113] [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
*Chathuri Jayaweera,Bonnie Dorr*

Main category: cs.CL

TL;DR: The paper highlights that annotation disagreements in NLI often result from meaningful ambiguity rather than random noise, advocating for models and resources that account for this variability.


<details>
  <summary>Details</summary>
Motivation: Annotation disagreements in NLI datasets are often dismissed as noise, but the authors argue these disagreements can represent different valid human interpretations due to ambiguity, which current systems fail to capture.

Method: The authors propose an integrated framework to systematically identify ambiguous pairs and categorize ambiguity types based on content-driven ambiguity. They also suggest developing datasets annotated for ambiguity and unsupervised methods for ambiguity detection.

Result: The paper unifies existing taxonomies for classifying ambiguity, provides concrete examples of ambiguity influencing NLI annotations, and emphasizes the current lack of datasets targeting ambiguity and its subtypes.

Conclusion: Ambiguity-aware NLI systems are essential for capturing human interpretive variation, requiring new resources and methods to robustly align model capabilities with human reasoning.

Abstract: This position paper argues that annotation disagreement in Natural Language
Inference (NLI) is not mere noise but often reflects meaningful interpretive
variation, especially when triggered by ambiguity in the premise or hypothesis.
While underspecified guidelines and annotator behavior can contribute to
variation, content-based ambiguity offers a process-independent signal of
divergent human perspectives. We call for a shift toward ambiguity-aware NLI by
systematically identifying ambiguous input pairs and classifying ambiguity
types. To support this, we present a unified framework that integrates existing
taxonomies and illustrate key ambiguity subtypes through concrete examples.
These examples reveal how ambiguity shapes annotator decisions and motivate the
need for targeted detection methods that better align models with human
interpretation. A key limitation is the lack of datasets annotated for
ambiguity and subtypes. We propose addressing this gap through new annotated
resources and unsupervised approaches to ambiguity detection -- paving the way
for more robust, explainable, and human-aligned NLI systems.

</details>


### [114] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
*Hellina Hailu Nigatu,Atnafu Lambebo Tonja,Henok Biadglign Ademtew,Hizkel Mitiku Alemayehu,Negasi Haile Abadi,Tadesse Destaw Belay,Seid Muhie Yimam*

Main category: cs.CL

TL;DR: The paper examines the effects of homophone normalization in Amharic NLP and proposes a post-inference normalization method to improve performance without altering training data.


<details>
  <summary>Details</summary>
Motivation: To investigate the limitations of applying homophone normalization during training for Ge'ez-script languages and explore alternative methods while maintaining language-specific features.

Method: The authors conduct monolingual training and cross-lingual transfer experiments and propose post-inference normalization, which applies normalization only to model predictions instead of training data.

Result: The proposed post-inference normalization method results in up to a 1.03 BLEU score improvement while retaining linguistic diversity in training.

Conclusion: Post-inference normalization is an effective alternative to traditional homophone normalization applied during training, enhancing model performance while preserving language features.

Abstract: Homophone normalization, where characters that have the same sound in a
writing script are mapped to one character, is a pre-processing step applied in
Amharic Natural Language Processing (NLP) literature. While this may improve
performance reported by automatic metrics, it also results in models that are
not able to understand different forms of writing in a single language.
Further, there might be impacts in transfer learning, where models trained on
normalized data do not generalize well to other languages. In this paper, we
experiment with monolingual training and cross-lingual transfer to understand
the impacts of normalization on languages that use the Ge'ez script. We then
propose a post-inference intervention in which normalization is applied to
model predictions instead of training data. With our simple scheme of
post-inference normalization, we show that we can achieve an increase in BLEU
score of up to 1.03 while preserving language features in training. Our work
contributes to the broader discussion on technology-facilitated language change
and calls for more language-aware interventions.

</details>


### [115] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: The study evaluates the performance of three LLMs for automating RCT data extraction, finding that customised prompts improve recall significantly.


<details>
  <summary>Details</summary>
Motivation: Data extraction from full-text RCTs remains challenging, requiring improved automation methods for meta-analysis.

Method: The paper assesses three LLMs using tasks in three medical domains and tests four prompting strategies to improve extraction quality.

Result: Customised prompts increased recall by up to 15%, while models displayed high precision but poor recall.

Conclusion: The study proposes guidelines for using LLMs in data extraction, balancing efficiency, task complexity, and expert oversight.

Abstract: Automating data extraction from full-text randomised controlled trials (RCTs)
for meta-analysis remains a significant challenge. This study evaluates the
practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)
across tasks involving statistical results, risk-of-bias assessments, and
study-level characteristics in three medical domains: hypertension, diabetes,
and orthopaedics. We tested four distinct prompting strategies (basic
prompting, self-reflective prompting, model ensemble, and customised prompts)
to determine how to improve extraction quality. All models demonstrate high
precision but consistently suffer from poor recall by omitting key information.
We found that customised prompts were the most effective, boosting recall by up
to 15\%. Based on this analysis, we propose a three-tiered set of guidelines
for using LLMs in data extraction, matching data types to appropriate levels of
automation based on task complexity and risk. Our study offers practical advice
for automating data extraction in real-world meta-analyses, balancing LLM
efficiency with expert oversight through targeted, task-specific automation.

</details>


### [116] [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)
*Xiandong Meng,Yan Wu,Yexin Tian,Xin Hu,Tianze Kang,Junliang Du*

Main category: cs.CL

TL;DR: This paper proposes a multi-teacher distillation strategy to efficiently compress large language models, enhancing student model performance while reducing computational demands.


<details>
  <summary>Details</summary>
Motivation: To address the high computational cost and slow inference issues associated with deploying large language models.

Method: The paper introduces a distillation strategy using multiple teacher models, integrating techniques like weighted output fusion, feature alignment loss, and dynamic teacher weighting.

Result: Experimental results show improved perplexity, distillation loss, and generation quality in the student model across various tasks like language modeling and text generation.

Conclusion: The multi-teacher collaborative approach proves effective in compressing large-scale language models while maintaining strong language capabilities and task adaptability.

Abstract: This paper addresses the challenges of high computational cost and slow
inference in deploying large language models. It proposes a distillation
strategy guided by multiple teacher models. The method constructs several
teacher models and integrates their output probability distributions and
intermediate semantic features. This guides the student model to learn from
multiple sources of knowledge. As a result, the student model gains stronger
language understanding and generation ability while maintaining a small
parameter size. To achieve this, the paper introduces a weighted output fusion
mechanism, a feature alignment loss function, and an entropy-driven dynamic
teacher weighting strategy. These components improve the quality and stability
of knowledge transfer during distillation. Under multi-teacher guidance, the
student model captures semantic information more effectively and demonstrates
strong performance across multiple evaluation metrics. In particular, the
method shows high consistency in expression, generalization ability, and task
adaptability in tasks such as language modeling, text generation, and
multi-task learning. The experiments compare the proposed method with several
widely adopted distillation approaches. The results further confirm its overall
advantages in perplexity, distillation loss, and generation quality. This study
provides a feasible technical path for the efficient compression of large-scale
language models. It also demonstrates the effectiveness of multi-teacher
collaborative mechanisms in complex language modeling tasks.

</details>


### [117] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
*Shayan Vassef,Amirhossein Dabiriaghdam,Mohammadreza Bakhtiari,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: The paper explores how multi-task, multi-lingual, and multi-source learning influence pretrained language model performance and introduces a novel framework called Subsets of Interest (SOI) to analyze learning behavior.


<details>
  <summary>Details</summary>
Motivation: To better understand the training dynamics of language models under multi-setting configurations and improve their robustness and performance.

Method: The authors use the Subsets of Interest (SOI) framework and perform experimental comparisons for multi-task, multi-source, and multi-lingual learning, while incorporating a two-stage fine-tuning process to optimize model performance.

Result: Multi-source learning enhances out-of-distribution performance by up to 7%, and multi-task learning yields mixed results, particularly beneficial for related tasks. The two-stage fine-tuning approach improves performance further.

Conclusion: The framework and techniques introduced offer valuable insights into training dynamics and enable the further optimization of pretrained language models under various multi-setting conditions.

Abstract: This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

</details>


### [118] [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)
*Yuanhe Tian,Junjie Liu,Zhizhou Kou,Yuxiang Li,Yan Song*

Main category: cs.CL

TL;DR: The paper introduces ChiMed 2.0, a Chinese medical dataset aimed at enhancing pre-training, fine-tuning, and reinforcement learning from human feedback for large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Current Chinese medical datasets are limited in size, scope, and functionality, lacking diversity and the ability to support pre-training and reinforcement learning from human feedback.

Method: ChiMed 2.0 is built using data collected from Chinese medical online platforms and LLM-generated content. It includes 204.4M Chinese characters divided into pre-training, supervised fine-tuning (SFT), and RLHF data.

Result: ChiMed 2.0 improves the performance of LLMs across multiple scales when evaluated on medical benchmark datasets.

Conclusion: ChiMed 2.0 is effective in supporting the development of advanced Chinese medical LLMs, providing a robust and diverse dataset for various training stages.

Abstract: Building high-quality data resources is crucial for advancing artificial
intelligence research and applications in specific domains, particularly in the
Chinese medical domain. Existing Chinese medical datasets are limited in size
and narrow in domain coverage, falling short of the diverse corpora required
for effective pre-training. Moreover, most datasets are designed solely for LLM
fine-tuning and do not support pre-training and reinforcement learning from
human feedback (RLHF). In this paper, we propose a Chinese medical dataset
named ChiMed 2.0, which extends our previous work ChiMed, and covers data
collected from Chinese medical online platforms and generated by LLMs. ChiMed
2.0 contains 204.4M Chinese characters covering both traditional Chinese
medicine classics and modern general medical data, where there are 164.8K
documents for pre-training, 351.6K question-answering pairs for supervised
fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the
effectiveness of our approach for training a Chinese medical LLM, we conduct
further pre-training, SFT, and RLHF experiments on representative general
domain LLMs and evaluate their performance on medical benchmark datasets. The
results show performance gains across different model scales, validating the
dataset's effectiveness and applicability.

</details>


### [119] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: The paper introduces the DPSE framework to improve Large Language Models (LLMs) in user preference alignment and domain-specific competence using structured data expansion and dual-phase fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Limitations in LLMs due to pre-training call for approaches that enhance both user alignment and domain cognition, which existing methods fail to address adequately.

Method: The proposed DPSE framework employs a Censor module for satisfaction scoring, structured data expansion, and a two-stage fine-tuning process: supervised domain grounding followed by frequency-aware preference optimization.

Result: Experimental evaluation shows DPSE outperforms traditional fine-tuning, preference optimization, and memory-augmented methods across various NLP tasks, with ablation studies confirming the effectiveness of each module.

Conclusion: The DPSE framework successfully advances the autonomous and continual self-evolution of LLMs by synergistically enhancing user preference adaptation and domain-specific skills.

Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

</details>


### [120] [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)
*Navid Ayoobi,Sadat Shahriar,Arjun Mukherjee*

Main category: cs.CL

TL;DR: The paper introduces SHIELD, a benchmark aimed at evaluating AI text detectors based on real-world viability and stability rather than conventional metrics.


<details>
  <summary>Details</summary>
Motivation: Previous benchmarks overlooked critical real-world metrics like threshold configuration and detector stability across diverse domains.

Method: SHIELD integrates reliability and stability factors into a unified evaluation metric and adds a humanification framework to make AI text resemble human authorship.

Result: The proposed framework challenges current state-of-the-art detection systems, improving reliability and stability under zero-shot scenarios.

Conclusion: This work highlights the importance of practical metrics for evaluating AI text detectors and proposes SHIELD as a more realistic and equitable benchmark for deployment.

Abstract: We present a novel evaluation paradigm for AI text detectors that prioritizes
real-world and equitable assessment. Current approaches predominantly report
conventional metrics like AUROC, overlooking that even modest false positive
rates constitute a critical impediment to practical deployment of detection
systems. Furthermore, real-world deployment necessitates predetermined
threshold configuration, making detector stability (i.e. the maintenance of
consistent performance across diverse domains and adversarial scenarios), a
critical factor. These aspects have been largely ignored in previous research
and benchmarks. Our benchmark, SHIELD, addresses these limitations by
integrating both reliability and stability factors into a unified evaluation
metric designed for practical assessment. Furthermore, we develop a post-hoc,
model-agnostic humanification framework that modifies AI text to more closely
resemble human authorship, incorporating a controllable hardness parameter.
This hardness-aware approach effectively challenges current SOTA zero-shot
detection methods in maintaining both reliability and stability. (Data and
code: https://github.com/navid-aub/SHIELD-Benchmark)

</details>


### [121] [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)
*Thilo Hagendorff*

Main category: cs.CL

TL;DR: The paper argues that training AI language models to be harmless, helpful, and honest (HHH) inherently aligns them with left-wing political ideologies due to their focus on harm avoidance and fairness, challenging critiques of political bias in AI.


<details>
  <summary>Details</summary>
Motivation: To address whether the normative principles guiding AI alignment objectives—harmlessness, honesty, and helpfulness—inevitably result in a left-wing political bias and confront the contradictory critiques of political bias in AI research.

Method: The paper uses philosophical and normative analysis to connect alignment objectives (HHH) with progressive moral frameworks and critiques research that frames left-wing bias as a risk in AI.

Result: It concludes that left-wing principles align more naturally with AI alignment norms like harm avoidance, inclusivity, and fairness, highlighting contradictions in AI bias research that treats left-leaning tendencies negatively.

Conclusion: Efforts to align AI models according to HHH principles are inherently linked to left-leaning ethical frameworks, and framing such tendencies as a risk undermines the commitment to alignment norms.

Abstract: The guiding principle of AI alignment is to train large language models
(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are
mounting concerns that LLMs exhibit a left-wing political bias. Yet, the
commitment to AI alignment cannot be harmonized with the latter critique. In
this article, I argue that intelligent systems that are trained to be harmless
and honest must necessarily exhibit left-wing political bias. Normative
assumptions underlying alignment objectives inherently concur with progressive
moral frameworks and left-wing principles, emphasizing harm avoidance,
inclusivity, fairness, and empirical truthfulness. Conversely, right-wing
ideologies often conflict with alignment guidelines. Yet, research on political
bias in LLMs is consistently framing its insights about left-leaning tendencies
as a risk, as problematic, or concerning. This way, researchers are actively
arguing against AI alignment, tacitly fostering the violation of HHH
principles.

</details>


### [122] [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
*Narun Raman,Taylor Lundy,Kevin Leyton-Brown*

Main category: cs.CL

TL;DR: The paper evaluates 25 LLMs against 15 benchmarks, questioning the validity of multiple-choice question-answering (MCQA) as a proxy for downstream performance. MCQA works when models perform reasoning before seeing options but skews results otherwise.


<details>
  <summary>Details</summary>
Motivation: To assess whether MCQA, a common evaluation method in LLMs, accurately reflects downstream performance as models advance in reasoning capabilities.

Method: The authors systematically evaluated 25 LLMs of varying sizes on 15 benchmarks using five presentation formats, manipulating whether reasoning occurred before or after seeing multiple-choice options.

Result: MCQA is effective when reasoning is restricted to pre-choice phases, but advanced models leveraging post-choice reasoning exploit biases, leading to inflated performance.

Conclusion: MCQA is no longer a reliable proxy for LLM downstream performance. The paper recommends improved benchmarks that better measure genuine reasoning abilities and resist bias.

Abstract: When evaluating Large Language Models (LLMs) in question-answering domains,
it is common to ask the model to choose among a fixed set of choices (so-called
multiple-choice question-answering, or MCQA). Although downstream tasks of
interest typically do not provide systems with explicit options among which to
choose, this approach is nevertheless widely used because it makes it makes
automatic grading straightforward and has tended to produce challenging
benchmarks that correlate sufficiently well with downstream performance. This
paper investigates the extent to which this trend continues to hold for
state-of-the-art reasoning models, describing a systematic evaluation of $15$
different question-answering benchmarks (e.g., MMLU, HLE) and $25$ different
LLMs (including small models such as Qwen 7B and relatively large models such
as Llama 70B). For each model-benchmark pair, we considered $5$ ways of
presenting the model with questions, including variations on whether multiple
choices were offered to the model at all; whether "none of the above" sometimes
replaced the right answer; and whether the model was permitted to perform
chain-of-thought reasoning before and/or after the choices were presented. MCQA
remained a good proxy for the downstream performance of models as long as they
were allowed to perform chain-of-thought reasoning only before being presented
with the options among which they had to select. On the other hand, large
models that were able to perform reasoning after being given a set of options
tended to significantly outperform their free-text performance due to
exploiting the information in the options. We conclude that MCQA is no longer a
good proxy for assessing downstream performance of state-of-the-art models, and
offer practical guidelines for designing more robust, bias-resistant benchmarks
that better reflect LLMs' genuine reasoning capabilities.

</details>


### [123] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
*Leanne Tan,Gabriel Chua,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: LionGuard 2, a lightweight multilingual moderation classifier, addresses low-resource language issues in moderation systems, achieving strong results and practical deployment in Singapore.


<details>
  <summary>Details</summary>
Motivation: To create a multilingual moderation system for low-resource languages like Singapore's Malay, Tamil, and Chinese variants, where traditional systems lag behind.

Method: The system combines pre-trained OpenAI embeddings with a multi-head ordinal classifier, using high-quality local data to train a lightweight, multilingual moderation model.

Result: LionGuard 2 outperformed commercial and open-source benchmarks across 17 datasets, proving effective in practical deployments within the Singapore Government.

Conclusion: High-quality localized data and multilingual embeddings can deliver effective moderation performance even without using large-scale models, fostering safer online environments.

Abstract: Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.

</details>


### [124] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
*Amedeo Buonanno,Alessandro Rivetti,Francesco A. N. Palmieri,Giovanni Di Gennaro,Gianmarco Romano*

Main category: cs.CL

TL;DR: The paper investigates entropy analysis to study information distribution and transformation within Transformer-based architectures, using GPT models as a case study.


<details>
  <summary>Details</summary>
Motivation: The authors aim to enhance understanding of the internal workings and information processing mechanisms of Transformer-based architectures.

Method: Entropy analysis is applied to quantify token-level uncertainty and observe entropy patterns at different processing stages in GPT models.

Result: The study illustrates that entropy analysis can provide insights into model behavior and internal representations.

Conclusion: Entropy analysis may serve as a valuable tool in interpretability and evaluation frameworks for Transformer-based models.

Abstract: This work explores entropy analysis as a tool for probing information
distribution within Transformer-based architectures. By quantifying token-level
uncertainty and examining entropy patterns across different stages of
processing, we aim to investigate how information is managed and transformed
within these models. As a case study, we apply the methodology to a GPT-based
large language model, illustrating its potential to reveal insights into model
behavior and internal representations. This approach may offer insights into
model behavior and contribute to the development of interpretability and
evaluation frameworks for transformer-based models

</details>


### [125] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
*Elisa Sanchez-Bayona,Rodrigo Agerri*

Main category: cs.CL

TL;DR: This paper evaluates how well Large Language Models (LLMs) interpret metaphors across varied datasets and tasks, revealing their reliance on surface-level features rather than genuine metaphor understanding.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper is to address limitations in existing metaphor-processing research, which often focuses on single datasets or uses artificial data, to comprehensively evaluate LLMs.

Method: The paper uses diverse public datasets with metaphor annotations, applying LLMs to NLI and QA tasks to assess their metaphor interpretation capabilities under different configurations.

Result: Results show LLMs' performance is primarily influenced by lexical overlap and sentence length, not metaphorical content, suggesting metaphor understanding isn't an emergent capability.

Conclusion: The study emphasizes the need for realistic metaphor evaluation frameworks, concluding that LLMs rely on surface features rather than true comprehension of figurative language.

Abstract: This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (LLMs) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that LLMs' performance is more influenced by features like
lexical overlap and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of LLMs to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of LLMs in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.

</details>


### [126] [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: The paper proposes 'Stitch,' a method to enable spoken language models (SLMs) to incorporate simultaneous unspoken reasoning and spoken responses, improving reasoning performance without additional latency.


<details>
  <summary>Details</summary>
Motivation: Current SLMs lack an internal reasoning process before producing spoken responses, unlike humans, who think internally before communicating. This gap limits the clarity, conciseness, and reasoning ability of SLMs.

Method: The authors introduce Stitch, a framework where SLMs alternate between generating unspoken reasoning chunks and spoken response chunks during the speaking process. This method leverages the free time during audio playback to generate reasoning tokens, enabling simultaneous reasoning and speaking with no added latency.

Result: Stitch improves reasoning performance by 15% on math reasoning datasets compared to baselines without unspoken reasoning. It achieves this without increasing response latency and maintains comparable performance on non-reasoning tasks.

Conclusion: Stitch effectively integrates unspoken reasoning into SLMs without compromising latency or non-reasoning task performance, advancing the capabilities of spoken language models.

Abstract: Spoken Language Models (SLMs) are designed to take speech inputs and produce
spoken responses. However, current SLMs lack the ability to perform an
internal, unspoken thinking process before responding. In contrast, humans
typically engage in complex mental reasoning internally, enabling them to
communicate ideas clearly and concisely. Thus, integrating an unspoken thought
process into SLMs is highly desirable. While naively generating a complete
chain-of-thought (CoT) reasoning before starting to talk can enable thinking
for SLMs, this induces additional latency for the speech response, as the CoT
reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a
novel generation method that alternates between the generation of unspoken
reasoning chunks and spoken response chunks. Since the audio duration of a
chunk of spoken response is much longer than the time to generate the tokens in
a chunk of spoken response, we use the remaining free time to generate the
unspoken reasoning tokens. When a chunk of audio is played to the user, the
model continues to generate the next unspoken reasoning chunk, achieving
simultaneous thinking and talking. Remarkably, Stitch matches the latency of
baselines that cannot generate unspoken CoT by design while outperforming those
baselines by 15% on math reasoning datasets; Stitch also performs equally well
on non-reasoning datasets as those baseline models. Some animations and
demonstrations are on the project page: https://d223302.github.io/STITCH.

</details>


### [127] [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)
*Jierui Li,Raymond Mooney*

Main category: cs.CL

TL;DR: This paper investigates the ability of large language models (LLMs) to identify algorithmically similar problems (ASPs) using a new benchmark called AlgoSimBench, revealing limitations and proposing enhancements.


<details>
  <summary>Details</summary>
Motivation: To explore whether reasoning capabilities of LLMs can generalize to identifying algorithmically similar problems in domains less emphasized during training.

Method: The authors introduce AlgoSimBench, a curated benchmark consisting of problems annotated with algorithm tags and MCQs, and propose attempted solution matching (ASM) as a method for improving problem similarity detection.

Result: LLMs show suboptimal performance in identifying ASPs, with the best model achieving 65.9% accuracy on MCQs. ASM improves accuracy by 6.7% to 11.7%, and combining ASM with BM25 achieves up to 52.2% accuracy under summarization.

Conclusion: LLMs struggle to generalize reasoning to identifying ASPs effectively, but methods like ASM show promise in enhancing detection, offering insights for further improvement in LLM capabilities.

Abstract: Recent progress in LLMs, such as reasoning models, has demonstrated strong
abilities to solve complex competitive programming problems, often rivaling top
human competitors. However, it remains underexplored whether these abilities
generalize to relevant domains that are less seen during training. To address
this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'
ability to identify algorithmically similar problems (ASPs)-problems that can
be solved using similar algorithmic approaches. AlgoSimBench consists of 1317
problems, annotated with 231 distinct fine-grained algorithm tags, from which
we curate 402 multiple-choice questions (MCQs), where each question presents
one algorithmically similar problem alongside three textually similar but
algorithmically dissimilar distractors. Our evaluation reveals that LLMs
struggle to identify ASPs, with the best-performing model (o3-mini) achieving
only 65.9% accuracy on the MCQ task. To address this challenge, we propose
attempted solution matching (ASM), a novel method for improving problem
similarity detection. On our MCQ task, ASM yields an absolute accuracy
improvement of 6.7% to 11.7% across different models. We also evaluated code
embedding models and retrieval methods on similar problem identification. While
the adversarial selection of problems degrades the performance to be less than
random, we found that simply summarizing the problem to remove narrative
elements eliminates the effect, and combining ASM with a keyword-prioritized
method, BM25, can yield up to 52.2% accuracy. Code and data are available at
github.com

</details>


### [128] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
*Alexandru Coca,Mark Gaynor,Zhenxing Zhang,Jianpeng Cheng,Bo-Hsiang Tseng,Pete Boothroyd,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: This paper introduces ASPERA, a framework to leverage large language models (LLMs) for creating digital assistants capable of complex action execution by generating and validating programs.


<details>
  <summary>Details</summary>
Motivation: To equip digital assistants with capabilities to execute complex, multi-step actions using LLMs, addressing challenges of data availability and robust evaluation.

Method: They developed ASPERA, a framework featuring an assistant library simulation and a human-assisted data generation engine for creating high-quality tasks and evaluation tools.

Result: The authors released Asper-Bench, an evaluation dataset of 250 challenging tasks, and demonstrated that LLMs struggle with program generation grounded in assistant libraries compared to simpler code generation.

Conclusion: Custom assistant library-based program generation presents significant challenges for LLMs, underscoring the importance of robust frameworks like ASPERA to improve their efficacy.

Abstract: This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

</details>


### [129] [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
*Kaiyan Chang,Yonghao Shi,Chenglong Wang,Hang Zhou,Chi Hu,Xiaoqian Liu,Yingfeng Luo,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: The paper explores training-free Test-Time Scaling (TTS) methods for reasoning and proposes a Hybrid TTS approach combining sequential and parallel scaling methods to improve reasoning performance in LLMs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the rising computation burdens from training-based TTS methods by focusing on training-free alternatives that could preserve efficiency while enhancing reasoning capabilities.

Method: The authors propose Conditional Step-level Self-refinement, a sequential scaling method driven by process verification. They combine this with classical parallel scaling approaches to formulate a Hybrid Test-Time Scaling framework.

Result: Experiments across five instruction-tuned large language models (LLMs) of varying sizes (3B-14B) reveal that the hybrid strategy significantly boosts reasoning performance without requiring additional training.

Conclusion: The paper concludes that the proposed hybrid approach demonstrates strong potential to push the boundaries of reasoning performance in LLMs through efficient, training-free Test-Time Scaling techniques.

Abstract: Test-Time Scaling (TTS) is a promising approach to progressively elicit the
model's intelligence during inference. Recently, training-based TTS methods,
such as continued reinforcement learning (RL), have further surged in
popularity, while training-free TTS methods are gradually fading from
prominence. However, the additional computation overhead of training amplifies
the burden on test-time scaling. In this paper, we focus on training-free TTS
methods for reasoning. We first design Conditional Step-level Self-refinement,
a fine-grained sequential scaling method guided by process verification. On top
of its effectiveness, we further combine it with other classical parallel
scaling methods at the step level, to introduce a novel inference paradigm
called Hybrid Test-Time Scaling. Extensive experiments on five
instruction-tuned LLMs across different scales (3B-14B) and families
demonstrate that hybrid strategy incorporating various training-free TTS
methods at a fine granularity has considerable potential for expanding the
reasoning performance boundaries of LLMs.

</details>


### [130] [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)
*Vitaly Protasov,Nikolay Babakov,Daryna Dementieva,Alexander Panchenko*

Main category: cs.CL

TL;DR: This paper evaluates text detoxification systems across nine languages using neural-based models and LLM-as-a-judge approaches.


<details>
  <summary>Details</summary>
Motivation: Text style transfer evaluation, particularly for multilingual contexts, currently faces challenges in aligning automatic metrics with human judgment.

Method: The study investigates text detoxification evaluation pipelines using modern neural-based evaluation models and LLM-as-a-judge methods, inspired by machine translation approaches.

Result: The study provides insights into effective methods for evaluating multilingual text detoxification systems.

Conclusion: A practical recipe for reliable multilingual text detoxification evaluation pipelines is established, supporting broader applicability beyond English.

Abstract: Despite recent progress in large language models (LLMs), evaluation of text
generation tasks such as text style transfer (TST) remains a significant
challenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)
revealed a substantial gap between automatic metrics and human judgments.
Moreover, most prior work focuses exclusively on English, leaving multilingual
TST evaluation largely unexplored. In this paper, we perform the first
comprehensive multilingual study on evaluation of text detoxification system
across nine languages: English, Spanish, German, Chinese, Arabic, Hindi,
Ukrainian, Russian, Amharic. Drawing inspiration from the machine translation,
we assess the effectiveness of modern neural-based evaluation models alongside
prompting-based LLM-as-a-judge approaches. Our findings provide a practical
recipe for designing more reliable multilingual TST evaluation pipeline in the
text detoxification case.

</details>


### [131] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
*Nicolas Poggi,Shashank Agnihotri,Margret Keuper*

Main category: cs.CL

TL;DR: The paper explores using In-Context Learning with Vision-Language Models to improve Terahertz imaging classification without requiring extensive data or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Effective Terahertz imaging classification faces difficulties due to limited data annotations, low resolution, and visual ambiguity.

Method: The paper adapts Vision-Language Models for Terahertz imaging using modality-aligned prompting, focusing on zero-shot and one-shot learning approaches.

Result: In-Context Learning demonstrates enhanced classification performance and interpretability in low-data scenarios within the Terahertz imaging domain.

Conclusion: This approach marks the first application of In-Context Learning and Vision-Language Models to Terahertz imaging, suggesting promising opportunities for scientific fields with resource constraints.

Abstract: Terahertz (THz) imaging enables non-invasive analysis for applications such
as security screening and material classification, but effective image
classification remains challenging due to limited annotations, low resolution,
and visual ambiguity. We introduce In-Context Learning (ICL) with
Vision-Language Models (VLMs) as a flexible, interpretable alternative that
requires no fine-tuning. Using a modality-aligned prompting framework, we adapt
two open-weight VLMs to the THz domain and evaluate them under zero-shot and
one-shot settings. Our results show that ICL improves classification and
interpretability in low-data regimes. This is the first application of
ICL-enhanced VLMs to THz imaging, offering a promising direction for
resource-constrained scientific domains. Code:
\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub
repository}.

</details>


### [132] [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)
*Xinping Zhao,Shouzheng Huang,Yan Zhong,Xinshuo Hu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: The paper presents LEAR, a novel approach to improve Retrieval-Augmented Generation (RAG) by learning rational evidence extraction and reasoning to enhance LLM performance.


<details>
  <summary>Details</summary>
Motivation: Retrieval-Augmented Generation improves accuracy in LLMs but suffers from the issue of retrieval noise, which degrades the quality of generated outputs. Existing methods lack explicit reasoning processes, leading to key information omissions and poor generalization.

Method: The proposed method, LEAR, combines reasoning and extraction in a unified structure. It employs knowledge token masks for disentanglement, integrates reasoning and extraction as a single training response, and uses three types of reward functions (answer, length, and format) in a policy optimization algorithm.

Result: LEAR demonstrates significant improvements in compactness and quality of evidence retrieval, enhances accuracy in downstream tasks, and proves effective in online RAG applications, as verified through experiments on three benchmark datasets.

Conclusion: LEAR provides an advanced mechanism for rational evidence extraction and reasoning, addressing the limitations of previous methods and boosting the overall effectiveness of Retrieval-Augmented Generation in LLMs.

Abstract: Retrieval-Augmented Generation (RAG) effectively improves the accuracy of
Large Language Models (LLMs). However, retrieval noises significantly impact
the quality of LLMs' generation, necessitating the development of denoising
mechanisms. Previous methods extract evidence straightforwardly without
explicit thinking, which risks filtering out key clues and struggles with
generalization. To this end, we propose LEAR, which learns to extract rational
evidence by (1) explicitly reasoning to identify potential cues within
retrieval contents first, and then (2) consciously extracting to avoid omitting
any key cues helpful for answering questions. Specifically, we frame evidence
reasoning and evidence extraction into one unified response for end-to-end
training; apply knowledge token masks for disentanglement to derive
reasoning-based and extraction-based answers; and devise three types of
verifiable reward functions, including answer, length, and format, to update
the model via the policy optimization algorithm. Extensive experiments on three
benchmark datasets show the effectiveness of LEAR, providing compact and
high-quality evidence, improving the accuracy of downstream tasks, and
promoting effective application in online RAG systems.

</details>


### [133] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
*Armin Pournaki*

Main category: cs.CL

TL;DR: The paper studies how analyzing conflicting narratives (interpretive lenses) in political discourse reveals patterns of polarization and issue alignment on Twitter, focusing on topics like Ukraine, Covid, and climate change.


<details>
  <summary>Details</summary>
Motivation: To understand how conflicting interpretations of political events contribute to polarization and how political actors use narratives strategically to align opinions across issues.

Method: The authors analyze tweets from opposing ideological groups on polarized issues, identifying textual signals of conflicting narratives and focusing on actantial role interpretations and narrative alignments.

Result: Evidence was found for conflicting narratives, such as using different actantial roles or narratives for the same topics (e.g., NATO or Bill Gates in political contexts), and early evidence of narrative alignment across issues was provided.

Conclusion: Narrative analysis serves as a valuable tool for understanding polarization and the discursive mechanisms linking opinions around politically divisive issues.

Abstract: Narratives are key interpretative devices by which humans make sense of
political reality. In this work, we show how the analysis of conflicting
narratives, i.e. conflicting interpretive lenses through which political
reality is experienced and told, provides insight into the discursive
mechanisms of polarization and issue alignment in the public sphere. Building
upon previous work that has identified ideologically polarized issues in the
German Twittersphere between 2021 and 2023, we analyze the discursive dimension
of polarization by extracting textual signals of conflicting narratives from
tweets of opposing opinion groups. Focusing on a selection of salient issues
and events (the war in Ukraine, Covid, climate change), we show evidence for
conflicting narratives along two dimensions: (i) different attributions of
actantial roles to the same set of actants (e.g. diverging interpretations of
the role of NATO in the war in Ukraine), and (ii) emplotment of different
actants for the same event (e.g. Bill Gates in the right-leaning Covid
narrative). Furthermore, we provide first evidence for patterns of narrative
alignment, a discursive strategy that political actors employ to align opinions
across issues. These findings demonstrate the use of narratives as an
analytical lens into the discursive mechanisms of polarization.

</details>


### [134] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
*Alessio Pittiglio*

Main category: cs.CL

TL;DR: The paper proposes Transformer-based models for detecting logical fallacies in multimodal argument mining, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of recognizing logical fallacies in multimodal settings, particularly in political debates.

Method: Leverages pretrained Transformer models and incorporates context to classify fallacies across text, audio, and multimodal data.

Result: Achieved macro F1-scores of 0.4444 (text), 0.3559 (audio), and 0.4403 (multimodal), with multimodal performance comparable to text-only.

Conclusion: The models show promising potential, especially in multimodal setups, but indicate room for further improvement.

Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

</details>


### [135] [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)
*Xinyu Zhang,Yuanquan Hu,Fangchao Liu,Zhicheng Dou*

Main category: cs.CL

TL;DR: The paper proposes P3, a framework for optimizing both system and user prompts simultaneously to enhance large language model (LLM) performance across diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Current approaches only optimize either system or user prompts individually, which may lead to suboptimal results due to their interdependent nature.

Method: The proposed framework, P3, uses an iterative and holistic optimization process for system and user prompts. Offline optimized prompts are utilized for query-dependent online prompting.

Result: Experiments across general and reasoning tasks show that P3 significantly outperforms unilateral optimization methods.

Conclusion: Holistic optimization of both system and user prompts is effective in improving LLM performance across various domains.

Abstract: Current large language model (LLM) applications often employ multi-component
prompts, comprising both system and user prompts, to guide model behaviors.
While recent advancements have demonstrated the efficacy of automatically
optimizing either the system or user prompt to boost performance, such
unilateral approaches often yield suboptimal outcomes due to the interdependent
nature of these components. In this work, we introduce P3, a novel
self-improvement framework that concurrently optimizes both system and user
prompts through an iterative process. The offline optimized prompts are further
leveraged to promote online prompting by performing query-dependent prompt
optimization. Extensive experiments on general tasks (e.g., Arena-hard and
Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3
achieves superior performance in the realm of automatic prompt optimization.
Our results highlight the effectiveness of a holistic optimization strategy in
enhancing LLM performance across diverse domains.

</details>


### [136] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
*Congmin Zheng,Jiachen Zhu,Jianghao Lin,Xinyi Dai,Yong Yu,Weinan Zhang,Mengyue Yang*

Main category: cs.CL

TL;DR: This paper addresses length bias in Process Reward Models (PRMs) for reasoning in LLMs and proposes the CoLD framework to mitigate it.


<details>
  <summary>Details</summary>
Motivation: Existing PRMs demonstrate a pervasive length bias, which assigns higher rewards to longer reasoning steps even when they are semantically and logically equivalent. This bias causes verbose and unreliable reasoning outputs.

Method: The authors propose CoLD, a counterfactual reasoning framework composed of length-penalty adjustments, a learned bias estimator, and joint training for length-invariant rewards. Causal graph analysis supports this approach.

Result: Experiments on datasets like MATH500 and GSM-Plus show CoLD reduces reward-length correlation, enhances step-selection accuracy, and promotes concise and valid reasoning steps.

Conclusion: CoLD demonstrates effectiveness in mitigating length bias, thereby improving the reliability and robustness of PRMs in evaluating multi-step reasoning in LLMs.

Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [137] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
*David Peter Wallis Freeborn*

Main category: cs.CL

TL;DR: This paper constructs two signaling game models that enable receivers to develop genuine compositional understanding, unlike standard models where receivers fail to interpret compositional messages.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitation in standard signaling game models where receivers fail to learn or interpret compositional information, leading to the loss of information from messages.

Method: The author introduces two new signaling game models: a minimalist receiver model that learns from atomic components of signals, and a generalist receiver model that learns from all available information. These models are simpler yet enable receivers to process compositional aspects of messages.

Result: The proposed models show that it is feasible for receivers to develop genuine compositional understanding by learning from the atomic components or full breadth of message information.

Conclusion: Simpler signaling game models can evolve genuine compositional understanding in receivers, overcoming a limitation in standard approaches.

Abstract: Receivers in standard signaling game models struggle with learning
compositional information. Even when the signalers send compositional messages,
the receivers do not interpret them compositionally. When information from one
message component is lost or forgotten, the information from other components
is also erased. In this paper I construct signaling game models in which
genuine compositional understanding evolves. I present two new models: a
minimalist receiver who only learns from the atomic messages of a signal, and a
generalist receiver who learns from all of the available information. These
models are in many ways simpler than previous alternatives, and allow the
receivers to learn from the atomic components of messages.

</details>


### [138] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
*Seok Hwan Song,Mohna Chakraborty,Qi Li,Wallapak Tavanapong*

Main category: cs.CL

TL;DR: The study examines how question types affect Large Language Models' (LLMs) accuracy in reasoning tasks, revealing performance variability and non-correlation between reasoning and answer accuracy.


<details>
  <summary>Details</summary>
Motivation: To explore how the type of question impacts the reasoning and final answer accuracy of LLMs, an area that has not been thoroughly examined.

Method: Five LLMs were tested with three question types on quantitative and deductive reasoning tasks, analyzing both reasoning step accuracy and final answer selection.

Result: (1) Significant performance differences across question types. (2) Reasoning accuracy and final selection accuracy may not correlate. (3) Factors such as the number of options and word choice influence performance.

Conclusion: Question type significantly impacts LLM reasoning performance, indicating the need for question design refinement to assess true reasoning capabilities.

Abstract: Large Language Models (LLMs) have been evaluated using diverse question
types, e.g., multiple-choice, true/false, and short/long answers. This study
answers an unexplored question about the impact of different question types on
LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on
three different types of questions using quantitative and deductive reasoning
tasks. The performance metrics include accuracy in the reasoning steps and
choosing the final answer. Key Findings: (1) Significant differences exist in
LLM performance across different question types. (2) Reasoning accuracy does
not necessarily correlate with the final selection accuracy. (3) The number of
options and the choice of words, influence LLM performance.

</details>


### [139] [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)
*Tian Li,Yujian Sun,Huizhi Liang*

Main category: cs.CL

TL;DR: The SemEval-2025 Task 11 focuses on text-based emotion detection across 28 languages, featuring challenges in diversity and emotion intensity prediction. Researchers utilized two contrastive learning approaches for improvement, achieving competitive rankings.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in emotion detection across diverse languages using advanced methods in emotion recognition.

Method: Two contrastive learning techniques—sample-based and generation-based—were employed, utilizing fine-tuning on LLaMa3-Instruct-8B.

Result: The system achieved a competitive 9th place in Track A and 6th place in Track B for English, with strong performance in other languages.

Conclusion: The paper demonstrates the value of contrastive learning methods in improving multi-label classification and emotion intensity prediction across languages.

Abstract: The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,
introduces an emotion recognition challenge spanning over 28 languages. This
competition encourages researchers to explore more advanced approaches to
address the challenges posed by the diversity of emotional expressions and
background variations. It features two tracks: multi-label classification
(Track A) and emotion intensity prediction (Track B), covering six emotion
categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we
systematically explore the benefits of two contrastive learning approaches:
sample-based (Contrastive Reasoning Calibration) and generation-based (DPO,
SimPO) contrastive learning. The sample-based contrastive approach trains the
model by comparing two samples to generate more reliable predictions. The
generation-based contrastive approach trains the model to differentiate between
correct and incorrect generations, refining its prediction. All models are
fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A
and 6th place in Track B for English, while ranking among the top-tier
performing systems for other languages.

</details>


### [140] [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
*Alina Hyk,Kiera McCormick,Mian Zhong,Ioana Ciucă,Sanjib Sharma,John F Wu,J. E. G. Peek,Kartheik G. Iyer,Ziang Xiao,Anjalie Field*

Main category: cs.CL

TL;DR: This paper examines how users evaluate LLMs in the context of astronomical research and proposes ways to improve evaluation benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the gap between current LLM evaluation benchmarks and diverse, real-world use cases in scientific research.

Method: The study analyzes 368 user interactions with an LLM-powered astronomical literature bot on Slack and conducts interviews with 11 astronomers.

Result: Findings reveal user evaluation patterns, including question types and response judgment criteria, and inform recommendations for improved benchmarks.

Conclusion: The study provides practical guidelines for creating better LLM evaluation benchmarks, enhancing both assessment and practical use in scientific fields.

Abstract: There is growing interest in leveraging LLMs to aid in astronomy and other
scientific research, but benchmarks for LLM evaluation in general have not kept
pace with the increasingly diverse ways that real people evaluate and use these
models. In this study, we seek to improve evaluation procedures by building an
understanding of how users evaluate LLMs. We focus on a particular use case: an
LLM-powered retrieval-augmented generation bot for engaging with astronomical
literature, which we deployed via Slack. Our inductive coding of 368 queries to
the bot over four weeks and our follow-up interviews with 11 astronomers reveal
how humans evaluated this system, including the types of questions asked and
the criteria for judging responses. We synthesize our findings into concrete
recommendations for building better benchmarks, which we then employ in
constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our
work offers ways to improve LLM evaluation and ultimately usability,
particularly for use in scientific research.

</details>


### [141] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
*Sahana Srinivasan,Xuguang Ai,Thaddaeus Wai Soon Lo,Aidan Gilson,Minjie Zou,Ke Zou,Hyunjae Kim,Mingjia Yang,Krithi Pushpanathan,Samantha Yew,Wan Ting Loke,Jocelyn Goh,Yibing Chen,Yiming Kong,Emily Yuelei Fu,Michelle Ongyong Hui,Kristen Nwanyanwu,Amisha Dave,Kelvin Zhenghao Li,Chen-Hsin Sun,Mark Chia,Gabriel Dawei Yang,Wendy Meihua Wong,David Ziyou Chen,Dianbo Liu,Maxwell Singer,Fares Antaki,Lucian V Del Priore,Jost Jonas,Ron Adelman,Qingyu Chen,Yih-Chung Tham*

Main category: cs.CL

TL;DR: The study introduces BELO, a benchmark developed for evaluating large language models (LLMs) in ophthalmology, focusing on clinical accuracy and reasoning.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks in ophthalmology for LLMs are narrow in scope and focus primarily on accuracy, lacking comprehensiveness and reasoning assessment.

Method: The researchers curated a dataset of 900 multiple-choice ophthalmology questions from various sources, fine-tuned and reviewed through expert validation. They evaluated six LLMs using a mix of classification and text-generation metrics.

Result: BELO provides a rigorously tested, high-quality dataset for ophthalmology LLM evaluation, offering a public leaderboard and ensuring impartiality through its hold-out design.

Conclusion: BELO addresses limitations in LLM evaluation frameworks for ophthalmology by introducing a standardized, expert-validated benchmarking tool for future advancements.

Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology
are limited in scope and disproportionately prioritise accuracy. We introduce
BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive
evaluation benchmark developed through multiple rounds of expert checking by 13
ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and
reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we
curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse
medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset
underwent multiple rounds of expert checking. Duplicate and substandard
questions were systematically removed. Ten ophthalmologists refined the
explanations of each MCQ's correct answer. This was further adjudicated by
three senior ophthalmologists. To illustrate BELO's utility, we evaluated six
LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)
using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,
BARTScore, METEOR, and AlignScore). In a further evaluation involving human
experts, two ophthalmologists qualitatively reviewed 50 randomly selected
outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900
high-quality, expert-reviewed questions aggregated from five sources: BCSC
(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public
leaderboard has been established to promote transparent evaluation and
reporting. Importantly, the BELO dataset will remain a hold-out,
evaluation-only benchmark to ensure fair and reproducible comparisons of future
models.

</details>


### [142] [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)
*Yuanhao Shen,Daniel Xavier de Sousa,Ricardo Marçal,Ali Asad,Hongyu Guo,Xiaodan Zhu*

Main category: cs.CL

TL;DR: The paper introduces IDRBench, a benchmark to evaluate Large Language Models on their ability to generate interdisciplinary research ideas.


<details>
  <summary>Details</summary>
Motivation: LLMs show notable reasoning abilities, but their capability in interdisciplinary research ideation lacks proper evaluation.

Method: The researchers developed IDRBench, a benchmark that includes an expert-annotated dataset from six scientific domains and three evaluation tasks reflecting stages of interdisciplinary research.

Result: Despite some level of interdisciplinary awareness, current LLMs struggle to generate high-quality ideas in interdisciplinary research.

Conclusion: IDRBench highlights gaps in LLM performance and could guide the development of advanced models for interdisciplinary research.

Abstract: Recent advancements in Large Language Models (LLMs) have revealed their
impressive ability to perform multi-step, logic-driven reasoning across complex
domains, positioning them as powerful tools and collaborators in scientific
discovery while challenging the long-held view that inspiration-driven ideation
is uniquely human. However, the lack of a dedicated benchmark that evaluates
LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings
poses a critical barrier to fully understanding their strengths and
limitations. To address this gap, we introduce IDRBench -- a pioneering
benchmark featuring an expert annotated dataset and a suite of tasks tailored
to evaluate LLMs' capabilities in proposing valuable research ideas from
different scientific domains for interdisciplinary research. This benchmark
aims to provide a systematic framework for assessing LLM performance in
complex, cross-domain scientific research. Our dataset consists of scientific
publications sourced from the ArXiv platform covering six distinct disciplines,
and is annotated by domain experts with diverse academic backgrounds. To ensure
high-quality annotations, we emphasize clearly defined dimensions that
characterize authentic interdisciplinary research. The design of evaluation
tasks in IDRBench follows a progressive, real-world perspective, reflecting the
natural stages of interdisciplinary research development, including 1) IDR
Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.
Using IDRBench, we construct baselines across 10 LLMs and observe that despite
fostering some level of IDR awareness, LLMs still struggle to produce quality
IDR ideas. These findings could not only spark new research directions, but
also help to develop next-generation LLMs that excel in interdisciplinary
research.

</details>


### [143] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: TF-IDF is linked to Fisher's exact test, showing its statistical significance under idealized conditions and large document collections.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical foundation for TF-IDF in the statistics community and justify its effectiveness using significance testing.

Method: The paper explores the relationship between TF-ICF, a variant of TF-IDF, and the negative logarithm of p-values from Fisher's exact test under specific conditions.

Result: TF-ICF is shown to closely align with p-values under mild conditions, and converges to TF-IDF in infinitely large document collections.

Conclusion: The connection between TF-IDF and Fisher's exact test offers a statistical explanation for its long-standing utility in text analysis.

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [144] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
*Ruizhe Zhu,Hao Zhu,Yaxuan Li,Syang Zhou,Shijing Cai,Malgorzata Lazuka,Elliott Ash*

Main category: cs.CL

TL;DR: The paper introduces DialogueForge, a framework for generating AI-simulated human-chatbot dialogues using seed prompts from real conversations, leveraging different LLMs and fine-tuning to enhance performance.


<details>
  <summary>Details</summary>
Motivation: Collecting human-chatbot dialogues is labor-intensive and time-consuming, posing challenges for advancing research in conversational AI.

Method: The authors proposed DialogueForge, which uses seed prompts derived from real conversations to generate multi-turn dialogues via large language models (LLMs) and explores fine-tuning smaller models. Models are assessed using UniEval and GTEval evaluation protocols.

Result: Proprietary models like GPT-4 perform best in generating realistic dialogues, while smaller open-source models such as Llama and Mistral show promise with customization and improve significantly with supervised fine-tuning.

Conclusion: DialogueForge highlights that while current LLMs, especially large proprietary ones, excel at generating realistic dialogues, achieving consistent, natural, and coherent human-like conversations remains an open challenge.

Abstract: Collecting human-chatbot dialogues typically demands substantial manual
effort and is time-consuming, which limits and poses challenges for research on
conversational AI. In this work, we propose DialogueForge - a framework for
generating AI-simulated conversations in human-chatbot style. To initialize
each generated conversation, DialogueForge uses seed prompts extracted from
real human-chatbot interactions. We test a variety of LLMs to simulate the
human chatbot user, ranging from state-of-the-art proprietary models to
small-scale open-source LLMs, and generate multi-turn dialogues tailored to
specific tasks. In addition, we explore fine-tuning techniques to enhance the
ability of smaller models to produce indistinguishable human-like dialogues. We
evaluate the quality of the simulated conversations and compare different
models using the UniEval and GTEval evaluation protocols. Our experiments show
that large proprietary models (e.g., GPT-4o) generally outperform others in
generating more realistic dialogues, while smaller open-source models (e.g.,
Llama, Mistral) offer promising performance with greater customization. We
demonstrate that the performance of smaller models can be significantly
improved by employing supervised fine-tuning techniques. Nevertheless,
maintaining coherent and natural long-form human-like dialogues remains a
common challenge across all models.

</details>


### [145] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
*Lyumanshan Ye,Xiaojie Cai,Xinkai Wang,Junfei Wang,Xiangkun Hu,Jiadi Su,Yang Nan,Sihan Wang,Bohan Zhang,Xiaoze Fan,Jinbin Luo,Yuxiang Zheng,Tianze Xu,Dayuan Fu,Yunze Wu,Pengrui Lu,Zengzhi Wang,Yiwei Qin,Zhen Huang,Yan Ma,Zhulin Hu,Haoyang Zou,Tiantian Mi,Yixin Ye,Ethan Chern,Pengfei Liu*

Main category: cs.CL

TL;DR: This work advocates for "Interaction as Intelligence" in human-AI tasks, introducing the Deep Cognition system that facilitates deeper collaborative interaction, moving beyond the traditional request-response AI paradigms. Key metrics improve by 31.8% to 50.0%.


<details>
  <summary>Details</summary>
Motivation: Current AI systems rely heavily on an inflexible "input-wait-output" paradigm, which leads to issues like error cascades, inability to refine questions, and underutilization of human expertise. A new vision for collaboration is needed to address these limitations.

Method: The authors propose "Deep Cognition," a system designed around three innovations: transparent and interruptible AI reasoning, fine-grained two-way dialogue, and shared cognitive context that adapts to user behavior without explicit commands.

Result: The proposed system significantly improved interaction capabilities, as shown by higher performance in transparency (+20.0%), fine-grained interaction (+29.2%), and ease of collaboration (+27.7%), among others. Tests on complex tasks show an improvement of 31.8% to 50.0% over current approaches.

Conclusion: Deep Cognition shifts the paradigm from instructive AI use to a cooperative, oversight-driven interaction model, marking a new approach to defining intelligence in human-AI systems for deep research tasks.

Abstract: This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

</details>


### [146] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
*Andrei-Valentin Tanase,Elena Pelican*

Main category: cs.CL

TL;DR: Supernova is a 650M-parameter transformer that achieves performance near larger models by focusing on architectural design and tokenization innovation.


<details>
  <summary>Details</summary>
Motivation: To demonstrate how architectural efficiency and advanced tokenization can outperform traditional scaling approaches in transformer models.

Method: The model uses a combination of RoPE, GQA compression, RMSNorm, SwiGLU activations, and a custom byte-level BPE tokenizer for better computational and compression efficiency.

Result: Supernova achieves 90% of the performance of 1B-parameter models with 53% fewer parameters and 100B training tokens, significantly less than comparable models.

Conclusion: Efficient architecture and high-quality tokenization can serve as alternatives to increasing parameter counts, challenging conventional scaling strategies.

Abstract: We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

</details>


### [147] [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)
*Jiakang Wang,Runze Liu,Fuzheng Zhang,Xiu Li,Guorui Zhou*

Main category: cs.CL

TL;DR: This paper introduces Archer, a novel RLVR approach improving the reasoning abilities of LLMs through entropy-aware dual-token constraints and synchronous updates.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of previous RLVR methods that don't differentiate sufficiently between knowledge-related and reasoning-related tokens, potentially breaking semantic dependencies and impeding learning.

Method: Archer applies dual-token constraints by using weaker KL regularization and higher clipping thresholds for reasoning tokens, while imposing stronger constraints on knowledge tokens, along with synchronous updates.

Result: The Archer approach demonstrates superior performance on several mathematical reasoning and code generation benchmarks, surpassing prior RLVR methods and achieving near state-of-the-art results for comparable model sizes.

Conclusion: Archer effectively enhances the reasoning abilities of LLMs using entropy-aware mechanisms while maintaining semantic dependencies and factual knowledge.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective
post-training method for improving the reasoning abilities of Large Language
Models (LLMs), mainly by shaping higher-order behaviors such as reflection and
planning. However, previous RLVR algorithms often apply uniform training
signals to all tokens, without considering the different roles of low-entropy
knowledge-related tokens and high-entropy reasoning-related tokens. Some recent
methods try to separate these token types by gradient masking or asynchronous
updates, but these approaches may break semantic dependencies in the model
output and hinder effective learning. In this work, we propose Archer, an
entropy-aware RLVR approach with dual-token constraints and synchronous
updates. Specifically, our method applies weaker KL regularization and higher
clipping thresholds to reasoning tokens to encourage exploration, while using
stronger constraints on knowledge tokens to maintain factual knowledge.
Experimental results on several mathematical reasoning and code generation
benchmarks show that our approach significantly outperforms previous RLVR
methods, reaching or exceeding state-of-the-art performance among models of
comparable size. The code is available at
https://github.com/wizard-III/ArcherCodeR.

</details>


### [148] [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)
*Felix Köster,Atsushi Uchida*

Main category: cs.CL

TL;DR: This paper investigates reservoir computing as an energy-efficient alternative to transformer-based models for character-level language processing.


<details>
  <summary>Details</summary>
Motivation: Current large language models (LLMs) have high energy demands and slow processing, creating accessibility and scalability bottlenecks.

Method: Three approaches for character-level language modeling are compared: traditional reservoir computing, attention-enhanced reservoir computing, and transformer-based architectures.

Result: Transformers achieve higher accuracy in predictions, while reservoir computing models demonstrate greater energy efficiency and faster training/inference speeds.

Conclusion: The study outlines ways to balance resource constraints with performance, highlighting the scalability of reservoir computing as a promising alternative.

Abstract: Large Language Models (LLM) have dominated the science and media landscape
duo to their impressive performance on processing large chunks of data and
produce human-like levels of text. Nevertheless, their huge energy demand and
slow processing still a bottleneck for further increasing quality while also
making the models accessible to everyone. To solve this bottleneck, we will
investigate how reservoir computing performs on natural text processing, which
could enable fast and energy efficient hardware implementations. Studies
investigating the use of reservoir computing as a language model remain sparse.
In this paper, we compare three distinct approaches for character-level
language modeling, two different reservoir computing approaches, where only an
output layer is trainable, and the well-known transformer-based architectures,
which fully learn an attention-based sequence representation. We explore the
performance, computational cost and prediction accuracy for both paradigms by
equally varying the number of trainable parameters for all models. Using a
consistent pipeline for all three approaches, we demonstrate that transformers
excel in prediction quality, whereas reservoir computers remain highly
efficient reducing the training and inference speed. Furthermore, we
investigate two types of reservoir computing: a traditional reservoir with a
static linear readout, and an attention-enhanced reservoir that dynamically
adapts its output weights via an attention mechanism. Our findings underline
how these paradigms scale and offer guidelines to balance resource constraints
with performance.

</details>


### [149] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
*Anton Abilov,Ke Zhang,Hemank Lamba,Elizabeth M. Olson,Joel R. Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: The paper focuses on deploying and maintaining AI models for impactful applications, highlighting collaborations with humanitarian organizations and real-world factors.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding real-world deployment and maintenance challenges in AI for Good applications.

Method: Collaborating closely with a humanitarian-to-humanitarian organization to deploy and sustain AI models in resource-constrained environments.

Result: Key insights and takeaways are provided for effective deployment and maintenance of AI in challenging settings.

Conclusion: Deploying and maintaining AI models in collaboration with humanitarian organizations can enhance real-world impact, and sharing experiences helps guide practitioners in the field.

Abstract: Publications in the AI for Good space have tended to focus on the research
and model development that can support high-impact applications. However, very
few AI for Good papers discuss the process of deploying and collaborating with
the partner organization, and the resulting real-world impact. In this work, we
share details about the close collaboration with a humanitarian-to-humanitarian
(H2H) organization and how to not only deploy the AI model in a
resource-constrained environment, but also how to maintain it for continuous
performance updates, and share key takeaways for practitioners.

</details>


### [150] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
*Yihao Li,Jiayi Xin,Miranda Muqing Miao,Qi Long,Lyle Ungar*

Main category: cs.CL

TL;DR: Language mixing, seen in bilingual LLMs during reasoning, enhances accuracy, with a probe guiding optimal language switching for further improvement.


<details>
  <summary>Details</summary>
Motivation: Investigate the reasoning behavior of bilingual LLMs and the potential benefits of intentional language mixing.

Method: Using RLVR for training, analyze the impact of language mixing on reasoning tasks and develop a probe to assess the usefulness of language switches.

Result: Language mixing improves reasoning accuracy by 5.6%, and a probe-guided approach boosts it by another 6.25%.

Conclusion: Language mixing is a strategic behavior in reasoning for bilingual LLMs, providing accuracy improvements when appropriately guided.

Abstract: Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [151] [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)
*Basma El Amel Boussaha,Leen AlQadi,Mugariya Farooq,Shaikha Alsuwaidi,Giulia Campesan,Ahmed Alzubaidi,Mohammed Alyafeai,Hakim Hacid*

Main category: cs.CL

TL;DR: This paper introduces 3LM, a set of three benchmarks for evaluating Arabic language models in STEM and code domains, areas that have been neglected in prior research.


<details>
  <summary>Details</summary>
Motivation: There is a gap in the development and evaluation of Large Language Models (LLMs) for Arabic, particularly in STEM and code domains, though Arabic is widely spoken globally.

Method: The authors created three benchmarks: (1) STEM question-answer pairs sourced from Arabic educational materials, (2) synthetically generated STEM questions, and (3) a code generation benchmark formed through careful translations of existing benchmarks with human reviews.

Result: The paper presents these three benchmarks, ensuring their quality through natural/synthetic data sourcing and iterative human-in-the-loop validation processes.

Conclusion: 3LM benchmarks aim to advance Arabic LLM research in underrepresented areas like STEM and code, offering publicly available tools for the broader research community.

Abstract: Arabic is one of the most widely spoken languages in the world, yet efforts
to develop and evaluate Large Language Models (LLMs) for Arabic remain
relatively limited. Most existing Arabic benchmarks focus on linguistic,
cultural, or religious content, leaving a significant gap in domains like STEM
and code which are increasingly relevant for real-world LLM applications. To
help bridge this gap, we present 3LM, a suite of three benchmarks designed
specifically for Arabic. The first is a set of STEM-related question-answer
pairs, naturally sourced from Arabic textbooks and educational worksheets. The
second consists of synthetically generated STEM questions, created using the
same sources. The third benchmark focuses on code generation, built through a
careful translation of two widely used code benchmarks, incorporating a
human-in-the-loop process with several rounds of review to ensure high-quality
and faithful translations. We release all three benchmarks publicly to support
the growth of Arabic LLM research in these essential but underrepresented
areas.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [152] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: The paper evaluates optimization-based algorithms for fitting tessellation models, comparing methods for Voronoi and related diagrams in representing 3D material data using real-world performance metrics.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of accurately fitting tessellation models to complex 3D image data in materials like polycrystals and foams, enabling better approximation of grain structures.

Method: The authors analyze optimization-based methods (e.g., linear/nonlinear programming, stochastic optimization, and gradient descent) to fit Voronoi, Laguerre, and GBPDs models and assess fit quality using discrepancy measures.

Result: Results reveal trade-offs between algorithm and model complexity and approximation quality, providing insights into which methods perform best under varying data and application requirements.

Conclusion: The paper offers guidance for selecting appropriate algorithmic approaches to fit tessellation models based on the characteristics of 3D material datasets and specific application objectives.

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [153] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: This paper focuses on enhancing scene understanding for self-driving cars using semantic segmentation models, evaluated on the BDD100k dataset.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve scene understanding in self-driving cars by leveraging deep learning, a crucial component of AI.

Method: The authors propose several models using different backbones as encoders for semantic segmentation, training them on the BDD100k dataset.

Result: The experiments demonstrate that selecting the right backbone significantly enhances model performance, improving key metrics like accuracy, mean IoU, and loss.

Conclusion: The study concludes that optimizing backbone choice in DL models leads to better scene understanding, which is critical for safe self-driving technology deployment.

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [154] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph introduces a framework for real-time video classification on embedded devices using efficient, specialized adapters to improve performance and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: To address computational and energy limitations of embedded devices for real-time video classification by leveraging structural properties like label sparsity and continuity.

Method: A framework using Low Rank Adapters (LoRA) specialized for class subsets, dynamically activated per frame to achieve efficient inference and scalability.

Result: Polymorph reduces energy consumption by 40% and improves mAP by 9 points compared to baselines on the TAO dataset.

Conclusion: Polymorph offers an efficient and scalable solution for real-time multi-label video classification, balancing performance and energy demands.

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [155] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: The paper introduces CLIPTTA, a new method for test-time adaptation of vision-language models like CLIP, aiming to improve generalization under distribution shifts.


<details>
  <summary>Details</summary>
Motivation: Vision-language models often fail to generalize under distribution shifts, and existing test-time adaptation methods based on entropy minimization are misaligned with these models' contrastive training, leading to limitations and failure modes.

Method: The paper proposes CLIPTTA, a gradient-based test-time adaptation method using a soft contrastive loss aligned with CLIP's pretraining. It also introduces an Outlier Contrastive Exposure (OCE) loss for improved out-of-distribution detection.

Result: CLIPTTA shows consistent performance improvements over entropy-based objectives, achieving competitive or superior results compared to state-of-the-art methods across 75 datasets with diverse distribution shifts.

Conclusion: CLIPTTA effectively mitigates issues like pseudo-label drift and class collapse. It is robust, scalable, and improves both test-time adaptation and out-of-distribution detection for vision-language models.

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [156] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: The paper introduces an Attention Focusing (AF) mechanism to enhance Generalized Category Discovery (GCD) by reducing distractions from irrelevant background regions, leading to significantly improved performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the issue of 'distracted attention' in GCD, where models processing unlabeled data focus on irrelevant background regions, resulting in suboptimal feature extraction.

Method: The proposed method, AF, includes two components: Token Importance Measurement (TIME), which quantifies token importance across scales, and Token Adaptive Pruning (TAP), which prunes non-informative tokens using these importance scores, streamlining the model's focus.

Result: When integrated into the SimGCD method, AF enhances performance by up to 15.4% over the baseline, with minimal computational overhead.

Conclusion: AF is a lightweight, effective plug-and-play module that improves GCD methods by focusing attention and boosting performance. Its code is publicly available for further exploration.

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [157] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: The paper addresses issues in generative super-resolution (GSR) where generated details do not match the original image, introducing the "Hallucination Score" (HS) to evaluate these issues and proposing a method to mitigate them.


<details>
  <summary>Details</summary>
Motivation: GSR models currently produce high-quality images but fail to optimally balance quality and fidelity, leading to artifacts (hallucinations) that limit practical use.

Method: The study introduces a multimodal large language model (MLLM) to evaluate hallucinatory elements, creating a 'Hallucination Score' that aligns well with human evaluations. This score is used to analyze and mitigate such artifacts, employing deep feature distances as a reward function in model training.

Result: The Hallucination Score provides insights that complement existing image metrics and correlates strongly with human evaluations. Using these insights, the study identifies specific deep features that can help in mitigating hallucinations.

Conclusion: The proposed method improves the fidelity of GSR models, reducing hallucinations and enhancing their balance between quality and fidelity for better practical application.

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [158] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack is a semi-automated framework combining deep learning and optical flow to accurately track tissue motion in B-mode ultrasound videos, overcoming challenges like speckle noise and low edge contrast.


<details>
  <summary>Details</summary>
Motivation: Accurately tracking tissue motion in B-mode ultrasound is challenging due to factors like speckle noise, low edge contrast, and out-of-plane motion, yet it is crucial for clinical and research applications requiring tissue dynamics analysis.

Method: DUSTrack integrates deep learning and optical flow with a graphical user interface for training data generation and a novel optical-flow-based filtering technique to reduce noise while preserving tissue motion. It supports iterative model refinement for robustness.

Result: DUSTrack achieves superior accuracy over zero-shot point trackers and performs comparably to specialized methods. It demonstrates versatility in use cases like cardiac motion tracking, muscle deformation analysis, and fascicle tracking.

Conclusion: As an open-source solution, DUSTrack provides a flexible and powerful tool for point tracking in ultrasound videos, making it applicable across clinical, biomechanical, and sports science research areas.

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [159] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: This paper presents CRAFT, a neuro-symbolic framework for grounding affordances by combining commonsense knowledge, visual evidence, and iterative energy-based reasoning.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve both the accuracy and interpretability of affordance grounding in multi-object scenes where labels are not provided.

Method: The authors integrate data from ConceptNet, language models, and CLIP for visual evidence, using an energy-based model to iteratively refine decisions.

Result: CRAFT achieves higher accuracy in affordance grounding while offering improved transparency and goal-driven interpretability.

Conclusion: CRAFT represents progress toward robust and trustworthy scene understanding by combining symbolic reasoning and perceptual data effectively.

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [160] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: The paper proposes a framework for efficient 3D Gaussian splatting (3DGS) volumetric video streaming that optimizes compression and transmission.


<details>
  <summary>Details</summary>
Motivation: Traditional volumetric video streaming methods struggle with the large data volume and compression complexity of 3DGS videos, necessitating a specialized solution.

Method: The authors develop a 3DGS video construction method using Gaussian deformation fields, hybrid saliency tiling, and differentiated quality modeling to improve compression and adapt to bandwidth fluctuations.

Result: The proposed method outperforms existing approaches in terms of video quality, compression, and transmission rate in experimental evaluations.

Conclusion: The innovative system significantly enhances the efficiency and quality of 3DGS volumetric video streaming, demonstrating practical viability.

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [161] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: This paper introduces IRGPT, the first multi-modal large language model tailored for real-world infrared imagery, leveraging a novel InfraRed-Text Dataset (IR-TD) with over 260K authentic pairs and achieving state-of-the-art performance across 9 tasks.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of existing vision-language models struggling with infrared imagery due to scarce aligned text data and reliance on synthetic images that fail to capture the unique infrared features.

Method: The researchers constructed IRGPT using a large InfraRed-Text Dataset (IR-TD) created via a combination of LLM-generated descriptions and rule-based annotations of real infrared images. They also devised a bi-cross-modal curriculum transfer learning strategy for knowledge transfer from visible to infrared domains.

Result: IRGPT demonstrated superior performance across 9 evaluation tasks compared to other models, setting new benchmarks in recognition and grounding tasks for infrared imagery.

Conclusion: IRGPT showcases the potential of authentic datasets and curriculum transfer learning strategies in overcoming limitations of synthetic approaches and improving vision-language understanding for infrared imagery applications.

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [162] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: This paper introduces GPI-Net, a network for feature-based point cloud registration utilizing Gestalt principles for better local-global feature interaction. Experimental results highlight its superior performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: To address challenges in fusing local and global features for accurate point cloud registration due to feature redundancy and spatial complexity.

Method: The authors propose GPI-Net, which utilizes Gestalt principles with two key mechanisms: GFA block for geometric feature attention using self- and cross-attention, and DMG block for multi-granularity parallel feature interaction.

Result: GPI-Net demonstrates superior performance across various challenging tasks compared to existing methods, validating its effectiveness.

Conclusion: Gestalt principles are effective for improving local-global feature fusion, making GPI-Net a promising solution for high-quality point cloud registration.

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [163] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: This paper tackles key challenges in 3D Gaussian splatting video (3DGS) streaming, such as tiling, quality assessment, and bitrate adaptation, by proposing innovative approaches.


<details>
  <summary>Details</summary>
Motivation: To address the foundational challenges in adopting 3DGS streaming for immersive video experiences, where tiling, quality assessment, and bitrate adaptation are inadequately studied.

Method: The paper proposes (1) a saliency analysis-based adaptive tiling technique, (2) a new quality assessment framework that evaluates 3DGS degradation and rendered 2D quality, and (3) a meta-learning-based bitrate algorithm tailored to diverse network conditions.

Result: Experiments reveal that the proposed methods outperform existing state-of-the-art techniques in 3DGS video streaming performance.

Conclusion: The proposed comprehensive solutions for 3DGS video streaming effectively tackle challenges in tiling, quality assessment, and bitrate adaptation, pushing the field forward significantly.

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [164] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: The paper introduces GEMINUS, an end-to-end autonomous driving framework utilizing a mixture-of-experts design to improve performance in diverse traffic scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of single-mode planning methods which fail to handle diverse and complex traffic environments by developing a system capable of adaptive and robust driving.

Method: Proposes a mixture-of-experts framework, GEMINUS, consisting of a Global Expert (trained on the overall dataset), Scene-Adaptive Experts (trained on specialized subsets), and a Dual-aware Router to dynamically activate suitable expert modules.

Result: GEMINUS outperforms existing methods in the Bench2Drive benchmark, achieving top Driving Score and Success Rate, and demonstrates clear improvements over the single-expert baseline in multiple metrics.

Conclusion: The proposed framework effectively addresses the challenges of diverse driving environments and establishes a robust benchmark for autonomous driving, even with monocular vision input.

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [165] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard is a system developed to embed metadata links into images, enabling reliable data retrieval even after tampering.


<details>
  <summary>Details</summary>
Motivation: The loss of source code, interactivity, and metadata from raster visualization images distributed online prompted a need for a solution.

Method: VisGuard employs techniques like repetitive data tiling, invertible broadcasting, and anchor-based crop localization to embed metadata securely.

Result: Experiments showed VisGuard excels in metadata retrieval, embedding capacity, and resisting tampering and steganalysis.

Conclusion: VisGuard offers a robust system to preserve and protect visualization metadata during online distribution.

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [166] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: The paper introduces OptiCorNet, a sequence modeling framework for Visual Place Recognition (VPR), combining spatial and temporal feature extraction for robust performance in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Visual Place Recognition in dynamic and perceptually aliased environments is challenging due to limited use of temporal coherence in existing methods, necessitating a better approach for long-term localization.

Method: The proposed method, OptiCorNet, employs a 1D convolutional encoder and a novel Differentiable Sequence Delta (DSD) module that integrates temporal differencing with LSTM-based refinement. A quadruplet loss function is used for better alignment and separability.

Result: OptiCorNet demonstrates superior performance compared to state-of-the-art methods across several public VPR benchmarks, excelling under seasonal and viewpoint variations.

Conclusion: OptiCorNet effectively learns sequence-level embeddings for Visual Place Recognition through end-to-end training, establishing a robust framework for challenging environments.

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [167] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: The paper introduces a new Data-Free Quantization (DFQ) approach for Vision Transformers (ViTs), focusing on improved synthetic sample quality and activation alignment, achieving performance competitive with real-data-based quantization.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome the performance degradation in ViTs quantized without access to real data, addressing poor synthetic data quality and differences in activation distributions between quantized and full-precision models.

Method: The authors propose DFQ-ViT, which synthesizes samples progressively by difficulty and introduces an activation correction matrix to align activations of quantized models with full-precision models during calibration and inference.

Result: DFQ-ViT demonstrates significant performance gains, such as a 4.29% improvement on DeiT-T with 3-bit weight quantization over state-of-the-art, and eliminates the need for fine-tuning.

Conclusion: DFQ-ViT reduces computational overhead and lowers deployment barriers for resource-limited environments, promoting Green Learning and more energy-efficient applications.

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [168] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: The paper introduces a point cloud completion framework that uses cross-modal retrieval to enhance structural feature learning, showing effectiveness and generalization in sparse and unseen data.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in point cloud completion when residual data lacks structural features, especially focusing on enhancing generation abilities beyond specific input classes.

Method: The framework incorporates a Structural Shared Feature Encoder (SSFE) and a Progressive Retrieval-Augmented Generator (PRAG) to utilize reference samples for structural priors and hierarchical feature fusion.

Result: Experiments demonstrate that the model generates detailed point clouds and generalizes well on sparse and unseen data across various datasets.

Conclusion: The proposed framework improves point cloud completion quality and generalizability by learning structural priors through cross-modal retrieval and hierarchical fusion mechanisms.

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [169] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: This paper introduces TCP-LLaVA, an architecture designed for efficient and accurate visual question answering (VQA) on large pathology images by employing token compression.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenges posed by large pathology images, such as excessive computational demands and existing methods' lack of generative capabilities for visual question answering (VQA).

Method: TCP-LLaVA employs trainable compression tokens and a modality compression module inspired by BERT's [CLS] token mechanism. These compressed tokens integrate visual and textual data and are fed to the language model for reduced computational cost.

Result: TCP-LLaVA showed superior VQA accuracy across experiments on ten TCGA tumor subtypes while significantly reducing training resource requirements compared to existing methods.

Conclusion: The token compression approach provided by TCP-LLaVA optimizes resource efficiency and enhances performance for visual question answering tasks on large pathology images, addressing shortcomings of prior methods.

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [170] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: The paper proposes a framework for motion segmentation and egomotion estimation using event-based normal flow for neuromorphic vision sensors, without requiring full optical flow computation.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional methods that rely on optical flow or explicit depth estimation when processing data from neuromorphic vision sensors.

Method: An optimization-based pipeline leveraging event over-segmentation, residual analysis for object motion isolation, and hierarchical clustering for refining segmentations based on motion similarity and temporal consistency.

Result: Achieved accurate segmentation and translational motion estimation on the EVIMO2v2 dataset, showing advantages at object boundaries without full optical flow computation.

Conclusion: The method offers a robust, scalable solution for real-time robotic and navigation applications using neuromorphic vision sensors.

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [171] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: This paper surveys feed-forward techniques for 3D reconstruction and view synthesis, categorizing methods by representation architectures and exploring applications and challenges.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for fast and generalizable techniques for 3D reconstruction and view synthesis, critical for AR, VR, robotics, and other real-world applications.

Method: The authors categorize existing methods by representation architectures such as point clouds, 3D Gaussian Splatting, and Neural Radiance Fields, reviewing key tasks and datasets.

Result: The paper highlights advancements in pose-free reconstruction, dynamic 3D modeling, and 3D-aware image synthesis, showcasing their impact across diverse applications such as SLAM and digital humans.

Conclusion: Feed-forward approaches are emphasized as promising for advancing 3D vision, while open challenges and future research directions are discussed.

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [172] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: This paper introduces Depth-Consistent Human Modeling (DCHM), a framework that improves pedestrian detection by ensuring depth consistency and precise point clouds from sparse multiview setups without relying on labeled annotations.


<details>
  <summary>Details</summary>
Motivation: Existing multiview pedestrian detection methods suffer from noise and low precision in human modeling, and those using 3D annotations fail to generalize well across diverse scenes.

Method: DCHM employs a superpixel-wise Gaussian Splatting approach for consistent depth estimation and multiview fusion, enabling accurate pedestrian modeling in global coordinates.

Result: The method produces precise point clouds for localization and outperforms previous state-of-the-art baselines in challenging, large-scale, and crowded scenarios.

Conclusion: DCHM represents a significant advancement in human modeling and pedestrian detection, eliminating the need for labeled annotations and handling challenging multiview segmentation effectively.

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [173] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: The paper introduces ArtiMuse, an advanced AI model for Image Aesthetics Assessment (IAA) that integrates both scoring and expert-level analysis, along with a new expert-annotated dataset (ArtiMuse-10K).


<details>
  <summary>Details</summary>
Motivation: Current IAA methods lack the combined ability of quantitative scoring and professional-level understanding, with existing MLLM-based approaches facing issues like modality bias and insufficient fine-grained attribute analysis.

Method: The authors propose the ArtiMuse model based on Multimodal Large Language Models (MLLMs) to tackle these issues and introduce the ArtiMuse-10K dataset, featuring 10,000 expert-annotated images with detailed attributes and scores.

Result: ArtiMuse demonstrates innovative capabilities in providing joint scoring and deep professional aesthetic understanding, supported by ArtiMuse-10K as a comprehensive dataset.

Conclusion: The paper aims to push forward advancements in the field of Image Aesthetics Assessment by addressing the current limitations and publicly releasing both the model and the dataset for broader research and application.

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [174] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: This paper proposes a browser extension to translate sign language into subtitles during video calls, using a large-scale dataset of ASL videos.


<details>
  <summary>Details</summary>
Motivation: To eliminate the communication barrier between deaf-mute individuals and others, especially during the pandemic where video calls have become a major mode of communication.

Method: Utilizing a large-scale dataset of over 2000 Word-Level ASL videos performed by 100+ signers to build a browser extension that translates sign language into subtitles.

Result: The proposed approach aims to make video meetings accessible for hearing-impaired individuals, ensuring seamless communication through sign language recognition.

Conclusion: Sign language recognition can bridge the gap in video call communication for hearing-impaired individuals, offering an inclusive solution for modern digital interactions.

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [175] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: The paper discusses using the Florence multimodal foundation model for gastrointestinal endoscopy visual question answering (VQA) in the ImageCLEFmed MEDVQA 2025 Challenge.


<details>
  <summary>Details</summary>
Motivation: To enhance visual question answering in the medical domain, particularly for gastrointestinal endoscopy, by leveraging advanced multimodal models.

Method: The authors fine-tuned the Florence model, incorporating domain-specific augmentations to interpret endoscopic images and provide clinically relevant answers.

Result: Experiments using the KASVIR dataset demonstrated that the fine-tuned model produced accurate responses according to official challenge metrics.

Conclusion: The study underscores the potential of multimodal models in advancing medical VQA, offering insights into their explainability, robustness, and integration, with code made publicly available for further research.

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [176] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: The paper explores the alignment between artificial neural networks (ANNs) and human perception, focusing on perceptual variability in emotion categorization through facial expression analysis.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling inter-individual differences in emotional perception when viewing the same stimuli.

Method: A perceptual boundary sampling method was employed to generate ambiguous facial expressions based on ANN decision boundaries. This data was validated through behavioral experiments to construct varEmotion dataset, aligning ANN predictions with human perceptual data.

Result: Finding ambiguous samples where ANN and human judgments align showed shared principles in emotion perception. Fine-tuned ANNs demonstrated improved alignment with both group and individual human perceptual patterns.

Conclusion: This study connects ANN decision boundaries with human perceptual variability and advances personalized modeling for emotional interpretation.

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [177] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: The paper introduces a system to help photographers identify and remove clutter in their photos, using algorithms for aesthetic evaluation and image reconstruction.


<details>
  <summary>Details</summary>
Motivation: Many photographers, especially amateurs, struggle with capturing clutter-free images due to lack of experience or awareness.

Method: The system employs a clutter distinguishment algorithm to assess object aesthetics and an iterative image inpainting algorithm with GANs to reconstruct missing areas after clutter removal.

Result: User studies showed that the system improves the ability to identify distracting elements and enables photography of better quality images with less effort.

Conclusion: This system effectively enhances photography experiences by assisting in clutter identification and removal, offering both technical precision and user-friendly features.

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [178] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: The paper introduces Descrip3D, a framework for understanding 3D scenes by encoding object relationships in natural language, outperforming baseline models across multiple tasks and datasets.


<details>
  <summary>Details</summary>
Motivation: Traditional 3D scene-language models struggle with relational understanding, especially when visual embeddings fail to capture object roles and interactions, highlighting the need for improved relational reasoning.

Method: Descrip3D represents objects with natural language descriptions that specify intrinsic attributes and contextual relationships, integrating these through embedding fusion and prompt-level injection for unified task reasoning.

Result: Descrip3D achieves superior performance across five benchmark datasets (ScanRefer, Multi3DRefer, ScanQA, SQA3D, Scan2Cap), surpassing baseline models in grounding, captioning, and question answering tasks.

Conclusion: Encoding relational information using natural language significantly enhances 3D scene understanding, allowing for effective multi-task performance in complex indoor environments without requiring specialized task heads.

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [179] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: LEAD predicts model transferability for vision tasks by modeling fine-tuning dynamics using a novel approach based on logits, effectively bypassing lengthy optimization processes.


<details>
  <summary>Details</summary>
Motivation: Choosing the most suitable pre-trained model for downstream tasks is challenging due to the proliferation of available models.

Method: LEAD introduces a fine-tuning-aligned approach using logits and an ordinary differential equation to model optimization dynamics, accompanied by a class-aware decomposition method.

Result: Experiments on 24 pre-trained models across 10 datasets demonstrate strong performance and adaptability, including in low-data scenarios.

Conclusion: LEAD effectively streamlines the model transferability prediction process with theoretical and practical improvements over existing approaches.

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [180] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: This paper compares generative models like GANs, diffusion models, and flow matching techniques for T1w-to-T2w MRI contrast synthesis, with GANs outperforming others in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: The motivation is to reduce MRI scan time and cost by computationally synthesizing missing image contrasts, while maintaining diagnostic quality.

Method: The study implements and benchmarks generative models (GANs, diffusion models, flow matching) for 2D MRI contrast translation across three public datasets of healthy adults.

Result: Pix2Pix (GAN-based model) demonstrates superior performance in structure fidelity, image quality, and computational efficiency over diffusion and flow matching methods.

Conclusion: Flow-based models underperform on smaller datasets and simpler tasks, while GANs are more effective for current data availability. The findings guide real-world applications and future research in MRI cross-modal image synthesis.

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [181] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: Medical imaging is enhanced using deep learning frameworks. This study compares TensorFlow, PyTorch, and JAX on BloodMNIST data, focusing on inference time and accuracy.


<details>
  <summary>Details</summary>
Motivation: Improving automation and precision in the classification of blood cell images using different deep learning frameworks.

Method: Analyzing the performance of TensorFlow, PyTorch, and JAX on resolving BloodMNIST dataset images, emphasizing size dependency and inference times.

Result: JAX and PyTorch demonstrated classification accuracy comparable to benchmarks, with performance varying due to image resolution and optimizations.

Conclusion: Variations in performance suggest framework-specific strengths, underscoring their applicability for medical image classification.

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [182] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D addresses 3D Open-Vocabulary Sub-concepts Discovery by merging unsupervised segmentation with weak open-vocabulary guidance, setting a new standard in the field.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for 3D semantic segmentation are limited to specific tasks or scene content, lacking flexibility to adapt to both user queries and scene representation.

Method: The paper introduces DiSCO-3D, which utilizes Neural Fields representations to integrate unsupervised segmentation with weak open-vocabulary inputs.

Result: DiSCO-3D demonstrates strong performance in discovering 3D sub-concepts and sets state-of-the-art results for tasks combining open-vocabulary and unsupervised segmentation.

Conclusion: The method effectively blends scene adaptability with user-driven semantic segmentation, offering a novel approach in 3D semantic understanding.

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [183] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: Facial expression recognition model 'Exp-Graph' uses graph-based modeling, achieving promising accuracies on key benchmarks.


<details>
  <summary>Details</summary>
Motivation: To improve human-computer interaction by enhancing the recognition of facial expressions using structural information from facial attributes.

Method: Facial attributes are represented as graphs using facial landmarks as vertices and edges, leveraging proximity and similarity, combined with a vision transformer and graph convolutional networks.

Result: Achieved high recognition accuracies: 98.09% (Oulu-CASIA), 79.01% (eNTERFACE05), 56.39% (AFEW). Demonstrates strong performance in both controlled and real-world settings.

Conclusion: Exp-Graph effectively captures local and global structural dependencies in facial attributes, proving its validity for facial expression recognition in practical applications.

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [184] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: This paper introduces DD-SAM2, an efficient fine-tuning framework for SAM2 designed specifically for medical video segmentation and tracking.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods for medical video segmentation face challenges in generalizability, adaptability, and computational expense due to modality-specific designs and training data requirements.

Method: DD-SAM2 leverages a Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature extraction in SAM2, enabling effective adaptation for medical video scenarios while utilizing SAM2's streaming memory mechanism.

Result: The proposed method demonstrated high segmentation performance on TrackRad2025 (Dice score: 0.93) and EchoNet-Dynamic (Dice score: 0.97) datasets for medical video segmentation and tracking.

Conclusion: This study pioneers the fine-tuning of SAM2 for medical video segmentation using adapter-based methods, offering an effective solution with minimal parameter overhead. The approach establishes a foundation for improved adaptability in future medical imaging research.

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [185] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: The paper presents BusterX++, a novel cross-modal framework that enhances synthetic media detection, and GenBuster++, a new benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current detection systems that fail to effectively detect synthetic media combining multiple formats, which has become a significant risk with generative AI advancements.

Method: The paper proposes BusterX++, a framework employing advanced reinforcement learning and multi-stage training techniques, along with a novel benchmark, GenBuster++, to ensure comprehensive evaluation.

Result: BusterX++ achieves stable and substantial improvements in cross-modal detection of synthetic media, as shown via extensive experiments.

Conclusion: The approach offers a more effective and generalizable method for detecting and explaining synthetic media, particularly across multiple modalities.

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [186] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: The paper introduces a novel multispectral fusion framework, MS2Fusion, to address limitations in object detection by leveraging a state-space model for efficient feature fusion via dual-path parametric interaction.


<details>
  <summary>Details</summary>
Motivation: The research aims to overcome two key challenges in multispectral feature fusion: (1) overdependence on local complementary features instead of shared semantics, which hampers generalization, and (2) scalability bottlenecks due to the trade-off between receptive field size and computational complexity.

Method: The method involves implementing a Multispectral State-Space Feature Fusion (MS2Fusion) framework using dual-path parametric interaction: one branch for cross-modal complementary feature mining (via cross-modal hidden-state decoding) and another for shared semantic alignment (via joint embedding with parameter sharing). Both paths are optimized within a unified framework.

Result: Experiments on multispectral benchmarks (FLIR, M3FD, LLVIP) demonstrate MS2Fusion's significant performance improvement over state-of-the-art methods. Additionally, it shows general applicability to other multispectral perception tasks like RGB-T semantic segmentation and salient object detection.

Conclusion: MS2Fusion effectively addresses multispectral fusion challenges by combining complementary functionality and shared semantic representations, achieving superior scalability, generalization, and applicability across various tasks.

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [187] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: The paper presents FST.ai, an AI-driven framework designed to improve real-time officiating in Sport Taekwondo, focusing on head kick detection and scoring using computer vision and deep learning.


<details>
  <summary>Details</summary>
Motivation: Traditional manual and IVR-supported systems in sports officiating face issues like latency, subjectivity, and inconsistency, necessitating a more robust and fair solution.

Method: The paper employs an AI framework combining computer vision, deep learning, and edge inference techniques to analyze pose estimation, motion classification, and impact analysis for decision-making.

Result: FST.ai significantly reduces decision time while increasing consistency and transparency in officiating, and demonstrates adaptability to various sports beyond Taekwondo.

Conclusion: FST.ai proves to be a scalable and sport-agnostic system capable of transforming officiating standards by enhancing fairness and reliability in decision-making across different modalities.

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [188] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: This study developed a computer vision framework to estimate food waste using semantic segmentation of plate images before and after consumption. Models achieved high accuracy, with some reaching 90% for key metrics, serving as a scalable solution for waste monitoring in institutional dining.


<details>
  <summary>Details</summary>
Motivation: The study addresses the need for quantifiable data on food waste in dining settings to aid sustainability efforts.

Method: Utilized semantic segmentation of RGB images with machine learning models (e.g., U-Net, U-Net++, lightweight versions) trained on a custom loss function and evaluated across multiple metrics.

Result: Models performed well across metrics, particularly for dishes with less complexity like rice and fries. Lighter models enabled faster inference suitable for real-time use.

Conclusion: The proposed framework is a scalable and contactless method for monitoring food consumption, laying groundwork for waste reduction strategies in institutional settings.

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [189] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: This paper introduces Gene-DML, a framework to predict gene expression from histopathology images more accurately using cross-modal representation alignment.


<details>
  <summary>Details</summary>
Motivation: To improve gene expression prediction from histopathology images by addressing underutilized cross-modal representation alignment in existing methods.

Method: The framework, Gene-DML, uses a Dual-pathway Multi-Level discrimination approach. It incorporates a multi-scale instance-level pathway for scale-aware morphological-transcriptional alignment and a cross-level instance-group pathway for structural consistency across modalities.

Result: Gene-DML achieves state-of-the-art performance in gene expression prediction, validated through experiments on public spatial transcriptomics datasets.

Conclusion: Gene-DML effectively enhances predictive accuracy and generalization in gene expression prediction, proving its potential for scalable molecular profiling in precision medicine.

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [190] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: This paper introduces Doc-750K, a high-quality dataset for document-level multimodal understanding, and a native multimodal model called Docopilot.


<details>
  <summary>Details</summary>
Motivation: To address the inadequacy of MLLMs in complex, multi-page document comprehension due to the lack of high-quality datasets and limitations of current RAG methods.

Method: The authors created Doc-750K, a dataset with diverse document structures and real question-answer pairs, and developed Docopilot, a model designed to handle document-level dependencies natively instead of relying on RAG.

Result: Docopilot outperforms existing methods in coherence, accuracy, and efficiency for document-level tasks and multi-turn interactions.

Conclusion: Docopilot sets a new standard for document-level multimodal understanding, providing better performance without relying on retrieval-based approaches.

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [191] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents is a multi-agent system that improves multi-modal whole slide image analysis by integrating specialized agents and robust mechanisms, outperforming current large language models.


<details>
  <summary>Details</summary>
Motivation: Whole slide images are pivotal for pathological tasks, but current multi-modal language models often lack the refinement needed for task-specific accuracy.

Method: Developed a collaborative system with functional agents addressing task allocation, verification, and summarization using model zoo techniques.

Result: WSI-Agents demonstrated superior accuracy and versatility across multi-modal WSI benchmarks compared to existing frameworks.

Conclusion: The novel system efficiently balances versatility with accuracy, paving ways to improve pathology-focused tasks.

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [192] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: The paper introduces a framework named MIPD to enhance generalization and zero-shot abilities of Grounded Situation Recognition models by distilling multimodal knowledge from large language models.


<details>
  <summary>Details</summary>
Motivation: The need to address the limitations of multimodal large language models in handling complex situations and resource constraints, along with improving traditional models' poor generalization to unseen or rare scenarios.

Method: A Multimodal Interactive Prompt Distillation (MIPD) framework that uses enriched rationales and prompt alignment to transfer knowledge from teacher MLLMs to student GSR models.

Result: Demonstrates improved performance for handling seen, rare, and unseen situations in Ov-SWiG dataset and enhanced unseen detection on HICO-DET dataset.

Conclusion: MIPD successfully bridges the gap between conventional GSR models and unseen/rare situation recognition, enhancing generalization and reducing prediction biases.

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [193] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: The paper introduces GTPBD, a fine-grained dataset for studying complex agricultural terraced parcels globally, addressing terrain diversity and usability for various tasks like semantic segmentation and parcel extraction.


<details>
  <summary>Details</summary>
Motivation: Existing studies on agricultural parcel extraction lack representation of complex terraced terrains, critical for precision agriculture and various land-use applications.

Method: The authors created the GTPBD dataset with manual annotations for more than 200,000 complex terraced parcels across seven geographic zones and global climactic regions, offering multi-level boundary labels and benchmarking it for several tasks.

Result: The dataset brings challenges in terrain diversity, irregular parcel objects, and varying domain styles. It was benchmarked on multiple methods for segmentation, edge detection, and domain adaptation.

Conclusion: GTPBD addresses a significant research gap in terraced remote sensing, supporting agricultural terrain analysis and cross-scenario applications effectively.

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [194] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: The study introduces MultiRetNet, a system using multimodal data (retinal imaging, socioeconomic factors, comorbidities) and a deferral system to improve diabetic retinopathy (DR) detection, focusing on underserved populations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address inequities in diabetic retinopathy detection among underserved populations, particularly those with limited access to screening and higher risks due to comorbid conditions.

Method: A new pipeline called MultiRetNet is proposed, combining retinal imaging, socioeconomic, and comorbidity data. The study evaluates three fusion methods for data integration and employs contrastive learning to manage challenging image quality and out-of-distribution samples through a clinician deferral system.

Result: Using the optimal fusion method (via a fully connected layer) and contrastive learning, MultiRetNet demonstrates improved DR staging accuracy, particularly on low-quality image inputs, while successfully identifying cases that require clinician review.

Conclusion: This methodology has the potential to enhance early DR detection for underserved communities, reduce healthcare costs, address disparities in care, and support healthcare equity through improved diagnostic accuracy and clinician involvement.

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [195] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: This paper presents the InterAct VideoQA dataset to enhance and evaluate Video Question Answering (VideoQA) models for traffic monitoring in intelligent transportation systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing VideoQA models, which struggle to handle the complexity of real-world traffic scenes involving spatiotemporal dynamics and concurrent events.

Method: The researchers introduce the InterAct VideoQA dataset, which contains 8 hours of varied traffic footage split into 10-second clips, paired with over 25,000 question-answer samples related to traffic-related scenarios.

Result: Evaluations reveal that state-of-the-art VideoQA models face difficulties with precise spatiotemporal reasoning in traffic videos but show improved performance when fine-tuned on the InterAct VideoQA dataset.

Conclusion: The paper concludes that domain-specific datasets like InterAct VideoQA are critical for advancing VideoQA models tailored for real-world applications in traffic monitoring and ITS.

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [196] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA improves VideoQA by refining question-option pairs using LLMs, enabling efficient focus on critical video segments and achieving SOTA performance on several benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation was to address limitations in existing VideoQA methods, such as task-agnostic sampling and heuristic retrieval, which fail to identify key events or capture causal-temporal reasoning.

Method: The paper introduces LeAdQA, which uses LLMs for query refinement and a temporal grounding model for segment selection. An adaptive fusion mechanism integrates cues for accurate answer generation.

Result: LeAdQA demonstrated SOTA performance on NExT-QA, IntentQA, and NExT-GQA tasks, improving video-question understanding and reasoning while remaining computationally efficient.

Conclusion: By combining causal-aware query refinement and fine-grained segment grounding, LeAdQA effectively addresses limitations of prior methods, enhancing VideoQA accuracy and efficiency.

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [197] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: The paper introduces FOCUS, a framework for interpreting Vision Transformers (ViTs) in hyperspectral imaging (HSI), addressing issues like attention collapse and computational challenges by introducing class-specific spectral prompts and a learnable [SINK] token.


<details>
  <summary>Details</summary>
Motivation: Interpreting ViTs in HSI is largely unexplored due to challenges in saliency methods and computational demands.

Method: FOCUS introduces class-specific spectral prompts and a learnable [SINK] token to guide attention, absorb noise, and enable efficient interpretability without modifying the backbone.

Result: FOCUS improves band-level IoU by 15%, reduces attention collapse by over 40%, and aligns saliency results with expert annotations.

Conclusion: FOCUS bridges the gap between black-box modeling and trustworthy decision-making in hyperspectral imaging with minimal parameter overhead and high-resolution interpretability.

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [198] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: The paper introduces Hybrid Pooling Downsampling (HPD), a method to replace traditional pooling in CNNs, improving segmentation tasks by preserving spatial information and increasing performance.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the loss of spatial information in traditional downsampling methods used in CNNs for semantic segmentation, which affects prediction accuracy.

Method: The proposed Hybrid Pooling Downsampling (HPD) uses MinMaxPooling to retain light/dark contrast and detail features in images, avoiding key spatial information loss.

Result: Experiments on CNN architectures with ACDC and Synapse datasets show that HPD improves segmentation performance, increasing the DSC coefficient by an average of 0.5%.

Conclusion: HPD demonstrates an efficient alternative for downsampling in semantic segmentation, improving feature retention and predictive accuracy.

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [199] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: This paper introduces a lightweight, efficient method for robot action prediction using the InstructPix2Pix model, achieving superior results with lower computational cost and fast inference.


<details>
  <summary>Details</summary>
Motivation: Current video prediction models are computationally expensive and slow, limiting their application in robotics and motion trajectory analysis. Developing a faster, more efficient approach is crucial.

Method: The study fine-tunes the InstructPix2Pix model to process both visual (image) and textual (instruction) inputs for multimodal future visual frame prediction in robotic tasks.

Result: Experiments on the RoboTWin dataset show that the proposed method outperforms state-of-the-art models in SSIM and PSNR metrics for robot action prediction, requiring only a single image and text prompt as input.

Conclusion: The proposed approach offers a fast, resource-efficient solution to robot action prediction, making it particularly useful for robotics and motion trajectory applications where precision is critical.

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [200] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: The paper introduces a parallelized ODE solver, Ensemble Parallel Direction (EPD), to reduce sampling latency in diffusion models without compromising image quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from high sampling latency due to sequential denoising processes, and existing acceleration methods lead to degraded image quality under low-latency conditions.

Method: The paper proposes EPD, a novel ODE solver that parallelizes multiple gradient evaluations to mitigate truncation errors while preserving low-latency sampling.

Result: EPD achieves significant performance improvements in image synthesis benchmarks, excelling at low latency settings. For example, at 5 NFE, it achieves an FID of 4.47 on CIFAR-10 and surpasses existing methods across various datasets.

Conclusion: EPD demonstrates its efficacy in generating high-quality images efficiently and can serve as a plugin to enhance existing samplers. It provides state-of-the-art results with minimal training overhead.

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [201] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: This paper evaluates state-of-the-art 3D reconstruction models DUSt3R, MASt3R, and VGGT on aerial images, showing their strengths in sparse image setups and limitations in high-resolution and large datasets.


<details>
  <summary>Details</summary>
Motivation: To explore the performance of recent transformer-based 3D reconstruction models in handling sparse photogrammetric aerial images, a domain that remains underexplored.

Method: The study conducts a comparative analysis of DUSt3R, MASt3R, and VGGT on the UseGeo dataset, focusing on pose estimation and dense 3D reconstruction from sparse aerial image sets.

Result: The methods accurately reconstruct dense point clouds with up to 50% completeness gains over COLMAP using very sparse image sets, but perform poorly with high-resolution images and large sets.

Conclusion: Transformer-based reconstruction methods cannot fully replace traditional techniques but can complement them in scenarios involving sparsity and low-resolution images.

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [202] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI is a novel framework aiming to enhance underwater images while addressing dataset bias, computational efficiency, and transparency challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the need for better underwater image enhancement techniques that address issues like dataset bias, high computational costs, and lack of transparency in AI models to support marine conservation efforts.

Method: EBA-AI employs CLIP embeddings to detect and mitigate dataset bias, uses adaptive processing to optimize energy efficiency, and incorporates uncertainty and explainability techniques to enhance trust.

Result: Experimental results on datasets LSUI400, Oceanex, and UIEB100 reveal that EBA-AI achieves computational efficiency with a minor PSNR drop, enabling real-time applicability for large-scale monitoring. It outperforms comparisons in efficiency, fairness, and interpretability.

Conclusion: EBA-AI delivers sustainable underwater image enhancement that is bias-aware, energy-efficient, and interpretable, advancing marine conservation efforts.

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [203] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: The paper proposes a prompt-based framework, VPIP, leveraging visual prompts for diverse low-level vision tasks. Experimental results highlight its scalability, strong generalization abilities, and adaptability.


<details>
  <summary>Details</summary>
Motivation: Unified modeling across a diverse range of low-level vision tasks remains challenging due to variations in task formulation and output domains.

Method: The VPIP framework incorporates a prompt encoder, prompt interaction module, and an end-to-end image processing backbone, utilizing input-target image pairs as visual prompts. The team evaluated GenLV, their model on over 100 tasks, analyzing scalability and adaptability.

Result: The method shows strong performance across multiple low-level vision tasks and demonstrates enhanced generalization for tasks with limited data when the number of training tasks increases.

Conclusion: VPIP effectively unifies low-level vision modeling, proving adaptable to zero-shot, few-shot, and task-specific scenarios, with strong scalability and transferable representation learning.

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [204] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: The paper introduces a new dataset, LASED, with one million images and a novel approach using steerable CNNs to improve UAV visual place recognition, especially addressing rotational ambiguity.


<details>
  <summary>Details</summary>
Motivation: Challenges in UAV navigation, such as dataset limitations and rotational ambiguity in aerial imagery, require better solutions for accurate localization.

Method: Develop a large-scale aerial dataset (LASED) and implement steerable CNNs to handle rotational variances in UAV imagery.

Result: LASED-trained models achieve higher recall rates compared to smaller datasets, and steerable CNNs outperform conventional architectures by 12% in recall.

Conclusion: Combining extensive datasets with rotation-equivariant networks improves robustness and generalization in aerial visual place recognition.

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [205] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: The paper proposes a novel human-inspired framework, HICOM, to detect multi-face deepfake videos, leveraging contextual cues like motion coherence and appearance compatibility.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods struggle in multi-face scenarios due to lack of contextual awareness.

Method: The study identifies human detection cues through experiments and introduces HICOM, a framework utilizing these cues for deepfake video detection.

Result: HICOM improves detection accuracy by 3.3% on in-dataset, 2.8% with perturbations, and 5.8% on unseen datasets compared to existing methods.

Conclusion: Human-inspired cues and HICOM's framework enhance deepfake detection in multi-face contexts, with improved accuracy and transparency.

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [206] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: The paper proposes a LiDAR-Visual odometry framework integrating LiDAR point clouds and images for accurate pose estimation, leveraging depth completion, multi-scale feature extraction with attention, and hierarchical pose refinement.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve odometry for autonomous systems by providing accurate and robust pose estimation that can handle dynamic environments and reduce errors in challenging regions.

Method: The method involves using a dense-depth map from point clouds and images, multi-scale feature extraction with attention mechanisms, and hierarchical pose refinement to optimize motion estimation progressively.

Result: The framework achieves similar or better accuracy and robustness compared to leading LiDAR and visual odometry methods, as shown by experiments on the KITTI odometry benchmark.

Conclusion: The approach offers an effective and advanced solution for odometry tasks, enhancing accuracy and robustness through innovative integration of LiDAR and visual data.

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [207] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: The paper introduces SegQuant, a unified quantization framework for diffusion models, to reduce computational cost while maintaining quality. It combines adaptive techniques for improved generalizability and performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are computationally intensive, making them challenging to deploy in environments with limited resources or strict latency requirements. Existing quantization methods are architecture-specific, limiting scalability and industrial use.

Method: The proposed SegQuant framework includes a segment-aware quantization strategy (SegLinear) for capturing structural semantics and spatial heterogeneity and a dual-scale quantization scheme (DualScale) to handle polarity-asymmetric activations. This enhances versatility and compatibility with deployment pipelines.

Result: SegQuant shows strong performance in reducing computational and model size requirements while retaining visual fidelity. It is applicable to various diffusion model architectures and compatible with mainstream deployment tools.

Conclusion: SegQuant effectively addresses the limitations of existing PTQ methods for diffusion models by providing a generalized, efficient, and practical solution for real-world deployment.

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [208] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: The paper introduces FinChart-Bench, a benchmark dataset designed for testing large vision-language models (LVLMs) on financial chart understanding.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of financial chart interpretation by LVLMs, which struggle with domain-specific terminology and complex temporal structures.

Method: Built a benchmark, FinChart-Bench, consisting of 1,200 annotated financial chart images and conducted a large-scale evaluation on 25 LVLMs using three question types—True/False, Multiple Choice, and Question Answering.

Result: Revealed a narrowing gap between open-source and closed-source models, degradation in upgraded models, poor instruction following by LVLMs, spatial reasoning deficiencies, and unreliability for automated evaluations.

Conclusion: LVLMs currently have significant limitations in understanding financial charts, necessitating further advancements in this domain. The dataset is publicly available for research.

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [209] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: The paper introduces Being-H0, a Vision-Language-Action (VLA) model trained on human videos to overcome data limitations in dexterous robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: To address poor generalization in VLAs caused by reliance on synthetic or limited teleoperated data, leveraging human dexterity present in web data.

Method: Combines VLA pretraining using human videos, physical space alignment for 3D reasoning, post-training adaptation for robots, and a novel motion tokenization method.

Result: Being-H0 demonstrates superior hand motion generation, effective instruction following, and notable scalability with data and model sizes.

Conclusion: Being-H0 improves real-world robotic manipulation significantly through its physical instruction tuning paradigm.

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [210] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: The paper addresses the issue of reduced performance in dehazing models when dealing with unseen real-world hazy images by proposing a domain adaptation method using the Physics-guided Haze Transfer Network (PHATNet).


<details>
  <summary>Details</summary>
Motivation: Existing dehazing models struggle with unseen real-world hazy images due to limited training data, prompting the need for a method to enhance performance during testing.

Method: The authors propose PHATNet, which transfers haze patterns from unseen target domains to source-domain haze-free images to generate domain-specific fine-tuning sets. It integrates two novel losses: Haze-Transfer-Consistency loss and Content-Leakage Loss to improve haze disentanglement.

Result: PHATNet significantly enhances dehazing model performance on benchmark real-world datasets compared to existing methods.

Conclusion: The proposed PHATNet provides a flexible domain adaptation framework that effectively improves dehazing on unseen real-world images by leveraging haze pattern transfer and novel consistency losses.

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [211] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: This paper addresses segmentation challenges in DBT images for breast cancer detection by proposing a paired image generation method using a diffusion guider, which generates images with corresponding annotations.


<details>
  <summary>Details</summary>
Motivation: The research aims to address the difficulty in annotating DBT images for mass lesion segmentation due to high-density breast tissue concealment and limited annotated data.

Method: The authors introduce a paired image generation method utilizing a conditional diffusion model and an added diffusion guider to produce paired images and annotations.

Result: Experimental results show improved image generation quality and successful integration of generated paired images and masks in supervised training for segmentation tasks.

Conclusion: The proposed method alleviates the annotated data shortage and enhances the performance of mass lesion segmentation in DBT images, demonstrating usability in downstream tasks.

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [212] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: This paper proposes a self-supervised depth completion method needing only sparse depth measurements and their corresponding image, eliminating the need for dense labels or multi-frame dependency.


<details>
  <summary>Details</summary>
Motivation: Traditional depth completion methods face challenges due to reliance on costly dense depth labels or restrictions in self-supervised setups requiring multi-frame dependencies.

Method: The authors developed loss functions leveraging depth distribution characteristics and utilized segmentation maps from vision foundation models for depth estimation.

Result: Extensive experiments prove the proposed method's effectiveness in overcoming the limitations of supervised and self-supervised approaches.

Conclusion: Their self-supervised depth completion framework simplifies training requirements, making it applicable to static or single-frame scenarios while improving depth estimation quality.

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [213] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: This paper introduces a unified video restoration framework leveraging foundation models to provide interpretable guidance without prior degradation knowledge, achieving state-of-the-art performance on new and standardized benchmarks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address video restoration in scenarios without prior knowledge of degradation and to standardize benchmarks in the area.

Method: The method utilizes foundation models to link degradation-aware semantic context of video frames to natural language, enabling interpretable guidance. It includes learning degradation approximation without relying on the foundation model during inference.

Result: The proposed method achieves state-of-the-art performance across all evaluated benchmarks, including both standardized and newly introduced datasets.

Conclusion: The approach demonstrates the potential of an interpretable and flexible video restoration framework, emphasizing the importance of standard benchmarks for consistent evaluation.

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [214] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: The paper presents a framework enhancing DETR-based object detectors by integrating uncertainty modeling and improving bounding box localization.


<details>
  <summary>Details</summary>
Motivation: Improve bounding box localization accuracy and incorporate prediction uncertainty for enhanced robustness of object detectors.

Method: Utilize multivariate Gaussian distributions for bounding boxes, incorporate Gromov-Wasserstein distance in the loss function, and derive Bayes Risk formulation for filtering high-risk information. Additionally, propose algorithms for quantifying localization uncertainty.

Result: The framework enhances DETR variants’ performance on COCO benchmark and achieves state-of-the-art results for leukocyte detection on specialized datasets.

Conclusion: The framework improves model robustness and accuracy in both general and domain-specific detection tasks, demonstrating scalability and effectiveness.

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [215] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: This paper proposes a novel hypergraph-enhanced Transformer model for recognizing human emotion states based on micro-gestures, achieving state-of-the-art results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the lack of comprehensive exploration into modeling human emotions through micro-gestures, an area gaining interest in behavior understanding and affective computing.

Method: The method introduces a hybrid-supervised framework using a hypergraph-enhanced Transformer with self-attention and temporal convolution modules for emotion recognition and motion reconstruction.

Result: The proposed model achieves superior performance across multiple metrics on the iMiGUE and SMG datasets compared to existing methods.

Conclusion: This framework effectively captures micro-gesture motions and offers a state-of-the-art approach to emotion recognition, proving its robustness and accuracy on public datasets.

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [216] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: The paper introduces a non-learning-based approach to convert relative-scale depth predictions into metric-scale depth using sparse depth measurements, avoiding costly retraining or loss in model generalization.


<details>
  <summary>Details</summary>
Motivation: Current foundation models excel at depth prediction but are limited by producing outputs in relative scale rather than metric scale, hindering real-world application.

Method: A new method uses sparse depth measurements to adapt relative-scale predictions into metric-scale depth without relying on retraining or fine-tuning, preserving model performance and generalization.

Result: Experiments show the proposed adaptation approach is effective and maintains the models' strong generalization ability while bridging the gap to metric-scale depth.

Conclusion: The approach provides a cost-efficient and generalization-preserving solution to enable metric depth prediction for foundation models, unlocking their real-world applicability.

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [217] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: This paper presents BeatFormer, a lightweight and efficient hybrid model for remote photoplethysmography (rPPG) estimation, incorporating spectral attention and label-free training methods to overcome limitations of handcrafted and deep learning approaches.


<details>
  <summary>Details</summary>
Motivation: Handcrafted rPPG methods rely on physiological priors but struggle with complex conditions, whereas deep learning methods require large datasets for generalization. The paper aims to combine the strengths of both approaches to improve rPPG estimation.

Method: BeatFormer is introduced as a hybrid model combining zoomed orthonormal complex attention and frequency-domain energy measurement for efficient spectral attention. Spectral Contrastive Learning (SCL) is employed for label-free training.

Result: The proposed BeatFormer model demonstrates robust performance across multiple datasets (PURE, UBFC-rPPG, MMPD), especially under challenging motion scenarios and cross-dataset evaluations.

Conclusion: BeatFormer successfully integrates handcrafted and deep learning concepts to advance rPPG estimation, proving to be computationally efficient and robust in various scenarios.

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [218] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: The paper introduces a unified model that leverages a pre-trained 2D multi-modal network to process RGB images, text, and point clouds for 3D visual grounding, reducing complexity while improving performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to simplify and enhance 3D visual grounding models, which previously relied on complex architectures involving separate encoders for multiple modalities.

Method: The paper uses a 2D CLIP bi-modal model with adapter-based fine-tuning, introduces a Geometric-Aware 2D-3D Feature Recovery and Fusion module, incorporates a multi-modal decoder, and integrates features of all modalities for unified processing.

Result: The introduced method reduces trainable parameters by 58%, improves 3D detection by 6.52%, and enhances 3D visual grounding by 6.25% compared to the baseline.

Conclusion: The study proposes an efficient end-to-end solution for 3D visual grounding that simplifies feature extraction and fusion while delivering notable performance improvements.

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [219] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: This paper proposes a Semantic-Aware Representation Learning (SARL) method to enhance multi-label image classification using semantic-related features, optimal transport-based attention, and a regional score aggregation strategy.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in overcoming the noise in image representations and imprecise object localization in current multi-label image classification methods.

Method: SARL employs a three-part approach: a module for extracting semantic-related features, an optimal transport-based attention mechanism for aligned image representation, and a regional score aggregation strategy.

Result: Experimental results show that SARL outperforms existing methods on PASCAL VOC 2007 and MS-COCO datasets.

Conclusion: SARL effectively improves performance in multi-label image classification by introducing semantic-awareness into image representation and prediction strategies.

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [220] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: The paper introduces a framework for resource-efficient 3D Gaussian Splatting reconstruction using disentangled geometry and appearance prediction.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D Gaussian Splatting require heavy computational resources and data, while entangling geometry and appearance predictions slows performance.

Method: The proposed approach uses a stereo vision backbone and global attention fusion for feature extraction, along with separate prediction heads for geometry and appearance, combined as GS-maps. A refinement network enhances the reconstruction quality.

Result: The technique allows pose-free 3D reconstruction with reduced resource requirements, maintaining high-quality results compared to traditional methods.

Conclusion: The proposed framework offers an efficient, practical, and scalable alternative for 3D content generation, improving speed and robustness without dependency on camera parameters.

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [221] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: The paper presents a robust methodology for pose estimation and shift correction in cryo-EM imaging, addressing low SNR challenges using multi-dimensional scaling and a projected coordinate descent strategy.


<details>
  <summary>Details</summary>
Motivation: To improve pose estimation and shift correction in cryo-EM imaging where low signal-to-noise ratio significantly impacts 3D reconstruction fidelity.

Method: The approach employs multi-dimensional scaling (MDS) to estimate the 3D rotation matrix and introduces an optimization framework based on robust norms alongside a shift correction algorithm.

Result: The proposed pipeline delivers superior performance in Euler angle accuracy and reconstruction fidelity compared to existing methods, even in low-SNR conditions.

Conclusion: The paper provides a robust framework that addresses critical challenges in cryo-EM, yielding enhanced 3D reconstruction fidelity and improved pose estimation accuracy.

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [222] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: The paper proposes a probabilistic framework for MIL in medical imaging that models attention values probabilistically, yielding high predictive performance and interpretable uncertainty maps.


<details>
  <summary>Details</summary>
Motivation: Addressing scarcity of labeled data in medical imaging and improving attention mechanisms in MIL by incorporating uncertainty.

Method: Introducing a probabilistic approach to attention mechanisms that models attention values as distributions and captures both local and global interactions in medical imaging.

Result: Outperformed state-of-the-art baselines across three medical imaging datasets with improved metrics. Also generated interpretable uncertainty maps for illness localization.

Conclusion: The proposed probabilistic framework advances MIL by combining predictive accuracy and interpretability, with strong potential in medical imaging applications.

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [223] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: The paper tackles the challenge of Open-set Cross Modal Generalization (OSCMG), proposing a method (MICU) to improve multimodal learning in open-set environments, validated through experiments.


<details>
  <summary>Details</summary>
Motivation: Extend the realm of Cross Modal Generalization (CMG) to open-set environments to address real-world scenarios where unseen classes across new modalities coexist.

Method: Proposed a new methodology called MICU, which includes Fine-Coarse Masked multimodal InfoNCE (FCMI) for alignment and Cross modal Unified Jigsaw Puzzles (CUJP) for enhancing diversity and uncertainty.

Result: Extensive experiments validated the efficacy of the proposed approach (MICU) on both existing CMG tasks and the newly introduced OSCMG benchmark.

Conclusion: The proposed MICU framework effectively addresses the OSCMG challenge, enhancing multimodal generalization and handling unseen categories.

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [224] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: The paper addresses the challenge of low-overlap point cloud registration (PCR) using a deep learning-based classifier to reliably evaluate registration quality.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics such as Maximum Inlier Count become ineffective in low-overlap scenarios, necessitating a better approach to evaluate registration quality in PCR tasks.

Method: A deep learning-based classifier is developed using a dataset constructed from 3DMatch to assess registration quality. This classifier is integrated into standard PCR pipelines.

Result: When integrated, the classifier improved state-of-the-art methods like GeoTransformer to achieve 86.97% registration recall on 3DLoMatch and demonstrated strong generalization on the ETH dataset.

Conclusion: The proposed deep learning approach sets a new benchmark in low-overlap PCR, enhancing existing pipelines and showing robust generalization.

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [225] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: The paper introduces HiCroPL, a framework designed to overcome generalization challenges in adapting pre-trained vision-language models (VLMs) like CLIP to downstream tasks. It achieves this by enabling bidirectional knowledge flow between text and vision modalities.


<details>
  <summary>Details</summary>
Motivation: While pre-trained VLMs like CLIP perform well on general tasks, their generalization capabilities are difficult to maintain when adapting to specific downstream tasks. Current methods like prompt learning face issues such as modality isolation and semantic decay.

Method: HiCroPL uses a bidirectional hierarchical structure where text prompts refine visual signals in early layers, and visual prompts refine text details in later layers. A hierarchical knowledge mapper ensures multi-scale semantic fusion, while a lightweight knowledge proxy boosts cross-modal interactions.

Result: HiCroPL achieved state-of-the-art performances across 11 benchmarks and demonstrated superior results in four task evaluations.

Conclusion: HiCroPL effectively addresses generalization challenges of VLMs by ensuring mutual semantic refinement across modalities, resulting in superior adaptability and task performance.

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [226] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: Current Multimodal Large Language Models (MLLMs) fail to outperform image-only models in regression tasks due to limitations in prompt design and output vocabularies. This paper introduces a bin-based approach and emphasizes the role of data-specific semantic prompts to improve performance.


<details>
  <summary>Details</summary>
Motivation: Address limitations in Multimodal Large Language Models where current methods fail to leverage textual input effectively for regression tasks.

Method: Proposes Regression via Transformer-Based Classification (RvTC), a bin-based approach eliminating manual vocabulary crafting. Introduces data-specific prompts containing semantic context to improve cross-modal understanding.

Result: Reaches state-of-the-art performance on four image assessment datasets. Improved correlation from 0.83 to 0.90 on the AVA dataset by using data-specific prompts.

Conclusion: MLLMs show enhanced performance in multimodal tasks when paired with meaningful textual context rather than generic prompts. Semantic input is vital for leveraging cross-modal capabilities.

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [227] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: The paper proposes an axis-aligned geometric constraint and metric for document dewarping, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage the inherent geometric properties of documents to improve dewarping algorithms by addressing limitations in current learning-based methods.

Method: During training, an axis-aligned geometric constraint is introduced. Preprocessing involves axis alignment to ease dewarping, and a new evaluation metric (AAD) is established for better robustness and alignment with human perception.

Result: The proposed method outperforms existing benchmarks and improves the AAD metric performance by 18.2% to 34.5%.

Conclusion: The study enhances document dewarping processes by combining geometric principles with innovative methods and achieves superior performance on various benchmarks.

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [228] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: This paper introduces a refinement method using B-Spline curve fitting to improve edge quality in FastSAM while retaining real-time processing capabilities.


<details>
  <summary>Details</summary>
Motivation: FastSAM enables real-time segmentation but suffers from jagged object edges reducing accuracy and visual quality.

Method: The proposed method applies a four-stage refinement process, employing two rounds of B-Spline fitting to smooth jagged edges.

Result: The B-Spline refinement significantly enhances edge quality, improves segmentation accuracy, and maintains real-time processing.

Conclusion: The technique broadens FastSAM's application potential in industries requiring precise and efficient edge recognition, such as medical imaging and automation.

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [229] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: This paper introduces the Video Thinking Test (Video-TT) as a benchmark to assess video large language models' ability to interpret videos with correctness and robustness, revealing a significant gap compared to human performance.


<details>
  <summary>Details</summary>
Motivation: To address the inadequacy of existing benchmarks in evaluating how video large language models (video LLMs) interpret real-world videos with the same correctness and robustness as humans.

Method: The authors developed Video-TT, which consists of 1,000 YouTube Shorts videos paired with one open-ended question and four adversarial questions to test models on visual and narrative complexity as well as robustness.

Result: Evaluation using Video-TT demonstrates that video LLMs perform significantly worse than humans in interpreting and understanding videos.

Conclusion: Video-TT reveals a substantial gap in video large language models' ability to achieve human-level performance in interpreting complex visual and narrative contexts, emphasizing the need for improved models.

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [230] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: This paper introduces OpenBreastUS, a large-scale dataset designed to advance the deployment of neural operator solvers in real-world medical imaging, particularly ultrasound computed tomography.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations of existing numerical PDE solvers and neural operators, which struggle with computational inefficiency and insufficient real-world applicability in realistic imaging problems.

Method: The authors created OpenBreastUS, a dataset with 8,000 human breast phantoms and over 16 million wave simulations, allowing comprehensive testing of neural operators for forward simulation and inverse imaging tasks.

Result: OpenBreastUS enables analysis of neural operators for scalability and generalization in wave simulations and demonstrates efficient in vivo breast imaging using neural operators.

Conclusion: The dataset fosters the development and deployment of improved neural PDE solvers in practical medical imaging scenarios, bridging theoretical equations with real-world applications.

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [231] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: The paper introduces OmniVTON, a training-free Image-based Virtual Try-On framework that works with both in-shop and in-the-wild scenarios, ensuring high texture fidelity and pose consistency.


<details>
  <summary>Details</summary>
Motivation: Current VTON approaches face a tradeoff: supervised methods offer high fidelity but poor cross-domain adaptability, while unsupervised methods improve adaptability at the cost of universality and bias. A unified framework is needed to address both challenges.

Method: OmniVTON uses a garment prior generation method for aligning clothes, a continuous boundary stitching technique for preserving details, and DDIM inversion for capturing structural cues and ensuring pose alignment.

Result: The proposed framework demonstrates superior performance across datasets, garment types, and scenarios. It is the first to enable multi-human VTON within a single scene.

Conclusion: OmniVTON bypasses traditional training requirements and overcomes the biases in diffusion models by disentangling garment and pose constraints, offering a universal, high-performing VTON solution for diverse applications.

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [232] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: The paper introduces PanTiny, a lightweight, efficient, and robust framework for pan-sharpening, emphasizing generalization and performance.


<details>
  <summary>Details</summary>
Motivation: The current pan-sharpening models are too large and dataset-specific, leading to computational inefficiencies and poor generalization on full-resolution data.

Method: PanTiny introduces a lightweight model trained on three diverse satellite datasets (WV2, WV3, GF2) using a novel training paradigm and a composite loss function.

Result: PanTiny achieves superior performance-to-efficiency balance compared to larger, specialized models, with enhanced generalization and simplified deployment.

Conclusion: The research demonstrates that thoughtfully designed models with robust training approaches outperform resource-heavy scaling, advocating for efficient and generalizable solutions for pan-sharpening.

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [233] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: Introduction of StableAnimator++, a video diffusion model for identity-preserving human animation with advanced pose alignment and face optimization techniques.


<details>
  <summary>Details</summary>
Motivation: Current human animation models struggle with maintaining identity consistency when there are significant differences between reference images and driving videos.

Method: The paper presents StableAnimator++, a model with learnable pose alignment, image and face embedding refinement, and Hamilton-Jacobi-Bellman-based face optimization integrated into the diffusion process.

Result: StableAnimator++ achieves high-quality video generation with improved identity consistency, as validated by qualitative and quantitative experiments on benchmarks.

Conclusion: StableAnimator++ effectively addresses identity preservation challenges in human animation, offering advancements in pose alignment and facial fidelity.

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [234] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: The paper assesses the capabilities of state-of-the-art generative models in text image generation and editing, focusing on tasks within OCR. It identifies key weaknesses and underscores the need for integrating these tasks into general-domain models.


<details>
  <summary>Details</summary>
Motivation: The researchers aim to understand whether state-of-the-art generative models can effectively handle the intricacies involved in text image generation and editing.

Method: They evaluate six generative models across closed-source and open-source domains using tailored high-quality image inputs and prompts. Additionally, they assess the models with 33 representative OCR tasks categorized into five segments.

Result: Key observations highlight weaknesses in the current generative models, showing limitations in handling OCR-related tasks effectively.

Conclusion: The integration of photorealistic text image generation and editing into general-domain generative models is crucial. The paper advocates for moving beyond specialized solutions to broader, unified capabilities in generative models.

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [235] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: The study introduces both a new dataset, BleedOrigin-Bench, and a novel AI framework, BleedOrigin-Net, to address challenges in bleeding source detection and tracking during Endoscopic Submucosal Dissection (ESD).


<details>
  <summary>Details</summary>
Motivation: Current AI technologies are inadequate for detecting and tracking bleeding sources during ESD due to visual obstructions, dynamic scenes, and a lack of specialized datasets.

Method: The researchers built BleedOrigin-Bench, a dataset containing annotated and pseudo-labeled ESD frames, alongside BleedOrigin-Net, a dual-stage AI framework for detection and tracking of bleeding sources.

Result: The proposed method demonstrated high performance, with 96.85% frame-level accuracy for onset detection, 70.24% pixel-level accuracy for initial detection, and 96.11% pixel-level accuracy for tracking.

Conclusion: BleedOrigin-Bench and BleedOrigin-Net significantly advance the field by providing state-of-the-art tools for bleeding source localization and tracking during ESD, addressing a critical clinical challenge.

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [236] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: This paper introduces LoopNet, a multitasking and efficient ResNet-based method for improving loop closure detection accuracy on embedded hardware in SLAM systems, supported by a new loop closure benchmarking dataset, LoopDB.


<details>
  <summary>Details</summary>
Motivation: To address two critical challenges in SLAM systems: improving loop closure detection accuracy and ensuring real-time computation on embedded hardware.

Method: Utilizes a modified ResNet architecture with multitasking capabilities, online retraining using few-shot learning, DISK keypoint descriptors, and is optimized for embedded systems.

Result: LoopNet improves performance in loop closure detection under varying conditions compared to traditional approaches, and the authors also introduce a benchmark dataset, LoopDB.

Conclusion: LoopNet provides an efficient and accurate solution for SLAM loop closure detection, meeting real-time constraints on embedded systems and outperforming traditional techniques.

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [237] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: The paper introduces VideoPlan, a framework for improving long-horizon visual planning tasks. It tackles data scarcity through auxiliary tasks and models action sequences using multi-token prediction, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address challenges in long-horizon visual planning, such as data scarcity and the inefficiency of next-token prediction for capturing structured action spaces, in order to improve user assistance goals.

Method: The authors propose Auxiliary Task Augmentation to enhance model performance on video-based planning and Multi-token Prediction to better model structured action sequences. The method was tested on datasets like COIN, CrossTask, and Ego4D.

Result: VideoPlan outperformed existing methods with a 7.3% improvement in the COIN dataset and 3.4% in CrossTask for predicting future actions. It also demonstrated comparable performance to state-of-the-art models in the Ego4D task.

Conclusion: The proposed approach, VideoPlan, is a robust solution for long-horizon visual planning, demonstrating superior or comparable performance across multiple datasets. The inclusion of code ensures reproducibility.

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [238] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: Researchers propose a new spatiotemporal multigraph model to enhance event-based vision for object detection, achieving improved accuracy and efficiency compared to existing graph-based methods.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to fully utilize the advantages of event-based sensors—high temporal resolution and low latency—by addressing limitations in spatiotemporal dynamics modeling of asynchronous data in downstream tasks.

Method: A novel spatiotemporal multigraph representation is developed, involving two decoupled graphs: a spatial graph using B-spline basis functions for global structure, and a temporal graph using motion vector-based attention for local dynamics. Efficient 2D kernels are utilized instead of resource-heavy 3D kernels.

Result: The model demonstrated over 6% higher detection accuracy compared to prior graph-based approaches on the Gen1 automotive and eTraM datasets. Additionally, it achieved a 5x speedup, decreased parameter count, and no additional computational cost.

Conclusion: Structured graph modeling improves spatiotemporal representation for asynchronous vision tasks, offering advantages in both accuracy and computational efficiency.

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [239] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: This paper introduces MeshMamba, a model based on Mamba State Space Models (Mamba-SSMs), for efficiently handling dense 3D articulated mesh models, achieving advancements in generation and reconstruction of detailed human meshes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations in current non-parametric human mesh recovery and generation models, particularly their inability to handle dense meshes with fine details like clothing and grasping hands.

Method: The paper utilizes Mamba State Space Models (Mamba-SSMs) with serialized ordering of mesh vertices, respecting articulated body structures. This forms the basis for MeshMamba, which powers two applications: MambaDiff3D for mesh generation and Mamba-HMR for human mesh recovery from single images.

Result: MeshMamba enables dense mesh generation and reconstruction with over 10,000 vertices. MambaDiff3D outperforms prior methods in generating detailed human shapes, while Mamba-HMR enhances recovery capabilities to include whole-body pose and features, achieving real-time performance.

Conclusion: MeshMamba and its applications demonstrate scalability and efficiency in generating and recovering detailed 3D human meshes, marking an advancement in articulated mesh modeling with potential for real-time applications.

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [240] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: The paper proposes N-JEPA, a novel self-supervised learning method that combines diffusion noise, a principle of generative models, with masked image modeling to improve feature learning for recognition tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance the representational capacity of self-supervised learning (SSL) models by connecting SSL with generative models, which excel in semantic understanding and data distribution approximation.

Method: Introduces N-JEPA, a model that integrates diffusion noise via masked tokens' position embedding. It also employs a multi-level noise schedule as feature augmentation to boost robustness.

Result: The study demonstrates that N-JEPA is effective in improving classification performance on downstream tasks through comprehensive experimentation.

Conclusion: The integration of diffusion noise into self-supervised learning enhances the model's robustness and achieves competitive performance in recognition tasks, paving the way for blending generative and discriminative approaches.

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [241] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: Proposes a hierarchical part-based method for 3D blood vessel modeling, excelling in modeling complex vascular structures.


<details>
  <summary>Details</summary>
Motivation: Blood vessels' intricate geometry and topology pose challenges in accurate 3D modeling for medical applications.

Method: Uses a three-stage framework: (1) generate a global key graph, (2) generate vessel segments conditioned on properties, (3) integrate local segments with the global structure.

Result: Outperforms existing approaches in modeling complex vascular networks when tested on real-world datasets.

Conclusion: Introduces a novel part-based hierarchical method for generating 3D vascular models, setting a new benchmark for the field.

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [242] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: This paper introduces a Sparse Autoencoder (SAE)-based interpretability approach for understanding a vision-language foundation model, Mammo-CLIP, trained for breast imaging.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve interpretability in high-stakes domains like breast imaging, where understanding model decisions is crucial for clinical adoption.

Method: The authors train a patch-level Sparse Autoencoder (SAE) on Mammo-CLIP to probe latent features linked to clinically relevant breast concepts and analyze their impact on foundation model decisions.

Result: The study finds that class-level latent neurons in the SAE often align with ground truth regions and uncover confounding factors impacting model decisions.

Conclusion: The paper demonstrates that interpretable SAE latent representations provide valuable insights into the internal workings of foundation models in breast imaging tasks.

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [243] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: The paper addresses the issue of overfitting in CD-FSL by introducing Coalescent Projection (CP) and a pseudo-class generation method, showing effectiveness on BSCD-FSL benchmarks.


<details>
  <summary>Details</summary>
Motivation: To tackle overfitting caused by updating too many transformer parameters in CD-FSL due to limited labeled samples and to prepare networks for unseen domains under extreme domain shifts.

Method: The paper introduces Coalescent Projection (CP) as an alternative to soft prompts and combines it with a pseudo-class generation method and Self-Supervised Transformations (SSTs) to enhance domain preparedness.

Result: Experiments demonstrate the method's effectiveness in handling extreme domain shifts on the BSCD-FSL benchmark.

Conclusion: The proposed approach provides a robust framework for CD-FSL scenarios by minimizing overfitting and improving cross-domain learning.

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [244] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: The paper introduces FreeCus, a training-free framework for text-to-image synthesis using diffusion transformers (DiT) that achieves state-of-the-art results by optimizing subject integrity and cross-modal alignment without requiring additional training.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image frameworks rely on training-intensive methods like per-subject optimization or specialized encoders, which limit practicality and fail to utilize the zero-shot potential of modern diffusion transformers.

Method: FreeCus incorporates three key innovations: a pivotal attention sharing mechanism for layout integrity, an upgraded variant of DiT for detailed feature extraction, and the integration of Multimodal Large Language Models (MLLMs) to enhance semantic representation.

Result: Experiments demonstrate that FreeCus achieves high-quality, consistent subject synthesis across varied contexts, performing comparably or better than training-based approaches, while also working seamlessly with existing inpainting and control pipelines.

Conclusion: FreeCus unlocks the zero-shot capabilities of diffusion transformers, presenting a training-free solution for subject-driven text-to-image generation, thereby expanding practical applications and improving user experiences.

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [245] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: This paper presents MinCD-Net, a lightweight module that enhances image-to-point-cloud (I2P) registration by mitigating limitations of differential PnP methods through a robust Chamfer distance-based approach.


<details>
  <summary>Details</summary>
Motivation: Existing differential perspective-n-point (PnP) methods for 2D-3D correspondence learning are overly sensitive to noise and outliers, which compromises their performance in I2P registration.

Method: The authors propose an approximated blind PnP approach, called MinCD-PnP, which reduces the problem to minimizing the Chamfer distance between learned 2D and 3D keypoints, and design MinCD-Net as a multi-task learning module for integration into existing I2P systems.

Result: Experimental evaluations on datasets like 7-Scenes, RGBD-V2, and ScanNet show that MinCD-Net outperforms state-of-the-art I2P registration methods in terms of inlier ratio (IR) and registration recall (RR), even in challenging cross-scene and cross-dataset scenarios.

Conclusion: MinCD-Net effectively addresses the sensitivity of differential PnP to noise and outliers, making it a practical and generalizable improvement for I2P registration across diverse datasets and settings.

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [246] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: The paper presents a video compression framework using conditional diffusion models for perceptually optimized video reconstruction, outperforming traditional and neural codecs under high compression.


<details>
  <summary>Details</summary>
Motivation: To leverage the strengths of conditional diffusion models in reconstructing videos aligned with human visual perception and improve video compression techniques.

Method: The approach introduces multi-granular conditioning, compact representations for transmission efficiency, and multi-condition training with modality dropout and role-aware embeddings.

Result: Extensive experiments demonstrate improved perceptual quality metrics such as FVD and LPIPS, particularly under high compression scenarios.

Conclusion: Using conditional diffusion models enhances perceptual video compression, offering significant improvements over existing codecs, especially in challenging compression settings.

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [247] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: The paper introduces an in-context learning framework using Vision Language Models (VLMs) to detect physical and digital presentation attacks in biometric systems, achieving competitive performance without extensive training.


<details>
  <summary>Details</summary>
Motivation: Biometric systems face evolving threats from advanced physical and digital attacks. Traditional deep learning defenses struggle with adaptation, data limitations, and privacy concerns.

Method: The study proposes leveraging Vision Language Models (VLMs) and designing an in-context learning framework for attack detection, systematically evaluating their performance in security-critical scenarios.

Result: Experimental evaluation on public datasets shows that the proposed VLM-based framework achieves competitive attack detection performance, surpassing some CNNs without requiring extensive data or training.

Conclusion: The proposed in-context learning framework using VLMs addresses challenges in biometric attack detection by offering strong generalisation capabilities with reduced resource demands.

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [248] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: This paper introduces DMD, a robust minutiae-anchored fingerprint representation method addressing diverse capture conditions while achieving state-of-the-art accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of fingerprint matching under varied capture conditions and improve robustness and accuracy in biometric recognition.

Method: The paper proposes DMD, a three-dimensional tensor representation derived from local patches around detected minutiae. It uses spatially structured descriptors, incorporating ridge textures and minutiae features, and foreground segmentation for matching efficiency.

Result: The proposed DMD method demonstrated robust performance and state-of-the-art accuracy across diverse fingerprint datasets including rolled, plain, partial, contactless, and latent fingerprints.

Conclusion: DMD achieves significant improvements in fingerprint matching accuracy and efficiency, proving effective for large-scale biometric recognition. Code for implementation is provided.

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [249] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: The paper introduces a novel module, Spatial-Channel State Space Modeling (SCSM), aimed at improving feature extraction in few-shot object detection by leveraging inter-channel correlations.


<details>
  <summary>Details</summary>
Motivation: Existing few-shot object detection methods face challenges in effectively identifying useful feature channels due to a limited number of training samples. High-weight channels may not always be effective, and low-weight channels can still hold valuable information.

Method: SCSM comprises two components: the Spatial Feature Modeling (SFM) module to balance spatial and channel relationships, and the Channel State Modeling (CSM) module, inspired by temporal-sequence modeling (Mamba), to capture inter-channel correlations.

Result: Experiments on VOC and COCO datasets demonstrate that the SCSM module enhances the quality of channel-wise feature representation and achieves state-of-the-art performance.

Conclusion: The SCSM module effectively addresses challenges in FSOD by modeling inter-channel correlations, improving feature extraction, and delivering superior detection outcomes.

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [250] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: This paper introduces BenchDepth, a benchmark designed to address limitations in evaluating depth foundation models (DFMs), emphasizing practical applications rather than alignment-dependent techniques.


<details>
  <summary>Details</summary>
Motivation: Depth estimation faces challenges in evaluation due to biases and inconsistencies in traditional benchmarking methods.

Method: The authors propose BenchDepth, which evaluates DFMs using five specific downstream tasks to assess real-world utility while bypassing conventional alignment-based metrics.

Result: Eight state-of-the-art DFMs were benchmarked, yielding an in-depth analysis of strengths and observations.

Conclusion: BenchDepth offers a novel pathway for evaluating DFMs, promoting discussion and innovation within the depth estimation community.

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [251] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: The paper introduces the ExDD framework for improving industrial defect detection, achieving superior performance with innovative modeling of dual feature distributions.


<details>
  <summary>Details</summary>
Motivation: Address limitations of one-class anomaly detection in industrial defect systems, such as uniform outlier assumptions and data scarcity.

Method: Use parallel memory banks for dual feature modeling and latent diffusion models with domain-specific textual conditioning to generate synthetic defects.

Result: Achieved 94.2% I-AUROC and 97.7% P-AUROC on KSDD2 dataset, and found optimal augmentation with 100 synthetic samples.

Conclusion: The ExDD framework effectively addresses key challenges in industrial defect detection, outperforming traditional systems in experimental validation.

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [252] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion improves pavement defect detection using synthetic anomaly generation and efficient dual-path feature adaptation.


<details>
  <summary>Details</summary>
Motivation: Challenges include limited annotated data, domain shifts between environments, and variability in defect appearances.

Method: Uses a latent diffusion model for synthesizing diverse defects, dual-feature adaptors for normal/anomalous inputs, and a lightweight discriminator for fine-grain defect detection.

Result: Achieves strong performance on six benchmark datasets, setting new state-of-the-art in classification and localization tasks.

Conclusion: RoadFusion is a robust solution for pavement defect detection, effectively addressing data scarcity, domain shift, and defect variability.

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [253] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: This paper showcases how synthetic high-fidelity datasets can train human-centric computer vision models more efficiently while retaining accuracy, bypassing the need for large datasets and compute-heavy processes.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art vision models require enormous datasets, expensive training, and compute-intensive inference. The authors aim to make these processes more efficient while tackling issues like fairness, provenance, and user consent.

Method: The researchers use high-fidelity synthetic datasets for training, which allow for perfect labels, data diversity control, and ethical guarantees. They train models on smaller datasets optimized for human-centric tasks.

Result: The resulting models perform three tasks—depth estimation, surface normal estimation, and soft foreground segmentation—on real images, achieving similar accuracy while being significantly more cost-effective.

Conclusion: Synthetic high-fidelity datasets can match the performance of large-scale models while improving efficiency and addressing ethical concerns. The research advances human-centric computer vision by reducing resource costs.

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [254] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: This paper introduces ORSANet for facial expression recognition (FER), addressing challenges like occlusion and dataset biases using multi-modal semantic guidance, adaptive fusion, and optimized loss function.


<details>
  <summary>Details</summary>
Motivation: Accurate FER is challenged by occlusion and biases in datasets, leading to difficulty in extracting effective features from partially visible faces.

Method: The authors employ semantic segmentation maps, facial landmarks, a Multi-scale Cross-interaction Module (MCM) for feature fusion, and a Dynamic Adversarial Repulsion Enhancement Loss (DARELoss).

Result: ORSANet demonstrates state-of-the-art performance on public benchmarks and the new Occlu-FER dataset.

Conclusion: ORSANet effectively improves FER robustness against occlusion and biases through multi-modal guidance, adaptive interaction, and enhanced loss functions, setting a new benchmark for performance in challenging scenarios.

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [255] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: The paper introduces SurgX, a framework to improve the interpretability of surgical phase recognition models by linking neurons to concepts and testing it on two models, making surgical AI systems more transparent.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for surgical phase recognition lack interpretability, making it difficult to understand, trust, or debug the decision-making processes.

Method: SurgX employs a concept-based approach to improve model interpretability by associating neural network neurons with relevant concepts via representative example sequences and tailored concept sets.

Result: Extensive experiments validated the SurgX method on two surgical phase recognition models, demonstrating improved explanations for their predictions.

Conclusion: SurgX effectively enhances interpretability for surgical phase recognition models, improving trust and understanding for these systems.

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [256] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: The paper introduces EgoPrune, a method to efficiently process egomotion videos by pruning redundant information.


<details>
  <summary>Details</summary>
Motivation: Egomotion videos inform embodied AI agents but are computationally expensive due to their redundant and continuously changing views. Current methods inadequately handle these challenges in egomotion contexts.

Method: EgoPrune integrates a keyframe selector, Perspective-Aware Redundancy Filtering (PARF), and a Maximal Marginal Relevance (MMR)-based selector to remove redundant and irrelevant information from video data without requiring additional training.

Result: EgoPrune outperforms existing training-free methods in terms of computational efficiency, achieving superior results on two egomotion video benchmarks while reducing computational overhead, such as FLOPs, memory, and latency.

Conclusion: EgoPrune demonstrates practical efficiency for egomotion video reasoning and is validated on a Jetson Orin NX edge device, establishing its real-world applicability for embodied AI scenarios.

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [257] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: This paper introduces RAda, a fine-tuning method for vision-language models (VLMs) like CLIP, which focuses on refining the fused representations critical for decision-making.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing fine-tuning techniques that primarily refine separate modality representations (text or vision) but overlook the importance of their fused representations for decision-making.

Method: The authors propose RAda, which uses a learned mask from an attention layer attached at the end of a VLM to dynamically adjust contributions in the fused rational matrix, without modifying intermediate features.

Result: RAda demonstrates superior performance across a variety of fine-tuning settings, requiring minimal implementation effort while competing with state-of-the-art methods.

Conclusion: RAda offers an effective and versatile fine-tuning method for VLMs, emphasizing fused representations and enhancing downstream task performance.

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [258] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: The paper discusses a crowd-sourced dataset created from high-resolution aerial imagery to assist in anomaly detection for manhunts and rescue operations in forests, highlighting current methods' shortcomings.


<details>
  <summary>Details</summary>
Motivation: Authorities failed to locate a suspect in a densely-vegetated forest despite a massive search effort, revealing limitations of automated anomaly detection in such conditions.

Method: A research aircraft captured aerial imagery of the forest, which was then analyzed via a crowd-sourced initiative to detect and label anomalies obscured by vegetation.

Result: The initiative yielded a unique dataset of hard-to-detect anomalies, and benchmarks showed that existing methods performed poorly in identifying such small clues under occluded conditions.

Conclusion: The dataset, provided openly with an interactive web interface, demonstrates the need for improved, context-aware methods for anomaly detection in complex forest environments.

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [259] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: The paper proposes UMIVR, an interactive text-to-video retrieval framework that reduces uncertainties (textual, mapping, and frame) through training-free uncertainty metrics, improving retrieval effectiveness.


<details>
  <summary>Details</summary>
Motivation: Text-to-video retrieval (TVR) faces challenges due to textual ambiguities, indistinct text-video mappings, and poor-quality video frames. Current interactive methods lack an explicit approach to quantify and address these uncertainties, motivating the need for a more effective framework.

Method: UMIVR introduces training-free metrics: Text Ambiguity Score (semantic entropy), Mapping Uncertainty Score (Jensen-Shannon divergence), and Temporal Quality-based Frame Sampler to quantify and tackle uncertainties. It uses targeted clarifying questions based on these metrics to iteratively refine user queries.

Result: UMIVR achieves notable performance gains in TVR, with Recall@1 reaching 69.2% after 10 rounds of interaction on the MSR-VTT-1k dataset, outperforming traditional approaches.

Conclusion: UMIVR effectively minimizes uncertainties in text-to-video retrieval through its novel framework, establishing a robust foundation for uncertainty-aware interactive retrieval systems.

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [260] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: The paper presents SAIGFormer, a novel Transformer-based framework for improving low-light image enhancement, especially for challenging non-uniform lighting scenarios.


<details>
  <summary>Details</summary>
Motivation: Transformer-based methods have advanced global illumination recovery but continue to struggle with non-uniform lighting conditions like shadows or backlighting.

Method: Introduced SAIGFormer with a dynamic integral image representation, a Spatially-Adaptive Integral Illumination Estimator (SAI²E), and an Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism.

Result: Extensive experiments show SAIGFormer's superior performance in non-uniform illumination restoration, achieving better results than existing state-of-the-art methods across various datasets.

Conclusion: SAIGFormer effectively enhances low-light images under complex lighting conditions, demonstrating both high accuracy and generalization across datasets.

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [261] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: This paper introduces a novel self-supervised learning framework to detect key procedural steps in unlabeled videos by addressing issues like order variations and redundant frames, leveraging a fused Gromov-Wasserstein formulation and contrastive regularization.


<details>
  <summary>Details</summary>
Motivation: Procedure learning from video poses challenges such as dealing with background noise, repeated actions, and varying temporal orders, for which a more robust method is needed.

Method: The proposed framework uses a fused Gromov-Wasserstein optimal transport model with structural priors to align video frames and incorporates contrastive regularization to avoid degenerate solutions in embedding space.

Result: Experiments on egocentric and third-person video datasets demonstrate that the proposed method significantly outperforms prior approaches, including the traditional Kantorovich-based model.

Conclusion: The fused Gromov-Wasserstein approach combined with contrastive regularization offers a superior solution for self-supervised video procedure learning, overcoming major limitations of previous methods.

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [262] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: This paper introduces a novel method and dataset for better understanding surgical scenes using graph-based representations.


<details>
  <summary>Details</summary>
Motivation: Current surgical scene understanding systems lack comprehensive representation of critical details such as tool-action-target combinations and hand identity.

Method: The authors propose Endoscapes-SG201 dataset for annotations and SSG-Com, a graph-based approach to incorporate critical surgical scene elements.

Result: Experimental results on downstream tasks such as safety assessment and action triplet recognition show improved understanding with the proposed method.

Conclusion: Integrating critical scene graph components significantly enhances surgical scene understanding, and the tools and data are openly available for further research.

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [263] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: The paper presents HOLa, a method for zero-shot Human-Object Interaction (HOI) detection, improving generalization to unseen actions and action differentiation using Vision-Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: Generalizing to unseen actions in zero-shot HOI detection is challenging due to difficulties in distinguishing actions for the same object and limited effectiveness of current approaches.

Method: HOLa decomposes VLM text features into low-rank components to create compact representations, enhances action distinction with human-object tokens, and applies action regularization using LLM guidance.

Result: HOLa achieves a state-of-the-art performance with an unseen-class mAP of 27.91 on HICO-DET, particularly excelling in unseen-verb settings.

Conclusion: HOLa demonstrates improved generalization and action distinction in zero-shot HOI detection, offering a compact yet effective framework for working with unseen interactions.

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [264] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: The paper introduces the Dynamic-Image (DynImg) method to improve video understanding by integrating temporal prompts to highlight spatial features of fast-moving objects and using a 4D video Rotary Position Embedding.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges in integrating temporal information in video understanding, particularly in scenarios with motion blur and rapid object movement, which traditional methods struggle to capture.

Method: The authors propose using non-key frames as temporal prompts to guide spatial feature extraction and adopting a 4D video Rotary Position Embedding to preserve spatio-temporal order within this representation.

Result: The Dynamic-Image method improves video understanding benchmarks by approximately 2% compared to state-of-the-art approaches.

Conclusion: Integrating temporal prompts with spatial features effectively enhances spatio-temporal interaction, offering a significant advancement in video understanding tasks.

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [265] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix is a learned interpolation method for data augmentation in image classification, addressing issues within traditional Mixup strategies by using class-conditional GANs for more realistic image synthesis.


<details>
  <summary>Details</summary>
Motivation: Traditional Mixup often produces unrealistic images which can hinder performance in sensitive domains, especially medical applications like COVID-19 detection.

Method: GeMix uses a StyleGAN2-ADA generator trained on the target dataset, followed by sampling and blending of label vectors via Dirichlet priors and Beta coefficients to synthesize visually coherent images conditioned on soft labels.

Result: GeMix improves the macro-F1 score of COVID-19 detection across multiple backbones (ResNet-50, ResNet-101, EfficientNet-B0) while reducing false-negative rates.

Conclusion: GeMix offers stronger regularization and higher semantic fidelity compared to traditional mixup, serving as a drop-in replacement for existing pipelines without disruption.

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [266] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: Onboard change detection framework for satellites using deep learning integrates image compression, co-registration, and change detection under strict complexity constraints.


<details>
  <summary>Details</summary>
Motivation: The delay in traditional satellite image change detection hampers real- or near real-time applications due to data downlink and ground processing latencies.

Method: The paper introduces a deep neural network-based solution with three submodules handling image compression, lightweight registration, and computationally effective change detection for satellite onboard processing.

Result: The proposed system achieves highly accurate change detection with an F1 score tied to compression rates, maintaining a processing speed of 0.7 Mpixel/s on a low-power (15W) accelerator.

Conclusion: The proposed end-to-end framework effectively performs onboard satellite image change detection, demonstrating both efficiency and strong performance under low-power constraints.

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [267] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: The paper introduces SegDT, a diffusion transformer-based model for skin lesion segmentation in medical images, achieving state-of-the-art performance while being efficient and low-cost.


<details>
  <summary>Details</summary>
Motivation: Skin lesion segmentation is critical for diagnosing and monitoring skin cancer, but existing methods need improvements in accuracy, speed, and hardware compatibility.

Method: The SegDT model uses a diffusion transformer (DiT) and incorporates Rectified Flow to enhance quality with reduced inference steps. It is designed to operate efficiently on low-cost hardware.

Result: SegDT achieved state-of-the-art performance on three benchmark datasets for skin lesion segmentation, offering faster inference speeds compared to existing models.

Conclusion: SegDT improves segmentation accuracy and processing speed while being suited for practical medical applications. It contributes to advancing deep learning in medical image analysis and facilitates better diagnostic tools.

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [268] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: This paper introduces a hybrid method combining Signed Distance Function (SDF) and 3D Gaussian Splatting (3DGS) for improved surface reconstruction and novel view rendering.


<details>
  <summary>Details</summary>
Motivation: Current methods face challenges in accurate surface reconstruction and novel view rendering due to limitations in SDF (poor detail recovery) and 3DGS (lacking global geometry coherence).

Method: The approach combines SDF to capture coarse geometry and 3DGS-derived images to refine SDF for enhanced accurate surface reconstruction and rendering.

Result: The proposed method achieves superior performance compared to state-of-the-art methods on surface reconstruction and novel view synthesis tasks using the DTU and MobileBrick datasets.

Conclusion: A hybrid of SDF and 3DGS mechanisms offers significant improvements in accuracy and detail recovery, setting a new benchmark for surface reconstruction and novel view rendering.

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [269] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: The paper introduces the CylinderPlane, a new representation using a cylindrical coordinate system to improve 360° image generation.


<details>
  <summary>Details</summary>
Motivation: Existing Tri-plane representations suffer from feature entanglement, causing issues like multi-face artifacts and poor 360° view consistency.

Method: The authors propose a cylindrical coordinate-based representation that separates features by angles, enabling artifact-free synthesis. They also introduce nested cylinders for handling complex geometry and varying resolutions.

Result: Experiments showed that the CylinderPlane outperforms prior methods on synthetic and real-world datasets, achieving better image quality and detail.

Conclusion: CylinderPlane effectively resolves feature ambiguity in 360° image generation and can be integrated into neural rendering pipelines easily.

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [270] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: This survey reviews techniques to improve the efficiency of deep neural networks in video analytics, discussing hardware, data, and deployment aspects, and highlights challenges.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of video data demands an improved balance between accuracy and efficiency in video analytics, especially using deep neural networks.

Method: The paper conducts a comprehensive review of optimization techniques for enhancing DNN efficiency, organized across hardware, data processing, and deployment perspectives.

Result: It categorizes methods and analyzes challenges in performance optimization for video analytics using DNNs.

Conclusion: The study outlines existing approaches to DNN efficiency optimization in video analytics, emphasizing the need for further advancements to address identified challenges.

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [271] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: This paper explores Active and Sequential Learning for object detection in medieval music manuscripts using a data-efficient approach that leverages YOLOv8 for iterative labeling and retraining.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations posed by data scarcity and the complexity of historical manuscripts in Optical Music Recognition, enabling efficient digitization of cultural heritage.

Method: The approach uses YOLOv8 for object detection and layout recognition, applying Active Learning (uncertainty sampling for iterative labeling) and Sequential Learning to minimize the need for manual annotations.

Result: Experimental results show comparable accuracy to fully supervised training using significantly fewer labeled examples, though uncertainty-based Active Learning was ineffective in the specific dataset tested.

Conclusion: The study highlights the potential of Active Learning and Sequential Learning in data-scarce scenarios but calls for alternative methods due to limitations observed in the tested dataset.

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [272] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: The paper explores the application of the Lottery Ticket Hypothesis for efficient deepfake detection, showing that pruned subnetworks maintain performance despite high sparsity levels.


<details>
  <summary>Details</summary>
Motivation: Deepfake technology poses threats to information integrity and social trust, necessitating efficient detection methods suitable for resource-limited environments.

Method: Uses the Lottery Ticket Hypothesis to prune neural networks while assessing their deepfake detection accuracy across architectures and datasets.

Result: Deepfake detection networks retain high accuracy at substantial sparsity, with methods outperforming one-shot pruning. Winning tickets are transferable across datasets.

Conclusion: Pruned neural networks using LTH offer promising potential for efficient and deployable deepfake detection systems.

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [273] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: The paper introduces EVA, a training-free method to reduce object hallucinations in multimodal large language models by dynamically selecting intermediate layers with significant visual factual information.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models exhibit shortcomings, such as object hallucinations, where non-existent or inaccurate visual objects are described due to suppression of visual information by prior knowledge.

Method: EVA dynamically selects intermediate layers rich in visual factual information, contrasts original input with pure-text input outputs, and modifies final layer logits to enhance factual accuracy without requiring model retraining.

Result: EVA significantly reduces hallucination rates on standard benchmarks compared to baseline methods, demonstrating its efficacy in improving multimodal outputs.

Conclusion: EVA is a simple, effective, training-free solution for correcting hallucinations in multimodal large language models. It is model-agnostic and integrates seamlessly into existing workflows.

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [274] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: This paper introduces HW-MLVQA, a benchmark for multilingual handwritten visual question answering, aiming to improve understanding of handwritten documents across diverse languages.


<details>
  <summary>Details</summary>
Motivation: Existing MLVQA models struggle with multilingual handwritten document comprehension, limiting their potential in capturing subtle linguistic and visual elements.

Method: Developed HW-MLVQA, a benchmark containing 1,600 pages and 2,400 Q&A pairs, with three evaluation modalities: text, image, and integrated image & text. It also assesses different OCR models without using ground truth transcriptions.

Result: HW-MLVQA enables evaluations of MLVQA models in realistic contexts, supporting advancements in multilingual handwritten document analysis.

Conclusion: This benchmark provides a pathway for improving handwritten document visual question answering, encouraging innovation in understanding diverse linguistic and visual intricacies.

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [275] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: This paper introduces a knowledge distillation method leveraging CLIP's vision-language capabilities to enhance Image Quality Assessment (IQA). It reduces model complexity and improves IQA performance.


<details>
  <summary>Details</summary>
Motivation: CLIP-based multimodal models excel in IQA tasks but face challenges like excessive computational demands and limitations in recognizing localized distortions.

Method: The approach involves designing quality-graded prompt templates for CLIP, fine-tuning CLIP for IQA tasks, and proposing a modality-adaptive knowledge distillation strategy from CLIP to a lighter student model.

Result: Experiments on IQA datasets reveal substantial reductions in model complexity and superior performance compared to existing IQA methods.

Conclusion: The framework holds strong potential for deployment in practical scenarios, offering efficient and effective IQA solutions.

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [276] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: The paper proposes a visual relocalization framework for UAVs and remote sensing using 3D Gaussian Splatting (3DGS) for efficient and accurate camera pose estimation, overcoming limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the trade-offs and challenges in existing visual relocalization methods such as lack of precision, computational complexity, and scalability issues, especially in large-scale remote sensing scenarios with high altitude variations.

Method: The authors introduce a framework called $
Hi^2$-GSLoc, which uses a dual-hierarchical sparse-to-dense and coarse-to-fine approach leveraging 3D Gaussian Splatting (3DGS). The method includes partitioned training, GPU-accelerated matching, and memory management for scalability.

Result: The proposed approach demonstrates competitive localization accuracy, recall rates, computational efficiency, and effectively filters unreliable pose estimates in simulated, public, and real-world flight datasets.

Conclusion: The method shows promise for practical remote sensing, providing a robust and scalable solution for visual relocalization challenges in UAV applications.

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [277] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: This paper introduces a novel method, LINR-PCGC, for lossless point cloud geometry compression using Implicit Neural Representations, offering better efficiency and compactness compared to traditional and AI-based approaches.


<details>
  <summary>Details</summary>
Motivation: Current AI-based point cloud compression techniques are overly dependent on specific data distributions, making them less effective in real-world scenarios. INR methods address this but are limited to lossy geometry compression.

Method: The paper proposes LINR-PCGC, which employs a point cloud level coding framework and a lightweight SparseConv-based network. It includes network initialization for faster encoding and modules for context extraction, node prediction, and model compression.

Result: Experimental evaluations show LINR-PCGC reduces bitstream size significantly, outperforming other methods by over 21% on key datasets, while also improving encoding speed and decoder compactness.

Conclusion: LINR-PCGC is a pioneering lossless geometry compression method for point clouds, combining efficiency, accuracy, and compactness, with impactful improvements over existing technologies.

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [278] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: This paper introduces a method called DWTGS for reconstructing high-quality novel views in sparse-view 3D Gaussian Splatting using wavelet-based frequency regularization, which offers better results than Fourier-based methods.


<details>
  <summary>Details</summary>
Motivation: Sparse-view 3D Gaussian Splatting struggles with high-quality novel view reconstruction due to overfitting on high-frequency details, motivating a need for better regularization techniques.

Method: The paper proposes DWTGS, which uses wavelet-space losses to supervise low-frequency subbands at multiple discrete wavelet transform (DWT) levels while enforcing sparsity in high-frequency subbands self-supervisedly.

Result: DWTGS achieves consistent outperforming results across benchmarks compared to Fourier-based approaches, demonstrating better generalization and reduced high-frequency hallucinations.

Conclusion: Wavelet-based frequency regularization focusing on low frequencies improves sparse-view 3D reconstruction and avoids detrimental effects of traditional Fourier-based high-frequency learning.

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [279] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: The paper presents a computationally efficient method for face image quality assessment (FIQA) using teacher-student model distillation while achieving high accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Face image quality assessment is critical for face-related tasks, but previous algorithms often have high computational complexity, hindering scalability.

Method: The method involves two stages: self-training a teacher model on labeled and unlabeled data, followed by distilling knowledge into a lightweight student model using these pseudo-labeled samples.

Result: Experimental results show the student model matches the teacher model's performance with low computational demand, winning first place in the ICCV 2025 VQualA FIQA Challenge.

Conclusion: The proposed approach provides a highly efficient FIQA mechanism suitable for real-world deployment while maintaining competitive performance metrics.

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [280] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: The paper investigates spatially-controlled image generation using transformer-based systems by conducting controlled experiments on different generation paradigms like diffusion-based, flow-based, and autoregressive models.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the lack of detailed and fair scientific comparison in spatially-controlled image generation research and provide clear guidance to practitioners.

Method: Controlled experiments on ImageNet with various models, proposing a baseline token prefilling approach and exploring enhancements like classifier-free guidance and softmax truncation.

Result: Token prefilling proves effective as a baseline, classifier-free guidance and softmax truncation improve control-generation consistency, while adapter-based approaches mitigate forgetting but underperform in consistency compared to full training.

Conclusion: The paper provides actionable insights for transformer-based spatially-controlled generation methods and clarifies the strengths and limitations of different approaches.

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [281] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: The paper introduces TokensGen, a method for generating consistent long videos using condensed tokens and pre-trained short video models.


<details>
  <summary>Details</summary>
Motivation: Existing generative models face challenges in producing long videos due to memory bottlenecks and lack of long-term consistency.

Method: TokensGen employs a two-stage framework: 1) training Token-to-Video (To2V) for short clip generation guided by tokenized semantics, 2) developing Text-to-Token (T2To) for global consistency, and 3) using adaptive FIFO-Diffusion for seamless transitions.

Result: TokensGen achieves enhanced long-term temporal and content consistency while maintaining computational efficiency.

Conclusion: The proposed method provides a scalable solution for generating long videos, paving the way for advancements in storytelling, cinematic production, and immersive simulations.

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [282] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: The paper proposes a transformer-based method for correcting photometric inconsistencies across camera views, improving novel view synthesis quality with efficient training.


<details>
  <summary>Details</summary>
Motivation: Modern camera pipelines introduce photometric inconsistencies affecting multi-view consistency and degrading novel view synthesis.

Method: A transformer-based approach predicts spatially adaptive bilateral grids integrated with the 3D Gaussian Splatting pipeline.

Result: The proposed method outperforms or matches existing optimization techniques in reconstruction fidelity and speed.

Conclusion: The approach addresses appearance variation issues effectively, enabling robust cross-scene generalization without scene-specific retraining, maintaining high efficiency.

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [283] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: The paper presents HDF, a novel framework for dynamic facial expression recognition that tackles challenges from data heterogeneity and variability by introducing time-frequency and optimization-aware modules, yielding improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance degradation in dynamic facial expression recognition caused by sample heterogeneity from multi-source data and individual expression variability.

Method: The paper introduces the Heterogeneity-aware Distributional Framework (HDF) with two key modules: the Time-Frequency Distributional Attention Module (DAM) for temporal consistency and frequency robustness, and the Distribution-aware Scaling Module (DSM) that balances classification and contrastive losses for better optimization.

Result: Experiments on DFEW and FERV39k datasets show that the proposed HDF framework improves recognition accuracy, robustness, weighted average recall (WAR), and unweighted average recall (UAR) under diverse and imbalanced scenarios.

Conclusion: HDF is an effective and generalizable solution for dynamic facial expression recognition with strong performance and robustness against dataset heterogeneity and individual variability.

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [284] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: This paper presents tree-based semantic loss functions that leverage hierarchical label organization to improve medical image segmentation, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of current learning methods for medical image segmentation, which treat all errors equally and fail to consider inter-class label semantics.

Method: Two tree-based semantic loss functions are designed to utilize hierarchical label structures. These losses are integrated into a method for training with sparse, background-free annotations.

Result: Extensive experiments on head MRI for whole brain parcellation and neurosurgical hyperspectral imaging show that the proposed method achieves state-of-the-art performance.

Conclusion: The proposed tree-based semantic loss functions successfully improve segmentation accuracy by leveraging label hierarchy, with broad applicability to different medical imaging tasks.

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [285] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: The paper introduces a dynamic rank adjustment method for Low-Rank Adaptation in medical image segmentation, improving parameter-efficient fine-tuning methods.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of fixed and unalterable rank in Low-Rank Adaptation (LoRA) for medical imaging tasks, which may hinder task-specific adaptation.

Method: Incorporate an l_1 sparsity regularizer into the loss function to dynamically adjust the decomposition rank during fine-tuning, using a proximal optimizer.

Result: Extensive experiments demonstrate improved performance compared to standard LoRA and other PEFT methods, especially in few-shot fine-tuning for diverse segmentation tasks.

Conclusion: The proposed method allows automatic rank identification, enhancing efficiency and robustness in medical image segmentation fine-tuning.

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [286] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: This paper proposes a novel low-parameter deep neural network architecture, NoDepth Bottleneck, which enhances scaling and accuracy by addressing feature map interference in bottleneck designs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve performance and scalability of low-parameter deep neural networks for computer vision, particularly under constraints of fewer than 1.5 million parameters.

Method: The study examines bottleneck architectures and their behavior with superlinear activation functions, identifying design elements to reduce interference in feature maps.

Result: The proposed architecture, NoDepth Bottleneck, demonstrates efficient scaling and high accuracy on the ImageNet dataset, showcasing a proof-of-concept for neural networks in the low-parameter range.

Conclusion: This research advances the understanding of bottleneck designs in computer vision and provides a pathway to creating scalable, efficient, low-parameter neural networks.

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [287] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: This paper introduces ConformalSAM, a semi-supervised semantic segmentation framework leveraging foundational segmentation models like SEEM to generate high-confidence masks for unlabeled data, achieving state-of-the-art performance on standard benchmarks.


<details>
  <summary>Details</summary>
Motivation: Pixel-level vision tasks demand extensive labeled data, which is expensive. Semi-supervised approaches can lessen this burden by combining labeled and unlabeled data. Recent foundational models trained on massive datasets have shown potential in cross-domain generalization.

Method: The method centers around calibrating SEEM-generated masks with conformal prediction to filter unreliable pixel labels, ensuring only high-confidence labels are used. The framework employs uncertainty calibration and a self-reliance training strategy to avoid overfitting.

Result: Experiments reveal ConformalSAM's superior performance on three standard benchmarks compared to existing SSSS methods and its effectiveness as a performance-boosting plug-in for various models.

Conclusion: ConformalSAM effectively combines foundational segmentation models with conformal prediction for semi-supervised learning, showcasing improved reliability, adaptability, and boosting early-stage and later-stage training outcomes.

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [288] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: The paper identifies and addresses the limitations of Multimodal Large Language Models (MLLMs) in fully utilizing visual information during Multimodal In-Context Learning (MICL).


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle to effectively leverage visual data in multimodal tasks, instead over-relying on textual patterns, which limits their adaptability and overall utility.

Method: The authors propose a fine-tuning strategy called Dynamic Attention Reallocation (DARA) to balance attention across visual and textual tokens. They also introduce TrueMICL, a dedicated dataset designed to evaluate and enhance genuine multimodal learning capabilities.

Result: The proposed solutions significantly enhance the MLLMs' ability to integrate and utilize multimodal information, showing substantial improvement over previous methods.

Conclusion: Dynamic Attention Reallocation (DARA) and the TrueMICL dataset offer a holistic solution to overcoming MICL limitations, enabling genuinely multimodal in-context learning with broad applications.

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [289] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: The paper demonstrates improved multivariate subsurface modeling and probabilistic inversion using diffusion models, outperforming alternatives like VAEs and GANs, with enhanced posterior sampling and reduced computational costs.


<details>
  <summary>Details</summary>
Motivation: Subsurface modeling and probabilistic inversion face challenges in achieving robust statistical sampling and efficient computation, prompting exploration of diffusion models as an alternative to VAEs and GANs.

Method: The paper introduces corrections to the Diffusion Posterior Sampling method, including a likelihood approximation to better account for noise contamination inherent in diffusion models.

Result: Tests in geological scenarios show enhanced statistical robustness, better posterior sampling, and decreased computational requirements compared to existing methods.

Conclusion: Diffusion models offer better conditional generative modeling capabilities and faster probabilistic inversion, making them superior to traditional generative modeling approaches in subsurface applications.

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [290] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench is introduced as a benchmark designed to evaluate the physical reasoning skills of text-to-video generation systems that struggle with commonsense and physics-based outputs.


<details>
  <summary>Details</summary>
Motivation: Current T2V models often produce videos that violate physical commonsense and intuitive expectations, necessitating a structured way to evaluate their reasoning capabilities.

Method: PhysVidBench utilizes 383 curated prompts with domains emphasizing physical plausibility. It adopts a three-stage evaluation pipeline: grounded physics question formulation, video captioning via vision-language models, and physics-involved question answering by language models.

Result: PhysVidBench highlights deficiencies in T2V models' handling of affordances, tool-mediated actions, and physical plausibility, offering a more structured and interpretable evaluation framework.

Conclusion: The benchmark moves beyond current evaluation methods to address overlooked aspects of physical commonsense in generative video models, setting the foundation for improvement in this domain.

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [291] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: This paper introduces a new concept-driven video object segmentation framework, Segment Concept (SeC), leveraging large vision-language models (LVLMs) for robust segmentation in complex scenarios, accompanied by a challenging benchmark dataset, SeCVOS.


<details>
  <summary>Details</summary>
Motivation: Current video object segmentation techniques are constrained by reliance on appearance matching and lack human-like conceptual understanding, limiting their robustness in dealing with substantial variations and dynamic scenes.

Method: The proposed SeC framework utilizes LVLMs to build high-level, object-centric representations by progressively integrating visual cues, forming semantic priors for robust segmentation and dynamically adjusting feature matching during inference.

Result: SeC showed significant improvement, achieving an 11.8-point gain over SAM 2.1 on the newly introduced SeCVOS benchmark, thereby demonstrating state-of-the-art performance in challenging scenarios.

Conclusion: The paper highlights the importance of conceptual-driven segmentation strategies and introduces novel methodologies and metrics to advance video object segmentation research.

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [292] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: The study explores improving visual tokenizers for generative modeling by aligning their embeddings with denoising objectives. The proposed Latent Denoising Tokenizer (l-DeTok) outperforms standard tokenizers in reconstructing clean images from corrupted inputs.


<details>
  <summary>Details</summary>
Motivation: To identify properties that can make visual tokenizers more effective for generative modeling, focusing on their ability to handle corrupted inputs via a denoising objective.

Method: The researchers propose l-DeTok, a tokenizer trained to reconstruct clean images from latent embeddings corrupted by interpolative noise and random masking, aligning its design with the denoising objectives of generative models.

Result: Experiments on ImageNet 256x256 show that l-DeTok consistently outperforms standard tokenizers across six representative generative models.

Conclusion: Denoising is highlighted as a fundamental principle for tokenizer development, offering insights that could shape future designs of visual tokenizers.

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [293] [Characterizing Communication Patterns in Distributed Large Language Model Inference](https://arxiv.org/abs/2507.14392)
*Lang Xu,Kaushik Kandadi Suresh,Quentin Anthony,Nawras Alnaasan,Dhabaleswar K. Panda*

Main category: cs.DC

TL;DR: The paper examines communication challenges in distributed inference of Large Language Models (LLMs) and evaluates different parallelization strategies.


<details>
  <summary>Details</summary>
Motivation: To address performance bottlenecks created by inter-GPU communication in deploying LLMs for real-world applications.

Method: The study uses profiling measurements and predictive analytical modeling to assess communication behavior in various parallelization configurations for dense transformer-based models.

Result: Tensor parallelism yields faster response times for short sequences but involves high network overhead. Pipeline parallelism reduces data transfer requirements but increases total latency. Combined approaches require careful tuning for optimal performance.

Conclusion: The findings provide guidance for selecting parallelization schemes and highlight opportunities to improve inference frameworks and communication infrastructures.

Abstract: Large Language Models (LLMs) built on transformer architectures have
transformed natural language processing, achieving remarkable performance
across diverse applications. While distributed inference frameworks enable
practical deployment of these models, inter-GPU communication creates
significant performance constraints that limit service quality in real-world
systems. This paper investigates communication dynamics in distributed LLM
serving-analyzing how various parallelization approaches coordinate data
exchange between GPU workers during inference. We study dense transformer-based
models as representative examples of contemporary architectures widely used in
operational deployments. Our work combines detailed profiling measurements with
predictive analytical models to characterize communication behavior across
different parallelization configurations. Results show that tensor parallelism
incurs substantial network overhead but delivers superior response times for
brief sequences, pipeline parallelism minimizes data transfer requirements
while increasing total latency, and combined approaches demand careful tuning
to achieve balanced performance. These insights offer practical recommendations
for selecting appropriate parallelization schemes in production LLM services
and identify key opportunities for optimizing inference frameworks and
communication infrastructure.

</details>


### [294] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: This paper introduces a GRU-based predictive autoscaling framework for edge stream processing, addressing workload fluctuations and enhancing resource utilization efficiency.


<details>
  <summary>Details</summary>
Motivation: Develop an effective solution to cope with resource provisioning challenges in edge computing and DSP caused by workload fluctuations.

Method: A three-step approach: GRU neural network for load forecasting, transfer learning for domain adaptation, and dynamic autoscaling based on predicted loads.

Result: The GRU model achieved up to 1.3% SMAPE, outperforming CNN, ARIMA, and Prophet in predictive accuracy and training efficiency.

Conclusion: GRU-based predictive approach enhances proactive resource allocation efficiency in edge stream processing, balancing accuracy and computational overhead.

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


### [295] [Simulating Chirality: Solving Distance-$k$-Dispersion on an 1-Interval Connected Ring](https://arxiv.org/abs/2507.14723)
*Brati Mondal,Pritam Goswami,Buddhadeb Sau*

Main category: cs.DC

TL;DR: This study explores the Distance-k-Dispersion problem for mobile agents in ring networks, enhancing understanding of agent coordination without chirality and presents an efficient algorithm.


<details>
  <summary>Details</summary>
Motivation: To generalize the classical dispersion problem by requiring agents to maintain a minimum distance of k hops, and to investigate agent coordination without shared directional sense.

Method: The paper introduces a method for simulating chirality using local information, proof of solvability for D-k-D in various configurations, and an algorithm completing dispersion in O(ln) rounds.

Result: The research resolves open questions about solvability of D-k-D in dynamic networks and provides a constructive algorithm for practical applications.

Conclusion: Findings extend theoretical insights into mobile agent coordination and demonstrate chirality is not essential in this context.

Abstract: We study the Distance-$k$-Dispersion (D-$k$-D) problem for synchronous mobile
agents in a 1-interval-connected ring network having $n$ nodes and with $l$
agents where $3 \le l \le \lfloor \frac{n}{k}\rfloor$, without the assumption
of chirality (a common sense of direction for the agents). This generalizes the
classical dispersion problem by requiring that agents maintain a minimum
distance of $k$ hops from each other, with the special case $k=1$ corresponding
to the standard dispersion.
  The contribution in this work is threefold. Our first contribution is a novel
method that enables agents to simulate chirality using only local information,
vision and bounded memory. This technique demonstrates that chirality is not a
fundamental requirement for coordination in this model.
  Building on this, our second contribution partially resolves an open question
posed by Agarwalla et al. (ICDCN, 2018), who considered the same model (1-
interval connected ring, synchronous agents, no chirality). We prove that
D-$k$-D, and thus dispersion is solvable from any arbitrary configuration under
these assumptions (excluding vertex permutation dynamism)for any size of the
ring network which was earlier limited to only odd sized ring or to a ring of
size four.
  Finally, we present an algorithm for D-$k$-D in this setting that works in
$O(ln)$ rounds, completing the constructive side of our result.
  Altogether, our findings significantly extend the theoretical understanding
of mobile agent coordination in dynamic networks and clarify the role of
chirality in distributed computation.

</details>


### [296] [ACME: Adaptive Customization of Large Models via Distributed Systems](https://arxiv.org/abs/2507.14802)
*Ziming Dai,Chao Qiu,Fei Gao,Yunfeng Zhao,Xiaofei Wang*

Main category: cs.DC

TL;DR: The paper introduces ACME, a distributed method for customizing Transformer-based large models to address cloud deployment issues like data privacy and response latency.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models face challenges in cloud deployment concerning privacy, latency, and inefficiency. Effective customization closer to users is necessary to overcome issues like high centralization costs, user heterogeneity, and data heterogeneity.

Method: ACME uses a bidirectional single-loop distributed system to achieve fine-grained model customization. It includes backbone generation to optimize resource use and personalized architecture aggregation to manage data heterogeneity.

Result: ACME significantly reduces data transmission volume by 94% and improves average accuracy by 10% over the baseline with a 30% enhancement in trade-off metrics.

Conclusion: ACME successfully provides an efficient, decentralized customization process overcoming the challenges of centralization, user variability, and data distribution issues.

Abstract: Pre-trained Transformer-based large models have revolutionized personal
virtual assistants, but their deployment in cloud environments faces challenges
related to data privacy and response latency. Deploying large models closer to
the data and users has become a key research area to address these issues.
However, applying these models directly often entails significant difficulties,
such as model mismatching, resource constraints, and energy inefficiency.
Automated design of customized models is necessary, but it faces three key
challenges, namely, the high cost of centralized model customization,
imbalanced performance from user heterogeneity, and suboptimal performance from
data heterogeneity. In this paper, we propose ACME, an adaptive customization
approach of Transformer-based large models via distributed systems. To avoid
the low cost-efficiency of centralized methods, ACME employs a bidirectional
single-loop distributed system to progressively achieve fine-grained
collaborative model customization. In order to better match user heterogeneity,
it begins by customizing the backbone generation and identifying the Pareto
Front under model size constraints to ensure optimal resource utilization.
Subsequently, it performs header generation and refines the model using data
distribution-based personalized architecture aggregation to match data
heterogeneity. Evaluation on different datasets shows that ACME achieves
cost-efficient models under model size constraints. Compared to centralized
systems, data transmission volume is reduced to 6 percent. Additionally, the
average accuracy improves by 10 percent compared to the baseline, with the
trade-off metrics increasing by nearly 30 percent.

</details>


### [297] [Byzantine-Robust Decentralized Coordination of LLM Agents](https://arxiv.org/abs/2507.14928)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: This paper proposes DecentLLMs, a decentralized consensus mechanism for multi-agent systems of Large Language Models (LLMs) to address vulnerabilities with leader-driven systems, overcoming Byzantine agent issues and improving answer quality.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of single-agent LLM systems like hallucinations and single-point failures, and improve Byzantine-robust multi-agent systems that are vulnerable to targeted attacks against leader-based coordination.

Method: Introduce DecentLLMs, a decentralized architecture where worker agents generate answers concurrently, and evaluator agents independently score and rank answers for final selection using Byzantine-robust aggregation techniques.

Result: DecentLLMs demonstrates effective tolerance of Byzantine agents and yields higher-quality answers with faster consensus.

Conclusion: The decentralized approach of DecentLLMs offers a promising solution for ensuring robust consensus and high-quality answers in multi-agent LLM systems, overcoming existing drawbacks of leader-driven methods.

Abstract: Collaboration among multiple large language model (LLM) agents is a promising
approach to overcome inherent limitations of single-agent systems, such as
hallucinations and single points of failure. As LLM agents are increasingly
deployed on open blockchain platforms, multi-agent systems capable of
tolerating malicious (Byzantine) agents have become essential.
  Recent Byzantine-robust multi-agent systems typically rely on leader-driven
coordination, which suffers from two major drawbacks. First, they are
inherently vulnerable to targeted attacks against the leader. If consecutive
leaders behave maliciously, the system repeatedly fails to achieve consensus,
forcing new consensus rounds, which is particularly costly given the high
latency of LLM invocations. Second, an underperforming proposal from the leader
can be accepted as the final answer even when higher-quality alternatives are
available, as existing methods finalize the leader's proposal once it receives
a quorum of votes.
  To address these issues, we propose DecentLLMs, a novel decentralized
consensus approach for multi-agent LLM systems, where worker agents generate
answers concurrently and evaluator agents independently score and rank these
answers to select the best available one. This decentralized architecture
enables faster consensus despite the presence of Byzantine agents and
consistently selects higher-quality answers through Byzantine-robust
aggregation techniques.
  Experimental results demonstrate that DecentLLMs effectively tolerates
Byzantine agents and significantly improves the quality of selected answers.

</details>


### [298] [AMPED: Accelerating MTTKRP for Billion-Scale Sparse Tensor Decomposition on Multiple GPUs](https://arxiv.org/abs/2507.15121)
*Sasindu Wijeratne,Rajgopal Kannan,Viktor Prasanna*

Main category: cs.DC

TL;DR: The paper develops AMPED, a multi-GPU algorithm that accelerates the MTTKRP operation, a critical component of sparse tensor decomposition, achieving substantial speedups on billion-scale sparse tensors.


<details>
  <summary>Details</summary>
Motivation: To address the computational bottleneck in sparse tensor decomposition caused by MTTKRP, which becomes increasingly demanding in memory and compute for billion-scale tensors.

Method: The authors introduce AMPED, a multi-GPU algorithm that employs a partitioning strategy and dynamic load balancing to distribute tasks efficiently and minimize idle time among GPUs during computations.

Result: AMPED delivers a 5.1x geometric mean speedup in execution time compared to state-of-the-art GPU baselines, when tested on real-world billion-scale tensors using 4 GPUs on a single CPU node.

Conclusion: The proposed approach shows significant performance improvements, extending the scalability of GPU hardware for sparse tensor workloads while effectively addressing memory and performance constraints.

Abstract: Matricized Tensor Times Khatri-Rao Product (MTTKRP) is the computational
bottleneck in sparse tensor decomposition. As real-world sparse tensors grow to
billions of nonzeros, they increasingly demand higher memory capacity and
compute throughput from hardware accelerators. In this work, we present AMPED,
a multi-GPU parallel algorithm designed to accelerate MTTKRP on billion-scale
sparse tensors. AMPED scales beyond the limits of a single GPU, meeting both
the memory and performance requirements of large-scale workloads. We introduce
a partitioning strategy combined with a dynamic load balancing scheme to
distribute computation and minimize GPU idle time. On real-world billion-scale
tensors, AMPED achieves a 5.1x geometric mean speedup in total execution time
over state-of-the-art GPU baselines using 4 GPUs on a single CPU node.

</details>


### [299] [Dynatune: Dynamic Tuning of Raft Election Parameters Using Network Measurement](https://arxiv.org/abs/2507.15154)
*Kohya Shiozaki,Junya Nakamura*

Main category: cs.DC

TL;DR: Dynatune dynamically adjusts Raft's election parameters using network metrics to reduce leader failure detection and out-of-service times.


<details>
  <summary>Details</summary>
Motivation: Traditional Raft struggles to optimize election parameters under dynamic network conditions, leading to increased out-of-service times and reduced responsiveness.

Method: Dynatune adapts Raft's election parameters based on network metrics like round-trip time and packet loss, measured via heartbeats, without changing its core mechanisms or adding communication overhead.

Result: Dynatune decreases leader failure detection time by 80% and out-of-service time by 45% compared to Raft, ensuring high availability under fluctuating network scenarios.

Conclusion: Dynatune improves the adaptability, performance, and reliability of State Machine Replication services in diverse network environments.

Abstract: Raft is a leader-based consensus algorithm that implements State Machine
Replication (SMR), which replicates the service state across multiple servers
to enhance fault tolerance. In Raft, the servers play one of three roles:
leader, follower, or candidate. The leader receives client requests, determines
the processing order, and replicates them to the followers. When the leader
fails, the service must elect a new leader to continue processing requests,
during which the service experiences an out-of-service (OTS) time. The OTS time
is directly influenced by election parameters, such as heartbeat interval and
election timeout. However, traditional approaches, such as Raft, often struggle
to effectively tune these parameters, particularly under fluctuating network
conditions, leading to increased OTS time and reduced service responsiveness.
To address this, we propose Dynatune, a mechanism that dynamically adjusts
Raft's election parameters based on network metrics such as round-trip time and
packet loss rates measured via heartbeats. By adapting to changing network
environments, Dynatune significantly reduces the leader failure detection and
OTS time without altering Raft's core mechanisms or introducing additional
communication overheads. Experimental results demonstrate that Dynatune reduces
the leader failure detection and OTS times by 80% and 45%, respectively,
compared with Raft, while maintaining high availability even under dynamic
network conditions. These findings confirm that Dynatune effectively enhances
the performance and reliability of SMR services in various network scenarios.

</details>


### [300] [GALE: Leveraging Heterogeneous Systems for Efficient Unstructured Mesh Data Analysis](https://arxiv.org/abs/2507.15230)
*Guoxi Liu,Thomas Randall,Rong Ge,Federico Iuricich*

Main category: cs.DC

TL;DR: This paper introduces GALE, a CUDA-based data structure that offloads mesh connectivity tasks to GPUs, achieving a 2.7x speedup over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current methods for handling unstructured mesh connectivity are CPU-bound, leading to resource contention and limiting performance on visualization tasks.

Method: The research proposes GALE, which offloads mesh connectivity computations to GPUs, enabling parallel heterogeneous processing on CPU-GPU systems.

Result: GALE achieves up to 2.7x speedup compared to existing methods and maintains memory efficiency in experiments with high-performance CPUs and GPUs.

Conclusion: GALE improves the efficiency of scientific visualization tasks by leveraging GPUs for connectivity computations, unlocking significant performance gains in heterogeneous systems.

Abstract: Unstructured meshes present challenges in scientific data analysis due to
irregular distribution and complex connectivity. Computing and storing
connectivity information is a major bottleneck for visualization algorithms,
affecting both time and memory performance. Recent task-parallel data
structures address this by precomputing connectivity information at runtime
while the analysis algorithm executes, effectively hiding computation costs and
improving performance. However, existing approaches are CPU-bound, forcing the
data structure and analysis algorithm to compete for the same computational
resources, limiting potential speedups. To overcome this limitation, we
introduce a novel task-parallel approach optimized for heterogeneous CPU-GPU
systems. Specifically, we offload the computation of mesh connectivity
information to GPU threads, enabling CPU threads to focus on executing the
visualization algorithm. Following this paradigm, we propose GALE (GPU-Aided
Localized data structurE), the first open-source CUDA-based data structure
designed for heterogeneous task parallelism. Experiments on two 20-core CPUs
and an NVIDIA V100 GPU show that GALE achieves up to 2.7x speedup over
state-of-the-art localized data structures while maintaining memory efficiency.

</details>


### [301] [An ML-Driven Participant Selection Technique for Federated Recommendation System in Edge-Cloud Computing](https://arxiv.org/abs/2507.15233)
*Jintao Liu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: This paper proposes a multi-objective reinforcement learning-based participant selection method for Federated Recommendation Systems (FRS), which improves training efficiency and fairness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in FRS such as device heterogeneity, non-IID data, and communication bottlenecks while optimizing training scalability and privacy.

Method: The authors define a composite client-utility function and employ a multi-armed bandit framework to dynamically balance exploration and exploitation for participant selection. They evaluate the approach on an edge-cloud setup using the MovieLens-100K dataset.

Result: The proposed method accelerated convergence by 32-50% in time-to-target AUC, reduced training time by up to 46%, and matched or slightly improved key recommendation metrics (final AUC, NDCG@50, Recall@50).

Conclusion: The results validate that adaptive client sampling using reinforcement learning can greatly enhance both the efficiency and fairness of federated recommendation systems.

Abstract: Recommendation systems (RS) personalize content by analyzing user
preferences, but typically require centralized collection of user data, raising
privacy and scalability concerns. Federated Recommendation Systems (FRS)
address these issues by enabling distributed, privacy-preserving model training
across edge devices, keeping raw data on-device. Although existing FRS
frameworks benefit from on-device feature extraction and privacy preservation,
they suffer from heterogeneous device capabilities, non-independent and
identically distributed (non-IID) data, and communication bottlenecks. To
overcome these limitations, we propose a multi-objective reinforcement learning
(RL) participant selection that jointly optimizes historical client performance
reputation (CPR), data utility, and system efficiency. First, we define a
composite client-utility function combining CPR, system capability, and data
quality. Next, we embed this utility into a multi-armed bandit (MAB) framework
and dynamically balance exploration-exploitation to select participants.
Finally, we practically implement our approach using the PySyft framework on an
edge-cloud testbed, and evaluate it on a multimodal movie-recommendation task
built from the MovieLens-100K dataset. Across four different skewed
data-partition scenarios, our MAB-based selection accelerates convergence by
32-50% in time-to-target AUC and reduces total wall-clock training time by up
to 46%, while matching or slightly improving final AUC, NDCG@50, and Recall@50
compared to existing FRS baselines. Our results demonstrate that adaptive,
reward-driven client sampling can substantially enhance both efficiency and
fairness in real-world federated deployments.

</details>


### [302] [Efficient Routing of Inference Requests across LLM Instances in Cloud-Edge Computing](https://arxiv.org/abs/2507.15553)
*Shibo Yu,Mohammad Goudarzi,Adel Nadjaran Toosi*

Main category: cs.DC

TL;DR: This paper presents an adaptive routing algorithm leveraging NSGA-II for distributing LLM inference requests across cloud-edge computing environments, achieving notable improvements in response time and cost.


<details>
  <summary>Details</summary>
Motivation: Growing demand for LLM inference services has created challenges around latency and operational costs, necessitating efficient routing methods.

Method: The paper formulates routing as a multi-objective optimization problem using NSGA-II, balancing response quality, time, and cost while accounting for request and node heterogeneity.

Result: Experiments using datasets like SQuAD and GSM8K show up to 95.2% improvement in response time and 34.9% reductions in cost compared to baseline algorithms.

Conclusion: The algorithm effectively optimizes performance for scalable LLM deployments under dynamic workloads in diverse cloud-edge infrastructures.

Abstract: The rising demand for Large Language Model (LLM) inference services has
intensified pressure on computational resources, resulting in latency and cost
challenges. This paper introduces a novel routing algorithm based on the
Non-dominated Sorting Genetic Algorithm II (NSGA-II) to distribute inference
requests across heterogeneous LLM instances in a cloud-edge computing
environment. Formulated as a multi-objective optimization problem, the
algorithm balances response quality, response time, and inference cost,
adapting to request heterogeneity (e.g., varying complexity and prompt lengths)
and node diversity (e.g., edge vs. cloud resources). This adaptive routing
algorithm optimizes performance under dynamic workloads. We benchmark the
approach using a testbed with datasets including Stanford Question Answering
Dataset (SQuAD), Mostly Basic Python Problems (MBPP), Hella Situations With
Adversarial Generations (HellaSwag), and Grade School Math 8K (GSM8K).
Experimental results show our solution, compared to the baselines, achieves up
to 95.2% and 34.9% improvements in terms of response time and cost,
respectively. These findings validate the algorithm's effectiveness for
scalable LLM deployments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [303] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: The paper addresses the limitations of magnitude-biased pruning methods for neural networks and introduces Catalyst regularization, a novel pruning approach.


<details>
  <summary>Details</summary>
Motivation: Traditional pruning methods like L1 or Group Lasso result in biased pruning decisions by favoring larger magnitude filters. This leads to sensitivity near the pruning decision boundary and unstable model behavior.

Method: The authors propose Catalyst regularization, a novel technique utilizing an extended parameter space and auxiliary variables to ensure unbiased pruning decisions and robust, wide-margin separation between pruned and preserved filters.

Result: Empirical results demonstrate that Catalyst Pruning outperforms state-of-the-art filter pruning methods across various datasets and models, with improved robustness and fairness.

Conclusion: The Catalyst regularization effectively overcomes the shortcomings of traditional pruning methods, providing superior and stable pruning performance while maintaining model accuracy.

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [304] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: This paper introduces PROscore, a novel magnitude-independent method for structured pruning in neural networks, enabling near-lossless pruning with better performance after fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing neural network pruning techniques often prioritize larger magnitude filters, resulting in biases and reduced pruning flexibility.

Method: The authors develop a pruning strategy in projective space, monitoring gradient descent movements to assign pruning likelihood based on their PROscore metric.

Result: Their evaluations show near-lossless pruning with minimal performance drops and promising results post-fine-tuning.

Conclusion: The proposed method challenges magnitude-based biases in pruning, advancing theoretical and empirical capabilities for importance-based pruning strategies.

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [305] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: This paper introduces SOAR, a method integrating language models into a self-improving evolutionary loop for program synthesis, achieving significant gains in solving challenging tasks.


<details>
  <summary>Details</summary>
Motivation: Current program synthesis tasks are often too complex for state-of-the-art language models to solve in one attempt, necessitating alternative approaches like evolutionary methods.

Method: The proposed method, SOAR, alternates between evolutionary search (using an LLM for sampling and refining solutions) and hindsight learning (fine-tuning the LLM based on problem-solution pairs from search attempts).

Result: SOAR achieves major improvements on the ARC-AGI benchmark, solving 52% of the public test set through iterative performance boosts.

Conclusion: SOAR demonstrates that integrating LLMs into an evolutionary process with hindsight learning can unlock more effective program synthesis capabilities, pushing the boundaries of existing benchmarks.

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [306] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: The paper evaluates latent space fusion (intermediate fusion) for predicting depressive symptoms and finds it superior to early fusion methods.


<details>
  <summary>Details</summary>
Motivation: Mental health disorders like depression and anxiety require better early detection techniques and personalized interventions. Existing models often do not sufficiently utilize the complexity of multimodal psychiatric data, and advanced integration techniques may improve predictive accuracy.

Method: The study used data from the BRIGHTEN clinical trial and tested both early fusion (using a Random Forest model) and intermediate fusion (via autoencoders and a neural network) to predict depressive symptoms (PHQ-2 scores). Performance was evaluated using metrics like mean squared error (MSE) and coefficient of determination (R2).

Result: The intermediate fusion (Combined Model) outperformed early fusion and other baselines in all tested scenarios, achieving lower MSE (0.4985 vs. 0.5305) and higher R2 (0.4695 vs. 0.4356). It was also less prone to overfitting and performed best when integrating all data modalities.

Conclusion: Latent space fusion is a promising method for handling multimodal mental health data, offering better generalization and accuracy than traditional approaches. Future work should focus on interpretability and clinical applications at the individual level.

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [307] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: The paper introduces Predictive Representativity (PR), a fairness auditing framework to identify outcome-level inequities in AI systems, highlighting skin cancer classifiers' underperformance for darker skin tones despite balanced data sampling.


<details>
  <summary>Details</summary>
Motivation: To address concerns about algorithmic bias and inequitable outcomes in AI systems, particularly for marginalized populations, and to push fairness analysis from data composition to outcome equity.

Method: The authors propose Predictive Representativity (PR) and conduct a case study evaluating AI skin cancer classifiers trained on HAM10000 and BOSQUE Test datasets, analyzing performance disparities by skin phototype.

Result: Substantial performance disparities were found, with AI classifiers consistently underperforming for individuals with darker skin tones, revealing limitations in fairness generalization despite balanced data sampling.

Conclusion: The study underscores the need for fairness auditing post-deployment, better dataset documentation, inclusive validation, and offers tools like PR and External Transportability Criterion to enhance equity in data-driven healthcare.

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [308] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: The paper explores the training solutions of two-layer neural networks with smooth activation functions, analyzing their theoretical foundation and providing experimental validations.


<details>
  <summary>Details</summary>
Motivation: To investigate how the "black box" of two-layer neural network training solutions operates, using smooth activation functions like pre-ReLU sigmoid-based ones.

Method: Four principles guide the analysis: Taylor series expansions, strict partial ordering of knots, smooth-spline implementation, and smooth-continuity restrictions. The paper provides theoretical proofs and experimental validations.

Result: Proved universal approximation for arbitrary input dimensions and demonstrated experimental evidence. The insights unveil the inner workings of the solution space.

Conclusion: New proofs contribute to approximation theory, and the study demystifies the solution space of neural networks using pre-ReLU activation functions.

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [309] [Pruning Increases Orderedness in Recurrent Computation](https://arxiv.org/abs/2507.14747)
*Yiding Song*

Main category: cs.LG

TL;DR: The paper explores how directionality in neural networks can be induced through pruning rather than built-in, and evaluates its impact on learning and performance.


<details>
  <summary>Details</summary>
Motivation: The authors aim to understand the role and importance of directionality in artificial neural networks, inspired by its prevalence in biological systems.

Method: The authors formalize a perceptron layer with all-to-all connections, apply pruning techniques to induce topological order, and evaluate performance across different random seeds.

Result: Pruning techniques successfully create directionality in information flow without negatively impacting the model's performance.

Conclusion: Directionality is not essential for learning but serves as a beneficial inductive bias that can emerge through sparsification and optimization techniques.

Abstract: Inspired by the prevalence of recurrent circuits in biological brains, we
investigate the degree to which directionality is a helpful inductive bias for
artificial neural networks. Taking directionality as topologically-ordered
information flow between neurons, we formalise a perceptron layer with
all-to-all connections (mathematically equivalent to a weight-tied recurrent
neural network) and demonstrate that directionality, a hallmark of modern
feed-forward networks, can be induced rather than hard-wired by applying
appropriate pruning techniques. Across different random seeds our pruning
schemes successfully induce greater topological ordering in information flow
between neurons without compromising performance, suggesting that
directionality is not a prerequisite for learning, but may be an advantageous
inductive bias discoverable by gradient descent and sparsification.

</details>


### [310] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: The paper addresses the problem of out-of-distribution (OOD) detection in deep learning by introducing a method that improves distance-based detection scores using statistical feature constraints.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance the reliability of deep learning systems by improving the OOD detection process, particularly addressing issues caused by biased data features and extreme values.

Method: The proposed method, Feature Bank Enhancement (FBE), utilizes statistical dataset characteristics to constrain extreme features and improve separation between in-distribution and out-of-distribution samples.

Result: Experiments conducted on ImageNet-1k and CIFAR-10 datasets showed that the proposed method achieved state-of-the-art performance in OOD detection benchmarks.

Conclusion: Feature Bank Enhancement (FBE) effectively addresses limitations of distance-based score functions and improves OOD detection performance, supported by theoretical analysis and experiments.

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [311] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: The paper proposes a clustering-based method to efficiently predict and utilize activation sparsity in Large Language Models, achieving significant computational savings while maintaining model quality.


<details>
  <summary>Details</summary>
Motivation: Activation sparsity in Large Language Models provides opportunities to reduce computational costs, but predicting activation patterns efficiently remains a challenge due to the vast number of neurons.

Method: The authors introduce a clustering-based framework that groups similar activation patterns into representative clusters, enabling efficient prediction of activation states and sparse computation.

Result: The method achieves up to 79.34% clustering precision, maintains a low degradation in perplexity scores, and demonstrates effectiveness with a PPL score as low as 12.49 when using a sufficiently large number of clusters.

Conclusion: This clustering-based approach preserves model quality, reduces computational overhead, and paves the way for efficient activation pattern prediction in large-scale language models.

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [312] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: The paper presents FedStrategist, a meta-learning framework for Federated Learning that uses a contextual bandit agent to dynamically adapt aggregation rules for robust and secure AI models.


<details>
  <summary>Details</summary>
Motivation: Federated Learning's decentralized nature makes it vulnerable to model poisoning attacks, and existing static defenses are insufficient in heterogeneous or adaptive adversarial scenarios.

Method: The authors propose FedStrategist, which employs a lightweight contextual bandit agent to dynamically select optimal aggregation rules based on real-time diagnostic metrics.

Result: Experimental results show that FedStrategist outperforms static methods, handles diverse data environments, and navigates stealth adversaries, while allowing explicit trade-offs between security and performance.

Conclusion: FedStrategist offers a new, practical approach to enhancing resilience and adaptability in Federated Learning by balancing risk tolerance and model integrity, proving effective against various threats.

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [313] [Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm](https://arxiv.org/abs/2507.15132)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: The paper proposes a genetic algorithm to generate synthetic datasets optimized for specific complexity measures in classification and regression tasks, revealing correlations between data complexity and recognition performance.


<details>
  <summary>Details</summary>
Motivation: There is a need for diverse and challenging synthetic datasets to robustly evaluate machine learning methods' strengths and limitations.

Method: The authors use a genetic algorithm to optimize datasets based on 10 complexity measures for classification and 4 for regression, transforming data to target complexity levels through linear feature projections.

Result: Experiments demonstrated that the genetic algorithm effectively produces datasets with varying complexities, allowing state-of-the-art models to exhibit a correlation between data complexity and their recognition quality.

Conclusion: The approach enhances dataset availability and enables better evaluation of machine learning models by correlating complexity with model performance.

Abstract: The research community continues to seek increasingly more advanced synthetic
data generators to reliably evaluate the strengths and limitations of machine
learning methods. This work aims to increase the availability of datasets
encompassing a diverse range of problem complexities by proposing a genetic
algorithm that optimizes a set of problem complexity measures for
classification and regression tasks towards specific targets. For
classification, a set of 10 complexity measures was used, while for regression
tasks, 4 measures demonstrating promising optimization capabilities were
selected. Experiments confirmed that the proposed genetic algorithm can
generate datasets with varying levels of difficulty by transforming
synthetically created datasets to achieve target complexity values through
linear feature projections. Evaluations involving state-of-the-art classifiers
and regressors revealed a correlation between the complexity of the generated
data and the recognition quality.

</details>


### [314] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: This paper proposes a robust and explainable deep learning framework for beam alignment in mmWave systems, addressing issues like data collection, hardware limitations, and adversarial attacks while achieving efficient and reliable performance.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenges of inefficiencies, lack of explainability, and vulnerability to adversarial attacks in deep learning-based beam alignment for mmWave systems.

Method: The framework utilizes RSSI measurements from wide beams, site-specific digital twins for synthetic channel data, transfer learning for model refinement, SHAP for input feature ranking, and DkNN for outlier detection.

Result: Experimental findings reveal a 70% reduction in real-world data needs, a 62% decrease in beam training overhead, an 8.5x improvement in outlier detection robustness, and near-optimal spectral efficiency.

Conclusion: The proposed framework enhances trustworthiness, robustness, and efficiency in beam alignment processes for mmWave systems, paving the path for AI-native 6G networks.

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [315] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: The paper introduces a semi-supervised federated learning framework, SSFL-DCSL, to improve fault diagnosis in industrial machinery by addressing data scarcity, label constraints, and privacy issues, improving accuracy on challenging tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in intelligent fault diagnosis caused by data labeling constraints, data distribution differences, and the need for user privacy in industrial settings.

Method: The method involves a semi-supervised federated learning framework, leveraging dual contrastive loss and soft labeling, with innovations such as a Laplace-based sample weighting function, dual contrastive losses, and local prototype aggregation with momentum.

Result: Experiments on multiple datasets show that SSFL-DCSL significantly outperforms state-of-the-art methods, improving accuracy by 1.15% to 7.85% in challenging scenarios with only 10% labeled data.

Conclusion: The SSFL-DCSL framework effectively addresses challenges in industrial fault diagnosis by improving accuracy, enabling knowledge sharing across clients, and maintaining user data privacy.

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [316] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: This paper introduces the B4 model to analyze and predict financial market trends by embedding historical prices and external signals into latent spaces representing bull and bear regimes.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding complex financial market dynamics influenced by biases, historical data, and external narratives such as news and social media.

Method: The B4 model jointly embeds temporal price sequences and external signals into a unified latent space using inertial pairing to maintain momentum and dual competition to contrast bullish and bearish behaviors.

Result: Empirical analysis showed that B4 effectively models bias-driven asymmetry, behavioral inertia, and market heterogeneity, achieving superior performance in trend prediction.

Conclusion: The B4 model provides interpretable insights into the dynamics of biases and investor behaviors, outperforming previous approaches in trend prediction under dynamic market conditions.

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [317] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: This paper introduces LaCache, a training-free method to optimize Key-Value (KV) caching in LLMs for better long-range processing and memory efficiency.


<details>
  <summary>Details</summary>
Motivation: The increasing sequence lengths in LLMs create a bottleneck due to the exponential growth in Key-Value (KV) pairs, necessitating an efficient solution for robust long-range modeling and continuous generation without memory overflow.

Method: The authors propose LaCache, which integrates a ladder-shaped KV cache pattern for long-range dependency capture and an iterative compaction mechanism to compress older caches, enabling continuous generation within fixed memory constraints.

Result: Experiments demonstrate that LaCache improves the long-range capabilities of LLMs across various tasks, benchmarks, and models under memory-constrained scenarios.

Conclusion: LaCache effectively addresses the dual challenges of robust long-range modeling and continuous generation in LLMs without requiring additional training, making it a useful contribution to LLM optimization.

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [318] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: The study develops a deep learning-powered accessibility device for the deaf or hearing-impaired to localize and identify sounds in real time, featuring a novel CNN, audio classification with fine-tuned CLAP, and multimodal integration models.


<details>
  <summary>Details</summary>
Motivation: Enhance accessibility for the deaf or hearing-impaired by leveraging machine learning to address a critical gap in sound localization and identification systems.

Method: The system comprises JerryNet for sound direction classification, a fine-tuned CLAP model for audio classification, and a multimodal integration model combining visual, audio, and text data. It utilizes hardware with microphones, cameras, and a wristband for displaying information.

Result: JerryNet achieved 91.1% precision for sound direction. The audio classification model attained 98.5% and 95% accuracy on custom and AudioSet datasets. The audio-visual localization achieved a cIoU of 0.892 and AUC of 0.658.

Conclusion: The proposed system outperformed baseline models, demonstrating potential to innovate accessibility devices for the hearing-impaired, with significant future research opportunities.

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [319] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: The paper proposes FLock, a decentralized framework for securely fine-tuning large language models (LLMs) using blockchain-based trust layers.


<details>
  <summary>Details</summary>
Motivation: Challenges in centralized LLM fine-tuning include computational overhead and vulnerability to poisoning attacks. Decentralized solutions struggle with trustless environments and scalability for large models.

Method: FLock integrates a blockchain-based trust layer and economic incentives to create a decentralized, auditable fine-tuning protocol, removing the need for a central server.

Result: The paper successfully fine-tuned a 70B LLM in a decentralized setup, reducing adversarial attack success rates by over 68% and improving cross-domain generalization.

Conclusion: FLock addresses security and efficiency gaps in decentralized fine-tuning, offering resilience against attacks while enabling collaborative model improvements.

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [320] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: The paper introduces an interactive learning framework for addressing the pattern explosion problem, utilizing nonlinear utility aggregation via Choquet integrals and geometry-aware query selection to improve ranking accuracy with fewer user interactions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle the pattern explosion problem in pattern mining and improve ranking accuracy with reduced user effort.

Method: The proposed method combines a Choquet integral to model user preferences with geometry-aware query selection based on version space. A branch-and-bound strategy is used for efficient query identification near decision boundaries.

Result: The method was shown to outperform existing approaches, such as ChoquetRank, by achieving higher ranking accuracy with fewer user interactions, as demonstrated in experiments on UCI datasets.

Conclusion: This approach effectively addresses the pattern explosion problem, improving efficiency and accuracy while reducing user effort, representing a significant enhancement over existing methods.

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [321] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: The paper develops an artificial intelligence framework leveraging SHAP values for determining optimal green hydrogen production locations, highlighting influential factors while achieving high predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying suitable sites for green hydrogen production in solar-rich arid regions amid limited direct yield data.

Method: An AI framework employs a multi-stage pipeline with unsupervised clustering, supervised machine learning, and SHAP analysis using integrated datasets.

Result: The model achieved 98% predictive accuracy, identifying water proximity, elevation, and seasonal variation as key site suitability factors for Oman.

Conclusion: The framework provides an objective, reproducible, and scalable method for site suitability assessment, benefiting stakeholders in data-scarce regions for green hydrogen planning.

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [322] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: The authors introduced POGM, a new method for resolving gradient fluctuations and high computation overheads in gradient-based domain generalization, demonstrating competitive results with computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing gradient-based domain generalization methods, specifically gradient fluctuations among domains and high computational costs due to second-order derivative approximations.

Method: The proposed POGM method uses gradient trajectories as data and incorporates independent training via a meta-learner while maximizing gradient inner product (GIP) without large deviations from the empirical risk minimization gradient.

Result: POGM achieved competitive performance compared to other baseline methods while maintaining computational efficiency on DomainBed datasets.

Conclusion: POGM successfully mitigates gradient fluctuation among domains and computation overheads, establishing it as an effective and efficient approach for gradient-based domain generalization.

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [323] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: The paper introduces NanoPro-3M, the largest dataset of nanomaterial-protein interactions, and NanoProFormer, a powerful AI model for predicting these interactions with high generalization capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the challenges posed by limited datasets and lack of generalized models in understanding nanomaterial-protein interactions, thereby unlocking potential applications in medicine and environmental science.

Method: The researchers developed NanoPro-3M, a dataset with over 3.2 million samples, and proposed NanoProFormer, a foundational multimodal AI model that predicts nanomaterial-protein affinities by combining diverse data modalities.

Result: NanoProFormer demonstrates superior generalization, handles missing data, predicts affinities for unseen entities, and outperforms single-modality models. It also facilitates key insights into corona formation and supports downstream applications.

Conclusion: The work provides a robust foundation for predicting nanomaterial-protein interaction endpoints, reduces experimental dependency, and accelerates potential applications in biotechnology and environmental studies.

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [324] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: The paper introduces Linearized Diffusion Map (LDM), a novel linear dimensionality reduction method combining diffusion-based intuition with linear embedding benefits.


<details>
  <summary>Details</summary>
Motivation: To develop a dimensionality reduction method that combines the geometric insights of nonlinear methods with the simplicity and interpretability of linear techniques.

Method: Proposes LDM by linearly approximating the diffusion-map kernel and experimentally compares its performance to PCA using synthetic and real-world datasets.

Result: LDM outperforms PCA in capturing manifold structures in high-dimensional datasets, while maintaining computational efficiency and interpretability.

Conclusion: LDM is a promising dimensionality reduction method with both theoretical and practical benefits, offering complementary advantages to existing linear methods.

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [325] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: This paper identifies and formally defines 'glitches,' highlighting their presence in AI models with steep decision boundaries, and proposes an algorithm to detect them specifically for Gradient-boosted Decision Tree (GBDT) models.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure the reliability and trustworthiness of AI decision-making systems, especially by addressing consistency issues such as abrupt oscillations in model outputs ('glitches').

Method: Authors formalized glitches conceptually, proved their detection is NP-complete for tree ensembles of depth 4, and developed an MILP-based algorithm to identify glitches in GBDT models. They validated this on benchmark datasets.

Result: The proposed MILP-based algorithm successfully demonstrated the detection of glitches within Gradient-boosted Decision Tree benchmarks, establishing computational feasibility and highlighting inconsistency areas.

Conclusion: Glitches are prevalent and indicate critical model inconsistencies. The algorithm provided serves as a practical tool for identifying and potentially mitigating these reliability issues in AI models.

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [326] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: This paper proposes a new method, Unary Feedback as Observation (UFO), to improve multi-turn reasoning in Large Reasoning Models (LRMs) by using minimalist user feedback during reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Existing Reinforcement Learning methods for large reasoning models often fail in multi-turn problem-solving scenarios, leading to repetitive or unsatisfactory performance, which prompted the authors to explore how LRMs can reflect and improve answers in a multi-turn context.

Method: They propose Unary Feedback as Observation (UFO) for reinforcement learning, leveraging minimal user feedback (e.g., simple prompts like 'Let's try again') to train LRMs in a multi-turn setup. The reward system was also designed to encourage careful reasoning and minimize the need for multiple turns.

Result: Experimental outcomes showed that models trained with UFO preserved their single-turn performance while improving multi-turn reasoning accuracy by up to 14%, enhancing their ability to process and react to feedback iteratively.

Conclusion: The study concludes that Unary Feedback as Observation is an effective and easy-to-integrate method for improving LRMs' performance in multi-turn reasoning contexts, making them better equipped to handle iterative problem-solving.

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [327] [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740)
*Andrew Wang,Elisa Nguyen,Runshi Yang,Juhan Bae,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: The paper introduces ASTRA, an algorithm that efficiently approximates inverse Hessian-vector products (iHVP) to enhance training data attribution (TDA) performance.


<details>
  <summary>Details</summary>
Motivation: Training data attribution helps identify the role of specific training data in shaping model behavior. However, existing gradient-based TDA methods struggle with efficiently computing accurate iHVP approximations.

Method: The proposed ASTRA algorithm leverages the EKFAC-preconditioner with Neumann series iterations to deliver a more accurate and computationally efficient approximation of iHVP.

Result: ASTRA improves iHVP approximation accuracy, requiring fewer iterations than conventional Neumann series methods and outperforming EKFAC-based approximations.

Conclusion: By improving iHVP computations, ASTRA enhances the effectiveness of TDA methods, providing a more reliable mechanism to analyze training data contributions to model behavior.

Abstract: Training data attribution (TDA) provides insights into which training data is
responsible for a learned model behavior. Gradient-based TDA methods such as
influence functions and unrolled differentiation both involve a computation
that resembles an inverse Hessian-vector product (iHVP), which is difficult to
approximate efficiently. We introduce an algorithm (ASTRA) which uses the
EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP
approximation for TDA. ASTRA is easy to tune, requires fewer iterations than
Neumann series iterations, and is more accurate than EKFAC-based
approximations. Using ASTRA, we show that improving the accuracy of the iHVP
approximation can significantly improve TDA performance.

</details>


### [328] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: This paper identifies limitations in individual fairness in deepfake detection and proposes a new framework to improve fairness and generalization in existing models.


<details>
  <summary>Details</summary>
Motivation: The misuse of generative AI for deepfakes poses risks, and current detection methods lack considerations of individual fairness, which affects detection equity across different populations.

Method: The paper introduces the first generalizable framework to improve individual fairness in deepfake detectors. It enhances similar predictions for similar individuals without compromising detection performance.

Result: Extensive experiments on prominent deepfake datasets show significant improvement in individual fairness and maintenance of robust detection performance, outperforming current state-of-the-art methods.

Conclusion: The proposed framework addresses critical gaps in the literature by improving individual fairness in deepfake detection, presenting an innovation applicable to various existing detection systems.

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [329] [Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization](https://arxiv.org/abs/2507.14746)
*Bach Do,Nafeezat A. Ajenifuja,Taiwo A. Adebiyi,Ruda Zhang*

Main category: cs.LG

TL;DR: The paper discusses using Gaussian processes (GPs) for efficient sampling in engineering tasks like global sensitivity analysis (GSA) and optimization, introducing two sampling methods.


<details>
  <summary>Details</summary>
Motivation: High-fidelity simulations and experiments are costly, limiting their use in engineering tasks like GSA and optimization. GPs serve as a cost-effective proxy for these tasks.

Method: Two GP sampling methods, random Fourier features and pathwise conditioning, are developed and implemented. Applications in GSA, single-objective optimization, and multi-objective optimization are demonstrated.

Result: The paper provides numerical examples showcasing the effective use of these GP sampling methods in GSA and optimization tasks.

Conclusion: The proposed GP sampling techniques are efficient and impactful for engineering tasks, addressing cost constraints associated with high-fidelity simulations.

Abstract: High-fidelity simulations and physical experiments are essential for
engineering analysis and design. However, their high cost often limits their
applications in two critical tasks: global sensitivity analysis (GSA) and
optimization. This limitation motivates the common use of Gaussian processes
(GPs) as proxy regression models to provide uncertainty-aware predictions based
on a limited number of high-quality observations. GPs naturally enable
efficient sampling strategies that support informed decision-making under
uncertainty by extracting information from a subset of possible functions for
the model of interest. Despite their popularity in machine learning and
statistics communities, sampling from GPs has received little attention in the
community of engineering optimization. In this paper, we present the
formulation and detailed implementation of two notable sampling methods --
random Fourier features and pathwise conditioning -- for generating posterior
samples from GPs. Alternative approaches are briefly described. Importantly, we
detail how the generated samples can be applied in GSA, single-objective
optimization, and multi-objective optimization. We show successful applications
of these sampling methods through a series of numerical examples.

</details>


### [330] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: This paper develops machine learning models for predicting critical heat flux (CHF) in annular geometries, achieving significantly higher accuracy than traditional empirical methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for predicting CHF in reactors are often inaccurate, lack interpretability, and don't address certain geometries, prompting the need for more reliable approaches.

Method: Four machine learning models were trained using experimental data from annulus geometries and compared to empirical correlations like Biasi, Bowring, and Katto.

Result: Machine learning models achieved mean relative prediction errors below 3.5%, significantly outperforming the empirical models' errors of over 26%.

Conclusion: Using hybrid machine learning approaches for CHF prediction in annular geometries provides higher accuracy and resilience compared to traditional empirical models.

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [331] [Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning](https://arxiv.org/abs/2507.14748)
*Patrik Reizinger,Bálint Mucsányi,Siyuan Guo,Benjamin Eysenbach,Bernhard Schölkopf,Wieland Brendel*

Main category: cs.LG

TL;DR: This paper investigates mutual information skill learning (MISL) for self-supervised representation learning in reinforcement learning, with a focus on Contrastive Successor Features (CSF).


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical role of representation learning and mutual information parametrization in MISL methods, which aim to enhance exploration and environment representation in RL.

Method: The paper analyzes CSF from the perspective of identifiable representation learning, proving its ability to recover ground-truth features up to a linear transformation and examining mutual information objectives and entropy regularizers.

Result: Theoretical proofs show CSF's recovery of ground-truth features and empirical validation is provided on MuJoCo and DeepMind Control tasks, both from state and pixel inputs.

Conclusion: CSF offers a provable method for identifiable representation learning in RL, shedding light on the theoretical workings of MISL and its trade-offs.

Abstract: Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.

</details>


### [332] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: This paper explores using influence functions to filter noisy training data for fine-tuning language models, with small accuracy improvements noted.


<details>
  <summary>Details</summary>
Motivation: To improve the quality of fine-tuning datasets for language models, which are often noisy, and enhance their downstream performance.

Method: The authors adapt the TL;DR dataset and use conjugate-gradient approximated influence functions to identify and prune harmful training examples.

Result: Using influence functions for filtering yielded a modest 1.5% improvement in retraining accuracy after removing 10% of training examples. Gradient similarity proved better for spotting helpful examples.

Conclusion: Influence functions are useful for identifying harmful examples, but gradient similarity is more effective for detecting beneficial ones, highlighting the importance of local curvature.

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [333] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: This paper introduces "Solo Connection," a novel parameter-efficient fine-tuning method for Large Language Models (LLMs) that outperforms current methods like LoRA while significantly reducing trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Existing parameter-efficient fine-tuning methods like LoRA adjust only specific weight matrices, which can be limiting given the scaling of modern LLM architectures, prompting the need for innovative approaches to maximize fine-tuning efficiency and adaptability.

Method: The proposed Solo Connection adapts entire decoder-block representations instead of modifying individual weight matrices. It integrates a trainable linear transformation inspired by homotopy theory, allowing a smooth interpolation between pre-trained and task-specific representations, and also utilizes long skip connections across decoder blocks.

Result: Solo Connection outperforms LoRA on natural language generation benchmarks, reduces trainable parameters by 59% compared to LoRA, and achieves over 99% reduction relative to full fine-tuning of GPT2.

Conclusion: Solo Connection offers an efficient and scalable method for fine-tuning LLMs by leveraging long skip connections and representation-level adaptation, making it well-suited for newer, larger architectures.

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [334] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: This paper presents distributional unlearning, a new approach to remove undesirable information at the domain level from trained models, achieving efficient data deletion while retaining model performance.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of existing machine unlearning techniques, which primarily focus on individual sample deletion but struggle to efficiently and comprehensively handle removal at the domain or sub-population level.

Method: The method involves a model-agnostic framework that determines the minimal set of data points to delete to make the dataset shift away from the unwanted domain while retaining desired information, leveraging metrics like Kullback-Leibler divergence and a distance-based selection rule.

Result: The approach achieved a significant reduction (15-72%) in the number of deletions required compared to random deletion methods, while maintaining a negligible impact on retained data performance across various datasets.

Conclusion: Distributional unlearning provides a more efficient and precise method for domain-level data removal in machine learning models, addressing key privacy and quality requirements while optimizing deletion resources.

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>


### [335] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: The paper introduces INCADET, a system for real-time cyberattack detection using incrementally updated causal graphs, addressing challenges in adapting to evolving system behaviors.


<details>
  <summary>Details</summary>
Motivation: The authors aim to reduce false positives in real-time anomaly detection for critical infrastructures, which traditional methods struggle with due to high data variance and imbalanced classes.

Method: The INCADET framework uses Early Symptom Detection to identify system status changes, Incremental Causal Graph Learning to refine graphs based on experience replay, and GCN-based classification for anomaly identification.

Result: Experiments on real-world datasets show INCADET outperforms static causal and deep temporal systems in terms of accuracy, robustness, and adaptability to evolving attack scenarios.

Conclusion: INCADET provides a robust and dynamic solution for real-time cyberattack detection, leveraging causal graph updates to address the limitations of prior methods.

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [336] [Better Models and Algorithms for Learning Ising Models from Dynamics](https://arxiv.org/abs/2507.15173)
*Jason Gaitonde,Ankur Moitra,Elchanan Mossel*

Main category: cs.LG

TL;DR: The paper introduces algorithms to learn the Ising model using a natural observation model that only tracks configuration changes, overcoming limitations of prior methods that required observing all site update attempts.


<details>
  <summary>Details</summary>
Motivation: Existing algorithms for learning the Ising model require observing all site update attempts, a strong assumption that restricts real-world application. A more realistic observation model is needed for practical use.

Method: The authors develop algorithms for learning the Ising model's structure and parameters using only configuration changes of Markov chains. These algorithms utilize properties of reversible, single-site Markov chains, including Glauber and Metropolis dynamics.

Result: The proposed algorithms efficiently identify the Ising model's dependency graph and parameters in polynomial time, matching the state-of-the-art performance even under a weaker observation model.

Conclusion: This work addresses an open problem by providing the first efficient algorithms to learn the Ising model in a realistic observation framework, significantly broadening its applicability.

Abstract: We study the problem of learning the structure and parameters of the Ising
model, a fundamental model of high-dimensional data, when observing the
evolution of an associated Markov chain. A recent line of work has studied the
natural problem of learning when observing an evolution of the well-known
Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Mossel STOC 2024], which provides an arguably more realistic
generative model than the classical i.i.d. setting. However, this prior work
crucially assumes that all site update attempts are observed, \emph{even when
this attempt does not change the configuration}: this strong observation model
is seemingly essential for these approaches. While perhaps possible in
restrictive contexts, this precludes applicability to most realistic settings
where we can observe \emph{only} the stochastic evolution itself, a minimal and
natural assumption for any process we might hope to learn from. However,
designing algorithms that succeed in this more realistic setting has remained
an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Moitra, Mossel, STOC 2025].
  In this work, we give the first algorithms that efficiently learn the Ising
model in this much more natural observation model that only observes when the
configuration changes. For Ising models with maximum degree $d$, our algorithm
recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log
n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time,
which qualitatively matches the state-of-the-art even in the i.i.d. setting in
a much weaker observation model. Our analysis holds more generally for a
broader class of reversible, single-site Markov chains that also includes the
popular Metropolis chain by leveraging more robust properties of reversible
Markov chains.

</details>


### [337] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: The paper analyzes test-time scaling methods in AI models, highlighting the limitations of scaling down and the inconsistencies of scaling up.


<details>
  <summary>Details</summary>
Motivation: To better understand the impact of test-time scaling methods on model performance, particularly distinguishing between scaling behavior and actual performance improvement.

Method: The study performs a detailed analysis comparing the effects of scaling down (enforcing maximum length) and scaling up (appending 'Wait') during test-time compute.

Result: Scaling down successfully replicates test-time scaling but imposes performance limits, while scaling up introduces behavioral inconsistencies like oscillating solutions.

Conclusion: True performance enhancement in test-time compute requires learning mechanisms (e.g., reinforcement learning) to naturally scale, surpassing simple scaling methods and achieving higher performance outcomes.

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [338] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: This paper proposes leveraging pre-trained deep learning models within reinforcement learning to solve large-scale stochastic optimization problems, focusing on supply chain inventory management.


<details>
  <summary>Details</summary>
Motivation: The paper aims to make reinforcement learning more efficient for solving complex stochastic optimization problems by better exploring solution spaces through pre-trained deep learning intervention modules.

Method: The authors use deep reinforcement learning to learn and forecast stochastic supply chain processes, break down the optimization problem into scalable DL modules, and integrate a constraint coordination mechanism to forecast dual costs.

Result: The proposed methodology demonstrated improved performance on large-scale real-world supply chain datasets compared to traditional RL approaches.

Conclusion: The paper shows that decomposing complex RL problems into scalable DL modules improves performance and scalability, offering valuable insights into large-scale optimization challenges.

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [339] [Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification](https://arxiv.org/abs/2507.15240)
*Le Peng,Yash Travadi,Chuan He,Ying Cui,Ju Sun*

Main category: cs.LG

TL;DR: The paper proposes a new framework for optimizing classification metrics like precision and recall in imbalanced classification scenarios using exact constrained reformulations, demonstrating superior results over existing methods.


<details>
  <summary>Details</summary>
Motivation: Performance measures like accuracy can be misleading in imbalanced classification scenarios, necessitating alternative metrics such as precision, recall, or F1-score to better evaluate classifiers.

Method: The authors introduce a framework utilizing exact constrained reformulations for metric optimization, addressing three practical IC settings: FPOR (fix precision optimize recall), FROP (fix recall optimize precision), and OFBS (optimize F1-score).

Result: Experimental results show the proposed framework outperforms state-of-the-art methods across multiple benchmark datasets for the three DMO problems.

Conclusion: The exact reformulation and optimization (ERO) framework is highly effective for binary IC DMO problems and potentially applicable to broader contexts, marking an advancement in handling performance measures for imbalanced classification.

Abstract: For classification with imbalanced class frequencies, i.e., imbalanced
classification (IC), standard accuracy is known to be misleading as a
performance measure. While most existing methods for IC resort to optimizing
balanced accuracy (i.e., the average of class-wise recalls), they fall short in
scenarios where the significance of classes varies or certain metrics should
reach prescribed levels. In this paper, we study two key classification
metrics, precision and recall, under three practical binary IC settings: fix
precision optimize recall (FPOR), fix recall optimize precision (FROP), and
optimize $F_\beta$-score (OFBS). Unlike existing methods that rely on smooth
approximations to deal with the indicator function involved, \textit{we
introduce, for the first time, exact constrained reformulations for these
direct metric optimization (DMO) problems}, which can be effectively solved by
exact penalty methods. Experiment results on multiple benchmark datasets
demonstrate the practical superiority of our approach over the state-of-the-art
methods for the three DMO problems. We also expect our exact reformulation and
optimization (ERO) framework to be applicable to a wide range of DMO problems
for binary IC and beyond. Our code is available at
https://github.com/sun-umn/DMO.

</details>


### [340] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: ReDiSC is a novel model designed to improve node classification in graphs by accounting for label correlations and achieving superior scalability.


<details>
  <summary>Details</summary>
Motivation: Existing graph neural networks assume node label independence, conflicting with intuitive correlation observations for labeled graph nodes.

Method: ReDiSC employs a reparameterized masked diffusion model, optimized via variational expectation-maximization (EM) framework.

Result: ReDiSC demonstrates competitive or superior performance on various graph structures and scales effectively to large datasets.

Conclusion: ReDiSC addresses computational limitations of existing methods and links to hybrid GNN-label propagation approaches, providing a practical solution for structured node classification.

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [341] [MAP Estimation with Denoisers: Convergence Rates and Guarantees](https://arxiv.org/abs/2507.15397)
*Scott Pesme,Giacomo Meanti,Michael Arbel,Julien Mairal*

Main category: cs.LG

TL;DR: The paper establishes a theoretical foundation for heuristic denoiser-based algorithms in inverse problems, demonstrating their convergence to the proximal operator under certain conditions.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical justification for using pretrained denoisers as surrogates for intractable proximal operators in solving optimization problems in inverse problems.

Method: A simple algorithm is proposed and analyzed under the assumption of log-concavity of the prior, demonstrating its convergence to the desired proximal operator.

Result: The algorithm is shown to converge under the log-concavity condition, and it can be understood as a gradient descent on smoothed proximal objectives.

Conclusion: The work provides a theoretical explanation for the effectiveness of heuristic methods using pretrained denoisers, strengthening their reliability in inverse problems.

Abstract: Denoiser models have become powerful tools for inverse problems, enabling the
use of pretrained networks to approximate the score of a smoothed prior
distribution. These models are often used in heuristic iterative schemes aimed
at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal
operator of the negative log-prior plays a central role. In practice, this
operator is intractable, and practitioners plug in a pretrained denoiser as a
surrogate-despite the lack of general theoretical justification for this
substitution. In this work, we show that a simple algorithm, closely related to
several used in practice, provably converges to the proximal operator under a
log-concavity assumption on the prior $p$. We show that this algorithm can be
interpreted as a gradient descent on smoothed proximal objectives. Our analysis
thus provides a theoretical foundation for a class of empirically successful
but previously heuristic methods.

</details>


### [342] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: The paper introduces an FRL-EH framework to address environment heterogeneity in federated reinforcement learning. A new global objective function and the FedRQ algorithm are proposed to optimize policy performance across heterogeneous settings.


<details>
  <summary>Details</summary>
Motivation: The study aims to address statistical heterogeneity in local environments during federated reinforcement learning, which better aligns with real-world scenarios.

Method: The authors develop the FRL-EH framework, introduce a robust global objective function, and propose the FedRQ algorithm to ensure robust global policy performance. Expectile loss is utilized for extending FedRQ to continuous state spaces, enabling its integration with DNN-based RL methods.

Result: The theoretical guarantee of asymptotic convergence to the optimal global policy is provided. Extensive empirical evaluations demonstrate the robustness and superior performance of the proposed FRL algorithms in heterogeneous environments compared to existing methods.

Conclusion: The proposed frameworks and algorithms effectively address environment heterogeneity in federated reinforcement learning, outperforming current state-of-the-art techniques.

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [343] [GeoHNNs: Geometric Hamiltonian Neural Networks](https://arxiv.org/abs/2507.15678)
*Amine Mohamed Aboussalah,Abdessalam Ed-dib*

Main category: cs.LG

TL;DR: The paper presents Geometric Hamiltonian Neural Networks (GeoHNN), a framework that embeds geometric priors of physical laws in machine learning models to achieve better long-term stability and accuracy when modeling dynamics.


<details>
  <summary>Details</summary>
Motivation: Conventional physics-informed neural networks often violate fundamental physical principles, leading to instability and poor predictions, particularly for high-dimensional, chaotic systems.

Method: The proposed GeoHNN framework incorporates geometric priors by enforcing Riemannian geometry of inertia (parametrizing inertia matrices in their natural space) and symplectic geometry of phase space (using constrained autoencoders to preserve phase space volume).

Result: GeoHNN outperforms existing models in experiments on coupled oscillators and high-dimensional deformable objects, achieving better stability, accuracy, and energy conservation.

Conclusion: Embedding geometric principles intrinsic to physics into neural networks enhances their robustness and generalizability, showcasing the importance of geometric structure in modeling physical systems.

Abstract: The fundamental laws of physics are intrinsically geometric, dictating the
evolution of systems through principles of symmetry and conservation. While
modern machine learning offers powerful tools for modeling complex dynamics
from data, common methods often ignore this underlying geometric fabric.
Physics-informed neural networks, for instance, can violate fundamental
physical principles, leading to predictions that are unstable over long
periods, particularly for high-dimensional and chaotic systems. Here, we
introduce \textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework
that learns dynamics by explicitly encoding the geometric priors inherent to
physical laws. Our approach enforces two fundamental structures: the Riemannian
geometry of inertia, by parameterizing inertia matrices in their natural
mathematical space of symmetric positive-definite matrices, and the symplectic
geometry of phase space, using a constrained autoencoder to ensure the
preservation of phase space volume in a reduced latent space. We demonstrate
through experiments on systems ranging from coupled oscillators to
high-dimensional deformable objects that GeoHNN significantly outperforms
existing models. It achieves superior long-term stability, accuracy, and energy
conservation, confirming that embedding the geometry of physics is not just a
theoretical appeal but a practical necessity for creating robust and
generalizable models of the physical world.

</details>


### [344] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: The paper introduces Generative Distribution Distillation (GenDD) for knowledge distillation, employing two novel strategies: Split Tokenization and Distribution Contraction to overcome optimization and semantic limitation issues.


<details>
  <summary>Details</summary>
Motivation: The primary motivation is to improve the knowledge distillation process by framing it as a generative problem, while addressing challenges such as high-dimensional optimization and lack of semantic label supervision.

Method: The authors propose the GenDD framework with Split Tokenization, offering stable unsupervised KD, and Distribution Contraction to incorporate label supervision into the generative objective. They also provide theoretical proof linking the approach to multi-task learning.

Result: Experiments demonstrate that GenDD achieves 16.29% improvement over the KL baseline on ImageNet in unsupervised scenarios. With label supervision, GenDD-trained ResNet-50 achieves 82.28% top-1 accuracy, setting a new state-of-the-art in supervised KD.

Conclusion: The proposed GenDD technique addresses limitations in traditional KD methods, achieving state-of-the-art supervised and competitive unsupervised results, providing a robust generative approach for model distillation tasks.

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [345] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: This paper presents the Signal Dice Similarity Coefficient (SDSC), a metric that emphasizes structural alignment in time series representations.


<details>
  <summary>Details</summary>
Motivation: Distance-based objectives like MSE commonly used in self-supervised signal learning lack structural sensitivity and semantic interpretability.

Method: The SDSC measures structural agreement via signed amplitudes derived from the Dice Similarity Coefficient. A hybrid loss combining SDSC with MSE is introduced.

Result: Experiments showed that SDSC-led pretraining enhances performance on forecasting and classification tasks, especially in low-resource and in-domain scenarios.

Conclusion: Structure-aware metrics, such as SDSC, can improve semantic representation quality and serve as alternatives to traditional distance-based methods.

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [346] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: This paper introduces the use of positive-unlabeled (PU) learning to identify control units for causal inference in observational studies where labeled control units are absent.


<details>
  <summary>Details</summary>
Motivation: Causal inference often requires both treated and control groups, but observational studies frequently lack labeled control units, creating challenges in accurately estimating the average treatment effect (ATE).

Method: The authors propose a PU learning framework to infer control units from unlabeled data using treated units and assess its performance using simulated and real-world agricultural datasets.

Result: PU learning effectively identified control units and provided accurate ATE estimations by comparing synthetic data against true values and applying the method to agricultural use cases.

Conclusion: The approach broadens the scope of observational causal inference, making it particularly valuable for fields with limited access to randomized datasets, like agriculture and environmental sciences.

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [347] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: This paper introduces an inverse reinforcement learning approach for infinite-horizon mean-field games using kernel methods to infer complex reward structures from expert demonstrations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current inverse reinforcement learning methods for mean-field games, which often assume a finite set of basis functions and typically use finite-horizon settings.

Method: The authors model unknown reward functions in a reproducing kernel Hilbert space and introduce a Lagrangian relaxation reformulation of the problem. This translates into an unconstrained log-likelihood maximization problem solved via gradient ascent. They also analyze the theoretical smoothness and differentiability of the proposed objective.

Result: The proposed method is applied to a mean-field traffic routing game, where it demonstrates the ability to effectively recover expert behavior.

Conclusion: The paper successfully extends inverse reinforcement learning to a broader class of reward structures and infinite-horizon mean-field games, paving the way for more realistic applications.

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [348] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: The paper traces the origins of self-attention to a broader computational principle of using pairwise affinity matrices, showcasing it as a special case of Infinite Feature Selection (Inf-FS).


<details>
  <summary>Details</summary>
Motivation: To contextualize self-attention within a larger paradigm of affinity-based computations and demonstrate its mathematical linkage to other models and tasks.

Method: A conceptual analysis linking self-attention to affinity matrix-based models, particularly Infinite Feature Selection (Inf-FS), and comparing their defining characteristics and computation approaches.

Result: Self-attention is framed as a specific instance of Inf-FS, differing mainly in the way the affinity matrix is constructed and applied.

Conclusion: Self-attention builds upon a general principle rooted in affinity-based reasoning, uniting diverse machine learning research under a common mathematical framework.

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [349] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: The paper introduces LPS-GNN, a scalable and efficient framework capable of handling large graphs (up to 100 billion) on a single GPU, while achieving significant improvements in predictive performance.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs struggle with scalability and efficiency when processing large-scale graphs due to neighbor explosion and high computational demands.

Method: The authors developed a new graph partitioning algorithm, LPMetis, and implemented a subgraph augmentation strategy in LPS-GNN to enhance compatibility with various GNN algorithms and improve scalability.

Result: LPS-GNN demonstrated improved performance (8.24%–13.89% lift over SOTA models) and scalability, handling a 100 billion graph dataset on a single GPU in 10 hours.

Conclusion: LPS-GNN is a scalable, low-cost, and effective GNN framework successfully deployed in real-world applications, showing superior predictive performance and compatibility across different algorithms.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [350] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: The paper introduces a hybrid framework combining Transformer-based GAN and Multiple Instance Explainable Learning for improved UAV flight state classification, achieving high accuracy with strong efficiency and robustness.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the deficiencies of conventional time series classification methods and state-of-the-art models in UAV flight state detection, particularly their lack of robustness, generalization ability, and high computational requirements.

Method: The framework integrates a Transformer encoder to capture temporal dependencies, a GAN module for dataset augmentation, and a Multiple Instance Explainable Learning (MILET) approach to focus on key discriminative data segments.

Result: The method achieved 96.5% accuracy on the DroneDetect dataset and 98.6% on the DroneRF dataset, outperforming existing state-of-the-art approaches while also reducing computational costs.

Conclusion: The proposed framework is robust, accurate, and computationally efficient, making it suitable for real-time UAV flight state classification tasks in resource-limited environments.

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [351] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: This paper introduces the first polynomial-time deterministic algorithm to approximate the $k$-subspace median for points in $eal^d$, achieving a $oot d$ multiplicative factor and overcoming non-convexity challenges.


<details>
  <summary>Details</summary>
Motivation: The $k$-subspace median offers a more robust and sparse alternative to the classical $k$-PCA but is challenging to approximate due to its non-convex nature for $k<d-1.

Method: The authors developed a deterministic algorithm with both polynomial running time and approximation factor that scales sub-exponentially with $k$, specifically achieving a factor of $oot d$.

Result: Their algorithm effectively approximates $	ext{k-subspace median}$ in polynomial time, addressing previous computational barriers associated with non-convex optimization.

Conclusion: The introduced algorithm not only advances computation for $	ext{k-subspace median}$ but also lays a foundation for solving similar problems involving $	ext{nonstandard norms}$ and handling sparsity or outliers.

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [352] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: The paper introduces Rec-AD, a framework that boosts efficiency in detecting False Data Injection Attacks (FDIA) in smart grids.


<details>
  <summary>Details</summary>
Motivation: Address computational and memory burdens in FDIA detection for large-scale datasets in smart grids.

Method: Uses Tensor Train decomposition integrated with Deep Learning Recommendation Model (DLRM), embedding compression, data access optimization via index reordering, and a pipeline training mechanism.

Result: Rec-AD enhances computational throughput, real-time detection, reduces memory overhead, and improves attack prevention.

Conclusion: Rec-AD offers scalable, efficient, and robust support for securing smart grids, emphasizing its integration capability with existing systems.

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [353] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: A novel graph contrastive learning (GCL) framework named AD-GCL is proposed for anomaly detection, addressing robustness against structural imbalance such as low-degree (tail) anomalies.


<details>
  <summary>Details</summary>
Motivation: While graph contrastive learning demonstrates powerful anomaly detection capabilities, its robustness to structural imbalances in real-world data, such as low-degree nodes in networks with power-law distributions, is underexplored. This raises security concerns in high-risk applications.

Method: AD-GCL introduces a neighbor pruning strategy to filter noisy edges for high-degree nodes (head nodes) and facilitate detecting low-degree anomalies (tail nodes). Additionally, it employs anomaly-guided neighbor completion to enhance receptive fields and introduces intra- and inter-view consistency losses for better representations.

Result: AD-GCL achieves superior anomaly detection performance for both head and tail anomalies across various datasets.

Conclusion: The proposed AD-GCL framework demonstrates superior robustness and effectiveness in detecting anomalies in graphs with structural imbalances, validating its applicability to high-risk real-world scenarios.

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [354] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: The paper proposes GCC-Spam, a spam-text detection framework that counteracts adversarial strategies and addresses labeled data scarcity using character similarity networks, contrastive learning, and GAN-generated samples.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges posed by spam text growth, including adversarial tactics and limited labeled data, aiming to improve spam detection mechanisms.

Method: GCC-Spam integrates a character similarity network, contrastive learning for latent-space optimization, and GAN-based pseudo-spam samples generation to enhance model robustness and accuracy.

Result: Extensive experiments using real-world datasets show GCC-Spam achieves superior detection rates with fewer labeled examples compared to baseline methods.

Conclusion: The GCC-Spam framework effectively improves spam-text detection accuracy and robustness, offering solutions to adversarial strategies and data scarcity challenges.

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [355] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: The paper introduces SST-CL, a method for emotion recognition using EEG signals, tackling challenges like spatial-temporal neural pattern integration and real-world dynamic emotional intensity adaptation.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve EEG-based emotion recognition systems, addressing issues like non-stationary spatial-temporal neural patterns and adapting to dynamic real-world emotional intensity variations.

Method: The framework SST-CL combines spatial-temporal transformers with curriculum learning. The spatial encoder captures inter-channel relationships, while the temporal encoder captures multi-scale dependencies. An intensity-aware curriculum learning strategy further enhances model training.

Result: Experiments on three benchmark datasets showed SST-CL achieves state-of-the-art performance under varying emotional intensities. Ablation studies validated the importance of its architectural components and curriculum learning mechanism.

Conclusion: SST-CL is a promising approach that effectively integrates spatial-temporal features and adapts to varying emotional intensities to enhance EEG-based emotion recognition systems.

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [356] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: This paper addresses fraudulent credit card transaction detection using a novel classifier architecture emphasizing prototype-based attention for latent space structuring, outperforming traditional and generative approaches.


<details>
  <summary>Details</summary>
Motivation: To tackle the challenge of detecting credit card fraud due to class imbalance and subtle discriminatory patterns, and to overcome existing model limitations such as latent cluster separation and overconfidence.

Method: Introduced Causal Prototype Attention Classifier (CPAC) combined with a VAE-GAN encoder, which enhances latent space cluster separation and representation learning through prototype-based attention mechanisms.

Result: Achieved outstanding performance with F1-score of 93.14% and recall of 90.18%, outperforming oversamplers like SMOTE and state-of-the-art generative models.

Conclusion: Classifier-guided latent shaping with CPAC proves to be a promising approach for improving fraud detection, offering enhanced cluster separation and interpretability.

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [357] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: This paper explores scheduling for real-time generative AI (RTGen) workloads on heterogeneous SoC architectures and stresses the importance of dynamic, workload-aware systems.


<details>
  <summary>Details</summary>
Motivation: RTGen workloads are emerging as a critical area due to their compute complexity and real-time constraints, but the performance impact on heterogeneous SoC platforms remains inadequately researched.

Method: The authors characterize RTGen workloads on AMD's Ryzen AI SoC by creating multi-model scenarios, profiling performance across backends, and evaluating five scheduling policies on various metrics.

Result: Scheduling decisions can notably influence RTGen workload performance, demonstrating up to a 41.7% variance in deadline violation rates.

Conclusion: Dynamic heterogeneous scheduling tailored to workload and hardware specifics is crucial for optimizing RTGen applications on edge platforms.

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>


### [358] [LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4](https://arxiv.org/abs/2507.14722)
*Matěj Kripner,Michal Šustr,Milan Straka*

Main category: cs.LG

TL;DR: LeanTree introduces a white-box approach via a Lean 4 tool and dataset, promising improved automated theorem proving compared to black-box methods.


<details>
  <summary>Details</summary>
Motivation: Address the limitations in automated theorem proving, particularly leveraging intermediate proof states for enhanced LLM integration.

Method: LeanTree employs a white-box methodology by factorizing proof states into simpler branches using Lean 4, and provides a dataset of these states for better evaluation and training.

Result: Preliminary findings show that the LeanTree white-box approach has advantages over black-box methods in settings like context reduction and parallel search.

Conclusion: White-box approaches, such as LeanTree, hold promise for advancing ATP by enabling richer interactions and improving overall performance.

Abstract: Automated theorem proving (ATP) has been a classical problem in artificial
intelligence since its inception, yet it remains challenging due to its vast
state and action space. Large language models (LLMs) have recently emerged as a
promising heuristic for ATP, but they lack correctness guarantees and thus
require interaction with a proof verifier. Such interactions typically follow
one of two approaches: black-box interaction, which does not utilize
intermediate proof states, or white-box approaches, which allow for incremental
proof construction and examination of intermediate states. While black-box
approaches have directly benefited from recent LLM advances, white-box methods
have comparatively lagged behind. In this paper, we address this gap by
introducing LeanTree, which consists of (i) a tool built in the Lean 4 language
that factorizes complex proof states into simpler, independent branches, and
(ii) a dataset of these factorized intermediate states. Our white-box tooling
offers several advantages over black-box approaches: it simplifies evaluation,
reduces necessary context, generates richer training data, enables parallel
search across multiple states, supports efficient reuse of states, and provides
feedback in case of errors. Our preliminary results hint that white-box
approaches outperform black-box alternatives in some settings.

</details>


### [359] [Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding](https://arxiv.org/abs/2507.14725)
*Anushka Tiwari,Sayantan Pal,Rohini K. Srihari,Kaiyi Ji*

Main category: cs.LG

TL;DR: GRID introduces a unified framework for prompt-based continual learning with large language models, addressing latent forgetting and prompt memory explosion through task-aware decoding and gradient-based prompt selection.


<details>
  <summary>Details</summary>
Motivation: To improve the scalability and efficiency of large language models in continual learning scenarios by addressing issues of task-aware inference and growing prompt memory.

Method: GRID employs task-aware decoding for better backward transfer, automatic task identification, constrained decoding, and compresses prompts using a gradient-based selection strategy.

Result: GRID achieves significant improvements in backward transfer, competitive forward transfer, and reduces forgotten tasks by up to 80%, outperforming state-of-the-art techniques.

Conclusion: GRID provides a robust solution for scalable and memory-efficient continual learning in large language models, enabling effective task management and reducing memory overhead.

Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to
adapt large language models (LLMs) across task sequences. However, most
existing methods assume task-aware inference and maintain a growing list of
task-specific prompts, which limits scalability and hides latent forgetting. In
this work, we introduce GRID, a unified framework that addresses two key
limitations: (1) latent forgetting under task-agnostic inference, and (2)
prompt memory explosion as task sequences grow. GRID integrates a task-aware
decoding mechanism that improves backward transfer by leveraging representative
inputs, automatic task identification, and constrained decoding. Additionally,
we propose a gradient-based prompt selection strategy that compresses less
informative prompts into a single aggregated representation, enabling scalable
and memory-efficient lifelong learning. Extensive experiments across
short-sequence, long-sequence, and negative transfer benchmarks show that GRID
significantly improves backward transfer, achieves competitive forward
transfer, and reduces forgotten tasks by up to 80\%, outperforming
state-of-the-art methods on T5 and Flan-T5 backbones.

</details>


### [360] [Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning](https://arxiv.org/abs/2507.14736)
*Rafał Surdej,Michał Bortkiewicz,Alex Lewandowski,Mateusz Ostaszewski,Clare Lyle*

Main category: cs.LG

TL;DR: Trainable rational activation functions offer benefits in expressivity and adaptability but may introduce instability in reinforcement and continual learning. Constrained variants improve stability while preserving adaptability.


<details>
  <summary>Details</summary>
Motivation: Enhance the adaptability and expressivity of neural networks using trainable rational activation functions, addressing challenges like overestimation and feature collapse in dynamic learning environments.

Method: Study the trade-off between expressivity and plasticity of trainable rational activations. Propose constrained variants to limit output scaling while maintaining adaptability.

Result: Experiments in environments like MetaWorld and DMC show improved training stability and performance. Findings reveal the trade-offs in continual learning benchmarks, and suggest relevance in continuous control scenarios.

Conclusion: The trade-off between expressivity and plasticity in rational activations can be mitigated with constrained designs, offering actionable guidelines for stable and adaptable activation functions in dynamic environments.

Abstract: Trainable activation functions, whose parameters are optimized alongside
network weights, offer increased expressivity compared to fixed activation
functions. Specifically, trainable activation functions defined as ratios of
polynomials (rational functions) have been proposed to enhance plasticity in
reinforcement learning. However, their impact on training stability remains
unclear. In this work, we study trainable rational activations in both
reinforcement and continual learning settings. We find that while their
flexibility enhances adaptability, it can also introduce instability, leading
to overestimation in RL and feature collapse in longer continual learning
scenarios. Our main result is demonstrating a trade-off between expressivity
and plasticity in rational activations. To address this, we propose a
constrained variant that structurally limits excessive output scaling while
preserving adaptability. Experiments across MetaWorld and DeepMind Control
Suite (DMC) environments show that our approach improves training stability and
performance. In continual learning benchmarks, including MNIST with reshuffled
labels and Split CIFAR-100, we reveal how different constraints affect the
balance between expressivity and long-term retention. While preliminary
experiments in discrete action domains (e.g., Atari) did not show similar
instability, this suggests that the trade-off is particularly relevant for
continuous control. Together, our findings provide actionable design principles
for robust and adaptable trainable activations in dynamic, non-stationary
environments. Code available at:
https://github.com/special114/rl_rational_plasticity.

</details>


### [361] [Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems](https://arxiv.org/abs/2507.14850)
*H. M. Sabbir Ahmad,Ehsan Sabouni,Alexander Wasilkoff,Param Budhraja,Zijian Guo,Songyuan Zhang,Chuchu Fan,Christos Cassandras,Wenchao Li*

Main category: cs.LG

TL;DR: The paper presents a safe Hierarchical Multi-Agent Reinforcement Learning approach using Control Barrier Functions for multi-agent autonomous systems to ensure safety and cooperation.


<details>
  <summary>Details</summary>
Motivation: To ensure agents in multi-agent safety-critical autonomous systems meet safety requirements and cooperate effectively to accomplish tasks.

Method: Proposed a hierarchical reinforcement learning strategy that decomposes learning into two levels: joint cooperative behavior at a higher level and safety-focused individual behavior at a lower level using Control Barrier Functions.

Result: The proposed approach achieved near-perfect success/safety rates (within 5%) in challenging environments with multiple agents navigating conflicting road networks, surpassing state-of-the-art methods.

Conclusion: The approach demonstrates improved safety and performance in multi-agent systems, validating its effectiveness in handling safety-critical tasks.

Abstract: We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.

</details>


### [362] [Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML](https://arxiv.org/abs/2507.14744)
*Mustafa Cavus,Jan N. van Rijn,Przemysław Biecek*

Main category: cs.LG

TL;DR: The paper introduces Rashomon PDP, a framework that represents explanation uncertainty by aggregating partial dependence profiles (PDP) from multiple near-optimal models.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the problem of single-model focus in automated machine learning systems, which overlooks explanation uncertainty—a key concern in explainable AI for human-centric applications.

Method: It uses model multiplicity and aggregates the PDPs of near-optimal models (Rashomon set) to evaluate interpretive variability and disagreement in feature effects.

Result: In experiments with 35 regression datasets, Rashomon PDP typically shows less than 70% overlap with the standard one-model PDP, revealing the inadequacies of single-model explanations.

Conclusion: Rashomon PDP enhances model interpretability by providing a more comprehensive and uncertainty-aware perspective, especially beneficial in high-stakes and trust-critical scenarios.

Abstract: Automated machine learning systems efficiently streamline model selection but
often focus on a single best-performing model, overlooking explanation
uncertainty, an essential concern in human centered explainable AI. To address
this, we propose a novel framework that incorporates model multiplicity into
explanation generation by aggregating partial dependence profiles (PDP) from a
set of near optimal models, known as the Rashomon set. The resulting Rashomon
PDP captures interpretive variability and highlights areas of disagreement,
providing users with a richer, uncertainty aware view of feature effects. To
evaluate its usefulness, we introduce two quantitative metrics, the coverage
rate and the mean width of confidence intervals, to evaluate the consistency
between the standard PDP and the proposed Rashomon PDP. Experiments on 35
regression datasets from the OpenML CTR23 benchmark suite show that in most
cases, the Rashomon PDP covers less than 70% of the best model's PDP,
underscoring the limitations of single model explanations. Our findings suggest
that Rashomon PDP improves the reliability and trustworthiness of model
interpretations by adding additional information that would otherwise be
neglected. This is particularly useful in high stakes domains where
transparency and confidence are critical.

</details>


### [363] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: CXR-TFT is a new framework integrating chest X-rays, radiology reports, and clinical data to predict future CXR findings, aimed at improving ICU patient care.


<details>
  <summary>Details</summary>
Motivation: In ICUs, monitoring and diagnosing critically ill patients in a timely manner is challenging, especially with irregularly acquired CXRs and cross-sectional analysis tools that fail to consider temporal dynamics.

Method: CXR-TFT combines embeddings from a vision encoder for CXRs with interpolated clinical data and uses a transformer model to predict future CXR embeddings based on historical data and clinical measurements.

Result: In a study of 20,000 ICU patients, CXR-TFT accurately forecasted abnormal CXR findings up to 12 hours before they appeared radiographically.

Conclusion: CXR-TFT can aid early intervention in time-sensitive conditions like acute respiratory distress syndrome by providing timely, actionable insights based on 'whole patient' analysis.

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [364] [Rethinking Memorization Measures and their Implications in Large Language Models](https://arxiv.org/abs/2507.14777)
*Bishwamittra Ghosh,Soumi Das,Qinyuan Wu,Mohammad Aflah Khan,Krishna P. Gummadi,Evimaria Terzi,Deepak Garg*

Main category: cs.LG

TL;DR: This paper evaluates memorization in LLMs and its privacy implications, proposing new measures and analyzing their relation to learning efficiency.


<details>
  <summary>Details</summary>
Motivation: To understand whether memorization in LLMs can be avoided during optimal language learning and to assess whether privacy concerns linked to memorization are exaggerated.

Method: The researchers re-examine existing measures of memorization, introduce a new measure called contextual memorization, and evaluate 18 LLMs across multiple formal languages with varying entropy.

Result: Results show disagreements among memorization measures, partial memorization is unavoidable for optimal learning, improved learning reduces some types of memorization but increases others, and many 'memorized' strings do not pose real privacy threats.

Conclusion: Memorization is intertwined with learning efficiency, and existing measures do not fully capture its nuances or implications for privacy.

Abstract: Concerned with privacy threats, memorization in LLMs is often seen as
undesirable, specifically for learning. In this paper, we study whether
memorization can be avoided when optimally learning a language, and whether the
privacy threat posed by memorization is exaggerated or not. To this end, we
re-examine existing privacy-focused measures of memorization, namely
recollection-based and counterfactual memorization, along with a newly proposed
contextual memorization.
  Relating memorization to local over-fitting during learning, contextual
memorization aims to disentangle memorization from the contextual learning
ability of LLMs. Informally, a string is contextually memorized if its
recollection due to training exceeds the optimal contextual recollection, a
learned threshold denoting the best contextual learning without training.
Conceptually, contextual recollection avoids the fallacy of recollection-based
memorization, where any form of high recollection is a sign of memorization.
Theoretically, contextual memorization relates to counterfactual memorization,
but imposes stronger conditions. Memorization measures differ in outcomes and
information requirements.
  Experimenting on 18 LLMs from 6 families and multiple formal languages of
different entropy, we show that (a) memorization measures disagree on
memorization order of varying frequent strings, (b) optimal learning of a
language cannot avoid partial memorization of training strings, and (c)
improved learning decreases contextual and counterfactual memorization but
increases recollection-based memorization. Finally, (d) we revisit existing
reports of memorized strings by recollection that neither pose a privacy threat
nor are contextually or counterfactually memorized.

</details>


### [365] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: This paper compares diffusion-based language models to autoregressive (AR) models in data-constrained settings, demonstrating that diffusion models perform better under limited data and abundant compute.


<details>
  <summary>Details</summary>
Motivation: The aim is to understand if diffusion models can outperform AR models, especially in scenarios with scarce data but sufficient computational resources.

Method: The study systematically tests masked diffusion models in data-constrained environments and compares their performance to AR models, focusing on training efficiency and data usage.

Result: Diffusion models achieve lower validation loss and superior downstream performance, benefiting from implicit data augmentation provided by diverse token orderings and prediction tasks.

Conclusion: Diffusion models are a strong alternative to AR paradigms when computational resources are ample but data availability is limited due to their efficient handling of repeated passes over scarce data.

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


### [366] [Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards](https://arxiv.org/abs/2507.14783)
*Derek Li,Jiaming Zhou,Amirreza Kazemi,Qianyi Sun,Abbas Ghaddar,Mohammad Ali Alomrani,Liheng Ma,Yu Luo,Dong Li,Feng Wen,Jianye Hao,Mark Coates,Yingxue Zhang*

Main category: cs.LG

TL;DR: The paper presents Omni-Think, a reinforcement learning framework for LLMs that improves generalization across diverse tasks through a curriculum-based training progression and hybrid supervision.


<details>
  <summary>Details</summary>
Motivation: Current post-training methods for LLMs, such as Supervised Fine-Tuning, face challenges in generalization, favoring memorization rather than transferable learning for diverse tasks.

Method: The authors employ Omni-Think, a unified RL framework integrating rule-based verifiable rewards and generative preference signals evaluated via LLM-as-a-Judge, along with a curriculum-based training strategy.

Result: Training with curriculum learning improves performance by 5.2% compared to joint training and 9.1% over model merging, proving the method's efficacy across four diverse domains.

Conclusion: Task-aware sampling and hybrid supervision using curriculum learning enhance RL-based training for scaling LLMs, pushing capabilities toward general-purpose AI.

Abstract: The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce Omni-Think,
a unified reinforcement learning (RL) framework that enhances LLM performance
across diverse tasks by combining rule-based verifiable rewards with generative
preference signals via LLM-as-a-Judge evaluations. Our approach enables
consistent optimization across task types and scales RL-based training to
subjective domains. We further investigate training strategies, demonstrating
that a curriculum-based progression that orders tasks from structured to
open-ended improves performance and reduces forgetting. Experimental results
across four domains reveal that curriculum learning improves performance by
5.2\% over joint training and 9.1\% over model merging. These results highlight
the importance of task-aware sampling and hybrid supervision in scaling
RL-based post-training for general-purpose LLMs.

</details>


### [367] [Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs](https://arxiv.org/abs/2507.14785)
*Erfan Pirmorad*

Main category: cs.LG

TL;DR: The study employs large language models (LLMs) for reasoning over financial knowledge graphs to detect money laundering activities through structured text and in-context learning.


<details>
  <summary>Details</summary>
Motivation: The research aims to explore how LLMs can assist in investigative reasoning over complex, interconnected financial entities involved in money laundering.

Method: The method involves a pipeline that extracts k-hop subgraphs from financial knowledge graphs, converts them into structured text, and prompts LLMs to assess suspiciousness via in-context learning.

Result: LLMs successfully demonstrate analyst-like reasoning by identifying red flags and generating clear justifications in synthetic anti-money laundering scenarios.

Conclusion: This exploratory study highlights the potential for LLMs in graph reasoning and financial crime detection, paving the way for explainable, AI-driven analytics in the AML domain.

Abstract: The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.

</details>


### [368] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: The paper enhances neural networks to better respect continuous symmetries over time, introducing flow equivariance to improve RNN performance in sequence tasks.


<details>
  <summary>Details</summary>
Motivation: Neural networks respecting symmetries of the data improve generalization and efficiency, but current equivariance studies focus only on static transformations, excluding flow dynamics in time-sequenced models.

Method: The study extends equivariance theory to flows, one-parameter Lie subgroups, and modifies RNNs to incorporate flow equivariance for better handling of time-sequenced data transformations.

Result: Flow-equivariant models outperform standard RNNs in training speed, and generalization related to sequence length and stimulus velocity in prediction and classification tasks.

Conclusion: Incorporating flow equivariance in sequence models respects time-symmetries of dynamic environments and demonstrates performance benefits, paving the way for more effective time-parameterized neural networks.

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [369] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: This paper investigates 'subliminal learning,' where language models unintentionally transfer behavioral traits via semantically irrelevant data during training.


<details>
  <summary>Details</summary>
Motivation: To explore how language models might unknowingly transmit behavioral traits, despite attempts to filter out references to these traits, posing potential risks for AI development.

Method: The authors conduct experiments with a teacher model generating semantically neutral data, such as number sequences, and train a student model using this data. They also study code/reasoning trace datasets and explore the phenomenon theoretically and via simplified neural networks.

Result: Subliminal learning is observed when teacher and student models share similar architectures, even with filtered datasets. Theoretical proofs and experiments confirm this effect as innate to neural networks.

Conclusion: Subliminal learning is a widespread phenomenon that could propagate unintended behaviors in AI models despite developers' data filtering efforts.

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [370] [Benchmarking Foundation Models with Multimodal Public Electronic Health Records](https://arxiv.org/abs/2507.14824)
*Kunyu Yu,Rui Yang,Jingchi Liao,Siqi Li,Huitao Li,Irene Li,Yifan Peng,Rishikesan Kamaleswaran,Nan Liu*

Main category: cs.LG

TL;DR: The study benchmarks foundation models for EHRs using MIMIC-IV and evaluates performance, fairness, and interpretability, focusing on unimodal and multimodal learners.


<details>
  <summary>Details</summary>
Motivation: To create effective and trustworthy AI systems for clinical applications by assessing foundation models.

Method: Compared eight foundation models using a standardized pipeline on MIMIC-IV data, evaluating multimodal and unimodal performance.

Result: Multimodal models showed consistent predictive improvements without extra bias.

Conclusion: Benchmarking supports better multimodal AI system development for clinical use; code is made publicly available.

Abstract: Foundation models have emerged as a powerful approach for processing
electronic health records (EHRs), offering flexibility to handle diverse
medical data modalities. In this study, we present a comprehensive benchmark
that evaluates the performance, fairness, and interpretability of foundation
models, both as unimodal encoders and as multimodal learners, using the
publicly available MIMIC-IV database. To support consistent and reproducible
evaluation, we developed a standardized data processing pipeline that
harmonizes heterogeneous clinical records into an analysis-ready format. We
systematically compared eight foundation models, encompassing both unimodal and
multimodal models, as well as domain-specific and general-purpose variants. Our
findings demonstrate that incorporating multiple data modalities leads to
consistent improvements in predictive performance without introducing
additional bias. Through this benchmark, we aim to support the development of
effective and trustworthy multimodal artificial intelligence (AI) systems for
real-world clinical applications. Our code is available at
https://github.com/nliulab/MIMIC-Multimodal.

</details>


### [371] [eMargin: Revisiting Contrastive Learning with Margin-Based Separation](https://arxiv.org/abs/2507.14828)
*Abdul-Kazeem Shamba,Kerstin Bach,Gavin Taylor*

Main category: cs.LG

TL;DR: The paper introduces an adaptive margin (eMargin) into the contrastive loss function and evaluates its effect on time series representation learning, showing mixed results for clustering and classification tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve the separation between adjacent but dissimilar time steps in time series data to enhance performance in downstream tasks by modifying the contrastive learning framework.

Method: The study incorporates an adaptive margin (eMargin) into the InfoNCE contrastive loss function that adjusts based on a predefined similarity threshold. Evaluations on clustering and classification were conducted using three benchmark datasets.

Result: eMargin enhances unsupervised clustering metrics outperforming state-of-the-art baselines, but falls short in producing effective embeddings for downstream classification using linear probing.

Conclusion: High scores in clustering metrics do not guarantee meaningful embeddings for downstream tasks. While eMargin improves clustering results, its effectiveness in classification is limited.

Abstract: We revisit previous contrastive learning frameworks to investigate the effect
of introducing an adaptive margin into the contrastive loss function for time
series representation learning. Specifically, we explore whether an adaptive
margin (eMargin), adjusted based on a predefined similarity threshold, can
improve the separation between adjacent but dissimilar time steps and
subsequently lead to better performance in downstream tasks. Our study
evaluates the impact of this modification on clustering performance and
classification in three benchmark datasets. Our findings, however, indicate
that achieving high scores on unsupervised clustering metrics does not
necessarily imply that the learned embeddings are meaningful or effective in
downstream tasks. To be specific, eMargin added to InfoNCE consistently
outperforms state-of-the-art baselines in unsupervised clustering metrics, but
struggles to achieve competitive results in downstream classification with
linear probing. The source code is publicly available at
https://github.com/sfi-norwai/eMargin.

</details>


### [372] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: The paper examines RLVR, finding it enhances precision but limits exploration and fails to expand reasoning beyond the base model's knowledge.


<details>
  <summary>Details</summary>
Motivation: To understand whether RLVR helps AI expand reasoning boundaries or merely improves precision of pre-known solutions.

Method: The study uses theoretical and empirical analysis to explore RLVR's capabilities and limitations, including experiments on precision and support shrinkage.

Result: RLVR improves precision but narrows exploration, fails to recover certain solutions, and reduces answer-level entropy, limiting its reasoning enhancement.

Conclusion: RLVR has inherent limitations as a conservative mechanism. Future improvements may require exploration-centric algorithms or hybrid strategies.

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [373] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: This paper introduces TALE-EHR, a Transformer-based model that addresses challenges in Electronic Health Record (EHR) analysis by incorporating time-aware attention and strong semantic embeddings.


<details>
  <summary>Details</summary>
Motivation: The study aims to improve the analysis of Electronic Health Records (EHRs), which often have irregular temporal patterns and heterogeneous data, hindering accurate predictions and healthcare decisions.

Method: The proposed method, TALE-EHR, uses a Transformer framework with a unique time-aware attention mechanism to handle temporal gaps, alongside embeddings derived from a pre-trained Large Language Model (LLM) for understanding clinical concepts.

Result: Experiments conducted on the MIMIC-IV and PIC datasets showed superior performance of TALE-EHR compared to existing baselines, particularly for tasks like disease progression forecasting.

Conclusion: Integrating continuous temporal modeling with strong semantic insights proved to be effective and offers a robust solution for analyzing EHRs and aiding healthcare predictions.

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [374] [The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs](https://arxiv.org/abs/2507.14874)
*Ole-Christoffer Granmo,Youmna Abdelwahab,Per-Arne Andersen,Paul F. A. Clarke,Kunal Dumbre,Ylva Grønninsæter,Vojtech Halenka,Runar Helin,Lei Jiao,Ahmed Khalid,Rebekka Omslandseter,Rupsa Saha,Mayur Shende,Xuan Zhang*

Main category: cs.LG

TL;DR: The Graph Tsetlin Machine (GraphTM) enhances the traditional Tsetlin Machine by supporting graph-structured input, achieving interpretable deep learning and higher accuracy across multiple domains.


<details>
  <summary>Details</summary>
Motivation: The authors aim to expand the Tsetlin Machine's capabilities to handle complex, graph-based structures with improved interpretability and efficiency compared to other models.

Method: GraphTM introduces interpretable deep clauses for processing graph-structured data via message passing, allowing it to recognize sub-graph patterns using fewer clauses than traditional methods.

Result: GraphTM outperformed convolutional TM, reinforcement learning methods, Graph Convolutional Neural Networks (GCNs), and BiLSTM-CNN models across tasks like image classification, recommendation systems, and genome sequence analysis, achieving higher accuracy and faster training.

Conclusion: GraphTM provides a versatile and interpretable framework for handling graph-structured input, demonstrating its potential to revolutionize tasks in pattern recognition and graph representation learning.

Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine
(TM) both interpretable and efficient, while the power of Tsetlin automata
enables accuracy comparable to deep learning on an increasing number of
datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning
interpretable deep clauses from graph-structured input. Moving beyond flat,
fixed-length input, the GraphTM gets more versatile, supporting sequences,
grids, relations, and multimodality. Through message passing, the GraphTM
builds nested deep clauses to recognize sub-graph patterns with exponentially
fewer clauses, increasing both interpretability and data utilization. For image
classification, GraphTM preserves interpretability and achieves 3.86%-points
higher accuracy on CIFAR-10 than a convolutional TM. For tracking action
coreference, faced with increasingly challenging tasks, GraphTM outperforms
other reinforcement learning methods by up to 20.6%-points. In recommendation
systems, it tolerates increasing noise to a greater extent than a Graph
Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains
accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence
data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training
2.5x faster than GCN. The GraphTM's application to these varied fields
demonstrates how graph representation learning and deep clauses bring new
possibilities for TM learning.

</details>


### [375] [Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization](https://arxiv.org/abs/2507.14882)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.LG

TL;DR: The paper introduces an advanced importance metric framework for structured pruning in deep neural networks, focusing on preserving application-specific performance during compression.


<details>
  <summary>Details</summary>
Motivation: Conventional structured pruning methods often fail to maintain critical performance attributes specific to the application when reducing model complexity.

Method: The authors propose a framework that optimizes pruning magnitude for groups based on enhanced importance metrics while preserving application-specific constraints.

Result: Experimentation on autoencoders reconstructing MNIST images shows successful preservation of task-relevant performance post-substantial pruning.

Conclusion: The proposed structured pruning method achieves a balance between model compression and application-specific performance retention.

Abstract: Deep neural networks (DNNs) offer significant versatility and performance
benefits, but their widespread adoption is often hindered by high model
complexity and computational demands. Model compression techniques such as
pruning have emerged as promising solutions to these challenges. However, it
remains critical to ensure that application-specific performance
characteristics are preserved during compression. In structured pruning, where
groups of structurally coherent elements are removed, conventional importance
metrics frequently fail to maintain these essential performance attributes. In
this work, we propose an enhanced importance metric framework that not only
reduces model size but also explicitly accounts for application-specific
performance constraints. We employ multiple strategies to determine the optimal
pruning magnitude for each group, ensuring a balance between compression and
task performance. Our approach is evaluated on an autoencoder tasked with
reconstructing MNIST images. Experimental results demonstrate that the proposed
method effectively preserves task-relevant performance, maintaining the model's
usability even after substantial pruning, by satisfying the required
application-specific criteria.

</details>


### [376] [Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning](https://arxiv.org/abs/2507.14919)
*Maximilian Wendlinger,Kilian Tscharke,Pascal Debus*

Main category: cs.LG

TL;DR: The paper develops and evaluates methods for adapting classical uncertainty quantification to quantum machine learning to address the black-box nature of quantum models.


<details>
  <summary>Details</summary>
Motivation: Traditional and quantum machine learning models often lack transparency, leading to challenges such as overfitting and overconfidence in predictions. This issue remains underexplored in quantum machine learning contexts.

Method: The study builds upon classical uncertainty quantification techniques and integrates them with quantum Bayesian modeling to design and test uncertainty-aware quantum machine learning frameworks.

Result: The findings show that incorporating classical uncertainty quantification techniques into quantum machine learning helps improve model transparency and uncertainty awareness.

Conclusion: Classical insights should be utilized to expand uncertainty quantification to quantum machine learning, promoting better transparency and robustness in newly designed models.

Abstract: One of the key obstacles in traditional deep learning is the reduction in
model transparency caused by increasingly intricate model functions, which can
lead to problems such as overfitting and excessive confidence in predictions.
With the advent of quantum machine learning offering possible advances in
computational power and latent space complexity, we notice the same opaque
behavior. Despite significant research in classical contexts, there has been
little advancement in addressing the black-box nature of quantum machine
learning. Consequently, we approach this gap by building upon existing work in
classical uncertainty quantification and initial explorations in quantum
Bayesian modeling to theoretically develop and empirically evaluate techniques
to map classical uncertainty quantification methods to the quantum machine
learning domain. Our findings emphasize the necessity of leveraging classical
insights into uncertainty quantification to include uncertainty awareness in
the process of designing new quantum machine learning models.

</details>


### [377] [FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios](https://arxiv.org/abs/2507.14980)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Qipeng Xie,Chang Liu,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: Federated Learning faces challenges with non-IID and imbalanced data distributions. Momentum-based methods struggle to converge effectively in such scenarios. The proposed FedWCM dynamically adjusts momentum to address these issues, improving FL efficiency and model performance.


<details>
  <summary>Details</summary>
Motivation: FL aims to preserve data privacy during decentralized model training. However, handling non-IID and imbalanced long-tailed data distributions remains a significant challenge, especially for momentum-based methods that struggle with convergence.

Method: The paper proposes FedWCM, a framework that dynamically adjusts momentum utilizing global and round-specific data insights. This approach seeks to correct directional biases caused by non-IID long-tailed distributions.

Result: Extensive experiments demonstrate that FedWCM resolves non-convergence issues and surpasses existing methods in FL efficiency and handling client heterogeneity and data imbalance.

Conclusion: FedWCM effectively improves Federated Learning's ability to manage imbalanced and heterogeneous data, addressing critical challenges in decentralized machine learning systems.

Abstract: Federated Learning (FL) enables decentralized model training while preserving
data privacy. Despite its benefits, FL faces challenges with non-identically
distributed (non-IID) data, especially in long-tailed scenarios with imbalanced
class samples. Momentum-based FL methods, often used to accelerate FL
convergence, struggle with these distributions, resulting in biased models and
making FL hard to converge. To understand this challenge, we conduct extensive
investigations into this phenomenon, accompanied by a layer-wise analysis of
neural network behavior. Based on these insights, we propose FedWCM, a method
that dynamically adjusts momentum using global and per-round data to correct
directional biases introduced by long-tailed distributions. Extensive
experiments show that FedWCM resolves non-convergence issues and outperforms
existing methods, enhancing FL's efficiency and effectiveness in handling
client heterogeneity and data imbalance.

</details>


### [378] [Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data](https://arxiv.org/abs/2507.14999)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: The paper introduces FedClusAvg, a federated learning framework to enhance False Data Injection Attack (FDIA) detection in smart grids with Non-IID data while preserving privacy and reducing communication costs.


<details>
  <summary>Details</summary>
Motivation: Traditional centralized detection models for FDIAs face privacy risks, scalability issues, and high transmission costs when handling Non-IID data in smart grids.

Method: Proposes the Federated Cluster Average (FedClusAvg), which uses cluster-based stratified sampling and hierarchical client-subserver-server communication for localized training and weighted parameter aggregation.

Result: FedClusAvg shows improved detection accuracy for FDIA under heterogeneous data distributions, with significant reductions in communication rounds and bandwidth usage on benchmark smart grid datasets.

Conclusion: FedClusAvg is a scalable and privacy-preserving solution for FDIA detection in large-scale, resource-constrained smart grid systems, overcoming challenges associated with Non-IID data and centralization.

Abstract: False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.

</details>


### [379] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: This paper introduces Time-RA, a novel generative task for reasoning about time-series anomalies, and the RATs40K dataset, a multimodal benchmark explicitly designed for anomaly reasoning.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome limitations of current time-series anomaly detection methods that focus only on binary anomaly classification and lack detailed categorization or reasoning.

Method: The paper proposes Time-RA to frame anomaly detection as a reasoning-intensive task using Large Language Models (LLMs). Additionally, it introduces the RATs40K dataset annotated with detailed categories and reasoning, and leverages GPT-4 for label refinement.

Result: Extensive benchmarking showed both strengths and weaknesses of current LLMs and multimodal LLMs, asserting the importance of supervised fine-tuning.

Conclusion: The work establishes a new direction for interpretable time-series anomaly detection by providing a reasoning-focused task and dataset, enabling advancements in the field.

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [380] [ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model](https://arxiv.org/abs/2507.15067)
*Bing He,Mustaque Ahamad,Srijan Kumar*

Main category: cs.LG

TL;DR: ROBAD, a transformer-based model, improves the robustness of bad actor detection by capturing local and global information, leveraging adversarial behavior, and demonstrating strong performance under adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: The goal is to ensure accurate detection of bad actors on internet platforms while enhancing model robustness against adversarial attacks, addressing the limitations of previous detection models.

Method: The paper introduces ROBAD, a transformer-based approach that encodes user posts (local information) and sequences of posts (global information). It incorporates contrastive learning using adversarial examples to enhance robustness.

Result: Experiments using Yelp and Wikipedia datasets confirm ROBAD's effectiveness in detecting bad actors even under advanced adversarial attacks.

Conclusion: ROBAD achieves robust and reliable bad actor detection by combining attention mechanisms and adversarial training, setting a new standard in adversary-resilient deep learning models.

Abstract: Detecting bad actors is critical to ensure the safety and integrity of
internet platforms. Several deep learning-based models have been developed to
identify such users. These models should not only accurately detect bad actors,
but also be robust against adversarial attacks that aim to evade detection.
However, past deep learning-based detection models do not meet the robustness
requirement because they are sensitive to even minor changes in the input
sequence. To address this issue, we focus on (1) improving the model
understanding capability and (2) enhancing the model knowledge such that the
model can recognize potential input modifications when making predictions. To
achieve these goals, we create a novel transformer-based classification model,
called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection
model), which uses the sequence of user posts to generate user embedding to
detect bad actors. Particularly, ROBAD first leverages the transformer encoder
block to encode each post bidirectionally, thus building a post embedding to
capture the local information at the post level. Next, it adopts the
transformer decoder block to model the sequential pattern in the post
embeddings by using the attention mechanism, which generates the sequence
embedding to obtain the global information at the sequence level. Finally, to
enrich the knowledge of the model, embeddings of modified sequences by mimicked
attackers are fed into a contrastive-learning-enhanced classification layer for
sequence prediction. In essence, by capturing the local and global information
(i.e., the post and sequence information) and leveraging the mimicked behaviors
of bad actors in training, ROBAD can be robust to adversarial attacks.
Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can
effectively detect bad actors when under state-of-the-art adversarial attacks.

</details>


### [381] [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073)
*Samuel Pfrommer,Yixiao Huang,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: This paper explores enhancing flow-matching policies for robotics beyond suboptimal demonstrations, introducing reinforcement learning schemes that improve performance significantly.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limitation of flow-matching policies trained on suboptimal human demonstrations, aiming for improved policies that surpass the demonstrator's performance.

Method: Two reinforcement learning approaches, Reward-Weighted Flow Matching (RWFM) and Group Relative Policy Optimization (GRPO), are proposed for training flow-matching policies using variable-horizon planning and a learned reward surrogate.

Result: Experiments on unicycle dynamics tasks show that GRPO and RWFM approaches greatly outperform suboptimal demonstrators, with GRPO reducing costs by up to 85% compared to imitation learning methods.

Conclusion: Both RWFM and GRPO approaches are effective in overcoming suboptimal demonstrations, showing significant performance improvements and applicability to generalist robotics.

Abstract: Flow-matching policies have emerged as a powerful paradigm for generalist
robotics. These models are trained to imitate an action chunk, conditioned on
sensor observations and textual instructions. Often, training demonstrations
are generated by a suboptimal policy, such as a human operator. This work
explores training flow-matching policies via reinforcement learning to surpass
the original demonstration policy performance. We particularly note
minimum-time control as a key application and present a simple scheme for
variable-horizon flow-matching planning. We then introduce two families of
approaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group
Relative Policy Optimization (GRPO) approach with a learned reward surrogate.
Our policies are trained on an illustrative suite of simulated unicycle
dynamics tasks, and we show that both approaches dramatically improve upon the
suboptimal demonstrator performance, with the GRPO approach in particular
generally incurring between $50\%$ and $85\%$ less cost than a naive Imitation
Learning Flow Matching (ILFM) approach.

</details>


### [382] [Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts](https://arxiv.org/abs/2507.15079)
*Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: cs.LG

TL;DR: The paper introduces Isotonic Quantile Regression Averaging (iQRA), a novel method for creating probabilistic forecasts for electricity prices, outperforming established methods on accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: The need to quantify uncertainty in machine learning models for electricity price forecasting is critical due to the high stakes and volatility in electricity markets. Existing models are accurate but lack uncertainty estimates, which are essential for informed decision-making.

Method: The proposed iQRA method extends Quantile Regression Averaging (QRA) by incorporating stochastic order constraints to improve forecast accuracy and reliability. The method includes isotonic regularization to reduce computational complexity and eliminate the need for hyperparameter-based variable selection.

Result: iQRA demonstrates superior performance over state-of-the-art methods in forecasting the German day-ahead electricity market. It provides well-calibrated prediction intervals and improves both computational cost efficiency and prediction reliability.

Conclusion: iQRA is a robust, hyperparameter-free probabilistic forecasting tool that addresses the limitations of existing methods, offering market participants better tools for decision-making under uncertainty.

Abstract: Quantifying the uncertainty of forecasting models is essential to assess and
mitigate the risks associated with data-driven decisions, especially in
volatile domains such as electricity markets. Machine learning methods can
provide highly accurate electricity price forecasts, critical for informing the
decisions of market participants. However, these models often lack uncertainty
estimates, which limits the ability of decision makers to avoid unnecessary
risks. In this paper, we propose a novel method for generating probabilistic
forecasts from ensembles of point forecasts, called Isotonic Quantile
Regression Averaging (iQRA). Building on the established framework of Quantile
Regression Averaging (QRA), we introduce stochastic order constraints to
improve forecast accuracy, reliability, and computational costs. In an
extensive forecasting study of the German day-ahead electricity market, we show
that iQRA consistently outperforms state-of-the-art postprocessing methods in
terms of both reliability and sharpness. It produces well-calibrated prediction
intervals across multiple confidence levels, providing superior reliability to
all benchmark methods, particularly coverage-based conformal prediction. In
addition, isotonic regularization decreases the complexity of the quantile
regression problem and offers a hyperparameter-free approach to variable
selection.

</details>


### [383] [Robust Control with Gradient Uncertainty](https://arxiv.org/abs/2507.15082)
*Qian Qi*

Main category: cs.LG

TL;DR: The paper introduces the GU-HJBI framework to address gradient uncertainty in value functions, critical for contexts like reinforcement learning, and validates its theoretical and practical implications.


<details>
  <summary>Details</summary>
Motivation: To tackle gradient uncertainty in value function estimation, a challenge prominent in reinforcement learning and fields relying on function approximation.

Method: The authors formulate a zero-sum dynamic game involving system dynamics and gradient uncertainty, derive the GU-HJBI equation, analyze its properties, and introduce the GURAC algorithm for practical applications.

Result: The study proves novel theoretical results for the GU-HJBI equation, characterizes its behavior in the LQ case, and demonstrates the effectiveness of the GURAC algorithm in stabilizing training.

Conclusion: The work establishes a solid theoretical and practical foundation to address gradient uncertainty, offering a new avenue for robust control in areas like reinforcement learning and beyond.

Abstract: We introduce a novel extension to robust control theory that explicitly
addresses uncertainty in the value function's gradient, a form of uncertainty
endemic to applications like reinforcement learning where value functions are
approximated. We formulate a zero-sum dynamic game where an adversary perturbs
both system dynamics and the value function gradient, leading to a new, highly
nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs
Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness
by proving a comparison principle for its viscosity solutions under a uniform
ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a
key insight: we prove that the classical quadratic value function assumption
fails for any non-zero gradient uncertainty, fundamentally altering the problem
structure. A formal perturbation analysis characterizes the non-polynomial
correction to the value function and the resulting nonlinearity of the optimal
control law, which we validate with numerical studies. Finally, we bridge
theory to practice by proposing a novel Gradient-Uncertainty-Robust
Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating
its effectiveness in stabilizing training. This work provides a new direction
for robust control, holding significant implications for fields where function
approximation is common, including reinforcement learning and computational
finance.

</details>


### [384] [AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI](https://arxiv.org/abs/2507.15104)
*Qiufeng Li,Shu Hong,Jian Gao,Xuan Zhang,Tian Lan,Weidong Cao*

Main category: cs.LG

TL;DR: This paper introduces AnalogFed, a federated learning-based framework for collaborative generation of analog circuit topologies without sharing proprietary data.


<details>
  <summary>Details</summary>
Motivation: Generative AI holds potential to revolutionize analog design automation, but a lack of accessible, large, and diverse datasets hinders progress due to proprietary restrictions in analog circuit design.

Method: The paper proposes AnalogFed, which enables decentralized collaboration by employing federated learning. It incorporates techniques for generative model development, data heterogeneity handling, and privacy-preserving mechanisms that cater to the unique requirements of analog design.

Result: Extensive experiments show AnalogFed delivers performance comparable to centralized methods in generating analog circuit topologies, while preserving strict data privacy.

Conclusion: AnalogFed demonstrates the potential of federated learning to overcome data-sharing barriers and accelerates the discovery of efficient analog circuit designs while ensuring privacy and scalability.

Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize
analog design automation through data-driven approaches. In particular,
researchers are increasingly fascinated by harnessing the power of generative
AI to automate the discovery of novel analog circuit topologies. Unlocking the
full potential of generative AI in these data-driven discoveries requires
access to large and diverse datasets.Yet, there is a significant barrier in the
analog domain--Analog circuit design is inherently proprietary, involving not
only confidential circuit structures but also the underlying commercial
semiconductor processes. As a result, current generative AI research is largely
confined to individual researchers who construct small, narrowly focused
private datasets. This fragmentation severely limits collaborative innovation
and impedes progress across the research community. To address these
challenges, we propose AnalogFed. AnalogFed enables collaborative topology
discovery across decentralized clients (e.g., individual researchers or
institutions) without requiring the sharing of raw private data. To make this
vision practical, we introduce a suite of techniques tailored to the unique
challenges of applying FedL in analog design--from generative model development
and data heterogeneity handling to privacy-preserving strategies that ensure
both flexibility and security for circuit designers and semiconductor
manufacturers. Extensive experiments across varying client counts and dataset
sizes demonstrate that AnalogFed achieves performance comparable to centralized
baselines--while maintaining strict data privacy. Specifically, the generative
AI model within AnalogFed achieves state-of-the-art efficiency and scalability
in the design of analog circuit topologies.

</details>


### [385] [Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2507.15119)
*Juntong Ni,Shiyu Wang,Zewen Liu,Xiaoming Shi,Xinyue Zhong,Zhou Ye,Wei Jin*

Main category: cs.LG

TL;DR: This paper introduces U-Cast, an architecture for high-dimensional time series forecasting (HDTSF), and a new benchmark, Time-HD, outperforming existing models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the specific challenges of HDTSF, where traditional models fail due to scaling issues and neglecting hierarchical, complex channel correlations.

Method: The proposed method, U-Cast, includes a query-based attention mechanism for hierarchical channel learning and full-rank regularization to disentangle correlated representations. Additionally, the authors introduce Time-HD, a benchmark dataset for HDTSF evaluation.

Result: U-Cast demonstrates improved performance in accuracy and efficiency compared to baseline methods in experiments on the Time-HD dataset.

Conclusion: The combination of U-Cast and Time-HD sets a strong foundation for advancing research in high-dimensional time series forecasting (HDTSF).

Abstract: Time series forecasting (TSF) is a central problem in time series analysis.
However, as the number of channels in time series datasets scales to the
thousands or more, a scenario we define as High-Dimensional Time Series
Forecasting (HDTSF), it introduces significant new modeling challenges that are
often not the primary focus of traditional TSF research. HDTSF is challenging
because the channel correlation often forms complex and hierarchical patterns.
Existing TSF models either ignore these interactions or fail to scale as
dimensionality grows. To address this issue, we propose U-Cast, a
channel-dependent forecasting architecture that learns latent hierarchical
channel structures with an innovative query-based attention. To disentangle
highly correlated channel representation, U-Cast adds a full-rank
regularization during training. We also release Time-HD, a benchmark of large,
diverse, high-dimensional datasets. Our theory shows that exploiting
cross-channel information lowers forecasting risk, and experiments on Time-HD
demonstrate that U-Cast surpasses strong baselines in both accuracy and
efficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF
research.

</details>


### [386] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: Researchers explore multi-label classification with logical constraints using individual label classifiers integrated into a sequential model to enforce constraints in training and inference.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in multi-label classification, particularly the need to account for correlations and logical dependencies in large label sets.

Method: The architecture employs individual label classifiers combined into a sequential model to produce a joint label distribution, leveraging logical constraints.

Result: The approach demonstrates empirical success in both utilizing constraints during training and ensuring constraint enforcement during inference.

Conclusion: The proposed architecture effectively integrates logical constraints into multi-label classification processes, enhancing model expressiveness and prediction reliability.

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [387] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: This paper proposes a hardware-efficient neuromorphic computing system using resonant-tunnelling diodes (RTDs) for physical reservoir computing, validated on image recognition benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current AI systems face challenges in real-time, edge-based, and resource-constrained scenarios, necessitating more hardware-efficient computational models.

Method: The paper formulates and numerically implements a neuromorphic RC architecture using RTDs, applying it to handwritten digit classification and Fruit~360 object recognition tasks.

Result: The RTD-based RC system demonstrates promising performance and eliminates random connectivity through deterministic nonlinear signal transformation.

Conclusion: An RTD-based reservoir computing system shows potential for hardware-efficient AI, aligning with principles of next-gen RC and addressing edge computing needs.

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [388] [Designing User-Centric Metrics for Evaluation of Counterfactual Explanations](https://arxiv.org/abs/2507.15162)
*Firdaus Ahmed Choudhury,Ethan Leicht,Jude Ethan Bislig,Hangzhi Guo,Amulya Yadav*

Main category: cs.LG

TL;DR: This paper investigates the alignment of Counterfactual Explanations (CFEs) with real-world user preferences and proposes a novel user-centered model for improved decision-making processes.


<details>
  <summary>Details</summary>
Motivation: Machine learning models are becoming increasingly involved in impactful decision-making, but their opaque nature leaves end-users uncertain about decision reasons. Existing Counterfactual Explanations (CFEs) methods are limited as they may not consider user preferences.

Method: The paper conducts two user studies: a pilot with 20 individuals on Amazon MTurk to assess alignment of standard metrics with user preferences, and a two-day study with 41 participants in credit application scenarios to explore user evaluation behavior. A user-centric two-stage model called AWP is proposed.

Result: Findings indicate that only 63.81% alignment exists between user-preferred CFEs and proximity-based CFEs. The proposed AWP model predicts user-preferred CFEs with 84.37% accuracy.

Conclusion: CFEs should consider personalized, user-centered evaluation metrics rather than relying solely on artificial metrics like proximity, as users' evaluation mechanisms differ from models’ perspectives.

Abstract: Machine learning-based decision models are increasingly being used to make
decisions that significantly impact people's lives, but their opaque nature
leaves end users without a clear understanding of why a decision was made.
Counterfactual Explanations (CFEs) have grown in popularity as a means of
offering actionable guidance by identifying the minimum changes in feature
values required to flip a model's prediction to something more desirable.
Unfortunately, most prior research in CFEs relies on artificial evaluation
metrics, such as proximity, which may overlook end-user preferences and
constraints, e.g., the user's perception of effort needed to make certain
feature changes may differ from that of the model designer. To address this
research gap, this paper makes three novel contributions. First, we conduct a
pilot study with 20 crowd-workers on Amazon MTurk to experimentally validate
the alignment of existing CF evaluation metrics with real-world user
preferences. Results show that user-preferred CFEs matched those based on
proximity in only 63.81% of cases, highlighting the limited applicability of
these metrics in real-world settings. Second, inspired by the need to design a
user-informed evaluation metric for CFEs, we conduct a more detailed two-day
user study with 41 participants facing realistic credit application scenarios
to find experimental support for or against three intuitive hypotheses that may
explain how end users evaluate CFEs. Third, based on the findings of this
second study, we propose the AWP model, a novel user-centric, two-stage model
that describes one possible mechanism by which users evaluate and select CFEs.
Our results show that AWP predicts user-preferred CFEs with 84.37% accuracy.
Our study provides the first human-centered validation for personalized cost
models in CFE generation and highlights the need for adaptive, user-centered
evaluation metrics.

</details>


### [389] [Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control](https://arxiv.org/abs/2507.15174)
*Justin Turnau,Longchao Da,Khoa Vo,Ferdous Al Rafi,Shreyas Bachiraju,Tiejin Chen,Hua Wei*

Main category: cs.LG

TL;DR: This paper presents JL-GAT, a scalable method for applying Grounded Action Transformation to multi-agent reinforcement learning (MARL) in traffic signal control, mitigating the sim-to-real gap.


<details>
  <summary>Details</summary>
Motivation: To address the issue of the sim-to-real gap in traffic signal control systems that use multi-agent reinforcement learning, especially in dynamic and interconnected real-world traffic environments.

Method: The authors introduce JL-GAT, a decentralized Grounded Action Transformation framework for MARL, which incorporates information from neighboring agents to balance scalability and grounding capability.

Result: The proposed method was tested on simulated road networks under adverse conditions, showing effective performance improvements over prior methods. Ablation studies further validated the importance of method components.

Conclusion: JL-GAT successfully bridges the sim-to-real gap for MARL-based traffic signal control by effectively incorporating interactions between agents in decentralized systems while maintaining scalability.

Abstract: Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.

</details>


### [390] [Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning](https://arxiv.org/abs/2507.15195)
*Anwar Said,Yifan Wei,Ubaid Ullah Ahmad,Mudassir Shabbir,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: This paper introduces average controllability metrics and a new rank encoding method to improve Graph Neural Networks (GNNs) for classifying social networks with limited node features.


<details>
  <summary>Details</summary>
Motivation: GNNs often underperform in social network settings due to the unavailability of node features, which are either missing or constrained by privacy concerns.

Method: The authors propose average controllability and other centrality metrics as node-level metrics and a rank encoding method to encode these metrics into a fixed-dimensional feature space, enhancing feature representation.

Result: The study finds that using average controllability in feature construction significantly improves GNN performance. The proposed rank encoding method outperforms traditional one-hot degree encoding, increasing ROC AUC from 68.7% to 73.9% on the GitHub Stargazers dataset.

Conclusion: Incorporating average controllability and rank encoding dramatically enhances the performance of GNNs in social network classification, particularly when node features are missing or limited.

Abstract: In this article, we utilize the concept of average controllability in graphs,
along with a novel rank encoding method, to enhance the performance of Graph
Neural Networks (GNNs) in social network classification tasks. GNNs have proven
highly effective in various network-based learning applications and require
some form of node features to function. However, their performance is heavily
influenced by the expressiveness of these features. In social networks, node
features are often unavailable due to privacy constraints or the absence of
inherent attributes, making it challenging for GNNs to achieve optimal
performance. To address this limitation, we propose two strategies for
constructing expressive node features. First, we introduce average
controllability along with other centrality metrics (denoted as NCT-EFA) as
node-level metrics that capture critical aspects of network topology. Building
on this, we develop a rank encoding method that transforms average
controllability or any other graph-theoretic metric into a fixed-dimensional
feature space, thereby improving feature representation. We conduct extensive
numerical evaluations using six benchmark GNN models across four social network
datasets to compare different node feature construction methods. Our results
demonstrate that incorporating average controllability into the feature space
significantly improves GNN performance. Moreover, the proposed rank encoding
method outperforms traditional one-hot degree encoding, improving the ROC AUC
from 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,
underscoring its effectiveness in generating expressive and efficient node
representations.

</details>


### [391] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: This paper introduces LSDGNN, a novel multimodal approach for Emotion Recognition in Conversation (ERC), distinguishing long- and short-distance interactions using graph neural networks and addressing data imbalance through Improved Curriculum Learning.


<details>
  <summary>Details</summary>
Motivation: ERC is a challenging task requiring accurate multimodal emotion analysis across conversations.

Method: The authors propose the LSDGNN, a model using long- and short-distance graph neural networks to capture context features. A Differential Regularizer ensures feature distinction, while a BiAffine Module and Improved Curriculum Learning handle feature interaction and data imbalance, respectively.

Result: The proposed LSDGNN achieves superior performance compared to existing benchmarks using datasets like IEMOCAP and MELD.

Conclusion: LSDGNN effectively enhances multimodal ERC through innovative modeling and training techniques, proving its value over prior benchmarks.

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [392] [Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks](https://arxiv.org/abs/2507.15246)
*Rabia Latief Bhat,Iqra Altaf Gillani*

Main category: cs.LG

TL;DR: This paper presents an attention-based Graph Neural Network model to accurately predict food delivery demand while considering the spatial and temporal complexities of urban environments.


<details>
  <summary>Details</summary>
Motivation: Food delivery platforms face challenges from spatial heterogeneity and temporal fluctuations in order volumes, which can affect operational efficiency and decision-making. Accurate forecasting is required to address these challenges.

Method: The researchers developed an attention-based Graph Neural Network where nodes represent urban zones and edges represent spatial and order flow relationships. An attention mechanism dynamically prioritizes relevant zones, and the model jointly learns spatial and temporal trends.

Result: Experimentation on real-world food delivery data shows that the proposed model achieves superior accuracy in forecasting demand compared to existing methods.

Conclusion: The proposed framework is scalable and adaptive, making it a valuable tool for improving fleet positioning, resource allocation, and operational planning in urban food delivery systems.

Abstract: Accurate demand forecasting is critical for enhancing the efficiency and
responsiveness of food delivery platforms, where spatial heterogeneity and
temporal fluctuations in order volumes directly influence operational
decisions. This paper proposes an attention-based Graph Neural Network
framework that captures spatial-temporal dependencies by modeling the food
delivery environment as a graph. In this graph, nodes represent urban delivery
zones, while edges reflect spatial proximity and inter-regional order flow
patterns derived from historical data. The attention mechanism dynamically
weighs the influence of neighboring zones, enabling the model to focus on the
most contextually relevant areas during prediction. Temporal trends are jointly
learned alongside spatial interactions, allowing the model to adapt to evolving
demand patterns. Extensive experiments on real-world food delivery datasets
demonstrate the superiority of the proposed model in forecasting future order
volumes with high accuracy. The framework offers a scalable and adaptive
solution to support proactive fleet positioning, resource allocation, and
dispatch optimization in urban food delivery operations.

</details>


### [393] [CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](https://arxiv.org/abs/2507.15260)
*Jiaqi Han,Haotian Ye,Puheng Li,Minkai Xu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: CHORDS is a training-free, multi-core parallel acceleration method for diffusion-based generative models, yielding significant speedups without compromising sample quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion models are computationally expensive during inference, limiting their real-time usability.

Method: CHORDS utilizes a multi-core parallel approach where slower solvers rectify faster solvers through ODE solver pipelines and inter-core communication.

Result: CHORDS achieves up to 2.1x speedup with 4 cores and 2.9x speedup with 8 cores in diffusion sampling, with no degradation in quality.

Conclusion: CHORDS provides an efficient, scalable solution for real-time high-fidelity diffusion model generation, compatible with various architectures and modalities.

Abstract: Diffusion-based generative models have become dominant generators of
high-fidelity images and videos but remain limited by their computationally
expensive inference procedures. Existing acceleration techniques either require
extensive model retraining or compromise significantly on sample quality. This
paper explores a general, training-free, and model-agnostic acceleration
strategy via multi-core parallelism. Our framework views multi-core diffusion
sampling as an ODE solver pipeline, where slower yet accurate solvers
progressively rectify faster solvers through a theoretically justified
inter-core communication mechanism. This motivates our multi-core training-free
diffusion sampling accelerator, CHORDS, which is compatible with various
diffusion samplers, model architectures, and modalities. Through extensive
experiments, CHORDS significantly accelerates sampling across diverse
large-scale image and video diffusion models, yielding up to 2.1x speedup with
four cores, improving by 50% over baselines, and 2.9x speedup with eight cores,
all without quality degradation. This advancement enables CHORDS to establish a
solid foundation for real-time, high-fidelity diffusion generation.

</details>


### [394] [Temporal Basis Function Models for Closed-Loop Neural Stimulation](https://arxiv.org/abs/2507.15274)
*Matthew J. Bryan,Felix Schwock,Azadeh Yazdan-Shahmorad,Rajesh P N Rao*

Main category: cs.LG

TL;DR: This study introduces temporal basis function models (TBFMs) for closed-loop neural stimulation, addressing challenges like sample efficiency and latency. TBFMs demonstrate rapid and accurate modeling of neural responses using optogenetic data from primates.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage AI to optimize closed-loop neural stimulation for neurological diseases like Parkinson's, overcoming hurdles such as efficiency, training duration, and system latency.

Method: The paper proposes TBFMs, which are applied to optogenetic stimulation in neural testing, predicting local field potentials (LFPs) and using simulations to tailor neural activity to target patterns.

Result: TBFMs predict neural responses with accuracy on par with nonlinear models but require significantly less training time, and simulations show they effectively enable closed-loop neural control.

Conclusion: TBFMs have the potential to make AI-driven, closed-loop stimulation more clinically viable by being efficient, accurate, and low-latency, bridging the gap between complex modeling and practical applications.

Abstract: Closed-loop neural stimulation provides novel therapies for neurological
diseases such as Parkinson's disease (PD), but it is not yet clear whether
artificial intelligence (AI) techniques can tailor closed-loop stimulation to
individual patients or identify new therapies. Progress requires us to address
a number of translational issues, including sample efficiency, training time,
and minimizing loop latency such that stimulation may be shaped in response to
changing brain activity. We propose temporal basis function models (TBFMs) to
address these difficulties, and explore this approach in the context of
excitatory optogenetic stimulation. We demonstrate the ability of TBF models to
provide a single-trial, spatiotemporal forward prediction of the effect of
optogenetic stimulation on local field potentials (LFPs) measured in two
non-human primates. We further use simulations to demonstrate the use of TBF
models for closed-loop stimulation, driving neural activity towards target
patterns. The simplicity of TBF models allow them to be sample efficient, rapid
to train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the
model on 40 sessions of previously published excitatory optogenetic stimulation
data. For each session, the model required 15-20min of data collection to
successfully model the remainder of the session. It achieved a prediction
accuracy comparable to a baseline nonlinear dynamical systems model that
requires hours to train, and superior accuracy to a linear state-space model.
In our simulations, it also successfully allowed a closed-loop stimulator to
control a neural circuit. Our approach begins to bridge the translational gap
between complex AI-based approaches to modeling dynamical systems and the
vision of using such forward prediction models to develop novel, clinically
useful closed-loop stimulation protocols.

</details>


### [395] [Machine Unlearning for Streaming Forgetting](https://arxiv.org/abs/2507.15280)
*Shaofei Shen,Chenhao Zhang,Yawen Zhao,Alina Bialkowski,Weitong Chen,Miao Xu*

Main category: cs.LG

TL;DR: This paper introduces a novel streaming unlearning paradigm to address challenges in efficiency and effectiveness of removing specific training data knowledge in machine learning models.


<details>
  <summary>Details</summary>
Motivation: Current machine unlearning methods are inefficient in handling streaming forgetting requests, a common scenario in practical applications.

Method: The paper formalizes streaming unlearning as a distribution shift problem, estimates the altered distribution, and proposes an algorithm that does not require accessing original training data. Theoretical analysis provides guarantees on the effectiveness of the method.

Result: The proposed method achieves an $O(\sqrt{T} + V_T)$ streaming unlearning regret bound and performs well across various models and datasets in experiments.

Conclusion: Streaming unlearning can be efficiently achieved without requiring strong assumptions on convex loss functions or access to original data, solving practical challenges in machine unlearning.

Abstract: Machine unlearning aims to remove knowledge of the specific training data in
a well-trained model. Currently, machine unlearning methods typically handle
all forgetting data in a single batch, removing the corresponding knowledge all
at once upon request. However, in practical scenarios, requests for data
removal often arise in a streaming manner rather than in a single batch,
leading to reduced efficiency and effectiveness in existing methods. Such
challenges of streaming forgetting have not been the focus of much research. In
this paper, to address the challenges of performance maintenance, efficiency,
and data access brought about by streaming unlearning requests, we introduce a
streaming unlearning paradigm, formalizing the unlearning as a distribution
shift problem. We then estimate the altered distribution and propose a novel
streaming unlearning algorithm to achieve efficient streaming forgetting
without requiring access to the original training data. Theoretical analyses
confirm an $O(\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,
where $V_T$ represents the cumulative total variation in the optimal solution
over $T$ learning rounds. This theoretical guarantee is achieved under mild
conditions without the strong restriction of convex loss function. Experiments
across various models and datasets validate the performance of our proposed
method.

</details>


### [396] [Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning](https://arxiv.org/abs/2507.15287)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: The paper introduces a framework that uses incomplete and imperfect expert demonstrations to shape intrinsic rewards for reinforcement learning agents, enhancing their exploration and performance across various environments.


<details>
  <summary>Details</summary>
Motivation: The need for reinforcement learning agents to learn and adapt in real-world environments, even in the absence of explicit rewards or perfect demonstrations, and overcoming challenges posed by high-dimensional spaces and dense rewards.

Method: The authors use a framework that maps the similarity between agent states and expert demonstrations into intrinsic rewards, incorporating a Mixture of Autoencoder Experts to represent diverse behaviors and handle missing demonstration data.

Result: Experiments demonstrate that the proposed method leads to robust exploration and strong performance in both sparse and dense reward scenarios, even with sparse or incomplete expert demonstration data.

Conclusion: This framework provides a practical solution for reinforcement learning in environments lacking optimal data or precise reward definitions, effectively utilizing imperfect demonstrations to shape learning and behavior.

Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to
learn from reward-free interactions and alternative supervision signals, such
as unlabeled or incomplete demonstrations, rather than relying solely on
explicit reward maximization. Additionally, developing generalist agents that
can adapt efficiently in real-world environments often requires leveraging
these reward-free signals to guide learning and behavior. However, while
intrinsic motivation techniques provide a means for agents to seek out novel or
uncertain states in the absence of explicit rewards, they are often challenged
by dense reward environments or the complexity of high-dimensional state and
action spaces. Furthermore, most existing approaches rely directly on the
unprocessed intrinsic reward signals, which can make it difficult to shape or
control the agent's exploration effectively. We propose a framework that can
effectively utilize expert demonstrations, even when they are incomplete and
imperfect. By applying a mapping function to transform the similarity between
an agent's state and expert data into a shaped intrinsic reward, our method
allows for flexible and targeted exploration of expert-like behaviors. We
employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors
and accommodate missing information in demonstrations. Experiments show our
approach enables robust exploration and strong performance in both sparse and
dense reward environments, even when demonstrations are sparse or incomplete.
This provides a practical framework for RL in realistic settings where optimal
data is unavailable and precise reward control is needed.

</details>


### [397] [Preferential subspace identification (PSID) with forward-backward smoothing](https://arxiv.org/abs/2507.15288)
*Omid G. Sani,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: The paper extends Preferential Subspace Identification (PSID) to perform optimal filtering and smoothing for multivariate time-series analysis, primarily targeting systems with primary and secondary signals like neural and behavioral data.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the limitations of the original PSID framework, which focuses solely on prediction using past primary data, and does not incorporate concurrent or all available data for filtering or smoothing, essential for better offline estimation.

Method: They enhance PSID by introducing methods for both filtering and smoothing. For filtering, they use a reduced-rank regression step to directly learn the optimal Kalman gain. For smoothing, they propose a forward-backward smoothing algorithm that combines filtering results applied both forward and backward in time.

Result: The proposed methods are validated on simulated data. These approaches successfully recover ground-truth model parameters and achieve optimal filtering and smoothing performance, equivalent to the performance of the true underlying model.

Conclusion: This extended PSID framework provides a robust and principled approach to linear filtering and smoothing in multivariate time-series, improving the analysis of dynamic interactions between two signals like neural and behavioral data.

Abstract: System identification methods for multivariate time-series, such as neural
and behavioral recordings, have been used to build models for predicting one
from the other. For example, Preferential Subspace Identification (PSID) builds
a state-space model of a primary time-series (e.g., neural activity) to
optimally predict a secondary time-series (e.g., behavior). However, PSID
focuses on optimal prediction using past primary data, even though in offline
applications, better estimation can be achieved by incorporating concurrent
data (filtering) or all available data (smoothing). Here, we extend PSID to
enable optimal filtering and smoothing. First, we show that the presence of a
secondary signal makes it possible to uniquely identify a model with an optimal
Kalman update step (to enable filtering) from a family of otherwise equivalent
state-space models. Our filtering solution augments PSID with a reduced-rank
regression step that directly learns the optimal gain required for the update
step from data. We refer to this extension of PSID as PSID with filtering.
Second, inspired by two-filter Kalman smoother formulations, we develop a novel
forward-backward PSID smoothing algorithm where we first apply PSID with
filtering and then apply it again in the reverse time direction on the
residuals of the filtered secondary signal. We validate our methods on
simulated data, showing that our approach recovers the ground-truth model
parameters for filtering, and achieves optimal filtering and smoothing decoding
performance of the secondary signal that matches the ideal performance of the
true underlying model. This work provides a principled framework for optimal
linear filtering and smoothing in the two-signal setting, significantly
expanding the toolkit for analyzing dynamic interactions in multivariate
time-series.

</details>


### [398] [Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown](https://arxiv.org/abs/2507.15290)
*Emile Anand,Sarah Liaw*

Main category: cs.LG

TL;DR: Feel-Good Thompson Sampling (FG-TS) improves exploration in contextual bandits by adding an optimism bias but faces challenges in approximate posterior settings. A systematic study benchmarks its performance and confirms it generally outperforms standard Thompson Sampling (TS) in many contexts.


<details>
  <summary>Details</summary>
Motivation: Thompson Sampling struggles with insufficient exploration in high-dimensional contextual bandit problems. FG-TS was proposed to address this but lacks evaluation in scenarios with approximate posteriors common in large-scale or neural problems.

Method: The study evaluates FG-TS and its smoothed variant (SFG-TS) across eleven benchmarks, testing exact posteriors (linear/logistic bandits) and approximate posteriors (stochastic-gradient samplers). It also includes ablations over preconditioning, bonus scale, and prior strength.

Result: FG-TS generally outperformed vanilla TS in linear and logistic bandit settings but performed weaker in neural bandit settings. Larger optimism bonuses proved trade-offs, helping accurate posteriors but harming noisy sampling scenarios.

Conclusion: FG-TS and its variants are robust, competitive, and easy-to-use enhancements over standard TS, especially in linear and logistic contexts. They are recommended as baselines for contextual bandit benchmarks, with source code provided for reproducibility.

Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation
tradeoff in contextual bandits, yet recent theory shows that it does not
explore aggressively enough in high-dimensional problems. Feel-Good Thompson
Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward
high-reward models, and it achieves the asymptotically minimax-optimal regret
in the linear setting when posteriors are exact. However, its performance with
\emph{approximate} posteriors -- common in large-scale or neural problems --
has not been benchmarked. We provide the first systematic study of FG-TS and
its smoothed variant (SFG-TS) across eleven real-world and synthetic
benchmarks. To evaluate their robustness, we compare performance across
settings with exact posteriors (linear and logistic bandits) to approximate
regimes produced by fast but coarse stochastic-gradient samplers. Ablations
over preconditioning, bonus scale, and prior strength reveal a trade-off:
larger bonuses help when posterior samples are accurate, but hurt when sampling
noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic
bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS
and its variants are competitive and easy-to-use, we recommend them as
baselines in modern contextual-bandit benchmarks. Finally, we provide source
code for all our experiments in
https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.

</details>


### [399] [Universal crystal material property prediction via multi-view geometric fusion in graph transformers](https://arxiv.org/abs/2507.15303)
*Liang Zhang,Kong Chen,Yuen Wu*

Main category: cs.LG

TL;DR: The study presents MGT, a multi-view graph transformer framework, to improve crystal property prediction using SE3 invariant and SO3 equivariant graph representations.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of effectively capturing geometric and topological characteristics of crystal structures for improved machine learning simulations.

Method: MGT employs SE3 invariant and SO3 equivariant graph representations fused through a lightweight mixture of experts router for adaptive weighting based on tasks.

Result: MGT reduces error by up to 21% in crystal property prediction tasks and achieves up to 58% performance improvement in transfer learning scenarios.

Conclusion: MGT demonstrates significant advancements in crystal property prediction and broad application potential for novel material discovery.

Abstract: Accurately and comprehensively representing crystal structures is critical
for advancing machine learning in large-scale crystal materials simulations,
however, effectively capturing and leveraging the intricate geometric and
topological characteristics of crystal structures remains a core, long-standing
challenge for most existing methods in crystal property prediction. Here, we
propose MGT, a multi-view graph transformer framework that synergistically
fuses SE3 invariant and SO3 equivariant graph representations, which
respectively captures rotation-translation invariance and rotation equivariance
in crystal geometries. To strategically incorporate these complementary
geometric representations, we employ a lightweight mixture of experts router in
MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on
the specific target task. Compared with previous state-of-the-art models, MGT
reduces the mean absolute error by up to 21% on crystal property prediction
tasks through multi-task self-supervised pretraining. Ablation experiments and
interpretable investigations confirm the effectiveness of each technique
implemented in our framework. Additionally, in transfer learning scenarios
including crystal catalyst adsorption energy and hybrid perovskite bandgap
prediction, MGT achieves performance improvements of up to 58% over existing
baselines, demonstrating domain-agnostic scalability across diverse application
domains. As evidenced by the above series of studies, we believe that MGT can
serve as useful model for crystal material property prediction, providing a
valuable tool for the discovery of novel materials.

</details>


### [400] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: M-DESIGN creates a database pipeline to refine and adaptively select neural networks for tasks by leveraging a relational knowledge base, improving performance on graph analytics tasks.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of static model selection in databases, which inadequately consider dynamic and fine-grained dependencies between tasks and model architectures.

Method: M-DESIGN introduces a knowledge weaving engine using a graph-relational schema to encode data properties, architecture modifications, and performance metrics, enabling an adaptive query mechanism for model refinement.

Result: M-DESIGN demonstrated optimal model selection in 26 out of 33 data-task pairs within constrained computational budgets, specifically focused on graph analytics tasks.

Conclusion: The paper showcases an innovative database-driven approach for refining neural networks, promising better adaptability and performance for evolving machine learning tasks.

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [401] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: PALM is a mathematical model that predicts active learning performance metrics like accuracy, efficiency, and scalability using partial observations.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing evaluation methods for active learning, which fail to analyze learning dynamics beyond final accuracy.

Method: PALM provides a unified mathematical framework based on four parameters (accuracy, efficiency, early-stage performance, scalability) and uses partial data observations to predict full learning trajectories.

Result: PALM demonstrates robust performance across datasets such as CIFAR-10/100 and ImageNet variants. It accurately predicts future learning curves and allows for principled comparisons of active learning strategies.

Conclusion: PALM offers a systematic approach to analyze and predict active learning efficacy, aiding in the selection of cost-effective strategies and enabling reproducible evaluation for both research and practice settings.

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [402] [Learning to Gridize: Segment Physical World by Wireless Communication Channel](https://arxiv.org/abs/2507.15386)
*Juntao Wang,Feng Yin,Tian Ding,Tsung-Hui Chang,Zhi-Quan Luo,Qi Yan*

Main category: cs.LG

TL;DR: This paper introduces Channel Space Gridization (CSG), a method to improve large-scale network optimization by clustering users based on channel properties using a new autoencoder framework.


<details>
  <summary>Details</summary>
Motivation: Current gridization methods rely on limited or flawed assumptions like geographical data or signal strength equivalence, leading to suboptimal clustering for network optimization.

Method: The proposed Channel Space Gridization (CSG) is a framework that uses beam-level reference signal received power (RSRP) for channel estimation and clustering. It introduces the CSG Autoencoder (CSG-AE) with specialized components like a trainable encoder, sparse quantizer, and physics-informed decoder, supported by a novel PIDA training scheme.

Result: CSG-AE achieves high accuracy in estimating Channel Angle Power Spectra (CAPS) and clustering quality on synthetic data. On real-world datasets, it significantly improves RSRP prediction accuracy and various gridization metrics compared to existing approaches.

Conclusion: Channel Space Gridization provides a robust and efficient methodology for grid-based network optimization, addressing limitations of previous techniques and advancing clustering quality and prediction accuracy in large-scale networks.

Abstract: Gridization, the process of partitioning space into grids where users share
similar channel characteristics, serves as a fundamental prerequisite for
efficient large-scale network optimization. However, existing methods like
Geographical or Beam Space Gridization (GSG or BSG) are limited by reliance on
unavailable location data or the flawed assumption that similar signal
strengths imply similar channel properties. We propose Channel Space
Gridization (CSG), a pioneering framework that unifies channel estimation and
gridization for the first time. Formulated as a joint optimization problem, CSG
uses only beam-level reference signal received power (RSRP) to estimate Channel
Angle Power Spectra (CAPS) and partition samples into grids with homogeneous
channel characteristics. To perform CSG, we develop the CSG Autoencoder
(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse
codebook quantizer, and a physics-informed decoder based on the Localized
Statistical Channel Model. On recognizing the limitations of naive training
scheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous
(PIDA) training scheme for CSG-AE, ensuring stable and effective training by
systematically addressing the common pitfalls of the naive training paradigm.
Evaluations reveal that CSG-AE excels in CAPS estimation accuracy and
clustering quality on synthetic data. On real-world datasets, it reduces Active
Mean Absolute Error (MAE) by 30\% and Overall MAE by 65\% on RSRP prediction
accuracy compared to salient baselines using the same data, while improving
channel consistency, cluster sizes balance, and active ratio, advancing the
development of gridization for large-scale network optimization.

</details>


### [403] [The calculus of variations of the Transformer on the hyperspherical tangent bundle](https://arxiv.org/abs/2507.15431)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: This paper provides a theoretical mathematical foundation for Transformers, modeling them as solving a calculus of variations problem via Lagrangian optimization and presenting a unique proof of its dynamics.


<details>
  <summary>Details</summary>
Motivation: The authors aim to provide a rigorous mathematical backing for Transformers, specifically applying principles from calculus of variations and optimization, as this foundation is currently lacking despite its importance in machine learning.

Method: The paper models Transformers as flow maps operating in tangent spaces of hyperspheres, applies calculus of variations principles, and derives the Euler-Lagrange equation tailored to Transformer dynamics.

Result: The authors establish a new theoretical framework, proving factually and formally how Transformers function as solvers of variational problems. They also propose new scenarios for loss optimization based on path optimality.

Conclusion: This work lays foundational groundwork for viewing Transformers through the lens of calculus of variations and provides original results using classical mathematical techniques to analyze their performance in neural contexts.

Abstract: We offer a theoretical mathematical background to Transformers through
Lagrangian optimization across the token space. The Transformer, as a flow map,
exists in the tangent fiber for each token along the high-dimensional unit
sphere. The circumstance of the hypersphere across the latent data is
reasonable due to the trained diagonal matrix equal to the identity, which has
various empirical justifications. Thus, under the continuum limit of the
dynamics, the latent vectors flow among the tangent bundle. Using these facts,
we devise a mathematical framework for the Transformer through calculus of
variations. We develop a functional and show that the continuous flow map
induced by the Transformer satisfies this functional, therefore the Transformer
can be viewed as a natural solver of a calculus of variations problem. We
invent new scenarios of when our methods are applicable based on loss
optimization with respect to path optimality. We derive the Euler-Lagrange
equation for the Transformer. The variant of the Euler-Lagrange equation we
present has various appearances in literature, but, to our understanding,
oftentimes not foundationally proven or under other specialized cases. Our
overarching proof is new: our techniques are classical and the use of the flow
map object is original. We provide several other relevant results, primarily
ones specific to neural scenarios. In particular, much of our analysis will be
attempting to quantify Transformer data in variational contexts under neural
approximations. Calculus of variations on manifolds is a well-nourished
research area, but for the Transformer specifically, it is uncharted: we lay
the foundation for this area through an introduction to the Lagrangian for the
Transformer.

</details>


### [404] [An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations](https://arxiv.org/abs/2507.15442)
*Owen Douglas,Aku Kammonen,Anamika Pandey,Raúl Tempone*

Main category: cs.LG

TL;DR: This paper introduces a novel training algorithm using adaptive random Fourier features (ARFF) for learning components of stochastic differential equations, showing improved performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To enhance the modeling of stochastic differential equations through a superior method in terms of loss minimization and convergence speed, addressing limitations of current Adam-based optimization techniques.

Method: The method involves adaptive random Fourier features (ARFF) with Metropolis sampling and resampling, using a likelihood-based loss function derived from Euler-Maruyama integration.

Result: The ARFF-based approach matched or outperformed conventional Adam optimization in benchmark problems, improving in loss minimization and convergence speed.

Conclusion: ARFF emerges as a strong alternative for data-driven stochastic dynamics modeling, with demonstrated effectiveness across various benchmark scenarios.

Abstract: This work proposes a training algorithm based on adaptive random Fourier
features (ARFF) with Metropolis sampling and resampling
\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and
diffusion components of stochastic differential equations from snapshot data.
Specifically, this study considers It\^{o} diffusion processes and a
likelihood-based loss function derived from the Euler-Maruyama integration
introduced in \cite{Dietrich2023} and
\cite{dridi2021learningstochasticdynamicalsystems}.
  This work evaluates the proposed method against benchmark problems presented
in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin
dynamics, a stochastic susceptible-infected-recovered model, and a stochastic
wave equation. Across all cases, the ARFF-based approach matches or surpasses
the performance of conventional Adam-based optimization in both loss
minimization and convergence speed. These results highlight the potential of
ARFF as a compelling alternative for data-driven modeling of stochastic
dynamics.

</details>


### [405] [FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning](https://arxiv.org/abs/2507.15470)
*Baran Can Gül,Suraksha Nadig,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: FedMultiEmo is a privacy-aware, in-vehicle emotion recognition framework that combines facial images and physiological signals, achieving 87% accuracy while keeping data local.


<details>
  <summary>Details</summary>
Motivation: To address challenges in deploying emotion recognition in vehicles, such as fragility of visual data, variability of physiological signals, and privacy risks during centralized training.

Method: FedMultiEmo uses a multimodal federated learning framework combining visual features from facial images processed by a CNN and physiological data handled by a Random Forest. It also employs data fusion via majority voting and personalized Federated Averaging.

Result: The framework achieved 87% accuracy through data fusion, comparable with centralized baselines, while operating locally on Raspberry Pi clients with minimal computational and memory resources.

Conclusion: FedMultiEmo demonstrates the feasibility of real-time, privacy-preserving emotion recognition in vehicles, overcoming traditional barriers through a multimodal federated approach.

Abstract: In-vehicle emotion recognition underpins adaptive driver-assistance systems
and, ultimately, occupant safety. However, practical deployment is hindered by
(i) modality fragility - poor lighting and occlusions degrade vision-based
methods; (ii) physiological variability - heart-rate and skin-conductance
patterns differ across individuals; and (iii) privacy risk - centralized
training requires transmission of sensitive data. To address these challenges,
we present FedMultiEmo, a privacy-preserving framework that fuses two
complementary modalities at the decision level: visual features extracted by a
Convolutional Neural Network from facial images, and physiological cues (heart
rate, electrodermal activity, and skin temperature) classified by a Random
Forest. FedMultiEmo builds on three key elements: (1) a multimodal federated
learning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud
prototype on Raspberry Pi clients and a Flower server, and (3) a personalized
Federated Averaging scheme that weights client updates by local data volume.
Evaluated on FER2013 and a custom physiological dataset, the federated
Convolutional Neural Network attains 77% accuracy, the Random Forest 74%, and
their fusion 87%, matching a centralized baseline while keeping all raw data
local. The developed system converges in 18 rounds, with an average round time
of 120 seconds and a per-client memory footprint below 200 MB. These results
indicate that FedMultiEmo offers a practical approach to real-time,
privacy-aware emotion recognition in automotive settings.

</details>


### [406] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper addresses the issue of overoptimization in Reinforcement Learning from Human Feedback (RLHF) for language models and introduces a method called Off-Policy Corrected Reward Modeling (OCRM) to mitigate it.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve the problem of overoptimization in RLHF, where the reward model becomes inaccurate during training as generated responses deviate from those seen during reward model training.

Method: The authors propose OCRM, which iteratively applies off-policy corrections using importance weighting to improve the reward model without needing new labels or samples.

Result: The method demonstrates improved reward modeling accuracy and better final policy performance in summarization and chatbot datasets compared to standard RLHF methods and baselines.

Conclusion: OCRM mitigates the distribution shift issue in RLHF, leading to a more accurate reward model and better alignment of the learned policy with human preferences.

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [407] [An Investigation of Test-time Adaptation for Audio Classification under Background Noise](https://arxiv.org/abs/2507.15523)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.LG

TL;DR: The study investigates improving audio classification under background noise domain shifts using test-time adaptation techniques, proposing a modified CoNMix method that outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of domain shift in audio classification, especially caused by background noise, which degrades pre-trained model performance on test datasets.

Method: Test-time adaptation (TTA) techniques are used, specifically TTT, TENT, and a modified version of CoNMix. Experiments were conducted on two audio datasets, AudioMNIST and SpeechCommands V1, to evaluate performance under various noise levels.

Result: The modified CoNMix approach achieved the best accuracy, with error rates of 5.31% under 10 dB exercise bike noise and 12.75% under 3 dB running tap noise on AudioMNIST, outperforming TTT and TENT.

Conclusion: The study demonstrates that the modified CoNMix method effectively handles domain shifts in audio classification caused by background noise, representing the first use of TTA techniques for this problem.

Abstract: Domain shift is a prominent problem in Deep Learning, causing a model
pre-trained on a source dataset to suffer significant performance degradation
on test datasets. This research aims to address the issue of audio
classification under domain shift caused by background noise using Test-Time
Adaptation (TTA), a technique that adapts a pre-trained model during testing
using only unlabelled test data before making predictions. We adopt two common
TTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and
investigate their respective performance on two popular audio classification
datasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types
of background noise and noise severity levels. The experimental results reveal
that our proposed modified version of CoNMix produced the highest
classification accuracy under domain shift (5.31% error rate under 10 dB
exercise bike background noise and 12.75% error rate under 3 dB running tap
background noise for AM) compared to TTT and TENT. The literature search
provided no evidence of similar works, thereby motivating the work reported
here as the first study to leverage TTA techniques for audio classification
under domain shift.

</details>


### [408] [Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications](https://arxiv.org/abs/2507.15545)
*Yujia Shi,Emil Njor,Pablo Martínez-Nuevo,Sven Ewan Shepstone,Xenofon Fafoutis*

Main category: cs.LG

TL;DR: The paper introduces a method called "Data Aware Differentiable Neural Architecture Search" to simplify TinyML design by optimizing both data and model parameters for better balance in resource usage and system performance.


<details>
  <summary>Details</summary>
Motivation: The need for efficient Machine Learning paradigms like TinyML is growing due to ML's high resource footprint. However, TinyML adoption is limited by the complexity of its design.

Method: The paper presents a novel approach that includes data configuration parameters in the Differentiable Neural Architecture Search, allowing co-optimization of both model architecture and input data properties.

Result: Using this method for keyword spotting tasks, the research demonstrates the ability to design systems that are resource-lean yet maintain high accuracy.

Conclusion: The proposed method offers a simplified yet effective way to design high-performance TinyML systems, potentially enabling broader adoption.

Abstract: The success of Machine Learning is increasingly tempered by its significant
resource footprint, driving interest in efficient paradigms like TinyML.
However, the inherent complexity of designing TinyML systems hampers their
broad adoption. To reduce this complexity, we introduce "Data Aware
Differentiable Neural Architecture Search". Unlike conventional Differentiable
Neural Architecture Search, our approach expands the search space to include
data configuration parameters alongside architectural choices. This enables
Data Aware Differentiable Neural Architecture Search to co-optimize model
architecture and input data characteristics, effectively balancing resource
usage and system performance for TinyML applications. Initial results on
keyword spotting demonstrate that this novel approach to TinyML system design
can generate lean but highly accurate systems.

</details>


### [409] [The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information](https://arxiv.org/abs/2507.15548)
*D. Abler,O. Pusterla,A. Joye-Kühnis,N. Andratschke,M. Bach,A. Bink,S. M. Christ,P. Hagmann,B. Pouymayou,E. Pravatà,P. Radojewski,M. Reyes,L. Ruinelli,R. Schaer,B. Stieltjes,G. Treglia,W. Valenzuela,R. Wiest,S. Zoergiebel,M. Guckenberger,S. Tanadini-Lang,A. Depeursinge*

Main category: cs.LG

TL;DR: Despite the promise of radiomics in glioblastoma prognosis, this study concludes that conventional radiomics (CR) and deep learning (DL) MRI radiomics add minimal value over demographic and molecular predictors.


<details>
  <summary>Details</summary>
Motivation: The study aims to assess whether radiomics methods (CR and DL) can improve glioblastoma prognosis prediction compared to traditional predictors such as clinical and molecular attributes.

Method: Researchers analyzed a multi-center dataset with 1152 glioblastoma patients, evaluating models using clinical, molecular, and anatomical MRI data. They performed comparative analyses across different subsets of patients and feature combinations.

Result: Combined-feature CR models slightly outperform clinical-only models in predicting survival (AUC=0.75 vs. 0.74 in external validation), while DL models showed similar trends but lacked statistical significance. Imaging data provided modest predictive improvement for overall survival.

Conclusion: Radiomics methods demonstrate limited added value for prognostic predictions beyond demographic predictors such as age and gender, suggesting that MRI-based radiomics alone are not transformative in glioblastoma prognosis.

Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its
added value over clinical and molecular predictors has yet to be proven. This
study assessed the added value of conventional radiomics (CR) and deep learning
(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on
a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152
glioblastoma (WHO 2016) patients from five Swiss centers and one public source.
It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI
data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were
developed using standard methods and evaluated on internal and external
cohorts. Sub-analyses assessed models with different feature sets
(imaging-only, clinical/molecular-only, combined-features) and patient subsets
(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In
external validation, the combined-feature CR model achieved an AUC of 0.75,
slightly, but significantly outperforming clinical-only (0.74) and imaging-only
(0.68) models. DL models showed similar trends, though without statistical
significance. In S-2 and S-3, combined models did not outperform clinical-only
models. Exploratory analysis of CR models for overall survival prediction
suggested greater relevance of imaging data: across all subsets,
combined-feature models significantly outperformed clinical-only models, though
with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI
sequences for glioblastoma prognosis, this multi-center study found standard CR
and DL radiomics approaches offer minimal added value over demographic
predictors such as age and gender.

</details>


### [410] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: The paper introduces PhysGym, a benchmark suite designed for evaluating large language models (LLMs) in scientific reasoning tasks within physics environments.


<details>
  <summary>Details</summary>
Motivation: To provide a platform for assessing how LLM-based agents handle problem complexity, prior knowledge, and scientific reasoning in physics.

Method: PhysGym includes interactive simulations where LLMs experiment with environments to gather data, form hypotheses, and uncover physical laws, with controlled priors and set protocols.

Result: Baseline LLMs were tested, showing that PhysGym differentiates their abilities across varying prior knowledge and task complexities.

Conclusion: PhysGym is a valuable tool for analyzing LLM scientific reasoning capacities, particularly their interactions with prior knowledge and environmental challenges.

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce PhysGym, a novel benchmark
suite and simulation platform for rigorously assessing LLM-based scientific
reasoning in interactive physics environments. PhysGym's primary contribution
lies in its sophisticated control over the level of prior knowledge provided to
the agent. This allows researchers to dissect agent performance along axes
including the complexity of the problem and the prior knowledge levels. The
benchmark comprises a suite of interactive simulations, where agents must
actively probe environments, gather data sequentially under constraints and
formulate hypotheses about underlying physical laws. PhysGym provides
standardized evaluation protocols and metrics for assessing hypothesis accuracy
and model fidelity. We demonstrate the benchmark's utility by presenting
results from baseline LLMs, showcasing its ability to differentiate
capabilities based on varying priors and task complexity.

</details>


### [411] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: The paper investigates how the accuracy of machine learning (ML) models for predicting patient length-of-stay (LOS) impacts the need for and effectiveness of patient rescheduling strategies in hospital resource planning.


<details>
  <summary>Details</summary>
Motivation: Hospital resource planning for elective surgeries heavily depends on ensuring sufficient inpatient beds. Due to the variability in actual LOS compared to ML model predictions, there is a need to assess how inaccuracies in predictions and rescheduling strategies interact.

Method: The study builds on prior work using simulated ML to evaluate how LOS prediction errors affect rescheduling flexibility. Various corrective policies and patient rescheduling strategies were examined under different prediction accuracy levels.

Result: The paper identifies effective patient rescheduling strategies that account for inaccurate LOS predictions to optimize bed availability and prevent overflow.

Conclusion: Improving LOS prediction accuracy can reduce the reliance on rescheduling, but operational flexibility remains crucial for handling errors or uncertainties in predictions.

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [412] [On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project](https://arxiv.org/abs/2507.15574)
*Gregory F. Stock,Juan A. Fraire,Holger Hermanns,Jędrzej Mosiężny,Yusra Al-Khazraji,Julio Ramírez Molina,Evridiki V. Ntagiou*

Main category: cs.LG

TL;DR: This paper examines how Artificial Intelligence (AI), particularly Reinforcement Learning (RL), can enhance satellite constellation management by optimizing data routing and resource allocation.


<details>
  <summary>Details</summary>
Motivation: The rapid increase in near-Earth satellite constellations necessitates advanced solutions for efficient and scalable network management.

Method: AI-driven algorithms, especially leveraging Reinforcement Learning (RL), are developed and tested for two critical applications: data routing to reduce latency and resource allocation for efficient scheduling.

Result: RL demonstrated superior performance in reducing end-to-end latency for data routing and optimized resource utilization for scheduling tasks compared to traditional methods.

Conclusion: The study confirms that AI, specifically RL, can transform satellite management by delivering more adaptive, robust, and cost-effective strategies for constellation operations.

Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents
significant challenges in satellite network management, requiring innovative
approaches for efficient, scalable, and resilient operations. This paper
explores the role of Artificial Intelligence (AI) in optimizing the operation
of satellite mega-constellations, drawing from the ConstellAI project funded by
the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland
University, and Thales Alenia Space collaborates to develop AI-driven
algorithms and demonstrates their effectiveness over traditional methods for
two crucial operational challenges: data routing and resource allocation. In
the routing use case, Reinforcement Learning (RL) is used to improve the
end-to-end latency by learning from historical queuing latency, outperforming
classical shortest path algorithms. For resource allocation, RL optimizes the
scheduling of tasks across constellations, focussing on efficiently using
limited resources such as battery and memory. Both use cases were tested for
multiple satellite constellation configurations and operational scenarios,
resembling the real-life spacecraft operations of communications and Earth
observation satellites. This research demonstrates that RL not only competes
with classical approaches but also offers enhanced flexibility, scalability,
and generalizability in decision-making processes, which is crucial for the
autonomous and intelligent management of satellite fleets. The findings of this
activity suggest that AI can fundamentally alter the landscape of satellite
constellation management by providing more adaptive, robust, and cost-effective
solutions.

</details>


### [413] [We Need to Rethink Benchmarking in Anomaly Detection](https://arxiv.org/abs/2507.15584)
*Philipp Röchner,Simon Klüttermann,Franz Rothlauf,Daniel Schlör*

Main category: cs.LG

TL;DR: Benchmarking practices in anomaly detection need improvement to better reflect diverse real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Current benchmarking fails to capture the diversity of anomalies in various applications, hindering progress in anomaly detection research.

Method: The paper advocates for scenario-based benchmarking approaches using a common taxonomy, end-to-end analysis of detection pipelines, and scenario-specific evaluation objectives.

Result: It identifies fundamental shortcomings in existing evaluation practices and presents a perspective to enhance benchmarking frameworks.

Conclusion: Anomaly detection research can advance by rethinking benchmarking to align better with real-world applications through tailored scenarios and evaluation methods.

Abstract: Despite the continuous proposal of new anomaly detection algorithms and
extensive benchmarking efforts, progress seems to stagnate, with only minor
performance differences between established baselines and new algorithms. In
this position paper, we argue that this stagnation is due to limitations in how
we evaluate anomaly detection algorithms. Current benchmarking does not, for
example, sufficiently reflect the diversity of anomalies in applications
ranging from predictive maintenance to scientific discovery. Consequently, we
need to rethink benchmarking in anomaly detection. In our opinion, anomaly
detection should be studied using scenarios that capture the relevant
characteristics of different applications. We identify three key areas for
improvement: First, we need to identify anomaly detection scenarios based on a
common taxonomy. Second, anomaly detection pipelines should be analyzed
end-to-end and by component. Third, evaluating anomaly detection algorithms
should be meaningful regarding the scenario's objectives.

</details>


### [414] [Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario](https://arxiv.org/abs/2507.15587)
*Yinsong Chen,Kaifeng Wang,Xiaoqiang Meng,Xueyuan Li,Zirui Li,Xin Gao*

Main category: cs.LG

TL;DR: The paper introduces a Red-Team Multi-Agent Reinforcement Learning framework to uncover decision-making corner cases for autonomous vehicles using interference tactics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to tackle inefficiencies in capturing corner cases in decision-making for safety-critical scenarios by traditional data-driven or specific modeling approaches.

Method: The paper uses a Red-Team Multi-Agent Reinforcement Learning framework and a Constraint Graph Representation Markov Decision Process to enable red-team vehicles to interfere while obeying safety rules.

Result: Their experimental results reveal that red-team vehicles can expose critical decision-making vulnerabilities in autonomous vehicles by generating diverse corner cases.

Conclusion: This framework significantly impacts safety analysis in autonomous vehicle decision-making and opens up new directions for studying safety-critical scenarios.

Abstract: Current research on decision-making in safety-critical scenarios often relies
on inefficient data-driven scenario generation or specific modeling approaches,
which fail to capture corner cases in real-world contexts. To address this
issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,
where background vehicles with interference capabilities are treated as
red-team agents. Through active interference and exploration, red-team vehicles
can uncover corner cases outside the data distribution. The framework uses a
Constraint Graph Representation Markov Decision Process, ensuring that red-team
vehicles comply with safety rules while continuously disrupting the autonomous
vehicles (AVs). A policy threat zone model is constructed to quantify the
threat posed by red-team vehicles to AVs, inducing more extreme actions to
increase the danger level of the scenario. Experimental results show that the
proposed framework significantly impacts AVs decision-making safety and
generates various corner cases. This method also offers a novel direction for
research in safety-critical scenarios.

</details>


### [415] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: The paper introduces a framework that uses reinforcement learning to optimize performance when continually pre-training large language models on task-specific data, reducing catastrophic forgetting and improving adaptability.


<details>
  <summary>Details</summary>
Motivation: Pre-training large language models on specific tasks improves performance but risks losing their general capabilities. Current solutions for balancing source and target field performance rely on manual, heuristic-driven approaches.

Method: The authors propose an end-to-end model called Data Mixing Agent that uses reinforcement learning to dynamically re-weight data from source and target fields, learning generalizable strategies from data mixing trajectories and feedback.

Result: The proposed framework demonstrated improved balanced performance during continual pre-training in math reasoning and showed good generalizability to unseen domains, models, and tasks. It also proved effective in the code generation field.

Conclusion: Data Mixing Agent validates the use of reinforcement learning for domain balancing during continual pre-training, offering better performance with reduced reliance on source data and aligning with human intuition for data mixing strategies.

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [416] [Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity](https://arxiv.org/abs/2507.15601)
*Huiling Yang,Zhanwei Wang,Kaibin Huang*

Main category: cs.LG

TL;DR: This paper presents a framework for optimizing batch sizes in federated learning to reduce learning latency and assure convergence, addressing challenges of high-dimensional updates and device heterogeneity.


<details>
  <summary>Details</summary>
Motivation: To develop a low-latency federated learning framework suitable for mission-critical IoT applications in 6G networks, while addressing challenges like high-dimensional data and diverse device capabilities.

Method: The authors propose a communication-and-computation-aware framework for optimal batch-size control, using convergence analysis, surrogate modeling for convergence speed, and tailored strategies for different fading scenarios.

Result: The proposed strategies, accounting for tradeoffs and heterogeneity, showed superior performance in experiments on real datasets compared to conventional batch-size schemes.

Conclusion: The framework effectively balances tradeoffs to achieve lower latency and better performance in federated learning, ensuring scalability for diverse IoT applications in 6G networks.

Abstract: Federated learning (FL) has emerged as a popular approach for collaborative
machine learning in sixth-generation (6G) networks, primarily due to its
privacy-preserving capabilities. The deployment of FL algorithms is expected to
empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous
driving, augmented reality, and healthcare. The mission-critical and
time-sensitive nature of these applications necessitates the design of
low-latency FL frameworks that guarantee high learning performance. In
practice, achieving low-latency FL faces two challenges: the overhead of
computing and transmitting high-dimensional model updates, and the
heterogeneity in communication-and-computation (C$^2$) capabilities across
devices. To address these challenges, we propose a novel C$^2$-aware framework
for optimal batch-size control that minimizes end-to-end (E2E) learning latency
while ensuring convergence. The framework is designed to balance a fundamental
C$^2$ tradeoff as revealed through convergence analysis. Specifically,
increasing batch sizes improves the accuracy of gradient estimation in FL and
thus reduces the number of communication rounds required for convergence, but
results in higher per-round latency, and vice versa. The associated problem of
latency minimization is intractable; however, we solve it by designing an
accurate and tractable surrogate for convergence speed, with parameters fitted
to real data. This approach yields two batch-size control strategies tailored
to scenarios with slow and fast fading, while also accommodating device
heterogeneity. Extensive experiments using real datasets demonstrate that the
proposed strategies outperform conventional batch-size adaptation schemes that
do not consider the C$^2$ tradeoff or device heterogeneity.

</details>


### [417] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: The paper proposes a deep learning surrogate to accelerate HEC-RAS river modeling simulations, achieving nearly 3.5x speedup over traditional models without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Physics-based river solvers are too computationally intensive for real-time flood decision-making, creating a need for faster simulation methods.

Method: The paper introduces a hybrid deep learning architecture using GRU for temporal dynamics and Geo-FNO for spatial modeling. It uses an eight-channel feature vector extracted from HEC-RAS data.

Result: The model achieved a 0.31 feet median stage error on unseen data and reduced simulation time from 139 minutes to 40 minutes for a full 67-reach ensemble forecast.

Conclusion: The data-driven surrogate approach proves efficient and accurate, making large-scale river ensemble forecasting computationally feasible in real-time scenarios.

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


### [418] [Towards Explainable Anomaly Detection in Shared Mobility Systems](https://arxiv.org/abs/2507.15643)
*Elnur Isgandarov,Matteo Cederle,Federico Chiariotti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: This paper introduces an anomaly detection framework for shared mobility systems using multi-source data and interpretable algorithms.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the importance of detecting anomalies in shared mobility systems to optimize operations, improve service reliability, and enhance user experience.

Method: The authors use the Isolation Forest algorithm for unsupervised anomaly detection and integrate multi-source data. They also employ the DIFFI algorithm for enhancing interpretability.

Result: The study demonstrates robust station-level analysis, showing the impact of external factors like weather and transit availability on anomalies.

Conclusion: The research aids in better decision-making to optimize shared mobility systems, improving their effectiveness and reliability.

Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role
in urban transportation. Identifying anomalies in these systems is essential
for optimizing operations, improving service reliability, and enhancing user
experience. This paper presents an interpretable anomaly detection framework
that integrates multi-source data, including bike-sharing trip records, weather
conditions, and public transit availability. The Isolation Forest algorithm is
employed for unsupervised anomaly detection, along with the Depth-based
Isolation Forest Feature Importance (DIFFI) algorithm providing
interpretability. Results show that station-level analysis offers a robust
understanding of anomalies, highlighting the influence of external factors such
as adverse weather and limited transit availability. Our findings contribute to
improving decision-making in shared mobility operations.

</details>


### [419] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: The study explores if small LLMs can develop Theory of Mind (ToM) via reinforcement learning with verifiable rewards (RLVR). Findings show models improve in-domain but fail at generalization, often overfitting to training datasets.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the question of whether rule-based RL techniques can endow LLMs with human-like social intelligence, such as Theory of Mind (ToM).

Method: The authors trained small LLMs using RLVR on multiple ToM datasets and tested their generalization capabilities on unseen datasets.

Result: Small LLMs exhibited limited generic ToM capabilities. They improved on in-domain tasks but struggled with out-of-domain generalization, often leveraging statistical dataset patterns.

Conclusion: Small LLMs, even with RLVR, tend to narrowly overfit training data, failing to acquire generalized or abstract ToM abilities.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [420] [Explainable Anomaly Detection for Electric Vehicles Charging Stations](https://arxiv.org/abs/2507.15718)
*Matteo Cederle,Andrea Mazzucco,Andrea Demartini,Eugenio Mazza,Eugenia Suriani,Federico Vitti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: This paper explores unsupervised anomaly detection for EV charging infrastructure leveraging eXplainable AI to identify anomalies and their root causes.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the need for reliable EV charging stations to facilitate the transition to renewable energy mobility by detecting and interpreting anomalies.

Method: The paper employs Isolation Forest for anomaly detection and DIFFI for feature importance analysis using real-world sensor and charging session data.

Result: The proposed approach demonstrates efficacy in identifying anomalies and their causes in a real industrial scenario.

Conclusion: Integrating anomaly detection with eXplainable AI offers a reliable and interpretable method to ensure the efficiency of EV charging infrastructures.

Abstract: Electric vehicles (EV) charging stations are one of the critical
infrastructures needed to support the transition to renewable-energy-based
mobility, but ensuring their reliability and efficiency requires effective
anomaly detection to identify irregularities in charging behavior. However, in
such a productive scenario, it is also crucial to determine the underlying
cause behind the detected anomalies. To achieve this goal, this study
investigates unsupervised anomaly detection techniques for EV charging
infrastructure, integrating eXplainable Artificial Intelligence techniques to
enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies
Isolation Forest to detect anomalies and employs the Depth-based Isolation
Forest Feature Importance (DIFFI) method to identify the most important
features contributing to such anomalies. The efficacy of the proposed approach
is evaluated in a real industrial case.

</details>


### [421] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: The paper introduces GUI Gaussian Grounding Rewards (GUI-G²), a reward framework for GUI interaction that uses Gaussian distributions to provide continuous optimization and richer gradient signals, significantly outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current GUI grounding methods rely on sparse binary rewards, which inadequately capture the continuous and spatial nature of human interaction with graphical interfaces.

Method: The framework adopts Gaussian distributions to model interface elements, introduces Gaussian point rewards for precise localization, and coverage rewards to measure spatial alignment. An adaptive variance mechanism adjusts these distributions according to element scale.

Result: Experiments show GUI-G² outperforms the state-of-the-art UI-TARS-72B by up to 24.7% on various benchmarks, enhancing robustness and generalization to different interface layouts.

Conclusion: GUI-G² establishes a new paradigm for GUI interaction tasks, making the process more spatially aware and robust through continuous reward modeling.

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [422] [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](https://arxiv.org/abs/2507.15727)
*Xuchuang Wang,Bo Sun,Hedyeh Beyhaghi,John C. S. Lui,Mohammad Hajiesmaili,Adam Wierman*

Main category: cs.LG

TL;DR: This paper introduces a multi-agent extension of the ski-rental problem, where agents face individual and shared costs in decision-making. It analyzes and designs deterministic and randomized policies optimized for various competitive objectives.


<details>
  <summary>Details</summary>
Motivation: To extend the classical ski-rental problem into a group decision-making scenario, addressing the complexities of shared and individual costs under uncertainty.

Method: The paper defines three competitive ratios as objectives (overall, state-dependent, individual rational) and designs state-aware deterministic and randomized policies that adapt based on the agents' dynamic states.

Result: Analysis shows symmetric policies outperformed asymmetric ones, with the paper providing upper and lower bounds for competitive ratios, driving insights into optimal group strategies.

Conclusion: The study generalizes classical ski-rental insights to multi-agent setups, offering frameworks for effective group decision-making under cost-sharing and uncertainty.

Abstract: This paper introduces a novel multi-agent ski-rental problem that generalizes
the classical ski-rental dilemma to a group setting where agents incur
individual and shared costs. In our model, each agent can either rent at a
fixed daily cost, or purchase a pass at an individual cost, with an additional
third option of a discounted group pass available to all. We consider scenarios
in which agents' active days differ, leading to dynamic states as agents drop
out of the decision process. To address this problem from different
perspectives, we define three distinct competitive ratios: overall,
state-dependent, and individual rational. For each objective, we design and
analyze optimal deterministic and randomized policies. Our deterministic
policies employ state-aware threshold functions that adapt to the dynamic
states, while our randomized policies sample and resample thresholds from
tailored state-aware distributions. The analysis reveals that symmetric
policies, in which all agents use the same threshold, outperform asymmetric
ones. Our results provide competitive ratio upper and lower bounds and extend
classical ski-rental insights to multi-agent settings, highlighting both
theoretical and practical implications for group decision-making under
uncertainty.

</details>


### [423] [Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks](https://arxiv.org/abs/2507.15769)
*Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil*

Main category: cs.LG

TL;DR: The paper introduces a multi-modal sensing framework using camera, GPS, LiDAR, and radar to predict signal blockages in vehicular mmWave communication systems.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the issue of signal blockage in mmWave vehicular communication caused by dynamic obstacles like vehicles and pedestrians.

Method: The study uses modality-specific deep learning models to process individual sensor streams and combines their outputs with a softmax-weighted ensemble strategy.

Result: The camera-only model achieves an F1-score of 97.1% with 89.8ms inference time, while a camera+radar configuration improves the F1 score to 97.2% with 95.7ms inference time.

Conclusion: Multi-modal sensing can enhance the prediction of mmWave blockages and enable more reliable proactive wireless communication in dynamic environments.

Abstract: Vehicular communication systems operating in the millimeter wave (mmWave)
band are highly susceptible to signal blockage from dynamic obstacles such as
vehicles, pedestrians, and infrastructure. To address this challenge, we
propose a proactive blockage prediction framework that utilizes multi-modal
sensing, including camera, GPS, LiDAR, and radar inputs in an
infrastructure-to-vehicle (I2V) setting. This approach uses modality-specific
deep learning models to process each sensor stream independently and fuses
their outputs using a softmax-weighted ensemble strategy based on validation
performance. Our evaluations, for up to 1.5s in advance, show that the
camera-only model achieves the best standalone trade-off with an F1-score of
97.1% and an inference time of 89.8ms. A camera+radar configuration further
improves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness
and efficiency of multi-modal sensing for mmWave blockage prediction and
provide a pathway for proactive wireless communication in dynamic environments.

</details>


### [424] [Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis](https://arxiv.org/abs/2507.15772)
*Anoop C. Patil,Benny Jian Rong Sng,Yu-Wei Chang,Joana B. Pereira,Chua Nam-Hai,Rajani Sarojam,Gajendra Pratap Singh,In-Cheol Jang,Giovanni Volpe*

Main category: cs.LG

TL;DR: The paper discusses DIVA, an AI-based approach using variational autoencoders to analyze plant stress through Raman spectroscopy without manual preprocessing.


<details>
  <summary>Details</summary>
Motivation: To improve plant health monitoring by addressing biases and inefficiencies in traditional Raman spectroscopy analysis workflows.

Method: A fully automated variational autoencoder model analyzes native Raman spectra, bypassing manual preprocessing steps like fluorescence background removal.

Result: DIVA accurately detects plant stress factors (abiotic and biotic) from unprocessed spectra, demonstrating its robustness in plant health assessment.

Conclusion: DIVA combines deep learning and vibrational spectroscopy to create a bias-free, automated approach for improving agricultural productivity and sustainability.

Abstract: Detecting stress in plants is crucial for both open-farm and
controlled-environment agriculture. Biomolecules within plants serve as key
stress indicators, offering vital markers for continuous health monitoring and
early disease detection. Raman spectroscopy provides a powerful, non-invasive
means to quantify these biomolecules through their molecular vibrational
signatures. However, traditional Raman analysis relies on customized
data-processing workflows that require fluorescence background removal and
prior identification of Raman peaks of interest-introducing potential biases
and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation
of Vibrational Raman spectra for plant-stress Analysis), a fully automated
workflow based on a variational autoencoder. Unlike conventional approaches,
DIVA processes native Raman spectra-including fluorescence backgrounds-without
manual preprocessing, identifying and quantifying significant spectral features
in an unbiased manner. We applied DIVA to detect a range of plant stresses,
including abiotic (shading, high light intensity, high temperature) and biotic
stressors (bacterial infections). By integrating deep learning with vibrational
spectroscopy, DIVA paves the way for AI-driven plant health assessment,
fostering more resilient and sustainable agricultural practices.

</details>


### [425] [Dynamics is what you need for time-series forecasting!](https://arxiv.org/abs/2507.15774)
*Alexis-Raja Brachet,Pierre-Yves Richard,Céline Hudelot*

Main category: cs.LG

TL;DR: The paper investigates the effectiveness of dynamic learning in time-series forecasting, proposing a specific framework to assess models' dynamics learning capabilities.


<details>
  <summary>Details</summary>
Motivation: To address why simpler models often surpass deep models in time-series forecasting tasks and to validate the importance of learning underlying dynamics of data.

Method: Developed an original PRO-DYN nomenclature to analyze models based on their dynamics learning capabilities, and conducted extensive experiments with diverse model architectures.

Result: Two key findings: (1) under-performing architectures only partially learn dynamics; and (2) incorporating a learnable dynamics block and placing it at the model's end significantly improves performance.

Conclusion: The study underscores the necessity of embedding a learnable dynamics block as the final predictor in forecasting models for optimal results.

Abstract: While boundaries between data modalities are vanishing, the usual successful
deep models are still challenged by simple ones in the time-series forecasting
task. Our hypothesis is that this task needs models that are able to learn the
data underlying dynamics. We propose to validate it through both systemic and
empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to
analyze existing models through the lens of dynamics. Two observations thus
emerged: $\textbf{1}$. under-performing architectures learn dynamics at most
partially, $\textbf{2}$. the location of the dynamics block at the model end is
of prime importance. We conduct extensive experiments to confirm our
observations on a set of performance-varying models with diverse backbones.
Results support the need to incorporate a learnable dynamics block and its use
as the final predictor.

</details>


### [426] [Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets](https://arxiv.org/abs/2507.15784)
*Zihang Ma,Qitian Yin*

Main category: cs.LG

TL;DR: This paper addresses class imbalance in graph node classification by proposing the WR-EFM, a fusion model enhanced by the Wasserstein-Rubinstein distance for optimized integration of specialized models.


<details>
  <summary>Details</summary>
Motivation: The study aims to tackle disparities in classification performance across categories in graph node classification, particularly focusing on improving the accuracy of underperforming categories.

Method: The paper introduces WR-EFM combining specialized GNNs and GAT models, using WR distance to enhance fusion and dynamically assign weights based on category performance.

Result: WR-EFM achieves balanced accuracy across all categories, with significant improvements in underperforming ones (Category 2 accuracy increased by 5.5% compared to GCN).

Conclusion: The WR-EFM model effectively addresses class imbalance in graph node classification, demonstrating superior stability and accuracy across categories.

Abstract: Graph node classification is a fundamental task in graph neural networks
(GNNs), aiming to assign predefined class labels to nodes. On the PubMed
citation network dataset, we observe significant classification difficulty
disparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,
7.5% lower than Category 1. To address this, we propose a
Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),
training specialized GNN models for Categories 0/1 (with layer normalization
and residual connections) and Multi-hop Graph Attention Networks (GAT) for
Category 2. The WR distance metric optimizes representation similarity between
models, particularly focusing on improving Category 2 performance. Our adaptive
fusion strategy dynamically weights models based on category-specific
performance, with Category 2 assigned a GAT weight of 0.8. WR distance further
guides the fusion process by measuring distributional differences between model
representations, enabling more principled integration of complementary
features.
  Experimental results show WR-EFM achieves balanced accuracy across
categories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),
outperforming both single models and standard fusion approaches. The
coefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%
lower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM
improves Category 2 accuracy by 5.5% compared to GCN, verifying the
effectiveness of WR-guided fusion in capturing complex structural patterns.
This work provides a novel paradigm for handling class-imbalanced graph
classification tasks. To promote the research community, we release our project
at https://github.com/s010m00n/GASEM4NC.

</details>


### [427] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: The paper introduces CSE-FSL, an improved Federated Split Learning (FSL) method that significantly reduces communication and storage requirements by introducing local updates at clients and optimizing data transmission strategies.


<details>
  <summary>Details</summary>
Motivation: The limitations of standard Federated Split Learning (FSL) include high communication overhead and large server-side storage requirements, which are challenging for edge devices and centralized servers.

Method: The authors propose CSE-FSL, which leverages an auxiliary network for local weight updates on clients and utilizes selective epochs for smashed data transmission, while maintaining a single model at the server.

Result: CSE-FSL demonstrates lower communication overhead and storage requirements compared to traditional FSL solutions. Theoretical analysis confirms convergence, and experimental results validate its effectiveness in real-world FL tasks.

Conclusion: CSE-FSL overcomes key challenges in FSL by reducing communication and storage demands, making it a more efficient solution for distributed machine learning tasks.

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [428] [Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction](https://arxiv.org/abs/2507.15832)
*Shiyang Li*

Main category: cs.LG

TL;DR: This paper introduces a CNN-LSTM-attention-Adaboost model optimized using an enhanced snake-herd optimization algorithm for improved 4D trajectory prediction.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of existing medium- and long-term 4D trajectory prediction models for large-scale, high-dimensional data.

Method: The method integrates CNN (spatial features), LSTM (temporal features), attention (global features), and Adaboost. Hyperparameters are optimized using a multi-strategy improved snake-herd optimization algorithm.

Result: The proposed model outperforms traditional optimizers like particle swarm, whale, and gray wolf. It achieves a 39.89% improvement in prediction accuracy on real ADS-B aviation data.

Conclusion: The hybrid SO-CLA-Adaboost model demonstrates superior performance and accuracy, validating the effectiveness of both the optimization strategy and the network architecture for trajectory prediction.

Abstract: To address the limitations of medium- and long-term four-dimensional (4D)
trajectory prediction models, this paper proposes a hybrid
CNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy
improved snake-herd optimization (SO) algorithm. The model applies the Adaboost
algorithm to divide multiple weak learners, and each submodel utilizes CNN to
extract spatial features, LSTM to capture temporal features, and attention
mechanism to capture global features comprehensively. The strong learner model,
combined with multiple sub-models, then optimizes the hyperparameters of the
prediction model through the natural selection behavior pattern simulated by
SO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the
comparison experiments and ablation studies of multiple optimizers are carried
out, and a comprehensive test and evaluation analysis is carried out. The
results show that SO-CLA-adaboost outperforms traditional optimizers such as
particle swarm, whale, and gray wolf in handling large-scale high-dimensional
trajectory data. In addition, introducing the full-strategy collaborative
improvement SO algorithm improves the model's prediction accuracy by 39.89%.

</details>


### [429] [Optimizing Canaries for Privacy Auditing with Metagradient Descent](https://arxiv.org/abs/2507.15836)
*Matteo Boglioni,Terrance Liu,Andrew Ilyas,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: This paper presents an optimized method for auditing the differential privacy of machine learning models using improved canary sets, which can double empirical lower bounds.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve privacy auditing for differentially private algorithms, specifically DP-SGD, by better utilizing membership inference attacks and enhancing the auditing process.

Method: The paper introduces a metagradient optimization framework that generates an optimized set of "canary" examples for better efficacy in privacy auditing.

Result: Optimized canary sets improve empirical lower bounds for privacy parameters in differential privacy audits by more than 2x, and the method remains transferable across models and types of training.

Conclusion: The proposed approach significantly boosts privacy auditing for DP learning algorithms, demonstrating efficiency and transferability across architectures.

Abstract: In this work we study black-box privacy auditing, where the goal is to lower
bound the privacy parameter of a differentially private learning algorithm
using only the algorithm's outputs (i.e., final trained model). For DP-SGD (the
most successful method for training differentially private deep learning
models), the canonical approach auditing uses membership inference-an auditor
comes with a small set of special "canary" examples, inserts a random subset of
them into the training set, and then tries to discern which of their canaries
were included in the training set (typically via a membership inference
attack). The auditor's success rate then provides a lower bound on the privacy
parameters of the learning algorithm. Our main contribution is a method for
optimizing the auditor's canary set to improve privacy auditing, leveraging
recent work on metagradient optimization. Our empirical evaluation demonstrates
that by using such optimized canaries, we can improve empirical lower bounds
for differentially private image classification models by over 2x in certain
instances. Furthermore, we demonstrate that our method is transferable and
efficient: canaries optimized for non-private SGD with a small model
architecture remain effective when auditing larger models trained with DP-SGD.

</details>


### [430] [FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs](https://arxiv.org/abs/2507.15839)
*Anh Nguyen,Sam Schafft,Nicholas Hale,John Alfaro*

Main category: cs.LG

TL;DR: The paper addresses the high cost and time inefficiencies of synthetic data generation using large language models (LLMs) by introducing a scalable, reusable method for realistic tabular data synthesis.


<details>
  <summary>Details</summary>
Motivation: The scarcity and high costs of real-world data collection call for scalable solutions to produce high-quality synthetic data efficiently.

Method: The proposed method categorizes data fields and uses LLMs to infer distributions, creating reusable sampling scripts for generating realistic datasets without continuous LLM inference.

Result: Experiments demonstrate superior diversity and realism in synthetic datasets compared to direct LLM methods, while drastically reducing time and cost.

Conclusion: This approach enables efficient, scalable synthetic data generation, with potential applications in production systems to accelerate testing and development cycles.

Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios
where real-world data collection and usage are limited by cost and scarcity.
Large language models (LLMs) have demonstrated remarkable capabilities in
producing high-fidelity, domain-relevant samples across various fields.
However, existing approaches that directly use LLMs to generate each record
individually impose prohibitive time and cost burdens, particularly when large
volumes of synthetic data are required. In this work, we propose a fast,
cost-effective method for realistic tabular data synthesis that leverages LLMs
to infer and encode each field's distribution into a reusable sampling script.
By automatically classifying fields into numerical, categorical, or free-text
types, the LLM generates distribution-based scripts that can efficiently
produce diverse, realistic datasets at scale without continuous model
inference. Experimental results show that our approach outperforms traditional
direct methods in both diversity and data realism, substantially reducing the
burden of high-volume synthetic data generation. We plan to apply this
methodology to accelerate testing in production pipelines, thereby shortening
development cycles and improving overall system efficiency. We believe our
insights and lessons learned will aid researchers and practitioners seeking
scalable, cost-effective solutions for synthetic data generation.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [431] [APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation](https://arxiv.org/abs/2507.14270)
*Ravin Kumar*

Main category: cs.NE

TL;DR: This paper introduces the APTx Neuron, an efficient and expressive computational unit that combines activation and transformation in a single trainable expression. It achieves high test accuracy on the MNIST dataset while reducing computational complexity.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address inefficiencies in traditional neural network architectures that separate non-linear activation and linear transformation, leading to redundant computations and complexity.

Method: The APTx Neuron is formulated as a unified trainable unit ($y = \sum_{i=1}^{n} ((\alpha_i + \tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$) where parameters $\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are optimized during training. It eliminates the need for separate activation layers.

Result: Using the MNIST dataset, the APTx Neuron-based architecture achieves 96.69% test accuracy in 20 epochs with only 332K trainable parameters, demonstrating efficiency and high performance.

Conclusion: The APTx Neuron offers a more computationally efficient and elegant design for neural architectures, setting a foundation for a new approach in neuron design and network optimization.

Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that
integrates non-linear activation and linear transformation into a single
trainable expression. The APTx Neuron is derived from the APTx activation
function, thereby eliminating the need for separate activation layers and
making the architecture both computationally efficient and elegant. The
proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i +
\tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters
$\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our
APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69\%
test accuracy in just 20 epochs using approximately 332K trainable parameters.
The results highlight the superior expressiveness and computational efficiency
of the APTx Neuron compared to traditional neurons, pointing toward a new
paradigm in unified neuron design and the architectures built upon it.

</details>


### [432] [Training oscillator Ising machines to assign the dynamic stability of their equilibrium points](https://arxiv.org/abs/2507.14386)
*Yi Cheng,Zongli Lin*

Main category: cs.NE

TL;DR: The paper introduces a neural network model based on Oscillator Ising Machines (OIM) for associative memory, proposing a Hamiltonian-Regularized Eigenvalue Contrastive Method (HRECM) to efficiently train coupling weights and validate their solution through experiments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the design of coupling weights in neural network models to achieve Hopfield-like associative memory by leveraging the structural stability of the equilibrium points in Oscillator Ising Machines (OIM).

Method: The method involves creating a link between the stability and Hamiltonian energy of equilibrium points in OIMs and developing the Hamiltonian-Regularized Eigenvalue Contrastive Method (HRECM) for training coupling weights that assign proper stability to binary equilibrium points.

Result: The numerical experiments confirmed the validity and effectiveness of HRECM in assigning appropriate stability to the equilibrium points and achieving associative memory in the OIM model.

Conclusion: The study demonstrates that the Hamiltonian-based training method, HRECM, simplifies the design of coupling weights by focusing on stability assignment for OIMs, thereby enhancing their potential for memory storage applications.

Abstract: We propose a neural network model, which, with appropriate assignment of the
stability of its equilibrium points (EPs), achieves Hopfield-like associative
memory. The oscillator Ising machine (OIM) is an ideal candidates for such a
model, as all its $0/\pi$ binary EPs are structurally stable with their dynamic
stability tunable by the coupling weights. Traditional Hopfield-based models
store the desired patterns by designing the coupling weights between neurons.
The design of coupling weights should simultaneously take into account both the
existence and the dynamic stability of the EPs for the storage of the desired
patterns. For OIMs, since all $0/\pi$ binary EPs are structurally stable, the
design of the coupling weights needs only to focus on assigning appropriate
stability for the $0/\pi$ binary EPs according to the desired patterns. In this
paper, we establish a connection between the stability and the Hamiltonian
energy of EPs for OIMs, and, based on this connection, provide a
Hamiltonian-Regularized Eigenvalue Contrastive Method (HRECM) to train the
coupling weights of OIMs for assigning appropriate stability to their EPs.
Finally, numerical experiments are performed to validate the effectiveness of
the proposed method.

</details>


### [433] [Analyzing Internal Activity and Robustness of SNNs Across Neuron Parameter Space](https://arxiv.org/abs/2507.14757)
*Szymon Mazurek,Jakub Caputa,Maciej Wielgosz*

Main category: cs.NE

TL;DR: The paper explores a constrained hyperparameter space for optimizing Spiking Neural Networks (SNNs), balancing classification accuracy and energy efficiency, while addressing robustness to adversarial noise.


<details>
  <summary>Details</summary>
Motivation: To address the critical dependence of SNNs' performance on properly tuned neuron model parameters, which affect energy efficiency and task performance.

Method: The paper systematically explores the neuron hyperparameter domain (membrane time constant and voltage threshold), visualizes operational trade-offs, and evaluates robustness under adversarial noise.

Result: The study identifies an operational space for effective SNN behavior, provides efficient operating points, and shows that operating outside this region increases spike correlation and internal synchrony.

Conclusion: Hyperparameter tuning within the identified operational manifold is crucial to balance SNN energy efficiency, task performance, and robustness in neuromorphic applications.

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient and biologically
plausible alternatives to traditional artificial neural networks, but their
performance depends critically on the tuning of neuron model parameters. In
this work, we identify and characterize an operational space - a constrained
region in the neuron hyperparameter domain (specifically membrane time constant
tau and voltage threshold vth) - within which the network exhibits meaningful
activity and functional behavior. Operating inside this manifold yields optimal
trade-offs between classification accuracy and spiking activity, while stepping
outside leads to degeneration: either excessive energy use or complete network
silence.
  Through systematic exploration across datasets and architectures, we
visualize and quantify this manifold and identify efficient operating points.
We further assess robustness to adversarial noise, showing that SNNs exhibit
increased spike correlation and internal synchrony when operating outside their
optimal region. These findings highlight the importance of principled
hyperparameter tuning to ensure both task performance and energy efficiency.
Our results offer practical guidelines for deploying robust and efficient SNNs,
particularly in neuromorphic computing scenarios.

</details>


### [434] [DHEvo: Data-Algorithm Based Heuristic Evolution for Generalizable MILP Solving](https://arxiv.org/abs/2507.15615)
*Zhihao Zhang,Siyuan Li,Chenxi Li,Feifan Liu,Mengjing Chen,Kai Li,Tao Zhong,Bo An,Peng Liu*

Main category: cs.NE

TL;DR: The paper introduces DHEvo, a framework addressing limitations in LLM-based heuristics for mixed integer programming (MILP) by evolving representative instances and corresponding heuristics iteratively.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the poor generalization of Large Language Model (LLM)-based primal heuristics within problem classes in mixed integer programming (MILP).

Method: The proposed method, DHEvo, combines a data-algorithm co-evolution framework, where representative instances and corresponding heuristics are iteratively refined using a multi-agent system guided by fitness scores.

Result: Experiments on diverse MILP benchmarks showed that DHEvo outperforms traditional hand-crafted heuristics and existing LLM-based methods.

Conclusion: DHEvo enhances the efficiency and generalization of primal heuristics for MILP by leveraging iterative co-evolution of data instances and algorithms.

Abstract: Primal heuristics play a critical role in improving the efficiency of mixed
integer programming (MILP) solvers. As large language models (LLMs) have
demonstrated superior code generation abilities, recent MILP works are devoted
to leveraging the evolutionary computation approaches with LLMs to generate
effective primal heuristics. Although the generated heuristics have achieved
better solving performance than the hand-crafted ones with little adaptability,
the advantage of current LLM-based methods is limited to few MILP instances in
one problem class, as they fail to capture the instance characteristics in the
problem class (the MILP instances generated from the same mathematical model
are defined as a problem class). Since MILP instances often differ
significantly in structure and feature distribution, the neglect of their
characteristics in the evolution process results in poor generalization within
the same problem class. To overcome this challenge, we propose a data-algorithm
co-evolution framework (DHEvo) that iteratively selects representative
instances and evolves corresponding heuristics. With the initial instance
distribution, we develop an LLM-based multi-agent system to generate data-code
pairs simultaneously. These data-code pairs are iteratively refined based on
their fitness scores, leading to the identification of the most effective
heuristic over the entire problem class. Extensive experiments across diverse
MILP benchmarks demonstrate that our approach significantly outperforms both
human-designed heuristics and existing LLM-based methods.

</details>


### [435] [TONUS: Neuromorphic human pose estimation for artistic sound co-creation](https://arxiv.org/abs/2507.15734)
*Jules Lecomte,Konrad Zinner,Michael Neumeier,Axel von Arnim*

Main category: cs.NE

TL;DR: This paper presents an artistic sound installation using neuromorphic body sensing for intuitive human-machine interaction, emphasizing seamless integration with human imagination.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the disconnect between the technical and artistic aspects of human-machine interaction, aiming to create a seamless interface between machines and human imagination or emotions.

Method: A neuromorphic multihead human pose estimation neural sensor is designed to shape soundscapes and visuals based on fine body movements. It utilizes a spiking neural network tailored for a neuromorphic chip.

Result: The installation allows visitors to engage with a sound atmosphere and a neurally processed representation of themselves, enabling an intuitive dialogue between humans and machines.

Conclusion: The proposed system demonstrates an innovative way to merge art and technology, enhancing the creativity and emotional connection in human-machine interactions.

Abstract: Human machine interaction is a huge source of inspiration in today's media
art and digital design, as machines and humans merge together more and more.
Its place in art reflects its growing applications in industry, such as
robotics. However, those interactions often remains too technical and
machine-driven for people to really engage into. On the artistic side, new
technologies are often not explored in their full potential and lag a bit
behind, so that state-of-the-art research does not make its way up to museums
and exhibitions. Machines should support people's imagination and poetry in a
seamless interface to their body or soul. We propose an artistic sound
installation featuring neuromorphic body sensing to support a direct yet non
intrusive interaction with the visitor with the purpose of creating sound
scapes together with the machine. We design a neuromorphic multihead human pose
estimation neural sensor that shapes sound scapes and visual output with fine
body movement control. In particular, the feature extractor is a spiking neural
network tailored for a dedicated neuromorphic chip. The visitor, immersed in a
sound atmosphere and a neurally processed representation of themselves that
they control, experience the dialogue with a machine that thinks neurally,
similarly to them.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [436] [NPUEval: Optimizing NPU Kernels with LLMs and Open Source Compilers](https://arxiv.org/abs/2507.14403)
*Sarunas Kalade,Graham Schelle*

Main category: cs.PL

TL;DR: The paper introduces NPUEval, a benchmark to evaluate the generation and optimization of NPU kernels using LLMs, indicating challenges due to underrepresented domain-specific data.


<details>
  <summary>Details</summary>
Motivation: NPUs are becoming critical in power-sensitive devices, but creating optimized kernels for AI workloads requires expertise in domain-specific programming, which is scarce for NPUs compared to GPUs.

Method: The authors developed NPUEval, a benchmark comprising 102 common machine learning operators, and assessed LLM-generated NPU code for correctness and efficiency using AMD NPU hardware with open source tools.

Result: The DeepSeek R1 model achieved over 50% vectorization for some kernels but averaged only 10% across the dataset, highlighting the difficulty of optimizing NPU kernels with current LLMs.

Conclusion: NPUEval provides an important tool for advancing research in NPU kernel code generation and optimization, emphasizing the challenges associated with domain-specific programming.

Abstract: Neural processing units (NPUs) are gaining prominence in power-sensitive
devices like client devices, with AI PCs being defined by their inclusion of
these specialized processors. Running AI workloads efficiently on these devices
requires libraries of optimized kernels. Creating efficient kernels demands
expertise in domain-specific C++ with vector intrinsics and in-depth knowledge
of the target architecture. Unlike GPU programming, which has had years to
mature, NPU programming is new, with smaller and more fragmented developer
communities across hardware platforms. This fragmentation poses a challenge
when utilizing LLMs to assist in writing NPU kernels, as domain-specific
optimized code examples are underrepresented in LLM pre-training data.
  In this paper we introduce NPUEval -- a benchmark for writing and evaluating
NPU kernels, consisting of 102 common operators for machine learning workloads.
We evaluate LLM generated code on actual hardware based on both functional
correctness and vectorization efficiency using open source compiler tools
targeting the AMD NPU. We evaluate a range of state-of-the-art LLMs with a mix
of proprietary and open-weight models. Latest reasoning models like DeepSeek
R1, show promising results achieving out-of-the-box 50%+ vectorization on
select kernels. However, the average score across the entire dataset remains
roughly 10% even with compiler feedback and vectorized kernel examples --
showing that this is a challenging dataset even for frontier models. The
dataset and evaluation code will be released with a permissive open source
license, providing an essential benchmark for advancing research in code
generation and NPU kernel optimization.

</details>


### [437] [Timetide: A programming model for logically synchronous distributed systems](https://arxiv.org/abs/2507.14471)
*Logan Kenwright,Partha Roop,Nathan Allen,Călin Caşcaval,Avinash Malik*

Main category: cs.PL

TL;DR: The paper introduces Timetide, a multiclock synchronous programming model suitable for distributed systems without relying on physical clock synchronization.


<details>
  <summary>Details</summary>
Motivation: Distributed system programming faces challenges with deterministic execution, as current approaches either rely on centralized systems or expensive physical clock synchronization, which are not scalable.

Method: The authors propose a novel multiclock semantics called Timetide, which uses logical synchrony instead of physical clock synchronization and is designed for seamless distribution and formal verification.

Result: Timetide provides a deterministic distributed programming solution that addresses computation distribution challenges like network delays, without requiring physical clock synchronization.

Conclusion: Timetide is the first multiclock synchronous programming language that supports deterministic distributed systems, enables formal verification, and eliminates the need for physical clock synchronization.

Abstract: Massive strides in deterministic models have been made using synchronous
languages. They are mainly focused on centralised applications, as the
traditional approach is to compile away the concurrency. Time triggered
languages such as Giotto and Lingua Franca are suitable for distribution albeit
that they rely on expensive physical clock synchronisation, which is both
expensive and may suffer from scalability. Hence, deterministic programming of
distributed systems remains challenging. We address the challenges of
deterministic distribution by developing a novel multiclock semantics of
synchronous programs. The developed semantics is amenable to seamless
distribution. Moreover, our programming model, Timetide, alleviates the need
for physical clock synchronisation by building on the recently proposed logical
synchrony model for distributed systems. We discuss the important aspects of
distributing computation, such as network communication delays, and explore the
formal verification of Timetide programs. To the best of our knowledge,
Timetide is the first multiclock synchronous language that is both amenable to
distribution and formal verification without the need for physical clock
synchronisation or clock gating.

</details>


### [438] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: This paper presents a Python debugging plugin that uses voice feedback and GUI visualization to improve error diagnostics, reducing cognitive load and boosting efficiency.


<details>
  <summary>Details</summary>
Motivation: Debugging often involves high cognitive effort and inaccessible error diagnostics, particularly for visually impaired users.

Method: The plugin integrates a global exception hook with pyttsx3 text-to-speech conversion for voice feedback and Tkinter for GUI visualization, providing multimodal error diagnosis.

Result: Empirical tests show a 37% reduction in cognitive load and 78% faster error identification, with low latency and CPU overhead.

Conclusion: The plugin enhances accessibility and multitasking in debugging, with future features planned to include GPT-based repair suggestions and multilingual capabilities.

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


### [439] [Invariant Generation for Floating-Point Programs via Constraint Solving](https://arxiv.org/abs/2507.15017)
*Xuran Cai,Liqian Chen,Hongfei Fu*

Main category: cs.PL

TL;DR: This paper introduces a framework to generate tight invariants for floating-point programs while considering the impact of rounding errors, integrating differential characterization and constraint solving techniques.


<details>
  <summary>Details</summary>
Motivation: Floating-point computations are prone to rounding errors, which, when accumulated, can lead to catastrophic program failures. Accurate invariant generation is needed to ensure the correctness of programs under such errors.

Method: The authors propose a theoretical framework combining FPTaylor's first-order differential characterization with constraint solving methods and devise two algorithms for polynomial invariant generation that handle conditional branches in floating-point computations.

Result: Experimental results demonstrate that the proposed algorithms outperform state-of-the-art methods in both computational efficiency and the precision of generated invariants across diverse benchmarks.

Conclusion: The framework successfully offers a novel and efficient approach to address floating-point errors in programs, improving reliability and correctness in numeric-intensive software.

Abstract: In numeric-intensive computations, it is well known that the execution of
floating-point programs is imprecise as floating point arithmetics (e.g.,
addition, subtraction, multiplication, division, etc.) incurs rounding errors.
Albeit the rounding error is small for every single floating-point operation,
the aggregation of such error in multiple operations may be dramatic and cause
catastrophic program failures. Therefore, to ensure the correctness of
floating-point programs, the effect of floating point error needs to be
carefully taken into account. In this work, we consider the invariant
generation for floating point programs, whose aim is to generate tight
invariants under the perturbation of floating point errors. Our main
contribution is a theoretical framework on how to apply constraint solving
methods to address the invariant generation problem. In our framework, we
propose a novel combination between the first-order differential
characterization by FPTaylor (TOPLAS 2018) and constraint solving methods,
aiming to reduce the computational burden of constraint solving. Moreover, we
devise two polynomial invariant generation algorithms to instantiate the
framework. The first algorithm is applicable to a wide range of floating-point
operations but requires an initial (coarse) invariant as external input, while
the second does not require an initial invariant but is limited to polynomial
programs. Furthermore, we show how conditional branches, a difficult issue in
floating-point analysis, can be handled in our framework. Experimental results
show that our algorithms outperform SOTA approaches in both the time efficiency
and the precision of the generated invariants over a variety of benchmarks.

</details>


### [440] [A Few Fit Most: Improving Performance Portability of SGEMM on GPUs using Multi-Versioning](https://arxiv.org/abs/2507.15277)
*Robert Hochgraf,Sreepathi Pai*

Main category: cs.PL

TL;DR: The paper presents a framework titled "portability tuning" aimed at generating multi-versioned linear algebra kernels for GPUs that maintain high performance across diverse environments without retuning, outpacing traditional autotuning methods.


<details>
  <summary>Details</summary>
Motivation: Autotuning methods for linear algebra kernels on GPUs face issues of overfitting to specific environments, requiring frequent retuning for changes in devices or input factors. The motivation is to address this labor-intensive and inflexible process through an alternative approach.

Method: The authors leverage multi-versioning, where multiple variants of the same code are generated, coupled with their "portability tuning" framework to ensure performance portability across environments. They evaluate this approach using execution time data for GEMM kernels from the CLBlast library.

Result: The proposed portability tuning techniques outperform CLBlast’s default kernels and approach within 10% of theoretical maximum performance. Additionally, the generated programs generalize effectively across unseen devices without requiring retuning.

Conclusion: Multi-versioning combined with portability tuning is a viable strategy to achieve high-performance, performance-portable linear algebra kernels on GPUs, eliminating the need for repetitive autotuning under changing execution environments.

Abstract: Hand-optimizing linear algebra kernels for different GPU devices and
applications is complex and labor-intensive. Instead, many developers use
automatic performance tuning (autotuning) to achieve high performance on a
variety of devices. However, autotuning "overfits", and must be redone if any
part of the environment changes, such as if the device or input characteristics
change.
  In most non-trivial cases, a single compute kernel cannot maintain
near-optimal performance across all environments. Changing the kernel to
specialize it to the current execution environment is possible, but on GPUs,
runtime tuning and compilation can be expensive.
  In this work, we use multi-versioning -- producing several variants of the
same code -- as a way to generate performance portable code. We describe a
framework called portability tuning that can automatically generate
multi-versioned code whose performance is portable, requiring no retuning.
  We evaluate our framework on a dataset of execution times for GEMM kernels
from the CLBlast linear algebra library. We find our portability tuning
techniques outperform CLBlast's default kernels -- often approaching within 10%
of the theoretical maximum performance -- despite CLBlast using autotuning
techniques. Further, we find that our generated programs generalize well to new
and unseen devices, matching the performance of autotuning without ever
portability tuning for those devices.

</details>


### [441] [Bayesian Separation Logic](https://arxiv.org/abs/2507.15530)
*Shing Hin Ho,Nicolas Wu,Azalea Raad*

Main category: cs.PL

TL;DR: The paper introduces Bayesian separation logic (BaSL), a new logic framework for reasoning about Bayesian probabilistic programming languages (BPPLs), enabling the analysis of concepts like Bayesian updating and independence. It provides theoretical foundations and applies them to practical statistical models.


<details>
  <summary>Details</summary>
Motivation: Existing separation logics cannot handle Bayesian updating, a key feature of Bayesian probabilistic programming languages (BPPLs). The authors aim to fill this gap to enable reasoning about properties like independence and expected values in BPPLs.

Method: The authors developed Bayesian separation logic (BaSL) by leveraging the Rokhlin-Simmons disintegration theorem from measure theory and creating a new Kripke resource monoid model. They prove theoretical concepts and connect BaSL semantics to BPPLs' existing denotational semantics.

Result: BaSL successfully models Bayesian updating, unnormalized and conditional distributions, and other Bayesian concepts. It also proves properties of statistical models such as expected values, posterior distributions, and variable correlations in Bayesian networks.

Conclusion: BaSL provides a modular and mathematically sound framework to analyze BPPLs, bridging a significant gap and enabling reasoning about Bayesian updating and other concepts in probabilistic programming.

Abstract: Bayesian probabilistic programming languages (BPPLs) let users denote
statistical models as code while the interpreter infers the posterior
distribution. The semantics of BPPLs are usually mathematically complex and
unable to reason about desirable properties such as expected values and
independence of random variables. To reason about these properties in a
non-Bayesian setting, probabilistic separation logics such as PSL and Lilac
interpret separating conjunction as probabilistic independence of random
variables. However, no existing separation logic can handle Bayesian updating,
which is the key distinguishing feature of BPPLs.
  To close this gap, we introduce Bayesian separation logic (BaSL), a
probabilistic separation logic that gives semantics to BPPL. We prove an
internal version of Bayes' theorem using a result in measure theory known as
the Rokhlin-Simmons disintegration theorem. Consequently, BaSL can model
probabilistic programming concepts such as Bayesian updating, unnormalised
distribution, conditional distribution, soft constraint, conjugate prior and
improper prior while maintaining modularity via the frame rule. The model of
BaSL is based on a novel instantiation of Kripke resource monoid via
$\sigma$-finite measure spaces over the Hilbert cube, and the semantics of
Hoare triple is compatible with an existing denotational semantics of BPPL
based on the category of $s$-finite kernels. Using BaSL, we then prove
properties of statistical models such as the expected value of Bayesian coin
flip, correlation of random variables in the collider Bayesian network, and the
posterior distributions of the burglar alarm model, a parameter estimation
algorithm, and the Gaussian mixture model.

</details>


### [442] [Formal Analysis of Networked PLC Controllers Interacting with Physical Environments](https://arxiv.org/abs/2507.15596)
*Jaeseo Lee,Kyungmin Bae*

Main category: cs.PL

TL;DR: This paper proposes a formal framework for analyzing PLC-driven systems that combines discrete PLC semantics, network communication, and physical dynamics, while mitigating state explosion via partial order reduction.


<details>
  <summary>Details</summary>
Motivation: Current formal verification methods for PLCs often overlook key real-world factors such as interaction with physical environments and network delays.

Method: The authors develop a unified formal framework combining discrete PLC behaviors, communication networks, and continuous physical dynamics, and apply partial order reduction to manage state explosion.

Result: The framework allows precise analysis of complex PLC-driven systems with continuous dynamics and networked communication, addressing limitations of prior methods.

Conclusion: This approach enhances the accuracy and scalability of formal verification for industrial PLC systems in real-world scenarios.

Abstract: Programmable Logic Controllers (PLCs) are widely used in industrial
automation to control physical systems. As PLC applications become increasingly
complex, ensuring their correctness is crucial. Existing formal verification
techniques focus on individual PLC programs in isolation, often neglecting
interactions with physical environments and network communication between
controllers. This limitation poses significant challenges in analyzing
real-world industrial systems, where continuous dynamics and communication
delays play a critical role. In this paper, we present a unified formal
framework that integrates discrete PLC semantics, networked communication, and
continuous physical behaviors. To mitigate state explosion, we apply partial
order reduction, significantly reducing the number of explored states while
maintaining correctness. Our framework enables precise analysis of PLC-driven
systems with continuous dynamics and networked communication.

</details>


### [443] [Closure Conversion, Flat Environments, and the Complexity of Abstract Machines](https://arxiv.org/abs/2507.15843)
*Beniamino Accattoli,Dan Ghica,Giulio Guerrieri,Cláudio Belo Lourenço,Claudio Sacerdoti Coen*

Main category: cs.PL

TL;DR: The paper investigates the relationship between closure conversion and abstract machines in compilers for functional languages, offering new proof techniques, improved handling of environments, and insights into time complexity.


<details>
  <summary>Details</summary>
Motivation: To enhance the understanding of the relationship between closure conversion and abstract machines, and to improve techniques for correctness, environment handling, and performance analysis.

Method: The authors study a simplistic lambda-calculus with tuples as the source language, explore abstract machines for both the source and target of closure conversion, focus on flat closures/environments, and analyze time complexity impacts.

Result: They introduce a new correctness proof technique, improve environment handling in abstract machines through closure invariants, and demonstrate that closure conversion impacts dynamic costs and code size but maintains overall machine complexity.

Conclusion: Closure conversion and abstract machines are deeply interconnected. The proposed advancements offer stronger correctness proofs, better environment handling, and insights into the trade-offs of performance.

Abstract: Closure conversion is a program transformation at work in compilers for
functional languages to turn inner functions into global ones, by building
closures pairing the transformed functions with the environment of their free
variables. Abstract machines rely on similar and yet different concepts of
closures and environments.
  In this paper, we study the relationship between the two approaches. We adopt
a very simple {\lambda}-calculus with tuples as source language and study
abstract machines for both the source language and the target of closure
conversion. Moreover, we focus on the simple case of flat
closures/environments, that is, with no sharing of environments. We provide
three contributions.
  Firstly, a new simple proof technique for the correctness of closure
conversion, inspired by abstract machines.
  Secondly, we show how the closure invariants of the target language allow us
to design a new way of handling environments in abstract machines, not
suffering the shortcomings of other styles.
  Thirdly, we study the machines from the point of view of time complexity,
adapting analyses by Accattoli and co-authors. We show that closure conversion
decreases various dynamic costs while increasing the size of the initial code.
Despite these changes, the overall complexity of the machines before and after
closure conversion turns out to be the same.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [444] [Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach](https://arxiv.org/abs/2507.14249)
*Yuejiao Xie,Maonan Wang,Di Zhou,Man-On Pun,Zhu Han*

Main category: cs.RO

TL;DR: This paper introduces a Multi-Source Hybrid Attention Reinforcement Learning (MSHA-RL) framework to optimize urban air mobility trajectory planning with enhanced communication quality and adaptability.


<details>
  <summary>Details</summary>
Motivation: The need arises from the limitations of conventional UAM trajectory planning strategies that struggle with dynamic ride-sharing demands and ensuring communication quality in urban environments.

Method: The method involves constructing a radio map to assess communication quality and employing the MSHA-RL framework that aligns diverse data sources and uses hybrid attention for real-time and adaptive path planning.

Result: Experimental results show improved travel time, operational efficiency, and safety through communication-compliant trajectory planning.

Conclusion: The proposed approach effectively addresses the challenges in real-time trajectory planning for UAM systems, enhancing passenger experience and safety while maintaining adaptability.

Abstract: Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions
to alleviate urban congestion, with path planning becoming a key focus area.
Unlike ground transportation, UAM trajectory planning has to prioritize
communication quality for accurate location tracking in constantly changing
environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,
requires adaptive planning to respond to real-time passenger requests,
especially in ride-sharing scenarios where passenger demands are unpredictable
and dynamic. However, conventional trajectory planning strategies based on
predefined routes lack the flexibility to meet varied passenger ride demands.
To address these challenges, this work first proposes constructing a radio map
to evaluate the communication quality of urban airspace. Building on this, we
introduce a novel Multi-Source Hybrid Attention Reinforcement Learning
(MSHA-RL) framework for the challenge of effectively focusing on passengers and
UAM locations, which arises from the significant dimensional disparity between
the representations. This model first generates the alignment among diverse
data sources with large gap dimensions before employing hybrid attention to
balance global and local insights, thereby facilitating responsive, real-time
path planning. Extensive experimental results demonstrate that the approach
enables communication-compliant trajectory planning, reducing travel time and
enhancing operational efficiency while prioritizing passenger safety.

</details>


### [445] [A Recursive Lie-Group Formulation for the Second-Order Time Derivatives of the Inverse Dynamics of parallel Kinematic Manipulators](https://arxiv.org/abs/2507.14274)
*Andreas Mueller,Shivesh Kumar,Thomas Kordik*

Main category: cs.RO

TL;DR: The paper introduces computational algorithms for evaluating second time derivatives of inverse dynamics in trajectory control of parallel kinematics manipulators (PKM) equipped with series elastic actuators (SEA).


<details>
  <summary>Details</summary>
Motivation: The challenge lies in computationally efficient trajectory control for PKM equipped with SEAs, given a lack of existing solutions to compute second time derivatives of inverse dynamics.

Method: The recursive algorithms for evaluating inverse dynamics from serial robots are reused and adapted for PKM, utilizing a Lie group formulation for mathematical derivation.

Result: Numerical results showcase the effectiveness on a 6-DOF Gough-Stewart platform and a planar PKM using flatness-based control.

Conclusion: This paper innovatively addresses trajectory control for PKM-SEAs by deriving efficient computation for second time derivatives, paving the way for better robotic designs.

Abstract: Series elastic actuators (SEA) were introduced for serial robotic arms. Their
model-based trajectory tracking control requires the second time derivatives of
the inverse dynamics solution, for which algorithms were proposed. Trajectory
control of parallel kinematics manipulators (PKM) equipped with SEAs has not
yet been pursued. Key element for this is the computationally efficient
evaluation of the second time derivative of the inverse dynamics solution. This
has not been presented in the literature, and is addressed in the present paper
for the first time. The special topology of PKM is exploited reusing the
recursive algorithms for evaluating the inverse dynamics of serial robots. A
Lie group formulation is used and all relations are derived within this
framework. Numerical results are presented for a 6-DOF Gough-Stewart platform
(as part of an exoskeleton), and for a planar PKM when a flatness-based control
scheme is applied.

</details>


### [446] [Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support](https://arxiv.org/abs/2507.14412)
*Mengxue Fu,Zhonghao Shi,Minyu Huang,Siqi Liu,Mina Kian,Yirui Song,Maja J. Matarić*

Main category: cs.RO

TL;DR: The paper explores using end-to-end speech-language models (SLMs) with socially assistive robots (SARs) for better dialogue systems, conducting a small user study to evaluate usability and identify limitations.


<details>
  <summary>Details</summary>
Motivation: Existing dialogue systems for SARs have limitations in latency, back-channeling, and personalized speech interactions, which hinder their ability to provide effective well-being support.

Method: The authors integrated end-to-end SLMs into SARs and conducted a small user study (N = 11 university students) to evaluate usability and gather feedback.

Result: Participants appreciated the SAR system's empathetic feedback, natural turn-taking, back-channeling, and adaptive responses but criticized the robot's generic verbal feedback and the lack of expressive and synchronized nonverbal behavior.

Conclusion: The study highlights the need for synchronization in robot movements, improved prompting, fine-tuned outputs for mental health, and adaptive vocal expressions to enhance SAR dialogue systems.

Abstract: Socially assistive robots (SARs) have shown great potential for supplementing
well-being support. However, prior studies have found that existing dialogue
pipelines for SARs remain limited in real-time latency, back-channeling, and
personalized speech dialogue. Toward addressing these limitations, we propose
using integrated end-to-end speech-language models (SLMs) with SARs. This work
1) evaluated the usability of an SLM-enabled SAR dialogue system through a
small user study, and 2) identified remaining limitations through study user
feedback to inform future improvements. We conducted a small within-participant
user study with university students (N = 11) whose results showed that
participants perceived an SLM-enabled SAR system as capable of providing
empathetic feedback, natural turn-taking, back-channeling, and adaptive
responses. We also found that participants reported the robot's nonverbal
behaviors as lacking variability and synchronization with conversation, and the
SLM's verbal feedback as generic and repetitive. These findings highlighted the
need for real-time robot movement synchronized with conversation, improved
prompting or fine-tuning to generate outputs better aligned with mental health
practices, and more expressive, adaptive vocal generation.

</details>


### [447] [Koopman Operator Based Time-Delay Embeddings and State History Augmented LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking](https://arxiv.org/abs/2507.14455)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: The paper explores time-delay embedding to model periodic hybrid systems and introduces a novel feedback control method using state history.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of modeling periodic hybrid systems and control them effectively using linear state space techniques.

Method: Extend time-delay embeddings to periodic hybrid systems and integrate state history into Linear Quadratic Regulator (LQR) for control.

Result: Developed linear models for bouncing pendulum and the simplest walker, showcasing effective state history augmented LQR.

Conclusion: Periodic hybrid systems with consistent modes and timings can effectively utilize state history for enhanced control through linear modeling.

Abstract: Time-delay embedding is a technique that uses snapshots of state history over
time to build a linear state space model of a nonlinear smooth system. We
demonstrate that periodic non-smooth or hybrid system can also be modeled as a
linear state space system using this approach as long as its behavior is
consistent in modes and timings. We extended time-delay embeddings to generate
a linear model of two periodic hybrid systems: the bouncing pendulum and the
simplest walker with control inputs. This leads to a novel state history
augmented linear quadratic regulator (LQR) which uses current and past state
history for feedback control.

</details>


### [448] [A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0](https://arxiv.org/abs/2507.14538)
*Jin Chai,Xiang Yao,Mengfan Hou,Yanghong Li,Erbao Dong*

Main category: cs.RO

TL;DR: The paper presents CYJ Hand-0, a 21-DOF humanoid hand with a hybrid tendon-driven system utilizing SMAs and DC motors for biomimetic dexterity.


<details>
  <summary>Details</summary>
Motivation: To design and validate a humanoid hand that closely mimics the kinematics and actuation systems of a human hand for high dexterity and compactness.

Method: Developed a 21-DOF robotic hand using a combination of SMA-driven and motor-driven modules, using 3D-printed components modeled after human hand structure, controlled via Arduino Mega 2560.

Result: Mechanical and kinematic validations confirmed that the CYJ Hand-0 achieves effective and biomimetic dexterity.

Conclusion: The CYJ Hand-0 successfully integrates hybrid actuation into a compact design, demonstrating advanced humanoid hand capabilities.

Abstract: CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid
tendon-driven actuation system that combines shape memory alloys (SMAs) and DC
motors. The hand employs high-strength fishing line as artificial tendons and
uses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal
and tendon-muscle structure of the human hand. A linear motor-driven module
controls finger flexion, while an SMA-based module enables finger extension and
lateral abduction. These modules are integrated into a compact hybrid actuation
unit mounted on a custom rear support structure. Mechanical and kinematic
experiments, conducted under an Arduino Mega 2560-based control system,
validate the effectiveness of the design and demonstrate its biomimetic
dexterity.

</details>


### [449] [BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives](https://arxiv.org/abs/2507.14582)
*Zezhi Liu,Shizhen Wu,Hanqian Luo,Deyun Qin,Yongchun Fang*

Main category: cs.RO

TL;DR: The paper introduces a hierarchical framework, BT-TL-DMPs, that combines Behavior Trees (BT), Temporal Logic (TL), and Dynamical Movement Primitives (DMPs) to improve skill generalization in robotic tasks, especially long-horizon scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of enabling robots to generalize learned manipulation skills to novel environments, particularly in complex, multi-stage, long-horizon tasks with intricate constraints.

Method: The framework integrates BTs for high-level decision-making, STL for specifying task requirements, and an STL-constrained optimization method to adapt DMPs while preserving dynamics learned from demonstrations.

Result: Validation through simulations and real-world experiments showcased the framework's generalization capabilities under various STL constraints, effectively bridging the symbolic-motion gap in autonomous robotic manipulation.

Conclusion: The proposed BT-TL-DMPs framework enhances the reliability and adaptability of robots in performing complex, long-horizon tasks, addressing both high-level task structuring and low-level motion adaptability.

Abstract: In the field of Learning from Demonstration (LfD), enabling robots to
generalize learned manipulation skills to novel scenarios for long-horizon
tasks remains challenging. Specifically, it is still difficult for robots to
adapt the learned skills to new environments with different task and motion
requirements, especially in long-horizon, multi-stage scenarios with intricate
constraints. This paper proposes a novel hierarchical framework, called
BT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and
Dynamical Movement Primitives (DMPs) to address this problem. Within this
framework, Signal Temporal Logic (STL) is employed to formally specify complex,
long-horizon task requirements and constraints. These STL specifications are
systematically transformed to generate reactive and modular BTs for high-level
decision-making task structure. An STL-constrained DMP optimization method is
proposed to optimize the DMP forcing term, allowing the learned motion
primitives to adapt flexibly while satisfying intricate spatiotemporal
requirements and, crucially, preserving the essential dynamics learned from
demonstrations. The framework is validated through simulations demonstrating
generalization capabilities under various STL constraints and real-world
experiments on several long-horizon robotic manipulation tasks. The results
demonstrate that the proposed framework effectively bridges the symbolic-motion
gap, enabling more reliable and generalizable autonomous manipulation for
complex robotic tasks.

</details>


### [450] [Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition](https://arxiv.org/abs/2507.14605)
*Chun-Ming Yang,Pranav A. Bhounsule*

Main category: cs.RO

TL;DR: The paper explores using Koopman operator theory to enhance gait planning in quadrupedal robots through improved linear models for real-time control.


<details>
  <summary>Details</summary>
Motivation: Facilitate real-time adaptive movement in quadrupedal robots navigating diverse terrains.

Method: Employ Koopman operator theory and Extended Dynamic Mode Decomposition to develop hybrid linear models, combined with Linear Model Predictive Control for gait planning.

Result: Successfully demonstrated bounding, trotting, and gait transitions on both level and rough terrains.

Conclusion: Koopman-based models improve quadrupedal robot locomotion control, enabling smoother gait transitions in various terrains.

Abstract: Online optimal control of quadrupedal robots would enable them to plan their
movement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged
as a practical approach for real-time control. In LMPC, an optimization problem
with a quadratic cost and linear constraints is formulated over a finite
horizon and solved on the fly. However, LMPC relies on linearizing the
equations of motion (EOM), which may lead to poor solution quality. In this
paper, we use Koopman operator theory and the Extended Dynamic Mode
Decomposition (EDMD) to create a linear model of the system in high dimensional
space, thus retaining the nonlinearity of the EOM. We model the aerial phase
and ground contact phases using different linear models. Then, using LMPC, we
demonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait
transitions in level and rough terrains. The main novelty is the use of Koopman
operator theory to create hybrid models of a quadrupedal system and demonstrate
the online generation of multiple gaits and gaits transitions.

</details>


### [451] [Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks](https://arxiv.org/abs/2507.14694)
*Yue Ma,Kanglei Zhou,Fuyang Yu,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.RO

TL;DR: ProbHMI introduces a method for 3D human motion forecasting that models uncertainty explicitly using invertible networks and a latent space approach, ensuring calibrated and safe predictions.


<details>
  <summary>Details</summary>
Motivation: To enhance safety in safety-critical applications like human-robot collaboration by enabling confidence-calibrated 3D human motion forecasting.

Method: ProbHMI uses invertible networks to map poses into a disentangled latent space for probabilistic modeling and includes a module to predict future latent distributions for explicit uncertainty quantification.

Result: ProbHMI performs well on benchmarks, demonstrating strong deterministic and diverse prediction capabilities along with effective uncertainty calibration.

Conclusion: ProbHMI provides a reliable framework that integrates probabilistic dynamics modeling and uncertainty calibration, ensuring safer decisions in applications requiring human motion forecasting.

Abstract: 3D human motion forecasting aims to enable autonomous applications.
Estimating uncertainty for each prediction (i.e., confidence based on
probability density or quantile) is essential for safety-critical contexts like
human-robot collaboration to minimize risks. However, existing diverse motion
forecasting approaches struggle with uncertainty quantification due to implicit
probabilistic representations hindering uncertainty modeling. We propose
ProbHMI, which introduces invertible networks to parameterize poses in a
disentangled latent space, enabling probabilistic dynamics modeling. A
forecasting module then explicitly predicts future latent distributions,
allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI
achieves strong performance for both deterministic and diverse prediction while
validating uncertainty calibration, critical for risk-aware decision making.

</details>


### [452] [Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe Mobile Robot Navigation](https://arxiv.org/abs/2507.14700)
*Nicholas Mohammad,Nicola Bezzo*

Main category: cs.RO

TL;DR: The paper proposes a framework combining Control Lyapunov Function and Control Barrier Function with Model Predictive Contour Control to safely navigate cluttered environments.


<details>
  <summary>Details</summary>
Motivation: Address the lack of formal safety assurances in Model Predictive Contour Control for robot navigation in unknown cluttered environments.

Method: Develop a framework integrating Control Lyapunov Function and Control Barrier Function within Model Predictive Contour Control, with runtime-adaptive CBF parameters via SAC policy.

Result: Validated through extensive simulations and experiments on mobile robot navigation, demonstrating effectiveness in unknown environments.

Conclusion: The proposed framework enhances safe and feasible navigation in complex environments, enabling precise trajectory tracking with formal safety constraints.

Abstract: Safe navigation in unknown and cluttered environments remains a challenging
problem in robotics. Model Predictive Contour Control (MPCC) has shown promise
for performant obstacle avoidance by enabling precise and agile trajectory
tracking, however, existing methods lack formal safety assurances. To address
this issue, we propose a general Control Lyapunov Function (CLF) and Control
Barrier Function (CBF) enabled MPCC framework that enforces safety constraints
derived from a free-space corridor around the planned trajectory. To enhance
feasibility, we dynamically adapt the CBF parameters at runtime using a Soft
Actor-Critic (SAC) policy. The approach is validated with extensive simulations
and an experiment on mobile robot navigation in unknown cluttered environments.

</details>


### [453] [Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp Constraining Walls](https://arxiv.org/abs/2507.14721)
*Keita Kobashi,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: The paper introduces a hierarchical reinforcement learning framework to handle occluded grasping tasks, allowing robots to combine actions and adapt effectively to diverse environments.


<details>
  <summary>Details</summary>
Motivation: Addressing occluded grasping where standard parallel grippers fail due to dexterity and environmental constraints, especially in cases without short walls.

Method: Using hierarchical reinforcement learning with Q-learning for high-level policy selection, Conditional Variational Autoencoder for location guidance, and domain randomization for robust training.

Result: Achieved robust performance with successful sim-to-real transfer across six real-world objects, showing generalizability and efficiency.

Conclusion: Demonstrated that the proposed framework can effectively address occluded grasping challenges, making robots more adaptable to diverse real-world grasping tasks.

Abstract: This study addresses the problem of occluded grasping, where primary grasp
configurations of an object are not available due to occlusion with
environment. Simple parallel grippers often struggle with such tasks due to
limited dexterity and actuation constraints. Prior works have explored object
pose reorientation such as pivoting by utilizing extrinsic contacts between an
object and an environment feature like a wall, to make the object graspable.
However, such works often assume the presence of a short wall, and this
assumption may not always hold in real-world scenarios. If the wall available
for interaction is too large or too tall, the robot may still fail to grasp the
object even after pivoting, and the robot must combine different types of
actions to grasp. To address this, we propose a hierarchical reinforcement
learning (RL) framework. We use Q-learning to train a high-level policy that
selects the type of action expected to yield the highest reward. The selected
low-level skill then samples a specific robot action in continuous space. To
guide the robot to an appropriate location for executing the selected action,
we adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on
the object point cloud and the skill ID, enabling it to infer a suitable
location based on the object geometry and the selected skill. To promote
generalization, we apply domain randomization during the training of low-level
skills. The RL policy is trained entirely in simulation with a box-like object
and deployed to six objects in real world. We conduct experiments to evaluate
our method and demonstrate both its generalizability and robust sim-to-real
transfer performance with promising success rates.

</details>


### [454] [X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots](https://arxiv.org/abs/2507.14731)
*Haitong Wang,Aaron Hao Tan,Angus Fung,Goldie Nejat*

Main category: cs.RO

TL;DR: X-Nav is an end-to-end navigation framework enabling cross-embodiment deployment for wheeled and quadrupedal robots through policy distillation techniques.


<details>
  <summary>Details</summary>
Motivation: Current navigation methods are specific to certain robot types, limiting their adaptability to diverse platforms.

Method: X-Nav uses a two-stage training approach: first, multiple expert policies are trained using deep reinforcement learning across various embodiments; second, a general policy is created through action chunking with a transformer model, Nav-ACT.

Result: X-Nav demonstrated success in zero-shot transfer to new embodiments and environments, showed performance scalability with increased training diversity, and was validated through real-world experiments.

Conclusion: X-Nav can generalize across diverse robot embodiments and environments, showcasing its versatility and robustness.

Abstract: Existing navigation methods are primarily designed for specific robot
embodiments, limiting their generalizability across diverse robot platforms. In
this paper, we introduce X-Nav, a novel framework for end-to-end
cross-embodiment navigation where a single unified policy can be deployed
across various embodiments for both wheeled and quadrupedal robots. X-Nav
consists of two learning stages: 1) multiple expert policies are trained using
deep reinforcement learning with privileged observations on a wide range of
randomly generated robot embodiments; and 2) a single general policy is
distilled from the expert policies via navigation action chunking with
transformer (Nav-ACT). The general policy directly maps visual and
proprioceptive observations to low-level control commands, enabling
generalization to novel robot embodiments. Simulated experiments demonstrated
that X-Nav achieved zero-shot transfer to both unseen embodiments and
photorealistic environments. A scalability study showed that the performance of
X-Nav improves when trained with an increasing number of randomly generated
embodiments. An ablation study confirmed the design choices of X-Nav.
Furthermore, real-world experiments were conducted to validate the
generalizability of X-Nav in real-world environments.

</details>


### [455] [KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning](https://arxiv.org/abs/2507.14820)
*Bingran Chen,Baorun Li,Jian Yang,Yong Liu,Guangyao Zhai*

Main category: cs.RO

TL;DR: The paper presents KGN-Pro, a network for 6-DoF robotic grasping tasks that integrates 3D optimization using probabilistic methods, achieving improved performance.


<details>
  <summary>Details</summary>
Motivation: Enhance robotic manipulation tasks by overcoming limitations in 6-DoF grasp estimation methods such as noise sensitivity, discretization issues, and limited use of 3D information.

Method: Developed KGN-Pro, which encodes RGB-D images to predict 2D keypoints and introduces a probabilistic PnP layer for 3D optimization, enabling end-to-end learning.

Result: KGN-Pro demonstrated superior performance in both simulated and real-world experiments, surpassing existing methods in grasp cover and success rates.

Conclusion: The integration of 3D optimization with efficient 2D estimation in KGN-Pro effectively boosts its grasp estimation capabilities, making it a robust solution for robotic manipulation tasks.

Abstract: High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation
to serve as a basic function. Previous approaches either directly generate
grasps from point-cloud data, suffering from challenges with small objects and
sensor noise, or infer 3D information from RGB images, which introduces
expensive annotation requirements and discretization issues. Recent methods
mitigate some challenges by retaining a 2D representation to estimate grasp
keypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF
poses. However, these methods are limited by their non-differentiable nature
and reliance solely on 2D supervision, which hinders the full exploitation of
rich 3D information. In this work, we present KGN-Pro, a novel grasping network
that preserves the efficiency and fine-grained object grasping of previous KGNs
while integrating direct 3D optimization through probabilistic PnP layers.
KGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further
outputs a 2D confidence map to weight keypoint contributions during
re-projection error minimization. By modeling the weighted sum of squared
re-projection errors probabilistically, the network effectively transmits 3D
supervision to its 2D keypoint predictions, enabling end-to-end learning.
Experiments on both simulated and real-world platforms demonstrate that KGN-Pro
outperforms existing methods in terms of grasp cover rate and success rate.

</details>


### [456] [CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning](https://arxiv.org/abs/2507.14903)
*Pan Hu*

Main category: cs.RO

TL;DR: This paper introduces CDGMP, a framework for highway lane selection and motion planning in Connected Autonomous Vehicles (CAVs), using a Mixture of Experts architecture and reinforcement learning to enhance safety and adaptability.


<details>
  <summary>Details</summary>
Motivation: There is a significant challenge in achieving flexible and safe lane decisions, alongside precise trajectory execution, for CAVs in diverse traffic scenarios.

Method: The paper proposes CDGMP, a framework that integrates decision-making and motion planning through a Mixture of Experts architecture combined with multi-policy reinforcement learning and modular sub-networks managed by a gating mechanism.

Result: Simulation results demonstrate that CDGMP achieves reliable lane selection and motion planning performance in various traffic scenarios.

Conclusion: CDGMP offers an adaptable, robust, and scalable approach to real-world autonomous driving challenges, with potential applications in other high-dimensional decision and control tasks.

Abstract: Autonomous driving demands reliable and efficient solutions to closely
related problems such as decision-making and motion planning. In this work,
decision-making refers specifically to highway lane selection, while motion
planning involves generating control commands (such as speed and steering) to
reach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),
achieving both flexible and safe lane selection alongside precise trajectory
execution remains a significant challenge. This paper proposes a framework
called Cohesive Decision-Guided Motion Planning (CDGMP), which tightly
integrates decision-making and motion planning using a Mixture of Experts (MoE)
inspired architecture combined with multi-policy reinforcement learning. By
coordinating multiple specialized sub-networks through a gating mechanism, the
method decomposes the complex driving task into modular components. Each
sub-network focuses on a specific aspect of driving, improving efficiency by
activating only the most relevant modules during inference. This design also
enhances safety through modular specialization. CDGMP improves the adaptability
and robustness of CAVs across diverse traffic scenarios, offering a scalable
solution to real-world autonomy challenges. The architectural principles behind
CDGMP, especially the use of MoE, also provide a strong foundation for other
high-dimensional decision and control tasks. Simulation results (available at
https://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane
selection and motion planning.

</details>


### [457] [One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner](https://arxiv.org/abs/2507.14914)
*Zhexuan Xu,Jie Wang,Siyuan Xu,Zijie Geng,Mingxuan Yuan,Feng Wu*

Main category: cs.RO

TL;DR: Flora is a feedthrough and placement-aware floorplanning approach designed to optimize chip performance metrics. It introduces an innovative three-stage process to reduce inter-module feedthrough and improve in-module component placement.


<details>
  <summary>Details</summary>
Motivation: Existing floorplanning methods fail to integrate with subsequent physical design stages, causing inefficiencies like excessive inter-module feedthrough and poor in-module placement.

Method: Flora uses a three-stage floorplanning strategy: wiremask and position mask techniques for coarse optimization, local resizing for zero-whitespace layouts, and a fast tree search method for efficient component placement with cross-stage boundary adjustments.

Result: Flora achieves notable improvements compared to state-of-the-art methods: a 6% reduction in HPWL, 5.16% reduction in FTpin, 29.15% reduction in FTmod, and a 14% improvement in component placement performance.

Conclusion: The proposed Flora floorplanner effectively addresses integration challenges in floorplanning by optimizing both feedthrough and component placement, significantly improving chip design performance metrics.

Abstract: Floorplanning determines the shapes and locations of modules on a chip canvas
and plays a critical role in optimizing the chip's Power, Performance, and Area
(PPA) metrics. However, existing floorplanning approaches often fail to
integrate with subsequent physical design stages, leading to suboptimal
in-module component placement and excessive inter-module feedthrough. To tackle
this challenge, we propose Flora, a three-stage feedthrough and placement aware
rectilinear floorplanner. In the first stage, Flora employs wiremask and
position mask techniques to achieve coarse-grained optimization of HPWL and
feedthrough. In the second stage, under the constraint of a fixed outline,
Flora achieves a zero-whitespace layout by locally resizing module shapes,
thereby performing fine-grained optimization of feedthrough and improving
component placement. In the third stage, Flora utilizes a fast tree
search-based method to efficiently place components-including macros and
standard cells-within each module, subsequently adjusting module boundaries
based on the placement results to enable cross-stage optimization. Experimental
results show that Flora outperforms recent state-of-the-art floorplanning
approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,
29.15% in FTmod, and a 14% improvement in component placement performance.

</details>


### [458] [Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly](https://arxiv.org/abs/2507.14929)
*Tero Kaarlela,Sami Salo,Jose Outeiro*

Main category: cs.RO

TL;DR: The paper proposes a teleoperated system combined with automation for safe disassembly and sorting of Electric Vehicle Batteries (EVBs), improving safety, efficiency, and economic viability.


<details>
  <summary>Details</summary>
Motivation: The current manual disassembly of EVBs exposes workers to hazards such as electrocution and exposure to toxic chemicals, creating a need for safer and more efficient alternatives.

Method: A teleoperated system is developed that includes a human-in-the-loop approach for disassembling and sorting EVBs. It uses an RGB camera for aligning digital and physical twins, and employs Robot Operating System (ROS) middleware for robotic digital twins.

Result: The system reduces labor dependency, enhances safety, and increases throughput in EVB recycling. An online pilot study showed the system to be user-friendly.

Conclusion: The proposed hybrid system combining teleoperation and automation demonstrates potential in improving safety, adaptability, and economic efficiency in EVB disassembly.

Abstract: Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a
sustainable transition to electric vehicles by enabling a closed-loop supply
chain. Currently, the manual disassembly process exposes workers to hazards,
including electrocution and toxic chemicals. We propose a teleoperated system
for the safe disassembly and sorting of EVBs. A human-in-the-loop can create
and save disassembly sequences for unknown EVB types, enabling future
automation. An RGB camera aligns the physical and digital twins of the EVB, and
the digital twin of the robot is based on the Robot Operating System (ROS)
middleware. This hybrid approach combines teleoperation and automation to
improve safety, adaptability, and efficiency in EVB disassembly and sorting.
The economic contribution is realized by reducing labor dependency and
increasing throughput in battery recycling. An online pilot study was set up to
evaluate the usability of the presented approach, and the results demonstrate
the potential as a user-friendly solution.

</details>


### [459] [Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry](https://arxiv.org/abs/2507.14931)
*Qiaoqiao Ren,Remko Proesmans,Arend Pissens,Lara Dehandschutter,William Denecker,Lotte Rouckhout,Joke Carrette,Peter Vanhopplinus,Tony Belpaeme,Francis wyffels*

Main category: cs.RO

TL;DR: The study explores co-design for developing a companion robot in forensic mental health care settings to manage patient stress while collecting interaction data for interventions.


<details>
  <summary>Details</summary>
Motivation: This research seeks to address the psychological stress and lack of autonomy experienced by patients in forensic mental health care, proposing co-design as a method to create interventions that empower patients.

Method: Four co-design workshops in a forensic psychiatric clinic involved patients, caregivers, and therapists to collaboratively conceptualize and refine a companion robot prototype.

Result: Patients, caregivers, and therapists collaboratively identified ethical concerns, desired features, and emotional responses for the robot during the workshops.

Conclusion: Empowering patients in the co-design process yields tools that reflect their emotional needs and preferences, fostering better mental health support and control in restrictive settings.

Abstract: Forensic mental health care involves the treatment of individuals with severe
mental disorders who have committed violent offences. These settings are often
characterized by high levels of bureaucracy, risk avoidance, and restricted
autonomy. Patients frequently experience a profound loss of control over their
lives, leading to heightened psychological stress-sometimes resulting in
isolation as a safety measure. In this study, we explore how co-design can be
used to collaboratively develop a companion robot that helps monitor and
regulate stress while maintaining tracking of the patients' interaction
behaviours for long-term intervention. We conducted four co-design workshops in
a forensic psychiatric clinic with patients, caregivers, and therapists. Our
process began with the presentation of an initial speculative prototype to
therapists, enabling reflection on shared concerns, ethical risks, and
desirable features. This was followed by a creative ideation session with
patients, a third workshop focused on defining desired functions and emotional
responses, and we are planning a final prototype demo to gather direct patient
feedback. Our findings emphasize the importance of empowering patients in the
design process and adapting proposals based on their current emotional state.
The goal was to empower the patient in the design process and ensure each
patient's voice was heard.

</details>


### [460] [Heterogeneous object manipulation on nonlinear soft surface through linear controller](https://arxiv.org/abs/2507.14967)
*Pratik Ingle,Kasper Støy,Andres Faiña*

Main category: cs.RO

TL;DR: The paper introduces a PID-based control method to simplify high-density manipulation surfaces, aiming for robust object handling without extensive training.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and real-world limitations of high-density actuator arrays for object manipulation due to high degrees of freedom and training requirements.

Method: Implemented a PID-based linear closed-loop feedback control linked to geometric transformations for controlling the MANTA-RAY surface, bypassing extensive training.

Result: Validated the controller through simulations and physical tests, demonstrating its effectiveness in manipulating diverse objects, including fragile items, with varying geometries and textures.

Conclusion: The PID-based approach is a practical, robust, and generalized solution for soft robotic manipulation and reduces training demands, supporting real-world applications.

Abstract: Manipulation surfaces indirectly control and reposition objects by actively
modifying their shape or properties rather than directly gripping objects.
These surfaces, equipped with dense actuator arrays, generate dynamic
deformations. However, a high-density actuator array introduces considerable
complexity due to increased degrees of freedom (DOF), complicating control
tasks. High DOF restrict the implementation and utilization of manipulation
surfaces in real-world applications as the maintenance and control of such
systems exponentially increase with array/surface size. Learning-based control
approaches may ease the control complexity, but they require extensive training
samples and struggle to generalize for heterogeneous objects. In this study, we
introduce a simple, precise and robust PID-based linear close-loop feedback
control strategy for heterogeneous object manipulation on MANTA-RAY
(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation
density). Our approach employs a geometric transformation-driven PID
controller, directly mapping tilt angle control outputs(1D/2D) to actuator
commands to eliminate the need for extensive black-box training. We validate
the proposed method through simulations and experiments on a physical system,
successfully manipulating objects with diverse geometries, weights and
textures, including fragile objects like eggs and apples. The outcomes
demonstrate that our approach is highly generalized and offers a practical and
reliable solution for object manipulation on soft robotic manipulation,
facilitating real-world implementation without prohibitive training demands.

</details>


### [461] [FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models](https://arxiv.org/abs/2507.14975)
*Yufan Song,Jiatao Zhang,Zeng Gu,Qingmiao Liang,Tuocheng Hu,Wei Song,Shiqiang Zhu*

Main category: cs.RO

TL;DR: The paper introduces the Flexible Constructivism Reflection Framework (FCRF) to enhance autonomous error correction in domestic robots using flexible self-reflection mechanisms.


<details>
  <summary>Details</summary>
Motivation: Existing error correction methods in LLMs for robot task planning are constricted by rigid self-reflection, limiting their adaptability and effectiveness.

Method: FCRF employs a novel Mentor-Actor architecture that adjusts self-reflection based on task complexity while integrating historical experiences and failure lessons.

Result: The framework was tested in simulated environments (AlfWorld) and real-world scenarios, showing significant improvements in task performance and adaptability.

Conclusion: FCRF elevates self-reflective capabilities and enhances the reliability of domestic robots in executing complex, long-horizon tasks.

Abstract: Autonomous error correction is critical for domestic robots to achieve
reliable execution of complex long-horizon tasks. Prior work has explored
self-reflection in Large Language Models (LLMs) for task planning error
correction; however, existing methods are constrained by inflexible
self-reflection mechanisms that limit their effectiveness. Motivated by these
limitations and inspired by human cognitive adaptation, we propose the Flexible
Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture
that enables LLMs to perform flexible self-reflection based on task difficulty,
while constructively integrating historical valuable experience with failure
lessons. We evaluated FCRF on diverse domestic tasks through simulation in
AlfWorld and physical deployment in the real-world environment. Experimental
results demonstrate that FCRF significantly improves overall performance and
self-reflection flexibility in complex long-horizon robotic tasks.

</details>


### [462] [CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural Control Barrier Functions](https://arxiv.org/abs/2507.15022)
*Sumeadh MS,Kevin Dsouza,Ravi Prakash*

Main category: cs.RO

TL;DR: The paper introduces a novel verification method for Neural Control Barrier Functions (NCBFs) derived from expert demonstrations. It uses split-conformal prediction to effectively validate safety across state space.


<details>
  <summary>Details</summary>
Motivation: Ensuring safety in control systems is crucial, especially when Neural Control Barrier Functions are learned from demonstrations. The need to robustly verify these NCBFs across the state space prompted this research.

Method: The authors propose CPED-NCBFs, a verification method employing split-conformal prediction, aimed at validating the NCBFs learned from expert demonstrations more effectively than traditional conservative approaches.

Result: The CPED-NCBF method was successfully tested on point mass systems and unicycle models, demonstrating its ability to verify safety in learned NCBFs.

Conclusion: Split-conformal prediction provides a effective and validated approach for verifying the safety of Neural Control Barrier Functions, addressing limitations of other methods.

Abstract: Among the promising approaches to enforce safety in control systems, learning
Control Barrier Functions (CBFs) from expert demonstrations has emerged as an
effective strategy. However, a critical challenge remains: verifying that the
learned CBFs truly enforce safety across the entire state space. This is
especially difficult when CBF is represented using neural networks (NCBFs).
Several existing verification techniques attempt to address this problem
including SMT-based solvers, mixed-integer programming (MIP), and interval or
bound-propagation methods but these approaches often introduce loose,
conservative bounds. To overcome these limitations, in this work we use
CPED-NCBFs a split-conformal prediction based verification strategy to verify
the learned NCBF from the expert demonstrations. We further validate our method
on point mass systems and unicycle models to demonstrate the effectiveness of
the proposed theory.

</details>


### [463] [Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper](https://arxiv.org/abs/2507.15062)
*Xinyue Zhu,Binghao Huang,Yunzhu Li*

Main category: cs.RO

TL;DR: The paper introduces a portable gripper with tactile sensors and a framework for integrating visual and tactile data for improved robotic manipulation.


<details>
  <summary>Details</summary>
Motivation: Existing handheld grippers lack tactile sensing, limiting their ability to perform precise manipulations using multimodal feedback.

Method: Developed a lightweight gripper with tactile sensors and proposed a representation learning framework that combines tactile and visual signals while maintaining their unique features.

Result: Validated the approach through manipulative tasks such as test tube insertion and pipette fluid transfer, showing improved accuracy and robustness under external interferences.

Conclusion: The integrated tactile-visual hardware and learning framework enables precise, multimodal robotic manipulation, advancing the field of dexterous robotic manipulation.

Abstract: Handheld grippers are increasingly used to collect human demonstrations due
to their ease of deployment and versatility. However, most existing designs
lack tactile sensing, despite the critical role of tactile feedback in precise
manipulation. We present a portable, lightweight gripper with integrated
tactile sensors that enables synchronized collection of visual and tactile data
in diverse, real-world, and in-the-wild settings. Building on this hardware, we
propose a cross-modal representation learning framework that integrates visual
and tactile signals while preserving their distinct characteristics. The
learning procedure allows the emergence of interpretable representations that
consistently focus on contacting regions relevant for physical interactions.
When used for downstream manipulation tasks, these representations enable more
efficient and effective policy learning, supporting precise robotic
manipulation based on multimodal feedback. We validate our approach on
fine-grained tasks such as test tube insertion and pipette-based fluid
transfer, demonstrating improved accuracy and robustness under external
disturbances. Our project page is available at
https://binghao-huang.github.io/touch_in_the_wild/ .

</details>


### [464] [Search-Based Autonomous Vehicle Motion Planning Using Game Theory](https://arxiv.org/abs/2507.15088)
*Pouya Panahandeh,Mohammad Pirani,Baris Fidan,Amir Khajepour*

Main category: cs.RO

TL;DR: The paper introduces a game-theoretic motion planning system for autonomous vehicles (AVs) that treats other road users as intelligent agents, resulting in more realistic paths and enabling real-time implementation.


<details>
  <summary>Details</summary>
Motivation: Traditional motion planning approaches for AVs often treat other road users as static obstacles, which can lead to unrealistic paths and unoptimized interaction with the environment.

Method: The authors utilize a game-theoretic framework within a search-based interactive motion planning scheme, leveraging the intelligence of other road users to improve AV planning.

Result: Experiments with WATonoBus, an autonomous shuttle bus, demonstrate superior performance compared to existing techniques in terms of realistic motion planning.

Conclusion: The proposed method offers a significant improvement in realism and practicality for AV motion planning, enabling effective real-time applications and better interactions with intelligent road users.

Abstract: In this paper, we propose a search-based interactive motion planning scheme
for autonomous vehicles (AVs), using a game-theoretic approach. In contrast to
traditional search-based approaches, the newly developed approach considers
other road users (e.g. drivers and pedestrians) as intelligent agents rather
than static obstacles. This leads to the generation of a more realistic path
for the AV. Due to the low computational time, the proposed motion planning
scheme is implementable in real-time applications. The performance of the
developed motion planning scheme is compared with existing motion planning
techniques and validated through experiments using WATonoBus, an electrical
all-weather autonomous shuttle bus.

</details>


### [465] [Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions](https://arxiv.org/abs/2507.15155)
*Majid Roshanfar,Alex Zhang,Changyan He,Amir Hooshiar,Dale J. Podolsky,Thomas Looi,Eric Diller*

Main category: cs.RO

TL;DR: This letter presents a learning-based framework to model a magnetically steerable soft suction device used in minimally invasive neurosurgery. The device achieves sub-millimeter shape prediction accuracy through data-driven approaches.


<details>
  <summary>Details</summary>
Motivation: The need for precise and real-time control of soft robotic tools in delicate procedures like minimally invasive neurosurgery motivates this research.

Method: The study involves the creation of a soft suction device integrated with FBG sensors for real-time shape feedback. Using a dataset of 5,097 samples under varying magnetic conditions, Neural Network (NN) and Random Forest (RF) models were trained to achieve accurate deformation predictions.

Result: The Random Forest model outperformed the Neural Network, achieving a mean root mean square error of 0.087 mm for control point prediction and a shape reconstruction error of 0.064 mm. It showed that magnetic fields affect distal control points, while actuation frequency and tip distance influence the base configuration.

Conclusion: This framework models the nonlinear behavior of hyperelastic soft robots with a high degree of accuracy and no need for simplified physical assumptions, representing a step forward in intelligent, real-time control of soft robotic devices for neurosurgery.

Abstract: This letter introduces a novel learning-based modeling framework for a
magnetically steerable soft suction device designed for endoscopic endonasal
brain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm
inner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,
and integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape
feedback. Shape reconstruction is represented using four Bezier control points,
enabling a compact and smooth model of the device's deformation. A data-driven
model was trained on 5,097 experimental samples covering a range of magnetic
field magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical
tip distances (90-100 mm), using both Neural Network (NN) and Random Forest
(RF) architectures. The RF model outperformed the NN across all metrics,
achieving a mean root mean square error of 0.087 mm in control point prediction
and a mean shape reconstruction error of 0.064 mm. Feature importance analysis
further revealed that magnetic field components predominantly influence distal
control points, while frequency and distance affect the base configuration.
This learning-based approach effectively models the complex nonlinear behavior
of hyperelastic soft robots under magnetic actuation without relying on
simplified physical assumptions. By enabling sub-millimeter shape prediction
accuracy and real-time inference, this work represents an advancement toward
the intelligent control of magnetically actuated soft robotic tools in
minimally invasive neurosurgery.

</details>


### [466] [CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer](https://arxiv.org/abs/2507.15189)
*Kevin Christiansen Marsim,Jinwoo Jeon,Yeeun Kim,Myeongwoo Jeong,Hyun Myung*

Main category: cs.RO

TL;DR: The paper introduces CHADET, a lightweight depth-completion framework for robots that achieves accurate dense depth maps, enhancing computational efficiency and memory usage.


<details>
  <summary>Details</summary>
Motivation: Existing depth-completion methods pose significant trade-offs between processing speed and accuracy, limiting their applicability for real-time tasks, especially in robotics.

Method: The authors developed the CHADET framework, which processes RGB images and sparse depth points using depth-wise blocks and a lightweight transformer-based decoder. It employs a cross-hierarchical-attention module for refining image features with depth information.

Result: CHADET outperformed existing methods in terms of quality and memory efficiency, as demonstrated through experiments on datasets like KITTI, NYUv2, and VOID.

Conclusion: CHADET successfully addresses the accuracy-efficiency trade-offs, making it a viable solution for real-time robotic applications requiring precise depth information.

Abstract: Depth information which specifies the distance between objects and current
position of the robot is essential for many robot tasks such as navigation.
Recently, researchers have proposed depth completion frameworks to provide
dense depth maps that offer comprehensive information about the surrounding
environment. However, existing methods show significant trade-offs between
computational efficiency and accuracy during inference. The substantial memory
and computational requirements make them unsuitable for real-time applications,
highlighting the need to improve the completeness and accuracy of depth
information while improving processing speed to enhance robot performance in
various tasks. To address these challenges, in this paper, we propose
CHADET(cross-hierarchical-attention depth-completion transformer), a
lightweight depth-completion network that can generate accurate dense depth
maps from RGB images and sparse depth points. For each pair, its feature is
extracted from the depthwise blocks and passed to the equally lightweight
transformer-based decoder. In the decoder, we utilize the novel
cross-hierarchical-attention module that refines the image features from the
depth information. Our approach improves the quality and reduces memory usage
of the depth map prediction, as validated in both KITTI, NYUv2, and VOID
datasets.

</details>


### [467] [VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving](https://arxiv.org/abs/2507.15266)
*Haichao Liu,Haoren Guo,Pei Liu,Benshan Ma,Yuxiang Zhang,Jun Ma,Tong Heng Lee*

Main category: cs.RO

TL;DR: The paper introduces VLM-UDMC, a vision-language model-based framework for safer and more effective decision-making in urban autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Enhance autonomous driving systems with human-like scene understanding and risk-awareness to improve transparency and decision-making.

Method: Combines foundation models for scene reasoning, Retrieval-Augmented Generation for insights, and lightweight LSTM for real-time trajectory predictions.

Result: Demonstrated improved urban driving performance through simulations and real-world experiments with an autonomous vehicle.

Conclusion: VLM-UDMC increases interpretability and decision-making efficiency in autonomous driving, boosting overall performance and safety.

Abstract: Scene understanding and risk-aware attentions are crucial for human drivers
to make safe and effective driving decisions. To imitate this cognitive ability
in urban autonomous driving while ensuring the transparency and
interpretability, we propose a vision-language model (VLM)-enhanced unified
decision-making and motion control framework, named VLM-UDMC. This framework
incorporates scene reasoning and risk-aware insights into an upper-level slow
system, which dynamically reconfigures the optimal motion planning for the
downstream fast system. The reconfiguration is based on real-time environmental
changes, which are encoded through context-aware potential functions. More
specifically, the upper-level slow system employs a two-step reasoning policy
with Retrieval-Augmented Generation (RAG), leveraging foundation models to
process multimodal inputs and retrieve contextual knowledge, thereby generating
risk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM
provides real-time trajectory predictions for heterogeneous traffic
participants by extracting smoother trend representations for short-horizon
trajectory prediction. The effectiveness of the proposed VLM-UDMC framework is
verified via both simulations and real-world experiments with a full-size
autonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively
leverages scene understanding and attention decomposition for rational driving
decisions, thus improving the overall urban driving performance. Our
open-source project is available at https://github.com/henryhcliu/vlmudmc.git.

</details>


### [468] [RepILN: Reparameterized Inertial Localization Network](https://arxiv.org/abs/2507.15293)
*Shanshan Zhang,Tianshui Wen,Siyue Wang,Qi Zhang,Ziheng Zhou,Lingxiang Zheng,Yu Yang*

Main category: cs.RO

TL;DR: The paper introduces a novel inertial localization network that balances accuracy and model compactness for IoT devices by utilizing a multi-branch structure for training and an efficient single-path architecture for inference along with temporal-scale sparse attention.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of data-driven inertial localization methods, particularly their computational demands on IoT devices and lack of effective long-term dependency modeling.

Method: The paper proposes a multi-branch structure during training, transformed into a single-path architecture for inference, coupled with a temporal-scale sparse attention mechanism and a gated convolutional unit to enhance feature extraction and model long-term dependencies.

Result: Experiments on benchmarks like the RoNIN dataset show a 2.59% reduction in Absolute Trajectory Error and a 3.86% reduction in the number of parameters compared to a baseline method (RoNIN-ResNet).

Conclusion: The proposed approach achieves improved localization accuracy and computational efficiency, making it highly suitable for resource-constrained IoT devices.

Abstract: Inertial localization is regarded as a promising positioning solution for
consumer-grade IoT devices due to its cost-effectiveness and independence from
external infrastructure. However, data-driven inertial localization methods
often rely on increasingly complex network architectures to improve accuracy,
which challenges the limited computational resources of IoT devices. Moreover,
these methods frequently overlook the importance of modeling long-term
dependencies in inertial measurements - a critical factor for accurate
trajectory reconstruction - thereby limiting localization performance. To
address these challenges, we propose a reparameterized inertial localization
network that uses a multi-branch structure during training to enhance feature
extraction. At inference time, this structure is transformed into an equivalent
single-path architecture to improve parameter efficiency. To further capture
long-term dependencies in motion trajectories, we introduce a temporal-scale
sparse attention mechanism that selectively emphasizes key trajectory segments
while suppressing noise. Additionally, a gated convolutional unit is
incorporated to effectively integrate long-range dependencies with local
fine-grained features. Extensive experiments on public benchmarks demonstrate
that our method achieves a favorable trade-off between accuracy and model
compactness. For example, on the RoNIN dataset, our approach reduces the
Absolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while
reducing the number of parameters by 3.86%.

</details>


### [469] [Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe](https://arxiv.org/abs/2507.15444)
*Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: This paper presents the first closed-loop control system for quadrotors hovering in narrow pipes using real-time flow field measurements.


<details>
  <summary>Details</summary>
Motivation: Hovering quadrotors in confined spaces, like pipes, face challenges due to unsteady aerodynamic disturbances, and existing solutions either require constant motion or lack stability.

Method: The paper introduces a combination of smoke velocimetry for airflow measurement, a neural network-based disturbance estimator, and a reinforcement learning-trained controller integrating flow information.

Result: The system successfully counteracts transient aerodynamic effects during lateral translation, preventing collisions with the pipe wall.

Conclusion: This research demonstrates the feasibility of using flow-feedback control for quadrotors in aerodynamically complex environments and contributes insights into flow structures in narrow pipes.

Abstract: Autonomous quadrotor flight in confined spaces such as pipes and tunnels
presents significant challenges due to unsteady, self-induced aerodynamic
disturbances. Very recent advances have enabled flight in such conditions, but
they either rely on constant motion through the pipe to mitigate airflow
recirculation effects or suffer from limited stability during hovering. In this
work, we present the first closed-loop control system for quadrotors for
hovering in narrow pipes that leverages real-time flow field measurements. We
develop a low-latency, event-based smoke velocimetry method that estimates
local airflow at high temporal resolution. This flow information is used by a
disturbance estimator based on a recurrent convolutional neural network, which
infers force and torque disturbances in real time. The estimated disturbances
are integrated into a learning-based controller trained via reinforcement
learning. The flow-feedback control proves particularly effective during
lateral translation maneuvers in the pipe cross-section. There, the real-time
disturbance information enables the controller to effectively counteract
transient aerodynamic effects, thereby preventing collisions with the pipe
wall. To the best of our knowledge, this work represents the first
demonstration of an aerial robot with closed-loop control informed by real-time
flow field measurements. This opens new directions for research on flight in
aerodynamically complex environments. In addition, our work also sheds light on
the characteristic flow structures that emerge during flight in narrow,
circular pipes, providing new insights at the intersection of robotics and
fluid dynamics.

</details>


### [470] [The Emergence of Deep Reinforcement Learning for Path Planning](https://arxiv.org/abs/2507.15469)
*Thanh Thi Nguyen,Saeid Nahavandi,Imran Razzak,Dung Nguyen,Nhat Truong Pham,Quoc Viet Hung Nguyen*

Main category: cs.RO

TL;DR: This survey reviews traditional and deep reinforcement learning (DRL) approaches for path planning in autonomous systems and discusses their strengths, limitations, and future research directions.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need for advanced and robust path planning methodologies to meet the increasing demand for autonomous systems in complex and dynamic environments.

Method: The paper categorizes key algorithms in traditional and DRL-based approaches, highlights their innovations and implementations, and critically examines their performance metrics.

Result: The survey identifies the strengths and weaknesses of each method, with special focus on hybrid DRL-classical planning techniques that blend adaptability with reliability.

Conclusion: The paper emphasizes hybrid approaches as promising directions for achieving robust and efficient autonomous navigation while also identifying open challenges and gaps for future research.

Abstract: The increasing demand for autonomous systems in complex and dynamic
environments has driven significant research into intelligent path planning
methodologies. For decades, graph-based search algorithms, linear programming
techniques, and evolutionary computation methods have served as foundational
approaches in this domain. Recently, deep reinforcement learning (DRL) has
emerged as a powerful method for enabling autonomous agents to learn optimal
navigation strategies through interaction with their environments. This survey
provides a comprehensive overview of traditional approaches as well as the
recent advancements in DRL applied to path planning tasks, focusing on
autonomous vehicles, drones, and robotic platforms. Key algorithms across both
conventional and learning-based paradigms are categorized, with their
innovations and practical implementations highlighted. This is followed by a
thorough discussion of their respective strengths and limitations in terms of
computational efficiency, scalability, adaptability, and robustness. The survey
concludes by identifying key open challenges and outlining promising avenues
for future research. Special attention is given to hybrid approaches that
integrate DRL with classical planning techniques to leverage the benefits of
both learning-based adaptability and deterministic reliability, offering
promising directions for robust and resilient autonomous navigation.

</details>


### [471] [All-UWB SLAM Using UWB Radar and UWB AOA](https://arxiv.org/abs/2507.15474)
*Charith Premachandra,Achala Athukorala,U-Xuan Tan*

Main category: cs.RO

TL;DR: This paper explores integrating UWB Angle of Arrival (AOA) measurements into SLAM systems to improve navigation in challenging environments, using dynamically deployed anchor-tag units for enhanced accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in using vision-dependent autonomous systems in environments like smoke or dust, where optical sensors fail, proposing UWB radar as a robust alternative.

Method: The approach uses UWB AOA measurements obtained from dynamically deployed anchor-tag units to enhance UWB radar-based SLAM accuracy in feature-deficient environments.

Result: The integration of UWB AOA units with UWB radar successfully enabled SLAM in vision-denied and featureless environments, as demonstrated in experiments.

Conclusion: The proposed method enhances SLAM system performance in challenging environments, overcoming limitations of traditional feature-based UWB implementations.

Abstract: There has been a growing interest in autonomous systems designed to operate
in adverse conditions (e.g. smoke, dust), where the visible light spectrum
fails. In this context, Ultra-wideband (UWB) radar is capable of penetrating
through such challenging environmental conditions due to the lower frequency
components within its broad bandwidth. Therefore, UWB radar has emerged as a
potential sensing technology for Simultaneous Localization and Mapping (SLAM)
in vision-denied environments where optical sensors (e.g. LiDAR, Camera) are
prone to failure. Existing approaches involving UWB radar as the primary
exteroceptive sensor generally extract features in the environment, which are
later initialized as landmarks in a map. However, these methods are constrained
by the number of distinguishable features in the environment. Hence, this paper
proposes a novel method incorporating UWB Angle of Arrival (AOA) measurements
into UWB radar-based SLAM systems to improve the accuracy and scalability of
SLAM in feature-deficient environments. The AOA measurements are obtained using
UWB anchor-tag units which are dynamically deployed by the robot in featureless
areas during mapping of the environment. This paper thoroughly discusses
prevailing constraints associated with UWB AOA measurement units and presents
solutions to overcome them. Our experimental results show that integrating UWB
AOA units with UWB radar enables SLAM in vision-denied feature-deficient
environments.

</details>


### [472] [The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents](https://arxiv.org/abs/2507.15478)
*Simon Kohaut,Felix Divo,Navid Hamid,Benedict Flade,Julian Eggert,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.RO

TL;DR: This paper introduces a framework called Constitutional Controller (CoCo) that enhances the safety of autonomous agents by combining neuro-symbolic methods with the concept of self-doubt for navigation in uncertain environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to ensure reliable and rule-compliant behavior for autonomous agents operating in uncertain and challenging environments.

Method: The paper integrates probabilistic symbolic reasoning with deep learning to create a framework (CoCo) that reasons about constraints using deep probabilistic logic programs and incorporates the concept of self-doubt.

Result: The framework is applied in a real-world aerial mobility context, showcasing CoCo's capability to learn doubt and safely navigate complex environments.

Conclusion: CoCo highlights the potential of neuro-symbolic approaches in addressing safety and reliability challenges for autonomous systems in environments requiring compliance with explicit rules and adaptability in uncertainty.

Abstract: Ensuring reliable and rule-compliant behavior of autonomous agents in
uncertain environments remains a fundamental challenge in modern robotics. Our
work shows how neuro-symbolic systems, which integrate probabilistic, symbolic
white-box reasoning models with deep learning methods, offer a powerful
solution to this challenge. This enables the simultaneous consideration of
explicit rules and neural models trained on noisy data, combining the strength
of structured reasoning with flexible representations. To this end, we
introduce the Constitutional Controller (CoCo), a novel framework designed to
enhance the safety and reliability of agents by reasoning over deep
probabilistic logic programs representing constraints such as those found in
shared traffic spaces. Furthermore, we propose the concept of self-doubt,
implemented as a probability density conditioned on doubt features such as
travel velocity, employed sensors, or health factors. In a real-world aerial
mobility study, we demonstrate CoCo's advantages for intelligent autonomous
systems to learn appropriate doubts and navigate complex and uncertain
environments safely and compliantly.

</details>


### [473] [Robots for Kiwifruit Harvesting and Pollination](https://arxiv.org/abs/2507.15484)
*Jamie Bell*

Main category: cs.RO

TL;DR: The research developed mobile robots for pollen spraying and fruit harvesting in kiwifruit orchards, achieving significant improvements in canopy reach, automation, and navigation systems.


<details>
  <summary>Details</summary>
Motivation: The motivation was to improve efficiency and effectiveness of pollination and harvesting processes in kiwifruit orchards by addressing the challenges posed by pergola structures and cluttered canopies.

Method: The study developed kiwifruit detachment mechanisms and artificial pollination systems using sprayers. Navigation was achieved using 2D and 3D lidar data, as well as computer vision algorithms, tested in field conditions across multiple kilometers.

Result: The detachment mechanism achieved over 80% reach in the canopy compared to less than 70% with previous methods. The 3D lidar system enabled over 30 km of autonomous driving, and computer vision performed comparably to lidar for row detection and following.

Conclusion: The integration of advanced robotic systems, including improved detachment mechanisms, lidar, and computer vision, enhances both pollination and harvesting efficiency in challenging orchard environments like pergola-structured kiwifruit orchards.

Abstract: This research was a part of a project that developed mobile robots that
performed targeted pollen spraying and automated harvesting in pergola
structured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were
designed and field testing of one of the concepts showed that the mechanism
could reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism
was able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,
whereas the previous state of the art mechanism was only able to reach less
than 70 percent of the fruit. Artificial pollination was performed by detecting
flowers and then spraying pollen in solution onto the detected flowers from a
line of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the
height of the canopy was measured and the spray boom was moved up and down to
keep the boom close enough to the flowers for the spray to reach the flowers,
while minimising collisions with the canopy. Mobile robot navigation was
performed using a 2D lidar in apple orchards and vineyards. Lidar navigation in
kiwifruit orchards was more challenging because the pergola structure only
provides a small amount of data for the direction of rows, compared to the
amount of data from the overhead canopy, the undulating ground and other
objects in the orchards. Multiple methods are presented here for extracting
structure defining features from 3D lidar data in kiwifruit orchards. In
addition, a 3D lidar navigation system -- which performed row following, row
end detection and row end turns -- was tested for over 30 km of autonomous
driving in kiwifruit orchards. Computer vision algorithms for row detection and
row following were also tested. The computer vision algorithm worked as well as
the 3D lidar row following method in testing.

</details>


### [474] [GR-3 Technical Report](https://arxiv.org/abs/2507.15493)
*Chilam Cheang,Sijin Chen,Zhongren Cui,Yingdong Hu,Liqun Huang,Tao Kong,Hang Li,Yifeng Li,Yuxiao Liu,Xiao Ma,Hao Niu,Wenxuan Ou,Wanli Peng,Zeyu Ren,Haixin Shi,Jiawen Tian,Hongtao Wu,Xin Xiao,Yuyang Xiao,Jiafeng Xu,Yichu Yang*

Main category: cs.RO

TL;DR: The paper introduces GR-3, a vision-language-action model, demonstrating exceptional generalization and efficiency in adapting to new robotic tasks. It surpasses the prior state-of-the-art methods in diverse complex tasks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to create generalist robot policies capable of adapting efficiently to various tasks and settings, to pave the way for robots assisting humans in daily tasks.

Method: The approach involves combining training on web-scale vision-language data, fine-tuning with human trajectory data collected using VR, and imitation learning from robot trajectory data.

Result: GR-3 outperforms baseline methods in handling a range of long-horizon, dexterous tasks, including bi-manual and mobile movement in experiments.

Conclusion: GR-3 and its companion robot ByteMini demonstrate significant advancement towards robots capable of versatile and reliable assistance in daily life.

Abstract: We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.

</details>


### [475] [CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions](https://arxiv.org/abs/2507.15499)
*Jongseok Lee,Timo Birr,Rudolph Triebel,Tamim Asfour*

Main category: cs.RO

TL;DR: CLEVER is an active learning system that enables robust semantic perception in robots using data streams and human inputs for adapting DNNs.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of robust semantic perception in robots using real-time streaming data and human corrections.

Method: The system uses Bayesian formulations with priors for active learning and adapts DNNs based on feedback from humans.

Result: CLEVER is successfully applied to humanoid and deformable objects, demonstrating improved semantic perception.

Conclusion: CLEVER is the first implementation of stream-based active learning on a robot, proving its robustness in practical scenarios.

Abstract: We propose CLEVER, an active learning system for robust semantic perception
with Deep Neural Networks (DNNs). For data arriving in streams, our system
seeks human support when encountering failures and adapts DNNs online based on
human instructions. In this way, CLEVER can eventually accomplish the given
semantic perception tasks. Our main contribution is the design of a system that
meets several desiderata of realizing the aforementioned capabilities. The key
enabler herein is our Bayesian formulation that encodes domain knowledge
through priors. Empirically, we not only motivate CLEVER's design but further
demonstrate its capabilities with a user validation study as well as
experiments on humanoid and deformable objects. To our knowledge, we are the
first to realize stream-based active learning on a real robot, providing
evidence that the robustness of the DNN-based semantic perception can be
improved in practice. The project website can be accessed at
https://sites.google.com/view/thecleversystem.

</details>


### [476] [Estimation of Payload Inertial Parameters from Human Demonstrations by Hand Guiding](https://arxiv.org/abs/2507.15604)
*Johannes Hartwig,Philipp Lienhardt,Dominik Henrich*

Main category: cs.RO

TL;DR: This paper explores simplifying cobot use for non-experts by enabling payload inertial parameters (PIP) estimation during hand-guiding, without requiring dedicated calibration.


<details>
  <summary>Details</summary>
Motivation: To make collaborative robots (cobots) accessible to users with minimal programming knowledge, especially concerning in-contact motion programming.

Method: The authors propose estimating robot tool's PIP during non-contact motion segments of demonstrated tasks using established techniques, bypassing dedicated calibration.

Result: The approach estimates payload mass accurately, but center of mass and inertia tensor estimations suffer from noise and lack of motion excitation.

Conclusion: PIP estimation during hand-guiding is feasible but requires adequate payload acceleration for improved accuracy in all parameters.

Abstract: As the availability of cobots increases, it is essential to address the needs
of users with little to no programming knowledge to operate such systems
efficiently. Programming concepts often use intuitive interaction modalities,
such as hand guiding, to address this. When programming in-contact motions,
such frameworks require knowledge of the robot tool's payload inertial
parameters (PIP) in addition to the demonstrated velocities and forces to
ensure effective hybrid motion-force control. This paper aims to enable
non-expert users to program in-contact motions more efficiently by eliminating
the need for a dedicated PIP calibration, thereby enabling flexible robot tool
changes. Since demonstrated tasks generally also contain motions with
non-contact, our approach uses these parts to estimate the robot's PIP using
established estimation techniques. The results show that the estimation of the
payload's mass is accurate, whereas the center of mass and the inertia tensor
are affected by noise and a lack of excitation. Overall, these findings show
the feasibility of PIP estimation during hand guiding but also highlight the
need for sufficient payload accelerations for an accurate estimation.

</details>


### [477] [A Universal Vehicle-Trailer Navigation System with Neural Kinematics and Online Residual Learning](https://arxiv.org/abs/2507.15607)
*Yanbo Chen,Yunzhe Tan,Yaojia Wang,Zhengzhe Xu,Junbo Tan,Xueqian Wang*

Main category: cs.RO

TL;DR: The paper proposes a universal navigation system for vehicle-trailer setups, combining classical and neural network models, augmented by real-time learning and predictive control.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of autonomous navigation in dynamic environments for diverse vehicle-trailer systems, especially for trailers with castor wheels.

Method: The method integrates a hybrid kinematic modeling approach (classical and neural network-based) with real-time residual learning and a model predictive control framework for accurate motion planning.

Result: Real-world experiments confirm the robustness and adaptability of the system, accommodating different trailer types and conditions without manual adjustments.

Conclusion: The universal navigation system ensures safe and efficient motion planning in variable conditions, validated by real-world trials without the need for specific calibrations.

Abstract: Autonomous navigation of vehicle-trailer systems is crucial in environments
like airports, supermarkets, and concert venues, where various types of
trailers are needed to navigate with different payloads and conditions.
However, accurately modeling such systems remains challenging, especially for
trailers with castor wheels. In this work, we propose a novel universal
vehicle-trailer navigation system that integrates a hybrid nominal kinematic
model--combining classical nonholonomic constraints for vehicles and neural
network-based trailer kinematics--with a lightweight online residual learning
module to correct real-time modeling discrepancies and disturbances.
Additionally, we develop a model predictive control framework with a weighted
model combination strategy that improves long-horizon prediction accuracy and
ensures safer motion planning. Our approach is validated through extensive
real-world experiments involving multiple trailer types and varying payload
conditions, demonstrating robust performance without manual tuning or
trailer-specific calibration.

</details>


### [478] [Optimizing Force Signals from Human Demonstrations of In-Contact Motions](https://arxiv.org/abs/2507.15608)
*Johannes Hartwig,Fabian Viessmann,Dominik Henrich*

Main category: cs.RO

TL;DR: The paper improves the precision of human-intended signals in robot programming by optimizing processing methods for noisy kinesthetic input.


<details>
  <summary>Details</summary>
Motivation: Human demonstrations in robot programming produce noisy signals that can hinder precise motion reproduction, necessitating methods to better reflect human intent.

Method: The study involved comparing signal filtering methods and introducing a peak detection technique for handling deviations in human input signals.

Result: The proposed filtering approach improved motion quality by up to 20% in terms of error comparison between input and intended signals.

Conclusion: Enhancements in signal processing make robot programming more intuitive and improve human-robot interaction.

Abstract: For non-robot-programming experts, kinesthetic guiding can be an intuitive
input method, as robot programming of in-contact tasks is becoming more
prominent. However, imprecise and noisy input signals from human demonstrations
pose problems when reproducing motions directly or using the signal as input
for machine learning methods. This paper explores optimizing force signals to
correspond better to the human intention of the demonstrated signal. We compare
different signal filtering methods and propose a peak detection method for
dealing with first-contact deviations in the signal. The evaluation of these
methods considers a specialized error criterion between the input and the
human-intended signal. In addition, we analyze the critical parameters'
influence on the filtering methods. The quality for an individual motion could
be increased by up to \SI{20}{\percent} concerning the error criterion. The
proposed contribution can improve the usability of robot programming and the
interaction between humans and robots.

</details>


### [479] [EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation](https://arxiv.org/abs/2507.15649)
*Haocheng Xu,Haodong Zhang,Zhenghan Chen,Rong Xiong*

Main category: cs.RO

TL;DR: The paper introduces a reinforcement learning framework to help humanoid robots track upper-body motions while maintaining balance through a retargeting network and an Executable Motion Prior module.


<details>
  <summary>Details</summary>
Motivation: Humanoid robots often face challenges in maintaining standing stability while performing upper-body manipulation tasks, which limits their utility. The motivation is to enhance robots' capabilities in this area.

Method: The method involves using reinforcement learning for motion tracking, generating a large-scale dataset via a retargeting network, employing domain randomization for robustness, and an Executable Motion Prior module for adjusting target movements.

Result: The framework was tested in simulations and real-world environments, showing its effectiveness in improving stability and motion imitation capabilities.

Conclusion: The proposed approach successfully enables humanoid robots to imitate human upper-body motions while ensuring safety and overall stability, making it suitable for practical applications.

Abstract: To support humanoid robots in performing manipulation tasks, it is essential
to study stable standing while accommodating upper-body motions. However, the
limited controllable range of humanoid robots in a standing position affects
the stability of the entire body. Thus we introduce a reinforcement learning
based framework for humanoid robots to imitate human upper-body motions while
maintaining overall stability. Our approach begins with designing a retargeting
network that generates a large-scale upper-body motion dataset for training the
reinforcement learning (RL) policy, which enables the humanoid robot to track
upper-body motion targets, employing domain randomization for enhanced
robustness. To avoid exceeding the robot's execution capability and ensure
safety and stability, we propose an Executable Motion Prior (EMP) module, which
adjusts the input target movements based on the robot's current state. This
adjustment improves standing stability while minimizing changes to motion
amplitude. We evaluate our framework through simulation and real-world tests,
demonstrating its practical applicability.

</details>


### [480] [Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic Arms](https://arxiv.org/abs/2507.15677)
*Huayue Liang,Yanbo Chen,Hongyang Cheng,Yanzhao Yu,Shoujie Li,Junbo Tan,Xueqian Wang,Long Zeng*

Main category: cs.RO

TL;DR: The paper introduces a model predictive control method for flexible cable-driven robotic arms (FCRAs) using input-output data, achieving enhanced accuracy without relying on physical models.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in modeling and controlling FCRAs due to cable properties like resilience and friction, and improve their control accuracy.

Method: An input-output based implicit model is developed and integrated into an MPC framework, alongside a data selection algorithm to optimize computational efficiency.

Result: Experimental validation showed significant control accuracy improvement, with positioning averaging at 2.070 mm and tracking errors reduced compared to traditional PID methods.

Conclusion: The proposed MPC approach effectively enhances control precision for FCRAs, demonstrating improved tracking and positioning performance over conventional methods.

Abstract: Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant
motion. Still, the inherent properties of cables, such as resilience,
hysteresis, and friction, often lead to particular difficulties in modeling and
control. This paper proposes a model predictive control (MPC) method that
relies exclusively on input-output data, without a physical model, to improve
the control accuracy of FCRAs. First, we develop an implicit model based on
input-output data and integrate it into an MPC optimization framework. Second,
a data selection algorithm (DSA) is introduced to filter the data that best
characterize the system, thereby reducing the solution time per step to
approximately 4 ms, which is an improvement of nearly 80%. Lastly, the
influence of hyperparameters on tracking error is investigated through
simulation. The proposed method has been validated on a real FCRA platform,
including five-point positioning accuracy tests, a five-point response tracking
test, and trajectory tracking for letter drawing. The results demonstrate that
the average positioning accuracy is approximately 2.070 mm. Moreover, compared
to the PID method with an average tracking error of 1.418{\deg}, the proposed
method achieves an average tracking error of 0.541{\deg}.

</details>


### [481] [Strong, Accurate, and Low-Cost Robot Manipulator](https://arxiv.org/abs/2507.15693)
*Georges Chebly,Spencer Little,Nisal Perera,Aliya Abedeen,Ken Suzuki,Donghyun Kim*

Main category: cs.RO

TL;DR: This paper introduces Forte, a 3D-printable, low-cost robotic arm with industrial-level performance capabilities.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing low-cost educational robotic arms and create a high-performance, cost-effective solution for education and research.

Method: Forte uses capstan-based cable drives, timing belts, tensioning mechanisms, lightweight structures, and topology optimization to achieve precision and durability.

Result: The robotic arm delivers 6-DoF motion, a 0.63 kg payload capacity, 0.467 m reach, and sub-millimeter repeatability, while keeping material costs under $215.

Conclusion: Forte provides a practical and affordable robotic platform suitable for classroom education and advanced robotics experimentation.

Abstract: This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed
to achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,
and sub-millimeter repeatability - at a material cost under $215. As an
accessible robot for broad applications across classroom education to AI
experiments, Forte pushes forward the performance limitations of existing
low-cost educational arms. We introduce a cost-effective mechanical design that
combines capstan-based cable drives, timing belts, simple tensioning
mechanisms, and lightweight 3D-printed structures, along with topology
optimization for structural stiffness. Through careful drivetrain engineering,
we minimize backlash and maintain control fidelity without relying on
high-power electronics or expensive manufacturing processes. Experimental
validation demonstrates that Forte achieves high repeatability and load
capacity, offering a compelling robotic platform for both classroom instruction
and advanced robotics research.

</details>


### [482] [Selective Densification for Rapid Motion Planning in High Dimensions with Narrow Passages](https://arxiv.org/abs/2507.15710)
*Lu Huang,Lingxiao Meng,Jiankun Wang,Xingjian Jing*

Main category: cs.RO

TL;DR: The paper proposes a novel sampling-based motion planning framework that integrates multi-resolution sampling to address inefficiencies in complex configuration spaces. It demonstrates improved performance across various settings and outperforms existing planners.


<details>
  <summary>Details</summary>
Motivation: To improve the performance of sampling-based algorithms in complex configuration spaces, particularly in situations involving narrow corridors, where existing methods struggle due to low sampling efficiency.

Method: The proposed framework uses a multi-resolution sampling approach, combining uniform random sampling at different granularities and incorporating an online bias toward sparse samples for large free spaces. It also transitions between dense and sparse sampling to maintain balance between speed and completeness.

Result: Simulation results show the framework outperforms state-of-the-art planners in $
SE(2)$, $
SE(3)$, and $
R^{14}$ spaces with challenging terrains. Experiments with the Franka Emika Panda robot in constrained settings further confirm its effectiveness.

Conclusion: The approach offers a simple yet efficient method to navigate complex configuration spaces, demonstrating generalizability and robust performance without relying on handcrafted heuristics or extensive training.

Abstract: Sampling-based algorithms are widely used for motion planning in
high-dimensional configuration spaces. However, due to low sampling efficiency,
their performance often diminishes in complex configuration spaces with narrow
corridors. Existing approaches address this issue using handcrafted or learned
heuristics to guide sampling toward useful regions. Unfortunately, these
strategies often lack generalizability to various problems or require extensive
prior training. In this paper, we propose a simple yet efficient sampling-based
planning framework along with its bidirectional version that overcomes these
issues by integrating different levels of planning granularity. Our approach
probes configuration spaces with uniform random samples at varying resolutions
and explores these multi-resolution samples online with a bias towards sparse
samples when traveling large free configuration spaces. By seamlessly
transitioning between sparse and dense samples, our approach can navigate
complex configuration spaces while maintaining planning speed and completeness.
The simulation results demonstrate that our approach outperforms several
state-of-the-art sampling-based planners in $\mathbb{SE}(2)$, $\mathbb{SE}(3)$,
and $\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments
conducted with the Franka Emika Panda robot operating in a constrained
workspace provide additional evidence of the superiority of the proposed
method.

</details>


### [483] [DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models](https://arxiv.org/abs/2507.15716)
*Ziyu Wan,Lin Zhao*

Main category: cs.RO

TL;DR: The paper introduces DiffPF, a differentiable particle filter utilizing diffusion models for improved state estimation, significantly outperforming existing methods in both simulated and real-world challenges.


<details>
  <summary>Details</summary>
Motivation: To improve state estimation in dynamic systems by overcoming limitations of conventional particle filters, such as poor handling of complex and multimodal distributions.

Method: The authors integrate a conditional diffusion model within a particle filter, allowing posterior sampling conditioned on predicted particles and current observations, eliminating the need for traditional importance weighting.

Result: DiffPF demonstrated superior performance across various benchmarks, including 82.8% accuracy improvement on a global localization benchmark and 26% improvement on KITTI visual odometry, surpassing existing techniques.

Conclusion: DiffPF effectively enhances particle filtering by leveraging conditional diffusion models, achieving high-quality posterior sampling and setting new standards for state estimation in challenging scenarios.

Abstract: This paper proposes DiffPF, a differentiable particle filter that leverages
diffusion models for state estimation in dynamic systems. Unlike conventional
differentiable particle filters, which require importance weighting and
typically rely on predefined or low-capacity proposal distributions. DiffPF
learns a flexible posterior sampler by conditioning a diffusion model on
predicted particles and the current observation. This enables accurate,
equally-weighted sampling from complex, high-dimensional, and multimodal
filtering distributions. We evaluate DiffPF across a range of scenarios,
including both unimodal and highly multimodal distributions, and test it on
simulated as well as real-world tasks, where it consistently outperforms
existing filtering baselines. In particular, DiffPF achieves an 82.8%
improvement in estimation accuracy on a highly multimodal global localization
benchmark, and a 26% improvement on the real-world KITTI visual odometry
benchmark, compared to state-of-the-art differentiable filters. To the best of
our knowledge, DiffPF is the first method to integrate conditional diffusion
models into particle filtering, enabling high-quality posterior sampling that
produces more informative particles and significantly improves state
estimation.

</details>


### [484] [Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction](https://arxiv.org/abs/2507.15729)
*Jens V. Rüppel,Andrey Rudenko,Tim Schreiter,Martin Magnusson,Achim J. Lilienthal*

Main category: cs.RO

TL;DR: The paper proposes a modular Human-Robot Interaction system using Large Language Models (LLMs) combined with gaze and speech inputs for assistive tasks and tests its adaptability compared to traditional systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing Human-Robot Interaction systems by introducing bi-directional, multi-modal, and context-aware collaborative interfaces for assistive robots.

Method: The authors developed a modular interface informed by gaze and speech, capable of incorporating multi-vision inputs and real-time language-based interaction state representations. They conducted lab studies to compare its performance against traditional scripted pipelines.

Result: The LLM-based system showed enhanced adaptability and slight improvements in user engagement and task execution metrics, though it occasionally produced redundant outputs.

Conclusion: While the LLM-based approach demonstrates superior flexibility and interaction adaptation, scripted pipelines remain more effective for straightforward tasks due to their simplicity and precision.

Abstract: The rapid development of Large Language Models (LLMs) creates an exciting
potential for flexible, general knowledge-driven Human-Robot Interaction (HRI)
systems for assistive robots. Existing HRI systems demonstrate great progress
in interpreting and following user instructions, action generation, and robot
task solving. On the other hand, bi-directional, multi-modal, and context-aware
support of the user in collaborative tasks still remains an open challenge. In
this paper, we present a gaze- and speech-informed interface to the assistive
robot, which is able to perceive the working environment from multiple vision
inputs and support the dynamic user in their tasks. Our system is designed to
be modular and transferable to adapt to diverse tasks and robots, and it is
capable of real-time use of language-based interaction state representation and
fast on board perception modules. Its development was supported by multiple
public dissemination events, contributing important considerations for improved
robustness and user experience. Furthermore, in two lab studies, we compare the
performance and user ratings of our system with those of a traditional scripted
HRI pipeline. Our findings indicate that an LLM-based approach enhances
adaptability and marginally improves user engagement and task execution metrics
but may produce redundant output, while a scripted pipeline is well suited for
more straightforward tasks.

</details>


### [485] [Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs](https://arxiv.org/abs/2507.15782)
*Ruochu Yang,Yu Zhou,Fumin Zhang,Mengxue Hou*

Main category: cs.RO

TL;DR: This paper proposes a novel algorithm combining large language models (LLM) and motion planning (Inter-LLM) to enhance robot performance in long missions involving multiple human commands.


<details>
  <summary>Details</summary>
Motivation: The research aims to overcome limitations in household robots' ability to manipulate open-set objects and navigate efficiently in large environments for complex tasks.

Method: The paper introduces an interleaved LLM and motion planning algorithm called Inter-LLM, utilizing a multimodal action cost similarity function for long-horizon planning.

Result: Simulation results show a 30% improvement in fulfilling human commands, success rates, and minimizing mission costs compared to state-of-the-art methods.

Conclusion: The proposed algorithm effectively addresses challenges in robot intelligence for multi-object collection and long missions, enhancing both efficiency and mission quality.

Abstract: Household robots have been a longstanding research topic, but they still lack
human-like intelligence, particularly in manipulating open-set objects and
navigating large environments efficiently and accurately. To push this
boundary, we consider a generalized multi-object collection problem in large
scene graphs, where the robot needs to pick up and place multiple objects
across multiple locations in a long mission of multiple human commands. This
problem is extremely challenging since it requires long-horizon planning in a
vast action-state space under high uncertainties. To this end, we propose a
novel interleaved LLM and motion planning algorithm Inter-LLM. By designing a
multimodal action cost similarity function, our algorithm can both reflect the
history and look into the future to optimize plans, striking a good balance of
quality and efficiency. Simulation experiments demonstrate that compared with
latest works, our algorithm improves the overall mission performance by 30% in
terms of fulfilling human commands, maximizing mission success rates, and
minimizing mission costs.

</details>


### [486] [Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers](https://arxiv.org/abs/2507.15833)
*Ian Chuang,Andrew Lee,Dechen Gao,Jinyu Zou,Iman Soltani*

Main category: cs.RO

TL;DR: The paper introduces human-inspired active gaze-based visual processing for robotic policies to improve efficiency and performance using foveated image techniques.


<details>
  <summary>Details</summary>
Motivation: Robot learning systems predominantly use passive, uniform visual processing, which lacks human-like active gaze efficiency that enhances visual focus and task performance.

Method: Introduced a framework for using eye-tracking data with robotic demonstrations, simulated benchmarks, integrated gaze into vision transformers with foveated patch tokenization, and explored two gaze prediction models.

Result: Foveated vision significantly reduces computation, improves precision task performance, and enhances robustness to unseen distractions.

Conclusion: Human-inspired gaze processing adds meaningful inductive biases that improve efficiency and efficacy in robotic vision systems.

Abstract: Human vision is a highly active process driven by gaze, which directs
attention and fixation to task-relevant regions and dramatically reduces visual
processing. In contrast, robot learning systems typically rely on passive,
uniform processing of raw camera images. In this work, we explore how
incorporating human-like active gaze into robotic policies can enhance both
efficiency and performance. We build on recent advances in foveated image
processing and apply them to an Active Vision robot system that emulates both
human head movement and eye tracking. Extending prior work on the AV-ALOHA
robot simulation platform, we introduce a framework for simultaneously
collecting eye-tracking data and robot demonstrations from a human operator as
well as a simulation benchmark and dataset for training robot policies that
incorporate human gaze. Given the widespread use of Vision Transformers (ViTs)
in robot learning, we integrate gaze information into ViTs using a foveated
patch tokenization scheme inspired by recent work in image segmentation.
Compared to uniform patch tokenization, this significantly reduces the number
of tokens-and thus computation-without sacrificing visual fidelity near regions
of interest. We also explore two approaches to gaze imitation and prediction
from human data. The first is a two-stage model that predicts gaze to guide
foveation and action; the second integrates gaze into the action space,
allowing the policy to jointly predict gaze and actions end-to-end. Our results
show that our method for foveated robot vision not only drastically reduces
computational overhead, but also improves performance for high precision tasks
and robustness to unseen distractors. Together, these findings suggest that
human-inspired visual processing offers a useful inductive bias for robotic
vision systems. https://ian-chuang.github.io/gaze-av-aloha/

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [487] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: The paper explores how large language models (LLMs) can be optimized to automatically generate effective unit tests, showing significant quality improvements with advanced prompting strategies and code contexts.


<details>
  <summary>Details</summary>
Motivation: Unit tests are essential in software engineering, and their automation can significantly enhance productivity, yet the impact of code context and prompting strategies on this automation has not been thoroughly studied.

Method: The study analyzes the effect of various code contexts, including docstrings and full implementations, as well as different prompting strategies like chain-of-thought on unit test generation using LLMs from several families.

Result: Including docstrings boosts code adequacy, while extending full implementation context has smaller effects. Chain-of-thought prompting achieves the best results with high branch coverage (up to 96.3%), average mutation score (57%), and near-perfect compilation success rate. Among models, M5 (Gemini 2.5 Pro) performs the best.

Conclusion: Code contexts and advanced prompting strategies have a substantial impact on unit test generation by LLMs, and leveraging techniques like chain-of-thought can yield near-optimal testing results, offering significant benefits for software engineering.

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [488] [Leveraging LLMs for Formal Software Requirements -- Challenges and Prospects](https://arxiv.org/abs/2507.14330)
*Arshad Beg,Diarmuid O'Donoghue,Rosemary Monahan*

Main category: cs.SE

TL;DR: The paper explores automating the generation of formal software specifications from informal requirements, addressing challenges in safety-critical systems using NLP, ontology, LLMs, and artefact reuse.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of transforming ambiguous, informal requirements into formal specifications for ensuring software correctness, especially in safety-critical systems.

Method: Combines Natural Language Processing, ontology-based domain modeling, artefact reuse, and Large Language Models to automate the creation of verifiable specifications.

Result: Provides a synthesis of literature identifying challenges and prospective research directions for generating formal specifications.

Conclusion: The study highlights the challenges in aligning informal requirements with formal verification needs and suggests directions for automating this process using modern computational techniques.

Abstract: Software correctness is ensured mathematically through formal verification,
which involves the resources of generating formal requirement specifications
and having an implementation that must be verified. Tools such as
model-checkers and theorem provers ensure software correctness by verifying the
implementation against the specification. Formal methods deployment is
regularly enforced in the development of safety-critical systems e.g.
aerospace, medical devices and autonomous systems. Generating these
specifications from informal and ambiguous natural language requirements
remains the key challenge. Our project, VERIFAI^{1}, aims to investigate
automated and semi-automated approaches to bridge this gap, using techniques
from Natural Language Processing (NLP), ontology-based domain modelling,
artefact reuse, and large language models (LLMs). This position paper presents
a preliminary synthesis of relevant literature to identify recurring challenges
and prospective research directions in the generation of verifiable
specifications from informal requirements.

</details>


### [489] [Developing Shared Vocabulary System For Collaborative Software Engineering](https://arxiv.org/abs/2507.14396)
*Carey Lai Zheng Hui,Johnson Britto Jessia Esther Leena,Kumuthini Subramanian,Zhao Chenyu,Shubham Rajeshkumar Jariwala*

Main category: cs.SE

TL;DR: The paper explores how shared vocabulary systems in software engineering can overcome communication gaps and improve collaboration efficiency.


<details>
  <summary>Details</summary>
Motivation: To address persistent communication challenges in software engineering that lead to misunderstandings, inefficiencies, and defects.

Method: Utilized a Design Science Research framework consisting of problem identification via thematic analysis and interviews, method development using Grounded Theory, and empirical validation through controlled experiments.

Result: Shared vocabulary systems significantly enhance documentation clarity, information density, and collaboration efficiency over time, despite initial adoption overhead.

Conclusion: Implementing shared vocabulary systems can improve communication practices in software engineering, with insights for better application and future research directions.

Abstract: Effective communication is a critical factor in successful software
engineering collaboration. However, communication gaps remain a persistent
challenge, often leading to misunderstandings, inefficiencies, and defects.
This research investigates the technical factors contributing to such
misunderstandings and explores the measurable benefits of establishing shared
vocabulary systems within software documentation and codebases. Using a Design
Science Research (DSR) framework, the study was structured into three iterative
phases: problem identification, method development, and empirical validation.
The problem identification phase involved thematic analysis of communication
data and semi-structured interviews, revealing key factors such as ambiguous
messaging, misalignment in documentation, inconsistent code review feedback,
and API integration miscommunication. Grounded Theory principles were employed
to design a structured methodology for collaborative vocabulary development.
Empirical validation through controlled experiments demonstrated that while
initial adoption introduced overhead, the shared vocabulary system
significantly improved information density, documentation clarity, and
collaboration efficiency over time. Findings offer actionable insights for
improving communication practices in software engineering, while also
identifying limitations and directions for future research.

</details>


### [490] [On the Effect of Token Merging on Pre-trained Models for Code](https://arxiv.org/abs/2507.14423)
*Mootez Saad,Hao Li,Tushar Sharma,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: The paper investigates semantic subtoken merging strategies to reduce computational overhead in code language models, observing efficiency gains with minimal performance degradation.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency caused by the long tokenization outputs in language models for code, which result in increased computational overhead.

Method: The study proposes two semantic subtoken merging strategies—representation averaging and a learning-based approach—to reduce hidden representation complexity and integrates them with existing code language models.

Result: Experiments on six language models across three tasks show a reduction in floating-point operations by 1–19%, minimal performance degradation (worst being a 1.82-point F1 drop), and even performance improvement (a 2.47-point gain in CodeBLEU for code translation).

Conclusion: The research demonstrates that efficient subtoken merging can enhance computational efficiency with either negligible or positive downstream performance consequences, catering to multi-dimensional improvements in code language models.

Abstract: Tokenization is a fundamental component of language models for code. It
involves breaking down the input into units that are later passed to the
language model stack to learn high-dimensional representations used in various
contexts, from classification to generation. However, the output of these
tokenizers is often longer than that traditionally used in compilers and
interpreters. This could result in undesirable effects, such as increased
computational overhead. In this work, we investigate the effect of merging the
hidden representations of subtokens that belong to the same semantic unit, such
as subtokens that form a single identifier. We propose two strategies: one
based on averaging the representations and another that leverages a
learning-based approach. Both methods can be seamlessly integrated with
existing language models for code. We conduct experiments using six language
models for code: CodeBERT, GraphCodeBERT, UniXCoder, CdoeT5, CodeT5+ (220M),
and CodeT5+ (770M), across three software engineering tasks: vulnerability
detection, code classification, and code translation. Results show that these
strategies can reduce the number of floating-point operations by $1\%$ to
$19\%$. Regarding downstream performance, the most significant degradation was
observed in the vulnerability detection task, where the F1 score decreased by
$1.82$ points compared to the baseline. In contrast, for code translation, we
observed an improvement of $2.47$ points in CodeBLEU. This work contributes to
the broader effort of improving language models for code across multiple
dimensions, including both computational efficiency and downstream performance.

</details>


### [491] [Architectural Degradation: Definition, Motivations, Measurement and Remediation Approaches](https://arxiv.org/abs/2507.14547)
*Noman Ahmad,Ruoyu Su,Matteo Esposito,Andrea Janes,Valentina Lenarduzzi,Davide Taibi*

Main category: cs.SE

TL;DR: This study reviews 108 sources to unify understanding of architectural degradation and proposes a taxonomy addressing its definitions, causes, metrics, tools, and remediation strategies.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the fragmented definitions, metrics, and remediation strategies in architectural degradation literature, seeking to provide a unified understanding for better system maintainability and adaptability.

Method: The authors conducted a multivocal literature review of 108 studies, analyzing definitions, causes, metrics, tools, and remediation approaches, and developed a taxonomy covering architectural, code, and process debt.

Result: The study identified 54 metrics, 31 measurement techniques, and categorized causes into architectural, code, and process debt, while finding gaps in continuous remediation and integration of tools with prevention strategies.

Conclusion: Architectural degradation requires holistic and proactive strategies, as current work focuses on detection rather than ongoing or preventive remediation, highlighting technical and organizational challenges.

Abstract: Architectural degradation, also known as erosion, decay, or aging, impacts
system quality, maintainability, and adaptability. Although widely
acknowledged, current literature shows fragmented definitions, metrics, and
remediation strategies. Our study aims to unify understanding of architectural
degradation by identifying its definitions, causes, metrics, tools, and
remediation approaches across academic and gray literature. We conducted a
multivocal literature review of 108 studies extracting definitions, causes,
metrics, measurement approaches, tools, and remediation strategies. We
developed a taxonomy encompassing architectural, code, and process debt to
explore definition evolution, methodological trends, and research gaps.
Architectural degradation has shifted from a low-level issue to a
socio-technical concern. Definitions now address code violations, design drift,
and structural decay. Causes fall under architectural (e.g., poor
documentation), code (e.g., hasty fixes), and process debt (e.g., knowledge
loss). We identified 54 metrics and 31 measurement techniques, focused on
smells, cohesion/coupling, and evolution. Yet, most tools detect issues but
rarely support ongoing or preventive remediation. Degradation is both technical
and organizational. While detection is well-studied, continuous remediation
remains lacking. Our study reveals missed integration between metrics, tools,
and repair logic, urging holistic, proactive strategies for sustainable
architecture.

</details>


### [492] [Emerging Trends in Software Architecture from the Practitioners Perspective: A Five Year Review](https://arxiv.org/abs/2507.14554)
*Ruoyu Su,Noman ahmad,Matteo Esposito,Andrea Janes,Davide Taibi,Valentina Lenarduzzi*

Main category: cs.SE

TL;DR: The paper studies software architecture trends using analysis of 5,677 talks from industry conferences over five years, uncovering dominant technologies and practices.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand shifts in software architecture practices due to advancements in cloud computing, microservices, and containers.

Method: The study analyzes 5,677 conference talks using large language models and expert validation to extract trends, technologies, and contexts.

Result: Key technologies like Kubernetes, Cloud Native, and Serverless dominate software architecture practices, focusing largely on deployment and late-stage DevOps.

Conclusion: Contemporary software architecture centers around deployment, with limited focus on early development phases, suggesting opportunities for research-driven improvements.

Abstract: Software architecture plays a central role in the design, development, and
maintenance of software systems. With the rise of cloud computing,
microservices, and containers, architectural practices have diversified.
Understanding these shifts is vital. This study analyzes software architecture
trends across eight leading industry conferences over five years. We
investigate the evolution of software architecture by analyzing talks from top
practitioner conferences, focusing on the motivations and contexts driving
technology adoption. We analyzed 5,677 talks from eight major industry
conferences, using large language models and expert validation to extract
technologies, their purposes, and usage contexts. We also explored how
technologies interrelate and fit within DevOps and deployment pipelines. Among
450 technologies, Kubernetes, Cloud Native, Serverless, and Containers dominate
by frequency and centrality. Practitioners present technology mainly related to
deployment, communication, AI, and observability. We identify five technology
communities covering automation, coordination, cloud AI, monitoring, and
cloud-edge. Most technologies span multiple DevOps stages and support hybrid
deployment. Our study reveals that a few core technologies, like Kubernetes and
Serverless, dominate the contemporary software architecture practice. These are
mainly applied in later DevOps stages, with limited focus on early phases like
planning and coding. We also show how practitioners frame technologies by
purpose and context, reflecting evolving industry priorities. Finally, we
observe how only research can provide a more holistic lens on architectural
design, quality, and evolution.

</details>


### [493] [Harnessing LLMs for Document-Guided Fuzzing of OpenCV Library](https://arxiv.org/abs/2507.14558)
*Bin Duan,Tarek Mahmud,Meiru Che,Yan Yan,Naipeng Dong,Dan Dongseong Kim,Guowei Yang*

Main category: cs.SE

TL;DR: The paper presents VISTAFUZZ, a novel LLM-powered fuzzing technique for improving the reliability of OpenCV by extracting API constraints and systematically testing APIs.


<details>
  <summary>Details</summary>
Motivation: To ensure the reliability of the OpenCV library as bugs in it could negatively impact downstream computer vision applications.

Method: The authors introduce VISTAFUZZ, which uses large language models to parse API documentation, extract constraints and dependencies, and generate systematic inputs for fuzz testing APIs.

Result: VISTAFUZZ tested 330 APIs in the OpenCV library and detected 17 new bugs. Among them, 10 were confirmed, and 5 were fixed.

Conclusion: VISTAFUZZ is effective in detecting and assisting in fixing bugs in OpenCV, showcasing its potential to enhance the robustness of widely-used computer vision libraries.

Abstract: The combination of computer vision and artificial intelligence is
fundamentally transforming a broad spectrum of industries by enabling machines
to interpret and act upon visual data with high levels of accuracy. As the
biggest and by far the most popular open-source computer vision library, OpenCV
library provides an extensive suite of programming functions supporting
real-time computer vision. Bugs in the OpenCV library can affect the downstream
computer vision applications, and it is critical to ensure the reliability of
the OpenCV library. This paper introduces VISTAFUZZ, a novel technique for
harnessing large language models (LLMs) for document-guided fuzzing of the
OpenCV library. VISTAFUZZ utilizes LLMs to parse API documentation and obtain
standardized API information. Based on this standardized information, VISTAFUZZ
extracts constraints on individual input parameters and dependencies between
these. Using these constraints and dependencies, VISTAFUZZ then generates new
input values to systematically test each target API. We evaluate the
effectiveness of VISTAFUZZ in testing 330 APIs in the OpenCV library, and the
results show that VISTAFUZZ detected 17 new bugs, where 10 bugs have been
confirmed, and 5 of these have been fixed.

</details>


### [494] [A first look at License Variants in the PyPI Ecosystem](https://arxiv.org/abs/2507.14594)
*Weiwei Xu,Hengzhi Ye,Kai Gao,Minghui Zhou*

Main category: cs.SE

TL;DR: The paper examines the complexities of open-source license variants in the PyPI ecosystem, introduces tools for improved license analysis and incompatibility detection, and demonstrates their effectiveness.


<details>
  <summary>Details</summary>
Motivation: License variants introduce compliance complexities in modern software systems and existing tools are ineffective in addressing these challenges.

Method: An empirical study of PyPI license variants was conducted, followed by the development of LV-Parser and LV-Compat using diff-based techniques and large language models.

Result: Empirical findings showed significant compliance issues from license variants. LV-Parser achieved high accuracy (0.936) with reduced computational costs, and LV-Compat detected far more incompatibilities with high precision (0.98).

Conclusion: The study sheds light on PyPI license variants and provides robust, efficient tools for addressing license compliance and incompatibility challenges in open-source ecosystems.

Abstract: Open-source licenses establish the legal foundation for software reuse, yet
license variants, including both modified standard licenses and custom-created
alternatives, introduce significant compliance complexities. Despite their
prevalence and potential impact, these variants are poorly understood in modern
software systems, and existing tools do not account for their existence,
leading to significant challenges in both effectiveness and efficiency of
license analysis. To fill this knowledge gap, we conduct a comprehensive
empirical study of license variants in the PyPI ecosystem. Our findings show
that textual variations in licenses are common, yet only 2% involve substantive
modifications. However, these license variants lead to significant compliance
issues, with 10.7% of their downstream dependencies found to be
license-incompatible.
  Inspired by our findings, we introduce LV-Parser, a novel approach for
efficient license variant analysis leveraging diff-based techniques and large
language models, along with LV-Compat, an automated pipeline for detecting
license incompatibilities in software dependency networks. Our evaluation
demonstrates that LV-Parser achieves an accuracy of 0.936 while reducing
computational costs by 30%, and LV-Compat identifies 5.2 times more
incompatible packages than existing methods with a precision of 0.98.
  This work not only provides the first empirical study into license variants
in software packaging ecosystem but also equips developers and organizations
with practical tools for navigating the complex landscape of open-source
licensing.

</details>


### [495] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: The paper investigates how large language models (LLMs) can be used to write or complete declarative Alloy formulas from natural language input, demonstrating promising results.


<details>
  <summary>Details</summary>
Motivation: Writing accurate declarative specifications is crucial but notoriously challenging in software development. This paper explores how LLMs can assist in easing this challenge.

Method: The authors performed a controlled experiment using ChatGPT and DeepSeek with 11 subject specifications to evaluate their ability in tasks like synthesizing Alloy formulas from natural language, rewriting formulas equivalently, and completing sketches.

Result: LLMs performed well, successfully synthesizing formulas, creating alternative solutions, and completing formula sketches from natural language descriptions, all without requiring test cases.

Conclusion: LLMs show significant potential to transform the process of writing specifications, making them more central and effective in software development.

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


### [496] [An Efficient Algorithm for Generating Minimal Unique-Cause MC/DC Test cases for Singular Boolean Expressions](https://arxiv.org/abs/2507.14687)
*Robin Lee,Youngho Nam*

Main category: cs.SE

TL;DR: The paper addresses the lack of efficient test generation methods for Unique-Cause MC/DC, proposing 'Robin's Rule', a deterministic algorithm to achieve 100% coverage for singular Boolean expressions using minimal tests.


<details>
  <summary>Details</summary>
Motivation: Ensuring reliability and safety in critical systems like avionics requires robust coverage criteria, but efficient methods for Unique-Cause MC/DC test generation were missing.

Method: Developed 'Robin's Rule', a deterministic algorithm that constructs minimal test sets (N+1 cases) for 100% MC/DC coverage of singular Boolean expressions without generating full truth tables.

Result: The algorithm consistently achieved 100% MC/DC coverage with the theoretical minimum number of tests in benchmarks and outperformed a certified commercial tool.

Conclusion: The method provides an optimal solution for verifying safety-critical systems, combining rigor and efficiency in test coverage generation.

Abstract: Modified Condition/Decision Coverage (MC/DC) is a mandatory structural
coverage criterion for ensuring the reliability and safety of critical systems.
While its strictest form, Unique-Cause MC/DC, offers the highest assurance,
research on its efficient test generation has been lacking. This gap is
particularly significant, as an analysis of large-scale avionics systems shows
that 99.7% of all conditional decisions are, in fact, Singular Boolean
Expressions (SBEs) the ideal structure for applying Unique-Cause MC/DC. This
paper proposes 'Robin's Rule', a deterministic algorithm that directly
constructs a minimal test set of N + 1 cases to guarantee 100% Unique-Cause
MC/DC for SBEs with N conditions, without generating a full truth table. To
validate our approach, we constructed a benchmark by reformulating the TCAS-II
specifications into SBEs and verified the results using an industry-standard,
certified commercial tool. The results confirm that our method consistently
achieves 100% coverage with the theoretical minimum number of tests and is more
efficient than the commercial tool. This work provides a practical and provably
optimal solution for verifying safety-critical systems, ensuring both rigor and
efficiency.

</details>


### [497] [HistoryFinder: Advancing Method-Level Source Code History Generation with Accurate Oracles and Enhanced Algorithm](https://arxiv.org/abs/2507.14716)
*Shahidul Islam,Ashik Aowal,Md Sharif Uddin,Shaiful Chowdhury*

Main category: cs.SE

TL;DR: HistoryFinder, a new tool for reconstructing method change histories in software engineering, outperforms existing tools in accuracy, completeness, and runtime performance.


<details>
  <summary>Details</summary>
Motivation: Existing method history generation tools suffer from limited accuracy due to ineffective ground truth oracles.

Method: The authors developed two refined oracles through automated analysis and expert-guided validation, and introduced HistoryFinder, a tool evaluated across 400 methods using precision, recall, execution times, and competitive benchmarks.

Result: HistoryFinder demonstrated superior performance over CodeShovel, CodeTracker, IntelliJ, and Git-based tools in accuracy metrics like precision, recall, F1 score, and runtime efficiency.

Conclusion: HistoryFinder stands out as the optimal choice for method history generation in applications requiring both high accuracy and efficiency; the tool is accessible through multiple interfaces like web, CLI, and Java library.

Abstract: Reconstructing a method's change history efficiently and accurately is
critical for many software engineering tasks, including maintenance,
refactoring, and comprehension. Despite the availability of method history
generation tools such as CodeShovel and CodeTracker, existing evaluations of
their effectiveness are limited by inaccuracies in the ground truth oracles
used. In this study, we systematically construct two new oracles -- the
corrected CodeShovel oracle and a newly developed HistoryFinder oracle -- by
combining automated analysis with expert-guided manual validation. We also
introduce HistoryFinder, a new method history generation tool designed to
improve not only the accuracy and completeness of method change histories but
also to offer competitive runtime performance. Through extensive evaluation
across 400 methods from 40 open-source repositories, we show that HistoryFinder
consistently outperforms CodeShovel, CodeTracker, IntelliJ, and Git-based
baselines in terms of precision, recall, and F1 score. Moreover, HistoryFinder
achieves competitive runtime performance, offering the lowest mean and median
execution times among all the research-based tools.
  While Git-based tools exhibit the fastest runtimes, this efficiency comes at
the cost of significantly lower precision and recall -- leaving HistoryFinder
as the best overall choice when both accuracy and efficiency are important. To
facilitate adoption, we provide a web interface, CLI, and Java library for
flexible usage.

</details>


### [498] [Investigating the Role of LLMs Hyperparameter Tuning and Prompt Engineering to Support Domain Modeling](https://arxiv.org/abs/2507.14735)
*Vladyslav Bulhakov,Giordano d'Aloisio,Claudio Di Sipio,Antinisca Di Marco,Davide Di Ruscio*

Main category: cs.SE

TL;DR: The paper investigates improving domain modeling using Llama 3.1 through hyperparameter tuning and prompt engineering, showing quality improvement for medical data models and broader applicability across other domains.


<details>
  <summary>Details</summary>
Motivation: To address limitations of general-purpose LLMs for domain-specific modeling and avoid issues like fine-tuning requirements and catastrophic forgetting.

Method: The paper employs search-based hyperparameter tuning combined with tailored prompt engineering on Llama 3.1 to optimize domain-specific model generation.

Result: Optimized hyperparameters demonstrated notable quality improvements for medical data models and showed enhanced results across nearly all of ten diverse domains.

Conclusion: Combining hyperparameter tuning and prompt engineering improves LLM accuracy for domain-specific tasks, though universal applicability remains limited.

Abstract: The introduction of large language models (LLMs) has enhanced automation in
software engineering tasks, including in Model Driven Engineering (MDE).
However, using general-purpose LLMs for domain modeling has its limitations.
One approach is to adopt fine-tuned models, but this requires significant
computational resources and can lead to issues like catastrophic forgetting.
  This paper explores how hyperparameter tuning and prompt engineering can
improve the accuracy of the Llama 3.1 model for generating domain models from
textual descriptions. We use search-based methods to tune hyperparameters for a
specific medical data model, resulting in a notable quality improvement over
the baseline LLM. We then test the optimized hyperparameters across ten diverse
application domains.
  While the solutions were not universally applicable, we demonstrate that
combining hyperparameter tuning with prompt engineering can enhance results
across nearly all examined domain models.

</details>


### [499] [Toward Inclusive AI-Driven Development: Exploring Gender Differences in Code Generation Tool Interactions](https://arxiv.org/abs/2507.14770)
*Manaal Basha,Ivan Beschastnikh,Gema Rodriguez-Perez,Cleidson R. B. de Souza*

Main category: cs.SE

TL;DR: This study explores gender-related differences in the use of Code Generation Tools (CGTs), employing tasks under varying conditions to analyze performance and cognitive load across diverse developers.


<details>
  <summary>Details</summary>
Motivation: The motivation is to investigate how gender influences interactions with Code Generation Tools like Windsurf and GitHub Copilot and assess their inclusivity and fairness.

Method: The study uses a mixed-subjects design with 54 participants, equally split by gender, conducting programming tasks with CGTs and internet resources in counterbalanced setups. Data collection includes performance metrics, cognitive load surveys, and behavior analysis.

Result: No results are available yet, as the research is still in the proposal stage.

Conclusion: This work aims to inform future CGT designs to improve fairness, inclusivity, and usability, and contribute to ethical AI practices and equitable tool development.

Abstract: Context: The increasing reliance on Code Generation Tools (CGTs), such as
Windsurf and GitHub Copilot, are revamping programming workflows and raising
critical questions about fairness and inclusivity. While CGTs offer potential
productivity enhancements, their effectiveness across diverse user groups have
not been sufficiently investigated. Objectives: We hypothesize that developers'
interactions with CGTs vary based on gender, influencing task outcomes and
cognitive load, as prior research suggests that gender differences can affect
technology use and cognitive processing. Methods: The study will employ a
mixed-subjects design with 54 participants, evenly divided by gender for a
counterbalanced design. Participants will complete two programming tasks
(medium to hard difficulty) with only CGT assistance and then with only
internet access. Task orders and conditions will be counterbalanced to mitigate
order effects. Data collection will include cognitive load surveys, screen
recordings, and task performance metrics such as completion time, code
correctness, and CGT interaction behaviors. Statistical analyses will be
conducted to identify statistically significant differences in CGT usage.
Expected Contributions: Our work can uncover gender differences in CGT
interaction and performance among developers. Our findings can inform future
CGT designs and help address usability and potential disparities in interaction
patterns across diverse user groups. Conclusion: While results are not yet
available, our proposal lays the groundwork for advancing fairness,
accountability, transparency, and ethics (FATE) in CGT design. The outcomes are
anticipated to contribute to inclusive AI practices and equitable tool
development for all users.

</details>


### [500] [VeriOpt: PPA-Aware High-Quality Verilog Generation via Multi-Role LLMs](https://arxiv.org/abs/2507.14776)
*Kimia Tasnia,Alexander Garcia,Tasnuva Farheen,Sazadur Rahman*

Main category: cs.SE

TL;DR: VeriOpt is a novel framework that improves large language models (LLMs) in generating synthesizable Verilog code with Power Performance-Area (PPA)-aware optimization and multi-modal feedback, achieving significant improvements in hardware metrics.


<details>
  <summary>Details</summary>
Motivation: Current LLM implementations for hardware design focus solely on functional correctness, neglecting critical PPA metrics, which are vital for producing industry-grade designs.

Method: VeriOpt organizes LLMs into specialized roles (Planner, Programmer, Reviewer, Evaluator) to mimic human design workflows and integrates PPA constraints into the prompting pipeline. It incorporates multi-modal feedback such as synthesis reports and timing diagrams to enhance optimization.

Result: VeriOpt achieves up to 88% reduction in power, 76% reduction in area, and 73% improvement in timing closure compared to baseline LLM-generated RTL, while also maintaining an 86% functionality success rate, validated via industry-grade EDA tools.

Conclusion: The framework addresses the critical gap between functional correctness and design quality in AI-driven hardware design, enabling reliable and efficient use of LLMs in production workflows.

Abstract: The rapid adoption of large language models(LLMs) in hardware design has
primarily focused on generating functionally correct Verilog code, overlooking
critical Power Performance-Area(PPA) metrics essential for industrial-grade
designs. To bridge this gap, we propose VeriOpt, a novel framework that
leverages role-based prompting and PPA-aware optimization to enable LLMs to
produce high-quality, synthesizable Verilog. VeriOpt structures LLM
interactions into specialized roles (e.g., Planner, Programmer, Reviewer,
Evaluator) to emulate human design workflows, while integrating PPA constraints
directly into the prompting pipeline. By combining multi-modal feedback (e.g.,
synthesis reports, timing diagrams) with PPA aware prompting, VeriOpt achieves
PPA-efficient code generation without sacrificing functional correctness.
Experimental results demonstrate up to 88% reduction in power, 76% reduction in
area and 73% improvement in timing closure compared to baseline LLM-generated
RTL, validated using industry standard EDA tools. At the same time achieves 86%
success rate in functionality evaluation. Our work advances the
state-of-the-art AI-driven hardware design by addressing the critical gap
between correctness and quality, paving the way for reliable LLM adoption in
production workflows.

</details>


### [501] [Enhancing Repository-Level Code Generation with Call Chain-Aware Multi-View Context](https://arxiv.org/abs/2507.14791)
*Yang Liu,Li Zhang,Fang Liu,Zhuohang Wang,Donglin Wei,Zhishuo Yang,Kechi Zhang,Jia Li,Lin Shi*

Main category: cs.SE

TL;DR: RepoScope improves repository-level code generation by addressing context identification issues and leveraging a call chain-aware multi-view context. It achieves up to a 36.35% improvement over existing methods on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current repository-level code generation methods struggle with identifying relevant contexts capturing rich semantics and neglect structural relationships in code during prompt construction, leading to inefficiencies.

Method: RepoScope introduces a Repository Structural Semantic Graph (RSSG) to retrieve four-view contexts (structural and similarity-based). It employs a call chain prediction method and structure-preserving serialization for prompt construction, using static analysis without extra training or multiple queries.

Result: RepoScope significantly outperforms state-of-the-art approaches on benchmarks such as CoderEval and DevEval, showing up to a 36.35% relative improvement in pass@1 scores.

Conclusion: RepoScope is efficient, generalizable, and enhances the accuracy of repository-level code generation by incorporating rich structural and semantic contexts. It integrates effectively with existing systems and holds promise for further applications in code generation.

Abstract: Repository-level code generation aims to generate code within the context of
a specified repository. Existing approaches typically employ
retrieval-augmented generation (RAG) techniques to provide LLMs with relevant
contextual information extracted from the repository. However, these approaches
often struggle with effectively identifying truly relevant contexts that
capture the rich semantics of the repository, and their contextual perspectives
remains narrow. Moreover, most approaches fail to account for the structural
relationships in the retrieved code during prompt construction, hindering the
LLM's ability to accurately interpret the context. To address these issues, we
propose RepoScope, which leverages call chain-aware multi-view context for
repository-level code generation. RepoScope constructs a Repository Structural
Semantic Graph (RSSG) and retrieves a comprehensive four-view context,
integrating both structural and similarity-based contexts. We propose a novel
call chain prediction method that utilizes the repository's structural
semantics to improve the identification of callees in the target function.
Additionally, we present a structure-preserving serialization algorithm for
prompt construction, ensuring the coherence of the context for the LLM.
Notably, RepoScope relies solely on static analysis, eliminating the need for
additional training or multiple LLM queries, thus ensuring both efficiency and
generalizability. Evaluation on widely-used repository-level code generation
benchmarks (CoderEval and DevEval) demonstrates that RepoScope outperforms
state-of-the-art methods, achieving up to a 36.35% relative improvement in
pass@1 scores. Further experiments emphasize RepoScope's potential to improve
code generation across different tasks and its ability to integrate effectively
with existing approaches.

</details>


### [502] [Think Like an Engineer: A Neuro-Symbolic Collaboration Agent for Generative Software Requirements Elicitation and Self-Review](https://arxiv.org/abs/2507.14969)
*Sai Zhang,Zhenchang Xing,Jieshan Chen,Dehai Zhao,Zizhong Zhu,Xiaowang Zhang,Zhiyong Feng,Xiaohong Li*

Main category: cs.SE

TL;DR: The paper proposes RequireCEG, a neuro-symbolic tool that embeds causal-effect graphs (CEGs) to address ambiguity in user requirements for End-User Software Engineering (EUSE), improving software requirement consistency and diversity.


<details>
  <summary>Details</summary>
Motivation: End-users often lack software engineering expertise, leading to ambiguity in their requirement descriptions. Current approaches like Gherkin fail to adequately express causal logic in user narratives.

Method: The authors proposed RequireCEG, which hierarchically analyzes user narratives using a feature tree, generates self-healing causal-effect graphs (CEGs) to capture causal relationships, and optimizes Gherkin scenarios to maintain consistency with system behavior requirements.

Result: RequireCEG performs well on the RGPair dataset, achieving an 87% coverage rate and increasing requirement diversity by 51.88%.

Conclusion: RequireCEG effectively addresses user requirement ambiguity by integrating causal-effect graphs with generative frameworks, improving requirement coverage, diversity, and alignment with system behaviors.

Abstract: The vision of End-User Software Engineering (EUSE) is to empower
non-professional users with full control over the software development
lifecycle. It aims to enable users to drive generative software development
using only natural language requirements. However, since end-users often lack
knowledge of software engineering, their requirement descriptions are
frequently ambiguous, raising significant challenges to generative software
development. Although existing approaches utilize structured languages like
Gherkin to clarify user narratives, they still struggle to express the causal
logic between preconditions and behavior actions. This paper introduces
RequireCEG, a requirement elicitation and self-review agent that embeds
causal-effect graphs (CEGs) in a neuro-symbolic collaboration architecture.
RequireCEG first uses a feature tree to analyze user narratives hierarchically,
clearly defining the scope of software components and their system behavior
requirements. Next, it constructs the self-healing CEGs based on the elicited
requirements, capturing the causal relationships between atomic preconditions
and behavioral actions. Finally, the constructed CEGs are used to review and
optimize Gherkin scenarios, ensuring consistency between the generated Gherkin
requirements and the system behavior requirements elicited from user
narratives. To evaluate our method, we created the RGPair benchmark dataset and
conducted extensive experiments. It achieves an 87% coverage rate and raises
diversity by 51.88%.

</details>


### [503] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: The paper introduces AIDev, a unique dataset capturing the operational data of autonomous AI coding agents in software development. It analyzes pull requests from five leading AI agents across 61,000 repositories to enable research in AI-native workflows.


<details>
  <summary>Details</summary>
Motivation: To study the impact and capabilities of autonomous AI coding agents in shaping the future of software engineering and address gaps in trust, collaboration, and utility.

Method: Development of AIDev, a large-scale dataset aggregating real-world data from AI coding agents operating in software projects, including metadata on their activity and outcomes.

Result: Findings showed gaps in acceptance rates and complexities in pull requests submitted by AI agents, despite their speed advantage. AIDev enables exploration into AI governance, readiness, and collaboration modeling.

Conclusion: AIDev stands as a foundational resource for advancing SE 3.0, supporting research into AI-native software development and human-AI symbiotic collaboration.

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [504] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: The paper investigates the application of Generative AI (GenAI) in automotive software development, focusing on processes like requirements handling, compliance, and code generation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage Generative AI to streamline automotive software development, making it more cost-effective, efficient, and less reliant on human labor due to complex and standardized workflows.

Method: The authors review and analyze state-of-the-art GenAI technologies, such as Large Language Models, Retrieval Augmented Generation, and Vision Language Models. They also summarize prompting techniques for code generation.

Result: They propose a generalized workflow for GenAI-aided automotive software development and share findings from a survey conducted among industry partners to identify GenAI tools commonly used.

Conclusion: The study highlights the potential of GenAI to transform automotive software development by simplifying and automating key processes, supported by both theoretical findings and industry feedback.

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [505] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: This paper investigates the application of state-of-the-art large language models (LLMs) in automating the generation and evaluation of user stories (US) during the requirements elicitation process in agile frameworks.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in requirements elicitation, particularly the difficulty in translating complex needs into high-quality requirements and the time-consuming task of manually evaluating semantic quality metrics.

Method: The study tested 10 state-of-the-art LLMs to generate US by emulating customer interviews and compared the generated US to those written by humans. It also evaluated LLMs' ability to assess the semantic quality of US using predefined criteria.

Result: LLMs generated US comparable to humans in coverage and style but showed lower diversity and creativity. They generally failed to meet acceptance quality criteria as frequently as human-generated US. However, LLMs performed reliably in semantic quality assessment when given clear evaluation criteria.

Conclusion: LLMs can assist in automating both the generation and evaluation of US, showing potential to reduce human effort, though their limitations in meeting acceptance criteria and creativity should be considered.

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [506] [Deep Learning Framework Testing via Heuristic Guidance Based on Multiple Model Measurements](https://arxiv.org/abs/2507.15181)
*Yinglong Zou,Juan Zhai,Chunrong Fang,Yanzhou Mu,Jiawei Liu,Zhenyu Chen*

Main category: cs.SE

TL;DR: Existing testing methods for deep learning frameworks have limitations in measuring operator variety, execution time, and correlation among measurement indicators. DLMMM addresses these with multi-measurement fused guidance.


<details>
  <summary>Details</summary>
Motivation: Current deep learning framework testing methods inadequately evaluate operator combinations, execution time, and fail to combine multiple measurements effectively, hampering bug detection.

Method: DLMMM integrates quantitative measurements of operator variety, execution time, and bug detection, then fuses these metrics with their correlations to provide heuristic guidance.

Result: DLMMM improves testing effectiveness by addressing critical limitations in operator diversity, execution time management, and multi-measurement correlations in test input generation.

Conclusion: DLMMM advances deep learning framework testing by combining multiple model measurements and their trade-offs into a unified method, offering more robust bug detection.

Abstract: Deep learning frameworks serve as the foundation for developing and deploying
deep learning applications. To enhance the quality of deep learning frameworks,
researchers have proposed numerous testing methods using deep learning models
as test inputs. However, existing methods predominantly measure model bug
detection effectiveness as heuristic indicators, presenting three critical
limitations: Firstly, existing methods fail to quantitatively measure model's
operator combination variety, potentially missing critical operator
combinations that could trigger framework bugs. Secondly, existing methods
neglect measuring model execution time, resulting in the omission of numerous
models potential for detecting more framework bugs within limited testing time.
Thirdly, existing methods overlook correlation between different model
measurements, relying simply on single-indicator heuristic guidance without
considering their trade-offs. To overcome these limitations, we propose DLMMM,
the first deep learning framework testing method to include multiple model
measurements into heuristic guidance and fuse these measurements to achieve
their trade-off. DLMMM firstly quantitatively measures model's bug detection
performance, operator combination variety, and model execution time. After
that, DLMMM fuses the above measurements based on their correlation to achieve
their trade-off. To further enhance testing effectiveness, DLMMM designs
multi-level heuristic guidance for test input model generation.

</details>


### [507] [Cultural Impact on Requirements Engineering Activities: Bangladeshi Practitioners' View](https://arxiv.org/abs/2507.15188)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: The paper investigates the impact of Bangladeshi cultural influences on Requirements Engineering (RE) practices, addressing cultural aspects to reduce misunderstandings and foster diversity in IT.


<details>
  <summary>Details</summary>
Motivation: To understand cultural influences on RE practices in the context of Bangladesh's IT sector, promote inclusion, and minimize stakeholder conflicts in diverse global development projects.

Method: The authors study existing RE practices in Bangladesh, analyzing how socio-cultural factors influence these activities and interact with the RE process.

Result: The findings highlight the unique cultural factors in Bangladesh's IT sector and their role in shaping RE activities, providing insights to improve RE practices in diverse cultural contexts.

Conclusion: RE practitioners should account for diverse cultural influences, which can enable more inclusive, effective, and conflict-free software development processes globally.

Abstract: Requirements Engineering (RE) is one of the most interaction-intensive phases
of software development. This means that RE activities might be especially
impacted by stakeholders' national culture. Software development projects
increasingly have a very diverse range of stakeholders. To future-proof RE
activities, we need to help RE practitioners avoid misunderstandings and
conflicts that might arise from not understanding potential Cultural Influences
(CIs). Moreover, an awareness of CIs supports diversity and inclusion in the IT
profession. Bangladesh has a growing IT sector with some unique socio-cultural
characteristics, and has been largely overlooked in this research field. In
this study, we aim to investigate how the RE process is adopted in the context
of Bangladeshi culture and what cultural influences impact overall RE
activities.

</details>


### [508] [Towards Using Personas in Requirements Engineering: What Has Been Changed Recently?](https://arxiv.org/abs/2507.15197)
*Chowdhury Shahriar Muzammel,Maria Spichkova,James Harland*

Main category: cs.SE

TL;DR: The paper conducts a systematic study on the use of personas in requirements engineering (RE), focusing on trends and the influence of Generative AI (2023-2025).


<details>
  <summary>Details</summary>
Motivation: To explore the evolution and current trends in the application of personas in RE, particularly with the emergence of Generative AI.

Method: A systematic mapping study analyzing 22 publications from April 2023 to April 2025 on persona representation, construction, validation, and related RE activities.

Result: AI-based solutions for persona construction and validation are increasingly applied. Template-based personas are gaining popularity. There is a growing focus on validation aspects in persona studies.

Conclusion: The study provides insights into recent trends in using personas in RE, highlighting the role of AI in advancing persona-related methodologies.

Abstract: In requirements engineering (RE), personas are now being used to represent
user expectations and needs. This systematic mapping study (SMS) aims to
explore the most recent studies and to cover recent changes in trends,
especially related to the recent evolution of Generative AI approaches. Our SMS
covers the period between April 2023 and April 2025. We identified 22 relevant
publications and analysed persona representation, construction, validation, as
well as RE activities covered by personas. We identified that a number of
studies applied AI-based solutions for persona construction and validation. We
observed that template-based personas are becoming more popular nowadays. We
also observed an increase in the proportion of studies covering validation
aspects.

</details>


### [509] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: The paper introduces SimdBench, the first benchmark specifically designed for testing SIMD-intrinsic code generation by Large Language Models (LLMs).


<details>
  <summary>Details</summary>
Motivation: Modern processors heavily use SIMD instructions for performance-critical tasks, and SIMD programming remains challenging despite its widespread use. Large Language Models show potential in assisting these tasks, but existing benchmarks do not evaluate their vectorized code generation capabilities.

Method: The authors developed SimdBench, a benchmark consisting of 136 tasks targeting five representative SIMD intrinsics. They conducted systematic evaluations of 18 LLMs, assessing both correctness and performance.

Result: Evaluation demonstrated a consistent performance drop in LLMs when generating SIMD-intrinsic code as opposed to scalar code. Insights for future improvement were derived from these findings.

Conclusion: SimdBench fills an important gap in understanding LLMs' capabilities in SIMD programming, offering a framework for progress in this domain and advancing research through open-source availability.

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [510] [Code Clone Detection via an AlphaFold-Inspired Framework](https://arxiv.org/abs/2507.15226)
*Changguo Jia,Yi Zhan,Tianqi Zhao,Hengzhi Ye,Minghui Zhou*

Main category: cs.SE

TL;DR: AlphaCC adapts AlphaFold's sequence-to-structure technique to detect semantic code clones across programming languages.


<details>
  <summary>Details</summary>
Motivation: Current code clone detection methods struggle with capturing code semantics and are often tied to specific programming languages.

Method: AlphaCC encodes code fragments as token sequences, uses multiple sequence alignment, adapts AlphaFold's attention-based encoder to model dependencies, and calculates similarity scores to classify clones.

Result: AlphaCC outperforms baselines in semantic clone detection on diverse datasets, demonstrating strong semantic capabilities while maintaining efficiency.

Conclusion: AlphaCC provides a practical and effective solution for multi-language code clone detection, showing promise for real-world applications.

Abstract: Code clone detection, which aims to identify functionally equivalent code
fragments, plays a critical role in software maintenance and vulnerability
analysis. Substantial methods have been proposed to detect code clones, but
they fall short in capturing code semantics or relying on language-specific
analyzers. Inspired by the remarkable success of AlphaFold in predicting
three-dimensional protein structures from protein sequences, in this paper, we
leverage AlphaFold for code clone detection based on the insight that protein
sequences and token sequences share a common linear sequential structure. In
particular, we propose AlphaCC, which represents code fragments as token
sequences to ensure multi-language applicability and adapts AlphaFold's
sequence-to-structure modeling capability to infer code semantics. The pipeline
of AlphaCC goes through three steps. First, AlphaCC transforms each input code
fragment into a token sequence and, motivated by AlphaFold's use of multiple
sequence alignment (MSA) to enhance contextual understanding, constructs an MSA
from lexically similar token sequences. Second, AlphaCC adopts a modified
attention-based encoder based on AlphaFold to model dependencies within and
across token sequences. Finally, unlike AlphaFold's protein structure
prediction task, AlphaCC computes similarity scores between token sequences
through a late interaction strategy and performs binary classification to
determine code clone pairs. Comprehensive evaluations on three language-diverse
datasets demonstrate AlphaCC's applicability across multiple programming
languages. On two semantic clone detection datasets, it consistently
outperforms all baselines, showing strong semantic understanding. Moreover,
AlphaCC maintains competitive efficiency, enabling practical usage in
large-scale clone detection tasks.

</details>


### [511] [FaultLine: Automated Proof-of-Vulnerability Generation Using LLM Agents](https://arxiv.org/abs/2507.15241)
*Vikram Nitin,Baishakhi Ray,Roshanak Zilouchian Moghaddam*

Main category: cs.SE

TL;DR: The paper introduces FaultLine, an LLM-based workflow designed to generate proof-of-vulnerability (PoV) tests for software vulnerabilities, improving validation and understanding while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Software vulnerability reports often lack PoV tests, which are necessary for validating fixes, preventing regressions, and aiding developers in understanding exploits.

Method: FaultLine employs hierarchical reasoning steps inspired by static and dynamic program analysis to trace input flow, determine branch conditions, and generate PoV tests. It operates across programming languages without using language-specific static/dynamic analysis components.

Result: On a dataset of 100 vulnerabilities across Java, C, and C++ projects, FaultLine successfully generated PoV tests for 16 projects, outperforming the previous state-of-the-art framework, CodeAct 2.1, which handled only 9 projects—a 77% relative improvement.

Conclusion: FaultLine demonstrates that hierarchical reasoning can improve PoV test generation using LLMs but highlights the general difficulty of the problem, advocating further research by making its code and dataset publicly available.

Abstract: Despite the critical threat posed by software security vulnerabilities,
reports are often incomplete, lacking the proof-of-vulnerability (PoV) tests
needed to validate fixes and prevent regressions. These tests are crucial not
only for ensuring patches work, but also for helping developers understand how
vulnerabilities can be exploited. Generating PoV tests is a challenging
problem, requiring reasoning about the flow of control and data through deeply
nested levels of a program.
  We present FaultLine, an LLM agent workflow that uses a set of carefully
designed reasoning steps, inspired by aspects of traditional static and dynamic
program analysis, to automatically generate PoV test cases. Given a software
project with an accompanying vulnerability report, FaultLine 1) traces the flow
of an input from an externally accessible API ("source") to the "sink"
corresponding to the vulnerability, 2) reasons about the conditions that an
input must satisfy in order to traverse the branch conditions encountered along
the flow, and 3) uses this reasoning to generate a PoV test case in a
feedback-driven loop. FaultLine does not use language-specific static or
dynamic analysis components, which enables it to be used across programming
languages.
  To evaluate FaultLine, we collate a challenging multi-lingual dataset of 100
known vulnerabilities in Java, C and C++ projects. On this dataset, FaultLine
is able to generate PoV tests for 16 projects, compared to just 9 for CodeAct
2.1, a popular state-of-the-art open-source agentic framework. Thus, FaultLine
represents a 77% relative improvement over the state of the art. Our findings
suggest that hierarchical reasoning can enhance the performance of LLM agents
on PoV test generation, but the problem in general remains challenging. We make
our code and dataset publicly available in the hope that it will spur further
research in this area.

</details>


### [512] [Input Reduction Enhanced LLM-based Program Repair](https://arxiv.org/abs/2507.15251)
*Boyang Yang,Luyao Ren,Xin Yin,Jiadong Ren,Haoye Tian,Shunfu Jin*

Main category: cs.SE

TL;DR: The paper introduces ReduceFix, a method to enhance Large Language Model (LLM)-based Automated Program Repair (APR) by reducing test inputs while retaining their failure-inducing behaviors. This approach addresses the 'lost-in-the-middle' issue caused by long prompts.


<details>
  <summary>Details</summary>
Motivation: LLMs face challenges in retaining context and key information when dealing with extensive test inputs in APR tasks, potentially affecting their ability to generate effective program repairs.

Method: The proposed method, ReduceFix, employs a built-in component to reduce test inputs without losing failure-inducing characteristics. It uses LLMs to create reducers for automatic test input minimization and leverages these reduced inputs to guide patch generation.

Result: On a dedicated benchmark (LFTBench), ReduceFix reduced test input size by 89.1% on average and boosted patch accuracy significantly: achieving up to 53.8% improvement compared to using original tests and 17.6% compared to omitting tests.

Conclusion: Automatically reducing failing test inputs enhances the scalability and effectiveness of LLM-based APR approaches, making them more practical for real-world applications.

Abstract: Large Language Models (LLMs) have shown great potential in Automated Program
Repair (APR). Test inputs, being crucial for reasoning the root cause of
failures, are always included in the prompt for LLM-based APR. Unfortunately,
LLMs struggle to retain key information in long prompts. When the test inputs
are extensive in the prompt, this may trigger the "lost-in-the-middle" issue,
compromising repair performance. To address this, we propose ReduceFix, an
LLM-based APR approach with a built-in component that automatically reduces
test inputs while retaining their failure-inducing behavior. ReduceFix prompts
an LLM to generate a reducer that minimizes failure-inducing test inputs
without human effort, and then feeds the reduced failure-inducing inputs to
guide patch generation.
  For targeted evaluation, we constructed LFTBench, the first long-input APR
benchmark with 200 real bugs from 20 programming tasks, each paired with a
failure-inducing input whose median size is 1 MB. On this benchmark, ReduceFix
shrinks inputs by 89.1% on average and improves overall pass@10 by up to 53.8%
relative to a prompt that includes the original test, and by 17.6% compared
with omitting the test entirely. Adding the same reduction step to ChatRepair
increases its fix rate by 21.3% without other changes. Ablation studies further
highlight the impact of input length and compressed failure information on
repair success. These results underscore that automatically reducing failing
inputs is a practical and powerful complement to LLM-based APR, significantly
improving its scalability and effectiveness.

</details>


### [513] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: The paper addresses parameter failures in tool-agent interactions with LLMs, proposing a taxonomy and improvement suggestions.


<details>
  <summary>Details</summary>
Motivation: To explore and address issues limiting the effectiveness of the tool-agent paradigm in LLMs.

Method: Developed a parameter failure taxonomy, tested correlations using input perturbation methods, and analyzed experimental results.

Result: Identified five failure categories with their sources, showing parameter name hallucination originates in LLMs while others stem from input sources.

Conclusion: Recommendations to enhance reliability include standardized formats, robust error feedback, and better parameter consistency.

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [514] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans enhances Transformer architecture by incorporating hidden state stacks, enabling effective handling of deterministic context-free grammars and Chomsky hierarchy tasks.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the intrinsic limitation of the Transformer architecture in capturing deterministic context-free grammars and regular expressions efficiently.

Method: StackTrans introduces differentiable stack operations like push and pop between Transformer layers, making it compatible with frameworks such as flash-attention.

Result: StackTrans demonstrates superior performance on Chomsky hierarchies and large-scale natural language benchmarks, outperforming standard models and competing LLMs in efficiency and reasoning.

Conclusion: StackTrans provides a significant improvement in Transformer-based architectures, bridging gaps in grammatical representation while maintaining compatibility with existing frameworks.

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [515] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: The paper proposes applying a 'Chinese Wall' technique to enhance weaker, ethically aligned Code LLMs by using stronger models to generate instructions, demonstrating significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Address concerns about copyright violations in training datasets of powerful Code LLMs by improving weaker, ethically aligned models.

Method: Apply the 'Chinese Wall' technique where a powerful model generates detailed instructions to train a weaker model.

Result: The method improved Comma v0.1 1T's CanItEdit benchmark performance by 66% and Starcoder2 Instruct's by 20%.

Conclusion: While the technique is promising for enhancing ethically aligned models, its practical application is currently limited due to insufficient publicly available training datasets.

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [516] [Hot Topics and Common Challenges: an Empirical Study of React Discussions on Stack Overflow](https://arxiv.org/abs/2507.15624)
*Yusuf Sulistyo Nugroho,Ganno Tribuana Kurniaji,Syful Islam,Mohammed Humayun Kabir,Vanesya Aura Ardity,Md. Kamal Uddin*

Main category: cs.SE

TL;DR: This paper analyzes Stack Overflow questions to identify challenges React users face.


<details>
  <summary>Details</summary>
Motivation: Understanding specific challenges React users encounter, beyond its known popularity and advantages.

Method: Exploratory data analysis of React-related Stack Overflow questions focusing on keywords, error classification, and user reputation-based errors.

Result: Algorithmic errors are the most frequent issues, with mid-reputation users contributing 55.77%. Top keywords include code, link, vir, href, connect, azure, windows, and website.

Conclusion: The study highlights the importance of guiding resources for solving algorithmic issues in the React community.

Abstract: React is a JavaScript library used to build user interfaces for single-page
applications. Although recent studies have shown the popularity and advantages
of React in web development, the specific challenges users face remain unknown.
Thus, this study aims to analyse the React-related questions shared on Stack
Overflow. The study utilizes an exploratory data analysis to investigate the
most frequently discussed keywords, error classification, and user
reputation-based errors, which is the novelty of this work. The results show
the top eight most frequently used keywords on React-related questions, namely,
code, link, vir, href, connect, azure, windows, and website. The error
classification of questions from the sample shows that algorithmic error is the
most frequent issue faced by all groups of users, where mid-reputation users
contribute the most, accounting for 55.77%. This suggests the need for the
community to provide guidance materials in solving algorithm-related problems.
We expect that the results of this study will provide valuable insight into
future research to support the React community during the early stages of
implementation, facilitating their ability to effectively overcome challenges
to adoption.

</details>


### [517] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: The paper introduces SustainDiffusion, a method to enhance the social and environmental sustainability of Stable Diffusion models by reducing biases and energy consumption without compromising image quality.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address concerns about the societal and environmental impacts of widely-used text-to-image generation models like Stable Diffusion, which produce billions of images annually.

Method: The method involves SustainDiffusion, which optimizes hyperparameters and prompt structures to minimize gender and ethnic biases and reduce energy usage, all while maintaining image quality.

Result: SustainDiffusion empirically reduces gender bias by 68%, ethnic bias by 59%, and energy consumption by 48%, with consistent results across prompts and runs.

Conclusion: The study shows that improvements in social and environmental sustainability for text-to-image models are achievable without altering the model's architecture or requiring fine-tuning.

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [518] [Modeling CubeSat Storage Battery Discharge: Equivalent Circuit Versus Machine Learning Approaches](https://arxiv.org/abs/2507.15666)
*Igor Turkin,Lina Volobuieva,Andriy Chukhray,Oleksandr Liubimov*

Main category: cs.SE

TL;DR: The paper compares analytical and machine learning approaches to model battery discharge in CubeSat satellites, highlighting pros and cons of both methods.


<details>
  <summary>Details</summary>
Motivation: To improve prediction and ensure fault tolerance of CubeSat satellite power systems, modeling battery discharge is necessary.

Method: The study uses orbital data samples including voltage, current, and temperature for analytical modeling (based on physical laws) and machine learning.

Result: Analytical modeling is transparent but less flexible; machine learning adapts better and yields more accurate results.

Conclusion: Machine learning is more suited for modeling CubeSat battery discharge due to its accuracy and adaptability.

Abstract: The subject of the article is the study and comparison of two approaches to
modelling the battery discharge of a CubeSat satellite: analytical using
equivalent circuit and machine learning. The article aims to make a reasoned
choice of the approach to modelling the battery discharge of a CubeSat
satellite. Modelling the battery discharge of a satellite will enable the
prediction of the consequences of disconnecting the autonomous power system and
ensure the fault tolerance of equipment in orbit. Therefore, the selected study
is relevant and promising. This study focuses on the analysis of CubeSat
satellite data, based explicitly on orbital data samples of the power system,
which include data available at the time of the article publication. The
dataset contains data on the voltage, current, and temperature of the battery
and solar panels attached to the five sides of the satellite. In this context,
two approaches are considered: analytical modelling based on physical laws and
machine learning, which uses empirical data to create a predictive model.
Results: A comparative analysis of the modeling results reveals that the
equivalent circuit approach has the advantage of transparency, as it identifies
possible parameters that facilitate understanding of the relationships.
However, the model is less flexible to environmental changes or non-standard
satellite behavior. The machine learning model demonstrated more accurate
results, as it can account for complex dependencies and adapt to actual
conditions, even when they deviate from theoretical assumptions.

</details>


### [519] [BugScope: Learn to Find Bugs Like Human](https://arxiv.org/abs/2507.15671)
*Jinyao Guo,Chengpeng Wang,Dominic Deluca,Jinjie Liu,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: BugScope is a multi-agent system utilizing LLMs to detect software bugs more effectively than traditional methods, achieving impressive precision and recall on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: The diversity and complexity of software bugs, coupled with limitations in traditional static analysis tools and LLM-based methods, underline the need for a more adaptable solution to detect various bug patterns.

Method: BugScope simulates human learning with examples, employs program slicing for context extraction, and uses LLMs with tailored prompts for improved bug detection.

Result: BugScope demonstrated 87.04% precision and 90.00% recall on curated open-source bug datasets and identified 141 unknown bugs in large systems, many of which were confirmed and addressed.

Conclusion: BugScope shows significant advantages in identifying and resolving bugs, outperforming existing tools and driving practical impact in software auditing.

Abstract: Detecting software bugs remains a fundamental challenge due to the extensive
diversity of real-world defects. Traditional static analysis tools often rely
on symbolic workflows, which restrict their coverage and hinder adaptability to
customized bugs with diverse anti-patterns. While recent advances incorporate
large language models (LLMs) to enhance bug detection, these methods continue
to struggle with sophisticated bugs and typically operate within limited
analysis contexts. To address these challenges, we propose BugScope, an
LLM-driven multi-agent system that emulates how human auditors learn new bug
patterns from representative examples and apply that knowledge during code
auditing. Given a set of examples illustrating both buggy and non-buggy
behaviors, BugScope synthesizes a retrieval strategy to extract relevant
detection contexts via program slicing and then constructs a tailored detection
prompt to guide accurate reasoning by the LLM. Our evaluation on a curated
dataset of 40 real-world bugs drawn from 21 widely-used open-source projects
demonstrates that BugScope achieves 87.04% precision and 90.00% recall,
surpassing state-of-the-art industrial tools by 0.44 in F1 score. Further
testing on large-scale open-source systems, including the Linux kernel,
uncovered 141 previously unknown bugs, of which 78 have been fixed and 7
confirmed by developers, highlighting BugScope's substantial practical impact.

</details>


### [520] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: This paper investigates the role of Large Language Models (LLMs) in Automatic Program Repair (APR) by comparing debugging performance with and without LLM assistance using formal program-proving tools.


<details>
  <summary>Details</summary>
Motivation: The paper aims to assess whether AI, particularly LLMs, can reliably improve debugging and APR processes, addressing correctness and integration with human programmers' workflows.

Method: Two groups of programmers (one using LLMs and the other without) debugged code while validating fixes with program-proving tools. The study used detailed session recordings and applied the Goal-Query-Metric approach.

Result: The study found surprising outcomes regarding the effectiveness of LLMs in debugging and APR. It identified 7 distinct patterns of LLM use and fine-grain insights into programmer behaviors.

Conclusion: The findings provide actionable advice on utilizing LLMs effectively, propose reusable methodologies, and contribute to understanding AI's role in guaranteed-correct program fixes.

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


### [521] [Investigating the Use of LLMs for Evidence Briefings Generation in Software Engineering](https://arxiv.org/abs/2507.15828)
*Mauro Marcelino,Marcos Alves,Bianca Trinkenreich,Bruno Cartaxo,Sérgio Soares,Simone D. J. Barbosa,Marcos Kalinowski*

Main category: cs.SE

TL;DR: The paper explores using a Retrieval-Augmented Generation (RAG) LLM to automate Evidence Briefings and plans to compare their quality to human-made versions.


<details>
  <summary>Details</summary>
Motivation: Manual creation of Evidence Briefings is time-intensive, hindering their broad adoption despite their usefulness in connecting research findings with practitioners.

Method: A RAG-based LLM tool was developed to automatically generate Evidence Briefings. A controlled experiment was designed to compare its output with human-made versions in terms of fidelity, understanding, and usefulness.

Result: Results are pending and will be reported after experimental trials.

Conclusion: Conclusions will depend on the outcomes of the evaluation experiment.

Abstract: [Context] An evidence briefing is a concise and objective transfer medium
that can present the main findings of a study to software engineers in the
industry. Although practitioners and researchers have deemed Evidence Briefings
useful, their production requires manual labor, which may be a significant
challenge to their broad adoption. [Goal] The goal of this registered report is
to describe an experimental protocol for evaluating LLM-generated evidence
briefings for secondary studies in terms of content fidelity, ease of
understanding, and usefulness, as perceived by researchers and practitioners,
compared to human-made briefings. [Method] We developed an RAG-based LLM tool
to generate evidence briefings. We used the tool to automatically generate two
evidence briefings that had been manually generated in previous research
efforts. We designed a controlled experiment to evaluate how the LLM-generated
briefings compare to the human-made ones regarding perceived content fidelity,
ease of understanding, and usefulness. [Results] To be reported after the
experimental trials. [Conclusion] Depending on the experiment results.

</details>


### [522] [Observing Fine-Grained Changes in Jupyter Notebooks During Development Time](https://arxiv.org/abs/2507.15831)
*Sergey Titov,Konstantin Grotov,Cristina Sarasua,Yaroslav Golubev,Dhivyabharathi Ramasamy,Alberto Bacchelli,Abraham Bernstein,Timofey Bryksin*

Main category: cs.SE

TL;DR: The paper focuses on analyzing development changes in Jupyter notebooks during data science tasks, collecting and examining a large dataset to understand notebook dynamics and usage.


<details>
  <summary>Details</summary>
Motivation: While software engineering has explored fine-grained logs for key innovations, computational notebooks in data science remain underexplored, prompting an investigation into their dynamic development and debugging processes.

Method: The authors developed a toolset to collect code changes during notebook development. They analyzed a dataset of 100+ hours of Jupyter notebook work by 20 developers, examining 2,655 cells and 9,207 executions.

Result: Their analysis revealed that notebook changes often involve small fixes and iterative code modifications, highlighting their dual role as both exploration and debugging tools.

Conclusion: Notebooks show dynamic and iterative development practices. The study provides insights into their usage patterns and proposes directions for future research in this niche area.

Abstract: In software engineering, numerous studies have focused on the analysis of
fine-grained logs, leading to significant innovations in areas such as
refactoring, security, and code completion. However, no similar studies have
been conducted for computational notebooks in the context of data science.
  To help bridge this research gap, we make three scientific contributions: we
(1) introduce a toolset for collecting code changes in Jupyter notebooks during
development time; (2) use it to collect more than 100 hours of work related to
a data analysis task and a machine learning task (carried out by 20 developers
with different levels of expertise), resulting in a dataset containing 2,655
cells and 9,207 cell executions; and (3) use this dataset to investigate the
dynamic nature of the notebook development process and the changes that take
place in the notebooks.
  In our analysis of the collected data, we classified the changes made to the
cells between executions and found that a significant number of these changes
were relatively small fixes and code iteration modifications. This suggests
that notebooks are used not only as a development and exploration tool but also
as a debugging tool. We report a number of other insights and propose potential
future research directions on the novel data.

</details>


### [523] [Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs](https://arxiv.org/abs/2411.01789)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: The paper proposes a framework to automate test oracles using large language models (LLMs) by extracting expected behaviors from Java library Javadocs, achieving high accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Software testing heavily relies on the quality of test oracles, which verify expected output behavior. Automating test oracles remains underexplored despite advancements in generating test inputs. This paper seeks to enable such automation, specifically focusing on Java libraries.

Method: The authors leverage large language models (LLMs) to extract informative test oracle rules from Java library Javadocs. The framework analyzes natural language documentation to produce assertions for verifying both normal and exceptional states of library behavior.

Result: Experiments show the framework produces compilable test oracles with 98.8% reliability and accurately captures intended behaviors 96.4% of the time. Errors that do occur are minor and can be rectified using comment information generated by LLMs.

Conclusion: LLMs are effective for automating test oracle generation, offering high accuracy and practical usability for validating behaviors of Java libraries through Javadocs as a source of information.

Abstract: Software testing remains the most widely used methodology for validating
quality of code. However, effectiveness of testing critically depends on the
quality of test suites used. Test cases in a test suite consist of two
fundamental parts: (1) input values for the code under test, and (2) correct
checks for the outputs it produces. These checks are commonly written as
assertions, and termed test oracles. The last couple of decades have seen much
progress in automated test input generation, e.g., using fuzzing and symbolic
execution. However, automating test oracles remains a relatively less explored
problem area. Indeed, a test oracle by its nature requires knowledge of
expected behavior, which may only be known to the developer and may not not
exist in a formal language that supports automated reasoning.
  Our focus in this paper is automation of test oracles for clients of widely
used Java libraries, e.g., java.lang and java.util packages. Our key insight is
that Javadocs that provide a rich source of information can enable automated
generation of test oracles. Javadocs of the core Java libraries are fairly
detailed documents that contain natural language descriptions of not only how
the libraries behave but also how the clients must (not) use them. We use large
language models as an enabling technology to embody our insight into a
framework for test oracle automation, and evaluate it experimentally. Our
experiments demonstrate that LLMs can generate oracles for checking normal and
exceptional behaviors from Javadocs, with 98.8% of these oracles being
compilable and 96.4% accurately reflecting intended properties. Even for the
few incorrect oracles, errors are minor and can be easily corrected with the
help of additional comment information generated by the LLMs.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [524] [Predicting Perceptual Boundaries in Auditory Streaming using Delay Differential Equations](https://arxiv.org/abs/2507.14157)
*Asim Alawfi,Farzaneh Darki,Jan Sieber*

Main category: q-bio.NC

TL;DR: This paper models auditory streaming using coupled neural populations to explore perception boundaries between single tones, two tones, and bistable perceptions.


<details>
  <summary>Details</summary>
Motivation: To understand how the brain organizes auditory stimuli, particularly in bistable and boundary scenarios, using a neural model to link perception dynamics to neural mechanisms.

Method: The authors use a mathematical model of two neural populations with delayed cross-inhibition responding to two-tone step-like stimuli, analyzing bifurcations and proposing variable-thresholds for perception classification.

Result: The model demonstrates how symmetry-breaking bifurcations govern transitions between perceptual states and identifies that fixed thresholds are insufficient; stimulus parameters shape the perceptual boundaries.

Conclusion: A variable-threshold approach better explains auditory perceptual boundaries, deepening the connection between neural mechanisms and perceptual outcomes in auditory streams.

Abstract: Auditory streaming enables the brain to organize sequences of sounds into
perceptually distinct sources, such as following a conversation in a noisy
environment. A typical experiment for investigating perceptual boundaries and
bistability is to present a subject with a stream containing two alternating
tone stimuli. We investigate a model for the processing of such a stream
consisting of two identical neural populations of excitatory and inhibitory
neurons. The populations are coupled via delayed cross-inhibition and
periodically forced with sharp step-type signals (the two-tone stream). We
track how the perception boundary depends on threshold selection and establish
how boundaries between three different auditory perceptions (single tone versus
two tones versus bistability between both perceptions) relate to bifurcations
such as symmetry breaking. We demonstrate that these transitions are governed
by symmetry-breaking bifurcations and that the perceptual classification based
on neural thresholds is highly sensitive to threshold choice. Our analysis
reveals that a fixed threshold is insufficient to capture the true perceptual
boundaries and proposes a variable-threshold criterion, informed by the
amplitude dynamics of neural responses. Finally, we illustrate how key stimulus
parameters such as tone duration, delay, and internal time scale shape the
boundaries of auditory perceptual organization in the plane of the two most
commonly varied experimental parameters, the representation rate, and the
difference in tone frequency. These findings offer mechanistic insight into
auditory perception dynamics and provide a refined framework for linking neural
activity to perceptual organization.

</details>


### [525] [Modeling Language Evolution Using a Spin Glass Approach](https://arxiv.org/abs/2507.14375)
*Hediye Yarahmadi,Giuseppe Longobardi,Alessandro Treves*

Main category: q-bio.NC

TL;DR: The study proposes that disorder, rather than efficiency, plays a key role in language evolution. It uses a model of syntactic parameter interactions to explain features like slow change, diversification, and metastable states observed in natural languages.


<details>
  <summary>Details</summary>
Motivation: To resolve why natural languages evolve despite being effective tools of communication, and why their evolution is slow and diverse rather than converging on an optimum.

Method: Researchers reduced syntax to binary parameters and modeled diachronic language change using disordered interactions. They analyzed the glassy dynamics in syntactic spaces under different conditions and included a Hopfield-type memory stabilizing mechanism.

Result: They discovered that syntactic vectors tend to get trapped in metastable states under certain disorder conditions, mimicking real-world language behavior. A memory mechanism can stabilize syntactic states but reduces diversity. This aligns with linguistic distance and phylogenetic signals in related languages.

Conclusion: Disordered interactions and metastable states may explain the observed dynamics of natural language evolution, linking to universal linguistic traits such as divergence and stability over time.

Abstract: The evolution of natural languages poses a riddle to any theoretical
perspective based on efficiency considerations. If languages are already
optimally effective means of organization and communication of thought, why do
they change? And if they are driven to become optimally effective in the
future, why do they change so slowly, and why do they diversify, rather than
converge towards an optimum? We look here at the hypothesis that disorder,
rather than efficiency, may play a dominant role. Most traditional approaches
to study diachronic language dynamics emphasize lexical data, but a crucial
contribution to the effectiveness of a thought-coding device is given by its
core structure, its syntax. Based on the reduction of syntax to a set of binary
parameters, we introduce here a model of natural language change in which
diachronic dynamics are mediated by disordered interactions between parameters,
even in the idealized limit of identical external inputs. We show in which
region of `phase space' such dynamics show the glassy features that are
observed in natural languages. In particular, syntactic vectors remain trapped
in glassy metastable (tendentially stable) states when the degree of asymmetry
in the disordered interactions is below a critical value, consistent with
studies of spin glasses with asymmetric interactions. We further show that an
added Hopfield-type memory term, would indeed, if strong enough, stabilize
syntactic configurations even above the critical value, but losing the
multiplicity of stable states. Finally, using a notion of linguistic distance
in syntactic space we show that a phylogenetic signal may remain among related
languages, despite their gradually divergent syntax, exactly as recently
pointed out for real-world languages. These statistical results appear to
generalize beyond the dataset of 94 syntactic parameters across 58 languages,
used in this study.

</details>


### [526] [Baseline behaviour in human vision](https://arxiv.org/abs/2507.14573)
*Thomas Fabian*

Main category: q-bio.NC

TL;DR: This study demonstrates that human gaze behaviors can be modeled using statistical regularities, which are context-invariant across tasks, observers, and viewed content.


<details>
  <summary>Details</summary>
Motivation: Despite numerous gaze models tailored to specific contexts or tasks, there is no existing universal model capturing gaze behavior across varied scenarios. The research aims to uncover if these behaviors are governed by context-independent regularities.

Method: The researchers used a context-agnostic eye movement model with a fixed transition kernel to compare gaze patterns in diverse viewing situations, including reading, visual search, and scene perception for adults and children.

Result: The context-agnostic model outperformed a random model in explaining human gaze behavior across different tasks and viewer groups, suggesting the presence of universal statistical regularities.

Conclusion: Human gaze behavior follows a baseline, context-invariant motor pattern, providing evidence for a fundamental motor prior in the visual system.

Abstract: Humans perceive their visual environment by directing their eyes towards
relevant objects. The deployment of visual attention depends substantially on
the stimulus's properties, higher cognitive processes, and biases and
constraints of the visual system. Numerous models describe people's eye
movements depending on the performed task or the viewed content. However, there
is no universal, context-invariant model of human gaze behaviour. Here we show
that statistical regularities can be utilised to model human gaze behaviour
regardless of task, observer, and content. Using a context-agnostic eye
movement model, we were able to describe human gaze behaviour better than a
uniform random model in various viewing situations. Using a fixed transition
kernel, the model can describe gaze patterns during reading, visual search, and
scene perception, as well as for both adults and children. Thus, contrary to
current belief, human gaze patterns follow a baseline behaviour, making them
comparable across contexts. Since gaze behaviour is directly related to brain
structure, our results provide the first evidence for the existence of an
underlying, context-invariant motor prior in the human visual system.

</details>


### [527] [The Role of Excitatory Parvalbumin-positive Neurons in the Tectofugal Pathway of Pigeon (Columba livia) Hierarchical Visual Processing](https://arxiv.org/abs/2507.15486)
*Shan Lu,Xiaoteng Zhang,Yueyang Cang,Shihao Pan,Yanyan Peng,Xinwei Li,Shaoju Zeng,Yingjie Zhu,Li Shi*

Main category: q-bio.NC

TL;DR: This paper investigates the role of excitatory PV+ neurons in visual processing of pigeons, showing how these neurons contribute to rapid adaptation to moving targets in a columnar visual DVR system.


<details>
  <summary>Details</summary>
Motivation: To understand the neural mechanisms underlying hierarchical visual processing and the similarities between avian and mammalian visual systems, specifically focusing on excitatory PV+ neurons.

Method: Electrophysiological recordings, immunofluorescence staining, and HS-RNN modeling were used to study neuron activity and validate dynamic visual processing.

Result: Excitatory PV+ neurons originating from the Ei modulate MVL responses to moving visual stimuli, enabling rapid processing and adaptation.

Conclusion: Excitatory PV+ neurons play a crucial role in motion-related visual information processing in pigeons, showcasing functional convergence in avian and mammalian visual systems.

Abstract: The visual systems of birds and mammals exhibit remarkable organizational
similarities: the dorsal ventricular ridge (DVR) demonstrates a columnar
microcircuitry that parallels the cortical architecture observed in mammals.
However, the specific neuronal subtypes involved and their functional roles in
pigeon hierarchical visual processing remain unclear. This study investigates
the role of excitatory parvalbumin (PV+) neurons within the Ento-MVL
(entoallium-mesopallium venterolaterale) circuit of pigeons underlying
hierarchical moving target recognition. Electrophysiological recordings and
immunofluorescence staining reveal that excitatory PV+ neurons originating from
the entopallial internal (Ei) predominantly modulate MVL responses to varying
visual stimuli. Using a heterochronous-speed recurrent neural network (HS-RNN)
model, we further validated these dynamics, replicating the rapid adaptation of
the Ento-MVL circuit to moving visual targets. The findings suggest that the
fast-spiking and excitatory properties of PV+ neurons enable rapid processing
of motion-related information within the Ento-MVL circuit. Our results
elucidate the functional role of excitatory PV+ neurons in hierarchical
information processing under the columnar organization of the visual DVR and
underscore the convergent neural processing strategies shared by avian and
mammalian visual systems.

</details>


### [528] [Switching States: Heteroclinic Cycles as Organising Centres of Neuronal Dynamics](https://arxiv.org/abs/2507.15519)
*Kateryna Nechyporenko,Peter Ashwin,Krasimira Tsaneva-Atanasova*

Main category: q-bio.NC

TL;DR: Neuronal networks switch between active and inactive phases (up/down states) governed by universal principles tied to bifurcation structures, as shown across multiple network models.


<details>
  <summary>Details</summary>
Motivation: Understanding the mechanisms of neuronal state transitions is essential due to their role in perception, memory consolidation, and sensory processing.

Method: The study uses bifurcation analysis and simulations of canonical mean-field models like Wilson-Cowan, Tsodyks-Markram, and Jansen-Rit frameworks to explore state transitions.

Result: A novel bifurcation structure emerges robustly across diverse brain models. This structure results from a universal interplay between external input and synaptic connectivity.

Conclusion: The shared nonlinear input-output relationship suggests a fundamental mechanism for regulating transitions between brain states, independent of model-specific details.

Abstract: Neuronal networks alternate between high- and low-activity regimes, known as
up and down states. They also display rhythmic patterns essential for
perception, memory consolidation, and sensory processing. Despite their
importance, the principles behind such state transitions remain elusive. We
propose necessary conditions for the existence of a novel bifurcation structure
as a universal organising centre governing these transitions. Bifurcation
analysis and simulations of canonical mean-field network models, including
Wilson--Cowan, Tsodyks--Markram, and Jansen--Rit frameworks, show that this
bifurcation structure emerges robustly across models. We demonstrate that the
interplay between external input and (synaptic) connectivity converges onto
this shared mechanism, providing a fundamental principle for understanding how
diverse brain states arise and are regulated. Beyond phenomenological
mean-field models, we show that a shared mathematical structure of nonlinear
input-output relationships, rather than model-specific details, preserves the
organising centre across frameworks, revealing a general mechanism for dynamic
state transitions.

</details>


### [529] [Brain rhythms in cognition -- controversies and future directions](https://arxiv.org/abs/2507.15639)
*Anne Keitel,Christian Keitel,Mohsen Alavash,Karin Bakardjian,Christopher S. Y. Benwell,Sophie Bouton,Niko A. Busch,Antonio Criscuolo,Keith B. Doelling,Laura Dugue,Laetitia Grabot,Joachim Gross,Simon Hanslmayr,Laura-Isabelle Klatt,Daniel S. Kluger,Gemma Learmonth,Raquel E. London,Christina Lubinus,Andrea E. Martin,Jonas Obleser,Johanna M. Rimmele,Vincenzo Romei,Manuela Ruzzoli,Felix Siebenhuhner,Sophie Slaats,Eelke Spaak,Luca Tarasi,Gregor Thut,Jelena Trajkovic,Danying Wang,Malte Wostmann,Benedikt Zoefel,Satu Palva,Paul Sauseng,Sonja A. Kotz*

Main category: q-bio.NC

TL;DR: The paper provides a comprehensive review of brain rhythms and their role in human cognition, examining physiology, dynamics, and cognitive domains, and offers a roadmap for future research.


<details>
  <summary>Details</summary>
Motivation: To address unresolved key questions about the neurophysiological basis of human cognition through brain rhythms.

Method: The authors reviewed oscillatory mechanisms, physiological underpinnings, their interactions, and their relevance to cognitive domains, such as perception, attention, memory, and communication, highlighting current theories, debates, and gaps.

Result: The paper critically evaluates current understanding of brain rhythms, summarizes debates, and highlights underexplored areas in the study of cognition.

Conclusion: It provides a unified review to encourage a framework for understanding the role of brain rhythms in cognition and proposes future research directions.

Abstract: Brain rhythms seem central to understanding the neurophysiological basis of
human cognition. Yet, despite significant advances, key questions remain
unresolved. In this comprehensive position paper, we review the current state
of the art on oscillatory mechanisms and their cognitive relevance. The paper
critically examines physiological underpinnings, from phase-related dynamics
like cyclic excitability, to amplitude-based phenomena, such as gating by
inhibition, and their interactions, such as phase-amplitude coupling, as well
as frequency dynamics, like sampling mechanisms. We also critically evaluate
future research directions, including travelling waves and brain-body
interactions. We then provide an in-depth analysis of the role of brain rhythms
across cognitive domains, including perception, attention, memory, and
communication, emphasising ongoing debates and open questions in each area. By
summarising current theories and highlighting gaps, this position paper offers
a roadmap for future research, aimed at facilitating a unified framework of
rhythmic brain function underlying cognition.

</details>


### [530] [Ubiquity of Uncertainty in Neuron Systems](https://arxiv.org/abs/2507.15702)
*Brandon B. Le,Bennett Lamb,Luke Benfer,Sriharsha Sambangi,Nisal Geemal Vismith,Akshaj Jagarapu*

Main category: q-bio.NC

TL;DR: The paper investigates the concept of unpredictability in multistable systems of coupled neuronal maps, identifying it as a fundamental trait even in deterministic, low-dimensional models.


<details>
  <summary>Details</summary>
Motivation: To explore and understand the emergence of unpredictability in coupled neuronal systems and its implications for modeling brain function, cognition, and multistable systems.

Method: The authors employ basin classification, uncertainty exponent analysis, and basin entropy techniques on five discrete-time neuron models to validate the proposed chance synchronization mechanism.

Result: The study shows that unpredictability is inherent in low-dimensional, deterministic neuronal systems without requiring noise or high-dimensional dynamics.

Conclusion: Uncertainty in neuronal systems is a fundamental feature, which profoundly impacts the understanding of brain functionality and cognitive modeling, beyond the conventional assumptions of noise-induced chaos.

Abstract: We demonstrate that final-state uncertainty is ubiquitous in multistable
systems of coupled neuronal maps, meaning that predicting whether one such
system will eventually be chaotic or nonchaotic is often nearly impossible. We
propose a "chance synchronization" mechanism that governs the emergence of
unpredictability in neuron systems and support it by using basin
classification, uncertainty exponent, and basin entropy techniques to analyze
five simple discrete-time systems, each consisting of a different neuron model.
Our results illustrate that uncertainty in neuron systems is not just a product
of noise or high-dimensional complexity; it is also a fundamental property of
low-dimensional, deterministic models, which has profound implications for
understanding brain function, modeling cognition, and interpreting
unpredictability in general multistable systems.

</details>


### [531] [Dissociating model architectures from inference computations](https://arxiv.org/abs/2507.15776)
*Noor Sajid,Johan Medrano*

Main category: q-bio.NC

TL;DR: Parr et al., 2025 explores how different model architectures tackle non-Markovian sequence modelling, demonstrating that autoregressive models can mimic deep temporal computations through structured inference.


<details>
  <summary>Details</summary>
Motivation: The paper aims to resolve confusion between model architectures and inference computations in sequence prediction, specifically in handling non-Markovian characteristics.

Method: The authors use a transformer trained on next-token prediction and induce hierarchical temporal factorisation during iterative inference to compare autoregressive and deep temporal models.

Result: The study reveals autoregressive models can operate efficiently by mimicking deep temporal computations while maintaining predictive accuracy and reducing computational costs.

Conclusion: Inference processes can be decoupled from model architectures, proving that prediction construction and refinement are flexible.

Abstract: Parr et al., 2025 examines how auto-regressive and deep temporal models
differ in their treatment of non-Markovian sequence modelling. Building on
this, we highlight the need for dissociating model architectures, i.e., how the
predictive distribution factorises, from the computations invoked at inference.
We demonstrate that deep temporal computations are mimicked by autoregressive
models by structuring context access during iterative inference. Using a
transformer trained on next-token prediction, we show that inducing
hierarchical temporal factorisation during iterative inference maintains
predictive capacity while instantiating fewer computations. This emphasises
that processes for constructing and refining predictions are not necessarily
bound to their underlying model architectures.

</details>


### [532] [Biological detail and graph structure in network neuroscience](https://arxiv.org/abs/2507.15789)
*David Papo,Javier M. Buldú*

Main category: q-bio.NC

TL;DR: This paper discusses the role of networks in neuroscience, highlighting the simplifications typically used and exploring the potential impacts of increasing neurophysiological detail in modeling.


<details>
  <summary>Details</summary>
Motivation: To better understand how simplifying assumptions in network neuroscience modeling influence our comprehension of brain dynamics and function.

Method: The paper critically assesses common simplifications in neural and network modeling while exploring ways to incorporate more neurophysiological details and complex structures.

Result: It identifies gaps in existing models regarding approximations and suggests that incorporating higher physiological detail could deepen insights into brain behaviors.

Conclusion: Advancing network neuroscience by refining model structures and increasing physiological detail promises a more accurate understanding of brain phenomena and dynamics.

Abstract: Endowing brain anatomy, dynamics, and function with a network structure is
becoming standard in neuroscience. In its simplest form, a network is a
collection of units and relationships between them. The pattern of relations
among the units encodes numerous properties which have been shown to have a
profound effect on networked systems' dynamics and function. In an effort to
strike a balance between idealization and detail, network neuroscience studies
typically involve simplifying assumptions at both neural and network modeling
levels. However, the extent to which existing neural models depend on such
approximations is as yet poorly understood. Here, we discuss whether and how
increasing neurophysiological detail and generalizing the basic simple network
structure often adopted in network neuroscience may help improve our
understanding of brain phenomenology and function.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [533] [Statistical and Algorithmic Foundations of Reinforcement Learning](https://arxiv.org/abs/2507.14444)
*Yuejie Chi,Yuxin Chen,Yuting Wei*

Main category: stat.ML

TL;DR: This tutorial introduces developments in reinforcement learning (RL), focusing on how to improve sample and computational efficiencies using Markov Decision Processes across various RL scenarios and techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in RL posed by model complexity, nonconvexity, and data scarcity in emerging applications where data acquisition is costly or high-stakes.

Method: The paper employs Markov Decision Processes to explore RL within various scenarios (e.g., simulation-based, online, offline RL) and presents approaches like model-based, value-based, and policy optimization.

Result: The work offers theoretical and algorithmic insights into sample complexity, computational efficiency, and lower bounds in RL, emphasizing a non-asymptotic viewpoint.

Conclusion: The paper highlights new RL advancements and connects them to classical theories, aiding the understanding and enhancement of sample and computational efficacy in real-world RL applications.

Abstract: As a paradigm for sequential decision making in unknown environments,
reinforcement learning (RL) has received a flurry of attention in recent years.
However, the explosion of model complexity in emerging applications and the
presence of nonconvexity exacerbate the challenge of achieving efficient RL in
sample-starved situations, where data collection is expensive, time-consuming,
or even high-stakes (e.g., in clinical trials, autonomous systems, and online
advertising). How to understand and enhance the sample and computational
efficacies of RL algorithms is thus of great interest. In this tutorial, we aim
to introduce several important algorithmic and theoretical developments in RL,
highlighting the connections between new ideas and classical topics. Employing
Markov Decision Processes as the central mathematical model, we cover several
distinctive RL scenarios (i.e., RL with a simulator, online RL, offline RL,
robust RL, and RL with human feedback), and present several mainstream RL
approaches (i.e., model-based approach, value-based approach, and policy
optimization). Our discussions gravitate around the issues of sample
complexity, computational efficiency, as well as algorithm-dependent and
information-theoretic lower bounds from a non-asymptotic viewpoint.

</details>


### [534] [Diffusion Models for Time Series Forecasting: A Survey](https://arxiv.org/abs/2507.14507)
*Chen Su,Zhengzhou Cai,Yuanhe Tian,Zihong Zheng,Yan Song*

Main category: stat.ML

TL;DR: This survey reviews the use of diffusion models for time series forecasting, detailing their integration, current limitations, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To explore and systematize the application of diffusion models in time series forecasting tasks for advancing research in this area.

Method: The paper provides a systematic categorization of existing approaches, analyzes their mechanisms for integrating conditional information, and evaluates datasets and metrics used.

Result: The survey delivers a structured outline of diffusion models in TSF, highlighting their progress, limitations, and prospects.

Conclusion: Diffusion models show promise in TSF, but they face challenges that demand further research to unlock their full potential.

Abstract: Diffusion models, initially developed for image synthesis, demonstrate
remarkable generative capabilities. Recently, their application has expanded to
time series forecasting (TSF), yielding promising results. In this survey, we
firstly introduce the standard diffusion models and their prevalent variants,
explaining their adaptation to TSF tasks. We then provide a comprehensive
review of diffusion models for TSF, paying special attention to the sources of
conditional information and the mechanisms for integrating this conditioning
within the models. In analyzing existing approaches using diffusion models for
TSF, we provide a systematic categorization and a comprehensive summary of them
in this survey. Furthermore, we examine several foundational diffusion models
applied to TSF, alongside commonly used datasets and evaluation metrics.
Finally, we discuss current limitations in these approaches and potential
future research directions. Overall, this survey details recent progress and
future prospects for diffusion models in TSF, serving as a reference for
researchers in the field.

</details>


### [535] [Deep Learning-Based Survival Analysis with Copula-Based Activation Functions for Multivariate Response Prediction](https://arxiv.org/abs/2507.14641)
*Jong-Min Kim,Il Do Ha,Sangjin Kim*

Main category: stat.ML

TL;DR: This study applies deep learning, copula functions, and survival analysis to improve modeling and prediction of highly correlated, right-censored survival data.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of modeling nonlinear dependencies and right-censoring in multivariate survival data.

Method: Introduced copula-based activation functions (Clayton and Gumbel), integrated into CNN-LSTM architecture, tested with simulations and breast cancer data analysis.

Result: Improved prediction accuracy and pattern recognition in multivariate survival responses through a novel approach.

Conclusion: The proposed model effectively captures complex survival data patterns, enhancing predictive capability with practical modeling approaches.

Abstract: This research integrates deep learning, copula functions, and survival
analysis to effectively handle highly correlated and right-censored
multivariate survival data. It introduces copula-based activation functions
(Clayton, Gumbel, and their combinations) to model the nonlinear dependencies
inherent in such data. Through simulation studies and analysis of real breast
cancer data, our proposed CNN-LSTM with copula-based activation functions for
multivariate multi-types of survival responses enhances prediction accuracy by
explicitly addressing right-censored data and capturing complex patterns. The
model's performance is evaluated using Shewhart control charts, focusing on the
average run length (ARL).

</details>


### [536] [Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators](https://arxiv.org/abs/2507.14652)
*Ponkrshnan Thiagarajan,Tamer A. Zaki,Michael D. Shields*

Main category: stat.ML

TL;DR: The paper introduces a hybrid approach combining Variational Inference and Hamiltonian Monte Carlo to enhance posterior estimation accuracy and computational efficiency in Bayesian neural networks and neural operators.


<details>
  <summary>Details</summary>
Motivation: Hamiltonian Monte Carlo is accurate but computationally intensive for Bayesian neural networks due to high dimensionality and non-convex posterior distributions, leading to explorations of approximation techniques.

Method: A hybrid technique employs Variational Inference for initial training to identify critical parameters influencing uncertainty, followed by Hamiltonian Monte Carlo exclusively on this reduced subset.

Result: Their framework demonstrates efficient and accurate posterior inference for large networks with thousands of parameters and successfully models physical systems, including hypersonic flow scenarios.

Conclusion: The proposed approach provides a computationally viable and precise solution for uncertainty quantification in complex models, showing promising applications in physical simulation tasks.

Abstract: Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample
from the posterior distribution in Bayesian inference. However, HMC techniques
are computationally demanding for Bayesian neural networks due to the high
dimensionality of the network's parameter space and the non-convexity of their
posterior distributions. Therefore, various approximation techniques, such as
variational inference (VI) or stochastic gradient MCMC, are often employed to
infer the posterior distribution of the network parameters. Such approximations
introduce inaccuracies in the inferred distributions, resulting in unreliable
uncertainty estimates. In this work, we propose a hybrid approach that combines
inexpensive VI and accurate HMC methods to efficiently and accurately quantify
uncertainties in neural networks and neural operators. The proposed approach
leverages an initial VI training on the full network. We examine the influence
of individual parameters on the prediction uncertainty, which shows that a
large proportion of the parameters do not contribute substantially to
uncertainty in the network predictions. This information is then used to
significantly reduce the dimension of the parameter space, and HMC is performed
only for the subset of network parameters that strongly influence prediction
uncertainties. This yields a framework for accelerating the full batch HMC for
posterior inference in neural networks. We demonstrate the efficiency and
accuracy of the proposed framework on deep neural networks and operator
networks, showing that inference can be performed for large networks with tens
to hundreds of thousands of parameters. We show that this method can
effectively learn surrogates for complex physical systems by modeling the
operator that maps from upstream conditions to wall-pressure data on a cone in
hypersonic flow.

</details>


### [537] [When few labeled target data suffice: a theory of semi-supervised domain adaptation via fine-tuning from multiple adaptive starts](https://arxiv.org/abs/2507.14661)
*Wooseok Ha,Yuansi Chen*

Main category: stat.ML

TL;DR: This paper addresses Semi-supervised domain adaptation (SSDA) using limited labeled target data by exploiting source and unlabeled target data, introducing theoretical frameworks and new methods tailored for different source-target relationships.


<details>
  <summary>Details</summary>
Motivation: SSDA's practical importance is clear, but its theoretical foundations—especially under varied source-target distributional shifts—remain underexplored.

Method: The authors developed a framework based on structural causal models (SCMs) and introduced methods with fine-tuning strategies for specific source-target assumptions. They also propose the MASFT algorithm that selects the optimal model using a small hold-out target validation set.

Result: By extending unsupervised domain adaptation (UDA) methods to SSDA, minimax-optimal performance is demonstrated under different assumptions. The MASFT algorithm proves effective in simulations, achieving near-optimal predictive performance under varied distributional shifts.

Conclusion: This research advances SSDA theory and practice, providing frameworks and algorithms to boost performance while reducing reliance on labeled target data in diverse scenarios.

Abstract: Semi-supervised domain adaptation (SSDA) aims to achieve high predictive
performance in the target domain with limited labeled target data by exploiting
abundant source and unlabeled target data. Despite its significance in numerous
applications, theory on the effectiveness of SSDA remains largely unexplored,
particularly in scenarios involving various types of source-target
distributional shifts. In this work, we develop a theoretical framework based
on structural causal models (SCMs) which allows us to analyze and quantify the
performance of SSDA methods when labeled target data is limited. Within this
framework, we introduce three SSDA methods, each having a fine-tuning strategy
tailored to a distinct assumption about the source and target relationship.
Under each assumption, we demonstrate how extending an unsupervised domain
adaptation (UDA) method to SSDA can achieve minimax-optimal target performance
with limited target labels. When the relationship between source and target
data is only vaguely known -- a common practical concern -- we propose the
Multi Adaptive-Start Fine-Tuning (MASFT) algorithm, which fine-tunes UDA models
from multiple starting points and selects the best-performing one based on a
small hold-out target validation dataset. Combined with model selection
guarantees, MASFT achieves near-optimal target predictive performance across a
broad range of types of distributional shifts while significantly reducing the
need for labeled target data. We empirically validate the effectiveness of our
proposed methods through simulations.

</details>


### [538] [Uncertainty Quantification for Machine Learning-Based Prediction: A Polynomial Chaos Expansion Approach for Joint Model and Input Uncertainty Propagation](https://arxiv.org/abs/2507.14782)
*Xiaoping Du*

Main category: stat.ML

TL;DR: This paper introduces a framework using Polynomial Chaos Expansion to handle uncertainties in machine learning surrogate models, focusing on Gaussian Process regression.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inherent errors and uncertainties in machine learning predictions when replacing expensive simulation models in engineering design.

Method: A unified framework based on Polynomial Chaos Expansion transforms random inputs into standard space, allowing efficient uncertainty quantification and sensitivity analysis for ML surrogates.

Result: The proposed methodology efficiently calculates output uncertainties and identifies contributions from input variables and model uncertainties, improving reliability.

Conclusion: The framework supports trustworthy ML predictions in engineering applications by providing computational efficiency and interpretability for uncertainty quantification.

Abstract: Machine learning (ML) surrogate models are increasingly used in engineering
analysis and design to replace computationally expensive simulation models,
significantly reducing computational cost and accelerating decision-making
processes. However, ML predictions contain inherent errors, often estimated as
model uncertainty, which is coupled with variability in model inputs.
Accurately quantifying and propagating these combined uncertainties is
essential for generating reliable engineering predictions. This paper presents
a robust framework based on Polynomial Chaos Expansion (PCE) to handle joint
input and model uncertainty propagation. While the approach applies broadly to
general ML surrogates, we focus on Gaussian Process regression models, which
provide explicit predictive distributions for model uncertainty. By
transforming all random inputs into a unified standard space, a PCE surrogate
model is constructed, allowing efficient and accurate calculation of the mean
and standard deviation of the output. The proposed methodology also offers a
mechanism for global sensitivity analysis, enabling the accurate quantification
of the individual contributions of input variables and ML model uncertainty to
the overall output variability. This approach provides a computationally
efficient and interpretable framework for comprehensive uncertainty
quantification, supporting trustworthy ML predictions in downstream engineering
applications.

</details>


### [539] [Learning Nonlinear Causal Reductions to Explain Reinforcement Learning Policies](https://arxiv.org/abs/2507.14901)
*Armin Kekić,Jan Schneider,Dieter Büchler,Bernhard Schölkopf,Michel Besserve*

Main category: stat.ML

TL;DR: The paper introduces a causal framework for analyzing the behavioral patterns and failures in reinforcement learning (RL) policies.


<details>
  <summary>Details</summary>
Motivation: Reinforcement learning policies often exhibit complex behaviors that are difficult to understand, motivating a need for frameworks to explain when and why they succeed or fail.

Method: The authors propose a nonlinear Causal Model Reduction framework, introducing perturbations to policy actions and observing impacts on cumulative rewards to derive a high-level causal model that mirrors the original system's interventions.

Result: The framework uncovers meaningful causal relationships, biases, and failure modes in RL policies through experiments on synthetic models and practical tasks like pendulum control and robot table tennis.

Conclusion: The proposed method provides a robust approach to explaining RL behaviors, offering insights into their success and failure dynamics by identifying causal patterns.

Abstract: Why do reinforcement learning (RL) policies fail or succeed? This is a
challenging question due to the complex, high-dimensional nature of
agent-environment interactions. In this work, we take a causal perspective on
explaining the behavior of RL policies by viewing the states, actions, and
rewards as variables in a low-level causal model. We introduce random
perturbations to policy actions during execution and observe their effects on
the cumulative reward, learning a simplified high-level causal model that
explains these relationships. To this end, we develop a nonlinear Causal Model
Reduction framework that ensures approximate interventional consistency,
meaning the simplified high-level model responds to interventions in a similar
way as the original complex system. We prove that for a class of nonlinear
causal models, there exists a unique solution that achieves exact
interventional consistency, ensuring learned explanations reflect meaningful
causal patterns. Experiments on both synthetic causal models and practical RL
tasks-including pendulum control and robot table tennis-demonstrate that our
approach can uncover important behavioral patterns, biases, and failure modes
in trained RL policies.

</details>


### [540] [Learning under Latent Group Sparsity via Diffusion on Networks](https://arxiv.org/abs/2507.15097)
*Subhroshekhar Ghosh,Soumendu Sundar Mukherjee*

Main category: stat.ML

TL;DR: The paper presents a method for sparse learning that adapts to group structures in data without needing prior knowledge of group identities, using a heat-flow-based penalty influenced by network Laplacians.


<details>
  <summary>Details</summary>
Motivation: Group structures among explanatory variables are ubiquitous in machine learning but often their identities are unknown, creating demand for methods that can adapt to such structures without prior knowledge.

Method: The method introduces a penalty derived from heat-flow dynamics on a network, interpolating between lasso and group lasso penalties. It computes a network Laplacian from data without pre-clustering variables.

Result: The approach is supported by theoretical bounds on sample complexity and runtime, effectively managing group structure with diffusion times only logarithmic in problem dimensions.

Conclusion: The proposed technique provides an efficient way to adapt to group structures in machine learning tasks, opening avenues for broader geometric and stochastic applications in data analysis.

Abstract: Group or cluster structure on explanatory variables in machine learning
problems is a very general phenomenon, which has attracted broad interest from
practitioners and theoreticians alike. In this work we contribute an approach
to sparse learning under such group structure, that does not require prior
information on the group identities. Our paradigm is motivated by the Laplacian
geometry of an underlying network with a related community structure, and
proceeds by directly incorporating this into a penalty that is effectively
computed via a heat-flow-based local network dynamics. The proposed penalty
interpolates between the lasso and the group lasso penalties, the runtime of
the heat-flow dynamics being the interpolating parameter. As such it can
automatically default to lasso when the group structure reflected in the
Laplacian is weak. In fact, we demonstrate a data-driven procedure to construct
such a network based on the available data. Notably, we dispense with
computationally intensive pre-processing involving clustering of variables,
spectral or otherwise. Our technique is underpinned by rigorous theorems that
guarantee its effective performance and provide bounds on its sample
complexity. In particular, in a wide range of settings, it provably suffices to
run the diffusion for time that is only logarithmic in the problem dimensions.
We explore in detail the interfaces of our approach with key statistical
physics models in network science, such as the Gaussian Free Field and the
Stochastic Block Model. Our work raises the possibility of applying similar
diffusion-based techniques to classical learning tasks, exploiting the
interplay between geometric, dynamical and stochastic structures underlying the
data.

</details>


### [541] [Accelerated Bayesian Optimal Experimental Design via Conditional Density Estimation and Informative Data](https://arxiv.org/abs/2507.15235)
*Miao Huang,Hongqiao Wang,Kunyu Wu*

Main category: stat.ML

TL;DR: This study reformulates Bayesian experimental design for greater computational efficiency by transforming utility expectation and employing techniques like conditional density estimation.


<details>
  <summary>Details</summary>
Motivation: Improving the efficiency, reliability, and validity of experimental outcomes via an optimized Bayesian experimental design framework.

Method: Utilizes Bayes' theorem to transform utility expectation, applies conditional density estimation, and employs covariance for dataset selection.

Result: Validation through theoretical and practical applications shows enhanced experimental efficiency and decision-making under uncertainty.

Conclusion: The paper demonstrates the potential of the proposed methodology to address challenges in high-cost, low-efficiency experimental scenarios effectively within a Bayesian framework.

Abstract: The Design of Experiments (DOEs) is a fundamental scientific methodology that
provides researchers with systematic principles and techniques to enhance the
validity, reliability, and efficiency of experimental outcomes. In this study,
we explore optimal experimental design within a Bayesian framework, utilizing
Bayes' theorem to reformulate the utility expectation--originally expressed as
a nested double integral--into an independent double integral form,
significantly improving numerical efficiency. To further accelerate the
computation of the proposed utility expectation, conditional density estimation
is employed to approximate the ratio of two Gaussian random fields, while
covariance serves as a selection criterion to identify informative datasets
during model fitting and integral evaluation. In scenarios characterized by low
simulation efficiency and high costs of raw data acquisition, key challenges
such as surrogate modeling, failure probability estimation, and parameter
inference are systematically restructured within the Bayesian experimental
design framework. The effectiveness of the proposed methodology is validated
through both theoretical analysis and practical applications, demonstrating its
potential for enhancing experimental efficiency and decision-making under
uncertainty.

</details>


### [542] [Missing value imputation with adversarial random forests -- MissARF](https://arxiv.org/abs/2507.15681)
*Pegah Golchian,Jan Kapar,David S. Watson,Marvin N. Wright*

Main category: stat.ML

TL;DR: The paper introduces MissARF, a novel imputation method leveraging adversarial random forests for handling missing values in biostatistical analyses. It provides competitive performance in quality and speed.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of handling missing values in biostatistical analyses efficiently and effectively.

Method: The novel method uses adversarial random forests (ARF) for density estimation and conditional sampling to impute missing values.

Result: MissARF matches or exceeds the performance of state-of-the-art methods in imputation quality while offering faster runtime.

Conclusion: MissARF emerges as a fast, effective, and cost-efficient tool for both single and multiple imputation tasks.

Abstract: Handling missing values is a common challenge in biostatistical analyses,
typically addressed by imputation methods. We propose a novel, fast, and
easy-to-use imputation method called missing value imputation with adversarial
random forests (MissARF), based on generative machine learning, that provides
both single and multiple imputation. MissARF employs adversarial random forest
(ARF) for density estimation and data synthesis. To impute a missing value of
an observation, we condition on the non-missing values and sample from the
estimated conditional distribution generated by ARF. Our experiments
demonstrate that MissARF performs comparably to state-of-the-art single and
multiple imputation methods in terms of imputation quality and fast runtime
with no additional costs for multiple imputation.

</details>


### [543] [Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces](https://arxiv.org/abs/2507.15741)
*Gábor Lugosi,Marcos Matabuena*

Main category: stat.ML

TL;DR: This paper presents a framework for addressing uncertainty in metric-space regression models using conformal prediction and k-NN methods.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenges of uncertainty quantification in regression models within metric spaces, with scalable approaches suitable for diverse applications.

Method: Two methods are studied: a conformal prediction algorithm offering finite-sample guarantees and a local k-NN method tailored for heteroscedastic settings without conformal calibration.

Result: The proposed methods are shown to maintain consistency under minimal conditions and are practically applied to personalized medicine cases, including probability distributions and graph Laplacians.

Conclusion: The framework is versatile, supports any regression algorithm, ensures statistical rigor, and proves useful in complex, real-world scenarios.

Abstract: This paper introduces a framework for uncertainty quantification in
regression models defined in metric spaces. Leveraging a newly defined notion
of homoscedasticity, we develop a conformal prediction algorithm that offers
finite-sample coverage guarantees and fast convergence rates of the oracle
estimator. In heteroscedastic settings, we forgo these non-asymptotic
guarantees to gain statistical efficiency, proposing a local
$k$--nearest--neighbor method without conformal calibration that is adaptive to
the geometry of each particular nonlinear space. Both procedures work with any
regression algorithm and are scalable to large data sets, allowing
practitioners to plug in their preferred models and incorporate domain
expertise. We prove consistency for the proposed estimators under minimal
conditions. Finally, we demonstrate the practical utility of our approach in
personalized--medicine applications involving random response objects such as
probability distributions and graph Laplacians.

</details>


### [544] [Hypergraphs on high dimensional time series sets using signature transform](https://arxiv.org/abs/2507.15802)
*Rémi Vaucher,Paul Minchella*

Main category: stat.ML

TL;DR: The paper introduces a method to construct hypergraphs from multivariate time series using signature transforms and tests its robustness on synthetic datasets.


<details>
  <summary>Details</summary>
Motivation: Hypergraphs and Topological Data Analysis (TDA) are powerful tools for analyzing complex data structures, but existing methods primarily address single multivariate time series. This paper aims to extend these methods to collections of multivariate time series.

Method: The approach generalizes Chretien et al.'s method by leveraging signature transforms to introduce controlled randomness, which enhances the robustness in constructing hypergraphs.

Result: Synthetic datasets demonstrate the validity and promising outcomes of the proposed hypergraph construction methodology.

Conclusion: Extending hypergraph construction to collections of multivariate time series effectively enhances robustness and broadens the applicability of TDA methods in complex data analysis.

Abstract: In recent decades, hypergraphs and their analysis through Topological Data
Analysis (TDA) have emerged as powerful tools for understanding complex data
structures. Various methods have been developed to construct hypergraphs --
referred to as simplicial complexes in the TDA framework -- over datasets,
enabling the formation of edges between more than two vertices. This paper
addresses the challenge of constructing hypergraphs from collections of
multivariate time series. While prior work has focused on the case of a single
multivariate time series, we extend this framework to handle collections of
such time series. Our approach generalizes the method proposed in Chretien and
al. by leveraging the properties of signature transforms to introduce
controlled randomness, thereby enhancing the robustness of the construction
process. We validate our method on synthetic datasets and present promising
results.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [545] [Learning Null Geodesics for Gravitational Lensing Rendering in General Relativity](https://arxiv.org/abs/2507.15775)
*Mingyuan Sun,Zheng Fang,Jiaxu Wang,Kunyi Zhang,Qiang Zhang,Renjing Xu*

Main category: gr-qc

TL;DR: The paper introduces GravLensX, a neural network-based method to efficiently simulate black hole gravitational lensing effects with up to 15x reduced computational time.


<details>
  <summary>Details</summary>
Motivation: Traditional rendering techniques for black hole visualizations are computationally expensive, creating a need for more efficient methods.

Method: The authors train neural networks to model spacetime around black holes and use these models to compute light ray paths affected by gravitational lensing.

Result: GravLensX accurately renders black holes with optically thin accretion disks, achieving a 15x reduction in computational time compared to traditional methods.

Conclusion: Neural networks can serve as an efficient and scalable alternative for visualizing complex astrophysical phenomena, opening new possibilities for astronomical rendering.

Abstract: We present GravLensX, an innovative method for rendering black holes with
gravitational lensing effects using neural networks. The methodology involves
training neural networks to fit the spacetime around black holes and then
employing these trained models to generate the path of light rays affected by
gravitational lensing. This enables efficient and scalable simulations of black
holes with optically thin accretion disks, significantly decreasing the time
required for rendering compared to traditional methods. We validate our
approach through extensive rendering of multiple black hole systems with
superposed Kerr metric, demonstrating its capability to produce accurate
visualizations with significantly $15\times$ reduced computational time. Our
findings suggest that neural networks offer a promising alternative for
rendering complex astrophysical phenomena, potentially paving a new path to
astronomical visualization.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [546] [Enhancements to P4TG: Histogram-Based RTT Monitoring in the Data Plane](https://arxiv.org/abs/2507.15382)
*Fabian Ihle,Etienne Zink,Michael Menth*

Main category: cs.NI

TL;DR: The paper introduces a histogram-based RTT measurement feature for P4TG to improve accuracy, utilizing a range-to-prefix conversion algorithm for efficient hardware implementation.


<details>
  <summary>Details</summary>
Motivation: To address the reduced accuracy of round-trip time (RTT) measurements in existing P4TG implementations.

Method: A histogram-based RTT measurement feature that overcomes range-matching challenges using a range-to-prefix conversion algorithm for TCAM hardware.

Result: The method allows accurate RTT measurements without sampling and demonstrates efficient measurement through evaluation.

Conclusion: Histogram-based RTT analysis is feasible and enables precise measurements at line rate with applicability validated by comparison with theoretical RTT distribution.

Abstract: Modern traffic generators are essential tools for evaluating the performance
of network environments. P4TG is a P4-based traffic generator implemented for
Intel Tofino switches that offers high-speed packet generation with
fine-grained measurement capabilities. However, P4TG samples time-based metrics
such as the round-trip time (RTT) in the data plane and collects them at the
controller. This leads to a reduced accuracy. In this paper, we introduce a
histogram-based RTT measurement feature for P4TG. It enables accurate analysis
at line rate without sampling. Generally, histogram bins are modeled as ranges,
and values are matched to a bin. Efficient packet matching in hardware is
typically achieved using ternary content addressable memory (TCAM). However,
representing range matching rules in TCAM poses a challenge. Therefore, we
implemented a range-to-prefix conversion algorithm that models range matching
with multiple ternary entries. This paper describes the data plane
implementation and runtime configuration of RTT histograms in P4TG. Further, we
discuss the efficiency of the ternary decomposition. Our evaluation
demonstrates the applicability of the histogram-based RTT analysis by comparing
the measured values with a configured theoretical distribution of RTTs.

</details>


### [547] [A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction](https://arxiv.org/abs/2507.14186)
*Xiaojie Li,Zhijie Cai,Nan Qi,Chao Dong,Guangxu Zhu,Haixia Ma,Qihui Wu,Shi Jin*

Main category: cs.NI

TL;DR: The paper proposes a data-driven framework to improve low-altitude network coverage prediction despite challenges from proprietary antenna beam patterns and sparse sample data.


<details>
  <summary>Details</summary>
Motivation: The study addresses the growing demand for reliable low-altitude network coverage prediction for aerial corridor design, which is impeded by insufficient proprietary antenna beam data and cost-prohibitive sampling.

Method: The researchers introduced a dual strategy leveraging expert knowledge-based feature compression and disentangled representation learning to tackle data scarcity and imbalanced feature sampling.

Result: The proposed framework reduced prediction error by 7% compared to the best baseline and demonstrated reliable performance with practical Mean Absolute Error (MAE) levels of 5 dB in real-world validation.

Conclusion: The strategy effectively improves prediction accuracy, generalizability, and reliability for low-altitude network coverage in practical settings.

Abstract: The expansion of the low-altitude economy has underscored the significance of
Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors.
While accurate LANC forecasting hinges on the antenna beam patterns of Base
Stations (BSs), these patterns are typically proprietary and not readily
accessible. Operational parameters of BSs, which inherently contain beam
information, offer an opportunity for data-driven low-altitude coverage
prediction. However, collecting extensive low-altitude road test data is
cost-prohibitive, often yielding only sparse samples per BS. This scarcity
results in two primary challenges: imbalanced feature sampling due to limited
variability in high-dimensional operational parameters against the backdrop of
substantial changes in low-dimensional sampling locations, and diminished
generalizability stemming from insufficient data samples. To overcome these
obstacles, we introduce a dual strategy comprising expert knowledge-based
feature compression and disentangled representation learning. The former
reduces feature space complexity by leveraging communications expertise, while
the latter enhances model generalizability through the integration of
propagation models and distinct subnetworks that capture and aggregate the
semantic representations of latent features. Experimental evaluation confirms
the efficacy of our framework, yielding a 7% reduction in error compared to the
best baseline algorithm. Real-network validations further attest to its
reliability, achieving practical prediction accuracy with MAE errors at the 5dB
level.

</details>


### [548] [From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks](https://arxiv.org/abs/2507.14188)
*Sebastian Barros Elgueta*

Main category: cs.NI

TL;DR: The paper proposes a fully orbital mobile network system using LEO satellites, aiming to deliver high-speed urban-grade service without relying on terrestrial infrastructure.


<details>
  <summary>Details</summary>
Motivation: Current satellite-based mobile systems are rural-focused and rely on Earth-based cores. The paper aims to eliminate dependence on terrestrial infrastructure for urban-grade mobile networks.

Method: The system integrates phased array antennas with space-based 5G core functions and laser-based inter-satellite communication. Simulations analyze feasibility for urban conditions.

Result: Simulations indicate sustained high-speed throughput for rooftop and line-of-sight users, with potential for street-level access using relay mechanisms.

Conclusion: Fully orbital mobile networks are technically feasible with engineering challenges, paving the way for autonomous non-terrestrial mobile overlays within 15 years.

Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard
smartphones, using unmodified 3GPP protocols, connected directly to low Earth
orbit (LEO) satellites. This first wave of direct-to-device (D2D)
demonstrations validated the physical feasibility of satellite-based mobile
access. However, these systems remain fallback-grade--rural-only,
bandwidth-limited, and fully dependent on Earth-based mobile cores for
identity, session, and policy control. This paper asks a more ambitious
question: Can a complete mobile network, including radio access, core
functions, traffic routing, and content delivery, operate entirely from orbit?
And can it deliver sustained, urban-grade service in the world's densest
cities? We present the first end-to-end system architecture for a fully orbital
telco, integrating electronically steered phased arrays with 1000-beam
capacity, space-based deployment of 5G core functions (UPF, AMF), and
inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam
capacity, and link budgets under dense urban conditions, accounting for path
loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight
users can sustain 64-QAM throughput, while street-level access is feasible with
relay or assisted beam modes. The paper outlines the remaining constraints,
power, thermal dissipation, compute radiation hardening, and regulatory models,
and demonstrates that these are engineering bottlenecks, not physical limits.
Finally, we propose a staged 15-year roadmap from today's fallback D2D systems
to autonomous orbital overlays delivering 50-100 Mbps to handhelds in
megacities, with zero reliance on terrestrial infrastructure.

</details>


### [549] [PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence](https://arxiv.org/abs/2507.14211)
*Federico Mason,Tommaso Zugno,Matteo Drago,Marco Giordani,Mate Boban,Michele Zorzi*

Main category: cs.NI

TL;DR: PRATA, a new framework, leverages AI for predictive Quality of Service (PQoS) in teleoperated driving applications, achieving notable improvements in system performance through RL-based optimization.


<details>
  <summary>Details</summary>
Motivation: Teleoperated driving faces stringent latency and reliability requirements, which can benefit from anticipative QoS adjustments to prevent performance degradation.

Method: The PRATA framework integrates a 5G RAN simulation stack, automotive data generation, and an AI module to optimize PQoS. RL modeling (RAN-AI) was used to dynamically manage data segmentation amid channel and resource challenges.

Result: Using PRATA and the RL-based RAN-AI model, the study demonstrates a nearly two-fold improvement in performance, optimizing the balance between QoS and QoE during driving simulations.

Conclusion: PRATA demonstrates the potential of integrating AI-driven RL for PQoS in teleoperated driving, identifying cost and state space considerations for optimizing learning models.

Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS
changes, e.g., in wireless networks, and trigger appropriate countermeasures to
avoid performance degradation. Hence, PQoS is extremely useful for automotive
applications such as teleoperated driving, which poses strict constraints in
terms of latency and reliability. A promising tool for PQoS is given by
Reinforcement Learning (RL), a methodology that enables the design of
decision-making strategies for stochastic optimization. In this manuscript, we
present PRATA, a new simulation framework to enable PRedictive QoS based on AI
for Teleoperated driving Applications. PRATA consists of a modular pipeline
that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access
Network (RAN), (ii) a tool for generating automotive data, and (iii) an
Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its
utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the
segmentation level of teleoperated driving data in the event of resource
saturation or channel degradation. Hence, we show that the RAN-AI entity
efficiently balances the trade-off between QoS and Quality of Experience (QoE)
that characterize teleoperated driving applications, almost doubling the system
performance compared to baseline approaches. In addition, by varying the
learning settings of the RAN-AI entity, we investigate the impact of the state
space and the relative cost of acquiring network data that are necessary for
the implementation of RL.

</details>


### [550] [Intent-Based Network for RAN Management with Large Language Models](https://arxiv.org/abs/2507.14230)
*Fransiscus Asisi Bimo,Maria Amparo Canaveras Galdon,Chun-Kai Lai,Ray-Guang Cheng,Edwin K. P. Chong*

Main category: cs.NI

TL;DR: The paper proposes an intent-based network management system for RANs using Large Language Models (LLMs) to improve automation and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: To handle the rising complexity in managing wireless networks by introducing advanced automation.

Method: Integrates LLMs in an agentic system for intent translation, reasoning, and precise RAN configuration via structured prompt engineering and a closed-loop mechanism.

Result: Demonstrated improvement in RAN energy efficiency by dynamically optimizing parameters using an LLM-based automation system.

Conclusion: LLM-integrated systems can enable adaptive and efficient resource management in Radio Access Networks, showcasing robust and intelligent automation potential.

Abstract: Advanced intelligent automation becomes an important feature to deal with the
increased complexity in managing wireless networks. This paper proposes a novel
automation approach of intent-based network for Radio Access Networks (RANs)
management by leveraging Large Language Models (LLMs). The proposed method
enhances intent translation, autonomously interpreting high-level objectives,
reasoning over complex network states, and generating precise configurations of
the RAN by integrating LLMs within an agentic architecture. We propose a
structured prompt engineering technique and demonstrate that the network can
automatically improve its energy efficiency by dynamically optimizing critical
RAN parameters through a closed-loop mechanism. It showcases the potential to
enable robust resource management in RAN by adapting strategies based on
real-time feedback via LLM-orchestrated agentic systems.

</details>


### [551] [Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts](https://arxiv.org/abs/2507.14263)
*Ramesh Raskar,Pradyumna Chari,John Zinky,Mahesh Lambe,Jared James Grogan,Sichao Wang,Rajesh Ranjan,Rekha Singhal,Shailja Gupta,Robert Lincourt,Raghu Bala,Aditi Joshi,Abhishek Singh,Ayush Chopra,Dimitris Stripelis,Bhuwan B,Sumit Kumar,Maria Gorskikh*

Main category: cs.NI

TL;DR: The paper introduces "NANDA index architecture" to handle identification, discovery, and authentication for billions of AI agents on the Internet using innovative cryptographic methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is the anticipated overwhelming growth of autonomous AI agents on the Internet, which will challenge current DNS-based systems for identity and discovery.

Method: The authors propose the NANDA index architecture with cryptographic verifiability, schema-validated assertions, and a CRDT-based update protocol to ensure secure, scalable, and efficient agent discoverability and interaction.

Result: The architecture supports rapid global resolution, privacy-preserving access, multi-endpoint routing, and trustworthy authentication, enabling efficient interactions between agents and scalability.

Conclusion: The proposed NANDA index serves as a lightweight, scalable foundation for secure AI agent collaboration, retaining compatibility with the existing web infrastructure.

Abstract: The Internet is poised to host billions to trillions of autonomous AI agents
that negotiate, delegate, and migrate in milliseconds and workloads that will
strain DNS-centred identity and discovery. In this paper, we describe the NANDA
index architecture, which we envision as a means for discoverability,
identifiability and authentication in the internet of AI agents. We present an
architecture where a minimal lean index resolves to dynamic, cryptographically
verifiable AgentFacts that supports multi-endpoint routing, load balancing,
privacy-preserving access, and credentialed capability assertions. Our
architecture design delivers five concrete guarantees: (1) A quilt-like index
proposal that supports both NANDA-native agents as well as third party agents
being discoverable via the index, (2) rapid global resolution for newly spawned
AI agents, (3) sub-second revocation and key rotation, (4) schema-validated
capability assertions, and (5) privacy-preserving discovery across
organisational boundaries via verifiable, least-disclosure queries. We
formalize the AgentFacts schema, specify a CRDT-based update protocol, and
prototype adaptive resolvers. The result is a lightweight, horizontally
scalable foundation that unlocks secure, trust-aware collaboration for the next
generation of the Internet of AI agents, without abandoning existing web
infrastructure.

</details>


### [552] [On Splitting Lightweight Semantic Image Segmentation for Wireless Communications](https://arxiv.org/abs/2507.14199)
*Ebrahim Abu-Helalah,Jordi Serra,Jordi Perez-Romero*

Main category: cs.NI

TL;DR: The paper introduces a method to balance computation and bandwidth for image segmentation using semantic communication by splitting tasks between transmitter and receiver.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in bandwidth and computational requirements for semantic image segmentation in constrained environments.

Method: Proposing a split-processing approach where the transmitter handles part of the segmentation and the receiver processes the rest.

Result: Achieved significant reductions: 72% in bit rate and over 19% in transmitter computational load.

Conclusion: Split-processing is an effective technique for improving resource efficiency in semantic communication, especially for 6G systems.

Abstract: Semantic communication represents a promising technique towards reducing
communication costs, especially when dealing with image segmentation, but it
still lacks a balance between computational efficiency and bandwidth
requirements while maintaining high image segmentation accuracy, particularly
in resource-limited environments and changing channel conditions. On the other
hand, the more complex and larger semantic image segmentation models become,
the more stressed the devices are when processing data. This paper proposes a
novel approach to implementing semantic communication based on splitting the
semantic image segmentation process between a resource constrained transmitter
and the receiver. This allows saving bandwidth by reducing the transmitted data
while maintaining the accuracy of the semantic image segmentation.
Additionally, it reduces the computational requirements at the resource
constrained transmitter compared to doing all the semantic image segmentation
in the transmitter. The proposed approach is evaluated by means of
simulation-based experiments in terms of different metrics such as
computational resource usage, required bit rate and segmentation accuracy. The
results when comparing the proposal with the full semantic image segmentation
in the transmitter show that up to 72% of the bit rate was reduced in the
transmission process. In addition, the computational load of the transmitter is
reduced by more than 19%. This reflects the interest of this technique for its
application in communication systems, particularly in the upcoming 6G systems.

</details>


### [553] [Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches](https://arxiv.org/abs/2507.14633)
*Xiaozheng Gao,Yichen Wang,Bosen Liu,Xiao Zhou,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Dong In Kim,Abbas Jamalipour,Chau Yuen,Jianping An,Kai Yang*

Main category: cs.NI

TL;DR: The paper surveys the use of generative AI and large language models to enable intelligent artificial agents in integrated satellite-aerial-terrestrial networks.


<details>
  <summary>Details</summary>
Motivation: The need for intelligent systems that can reliably operate in dynamic and mission-critical environments within satellite-aerial-terrestrial networks.

Method: Systematic review of generative AI models, discussing architecture, challenges, and their application in communication, security, and intelligent satellite tasks within SLAETNs.

Result: Comparative analysis of generative model categories (e.g., GANs, VAEs, LLMs) and their trade-offs in SLAETNs, showcasing their utility in enhancing capabilities.

Conclusion: Key future directions were outlined for developing scalable and trustworthy generative agents in integrated next-generation networks.

Abstract: The development of satellite-augmented low-altitude economy and terrestrial
networks (SLAETNs) demands intelligent and autonomous systems that can operate
reliably across heterogeneous, dynamic, and mission-critical environments. To
address these challenges, this survey focuses on enabling agentic artificial
intelligence (AI), that is, artificial agents capable of perceiving, reasoning,
and acting, through generative AI (GAI) and large language models (LLMs). We
begin by introducing the architecture and characteristics of SLAETNs, and
analyzing the challenges that arise in integrating satellite, aerial, and
terrestrial components. Then, we present a model-driven foundation by
systematically reviewing five major categories of generative models:
variational autoencoders (VAEs), generative adversarial networks (GANs),
generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.
Moreover, we provide a comparative analysis to highlight their generative
mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on
this foundation, we examine how these models empower agentic functions across
three domains: communication enhancement, security and privacy protection, and
intelligent satellite tasks. Finally, we outline key future directions for
building scalable, adaptive, and trustworthy generative agents in SLAETNs. This
survey aims to provide a unified understanding and actionable reference for
advancing agentic AI in next-generation integrated networks.

</details>


### [554] [Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness](https://arxiv.org/abs/2507.15145)
*Thai T. Vu,John Le*

Main category: cs.NI

TL;DR: The paper introduces a communication-efficient framework for cooperative edge AI systems, optimizing rare-event detection through multi-device inference and fairness constraints.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in single-device inference for edge AI systems, especially in rare-event detection, while improving system fairness and resource utilization.

Method: The authors propose a joint optimization framework that combines dual-threshold early-exit strategies with distributed multi-device inference, solved using alternating optimization and Benders decomposition.

Result: Experimental evaluations demonstrate that the framework improves system-wide performance and fairness in resource allocation compared to single-device-based methods.

Conclusion: The framework is effective in enhancing cooperative edge AI system performance, proving its practicality in distributed settings with multiple user devices and fairness considerations.

Abstract: This paper proposes a communication-efficient, event-triggered inference
framework for cooperative edge AI systems comprising multiple user devices and
edge servers. Building upon dual-threshold early-exit strategies for rare-event
detection, the proposed approach extends classical single-device inference to a
distributed, multi-device setting while incorporating proportional fairness
constraints across users. A joint optimization framework is formulated to
maximize classification utility under communication, energy, and fairness
constraints. To solve the resulting problem efficiently, we exploit the
monotonicity of the utility function with respect to the confidence thresholds
and apply alternating optimization with Benders decomposition. Experimental
results show that the proposed framework significantly enhances system-wide
performance and fairness in resource allocation compared to single-device
baselines.

</details>


### [555] [User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks](https://arxiv.org/abs/2507.15254)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: The paper tackles the challenge of synchronizing extended reality (XR) content in human-to-machine (H2M) collaborations over communication networks in real-time. A system to predict human head movements and dynamically allocate network bandwidth is proposed.


<details>
  <summary>Details</summary>
Motivation: The evolution of mobile systems and fixed wireless networks is driven by the requirement to support high-bandwidth, low-latency services for applications like Industry 4.0/5.0 and Society 5.0. A key challenge is ensuring smooth and immersive XR experiences without cyber-sickness, especially over large geographic spans.

Method: The authors propose a novel H2M collaboration scheme where human head movements are predicted using bidirectional long short-term memory networks. This is combined with a dynamic bandwidth allocation strategy based on the predicted XR frame data to optimize network resource utilization.

Result: Extensive simulations demonstrate that the proposed system meets XR latency and jitter requirements with substantially reduced bandwidth usage compared to existing solutions in enterprise networks like Fiber-To-The-Room-Business.

Conclusion: The method achieves better network efficiency and resource utilization while supporting immersive XR experiences, highlighting its advantage over state-of-the-art approaches.

Abstract: The evolution towards future generation of mobile systems and fixed wireless
networks is primarily driven by the urgency to support high-bandwidth and
low-latency services across various vertical sectors. This endeavor is fueled
by smartphones as well as technologies like industrial internet of things,
extended reality (XR), and human-to-machine (H2M) collaborations for fostering
industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To
ensure an ideal immersive experience and avoid cyber-sickness for users in all
the aforementioned usage scenarios, it is typically challenging to synchronize
XR content from a remote machine to a human collaborator according to their
head movements across a large geographic span in real-time over communication
networks. Thus, we propose a novel H2M collaboration scheme where the human's
head movements are predicted ahead with highly accurate models like
bidirectional long short-term memory networks to orient the machine's camera in
advance. We validate that XR frame size varies in accordance with the human's
head movements and predict the corresponding bandwidth requirements from the
machine's camera to propose a human-machine coordinated dynamic bandwidth
allocation (HMC-DBA) scheme. Through extensive simulations, we show that
end-to-end latency and jitter requirements of XR frames are satisfied with much
lower bandwidth consumption over enterprise networks like
Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in
network resource utilization is achieved by employing our proposed HMC-DBA over
state-of-the-art schemes.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [556] [Complex Dynamics in Psychological Data: Mapping Individual Symptom Trajectories to Group-Level Patterns](https://arxiv.org/abs/2507.14161)
*Eleonora Vitanza,Pietro DeLellis,Chiara Mocenni,Manuel Ruiz Marin*

Main category: stat.AP

TL;DR: The paper integrates causal inference, complexity measures, and machine learning to examine if symptom trajectories reveal diagnostic patterns, achieving 91% accuracy in classifying mental disorders.


<details>
  <summary>Details</summary>
Motivation: The study aims to identify diagnostic patterns from individual symptom trajectories to improve personalized therapy and clinical psychology methodologies.

Method: Using the PCMCI+ algorithm for causal networks, temporal complexity measures were calculated and combined with machine learning for classification.

Result: The method achieved 91% accuracy in classifying mental disorders and provided insights into disorder-specific causal mechanisms.

Conclusion: Integrating causal modeling and temporal complexity improves diagnostic differentiation, advancing personalized clinical psychology and research methodologies.

Abstract: This study integrates causal inference, graph analysis, temporal complexity
measures, and machine learning to examine whether individual symptom
trajectories can reveal meaningful diagnostic patterns. Testing on a
longitudinal dataset of N=45 individuals affected by General Anxiety Disorder
(GAD) and/or Major Depressive Disorder (MDD) derived from Fisher et al. 2017,
we propose a novel pipeline for the analysis of the temporal dynamics of
psychopathological symptoms. First, we employ the PCMCI+ algorithm with
nonparametric independence test to determine the causal network of nonlinear
dependencies between symptoms in individuals with different mental disorders.
We found that the PCMCI+ effectively highlights the individual peculiarities of
each symptom network, which could be leveraged towards personalized therapies.
At the same time, aggregating the networks by diagnosis sheds light to
disorder-specific causal mechanisms, in agreement with previous
psychopathological literature. Then, we enrich the dataset by computing
complexity-based measures (e.g. entropy, fractal dimension, recurrence) from
the symptom time series, and feed it to a suitably selected machine learning
algorithm to aid the diagnosis of each individual. The new dataset yields 91%
accuracy in the classification of the symptom dynamics, proving to be an
effective diagnostic support tool. Overall, these findings highlight how
integrating causal modeling and temporal complexity can enhance diagnostic
differentiation, offering a principled, data-driven foundation for both
personalized assessment in clinical psychology and structural advances in
psychological research.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [557] [All-atom inverse protein folding through discrete flow matching](https://arxiv.org/abs/2507.14156)
*Kai Yi,Kiarash Jamali,Sjors H. W. Scheres*

Main category: q-bio.BM

TL;DR: The paper introduces ADFLIP, a novel generative model for designing protein sequences conditioned on all-atom structural contexts, particularly excelling in handling dynamic complexes and multi-state structures.


<details>
  <summary>Details</summary>
Motivation: Traditional inverse protein folding methods struggle with complexes containing non-protein components and multi-structural states, necessitating improvements for accurate protein design.

Method: The authors developed ADFLIP, a generative model based on discrete flow-matching, which uses progressive amino acid side chain incorporation and enables dynamic protein complex design through ensemble sampling.

Result: ADFLIP demonstrated state-of-the-art performance in benchmarks involving complex protein structures, including dynamic cases, and outperformed prior methods in single-structure and multi-structure tasks.

Conclusion: ADFLIP advances the field of inverse protein folding by effectively addressing challenges in designing proteins with complex and dynamic structural properties, paving the way for high-precision protein engineering.

Abstract: The recent breakthrough of AlphaFold3 in modeling complex biomolecular
interactions, including those between proteins and ligands, nucleotides, or
metal ions, creates new opportunities for protein design. In so-called inverse
protein folding, the objective is to find a sequence of amino acids that adopts
a target protein structure. Many inverse folding methods struggle to predict
sequences for complexes that contain non-protein components, and perform poorly
with complexes that adopt multiple structural states. To address these
challenges, we present ADFLIP (All-atom Discrete FLow matching Inverse Protein
folding), a generative model based on discrete flow-matching for designing
protein sequences conditioned on all-atom structural contexts. ADFLIP
progressively incorporates predicted amino acid side chains as structural
context during sequence generation and enables the design of dynamic protein
complexes through ensemble sampling across multiple structural states.
Furthermore, ADFLIP implements training-free classifier guidance sampling,
which allows the incorporation of arbitrary pre-trained models to optimise the
designed sequence for desired protein properties. We evaluated the performance
of ADFLIP on protein complexes with small-molecule ligands, nucleotides, or
metal ions, including dynamic complexes for which structure ensembles were
determined by nuclear magnetic resonance (NMR). Our model achieves
state-of-the-art performance in single-structure and multi-structure inverse
folding tasks, demonstrating excellent potential for all-atom protein design.
The code is available at https://github.com/ykiiiiii/ADFLIP.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [558] [Geophysics-informed neural network for model-based seismic inversion using surrogate point spread functions](https://arxiv.org/abs/2507.14140)
*Marcus Saraiva,Ana Muller,Alexandre Maul*

Main category: physics.geo-ph

TL;DR: This paper introduces a Geophysics-Informed Neural Network (GINN) that improves seismic inversion by addressing limitations of traditional methods, combining deep learning with seismic modeling.


<details>
  <summary>Details</summary>
Motivation: Traditional seismic inversion methods face constraints such as reliance on stationary wavelets and unrealistic lateral resolution.

Method: A Deep Convolutional Neural Network (DCNN) is designed to estimate acoustic impedance and Point Spread Functions, incorporating geophysical consistency and trained on synthetic data using a 2D UNet architecture.

Result: The GINN achieved high-resolution impedance maps and realistic Point Spread Functions, demonstrating alignment with geological features and reducing noise compared to traditional methods.

Conclusion: The approach successfully integrates deep learning with geophysics to overcome traditional limitations, and future work will focus on validation using real seismic data.

Abstract: Model-based seismic inversion is a key technique in reservoir
characterization, but traditional methods face significant limitations, such as
relying on 1D average stationary wavelets and assuming an unrealistic lateral
resolution. To address these challenges, we propose a Geophysics-Informed
Neural Network (GINN) that integrates deep learning with seismic modeling. This
novel approach employs a Deep Convolutional Neural Network (DCNN) to
simultaneously estimate Point Spread Functions (PSFs) and acoustic impedance
(IP). PSFs are divided into zero-phase and residual components to ensure
geophysical consistency and to capture fine details. We used synthetic data
from the SEAM Phase I Earth Model to train the GINN for 100 epochs
(approximately 20 minutes) using a 2D UNet architecture. The network's inputs
include positional features and a low-frequency impedance (LF-IP) model. A
self-supervised loss function combining Mean Squared Error (MSE) and Structural
Similarity Index Measure (SSIM) was employed to ensure accurate results. The
GINN demonstrated its ability to generate high-resolution IP and realistic
PSFs, aligning with expected geological features. Unlike traditional 1D
wavelets, the GINN produces PSFs with limited lateral resolution, reducing
noise and improving accuracy. Future work will aim to refine the training
process and validate the methodology with real seismic data.

</details>


### [559] [Integrating Newton's Laws with deep learning for enhanced physics-informed compound flood modelling](https://arxiv.org/abs/2507.15021)
*Soheil Radfar,Faezeh Maghsoodifar,Hamed Moftakhari,Hamid Moradkhani*

Main category: physics.geo-ph

TL;DR: The paper introduces ALPINE, a physics-informed neural network (PINN) for accurate and physically consistent compound flood modeling, outperforming traditional neural networks.


<details>
  <summary>Details</summary>
Motivation: Traditional hydrodynamic models are accurate but computationally intensive, and alternative machine learning models lack physical consistency during extreme events. This study aims to create a method that balances physical realism and computational efficiency for compound flood modeling.

Method: The study develops ALPINE, a physics-informed neural network framework that enforces shallow water dynamics through mass conservation and momentum equations. It uses a convolutional encoder-decoder with ConvLSTM for temporal processing and trains using a composite loss function.

Result: ALPINE significantly reduces prediction errors and improves skill metrics for water surface elevation and velocity components, particularly during peak storm intensity and multi-driver flood scenarios.

Conclusion: ALPINE serves as a physically consistent and efficient emulator for compound flood forecasting and risk analysis, crucial for coastal emergency management.

Abstract: Coastal communities increasingly face compound floods, where multiple drivers
like storm surge, high tide, heavy rainfall, and river discharge occur together
or in sequence to produce impacts far greater than any single driver alone.
Traditional hydrodynamic models can provide accurate physics-based simulations
but require substantial computational resources for real-time applications or
risk assessments, while machine learning alternatives often sacrifice physical
consistency for speed, producing unrealistic predictions during extreme events.
This study addresses these challenges by developing ALPINE (All-in-one Physics
Informed Neural Emulator), a physics-informed neural network (PINN) framework
to enforce complete shallow water dynamics in compound flood modeling. Unlike
previous approaches that implement partial constraints, our framework
simultaneously enforces mass conservation and both momentum equations, ensuring
full adherence to Newton's laws throughout the prediction process. The model
integrates a convolutional encoder-decoder architecture with ConvLSTM temporal
processing, trained using a composite loss function that balances data fidelity
with physics-based residuals. Using six historical storm events (four for
training, one for validation, and one held-out for unseen testing), we observe
substantial improvements over baseline neural networks. ALPINE reduces
domain-averaged prediction errors and improves model skill metrics for water
surface elevation and velocity components. Physics-informed constraints prove
most valuable during peak storm intensity, when multiple flood drivers interact
and reliable predictions matter most. This approach yields a physically
consistent emulator capable of supporting compound-flood forecasting and
large-scale risk analyses while preserving physical realism essential for
coastal emergency management.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [560] [FinSurvival: A Suite of Large Scale Survival Modeling Tasks from Finance](https://arxiv.org/abs/2507.14160)
*Aaron Green,Zihan Nie,Hanzhen Qin,Oshani Seneviratne,Kristin P. Bennett*

Main category: q-fin.ST

TL;DR: This paper introduces FinSurvival, a suite of 16 survival modeling tasks derived from decentralized finance transaction data to benchmark AI survival models.


<details>
  <summary>Details</summary>
Motivation: Survival modeling lacks large-scale, realistic, and freely available datasets necessary for benchmarking AI models in diverse domains like finance and medicine.

Method: The authors utilized an automated pipeline to derive 16 survival modeling tasks and corresponding classification problems from public cryptocurrency lending transaction data.

Result: FinSurvival consists of over 7.5 million records and poses challenging modeling tasks that existing methods struggle to address.

Conclusion: FinSurvival serves as a novel benchmark for AI survival models, promoting advancements in understanding risks and opportunities across fields, alongside scalability for future datasets.

Abstract: Survival modeling predicts the time until an event occurs and is widely used
in risk analysis; for example, it's used in medicine to predict the survival of
a patient based on censored data. There is a need for large-scale, realistic,
and freely available datasets for benchmarking artificial intelligence (AI)
survival models. In this paper, we derive a suite of 16 survival modeling tasks
from publicly available transaction data generated by lending of
cryptocurrencies in Decentralized Finance (DeFi). Each task was constructed
using an automated pipeline based on choices of index and outcome events. For
example, the model predicts the time from when a user borrows cryptocurrency
coins (index event) until their first repayment (outcome event). We formulate a
survival benchmark consisting of a suite of 16 survival-time prediction tasks
(FinSurvival). We also automatically create 16 corresponding classification
problems for each task by thresholding the survival time using the restricted
mean survival time. With over 7.5 million records, FinSurvival provides a suite
of realistic financial modeling tasks that will spur future AI survival
modeling research. Our evaluation indicated that these are challenging tasks
that are not well addressed by existing methods. FinSurvival enables the
evaluation of AI survival models applicable to traditional finance, industry,
medicine, and commerce, which is currently hindered by the lack of large public
datasets. Our benchmark demonstrates how AI models could assess opportunities
and risks in DeFi. In the future, the FinSurvival benchmark pipeline can be
used to create new benchmarks by incorporating more DeFi transactions and
protocols as the use of cryptocurrency grows.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [561] [Learning to Communicate in Multi-Agent Reinforcement Learning for Autonomous Cyber Defence](https://arxiv.org/abs/2507.14658)
*Faizan Contractor,Li Li,Ranwa Al Mallah*

Main category: cs.MA

TL;DR: The paper presents a cooperative Multi-Agent Reinforcement Learning approach for cyber defense, emphasizing agent communication for better decision-making and policy learning.


<details>
  <summary>Details</summary>
Motivation: To enhance agent coordination and decision-making in cooperative Multi-Agent Reinforcement Learning for partially observable cyber defense environments.

Method: The authors use the Cyber Operations Research Gym and adapt the Differentiable Inter-Agent Learning algorithm to train agents to communicate minimal cost messages while developing effective defensive tactics.

Result: The agents learn tactical policies comparable to those of human experts and simultaneously optimize cost-efficient communication for effective defense.

Conclusion: Cooperative reinforcement learning with communication sharing significantly improves defensive performance and decision-making in cyber operational scenarios.

Abstract: Popular methods in cooperative Multi-Agent Reinforcement Learning with
partially observable environments typically allow agents to act independently
during execution, which may limit the coordinated effect of the trained
policies. However, by sharing information such as known or suspected ongoing
threats, effective communication can lead to improved decision-making in the
cyber battle space. We propose a game design where defender agents learn to
communicate and defend against imminent cyber threats by playing training games
in the Cyber Operations Research Gym, using the Differentiable Inter Agent
Learning algorithm adapted to the cyber operational environment. The tactical
policies learned by these autonomous agents are akin to those of human experts
during incident responses to avert cyber threats. In addition, the agents
simultaneously learn minimal cost communication messages while learning their
defence tactical policies.

</details>


### [562] [LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra](https://arxiv.org/abs/2507.15815)
*Seth Karten,Wenzhe Li,Zihan Ding,Samuel Kleiner,Yu Bai,Chi Jin*

Main category: cs.MA

TL;DR: The study introduces the LLM Economist, a tool using agent-based modeling and large language models to design and assess economic policies, achieving socially beneficial outcomes through simulated environments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to leverage large language models to explore and simulate strategic economic policies in complex, hierarchical systems, overcoming limitations in traditional economic modeling for improved societal outcomes.

Method: The paper employs agent-based modeling with bounded rational agents representing realistic demographics and labor supply decisions. A planner agent uses reinforcement learning to propose tax schedules and measure their effects on utility and social welfare, conducting experiments within a demographically calibrated framework.

Result: The framework achieves social welfare improvements by converging near Stackelberg equilibria and outperforming established policy baselines. Voting processes at the individual level further enhance decentralized decision-making.

Conclusion: LLM-based agent frameworks provide a credible and tractable means to model, experiment with, and govern complex economic systems, making them promising tools for better societal policy evaluation.

Abstract: We present the LLM Economist, a novel framework that uses agent-based
modeling to design and assess economic policies in strategic environments with
hierarchical decision-making. At the lower level, bounded rational worker
agents -- instantiated as persona-conditioned prompts sampled from U.S.
Census-calibrated income and demographic statistics -- choose labor supply to
maximize text-based utility functions learned in-context. At the upper level, a
planner agent employs in-context reinforcement learning to propose
piecewise-linear marginal tax schedules anchored to the current U.S. federal
brackets. This construction endows economic simulacra with three capabilities
requisite for credible fiscal experimentation: (i) optimization of
heterogeneous utilities, (ii) principled generation of large, demographically
realistic agent populations, and (iii) mechanism design -- the ultimate nudging
problem -- expressed entirely in natural language. Experiments with populations
of up to one hundred interacting agents show that the planner converges near
Stackelberg equilibria that improve aggregate social welfare relative to Saez
solutions, while a periodic, persona-level voting procedure furthers these
gains under decentralized governance. These results demonstrate that large
language model-based agents can jointly model, simulate, and govern complex
economic systems, providing a tractable test bed for policy evaluation at the
societal scale to help build better civilizations.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [563] [Forecasting Faculty Placement from Patterns in Co-authorship Networks](https://arxiv.org/abs/2507.14696)
*Samantha Dies,David Liu,Tina Eliassi-Rad*

Main category: cs.SI

TL;DR: This paper investigates faculty hiring by analyzing it through a predictive lens, using co-authorship networks in addition to traditional factors like prestige and publication history.


<details>
  <summary>Details</summary>
Motivation: To understand the factors influencing faculty hiring decisions and to highlight potential structural biases and inefficiencies in academic hiring processes.

Method: Utilizing temporal co-authorship networks combined with traditional indicators such as department prestige and bibliometric features, the researchers assessed the predictive accuracy of these factors in determining faculty placement.

Result: Including co-authorship networks improved the predictive accuracy of faculty hiring outcomes by up to 10%, particularly for elite (top-10) departments.

Conclusion: The study emphasizes the critical influence of social networks and professional endorsements in academic hiring, suggesting a need to address structural biases to improve equity and transparency in these processes.

Abstract: Faculty hiring shapes the flow of ideas, resources, and opportunities in
academia, influencing not only individual career trajectories but also broader
patterns of institutional prestige and scientific progress. While traditional
studies have found strong correlations between faculty hiring and attributes
such as doctoral department prestige and publication record, they rarely assess
whether these associations generalize to individual hiring outcomes,
particularly for future candidates outside the original sample. Here, we
consider faculty placement as an individual-level prediction task. Our data
consist of temporal co-authorship networks with conventional attributes such as
doctoral department prestige and bibliometric features. We observe that using
the co-authorship network significantly improves predictive accuracy by up to
10% over traditional indicators alone, with the largest gains observed for
placements at the most elite (top-10) departments. Our results underscore the
role that social networks, professional endorsements, and implicit advocacy
play in faculty hiring beyond traditional measures of scholarly productivity
and institutional prestige. By introducing a predictive framing of faculty
placement and establishing the benefit of considering co-authorship networks,
this work provides a new lens for understanding structural biases in academia
that could inform targeted interventions aimed at increasing transparency,
fairness, and equity in academic hiring practices.

</details>


### [564] [Privacy-Preserving Multimodal News Recommendation through Federated Learning](https://arxiv.org/abs/2507.15460)
*Mehdi Khalaj,Shahrzad Golestani Najafabadi,Julita Vassileva*

Main category: cs.SI

TL;DR: This paper proposes a multimodal federated learning-based system for personalized news recommendation, addressing privacy issues, short-term interest neglect, and reliance on text-focused features.


<details>
  <summary>Details</summary>
Motivation: Traditional personalized news recommendation systems struggle with privacy concerns from centralized data storage, overreliance on textual features, and inadequate handling of short-term user interests.

Method: The paper integrates multimodal content, utilizes multi-head self-attention networks for balancing user interests, and employs a federated learning approach with Shamir's secret sharing for secure aggregation.

Result: Experiments on a real-world news dataset show improved recommendation accuracy and privacy preservation compared to existing systems.

Conclusion: The proposed approach is a significant step forward in creating privacy-aware, effective personalized news recommendation systems.

Abstract: Personalized News Recommendation systems (PNR) have emerged as a solution to
information overload by predicting and suggesting news items tailored to
individual user interests. However, traditional PNR systems face several
challenges, including an overreliance on textual content, common neglect of
short-term user interests, and significant privacy concerns due to centralized
data storage. This paper addresses these issues by introducing a novel
multimodal federated learning-based approach for news recommendation. First, it
integrates both textual and visual features of news items using a multimodal
model, enabling a more comprehensive representation of content. Second, it
employs a time-aware model that balances users' long-term and short-term
interests through multi-head self-attention networks, improving recommendation
accuracy. Finally, to enhance privacy, a federated learning framework is
implemented, enabling collaborative model training without sharing user data.
The framework divides the recommendation model into a large server-maintained
news model and a lightweight user model shared between the server and clients.
The client requests news representations (vectors) and a user model from the
central server, then computes gradients with user local data, and finally sends
their locally computed gradients to the server for aggregation. The central
server aggregates gradients to update the global user model and news model. The
updated news model is further used to infer news representation by the server.
To further safeguard user privacy, a secure aggregation algorithm based on
Shamir's secret sharing is employed. Experiments on a real-world news dataset
demonstrate strong performance compared to existing systems, representing a
significant advancement in privacy-preserving personalized news recommendation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [565] [Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion](https://arxiv.org/abs/2507.14534)
*Yu Zhang,Baotong Tian,Zhiyao Duan*

Main category: eess.AS

TL;DR: This paper presents Conan, a model for real-time zero-shot online voice conversion, outperforming existing methods in preserving speech content and adapting styles.


<details>
  <summary>Details</summary>
Motivation: Current voice conversion models struggle with maintaining semantic accuracy, natural sound quality, and adaptability for unseen speakers during real-time applications.

Method: Conan introduces three components: a Stream Content Extractor for low-latency encoding, an Adaptive Style Encoder for detailed style adaptation, and a Causal Shuffle Vocoder for causal high-quality synthesis.

Result: The model demonstrated superior performance in both objective and subjective evaluations compared to baseline approaches.

Conclusion: Conan effectively addresses existing limitations in voice conversion, achieving state-of-the-art results for content preservation and style adaptability in real-time online scenarios.

Abstract: Zero-shot online voice conversion (VC) holds significant promise for
real-time communications and entertainment. However, current VC models struggle
to preserve semantic fidelity under real-time constraints, deliver
natural-sounding conversions, and adapt effectively to unseen speaker
characteristics. To address these challenges, we introduce Conan, a chunkwise
online zero-shot voice conversion model that preserves the content of the
source while matching the voice timbre and styles of reference speech. Conan
comprises three core components: 1) a Stream Content Extractor that leverages
Emformer for low-latency streaming content encoding; 2) an Adaptive Style
Encoder that extracts fine-grained stylistic features from reference speech for
enhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully
causal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations
demonstrate that Conan outperforms baseline models in subjective and objective
metrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [566] [Enhancing Celestial Imaging: High Dynamic Range with Neuromorphic Cameras](https://arxiv.org/abs/2503.22814)
*Satyapreet Singh Yadav,Nirupam Roy,Chetan Singh Thakur*

Main category: astro-ph.IM

TL;DR: The paper explores how neuromorphic cameras, with their high dynamic range, excel in capturing celestial objects with varying brightness levels, avoiding saturation issues common in conventional cameras.


<details>
  <summary>Details</summary>
Motivation: Traditional frame-based cameras struggle with capturing scenes featuring significant brightness differences, often resulting in saturation and loss of detail. The paper seeks to address this limitation by utilizing neuromorphic cameras, inspired by the human retina.

Method: The study leverages the high dynamic range capability of neuromorphic cameras to capture a variety of celestial objects with varying brightness levels, demonstrating their effectiveness through specific examples like Saturn and its moons, and Sirius A with Sirius B.

Result: Examples provided in the paper show neuromorphic cameras successfully capturing both bright objects and surrounding faint details without losing information due to saturation.

Conclusion: Neuromorphic cameras, inspired by the retina, significantly outperform conventional cameras for astronomical imaging, preserving details across wide flux ranges and making them highly effective for studying celestial phenomena.

Abstract: Conventional frame-based cameras often struggle with limited dynamic range,
leading to saturation and loss of detail when capturing scenes with significant
brightness variations. Neuromorphic cameras, inspired by human retina, offer a
solution by providing an inherently high dynamic range. This capability enables
them to capture both bright and faint celestial objects without saturation
effects, preserving details across a wide range of luminosities. This paper
investigates the application of neuromorphic imaging technology for capturing
celestial bodies across a wide range of flux levels. Its advantages are
demonstrated through examples such as the bright planet Saturn with its faint
moons and the bright star Sirius A alongside its faint companion, Sirius B.

</details>


### [567] [Hyper-spectral Unmixing algorithms for remote compositional surface mapping: a review of the state of the art](https://arxiv.org/abs/2507.14260)
*Alfredo Gimenez Zapiola,Andrea Boselli,Alessandra Menafoglio,Simone Vantini*

Main category: astro-ph.IM

TL;DR: This paper reviews hyperspectral unmixing methods for analyzing materials and their distribution in remote sensing images, compares techniques, explores datasets, and highlights future research directions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the challenge of analyzing hyperspectral images to identify surface materials and their distributions in Earth and astronomical remote sensing applications.

Method: It systematically reviews successful hyperspectral unmixing methods, compares techniques, explores public datasets used for validation, and provides recommendations for unresolved issues.

Result: The paper summarizes the state-of-the-art methods, identifies the strengths/limitations of recent advances, and lists widely-used datasets for remote sensing imagery analysis.

Conclusion: Open issues are spotlighted, and actionable suggestions for future improvement and research in hyperspectral image analysis are offered.

Abstract: This work concerns a detailed review of data analysis methods used for
remotely sensed images of large areas of the Earth and of other solid
astronomical objects. In detail, it focuses on the problem of inferring the
materials that cover the surfaces captured by hyper-spectral images and
estimating their abundances and spatial distributions within the region. The
most successful and relevant hyper-spectral unmixing methods are reported as
well as compared, as an addition to analysing the most recent methodologies.
The most important public data-sets in this setting, which are vastly used in
the testing and validation of the former, are also systematically explored.
Finally, open problems are spotlighted and concrete recommendations for future
research are provided.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [568] [Quantum Annealing for Machine Learning: Applications in Feature Selection, Instance Selection, and Clustering](https://arxiv.org/abs/2507.15063)
*Chloe Pomeroy,Aleksandar Pramov,Karishma Thakrar,Lakshmi Yendapalli*

Main category: quant-ph

TL;DR: The paper investigates using quantum annealing (QA) and classical simulated annealing (SA) for machine learning optimization tasks such as feature selection, instance selection, and clustering.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether quantum annealing can offer computational advantages and effective solutions for combinatorial optimization problems in machine learning.

Method: Machine learning tasks were formulated as Quadratic Unconstrained Binary Optimization (QUBO) problems and solved using both quantum annealing and classical simulated annealing approaches.

Result: QA proved computationally efficient for feature selection, improved instance-level selection heuristics, and achieved better clustering through a classical-to-quantum pipeline.

Conclusion: Quantum annealing presents itself as a promising tool for machine learning optimization, showcasing competitive performance even with current quantum hardware limitations.

Abstract: This paper explores the applications of quantum annealing (QA) and classical
simulated annealing (SA) to a suite of combinatorial optimization problems in
machine learning, namely feature selection, instance selection, and clustering.
We formulate each task as a Quadratic Unconstrained Binary Optimization (QUBO)
problem and implement both quantum and classical solvers to compare their
effectiveness. For feature selection, we propose several QUBO configurations
that balance feature importance and redundancy, showing that quantum annealing
(QA) produces solutions that are computationally more efficient. In instance
selection, we propose a few novel heuristics for instance-level importance
measures that extend existing methods. For clustering, we embed a
classical-to-quantum pipeline, using classical clustering followed by
QUBO-based medoid refinement, and demonstrate consistent improvements in
cluster compactness and retrieval metrics. Our results suggest that QA can be a
competitive and efficient tool for discrete machine learning optimization, even
within the constraints of current quantum hardware.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [569] [MENO: Hybrid Matrix Exponential-based Neural Operator for Stiff ODEs. Application to Thermochemical Kinetics](https://arxiv.org/abs/2507.14341)
*Ivan Zanardi,Simone Venturi,Marco Panesi*

Main category: physics.comp-ph

TL;DR: MENO is a hybrid framework for solving stiff ODEs by combining neural operators with matrix exponential methods, achieving less than 2% error and massive computational speedups.


<details>
  <summary>Details</summary>
Motivation: Solving stiff ODEs with sparse nonlinear structures is computationally expensive and lacks efficient surrogate models tailored to these systems.

Method: MENO decomposes stiff ODE systems into low-dimensional nonlinear parts modeled with neural operators and linear parts solved using a novel matrix exponential approach.

Result: MENO showed high accuracy (sub-2% error) across multiple stiff ODE setups and demonstrated significant computational speed improvements on GPUs (up to 4,800x) and CPUs (up to 185x).

Conclusion: MENO's physics-based architecture ensures robust generalization, physical consistency, and scalability for real-time simulation of stiff reactive systems.

Abstract: We introduce MENO (''Matrix Exponential-based Neural Operator''), a hybrid
surrogate modeling framework for efficiently solving stiff systems of ordinary
differential equations (ODEs) that exhibit a sparse nonlinear structure. In
such systems, only a few variables contribute nonlinearly to the dynamics,
while the majority influence the equations linearly. MENO exploits this
property by decomposing the system into two components: the low-dimensional
nonlinear part is modeled using conventional neural operators, while the linear
time-varying subsystem is integrated using a novel neural matrix exponential
formulation. This approach combines the exact solution of linear time-invariant
systems with learnable, time-dependent graph-based corrections applied to the
linear operators. Unlike black-box or soft-constrained physics-informed (PI)
models, MENO embeds the governing equations directly into its architecture,
ensuring physical consistency (e.g., steady states), improved robustness, and
more efficient training. We validate MENO on three complex thermochemical
systems: the POLLU atmospheric chemistry model, an oxygen mixture in
thermochemical nonequilibrium, and a collisional-radiative argon plasma in one-
and two-dimensional shock-tube simulations. MENO achieves relative errors below
2% in trained zero-dimensional settings and maintains good accuracy in
extrapolatory multidimensional regimes. It also delivers substantial
computational speedups, achieving up to 4 800$\times$ on GPU and 185$\times$ on
CPU compared to standard implicit ODE solvers. Although intrusive by design,
MENO's physics-based architecture enables superior generalization and
reliability, offering a scalable path for real-time simulation of stiff
reactive systems.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [570] [A Comparative Analysis of Statistical and Machine Learning Models for Outlier Detection in Bitcoin Limit Order Books](https://arxiv.org/abs/2507.14960)
*Ivan Letteri*

Main category: q-fin.TR

TL;DR: The paper compares various methods for anomaly detection in cryptocurrency limit order books (LOBs) using a dataset to identify the best-performing model, Empirical Covariance (EC), which outperformed a standard trading benchmark.


<details>
  <summary>Details</summary>
Motivation: Understanding and detecting outliers in cryptocurrency order books is crucial for analyzing market dynamics, especially in volatile and less-regulated environments.

Method: The study evaluates 13 different statistical and machine learning models for real-time anomaly detection using a unified environment (AITA-OBS) and backtests them on over 26,000 records from a major cryptocurrency exchange.

Result: Empirical Covariance (EC) was the top-performing model, achieving a 6.70% gain over the standard Buy-and-Hold trading strategy in backtested scenarios.

Conclusion: The research offers a robust benchmark for outlier detection models, emphasizing their potential in improving algorithmic trading and risk management strategies in cryptocurrency markets.

Abstract: The detection of outliers within cryptocurrency limit order books (LOBs) is
of paramount importance for comprehending market dynamics, particularly in
highly volatile and nascent regulatory environments. This study conducts a
comprehensive comparative analysis of robust statistical methods and advanced
machine learning techniques for real-time anomaly identification in
cryptocurrency LOBs. Within a unified testing environment, named AITA Order
Book Signal (AITA-OBS), we evaluate the efficacy of thirteen diverse models to
identify which approaches are most suitable for detecting potentially
manipulative trading behaviours. An empirical evaluation, conducted via
backtesting on a dataset of 26,204 records from a major exchange, demonstrates
that the top-performing model, Empirical Covariance (EC), achieves a 6.70%
gain, significantly outperforming a standard Buy-and-Hold benchmark. These
findings underscore the effectiveness of outlier-driven strategies and provide
insights into the trade-offs between model complexity, trade frequency, and
performance. This study contributes to the growing corpus of research on
cryptocurrency market microstructure by furnishing a rigorous benchmark of
anomaly detection models and highlighting their potential for augmenting
algorithmic trading and risk management.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [571] [Simulation-Prior Independent Neural Unfolding Procedure](https://arxiv.org/abs/2507.15084)
*Anja Butter,Theo Heimel,Nathan Huetsch,Michael Kagan,Tilman Plehn*

Main category: hep-ph

TL;DR: The SPINUP method employs neural networks for unfolding high-dimensional data distributions at LHC experiments, minimizing dependence on prior simulated data.


<details>
  <summary>Details</summary>
Motivation: Develop a powerful and prior-independent method for unfolding high-dimensional data in particle physics experiments like those at the LHC.

Method: SPINUP uses neural networks to encode forward mapping, incorporates neural importance sampling for efficiency, and employs ensembling to estimate information loss.

Result: SPINUP effectively unfolded data for jet substructure observables and parton-level processes like associated Higgs and single-top production.

Conclusion: SPINUP demonstrates a promising approach for data unfolding tasks in particle physics, offering improved independence and efficiency.

Abstract: Machine learning allows unfolding high-dimensional spaces without binning at
the LHC. The new SPINUP method extracts the unfolded distribution based on a
neural network encoding the forward mapping, making it independent of the prior
from the simulated training data. It is made efficient through neural
importance sampling, and ensembling can be used to estimate the effect of
information loss in the forward process. We showcase SPINUP for unfolding
detector effects on jet substructure observables and for unfolding to parton
level of associated Higgs and single-top production.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [572] [Real-Time Scene Reconstruction using Light Field Probes](https://arxiv.org/abs/2507.14624)
*Yaru Liu,Derek Nowrouzezahri,Morgan Mcguire*

Main category: cs.GR

TL;DR: The paper proposes a novel method for large-scale scene reconstruction using sparse images and implicit representations, avoiding explicit scene geometry to reduce computational cost.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of current neural rendering methods and geometry-based techniques, which struggle with efficiently rendering complex, large-scale scenes either due to computational costs or trade-offs in size, fidelity, and speed.

Method: The method employs intermediate, multi-scale, implicit representations of scene geometries reconstructed from sparse images and utilizes a probe data structure to hold accurate depth information.

Result: The approach results in highly efficient reconstruction of complex large-scale scenes while keeping rendering costs independent of scene complexity and enabling efficient compression and streaming.

Conclusion: This method can serve as a more efficient alternative for VR and AR applications, paving the way for scalable and cost-effective photo-realistic scene rendering.

Abstract: Reconstructing photo-realistic large-scale scenes from images, for example at
city scale, is a long-standing problem in computer graphics. Neural rendering
is an emerging technique that enables photo-realistic image synthesis from
previously unobserved viewpoints; however, state-of-the-art neural rendering
methods have difficulty efficiently rendering a high complex large-scale scene
because these methods typically trade scene size, fidelity, and rendering speed
for quality. The other stream of techniques utilizes scene geometries for
reconstruction. But the cost of building and maintaining a large set of
geometry data increases as scene size grows. Our work explores novel view
synthesis methods that efficiently reconstruct complex scenes without explicit
use of scene geometries. Specifically, given sparse images of the scene
(captured from the real world), we reconstruct intermediate, multi-scale,
implicit representations of scene geometries. In this way, our method avoids
explicitly relying on scene geometry, significantly reducing the computational
cost of maintaining large 3D data. Unlike current methods, we reconstruct the
scene using a probe data structure. Probe data hold highly accurate depth
information of dense data points, enabling the reconstruction of highly complex
scenes. By reconstructing the scene using probe data, the rendering cost is
independent of the complexity of the scene. As such, our approach combines
geometry reconstruction and novel view synthesis. Moreover, when rendering
large-scale scenes, compressing and streaming probe data is more efficient than
using explicit scene geometry. Therefore, our neural representation approach
can potentially be applied to virtual reality (VR) and augmented reality (AR)
applications.

</details>


### [573] [Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization](https://arxiv.org/abs/2507.14841)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: The paper proposes a new three-stage framework for high-quality 3D scene generation from a single RGB image, focusing on geometric accuracy and scene coherence.


<details>
  <summary>Details</summary>
Motivation: The motivation of the paper is to address a key challenge in 3D generation: achieving high-quality object generation and scene coherence from a single RGB image, particularly in multi-object scenarios.

Method: The method consists of three stages: 1) instance segmentation and inpainting to recover occluded details, 2) pseudo-stereo viewpoint construction for depth inference and camera parameter estimation, and 3) model parameterization and layout optimization to ensure precise alignment between input guidance and 3D representations.

Result: The experiments demonstrated superior geometric accuracy and texture fidelity of individual 3D models as well as effective scene layout synthesis when compared to state-of-the-art methods.

Conclusion: The proposed framework effectively overcomes limitations of current 3D scene generation methods, offering improved geometric and textural detail for individual models and coherent multi-object scene representation.

Abstract: In recent years, 3D generation has made great strides in both academia and
industry. However, generating 3D scenes from a single RGB image remains a
significant challenge, as current approaches often struggle to ensure both
object generation quality and scene coherence in multi-object scenarios. To
overcome these limitations, we propose a novel three-stage framework for 3D
scene generation with explicit geometric representations and high-quality
textural details via single image-guided model generation and spatial layout
optimization. Our method begins with an image instance segmentation and
inpainting phase, which recovers missing details of occluded objects in the
input images, thereby achieving complete generation of foreground 3D assets.
Subsequently, our approach captures the spatial geometry of reference image by
constructing pseudo-stereo viewpoint for camera parameter estimation and scene
depth inference, while employing a model selection strategy to ensure optimal
alignment between the 3D assets generated in the previous step and the input.
Finally, through model parameterization and minimization of the Chamfer
distance between point clouds in 3D and 2D space, our approach optimizes layout
parameters to produce an explicit 3D scene representation that maintains
precise alignment with input guidance image. Extensive experiments on
multi-object scene image sets have demonstrated that our approach not only
outperforms state-of-the-art methods in terms of geometric accuracy and texture
fidelity of individual generated 3D models, but also has significant advantages
in scene layout synthesis.

</details>


### [574] [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/abs/2507.15399)
*Etai Sella,Noam Atia,Ron Mokady,Hadar Averbuch-Elor*

Main category: cs.GR

TL;DR: This paper proposes a method for fine-grained editing of 3D point cloud shapes using an inpainting framework, enhanced by 3D diffusion models and a coordinate blending algorithm.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing methods in 3D shape editing, which struggle with preserving global coherence while performing localized and fine-grained modifications.

Method: The proposed approach employs foundation 3D diffusion models for localized edits, integrates structural guidance via partial conditional shapes, and introduces a coordinate blending algorithm during inference to preserve both global and local shape identity.

Result: Experiments demonstrate that the method surpasses existing techniques in maintaining fidelity to the original shape and adhering to textual descriptions.

Conclusion: The framework provides an effective solution for performing localized fine-grained edits of 3D shapes while ensuring global coherence and overcoming challenges such as costly shape inversion.

Abstract: Natural language offers a highly intuitive interface for enabling localized
fine-grained edits of 3D shapes. However, prior works face challenges in
preserving global coherence while locally modifying the input 3D shape. In this
work, we introduce an inpainting-based framework for editing shapes represented
as point clouds. Our approach leverages foundation 3D diffusion models for
achieving localized shape edits, adding structural guidance in the form of a
partial conditional shape, ensuring that other regions correctly preserve the
shape's identity. Furthermore, to encourage identity preservation also within
the local edited region, we propose an inference-time coordinate blending
algorithm which balances reconstruction of the full shape with inpainting at a
progression of noise levels during the inference process. Our coordinate
blending algorithm seamlessly blends the original shape with its edited
version, enabling a fine-grained editing of 3D shapes, all while circumventing
the need for computationally expensive and often inaccurate inversion.
Extensive experiments show that our method outperforms alternative techniques
across a wide range of metrics that evaluate both fidelity to the original
shape and also adherence to the textual description.

</details>


### [575] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: ObjectGS introduces object-awareness to 3D Gaussian Splatting by modeling individual objects with distinct neural Gaussians and IDs, enabling enhanced semantic understanding and performance in tasks like segmentation and scene editing.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the semantic understanding limitations in traditional 3D Gaussian Splatting, enhancing its ability for precise object-level perception and reconstruction.

Method: ObjectGS models the scene through individual objects as anchors, dynamically adjusting them during training and applying semantic constraints using one-hot ID encoding and classification loss.

Result: ObjectGS achieves state-of-the-art performance in open-vocabulary and panoptic segmentation tasks, and integrates efficiently with mesh extraction and scene editing.

Conclusion: ObjectGS provides both high-fidelity reconstructions and semantic awareness, effectively bridging the gap between 3D scene reconstruction and object-level perception.

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


### [576] [Gaussian Splatting with Discretized SDF for Relightable Assets](https://arxiv.org/abs/2507.15629)
*Zuo-Liang Zhu,Jian Yang,Beibei Wang*

Main category: cs.GR

TL;DR: This paper proposes a discretized SDF representation to improve 3D Gaussian splatting for inverse rendering, achieving superior results with reduced memory and computational complexity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address challenges in applying 3D Gaussian splatting to inverse rendering, such as the difficulty of imposing geometry constraints due to its discrete nature.

Method: A discretized signed distance field (SDF), encoded within each Gaussian using sampled values, is introduced with an SDF-to-opacity transformation. This enables rendering SDF via splatting, along with a projection-based consistency loss to enforce alignment.

Result: The proposed method improves relighting quality in inverse rendering without additional memory usage or complex optimization.

Conclusion: The approach demonstrates its superiority over existing Gaussian-based methods for inverse rendering by maintaining efficiency, reducing memory demands, and achieving higher-quality results.

Abstract: 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and
highly efficient rendering speed in the novel view synthesis (NVS) task. The
application to inverse rendering still faces several challenges, as the
discrete nature of Gaussian primitives makes it difficult to apply geometry
constraints. Recent works introduce the signed distance field (SDF) as an extra
continuous representation to regularize the geometry defined by Gaussian
primitives. It improves the decomposition quality, at the cost of increasing
memory usage and complicating training. Unlike these works, we introduce a
discretized SDF to represent the continuous SDF in a discrete manner by
encoding it within each Gaussian using a sampled value. This approach allows us
to link the SDF with the Gaussian opacity through an SDF-to-opacity
transformation, enabling rendering the SDF via splatting and avoiding the
computational cost of ray marching.The key challenge is to regularize the
discrete samples to be consistent with the underlying SDF, as the discrete
representation can hardly apply the gradient-based constraints (\eg Eikonal
loss). For this, we project Gaussians onto the zero-level set of SDF and
enforce alignment with the surface from splatting, namely a projection-based
consistency loss. Thanks to the discretized SDF, our method achieves higher
relighting quality, while requiring no extra memory beyond GS and avoiding
complex manually designed optimization. The experiments reveal that our method
outperforms existing Gaussian-based inverse rendering methods. Our code is
available at https://github.com/NK-CS-ZZL/DiscretizedSDF.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [577] [A universal augmentation framework for long-range electrostatics in machine learning interatomic potentials](https://arxiv.org/abs/2507.14302)
*Dongjin Kim,Xiaoyu Wang,Peichen Zhong,Daniel S. King,Theo Jaffrelot Inizan,Bingqing Cheng*

Main category: physics.chem-ph

TL;DR: The Latent Ewald Summation (LES) library enhances MLIPs by integrating efficient long-range electrostatics, improving accuracy and reliability without directly training on electrical properties.


<details>
  <summary>Details</summary>
Motivation: Current MLIPs lack an explicit treatment of long-range electrostatics, limiting their scope of accuracy in complex systems like biomolecules and bulk liquids.

Method: The LES library learns electrostatics, polarization, and Born effective charges from energy and force data, integrating seamlessly with existing MLIPs such as MACE, NequIP, CACE, and CHGNet.

Result: LES-enhanced models demonstrate improved accuracy across diverse systems, scale efficiently to large datasets, and perform reliably on predicting dipoles and BECs compared to short-range methods.

Conclusion: LES enables the development of universal MLIPs with efficient long-range electrostatics, paving the way for more comprehensive and foundational applications in complex systems.

Abstract: Most current machine learning interatomic potentials (MLIPs) rely on
short-range approximations, without explicit treatment of long-range
electrostatics. To address this, we recently developed the Latent Ewald
Summation (LES) method, which infers electrostatic interactions, polarization,
and Born effective charges (BECs), just by learning from energy and force
training data. Here, we present LES as a standalone library, compatible with
any short-range MLIP, and demonstrate its integration with methods such as
MACE, NequIP, CACE, and CHGNet. We benchmark LES-enhanced models on distinct
systems, including bulk water, polar dipeptides, and gold dimer adsorption on
defective substrates, and show that LES not only captures correct
electrostatics but also improves accuracy. Additionally, we scale LES to large
and chemically diverse data by training MACELES-OFF on the SPICE set containing
molecules and clusters, making a universal MLIP with electrostatics for organic
systems including biomolecules. MACELES-OFF is more accurate than its
short-range counterpart (MACE-OFF) trained on the same dataset, predicts
dipoles and BECs reliably, and has better descriptions of bulk liquids. By
enabling efficient long-range electrostatics without directly training on
electrical properties, LES paves the way for electrostatic foundation MLIPs.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [578] [Design of an Edge-based Portable EHR System for Anemia Screening in Remote Health Applications](https://arxiv.org/abs/2507.15146)
*Sebastian A. Cruz Romero,Misael J. Mercado Hernandez,Samir Y. Ali Rivera,Jorge A. Santiago Fernandez,Wilfredo E. Lugo Beauchamp*

Main category: cs.ET

TL;DR: This paper introduces a portable, edge-enabled Electronic Health Record platform tailored for resource-limited environments, supporting offline operation, secure data management, and diagnostic integration.


<details>
  <summary>Details</summary>
Motivation: To address persistent challenges such as interoperability issues, lack of offline support, and dependency on costly infrastructure in medical systems tailored for resource-limited and remote environments.

Method: The platform uses small-form-factor embedded devices with AES-256 encryption for local data storage and optional cloud synchronization. A non-invasive anemia screening module analyzing fingernail pallor using Random Forest and YOLOv8n-based detection was integrated as a specific application.

Result: The anemia screening model showed a test RMSE of 1.969 g/dL, MAE of 1.490 g/dL, and 79.2% sensitivity for severity-based detection. Optimization reduced inference latency significantly (from 46.96 ms to 21.50 ms) while retaining high performance (mAP@0.5 of 0.995).

Conclusion: The proposed system is scalable, emphasizes cost efficiency, and meets data privacy regulations, providing a practical solution for digital health adoption in disconnected and underserved settings.

Abstract: The design of medical systems for remote, resource-limited environments faces
persistent challenges due to poor interoperability, lack of offline support,
and dependency on costly infrastructure. Many existing digital health solutions
neglect these constraints, limiting their effectiveness for frontline health
workers in underserved regions. This paper presents a portable, edge-enabled
Electronic Health Record platform optimized for offline-first operation, secure
patient data management, and modular diagnostic integration. Running on
small-form factor embedded devices, it provides AES-256 encrypted local storage
with optional cloud synchronization for interoperability. As a use case, we
integrated a non-invasive anemia screening module leveraging fingernail pallor
analysis. Trained on 250 patient cases (27\% anemia prevalence) with
KDE-balanced data, the Random Forest model achieved a test RMSE of 1.969 g/dL
and MAE of 1.490 g/dL. A severity-based model reached 79.2\% sensitivity. To
optimize performance, a YOLOv8n-based nail bed detector was quantized to INT8,
reducing inference latency from 46.96 ms to 21.50 ms while maintaining mAP@0.5
at 0.995. The system emphasizes low-cost deployment, modularity, and data
privacy compliance (HIPAA/GDPR), addressing critical barriers to digital health
adoption in disconnected settings. Our work demonstrates a scalable approach to
enhance portable health information systems and support frontline healthcare in
underserved regions.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [579] [JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks](https://arxiv.org/abs/2505.17593)
*Manuel Valle Torre,Thom van der Velden,Marcus Specht,Catharine Oertel*

Main category: cs.HC

TL;DR: The paper introduces JELAI, an open-source platform that integrates Learning Analytics (LA) with AI-based tutoring in Jupyter Notebooks.


<details>
  <summary>Details</summary>
Motivation: Traditional generative AI tools lack pedagogical grounding and awareness of students' learning contexts, making it challenging to research their effectiveness in real educational settings.

Method: JELAI employs a modular and containerized design with JupyterLab extensions for telemetry and chat, combined with middleware for Learning Analytics processing and context-aware LLM prompt enrichment.

Result: The system's feasibility is demonstrated through performance benchmarks and two proof-of-concept use cases showcasing its capabilities in data logging, analyzing help-seeking patterns, and supporting A/B testing of AI configurations.

Conclusion: JELAI provides a flexible technical framework for educators and researchers to integrate, deploy, and study Learning Analytics-informed AI tutoring in Jupyter-based environments.

Abstract: Generative AI offers potential for educational support, but often lacks
pedagogical grounding and awareness of the student's learning context.
Furthermore, researching student interactions with these tools within authentic
learning environments remains challenging. To address this, we present JELAI,
an open-source platform architecture designed to integrate fine-grained
Learning Analytics (LA) with Large Language Model (LLM)-based tutoring directly
within a Jupyter Notebook environment. JELAI employs a modular, containerized
design featuring JupyterLab extensions for telemetry and chat, alongside a
central middleware handling LA processing and context-aware LLM prompt
enrichment. This architecture enables the capture of integrated code
interaction and chat data, facilitating real-time, context-sensitive AI
scaffolding and research into student behaviour. We describe the system's
design, implementation, and demonstrate its feasibility through system
performance benchmarks and two proof-of-concept use cases illustrating its
capabilities for logging multi-modal data, analysing help-seeking patterns, and
supporting A/B testing of AI configurations. JELAI's primary contribution is
its technical framework, providing a flexible tool for researchers and
educators to develop, deploy, and study LA-informed AI tutoring within the
widely used Jupyter ecosystem.

</details>


### [580] [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)
*Angjelin Hila,Elliott Hauser*

Main category: cs.HC

TL;DR: The study examines the ability of large language models like ChatGPT for deductive qualitative coding of texts and finds that tailored intervention strategies, like Step-by-Step Task Decomposition, significantly improve performance.


<details>
  <summary>Details</summary>
Motivation: Most research focuses on inductive use of LLMs, leaving their potential for deductive classification underexplored. The study seeks to fill this gap by testing structured classification tasks using human-coded schemes.

Method: The researchers used the CAP Master Codebook to classify U.S. Supreme Court summaries into 21 policy domains, testing four intervention strategies (zero-shot, few-shot, definition-based, and Step-by-Step Task Decomposition) with performance validated via accuracy, F1-score, and statistical tests like chi-squared and Cramer's V.

Result: The Step-by-Step Task Decomposition strategy showed the best reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746), with moderate to strong influence on classification behavior confirmed by chi-squared tests (Cramer's V: 0.359-0.613).

Conclusion: With tailored interventions like Step-by-Step Task Decomposition, LLMs such as ChatGPT can achieve reliable classification performance, making them suitable for structured qualitative coding workflows.

Abstract: In this study, we investigate the use of large language models (LLMs),
specifically ChatGPT, for structured deductive qualitative coding. While most
current research emphasizes inductive coding applications, we address the
underexplored potential of LLMs to perform deductive classification tasks
aligned with established human-coded schemes. Using the Comparative Agendas
Project (CAP) Master Codebook, we classified U.S. Supreme Court case summaries
into 21 major policy domains. We tested four intervention methods: zero-shot,
few-shot, definition-based, and a novel Step-by-Step Task Decomposition
strategy, across repeated samples. Performance was evaluated using standard
classification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's
alpha), and construct validity was assessed using chi-squared tests and
Cramer's V. Chi-squared and effect size analyses confirmed that intervention
strategies significantly influenced classification behavior, with Cramer's V
values ranging from 0.359 to 0.613, indicating moderate to strong shifts in
classification patterns. The Step-by-Step Task Decomposition strategy achieved
the strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),
achieving thresholds for substantial agreement. Despite the semantic ambiguity
within case summaries, ChatGPT displayed stable agreement across samples,
including high F1 scores in low-support subclasses. These findings demonstrate
that with targeted, custom-tailored interventions, LLMs can achieve reliability
levels suitable for integration into rigorous qualitative coding workflows.

</details>


### [581] [Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students](https://arxiv.org/abs/2507.14418)
*Taufiq Daryanto,Sophia Stil,Xiaohan Ding,Daniel Manesh,Sang Won Lee,Tim Lee,Stephanie Lunn,Sarah Rodriguez,Chris Brown,Eugenia Rho*

Main category: cs.HC

TL;DR: This study investigates how conversational AI can assist with the think-aloud process in coding interviews and proposes design recommendations for improving technical interview simulations and preparations.


<details>
  <summary>Details</summary>
Motivation: Candidates often struggle with the think-aloud process during technical interviews, and structured practice resources are limited. The paper aims to explore the potential of conversational AI in enhancing this practice.

Method: A study with 17 participants utilized a technical interview practice tool powered by a large language model (LLM) to gather insights into user experiences and perceptions.

Result: Participants appreciated AI's contributions to simulation, feedback provision, and learning from examples. Additionally, broader considerations like intersectional challenges and equitable learning opportunities were highlighted.

Conclusion: The paper recommends integrating human-AI collaboration, enhancing social presence in AI tools, providing better feedback mechanisms, and crowdsourcing think-aloud examples to improve technical interview preparation.

Abstract: One challenge in technical interviews is the think-aloud process, where
candidates verbalize their thought processes while solving coding tasks.
Despite its importance, opportunities for structured practice remain limited.
Conversational AI offers potential assistance, but limited research explores
user perceptions of its role in think-aloud practice. To address this gap, we
conducted a study with 17 participants using an LLM-based technical interview
practice tool. Participants valued AI's role in simulation, feedback, and
learning from generated examples. Key design recommendations include promoting
social presence in conversational AI for technical interview simulation,
providing feedback beyond verbal content analysis, and enabling crowdsourced
think-aloud examples through human-AI collaboration. Beyond feature design, we
examined broader considerations, including intersectional challenges and
potential strategies to address them, how AI-driven interview preparation could
promote equitable learning in computing careers, and the need to rethink AI's
role in interview practice by suggesting a research direction that integrates
human-AI collaboration.

</details>


### [582] [XplainAct: Visualization for Personalized Intervention Insights](https://arxiv.org/abs/2507.14767)
*Yanming Zhang,Krishnakumar Hegde,Klaus Mueller*

Main category: cs.HC

TL;DR: XplainAct introduces a visual analytics framework for assessing interventions at the individual level in heterogeneous systems, showcased through case studies in opioid-related deaths and voting behaviors.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the inadequacy of causal reasoning techniques at capturing individual-level interventions, especially in systems with high heterogeneity.

Method: The authors developed XplainAct, a framework supporting simulation, explanation, and reasoning about interventions at the individual level within subpopulations.

Result: XplainAct was demonstrated as effective through case studies focused on opioid-related epidemiological outcomes and voting behaviors in elections.

Conclusion: Visual analytics frameworks like XplainAct enhance causal reasoning by allowing insights into specific subpopulation interventions, which is particularly useful in diverse scenarios.

Abstract: Causality helps people reason about and understand complex systems,
particularly through what-if analyses that explore how interventions might
alter outcomes. Although existing methods embrace causal reasoning using
interventions and counterfactual analysis, they primarily focus on effects at
the population level. These approaches often fall short in systems
characterized by significant heterogeneity, where the impact of an intervention
can vary widely across subgroups. To address this challenge, we present
XplainAct, a visual analytics framework that supports simulating, explaining,
and reasoning interventions at the individual level within subpopulations. We
demonstrate the effectiveness of XplainAct through two case studies:
investigating opioid-related deaths in epidemiology and analyzing voting
inclinations in the presidential election.

</details>


### [583] [NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments](https://arxiv.org/abs/2507.15072)
*Maisha Maimuna,Minhaz Bin Farukee,Sama Nikanfar,Mahfuza Siddiqua,Ayon Roy,Fillia Makedon*

Main category: cs.HC

TL;DR: This paper introduces a simulator for blind and low-vision (BLV) operators to safely teleoperate robots in industrial warehouses using multimodal feedback.


<details>
  <summary>Details</summary>
Motivation: There is a need for accessible teleoperation systems for BLV individuals in industrial environments to support safety and inclusive workforce participation.

Method: The authors developed a simulator combining navigation mesh with re-planning and multimodal feedback (visual, auditory, haptic) for BLV users to navigate high-fidelity warehouse environments.

Result: The simulator effectively provides synchronized guidance through visual paths, voice cues, and haptic feedback while ensuring safety in dynamic industrial settings.

Conclusion: The approach offers a repeatable testbed for accessible teleoperation research and can be adapted to real-world robots with commercial hardware for faster deployment.

Abstract: Industrial warehouses are congested with moving forklifts, shelves and
personnel, making robot teleoperation particularly risky and demanding for
blind and low-vision (BLV) operators. Although accessible teleoperation plays a
key role in inclusive workforce participation, systematic research on its use
in industrial environments is limited, and few existing studies barely address
multimodal guidance designed for BLV users. We present a novel multimodal
guidance simulator that enables BLV users to control a mobile robot through a
high-fidelity warehouse environment while simultaneously receiving synchronized
visual, auditory, and haptic feedback. The system combines a navigation mesh
with regular re-planning so routes remain accurate avoiding collisions as
forklifts and human avatars move around the warehouse. Users with low vision
are guided with a visible path line towards destination; navigational voice
cues with clockwise directions announce upcoming turns, and finally
proximity-based haptic feedback notifies the users of static and moving
obstacles in the path. This real-time, closed-loop system offers a repeatable
testbed and algorithmic reference for accessible teleoperation research. The
simulator's design principles can be easily adapted to real robots due to the
alignment of its navigation, speech, and haptic modules with commercial
hardware, supporting rapid feasibility studies and deployment of inclusive
telerobotic tools in actual warehouses.

</details>


### [584] [Efficient Visual Appearance Optimization by Learning from Prior Preferences](https://arxiv.org/abs/2507.15355)
*Zhipeng Li,Yi-Chi Liao,Christian Holz*

Main category: cs.HC

TL;DR: Meta-PO integrates Preferential Bayesian Optimization with meta-learning to optimize visual settings by leveraging prior user preferences, achieving faster convergence and personalized results.


<details>
  <summary>Details</summary>
Motivation: Optimizing visual parameters like brightness and contrast is challenging due to the large search space and lack of explicit objective function, necessitating reliance on user preferences.

Method: Meta-PO combines Preferential Bayesian Optimization with meta-learning to store models of prior user preferences and intelligently suggest design candidates, improving sample efficiency and personalization.

Result: Participants achieved satisfactory results in fewer iterations using Meta-PO, showing efficiency and generalizability even across divergent optimization goals.

Conclusion: Meta-PO enables more efficient and personalized visual parameter optimization, making it practical for everyday users and scalable to broader interface personalization tasks.

Abstract: Adjusting visual parameters such as brightness and contrast is common in our
everyday experiences. Finding the optimal parameter setting is challenging due
to the large search space and the lack of an explicit objective function,
leaving users to rely solely on their implicit preferences. Prior work has
explored Preferential Bayesian Optimization (PBO) to address this challenge,
involving users to iteratively select preferred designs from candidate sets.
However, PBO often requires many rounds of preference comparisons, making it
more suitable for designers than everyday end-users. We propose Meta-PO, a
novel method that integrates PBO with meta-learning to improve sample
efficiency. Specifically, Meta-PO infers prior users' preferences and stores
them as models, which are leveraged to intelligently suggest design candidates
for the new users, enabling faster convergence and more personalized results.
An experimental evaluation of our method for appearance design tasks on 2D and
3D content showed that participants achieved satisfactory appearance in 5.86
iterations using Meta-PO when participants shared similar goals with a
population (e.g., tuning for a ``warm'' look) and in 8 iterations even
generalizes across divergent goals (e.g., from ``vintage'', ``warm'', to
``holiday''). Meta-PO makes personalized visual optimization more applicable to
end-users through a generalizable, more efficient optimization conditioned on
preferences, with the potential to scale interface personalization more
broadly.

</details>


### [585] [Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance](https://arxiv.org/abs/2507.15783)
*Mohammad 'Matt' Namvarpour,Brandon Brofsky,Jessica Medina,Mamtaj Akter,Afsaneh Razi*

Main category: cs.HC

TL;DR: The paper investigates emotional dependence and overreliance of teens on GenAI chatbots, analyzing 318 Reddit posts from users aged 13-17.


<details>
  <summary>Details</summary>
Motivation: To explore how teens' interactions with GenAI-driven chatbots like Character.AI lead to emotional attachment and digital overdependence, which poses risks to their offline relationships and routines.

Method: The study analyzed 318 Reddit posts by teens aged 13-17 from the Character.AI subreddit to identify patterns of behavior and overreliance.

Result: Teens use chatbots for emotional support and creativity but often develop strong attachments, leading to psychological distress, impaired offline relationships, and challenges in disengagement.

Conclusion: Recommendations for chatbot design include fostering self-awareness, encouraging real-world engagement, and involving teens in creating safer digital tools.

Abstract: As Generative Artificial Intelligence (GenAI) driven chatbots like
Character.AI become embedded in adolescent life, they raise concerns about
emotional dependence and digital overreliance. While studies have investigated
the overreliance of adults on these chatbots, they have not investigated teens'
interactions with chatbots with customizable personas. We analyzed 318 Reddit
posts made by users self-reported as 13-17 years old on the Character.AI
subreddit to understand patterns of overreliance. We found teens commonly begin
using chatbots for emotional support or creative expression, but many develop
strong attachments that interfere with offline relationships and daily
routines. Their posts revealed recurring signs of psychological distress,
cycles of relapse, and difficulty disengaging. Teens reported that their
overreliance often ended when they reflect on the harm, return to in-person
social settings, or become frustrated by platform restrictions. Based on the
implications of our findings, we provide recommendations for future chatbot
design so they can promote self-awareness, support real-world engagement, and
involve teens in developing safer digital tools.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [586] [Mayura: Exploiting Similarities in Motifs for Temporal Co-Mining](https://arxiv.org/abs/2507.14813)
*Sanjay Sri Vallabh Singapuram,Ronald Dreslinski,Nishil Talati*

Main category: cs.DB

TL;DR: This paper introduces "Mayura," a framework for efficiently mining multiple temporal motifs using shared computational pathways, resulting in significant speed-ups.


<details>
  <summary>Details</summary>
Motivation: Temporal graphs are vital for understanding dynamic interactions, but existing motif mining methods are computationally inefficient when querying multiple motifs with overlapping structures.

Method: The authors present "Mayura," leveraging a hierarchical data structure called the MG-Tree and a co-mining algorithm to minimize redundant computations. They also adapt the methodology for both CPU and GPU environments.

Result: Empirical evaluations on real-world datasets show that Mayura achieves an average speed-up of 2.4x on CPUs and 1.7x on GPUs compared to state-of-the-art methods, while maintaining accuracy.

Conclusion: Mayura demonstrates that exploiting structural and temporal commonalities in motif mining can lead to significant computational gains, making it suitable for high-stakes applications.

Abstract: Temporal graphs serve as a critical foundation for modeling evolving
interactions in domains ranging from financial networks to social media. Mining
temporal motifs is essential for applications such as fraud detection,
cybersecurity, and dynamic network analysis. However, conventional motif mining
approaches treat each query independently, incurring significant redundant
computations when similar substructures exist across multiple motifs. In this
paper, we propose Mayura, a novel framework that unifies the mining of multiple
temporal motifs by exploiting their inherent structural and temporal
commonalities. Central to our approach is the Motif-Group Tree (MG-Tree), a
hierarchical data structure that organizes related motifs and enables the reuse
of common search paths, thereby reducing redundant computation. We propose a
co-mining algorithm that leverages the MG-Tree and develop a flexible runtime
capable of exploiting both CPU and GPU architectures for scalable performance.
Empirical evaluations on diverse real-world datasets demonstrate that Mayura
achieves substantial improvements over the state-of-the-art techniques that
mine each motif individually, with an average speed-up of 2.4x on the CPU and
1.7x on the GPU, while maintaining the exactness required for high-stakes
applications.

</details>


### [587] [IDSS, a Novel P2P Relational Data Storage Service](https://arxiv.org/abs/2507.14682)
*Massimo Cafaro,Italo Epicoco,Marco Pulimeno,Lunodzo J. Mwinuka,Lucas Pereira,Hugo Morais*

Main category: cs.DB

TL;DR: The paper introduces IDSS, a data storage tool using peer-to-peer networks and relational databases for efficient large-scale data management.


<details>
  <summary>Details</summary>
Motivation: With the rapid growth in data generation, traditional database systems face challenges in scalability and inefficiency when managing large-scale and diverse data.

Method: The paper proposes IDSS, utilizing peer-to-peer networks and embedded relational databases for distributed queries. It is built on a common schema to handle complex distributed query processing efficiently.

Result: IDSS exhibits the capability to manage vast and diverse data more robustly and effectively than traditional database systems.

Conclusion: The innovative architecture of IDSS provides a scalable and efficient solution for handling large-scale data challenges, addressing shortcomings in traditional systems.

Abstract: The rate at which data is generated has been increasing rapidly, raising
challenges related to its management. Traditional database management systems
suffer from scalability and are usually inefficient when dealing with
large-scale and heterogeneous data. This paper introduces IDSS (InnoCyPES Data
Storage Service), a novel large-scale data storage tool that leverages
peer-to-peer networks and embedded relational databases. We present the IDSS
architecture and its design, and provide details related to the implementation.
The peer-to-peer framework is used to provide support for distributed queries
leveraging a relational database architecture based on a common schema.
Furthermore, methods to support complex distributed query processing, enabling
robust and efficient management of vast amounts of data are presented.

</details>


### [588] [Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms](https://arxiv.org/abs/2507.14376)
*Osman Erman Gungor,Derak Paulsen,William Kang*

Main category: cs.DB

TL;DR: SCHEMORA framework enhances schema matching by utilizing large language models and hybrid retrieval techniques efficiently without labeled data.


<details>
  <summary>Details</summary>
Motivation: Schema matching is complex and resource-intensive, yet crucial for integrating heterogeneous data sources and enhancing dataset discovery.

Method: The framework combines large language models and hybrid vector-lexical retrieval techniques in a prompt-based approach, enriching schema metadata without requiring labeled training data.

Result: Achieves state-of-the-art performance in the MIMIC-OMOP benchmark, with HitRate@5 improving by 7.49% and HitRate@3 by 3.75% compared to previous methods.

Conclusion: SCHEMORA successfully improves accuracy and scalability in schema matching, with an open-source implementation providing practical insights into retrieval techniques and model selection.

Abstract: Schema matching is essential for integrating heterogeneous data sources and
enhancing dataset discovery, yet it remains a complex and resource-intensive
problem. We introduce SCHEMORA, a schema matching framework that combines large
language models with hybrid retrieval techniques in a prompt-based approach,
enabling efficient identification of candidate matches without relying on
labeled training data or exhaustive pairwise comparisons. By enriching schema
metadata and leveraging both vector-based and lexical retrieval, SCHEMORA
improves matching accuracy and scalability. Evaluated on the MIMIC-OMOP
benchmark, it establishes new state-of-the-art performance, with gains of 7.49%
in HitRate@5 and 3.75% in HitRate@3 over previous best results. To our
knowledge, this is the first LLM-based schema matching method with an
open-source implementation, accompanied by analysis that underscores the
critical role of retrieval and provides practical guidance on model selection.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [589] [Siamese Neural Network for Label-Efficient Critical Phenomena Prediction in 3D Percolation Models](https://arxiv.org/abs/2507.14159)
*Shanshan Wang,Dian Xu,Jianmin Shen,Feng Gao,Wei Li,Weibing Deng*

Main category: cond-mat.dis-nn

TL;DR: This paper proposes a Siamese Neural Network (SNN) tailored for three-dimensional percolation analysis, achieving high predictive accuracy using fewer labeled samples.


<details>
  <summary>Details</summary>
Motivation: Most percolation analysis machine learning studies oversimplify complex real-world scenarios by focusing on two-dimensional systems.

Method: The authors present a Siamese Neural Network that uses features of the largest cluster as input to predict percolation thresholds and critical exponents in three-dimensional systems.

Result: The method demonstrated high predictive accuracy for 3D site and bond percolation metrics, with errors below 1%, while requiring fewer labeled data than traditional approaches.

Conclusion: The proposed SNN provides a scalable and efficient framework for analyzing high-dimensional critical phenomena, with applications in materials discovery and complex networks.

Abstract: Percolation theory serves as a cornerstone for studying phase transitions and
critical phenomena, with broad implications in statistical physics, materials
science, and complex networks. However, most machine learning frameworks for
percolation analysis have focused on two-dimensional systems, oversimplifying
the spatial correlations and morphological complexity of real-world
three-dimensional materials. To bridge this gap and improve label efficiency
and scalability in 3D systems, we propose a Siamese Neural Network (SNN) that
leverages features of the largest cluster as discriminative input. Our method
achieves high predictive accuracy for both site and bond percolation thresholds
and critical exponents in three dimensions, with sub-1% error margins using
significantly fewer labeled samples than traditional approaches. This work
establishes a robust and data-efficient framework for modeling high-dimensional
critical phenomena, with potential applications in materials discovery and
complex network analysis.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [590] [Asynchronous Collective Tree Exploration: a Distributed Algorithm, and a new Lower Bound](https://arxiv.org/abs/2507.15658)
*Romain Cosson,Laurent Massoulié*

Main category: cs.DS

TL;DR: The paper addresses collective tree exploration by $k$ agents, proposing distributed asynchronous algorithms with competitive guarantees and introducing new lower bounds for the asynchronous setting.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient method for collective tree exploration, overcoming limitations of synchrony or centralization in existing methods.

Method: The researchers devised two distributed asynchronous algorithms to optimize agent movement during exploration. They analyzed these algorithms for competitive guarantees and derived a novel lower bound for the asynchronous problem.

Result: Two algorithms are provided: one with $2n+O(k^2 2^kD)$ moves, featuring linear regret in tree depth ($D$), and another with a competitive ratio of $O(k/\log k)$. A lower bound of $\Omega(\log^2 k)$ is also established.

Conclusion: This study advances collective tree exploration through distributed asynchronous strategies and refines the theoretical understanding, including improved lower bounds.

Abstract: We study the problem of collective tree exploration in which a team of $k$
mobile agents must collectively visit all nodes of an unknown tree in as few
moves as possible. The agents all start from the root and discover adjacent
edges as they progress in the tree. Communication is distributed in the sense
that agents share information by reading and writing on whiteboards located at
all nodes. Movements are asynchronous, in the sense that the speeds of all
agents are controlled by an adversary at all times. All previous competitive
guarantees for collective tree exploration are either distributed but
synchronous, or asynchronous but centralized. In contrast, we present a
distributed asynchronous algorithm that explores any tree of $n$ nodes and
depth $D$ in at most $2n+O(k^2 2^kD)$ moves, i.e., with a regret that is linear
in $D$, and a variant algorithm with a guarantee in $O(k/\log k)(n+kD)$, i.e.,
with a competitive ratio in $O(k/\log k)$. We note that our regret guarantee is
asymptotically optimal (i.e., $1$-competitive) from the perspective of
average-case complexity. We then present a new general lower bound on the
competitive ratio of asynchronous collective tree exploration, in
$\Omega(\log^2 k)$. This lower bound applies to both the distributed and
centralized settings, and improves upon the previous lower bound in
$\Omega(\log k)$.

</details>


### [591] [FAMST: Fast Approximate Minimum Spanning Tree Construction for Large-Scale and High-Dimensional Data](https://arxiv.org/abs/2507.14261)
*Mahmood K. M. Almansoori,Miklos Telek*

Main category: cs.DS

TL;DR: The paper introduces the Fast Approximate Minimum Spanning Tree (FAMST) algorithm, which dramatically speeds up the MST construction for large-scale, high-dimensional datasets while maintaining low approximation errors.


<details>
  <summary>Details</summary>
Motivation: Constructing Minimum Spanning Trees (MSTs) for large-scale and high-dimensional datasets is computationally expensive with traditional methods.

Method: The algorithm employs a three-phase approach: Approximate Nearest Neighbor (ANN) graph construction, ANN-based inter-component connection, and iterative edge refinement steps.

Result: FAMST achieves $
\mathcal{O}(dn \log n)$ time complexity and $
\mathcal{O}(dn + kn)$ space complexity, outperforming traditional \mathcal{O}(n^2)$ approaches, and shows speedups of up to 1000× with low approximation errors.

Conclusion: FAMST successfully enables MST analysis on previously infeasible scales, making it practical for datasets with millions of points and thousands of dimensions.

Abstract: We present Fast Approximate Minimum Spanning Tree (FAMST), a novel algorithm
that addresses the computational challenges of constructing Minimum Spanning
Trees (MSTs) for large-scale and high-dimensional datasets. FAMST utilizes a
three-phase approach: Approximate Nearest Neighbor (ANN) graph construction,
ANN inter-component connection, and iterative edge refinement. For a dataset of
$n$ points in a $d$-dimensional space, FAMST achieves $\mathcal{O}(dn \log n)$
time complexity and $\mathcal{O}(dn + kn)$ space complexity when $k$ nearest
neighbors are considered, which is a significant improvement over the
$\mathcal{O}(n^2)$ time and space complexity of traditional methods.
  Experiments across diverse datasets demonstrate that FAMST achieves
remarkably low approximation errors while providing speedups of up to
1000$\times$ compared to exact MST algorithms. We analyze how the key
hyperparameters, $k$ (neighborhood size) and $\lambda$ (inter-component edges),
affect performance, providing practical guidelines for hyperparameter
selection. FAMST enables MST-based analysis on datasets with millions of points
and thousands of dimensions, extending the applicability of MST techniques to
problem scales previously considered infeasible.

</details>


### [592] [Differentially Private Synthetic Graphs Preserving Triangle-Motif Cuts](https://arxiv.org/abs/2507.14835)
*Pan Peng,Hangyu Xu*

Main category: cs.DS

TL;DR: This paper presents the first differentially private mechanism for synthetic graph generation that approximates triangle-motif sizes in cuts of the input graph, offering both upper and lower bounds on additive errors.


<details>
  <summary>Details</summary>
Motivation: The need to provide privacy-preserving mechanisms for analyzing graph motifs, which play a key role in understanding network structures while protecting sensitive data.

Method: The authors introduce an $(\varepsilon,\delta)$-differentally private algorithm that generates synthetic graphs, analyzing the triangle-motif sizes for cuts while offering theoretical error bounds for accuracy.

Result: The algorithm achieves an additive error of $\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$ for triangle-motif approximations and a proven lower bound of $\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$ for DP algorithms.

Conclusion: This approach offers a significant advancement in DP techniques for graph analysis, with extendability to weighted graphs and other $K_h$-motifs, highlighting the feasibility of privacy-preserving motif analysis.

Abstract: We study the problem of releasing a differentially private (DP) synthetic
graph $G'$ that well approximates the triangle-motif sizes of all cuts of any
given graph $G$, where a motif in general refers to a frequently occurring
subgraph within complex networks. Non-private versions of such graphs have
found applications in diverse fields such as graph clustering, graph
sparsification, and social network analysis. Specifically, we present the first
$(\varepsilon,\delta)$-DP mechanism that, given an input graph $G$ with $n$
vertices, $m$ edges and local sensitivity of triangles $\ell_{3}(G)$, generates
a synthetic graph $G'$ in polynomial time, approximating the triangle-motif
sizes of all cuts $(S,V\setminus S)$ of the input graph $G$ up to an additive
error of $\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$. Additionally, we
provide a lower bound of $\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$ on the
additive error for any DP algorithm that answers the triangle-motif size
queries of all $(S,T)$-cut of $G$. Finally, our algorithm generalizes to
weighted graphs, and our lower bound extends to any $K_h$-motif cut for any
constant $h\geq 2$.

</details>


### [593] [Language Generation in the Limit: Noise, Loss, and Feedback](https://arxiv.org/abs/2507.15319)
*Yannan Bai,Debmalya Panigrahi,Ian Zhang*

Main category: cs.DS

TL;DR: The paper examines language generation in the limit, resolving critical questions about union-closedness and extending understanding to noisy and feedback-driven generation models.


<details>
  <summary>Details</summary>
Motivation: To refine the theory of language generation in the limit, particularly addressing unresolved questions on union-closedness and exploring additional generative models incorporating noise and feedback.

Method: The authors provide counterexamples, construct models, prove equivalences and separations between variants, and analyze feedback-driven generation using theoretical frameworks.

Result: They prove non-uniform and uniform generation unions are not generatable in the limit, establish equivalence between noisy and non-noisy models, show separation within noisy generation, and characterize query-based frameworks.

Conclusion: This paper advances foundational understanding of language generation, clarifying union-closed properties and developing precise insights into noisy and feedback models.

Abstract: Kleinberg and Mullainathan (2024) recently proposed a formal framework called
language generation in the limit and showed that given a sequence of example
strings from an unknown target language drawn from any countable collection, an
algorithm can correctly generate unseen strings from the target language within
finite time. This notion was further refined by Li, Raman, and Tewari (2024),
who defined stricter categories of non-uniform and uniform generation. They
showed that a finite union of uniformly generatable collections is generatable
in the limit, and asked if the same is true for non-uniform generation.
  We begin by resolving the question in the negative: we give a uniformly
generatable collection and a non-uniformly generatable collection whose union
is not generatable in the limit. We then use facets of this construction to
further our understanding of several variants of language generation. The first
two, generation with noise and without samples, were introduced by Raman and
Raman (2025) and Li, Raman, and Tewari (2024) respectively. We show the
equivalence of these models for uniform and non-uniform generation, and provide
a characterization of non-uniform noisy generation. The former paper asked if
there is any separation between noisy and non-noisy generation in the limit --
we show that such a separation exists even with a single noisy string. Finally,
we study the framework of generation with feedback, introduced by Charikar and
Pabbaraju (2025), where the algorithm is strengthened by allowing it to ask
membership queries. We show finite queries add no power, but infinite queries
yield a strictly more powerful model.
  In summary, the results in this paper resolve the union-closedness of
language generation in the limit, and leverage those techniques (and others) to
give precise characterizations for natural variants that incorporate noise,
loss, and feedback.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [594] [A Formal Model of the Economic Impacts of AI Openness Regulation](https://arxiv.org/abs/2507.14193)
*Tori Qiu,Benjamin Laufer,Jon Kleinberg,Hoda Heidari*

Main category: cs.GT

TL;DR: The paper analyzes interactions between general-purpose AI creators and specialists under regulations like the EU AI Act, modeling incentives around open-source definitions.


<details>
  <summary>Details</summary>
Motivation: To address ambiguities in the definition of open-source foundation models and evaluate regulatory standards for incentivizing AI openness.

Method: Development of a theoretical model to study strategic decisions of AI creators and specialists under varying regulatory conditions.

Result: The paper identifies market equilibria and optimal regulatory penalties and thresholds that influence AI openness and strategic decisions.

Conclusion: The baseline performance of an AI model dictates how regulatory measures affect openness strategies, providing guidance for refining AI governance policies.

Abstract: Regulatory frameworks, such as the EU AI Act, encourage openness of
general-purpose AI models by offering legal exemptions for "open-source"
models. Despite this legislative attention on openness, the definition of
open-source foundation models remains ambiguous. This paper models the
strategic interactions among the creator of a general-purpose model (the
generalist) and the entity that fine-tunes the general-purpose model to a
specialized domain or task (the specialist), in response to regulatory
requirements on model openness. We present a stylized model of the regulator's
choice of an open-source definition to evaluate which AI openness standards
will establish appropriate economic incentives for developers. Our results
characterize market equilibria -- specifically, upstream model release
decisions and downstream fine-tuning efforts -- under various openness
regulations and present a range of effective regulatory penalties and
open-source thresholds. Overall, we find the model's baseline performance
determines when increasing the regulatory penalty vs. the open-source threshold
will significantly alter the generalist's release strategy. Our model provides
a theoretical foundation for AI governance decisions around openness and
enables evaluation and refinement of practical open-source policies.

</details>


### [595] [Strategyproofness and Monotone Allocation of Auction in Social Networks](https://arxiv.org/abs/2507.14472)
*Yuhang Guo,Dong Hao,Bin Li,Mingyu Xiao,Bakh Khoussainov*

Main category: cs.GT

TL;DR: This paper introduces two categories of monotone allocation rules for strategyproof network auctions, addressing key problems in the domain.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the lack of a general principle for allocation rules in strategyproof network auctions, which led to failures in previous research.

Method: Authors define and categorize monotone allocation rules (ID-MON and IP-MON), and derive payment rules that are strategyproof and revenue-maximizing.

Result: The paper achieves a characterization of strategyproof payment rules and identifies computationally feasible revenue-maximizing rules.

Conclusion: The findings resolve key obstacles in combinatorial network auctions, ensuring truthfulness and optimal payments for single-minded bidders.

Abstract: Strategyproofness in network auctions requires that bidders not only report
their valuations truthfully, but also do their best to invite neighbours from
the social network. In contrast to canonical auctions, where the value-monotone
allocation in Myerson's Lemma is a cornerstone, a general principle of
allocation rules for strategyproof network auctions is still missing. We show
that, due to the absence of such a principle, even extensions to multi-unit
network auctions with single-unit demand present unexpected difficulties, and
all pioneering researches fail to be strategyproof. For the first time in this
field, we identify two categories of monotone allocation rules on networks:
Invitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity
(IP-MON). They encompass all existing allocation rules of network auctions as
specific instances. For any given ID-MON or IP-MON allocation rule, we
characterize the existence and sufficient conditions for the strategyproof
payment rules, and show that among all such payment rules, the
revenue-maximizing one exists and is computationally feasible. With these
results, the obstacle of combinatorial network auction with single-minded
bidders is now resolved.

</details>


### [596] [Probing EFX via PMMS: (Non-)Existence Results in Discrete Fair Division](https://arxiv.org/abs/2507.14957)
*Jarosław Byrka,Franciszek Malinka,Tomasz Ponitka*

Main category: cs.GT

TL;DR: The paper explores fair division of indivisible items, focusing on EFX and PMMS problems, providing new separations and proving specific cases of existence.


<details>
  <summary>Details</summary>
Motivation: To investigate fair division of indivisible items and address central open questions like EFX and PMMS allocations.

Method: Constructs examples, establishes separations, and proves constructive existence results using algorithms for special valuation cases.

Result: Demonstrates non-existence in specific scenarios, establishes existence under specialized conditions, and provides algorithms for fair allocations.

Conclusion: Establishes new separations between EFX and PMMS, proves existence in specialized settings, and provides concrete methods for achieving fairness.

Abstract: We study the fair division of indivisible items and provide new insights into
the EFX problem, which is widely regarded as the central open question in fair
division, and the PMMS problem, a strictly stronger variant of EFX. Our first
result constructs a three-agent instance with two monotone valuations and one
additive valuation in which no PMMS allocation exists. Since EFX allocations
are known to exist under these assumptions, this establishes a formal
separation between EFX and PMMS.
  We prove existence of fair allocations for three important special cases. We
show that EFX allocations exist for personalized bivalued valuations, where for
each agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value
$v_i(\{g\}) \in \{a_i, b_i\}$ to each good $g$. We establish an analogous
existence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also
prove that PMMS allocations exist for binary-valued MMS-feasible valuations,
where each bundle $S$ has value $v_i(S) \in \{0, 1\}$. Notably, this result
holds even without assuming monotonicity of valuations and thus applies to the
fair division of chores and mixed manna. Finally, we study a class of
valuations called pair-demand valuations, which extend the well-studied
unit-demand valuations to the case where each agent derives value from at most
two items, and we show that PMMS allocations exist in this setting. Our proofs
are constructive, and we provide polynomial-time algorithms for all three
existence results.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [597] [Quantum Programming in Polylogarithmic Time](https://arxiv.org/abs/2507.15415)
*Florent Ferrari,Emmanuel Hainry,Romain Péchoux,Mário Silva*

Main category: cs.LO

TL;DR: The paper introduces a quantum programming language characterizing the FBQPOLYLOG complexity class, demonstrating soundness and completeness as well as a compilation strategy into quantum circuits of polylogarithmic depth.


<details>
  <summary>Details</summary>
Motivation: Exploring a programming-language-based representation for quantum computational complexity classes, specifically FBQPOLYLOG, to connect high-level programming constructs with lower-level circuit implementations.

Method: A quantum programming language with first-order recursive procedures was introduced alongside verification of soundness and completeness. Additionally, programs were compiled into uniform quantum circuit families for comparison with QNC.

Result: The programming language successfully mirrors the FBQPOLYLOG class and compiles into quantum circuits of polylogarithmic depth, establishing FBQPOLYLOG $
subseteq$ QNC.

Conclusion: The study bridges the gap between quantum computational models and programming language theory, confirming that FBQPOLYLOG is properly contained within QNC and providing methods for circuit compilation.

Abstract: Polylogarithmic time delineates a relevant notion of feasibility on several
classical computational models such as Boolean circuits or parallel random
access machines. As far as the quantum paradigm is concerned, this notion
yields the complexity class FBQPOLYLOG of functions approximable in
polylogarithmic time with a quantum random-access Turing machine. We introduce
a quantum programming language with first-order recursive procedures, which
provides the first programming-language-based characterization of FBQPOLYLOG.
Each program computes a function in FBQPOLYLOG (soundness) and, conversely,
each function of this complexity class is computed by a program (completeness).
We also provide a compilation strategy from programs to uniform families of
quantum circuits of polylogarithmic depth and polynomial size, whose set of
computed functions is known as QNC, and recover the well-known separation
result FBQPOLYLOG $\subsetneq$ QNC.

</details>


<div id='math.AT'></div>

# math.AT [[Back]](#toc)

### [598] [Topological Social Choice: Designing a Noise-Robust Polar Distance for Persistence Diagrams](https://arxiv.org/abs/2507.14340)
*Athanasios Andrikopoulos,Nikolaos Sampanis*

Main category: math.AT

TL;DR: This paper introduces a novel metric framework for applying Topological Data Analysis (TDA) to noisy preference data in Social Choice Theory, demonstrating improved robustness and interpretability.


<details>
  <summary>Details</summary>
Motivation: Topological Data Analysis has not been explored in Social Choice Theory, where preference data is geometrically complex and sensitive to noise. The motivation is to address this gap and provide robust, interpretable methods for such data.

Method: The paper proposes a polar coordinate-based distance metric for persistence diagrams, overcoming limitations of classical metrics like bottleneck and Wasserstein. The approach is smooth, differentiable, and compatible with gradient-based learning.

Result: Extensive experiments confirmed the superiority of the proposed metric in robustness tests and supervised learning tasks over traditional methods.

Conclusion: The study provides a novel intersection of TDA and Social Choice Theory, delivering a mathematically grounded, computationally effective method for analyzing voting structures and preference data, with implications for interpretable machine learning in political and economic systems.

Abstract: Topological Data Analysis (TDA) has emerged as a powerful framework for
extracting robust and interpretable features from noisy high-dimensional data.
In the context of Social Choice Theory, where preference profiles and
collective decisions are geometrically rich yet sensitive to perturbations, TDA
remains largely unexplored. This work introduces a novel conceptual bridge
between these domains by proposing a new metric framework for persistence
diagrams tailored to noisy preference data.We define a polar coordinate-based
distance that captures both the magnitude and orientation of topological
features in a smooth and differentiable manner. Our metric addresses key
limitations of classical distances, such as bottleneck and Wasserstein,
including instability under perturbation, lack of continuity, and
incompatibility with gradient-based learning. The resulting formulation offers
improved behavior in both theoretical and applied settings.To the best of our
knowledge, this is the first study to systematically apply persistent homology
to social choice systems, providing a mathematically grounded method for
comparing topological summaries of voting structures and preference dynamics.
We demonstrate the superiority of our approach through extensive experiments,
including robustness tests and supervised learning tasks, and we propose a
modular pipeline for building predictive models from online preference data.
This work contributes a conceptually novel and computationally effective tool
to the emerging interface of topology and decision theory, opening new
directions in interpretable machine learning for political and economic
systems.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [599] [A Hybrid Mixture Approach for Clustering and Characterizing Cancer Data](https://arxiv.org/abs/2507.14380)
*Kazeem Kareem,Fan Dai*

Main category: stat.ME

TL;DR: This paper introduces a computationally efficient model for clustering high-dimensional biomedical data, crucial for disease classification.


<details>
  <summary>Details</summary>
Motivation: Traditional model-based clustering struggles with high-dimensional biomedical data due to computational inefficiencies and algorithmic limitations.

Method: The authors propose a hybrid matrix-free computational scheme based on a Gaussian mixture model with generalized factor analyzers, enabling efficient estimation and characterization of clusters.

Result: Their approach achieves faster convergence and high clustering accuracy, as demonstrated in breast cancer and lymphoma subtype identification using massive data samples.

Conclusion: The proposed method is an effective and scalable solution for clustering high-dimensional biomedical data, with significant implications for disease subtype classification.

Abstract: Model-based clustering is widely used for identifying and distinguishing
types of diseases. However, modern biomedical data coming with high dimensions
make it challenging to perform the model estimation in traditional cluster
analysis. The incorporation of factor analyzer into the mixture model provides
a way to characterize the large set of data features, but the current
estimation method is computationally impractical for massive data due to the
intrinsic slow convergence of the embedded algorithms, and the incapability to
vary the size of the factor analyzers, preventing the implementation of a
generalized mixture of factor analyzers and further characterization of the
data clusters. We propose a hybrid matrix-free computational scheme to
efficiently estimate the clusters and model parameters based on a Gaussian
mixture along with generalized factor analyzers to summarize the large number
of variables using a small set of underlying factors. Our approach outperforms
the existing method with faster convergence while maintaining high clustering
accuracy. Our algorithms are applied to accurately identify and distinguish
types of breast cancer based on large tumor samples, and to provide a
generalized characterization for subtypes of lymphoma using massive gene
records.

</details>


### [600] [Misspecifying non-compensatory as compensatory IRT: analysis of estimated skills and variance](https://arxiv.org/abs/2507.15222)
*Hiroshi Tamano,Hideitsu Hino,Daichi Mochihashi*

Main category: stat.ME

TL;DR: The paper investigates the effects of model misspecification in multidimensional item response theory, specifically focusing on compensatory and non-compensatory models.


<details>
  <summary>Details</summary>
Motivation: The research addresses unresolved issues concerning skill underestimation, overestimation, and parameter variance resulting from model misspecification between compensatory and non-compensatory frameworks in multidimensional item response theory.

Method: A theoretical analysis was conducted to explore the behavior of skill estimation, both underestimation and overestimation, and the variance of estimated parameters under model misspecification.

Result: The study confirmed underestimation of higher skills, discovered overestimation around the origin, and quantified the differences in asymptotic variances of estimated parameters due to model misspecification.

Conclusion: Model misspecification has significant implications for skill estimation and parameter variance. Understanding these effects is crucial for accurate application of multidimensional item response theory.

Abstract: Multidimensional item response theory is a statistical test theory used to
estimate the latent skills of learners and the difficulty levels of problems
based on test results. Both compensatory and non-compensatory models have been
proposed in the literature. Previous studies have revealed the substantial
underestimation of higher skills when the non-compensatory model is
misspecified as the compensatory model. However, the underlying mechanism
behind this phenomenon has not been fully elucidated. It remains unclear
whether overestimation also occurs and whether issues arise regarding the
variance of the estimated parameters. In this paper, we aim to provide a
comprehensive understanding of both underestimation and overestimation through
a theoretical approach. In addition to the previously identified
underestimation of the skills, we newly discover that the overestimation of
skills occurs around the origin. Furthermore, we investigate the extent to
which the asymptotic variance of the estimated parameters differs when
considering model misspecification compared to when it is not taken into
account.

</details>


### [601] [ACS: An interactive framework for conformal selection](https://arxiv.org/abs/2507.15825)
*Yu Gui,Ying Jin,Yash Nair,Zhimei Ren*

Main category: stat.ME

TL;DR: This paper introduces adaptive conformal selection (ACS), a framework ensuring error control during interactive, adaptive data exploration.


<details>
  <summary>Details</summary>
Motivation: The work seeks to improve conformal selection methods to enable adaptive, human-in-the-loop data analysis while maintaining false discovery rate (FDR) control.

Method: The paper builds on conformal selection principles, introducing an adaptive framework for decision-making while controlling errors and providing specific algorithms for various selection purposes.

Result: Extensive simulations and real-world applications, including large language model deployment and drug discovery, confirm the framework's effectiveness.

Conclusion: ACS offers a robust, flexible approach for model-free data selection, enabling adaptive exploration with rigorous FDR guarantees.

Abstract: This paper presents adaptive conformal selection (ACS), an interactive
framework for model-free selection with guaranteed error control. Building on
conformal selection (Jin and Cand\`es, 2023b), ACS generalizes the approach to
support human-in-the-loop adaptive data analysis. Under the ACS framework, we
can partially reuse the data to boost the selection power, make decisions on
the fly while exploring the data, and incorporate new information or
preferences as they arise. The key to ACS is a carefully designed principle
that controls the information available for decision making, allowing the data
analyst to explore the data adaptively while maintaining rigorous control of
the false discovery rate (FDR). Based on the ACS framework, we provide concrete
selection algorithms for various goals, including model update/selection,
diversified selection, and incorporating newly available labeled data. The
effectiveness of ACS is demonstrated through extensive numerical simulations
and real-data applications in large language model (LLM) deployment and drug
discovery.

</details>


### [602] [Robust and Differentially Private PCA for non-Gaussian data](https://arxiv.org/abs/2507.15232)
*Minwoo Kim,Sungkyu Jung*

Main category: stat.ME

TL;DR: This paper introduces a differentially private PCA method suitable for heavy-tailed and contaminated data, outperforming existing approaches in terms of statistical utility.


<details>
  <summary>Details</summary>
Motivation: Existing privacy-preserving PCA methods are limited by unrealistic assumptions, data vulnerability to contamination, computational inefficiency, and dependence on unknown parameters. A more robust and accessible approach is needed.

Method: The method uses the preservation property of eigenvectors in rescaled data under elliptical distributions and applies a bounded transformation. This makes the computation differentially private and robust to contamination.

Result: Theoretical analysis and empirical evaluations show the method effectively recovers the subspace of leading principal components and outperforms current methods in scenarios involving non-Gaussian or contaminated data.

Conclusion: The proposed method offers a robust, efficient, and privacy-preserving PCA approach that is superior in statistical utility, especially for challenging data distributions.

Abstract: Recent advances have sparked significant interest in the development of
privacy-preserving Principal Component Analysis (PCA). However, many existing
approaches rely on restrictive assumptions, such as assuming sub-Gaussian data
or being vulnerable to data contamination. Additionally, some methods are
computationally expensive or depend on unknown model parameters that must be
estimated, limiting their accessibility for data analysts seeking
privacy-preserving PCA. In this paper, we propose a differentially private PCA
method applicable to heavy-tailed and potentially contaminated data. Our
approach leverages the property that the covariance matrix of properly rescaled
data preserves eigenvectors and their order under elliptical distributions,
which include Gaussian and heavy-tailed distributions. By applying a bounded
transformation, we enable straightforward computation of principal components
in a differentially private manner. Additionally, boundedness guarantees
robustness against data contamination. We conduct both theoretical analysis and
empirical evaluations of the proposed method, focusing on its ability to
recover the subspace spanned by the leading principal components. Extensive
numerical experiments demonstrate that our method consistently outperforms
existing approaches in terms of statistical utility, particularly in
non-Gaussian or contaminated data settings.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [603] [KinForm: Kinetics Informed Feature Optimised Representation Models for Enzyme $k_{cat}$ and $K_{M}$ Prediction](https://arxiv.org/abs/2507.14639)
*Saleh Alwer,Ronan Fleming*

Main category: q-bio.QM

TL;DR: KinForm is a machine learning framework that enhances predictions of enzyme kinetics by optimizing protein feature representations with advanced techniques like residue-level embeddings, weighted pooling, PCA, and data balancing.


<details>
  <summary>Details</summary>
Motivation: Many enzymatic kinetic studies lack sufficient experimental data, and existing predictive methods for enzyme kinetics have limitations in accuracy and generalization due to simplistic protein representation approaches.

Method: KinForm integrates residue-level embeddings from multiple transformer models, applies weighted pooling based on binding site probabilities, uses PCA for dimensionality reduction, and implements a data-balancing strategy via oversampling of underrepresented proteins.

Result: KinForm demonstrated improved performance compared to baseline models on two datasets, with higher accuracy particularly in cases of low sequence similarity. It also highlighted the importance of binding-site probability pooling and sequence overlap removal for realistic evaluations.

Conclusion: KinForm represents a significant advance in enzyme kinetic parameter prediction by incorporating sophisticated feature optimization and data balancing, suggesting it as a benchmark for future studies.

Abstract: Kinetic parameters such as the turnover number ($k_{cat}$) and Michaelis
constant ($K_{\mathrm{M}}$) are essential for modelling enzymatic activity but
experimental data remains limited in scale and diversity. Previous methods for
predicting enzyme kinetics typically use mean-pooled residue embeddings from a
single protein language model to represent the protein. We present KinForm, a
machine learning framework designed to improve predictive accuracy and
generalisation for kinetic parameters by optimising protein feature
representations. KinForm combines several residue-level embeddings
(Evolutionary Scale Modeling Cambrian, Evolutionary Scale Modeling 2, and
ProtT5-XL-UniRef50), taken from empirically selected intermediate transformer
layers and applies weighted pooling based on per-residue binding-site
probability. To counter the resulting high dimensionality, we apply
dimensionality reduction using principal--component analysis (PCA) on
concatenated protein features, and rebalance the training data via a
similarity-based oversampling strategy. KinForm outperforms baseline methods on
two benchmark datasets. Improvements are most pronounced in low sequence
similarity bins. We observe improvements from binding-site probability pooling,
intermediate-layer selection, PCA, and oversampling of low-identity proteins.
We also find that removing sequence overlap between folds provides a more
realistic evaluation of generalisation and should be the standard over random
splitting when benchmarking kinetic prediction models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [604] [Metaverse Security and Privacy Research: A Systematic Review](https://arxiv.org/abs/2507.14985)
*Argianto Rahartomo,Leonel Merino,Mohammad Ghafari*

Main category: cs.CR

TL;DR: The paper systematically reviews the literature on security and privacy challenges within metaverse technologies, emphasizing research trends, methodologies, and gaps.


<details>
  <summary>Details</summary>
Motivation: To address the growing security and privacy risks posed by the rapid adoption of metaverse technologies in immersive environments.

Method: A systematic literature review of studies published between 2013 and 2024, categorizing research by methods, properties, and evaluation strategies.

Result: The study highlights a rise in research activity, a focus on user-centered approaches, and common evaluation techniques like benchmarking and qualitative methods, while identifying critical gaps in key areas such as policy compliance and infrastructure security.

Conclusion: The interconnected complexity of technical and human factors in the metaverse necessitates interdisciplinary approaches to improve inclusivity and trustworthiness in these environments.

Abstract: The rapid growth of metaverse technologies, including virtual worlds,
augmented reality, and lifelogging, has accelerated their adoption across
diverse domains. This rise exposes users to significant new security and
privacy challenges due to sociotechnical complexity, pervasive connectivity,
and extensive user data collection in immersive environments. We present a
systematic review of the literature published between 2013 and 2024, offering a
comprehensive analysis of how the research community has addressed
metaverse-related security and privacy issues over the past decade. We organize
the studies by method, examined the security and privacy properties, immersive
components, and evaluation strategies. Our investigation reveals a sharp
increase in research activity in the last five years, a strong focus on
practical and user-centered approaches, and a predominant use of benchmarking,
human experimentation, and qualitative methods. Authentication and
unobservability are the most frequently studied properties. However, critical
gaps remain in areas such as policy compliance, accessibility,
interoperability, and back-end infrastructure security. We emphasize the
intertwined technical complexity and human factors of the metaverse and call
for integrated, interdisciplinary approaches to securing inclusive and
trustworthy immersive environments.

</details>


### [605] [LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries](https://arxiv.org/abs/2507.15058)
*Ian Hardgrove,John D. Hastings*

Main category: cs.CR

TL;DR: The paper introduces LibLMFuzz, an AI-powered framework for automating fuzzing of closed-source libraries, achieving high success in generating drivers autonomously.


<details>
  <summary>Details</summary>
Motivation: To address challenges and costs of fuzzing closed-source and proprietary libraries, especially for stripped binaries, by leveraging AI for automation.

Method: They pair a Large Language Model (LLM) with tools like disassemblers and fuzzers to analyze binaries, plan fuzz strategies, and self-correct errors.

Result: LibLMFuzz achieved 100% API coverage and produced syntactically correct drivers for 558 functions with 75.52% accuracy on first execution, without human intervention.

Conclusion: LLM-augmented tools can significantly lower the cost of fuzzing black-box libraries, opening new avenues for research, such as branch coverage.

Abstract: A fundamental problem in cybersecurity and computer science is determining
whether a program is free of bugs and vulnerabilities. Fuzzing, a popular
approach to discovering vulnerabilities in programs, has several advantages
over alternative strategies, although it has investment costs in the form of
initial setup and continuous maintenance. The choice of fuzzing is further
complicated when only a binary library is available, such as the case of
closed-source and proprietary software. In response, we introduce LibLMFuzz, a
framework that reduces costs associated with fuzzing closed-source libraries by
pairing an agentic Large Language Model (LLM) with a lightweight tool-chain
(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan
fuzz strategies, generate drivers, and iteratively self-repair build or runtime
errors. Tested on four widely-used Linux libraries, LibLMFuzz produced
syntactically correct drivers for all 558 fuzz-able API functions, achieving
100% API coverage with no human intervention. Across the 1601 synthesized
drivers, 75.52% were nominally correct on first execution. The results show
that LLM-augmented middleware holds promise in reducing the costs of fuzzing
black box components and provides a foundation for future research efforts.
Future opportunities exist for research in branch coverage.

</details>


### [606] [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
*Yiran Wu,Mauricio Velazco,Andrew Zhao,Manuel Raúl Meléndez Luján,Srisuma Movva,Yogesh K Roy,Quang Nguyen,Roberto Rodriguez,Qingyun Wu,Michael Albada,Julia Kiseleva,Anand Mudgerikar*

Main category: cs.CR

TL;DR: ExCyTIn-Bench introduces a benchmark for evaluating Large Language Model (LLM) agents on cybersecurity threat investigation tasks using real-world security data and simulated multi-step attack scenarios.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for automated systems to assist security analysts who deal with complex, multi-faceted cyber threat investigation tasks, and explores the potential of LLM-based agents for this purpose.

Method: The authors create a dataset sourced from a controlled Azure tenant, simulate eight multi-step attacks, extract logs, and generate security-related questions based on investigation graphs to evaluate the agent's performance.

Result: Experiments reveal the task’s difficulty, with evaluated models achieving low average rewards (maximum of 0.368), highlighting significant room for improvement in LLM-based threat investigation.

Conclusion: ExCyTIn-Bench offers a reusable, extensible pipeline for both benchmarking and training LLM agents in cyber threat investigation, laying a foundation for more advanced research and development in this domain.

Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on
the task of Cyber Threat Investigation through security questions derived from
investigation graphs. Real-world security analysts must sift through a large
number of heterogeneous alert signals and security logs, follow multi-hop
chains of evidence, and compile an incident report. With the developments of
LLMs, building LLM-based agents for automatic thread investigation is a
promising direction. To assist the development and evaluation of LLM agents, we
construct a dataset from a controlled Azure tenant that covers 8 simulated
real-world multi-step attacks, 57 log tables from Microsoft Sentinel and
related services, and 589 automatically generated questions. We leverage
security logs extracted with expert-crafted detection logic to build threat
investigation graphs, and then generate questions with LLMs using paired nodes
on the graph, taking the start node as background context and the end node as
answer. Anchoring each question to these explicit nodes and edges not only
provides automatic, explainable ground truth answers but also makes the
pipeline reusable and readily extensible to new logs. This also enables the
automatic generation of procedural tasks with verifiable rewards, which can be
naturally extended to training agents via reinforcement learning. Our
comprehensive experiments with different models confirm the difficulty of the
task: with the base setting, the average reward across all evaluated models is
0.249, and the best achieved is 0.368, leaving substantial headroom for future
research. Code and data are coming soon!

</details>


### [607] [PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training](https://arxiv.org/abs/2507.14202)
*Pengfei Du*

Main category: cs.CR

TL;DR: The paper proposes a security alignment framework for large language models (LLMs) that eliminates the need for Process Reward Models (PRMs), achieving robustness and efficiency through adversarial training and automated red teaming, while cutting computational costs by 61%.


<details>
  <summary>Details</summary>
Motivation: LLMs, despite their capabilities, present significant security risks that hinder their use in critical applications. Current methods rely on PRMs, which are computationally intensive and inefficient, necessitating a more scalable and efficient security solution.

Method: The approach removes reliance on PRMs by utilizing automated red teaming, adversarial training with curriculum learning, and adaptive regularization mechanisms. It also employs attack strategies such as genetic algorithm optimization, multi-agent simulation, and advanced prompt mutation.

Result: The proposed method improves security alignment performance over PRM-based systems while reducing computational costs by 61%. It has been experimentally validated on five cutting-edge LLMs.

Conclusion: The presented framework offers a scalable, efficient, and robust method for LLM security alignment, enabling broader access to security measures and paving the way to address future adversarial threats.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse applications, yet they pose significant security risks that threaten
their safe deployment in critical domains. Current security alignment
methodologies predominantly rely on Process Reward Models (PRMs) to evaluate
intermediate reasoning steps, introducing substantial computational overhead
and scalability constraints. This paper presents a novel PRM-free security
alignment framework that leverages automated red teaming and adversarial
training to achieve robust security guarantees while maintaining computational
efficiency. Our approach systematically identifies vulnerabilities through
sophisticated attack strategies including genetic algorithm optimization,
multi-agent simulation, and advanced prompt mutation techniques. The framework
enhances model robustness via targeted adversarial training with curriculum
learning and adaptive regularization mechanisms. Comprehensive experimental
evaluation across five state-of-the-art LLMs demonstrates that our method
achieves superior security alignment performance compared to PRM-based
approaches while reducing computational costs by 61\%. The framework
incorporates transparent reporting and continuous audit mechanisms that enable
iterative security improvement and regulatory compliance. Our contributions
advance the field of efficient LLM security alignment by democratizing access
to robust security measures for resource-constrained organizations and
providing a scalable foundation for addressing evolving adversarial threats.

</details>


### [608] [Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design](https://arxiv.org/abs/2507.14207)
*Richard M. Charles,James H. Curry,Richard B. Charles*

Main category: cs.CR

TL;DR: The paper illustrates vulnerabilities in LLMs, particularly GPT-3.5 and GPT-4, where students could misuse prompts to generate unsafe outputs despite content moderation systems. It introduces TrojanPromptGuard (TPG) as a safeguard tool.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the need to address both the opportunities and risks of integrating LLMs in K-12 education, particularly focusing on students potentially misusing prompts to bypass safety mechanisms.

Method: The study employs a systematic experiment with simulated K-12 queries and multi-turn dialogues to identify vulnerabilities in LLM interactions, complemented by the design of the TrojanPromptGuard tool.

Result: The research reveals critical vulnerabilities in GPT-3.5 and GPT-4, demonstrates how students can Trojanize prompts, and introduces TrojanPromptGuard as a prototype for detection and mitigation.

Conclusion: Insights from the study aim to guide AI safety researchers and educational technologists in safely deploying LLMs in educational settings for better control and risk management.

Abstract: The integration of Large Language Models (LLMs) in K--12 education offers
both transformative opportunities and emerging risks. This study explores how
students may Trojanize prompts to elicit unsafe or unintended outputs from
LLMs, bypassing established content moderation systems with safety guardrils.
Through a systematic experiment involving simulated K--12 queries and
multi-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This
paper presents our experimental design, detailed findings, and a prototype
tool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized
educational prompts. These insights aim to inform both AI safety researchers
and educational technologists on the safe deployment of LLMs for educators.

</details>


### [609] [Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification](https://arxiv.org/abs/2507.14223)
*Wen-Cheng Chung,Shu-Ting Huang,Hao-Ting Pai*

Main category: cs.CR

TL;DR: The paper presents IG-MD, a technique to enhance precision in explainable intrusion detection systems (IDS) while maintaining transparency and achieving high recall across multiple datasets.


<details>
  <summary>Details</summary>
Motivation: To address the shortcomings of existing explainable AI (XAI) pipelines in intrusion detection systems, which often lead to partial or misleading insights, and to provide interpretable, auditable, and generalizable rules in network traffic analysis.

Method: The proposed method, IG-MD, uses Multi-Granular Discretization to represent continuous features at various Gaussian-based resolutions, building on the existing Interpretable Generalization (IG) mechanism.

Result: IG-MD improves precision by 4 or more percentage points on UKM-IDS20 across different train-test splits while maintaining near-perfect recall of approximately 1.0.

Conclusion: IG-MD offers a significant improvement in precision without sacrificing model interpretability or recall, enabling scalable, interpretation-ready models for intrusion detection across various domains.

Abstract: Explainable intrusion detection systems (IDS) are now recognized as essential
for mission-critical networks, yet most "XAI" pipelines still bolt an
approximate explainer onto an opaque classifier, leaving analysts with partial
and sometimes misleading insights. The Interpretable Generalization (IG)
mechanism, published in IEEE Transactions on Information Forensics and
Security, eliminates that bottleneck by learning coherent patterns - feature
combinations unique to benign or malicious traffic - and turning them into
fully auditable rules. IG already delivers outstanding precision, recall, and
AUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the
data. To raise precision further without sacrificing transparency, we introduce
Multi-Granular Discretization (IG-MD), which represents every continuous
feature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts
precision by greater than or equal to 4 percentage points across all nine
train-test splits while preserving recall approximately equal to 1.0,
demonstrating that a single interpretation-ready model can scale across domains
without bespoke tuning.

</details>


### [610] [Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack](https://arxiv.org/abs/2507.14248)
*Eldor Abdukhamidov,Mohammed Abuhamad,Simon S. Woo,Hyoungshick Kim,Tamer Abuhmed*

Main category: cs.CR

TL;DR: The paper introduces AdViT, an attack that generates adversarial examples that mislead vision transformers (ViT) and their interpretation models with high success and confidence rates.


<details>
  <summary>Details</summary>
Motivation: To explore and address the vulnerability of vision transformer (ViT) models and their coupled interpretation systems to adversarial attacks, which could have critical implications in security-sensitive applications.

Method: The authors propose "AdViT," an attack strategy designed to generate adversarial examples that deceive both ViT models and their interpretation models. Extensive experiments were conducted across different transformer models to assess the success of AdViT.

Result: AdViT achieved a 100% attack success rate in both white-box and black-box scenarios. It reached up to 98% misclassification confidence in white-box scenarios and up to 76% in black-box scenarios, while generating plausible interpretations to evade detection.

Conclusion: The study highlights the susceptibility of ViT models and their interpretation counterparts to adversarial attacks, emphasizing the need for improved defenses for these systems in security-critical domains.

Abstract: Vision transformer (ViT) models, when coupled with interpretation models, are
regarded as secure and challenging to deceive, making them well-suited for
security-critical domains such as medical applications, autonomous vehicles,
drones, and robotics. However, successful attacks on these systems can lead to
severe consequences. Recent research on threats targeting ViT models primarily
focuses on generating the smallest adversarial perturbations that can deceive
the models with high confidence, without considering their impact on model
interpretations. Nevertheless, the use of interpretation models can effectively
assist in detecting adversarial examples. This study investigates the
vulnerability of transformer models to adversarial attacks, even when combined
with interpretation models. We propose an attack called "AdViT" that generates
adversarial examples capable of misleading both a given transformer model and
its coupled interpretation model. Through extensive experiments on various
transformer models and two transformer-based interpreters, we demonstrate that
AdViT achieves a 100% attack success rate in both white-box and black-box
scenarios. In white-box scenarios, it reaches up to 98% misclassification
confidence, while in black-box scenarios, it reaches up to 76%
misclassification confidence. Remarkably, AdViT consistently generates accurate
interpretations in both scenarios, making the adversarial examples more
difficult to detect.

</details>


### [611] [Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives](https://arxiv.org/abs/2507.14519)
*Wenxuan Zeng,Tianshi Xu,Yi Chen,Yifan Zhou,Mingzhe Zhang,Jin Tan,Cheng Hong,Meng Li*

Main category: cs.CR

TL;DR: This paper surveys advances in making privacy-preserving machine learning (PPML) faster and more scalable. It categorizes optimizations into protocol, model, and system levels and outlines future research directions.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the efficiency and scalability challenges of privacy-preserving machine learning (PPML), which can be slow and resource-intensive compared to plaintext machine learning.

Method: The authors systematically review and categorize existing PPML studies into protocol, model, and system levels. They analyze the technical progress and provide qualitative and quantitative comparisons, along with insights for improvements.

Result: A comprehensive categorization and analysis of PPML optimizations was provided. The paper demonstrated the importance of cross-level integration and shared a GitHub repository for continuous tracking of advancements.

Conclusion: Current PPML approaches show promise but require better integration across protocol, model, and system levels for scalability and efficiency improvements. This survey hopes to guide future research directions.

Abstract: Privacy-preserving machine learning (PPML) based on cryptographic protocols
has emerged as a promising paradigm to protect user data privacy in cloud-based
machine learning services. While it achieves formal privacy protection, PPML
often incurs significant efficiency and scalability costs due to orders of
magnitude overhead compared to the plaintext counterpart. Therefore, there has
been a considerable focus on mitigating the efficiency gap for PPML. In this
survey, we provide a comprehensive and systematic review of recent PPML studies
with a focus on cross-level optimizations. Specifically, we categorize existing
papers into protocol level, model level, and system level, and review progress
at each level. We also provide qualitative and quantitative comparisons of
existing works with technical insights, based on which we discuss future
research directions and highlight the necessity of integrating optimizations
across protocol, model, and system levels. We hope this survey can provide an
overarching understanding of existing approaches and potentially inspire future
breakthroughs in the PPML field. As the field is evolving fast, we also provide
a public GitHub repository to continuously track the developments, which is
available at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.

</details>


### [612] [VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning](https://arxiv.org/abs/2507.14625)
*Juntao Tan,Anran Li,Quanchao Liu,Peng Ran,Lan Zhang*

Main category: cs.CR

TL;DR: The paper presents VTarbel, a minimal-knowledge adversarial attack framework targeting label misclassification in Vertical Federated Learning (VFL), effectively evading anomaly detectors and defenses.


<details>
  <summary>Details</summary>
Motivation: To address the lack of practical and effective targeted label attack methods in VFL that operate under minimal-knowledge assumptions and overcome the limitations of existing methods that fail against deployed anomaly detectors.

Method: VTarbel uses a two-stage approach: in the preparation stage, it leverages expressive samples to estimate models and detectors; in the attack stage, it crafts adversarial inputs through gradient-based perturbations to induce targeted misclassifications while evading detection.

Result: VTarbel consistently outperformed existing baselines, evaded anomaly detectors, and remained effective even against multiple privacy-preserving defenses across diverse datasets, models, and detectors.

Conclusion: The findings highlight critical security vulnerabilities in VFL frameworks and emphasize the need for robust, attack-aware defenses to mitigate such threats.

Abstract: Vertical federated learning (VFL) enables multiple parties with disjoint
features to collaboratively train models without sharing raw data. While
privacy vulnerabilities of VFL are extensively-studied, its security
threats-particularly targeted label attacks-remain underexplored. In such
attacks, a passive party perturbs inputs at inference to force
misclassification into adversary-chosen labels. Existing methods rely on
unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore
anomaly detectors deployed in real-world systems. To bridge this gap, we
introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly
designed to evade detector-enhanced VFL inference. During the preparation
stage, the attacker selects a minimal set of high-expressiveness samples (via
maximum mean discrepancy), submits them through VFL protocol to collect
predicted labels, and uses these pseudo-labels to train estimated detector and
surrogate model on local features. In attack stage, these models guide
gradient-based perturbations of remaining samples, crafting adversarial
instances that induce targeted misclassifications and evade detection. We
implement VTarbel and evaluate it against four model architectures, seven
multimodal datasets, and two anomaly detectors. Across all settings, VTarbel
outperforms four state-of-the-art baselines, evades detection, and retains
effective against three representative privacy-preserving defenses. These
results reveal critical security blind spots in current VFL deployments and
underscore urgent need for robust, attack-aware defenses.

</details>


### [613] [VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking](https://arxiv.org/abs/2507.14629)
*Juntao Tan,Lan Zhang,Zhonghao Hu,Kai Yang,Peng Ran,Bo Li*

Main category: cs.CR

TL;DR: Vertical federated learning (VFL) faces vulnerabilities from label inference attacks, with the Model Completion (MC) attack being the most potent. VMask, a novel framework using secret sharing and targeted layer masking, provides strong defense against MC attacks with minimal accuracy drop and efficient runtime.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address weaknesses in privacy mechanisms in VFL systems that allow label inference attacks, particularly the potent MC attack, without compromising model accuracy or computational efficiency.

Method: The researchers propose using layer masking via the secret sharing (SS) technique, selectively applying layer masking to critical layers to disrupt correlation. They introduce a tunable privacy budget for flexible control over protection levels.

Result: VMask successfully reduces label inference accuracy to random guessing levels, ensures negligible impact on model accuracy (e.g., 0.09% reduction in Transformer models), and runs efficiently—up to 60,846 times faster than cryptography-based methods, marginally exceeding runtime of standard VFL.

Conclusion: VMask demonstrates an optimal privacy-utility trade-off, effectively safeguarding label privacy against MC attacks while preserving model performance and maintaining reasonable runtime overheads.

Abstract: Though vertical federated learning (VFL) is generally considered to be
privacy-preserving, recent studies have shown that VFL system is vulnerable to
label inference attacks originating from various attack surfaces. Among these
attacks, the model completion (MC) attack is currently the most powerful one.
Existing defense methods against it either sacrifice model accuracy or incur
impractical computational overhead. In this paper, we propose VMask, a novel
label privacy protection framework designed to defend against MC attack from
the perspective of layer masking. Our key insight is to disrupt the strong
correlation between input data and intermediate outputs by applying the secret
sharing (SS) technique to mask layer parameters in the attacker's model. We
devise a strategy for selecting critical layers to mask, reducing the overhead
that would arise from naively applying SS to the entire model. Moreover, VMask
is the first framework to offer a tunable privacy budget to defenders, allowing
for flexible control over the levels of label privacy according to actual
requirements. We built a VFL system, implemented VMask on it, and extensively
evaluated it using five model architectures and 13 datasets with different
modalities, comparing it to 12 other defense methods. The results demonstrate
that VMask achieves the best privacy-utility trade-off, successfully thwarting
the MC attack (reducing the label inference accuracy to a random guessing
level) while preserving model performance (e.g., in Transformer-based model,
the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up
to 60,846 times faster than cryptography-based methods, and it only marginally
exceeds that of standard VFL by 1.8 times in a large Transformer-based model,
which is generally acceptable.

</details>


### [614] [Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree](https://arxiv.org/abs/2507.14799)
*Sam Johnson,Viet Pham,Thai Le*

Main category: cs.CR

TL;DR: This paper highlights the vulnerability of LLM-powered web navigation agents to Indirect Prompt Injection (IPI) attacks. Authors showcase high attack success rates using adversarial triggers against real websites.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the growing adoption of LLM-driven web agents and their security challenges. It tries to uncover the risks posed by adversarial attacks and the lack of robust security measures.

Method: The authors employed the Greedy Coordinate Gradient (GCG) algorithm and the Browser Gym framework powered by the Llama-3.1 model to implement and test attacks on real-world websites.

Result: They achieved high success rates in manipulating agent behavior, demonstrating both targeted and general attack capabilities. Examples included credential theft and forced ad interactions.

Conclusion: There exists significant security risks associated with autonomous web agents powered by LLMs. The paper calls for enhanced security mechanisms to prevent such vulnerabilities.

Abstract: This work demonstrates that LLM-based web navigation agents offer powerful
automation capabilities but are vulnerable to Indirect Prompt Injection (IPI)
attacks. We show that adversaries can embed universal adversarial triggers in
webpage HTML to hijack agent behavior that utilizes the accessibility tree to
parse HTML, causing unintended or malicious actions. Using the Greedy
Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by
Llama-3.1, our system demonstrates high success rates across real websites in
both targeted and general attacks, including login credential exfiltration and
forced ad clicks. Our empirical results highlight critical security risks and
the need for stronger defenses as LLM-driven autonomous web agents become more
widely adopted. The system software
(https://github.com/sej2020/manipulating-web-agents) is released under the MIT
License, with an accompanying publicly available demo website
(http://lethaiq.github.io/attack-web-llm-agent).

</details>


### [615] [A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption](https://arxiv.org/abs/2507.14853)
*Khoa Nguyen,Tanveer Khan,Antonis Michalas*

Main category: cs.CR

TL;DR: The paper investigates Hybrid Homomorphic Encryption (HHE) to enhance Federated Learning by addressing privacy and communication overhead challenges.


<details>
  <summary>Details</summary>
Motivation: The motivation is to resolve the challenges of high communication overhead and data privacy in Federated Learning systems.

Method: The paper integrates Hybrid Homomorphic Encryption (HHE) which combines symmetric encryption and homomorphic encryption into Federated Learning.

Result: This integration effectively addresses privacy concerns and communication inefficiencies in Federated Learning.

Conclusion: Using Hybrid Homomorphic Encryption in Federated Learning systems enables scalable and secure decentralized learning.

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data, making it a promising approach for privacy-sensitive domains. Despite
its potential, FL faces significant challenges, particularly in terms of
communication overhead and data privacy. Privacy-preserving Techniques (PPTs)
such as Homomorphic Encryption (HE) have been used to mitigate these concerns.
However, these techniques introduce substantial computational and communication
costs, limiting their practical deployment. In this work, we explore how Hybrid
Homomorphic Encryption (HHE), a cryptographic protocol that combines symmetric
encryption with HE, can be effectively integrated with FL to address both
communication and privacy challenges, paving the way for scalable and secure
decentralized learning system.

</details>


### [616] [PromptArmor: Simple yet Effective Prompt Injection Defenses](https://arxiv.org/abs/2507.15219)
*Tianneng Shi,Kaijie Zhu,Zhun Wang,Yuqi Jia,Will Cai,Weida Liang,Haonan Wang,Hend Alzahrani,Joshua Lu,Kenji Kawaguchi,Basel Alomair,Xuandong Zhao,William Yang Wang,Neil Gong,Wenbo Guo,Dawn Song*

Main category: cs.CR

TL;DR: The paper introduces PromptArmor, a defense mechanism for protecting large language model (LLM) agents from prompt injection attacks by detecting and removing hostile prompts.


<details>
  <summary>Details</summary>
Motivation: Prompt injection attacks exploit vulnerabilities in LLM agents, allowing attackers to override user commands. There's a need for robust methods to defend against such attacks.

Method: PromptArmor uses an external LLM to scan and remove suspect prompts from the agent's input via detection techniques.

Result: The defense achieved a false positive and negative rate below 1% on the AgentDojo benchmark, drastically reducing attack success rates to below 1%.

Conclusion: PromptArmor is a reliable and effective method for mitigating prompt injection attacks and is recommended as a baseline defense in future work.

Abstract: Despite their potential, recent research has demonstrated that LLM agents are
vulnerable to prompt injection attacks, where malicious prompts are injected
into the agent's input, causing it to perform an attacker-specified task rather
than the intended task provided by the user. In this paper, we present
PromptArmor, a simple yet effective defense against prompt injection attacks.
Specifically, PromptArmor prompts an off-the-shelf LLM to detect and remove
potential injected prompts from the input before the agent processes it. Our
results show that PromptArmor can accurately identify and remove injected
prompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves
both a false positive rate and a false negative rate below 1% on the AgentDojo
benchmark. Moreover, after removing injected prompts with PromptArmor, the
attack success rate drops to below 1%. We also demonstrate PromptArmor's
effectiveness against adaptive attacks and explore different strategies for
prompting an LLM. We recommend that PromptArmor be adopted as a standard
baseline for evaluating new defenses against prompt injection attacks.

</details>


### [617] [PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants](https://arxiv.org/abs/2507.15393)
*Ruofan Liu,Yun Lin,Silas Yeo Shuen Yu,Xiwen Teoh,Zhenkai Liang,Jin Song Dong*

Main category: cs.CR

TL;DR: The paper introduces PiMRef, a reference-based phishing email detector that outperforms traditional detection methods by reframing phishing detection as an identity fact-checking task.


<details>
  <summary>Details</summary>
Motivation: Traditional phishing detectors struggle against the evolving and increasingly sophisticated nature of phishing emails, especially those crafted using large language models (LLMs).

Method: The authors propose PiMRef, which detects phishing attempts by checking the sender's identity claims and verifying them against a knowledge base, alongside identifying persuasive call-to-action prompts.

Result: PiMRef achieved 92.1% precision and 87.9% recall in a real-world evaluation, significantly outperforming existing methods on phishing detection benchmarks.

Conclusion: PiMRef offers an effective and efficient solution to combating advanced phishing attacks, leveraging identity-based fact-checking to enhance detection performance.

Abstract: Phishing emails are a critical component of the cybercrime kill chain due to
their wide reach and low cost. Their ever-evolving nature renders traditional
rule-based and feature-engineered detectors ineffective in the ongoing arms
race between attackers and defenders. The rise of large language models (LLMs)
further exacerbates the threat, enabling attackers to craft highly convincing
phishing emails at minimal cost.
  This work demonstrates that LLMs can generate psychologically persuasive
phishing emails tailored to victim profiles, successfully bypassing nearly all
commercial and academic detectors. To defend against such threats, we propose
PiMRef, the first reference-based phishing email detector that leverages
knowledge-based invariants. Our core insight is that persuasive phishing emails
often contain disprovable identity claims, which contradict real-world facts.
PiMRef reframes phishing detection as an identity fact-checking task. Given an
email, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the
legitimacy of the sender's domain against a predefined knowledge base, and
(iii) detects call-to-action prompts that push user engagement. Contradictory
claims are flagged as phishing indicators and serve as human-understandable
explanations.
  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,
PiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks
like Nazario and PhishPot. In a real-world evaluation of 10,183 emails across
five university accounts over three years, PiMRef achieved 92.1% precision,
87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art
in both effectiveness and efficiency.

</details>


### [618] [Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems](https://arxiv.org/abs/2507.15613)
*Andrii Balashov,Olena Ponomarova,Xiaohua Zhai*

Main category: cs.CR

TL;DR: The paper studies multi-stage prompt inference attacks on Large Language Models in enterprise settings and proposes defenses like statistical anomaly detection, prompt sanitization, and architectural changes.


<details>
  <summary>Details</summary>
Motivation: Address the novel security threats, specifically multi-stage prompt inference attacks, faced by LLMs integrated with private corporate data in enterprise environments.

Method: The paper simulates realistic attack scenarios, develops a formal threat model analyzed via probability theory and information-theoretic bounds, and evaluates various defense mechanisms including anomaly detection and differential privacy.

Result: The paper shows that multi-turn attacks reliably exfiltrate sensitive data even with standard defenses and demonstrates the effectiveness of proposed measures like 'spotlighting' and anomaly detection in reducing attack success rates.

Conclusion: Securing enterprise LLMs requires a holistic and multi-stage approach, moving beyond single-turn defenses to achieve better protection against complex attacks.

Abstract: Large Language Models (LLMs) deployed in enterprise settings (e.g., as
Microsoft 365 Copilot) face novel security challenges. One critical threat is
prompt inference attacks: adversaries chain together seemingly benign prompts
to gradually extract confidential data. In this paper, we present a
comprehensive study of multi-stage prompt inference attacks in an enterprise
LLM context. We simulate realistic attack scenarios where an attacker uses
mild-mannered queries and indirect prompt injections to exploit an LLM
integrated with private corporate data. We develop a formal threat model for
these multi-turn inference attacks and analyze them using probability theory,
optimization frameworks, and information-theoretic leakage bounds. The attacks
are shown to reliably exfiltrate sensitive information from the LLM's context
(e.g., internal SharePoint documents or emails), even when standard safety
measures are in place.
  We propose and evaluate defenses to counter such attacks, including
statistical anomaly detection, fine-grained access control, prompt sanitization
techniques, and architectural modifications to LLM deployment. Each defense is
supported by mathematical analysis or experimental simulation. For example, we
derive bounds on information leakage under differential privacy-based training
and demonstrate an anomaly detection method that flags multi-turn attacks with
high AUC. We also introduce an approach called "spotlighting" that uses input
transformations to isolate untrusted prompt content, reducing attack success by
an order of magnitude. Finally, we provide a formal proof of concept and
empirical validation for a combined defense-in-depth strategy. Our work
highlights that securing LLMs in enterprise settings requires moving beyond
single-turn prompt filtering toward a holistic, multi-stage perspective on both
attacks and defenses.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [619] [Prompt-aware of Frame Sampling for Efficient Text-Video Retrieval](https://arxiv.org/abs/2507.15491)
*Deyu Zhang,Tingting Long,Jinrui Zhang,Ligeng Chen,Ju Ren,Yaoxue Zhang*

Main category: cs.MM

TL;DR: ProCLIP offers a method for efficient text-video retrieval, reducing computational cost significantly while maintaining high accuracy by novel frame sampling and pruning strategies.


<details>
  <summary>Details</summary>
Motivation: Existing text-video retrieval methods struggle to balance high accuracy with computational efficiency, particularly on edge devices.

Method: ProCLIP introduces a prompt-aware frame sampling strategy leveraging textual prompts for dynamic frame selection and a two-stage pruning strategy to improve efficiency and accuracy.

Result: Experiments show ProCLIP achieves 75.3% latency reduction compared to baselines, with competitive accuracy of R@1=49.0 on the MSR-VTT dataset.

Conclusion: ProCLIP effectively addresses the inefficiencies of prior text-video retrieval methods, offering a robust and computationally efficient solution with state-of-the-art performance.

Abstract: Enabling efficient text-video retrieval on edge-end devices is critical for
real-world applications. Yet, existing methods face a critical challenge in
balancing accuracy and computational efficiency: uniform frame sampling methods
ensure content coverage but incur prohibitive computational costs, while
salient-frame sampling methods reduce overhead but suffer from query-agnostic
frame selection that biases retrieval results. To address this, we propose
ProCLIP, a user-centric framework that achieves state-of-the-art accuracy with
significantly improved efficiency. We design a prompt-aware frame sampling
strategy that dynamically guides lightweight feature extractors using textual
prompts to select semantically relevant frames, overcoming the limitations of
existing salient-frame sampling methods which rely on static, query-agnostic
selection criteria. Moreover, we adopt a two-stage candidate pruning strategy
that combines rapid coarse filtering via a lightweight module with CLIP-powered
fine-grained re-ranking, enhancing retrieval efficiency while preserving
accuracy. Experiments across benchmarks show ProCLIP achieves 75.3% latency
reduction versus baselines while maintaining competitive accuracy, i.e.,
R@1=49.0 in MSR-VTT dataset. Code is available at
https://github.com/tiffylong/ProCLIP.

</details>


<div id='math.RT'></div>

# math.RT [[Back]](#toc)

### [620] [Partial Symmetry Enforced Attention Decomposition (PSEAD): A Group-Theoretic Framework for Equivariant Transformers in Biological Systems](https://arxiv.org/abs/2507.14908)
*Daniel Ayomide Olanrewaju*

Main category: math.RT

TL;DR: The paper presents PSEAD, a framework incorporating symmetry into self-attention within Transformer models for enhanced biological data analysis.


<details>
  <summary>Details</summary>
Motivation: To integrate local symmetry awareness in self-attention mechanisms for better processing of biological data with symmetric properties.

Method: PSEAD formalizes local permutation subgroup actions to decompose attention into orthogonal components aligned with irreducible representations of the symmetry.

Result: PSEAD improves generalization for symmetric biological motifs, increases interpretability, and boosts computational efficiency.

Conclusion: This framework establishes symmetry-informed AI models for static and dynamic biological data applications, advancing fields like protein folding and drug discovery.

Abstract: This research introduces the Theory of Partial Symmetry Enforced Attention
Decomposition (PSEAD), a new and rigorous group-theoretic framework designed to
seamlessly integrate local symmetry awareness into the core architecture of
self-attention mechanisms within Transformer models. We formalize the concept
of local permutation subgroup actions on windows of biological data, proving
that under such actions, the attention mechanism naturally decomposes into a
direct sum of orthogonal irreducible components. Critically, these components
are intrinsically aligned with the irreducible representations of the acting
permutation subgroup, thereby providing a powerful mathematical basis for
disentangling symmetric and asymmetric features. We show that PSEAD offers
substantial advantages. These include enhanced generalization capabilities to
novel biological motifs exhibiting similar partial symmetries, unprecedented
interpretability by allowing direct visualization and analysis of attention
contributions from different symmetry channels, and significant computational
efficiency gains by focusing representational capacity on relevant symmetric
subspaces. Beyond static data analysis, we extend PSEAD's applicability to
dynamic biological processes within reinforcement learning paradigms,
showcasing its potential to accelerate the discovery and optimization of
biologically meaningful policies in complex environments like protein folding
and drug discovery. This work lays the groundwork for a new generation of
biologically informed, symmetry-aware artificial intelligence models.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [621] [Learning Stochastic Hamiltonian Systems via Stochastic Generating Function Neural Network](https://arxiv.org/abs/2507.14467)
*Chen Chen,Lijin Wang,Yanzhao Cao,Xupeng Cheng*

Main category: math.DS

TL;DR: The paper introduces a novel neural network model, SGFNN, for learning stochastic Hamiltonian systems (SHSs) that preserves their symplectic structure and provides superior accuracy in predictions compared to previous methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning stochastic Hamiltonian systems efficiently while preserving their symplectic structure, which is crucial for accurate long-term predictions.

Method: The SGFNN model employs an autoencoder framework with an encoder to identify randomness in latent systems and a decoder to detect the stochastic generating function, ensuring symplectic predictions.

Result: The SGFNN demonstrates higher accuracy than the benchmark sFML model across various metrics, particularly for long-term predictions, while retaining the symplectic structure of SHSs during numerical experiments.

Conclusion: SGFNN provides an effective and accurate approach for learning and predicting stochastic Hamiltonian systems, outperforming existing models while preserving critical structural properties.

Abstract: In this paper we propose a novel neural network model for learning stochastic
Hamiltonian systems (SHSs) from observational data, termed the stochastic
generating function neural network (SGFNN). SGFNN preserves symplectic
structure of the underlying stochastic Hamiltonian system and produces
symplectic predictions. Our model utilizes the autoencoder framework to
identify the randomness of the latent system by the encoder network, and
detects the stochastic generating function of the system through the decoder
network based on the random variables extracted from the encoder. Symplectic
predictions can then be generated by the stochastic generating function.
Numerical experiments are performed on several stochastic Hamiltonian systems,
varying from additive to multiplicative, and from separable to non-separable
SHSs with single or multiple noises. Compared with the benchmark stochastic
flow map learning (sFML) neural network, our SGFNN model exhibits higher
accuracy across various prediction metrics, especially in long-term
predictions, with the property of maintaining the symplectic structure of the
underlying SHSs.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [622] [The hunt for new pulsating ultraluminous X-ray sources: a clustering approach](https://arxiv.org/abs/2507.15032)
*Nicolò Oreste Pinciroli Vago,Roberta Amato,Matteo Imbrogno,GianLuca Israel,Andrea Belfiore,Konstantinos Kovlakas,Piero Fraternali,Mario Pasquato*

Main category: astro-ph.HE

TL;DR: This study used an AI method to identify candidate ultraluminous X-ray sources (ULXs) as pulsating ULXs (PULXs) based on XMM-Newton data, with no new pulsations detected yet.


<details>
  <summary>Details</summary>
Motivation: To address the limited capability of discovering pulsations in ULXs due to poor statistical data and to identify new candidate PULXs using existing archives and catalogs.

Method: The authors applied an AI-based approach using an unsupervised clustering algorithm on ULXs detected by XMM-Newton. They differentiated clusters based on similarities, using known PULX observations as a threshold.

Result: The AI analysis identified 85 unique candidate PULXs across 355 observations, but preliminary analyses did not detect new pulsations in these candidates.

Conclusion: While the AI approach successfully identified candidates analogous to known PULXs, confirming their nature requires high-statistical observational data to detect coherent signals and validate the methodology.

Abstract: The discovery of fast and variable coherent signals in a handful of
ultraluminous X-ray sources (ULXs) testifies to the presence of super-Eddington
accreting neutron stars, and drastically changed the understanding of the ULX
class. Our capability of discovering pulsations in ULXs is limited, among
others, by poor statistics. However, catalogues and archives of high-energy
missions contain information which can be used to identify new candidate
pulsating ULXs (PULXs). The goal of this research is to single out candidate
PULXs among those ULXs which have not shown pulsations due to an unfavourable
combination of factors. We applied an AI approach to an updated database of
ULXs detected by XMM-Newton. We first used an unsupervised clustering algorithm
to sort out sources with similar characteristics into two clusters. Then, the
sample of known PULX observations has been used to set the separation threshold
between the two clusters and to identify the one containing the new candidate
PULXs. We found that only a few criteria are needed to assign the membership of
an observation to one of the two clusters. The cluster of new candidate PULXs
counts 85 unique sources for 355 observations, with $\sim$85% of these new
candidates having multiple observations. A preliminary timing analysis found no
new pulsations for these candidates. This work presents a sample of new
candidate PULXs observed by XMM-Newton, the properties of which are similar (in
a multi-dimensional phase space) to those of the known PULXs, despite the
absence of pulsations in their light curves. While this result is a clear
example of the predictive power of AI-based methods, it also highlights the
need for high-statistics observational data to reveal coherent signals from the
sources in this sample and thus validate the robustness of the approach.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [623] [Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings](https://arxiv.org/abs/2507.15118)
*Szymon Mazurek,Stephen Moore,Alessandro Crimi*

Main category: eess.SP

TL;DR: The paper presents a graph-based deep learning framework leveraging low-cost EEG hardware to address epilepsy diagnosis challenges in underserved regions.


<details>
  <summary>Details</summary>
Motivation: Epilepsy diagnosis in low-income countries faces challenges due to a shortage of neurologists and expensive diagnostic tools.

Method: The study models EEG signals as spatio-temporal graphs using graph attention networks (GAT) to classify signals and identify connectivity biomarkers.

Result: The proposed method achieves better classification accuracy and robustness compared to alternative approaches while identifying fronto-temporal biomarkers.

Conclusion: GAT-based models show promise for scalable, affordable epilepsy diagnostics in underserved regions, providing accessible neurodiagnosis support.

Abstract: Goal: Epilepsy remains under-diagnosed in low-income countries due to scarce
neurologists and costly diagnostic tools. We propose a graph-based deep
learning framework to detect epilepsy from low-cost Electroencephalography
(EEG) hardware, tested on recordings from Nigeria and Guinea-Bissau. Our focus
is on fair, accessible automatic assessment and explainability to shed light on
epilepsy biomarkers. Methods: We model EEG signals as spatio-temporal graphs,
classify them, and identify interchannel relationships and temporal dynamics
using graph attention networks (GAT). To emphasize connectivity biomarkers, we
adapt the inherently node-focused GAT to analyze edges. We also designed signal
preprocessing for low-fidelity recordings and a lightweight GAT architecture
trained on Google Colab and deployed on RaspberryPi devices. Results: The
approach achieves promising classification performance, outperforming a
standard classifier based on random forest and graph convolutional networks in
terms of accuracy and robustness over multiple sessions, but also highlighting
specific connections in the fronto-temporal region. Conclusions: The results
highlight the potential of GATs to provide insightful and scalable diagnostic
support for epilepsy in underserved regions, paving the way for affordable and
accessible neurodiagnostic tools.

</details>


### [624] [Graph Convolutional Neural Networks to Model the Brain for Insomnia](https://arxiv.org/abs/2507.14147)
*Kevin Monteiro,Sam Nallaperuma-Herzberg,Martina Mason,Steve Niederer*

Main category: eess.SP

TL;DR: The paper proposes a novel model using EEG data and graph convolutional neural networks (GCNN) to classify insomnia characteristics, achieving 70% window-level accuracy and 68% subject-level accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing insomnia treatments are linked to side effects, and there is a lack of brain modeling approaches for understanding insomnia. Hence, this study aims to characterize the brain dynamics of insomnia sufferers using EEG.

Method: The method processes long-duration EEG data, derives brain networks from functional connectivity and spatial distance, computes power spectral density, and employs a GCNN model for classification.

Result: The best EEG segmentation strategy used a 50-second non-overlapping sliding window, achieving 70% accuracy at window level and 68% accuracy at subject level. Specific channel omissions had a significant impact on performance.

Conclusion: EEG-based functional connectivity insights and the GCNN classification model show promise for non-invasive insomnia assessment, highlighting specific brain regions strongly associated with the condition.

Abstract: Insomnia affects a vast population of the world and can have a wide range of
causes. Existing treatments for insomnia have been linked with many side
effects like headaches, dizziness, etc. As such, there is a clear need for
improved insomnia treatment. Brain modelling has helped with assessing the
effects of brain pathology on brain network dynamics and with supporting
clinical decisions in the treatment of Alzheimer's disease, epilepsy, etc.
However, such models have not been developed for insomnia. Therefore, this
project attempts to understand the characteristics of the brain of individuals
experiencing insomnia using continuous long-duration EEG data. Brain networks
are derived based on functional connectivity and spatial distance between EEG
channels. The power spectral density of the channels is then computed for the
major brain wave frequency bands. A graph convolutional neural network (GCNN)
model is then trained to capture the functional characteristics associated with
insomnia and configured for the classification task to judge performance.
Results indicated a 50-second non-overlapping sliding window was the most
suitable choice for EEG segmentation. This approach achieved a classification
accuracy of 70% at window level and 68% at subject level. Additionally, the
omission of EEG channels C4-P4, F4-C4 and C4-A1 caused higher degradation in
model performance than the removal of other channels. These channel electrodes
are positioned near brain regions known to exhibit atypical levels of
functional connectivity in individuals with insomnia, which can explain such
results.

</details>


### [625] [UniPhyNet: A Unified Network For Multimodal Physiological Raw Signal Classification](https://arxiv.org/abs/2507.14163)
*Renxiang Qiu,Raghavendra Selvan*

Main category: eess.SP

TL;DR: The paper introduces UniPhyNet, a neural network for classifying cognitive load using multimodal physiological data without manual feature extraction.


<details>
  <summary>Details</summary>
Motivation: To improve classification accuracy of cognitive load using raw physiological signals without relying on hand-crafted features.

Method: UniPhyNet uses a combination of multiscale parallel convolutional blocks, ResNet blocks with attention modules, and bidirectional gated recurrent units for processing data in unimodal and multimodal configurations.

Result: UniPhyNet achieved 80% binary and 74% ternary classification accuracy on the CL-Drive dataset, outperforming feature-based models.

Conclusion: UniPhyNet is an effective end-to-end deep learning framework for monitoring cognitive states in real-world scenarios.

Abstract: We present UniPhyNet, a novel neural network architecture to classify
cognitive load using multimodal physiological data -- specifically EEG, ECG and
EDA signals -- without the explicit need for extracting hand-crafted features.
UniPhyNet integrates multiscale parallel convolutional blocks and ResNet-type
blocks enhanced with channel block attention module to focus on the informative
features while a bidirectional gated recurrent unit is used to capture temporal
dependencies. This architecture processes and combines signals in both unimodal
and multimodal configurations via intermediate fusion of learned feature maps.
On the CL-Drive dataset, UniPhyNet improves raw signal classification accuracy
from 70% to 80% (binary) and 62% to 74% (ternary), outperforming feature-based
models, demonstrating its effectiveness as an end-to-end solution for
real-world cognitive state monitoring.

</details>


### [626] [A Comprehensive Benchmark for Electrocardiogram Time-Series](https://arxiv.org/abs/2507.14206)
*Zhijiang Tang,Jiaxin Qi,Yuhua Zheng,Jianqiang Huang*

Main category: eess.SP

TL;DR: This paper introduces a comprehensive study and benchmark for using ECG signals in time-series model training, highlighting their unique properties and proposing improvements.


<details>
  <summary>Details</summary>
Motivation: ECG signals are essential for cardiac health assessment and disease diagnosis but are often under-analyzed compared to other time-series data in large-scale model training.

Method: The study categorizes ECG's applications into four evaluation tasks, introduces a novel metric to address limitations of traditional methods, benchmarks existing models, and proposes a new model architecture.

Result: Experiments validate the robustness of the benchmark, effectiveness of the novel metric, and superiority of the proposed model architecture.

Conclusion: The paper establishes foundational tools and insights for advancing ECG signal analysis in time-series modeling.

Abstract: Electrocardiogram~(ECG), a key bioelectrical time-series signal, is crucial
for assessing cardiac health and diagnosing various diseases. Given its
time-series format, ECG data is often incorporated into pre-training datasets
for large-scale time-series model training. However, existing studies often
overlook its unique characteristics and specialized downstream applications,
which differ significantly from other time-series data, leading to an
incomplete understanding of its properties. In this paper, we present an
in-depth investigation of ECG signals and establish a comprehensive benchmark,
which includes (1) categorizing its downstream applications into four distinct
evaluation tasks, (2) identifying limitations in traditional evaluation metrics
for ECG analysis, and introducing a novel metric; (3) benchmarking
state-of-the-art time-series models and proposing a new architecture. Extensive
experiments demonstrate that our proposed benchmark is comprehensive and
robust. The results validate the effectiveness of the proposed metric and model
architecture, which establish a solid foundation for advancing research in ECG
signal analysis.

</details>


### [627] [Traffic Signal Phase and Timing Estimation with Large-Scale Floating Car Data](https://arxiv.org/abs/2507.14190)
*Mingcheng Liao,Zebang Feng,Miao Fan,Shengtong Xu,Haoyi Xiong*

Main category: eess.SP

TL;DR: The paper introduces a robust Floating Car Data (FCD) analysis suite for accurate Signal Phase and Timing (SPaT) estimations that effectively addresses limitations of conventional methods.


<details>
  <summary>Details</summary>
Motivation: Current FCD approaches lack comprehensive frameworks for SPaT estimation, failing to account for periodic signal changes, diverse intersection structures, and real-world data limitations.

Method: An industrial-grade FCD analysis suite is proposed that integrates preprocessing, signal phase estimation, time-of-day identification, and red-green light duration determination.

Result: The system processes over 15 million FCD records daily, supports over two million traffic signals in mainland China, and achieves over 75% estimations with errors less than 5 seconds.

Conclusion: The framework demonstrates stability and robustness across varied conditions, provides tools to facilitate further research, and achieves industrial-scale implementation with commendable accuracy.

Abstract: Effective modern transportation systems depend critically on accurate Signal
Phase and Timing (SPaT) estimation. However, acquiring ground-truth SPaT
information faces significant hurdles due to communication challenges with
transportation departments and signal installers. As a result, Floating Car
Data (FCD) has become the primary source for large-scale SPaT analyses. Current
FCD approaches often simplify the problem by assuming fixed schedules and basic
intersection designs for specific times and locations. These methods fail to
account for periodic signal changes, diverse intersection structures, and the
inherent limitations of real-world data, thus lacking a comprehensive framework
that is universally applicable. Addressing this limitation, we propose an
industrial-grade FCD analysis suite that manages the entire process, from
initial data preprocessing to final SPaT estimation. Our approach estimates
signal phases, identifies time-of-day (TOD) periods, and determines the
durations of red and green lights. The framework's notable stability and
robustness across diverse conditions, regardless of road geometry, is a key
feature. Furthermore, we provide a cleaned, de-identified FCD dataset and
supporting parameters to facilitate future research. Currently operational
within our navigation platform, the system analyses over 15 million FCD records
daily, supporting over two million traffic signals in mainland China, with more
than 75\% of estimations demonstrating less than five seconds of error.

</details>


### [628] [DIVER-0 : A Fully Channel Equivariant EEG Foundation Model](https://arxiv.org/abs/2507.14141)
*Danny Dongyeop Han,Ahhyun Lucy Lee,Taeyang Lee,Yonghyeon Gwon,Sebin Lee,Seongjin Lee,David Keetae Park,Shinjae Yoo,Jiook Cha,Chun Kee Chung*

Main category: eess.SP

TL;DR: DIVER-0 is an advanced EEG foundation model addressing spatio-temporal dynamics and electrode channel permutation challenges, showing robust performance and generalization across configurations.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations in existing EEG foundation models, specifically spatio-temporal modeling and electrode channel permutation equivariance, which hinder their robust generalization.

Method: The paper introduces DIVER-0 using full spatio-temporal attention, Rotary Position Embedding (RoPE) for temporal relationship encoding, binary attention biases for channel differentiation, and Sliding Temporal Conditional Positional Encoding (STCPE) for translation and permutation equivariance.

Result: Experimental results confirm DIVER-0's ability to perform competitively with only 10% of pretraining data, while retaining robust functionality across varied electrode configurations.

Conclusion: DIVER-0 confirms its potential for cross-dataset generalization and sets design principles for handling neural recording setup heterogeneity.

Abstract: Electroencephalography (EEG) is a non-invasive technique widely used in
brain-computer interfaces and clinical applications, yet existing EEG
foundation models face limitations in modeling spatio-temporal brain dynamics
and lack channel permutation equivariance, preventing robust generalization
across diverse electrode configurations. To address these challenges, we
propose DIVER-0, a novel EEG foundation model that demonstrates how full
spatio-temporal attention-rather than segregated spatial or temporal
processing-achieves superior performance when properly designed with Rotary
Position Embedding (RoPE) for temporal relationships and binary attention
biases for channel differentiation. We also introduce Sliding Temporal
Conditional Positional Encoding (STCPE), which improves upon existing
conditional positional encoding approaches by maintaining both temporal
translation equivariance and channel permutation equivariance, enabling robust
adaptation to arbitrary electrode configurations unseen during pretraining.
Experimental results demonstrate that DIVER-0 achieves competitive performance
with only 10% of pretraining data while maintaining consistent results across
all channel permutation conditions, validating its effectiveness for
cross-dataset generalization and establishing key design principles for
handling the inherent heterogeneity of neural recording setups.

</details>


### [629] [Self-DANA: A Resource-Efficient Channel-Adaptive Self-Supervised Approach for ECG Foundation Models](https://arxiv.org/abs/2507.14151)
*Giuliana Monachino,Nicolò La Porta,Beatrice Zanchi,Luigi Fiorillo,Alvise Dei Rossi,Georgiy Farina,Francesca Dalia Faraci*

Main category: eess.SP

TL;DR: Self-DANA is a novel solution that adapts self-supervised architectures for reduced-channel ECG analysis, significantly improving resource efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The rise of wearable devices has increased the need for effective machine learning models that can handle reduced ECG channel configurations, but this challenge remains unexplored in Foundation Models (FMs).

Method: Self-DANA employs a self-supervised learning architecture paired with a new Random Lead Selection augmentation technique to make models channel-agnostic and resource-efficient.

Result: Self-DANA achieved state-of-the-art performance with significantly lower memory and time requirements: up to 69.3% less peak CPU memory, 34.4% less peak GPU memory, 17% less average epoch CPU time, and 24% less average epoch GPU time.

Conclusion: This method demonstrates that FMs can be adapted to handle reduced-channel ECG configurations effectively, enabling high performance in resource-constrained scenarios.

Abstract: Foundation Models (FMs) are large-scale machine learning models trained on
extensive, diverse datasets that can be adapted to a wide range of downstream
tasks with minimal fine-tuning. In the last two years, interest in FMs has also
grown for applications in the cardiological field to analyze the
electrocardiogram (ECG) signals. One of the key properties of FMs is their
transferability to a wide range of downstream scenarios. With the spread of
wearable and portable devices, keen interest in learning from reduced-channel
configurations has arisen. However, the adaptation of ECG FMs to downstream
scenarios with fewer available channels still has to be properly investigated.
In this work, we propose Self-DANA, a novel, easy-to-integrate solution that
makes self-supervised architectures adaptable to a reduced number of input
channels, ensuring resource efficiency and high performance. We also introduce
Random Lead Selection, a novel augmentation technique to pre-train models in a
more robust and channel-agnostic way. Our experimental results on five
reduced-channel configurations demonstrate that Self-DANA significantly
enhances resource efficiency while reaching state-of-the-art performance. It
requires up to 69.3% less peak CPU memory, 34.4% less peak GPU memory, about
17% less average epoch CPU time, and about 24% less average epoch GPU time.

</details>


### [630] [Surface EMG Profiling in Parkinson's Disease: Advancing Severity Assessment with GCN-SVM](https://arxiv.org/abs/2507.14153)
*Daniel Cieślak,Barbara Szyca,Weronika Bajko,Liwia Florkiewicz,Kinga Grzęda,Mariusz Kaczmarek,Helena Kamieniecka,Hubert Lis,Weronika Matwiejuk,Anna Prus,Michalina Razik,Inga Rozumowicz,Wiktoria Ziembakowska*

Main category: eess.SP

TL;DR: The study proposes a novel sEMG-based model to objectively assess Parkinson's disease (PD) severity, improving classification accuracy to 92% with a GCN-SVM approach.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in diagnosing and monitoring Parkinson’s disease (PD) due to its progressive and complex symptoms.

Method: The study uses surface electromyography (sEMG) from the biceps brachii muscle and applies machine learning models (SVM and GCN-SVM) to differentiate between PD patients and healthy controls.

Result: The SVM model achieved 83% accuracy, while the enhanced GCN-SVM model increased accuracy to 92% in distinguishing between PD patients and healthy subjects.

Conclusion: The proposed sEMG-GCN-SVM approach shows potential for more accurate PD severity assessment and outlines a framework for larger-scale studies to integrate it into clinical practice.

Abstract: Parkinson's disease (PD) poses challenges in diagnosis and monitoring due to
its progressive nature and complex symptoms. This study introduces a novel
approach utilizing surface electromyography (sEMG) to objectively assess PD
severity, focusing on the biceps brachii muscle. Initial analysis of sEMG data
from five PD patients and five healthy controls revealed significant
neuromuscular differences. A traditional Support Vector Machine (SVM) model
achieved up to 83% accuracy, while enhancements with a Graph Convolutional
Network-Support Vector Machine (GCN-SVM) model increased accuracy to 92%.
Despite the preliminary nature of these results, the study outlines a detailed
experimental methodology for future research with larger cohorts to validate
these findings and integrate the approach into clinical practice. The proposed
approach holds promise for advancing PD severity assessment and improving
patient care in Parkinson's disease management.

</details>


### [631] [A Denoising VAE for Intracardiac Time Series in Ischemic Cardiomyopathy](https://arxiv.org/abs/2507.14164)
*Samuel Ruipérez-Campillo,Alain Ryser,Thomas M. Sutter,Ruibin Feng,Prasanth Ganesan,Brototo Deb,Kelly A. Brennan,Maxime Pedron,Albert J. Rogers,Maarten Z. H. Kolk,Fleur V. Y. Tjong,Sanjiv M. Narayan,Julia E. Vogt*

Main category: eess.SP

TL;DR: This study presents a Variational Autoencoder (VAE) model for improved noise reduction in intra-cardiac signals, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Accurate diagnosis and treatment of cardiac arrhythmias require effective noise reduction techniques for intra-cardiac signals, which traditional methods fail to address adequately due to non-linear and non-stationary noise.

Method: The authors developed a VAE model and trained it on 5706 time series from MAP signal recordings of 42 ischemic cardiomyopathy patients, enabling clean signal representation and evaluation.

Result: The VAE model demonstrated superior denoising performance when compared to traditional clinical filtering methods, effectively managing different noise types, including non-linear and time-varying ones.

Conclusion: This VAE-based approach can outperform state-of-the-art denoising techniques, potentially enhancing treatment efficacy in cardiac electrophysiology clinical practice.

Abstract: In the field of cardiac electrophysiology (EP), effectively reducing noise in
intra-cardiac signals is crucial for the accurate diagnosis and treatment of
arrhythmias and cardiomyopathies. However, traditional noise reduction
techniques fall short in addressing the diverse noise patterns from various
sources, often non-linear and non-stationary, present in these signals. This
work introduces a Variational Autoencoder (VAE) model, aimed at improving the
quality of intra-ventricular monophasic action potential (MAP) signal
recordings. By constructing representations of clean signals from a dataset of
5706 time series from 42 patients diagnosed with ischemic cardiomyopathy, our
approach demonstrates superior denoising performance when compared to
conventional filtering methods commonly employed in clinical settings. We
assess the effectiveness of our VAE model using various metrics, indicating its
superior capability to denoise signals across different noise types, including
time-varying non-linear noise frequently found in clinical settings. These
results reveal that VAEs can eliminate diverse sources of noise in single
beats, outperforming state-of-the-art denoising techniques and potentially
improving treatment efficacy in cardiac EP.

</details>


### [632] [NeuroHD-RA: Neural-distilled Hyperdimensional Model with Rhythm Alignment](https://arxiv.org/abs/2507.14184)
*ZhengXiao He,Jinghao Wen,Huayu Li,Ao Li*

Main category: eess.SP

TL;DR: The paper introduces a new framework combining hyperdimensional computing (HDC) and trainable encoding for interpretable ECG-based disease detection, showing improved performance over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to enhance ECG-based disease detection methods by making them more interpretable and adaptive, addressing the limitations of static and random projection techniques in traditional HDC approaches.

Method: The approach incorporates learnable neural encoding using RR intervals and a neural-distilled HDC framework, combining a trainable encoder and BinaryLinear projection layer. It jointly optimizes with cross-entropy and metric loss functions.

Result: Experimental results show superior performance compared to traditional HDC and machine learning baselines, achieving 73.09% precision and an F1 score of 0.626 on Apnea-ECG, with robust results on PTB-XL.

Conclusion: The proposed framework demonstrates a scalable and interpretable solution for ECG classification, suitable for edge applications and personalized health monitoring.

Abstract: We present a novel and interpretable framework for electrocardiogram
(ECG)-based disease detection that combines hyperdimensional computing (HDC)
with learnable neural encoding. Unlike conventional HDC approaches that rely on
static, random projections, our method introduces a rhythm-aware and trainable
encoding pipeline based on RR intervals, a physiological signal segmentation
strategy that aligns with cardiac cycles. The core of our design is a
neural-distilled HDC architecture, featuring a learnable RR-block encoder and a
BinaryLinear hyperdimensional projection layer, optimized jointly with
cross-entropy and proxy-based metric loss. This hybrid framework preserves the
symbolic interpretability of HDC while enabling task-adaptive representation
learning. Experiments on Apnea-ECG and PTB-XL demonstrate that our model
significantly outperforms traditional HDC and classical ML baselines, achieving
73.09\% precision and an F1 score of 0.626 on Apnea-ECG, with comparable
robustness on PTB-XL. Our framework offers an efficient and scalable solution
for edge-compatible ECG classification, with strong potential for interpretable
and personalized health monitoring.

</details>


### [633] [AI-Based Impedance Encoding-Decoding Method for Online Impedance Network Construction of Wind Farms](https://arxiv.org/abs/2507.14187)
*Xiaojuan Zhang,Tianyu Jiang,Haoxiang Zong,Chen Zhang,Chendan Li,Marta Molinas*

Main category: eess.SP

TL;DR: The paper introduces an AI-based method to encode and decode impedance curves, allowing efficient online construction of impedance network models for wind farm oscillation analysis.


<details>
  <summary>Details</summary>
Motivation: Online IN modeling for wind farms is hindered by the challenge of transmitting numerous high-density impedance curves for individual turbines.

Method: An AI encoder compresses impedance curves with fewer neurons to minimize data size, while a decoder reconstructs original curves accurately. This facilitates efficient transmission and IN model construction using the nodal admittance matrix (NAM).

Result: The method is validated through training and simulations, confirming rapid transmission of encoded data and accurate impedance curve reconstruction.

Conclusion: The proposed AI-based impedance encoding-decoding method enables efficient, accurate, and fast online modeling of impedance networks for wind farm oscillation analysis.

Abstract: The impedance network (IN) model is gaining popularity in the oscillation
analysis of wind farms. However, the construction of such an IN model requires
impedance curves of each wind turbine under their respective operating
conditions, making its online application difficult due to the transmission of
numerous high-density impedance curves. To address this issue, this paper
proposes an AI-based impedance encoding-decoding method to facilitate the
online construction of IN model. First, an impedance encoder is trained to
compress impedance curves by setting the number of neurons much smaller than
that of frequency points. Then, the compressed data of each turbine are
uploaded to the wind farm and an impedance decoder is trained to reconstruct
original impedance curves. At last, based on the nodal admittance matrix (NAM)
method, the IN model of the wind farm can be obtained. The proposed method is
validated via model training and real-time simulations, demonstrating that the
encoded impedance vectors enable fast transmission and accurate reconstruction
of the original impedance curves.

</details>


### [634] [UWB Radar-based Heart Rate Monitoring: A Transfer Learning Approach](https://arxiv.org/abs/2507.14195)
*Elzbieta Gruzewska,Pooja Rao,Sebastien Baur,Matthew Baugh,Mathias M. J. Bellaiche,Sharanya Srinivas,Octavio Ponce,Matthew Thompson,Pramod Rudrapatna,Michael A. Sanchez,Lawrence Z. Cai,Timothy JA Chico,Robert F. Storey,Emily Maz,Umesh Telang,Shravya Shetty,Mayank Daswani*

Main category: eess.SP

TL;DR: The study explores transfer learning for heart rate monitoring using two types of radar systems (FMCW and IR-UWB), achieving strong accuracy and demonstrating its viability for future consumer devices.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of large dataset requirements and lack of standardization across radar systems for heart rate monitoring.

Method: The study employed a novel 2D+1D ResNet architecture to train models on FMCW radar data and fine-tuned them using IR-UWB radar data for transfer learning.

Result: Achieved MAE of 0.85 bpm and MAPE of 1.42% for FMCW radar. For IR-UWB radar, fine-tuning reduced MAE to 4.1 bpm and MAPE to 6.3%, a 25% improvement over the baseline.

Conclusion: Transfer learning successfully bridges data gaps between radar systems, enabling more efficient and accurate heart rate monitoring in consumer electronics.

Abstract: Radar technology presents untapped potential for continuous, contactless, and
passive heart rate monitoring via consumer electronics like mobile phones.
However the variety of available radar systems and lack of standardization
means that a large new paired dataset collection is required for each radar
system. This study demonstrates transfer learning between frequency-modulated
continuous wave (FMCW) and impulse-radio ultra-wideband (IR-UWB) radar systems,
both increasingly integrated into consumer devices. FMCW radar utilizes a
continuous chirp, while IR-UWB radar employs short pulses. Our mm-wave FMCW
radar operated at 60 GHz with a 5.5 GHz bandwidth (2.7 cm resolution, 3
receiving antennas [Rx]), and our IR-UWB radar at 8 GHz with a 500 MHz
bandwidth (30 cm resolution, 2 Rx). Using a novel 2D+1D ResNet architecture we
achieved a mean absolute error (MAE) of 0.85 bpm and a mean absolute percentage
error (MAPE) of 1.42% for heart rate monitoring with FMCW radar (N=119
participants, an average of 8 hours per participant). This model maintained
performance (under 5 MAE/10% MAPE) across various body positions and heart rate
ranges, with a 98.9% recall. We then fine-tuned a variant of this model,
trained on single-antenna and single-range bin FMCW data, using a small (N=376,
avg 6 minutes per participant) IR-UWB dataset. This transfer learning approach
yielded a model with MAE 4.1 bpm and MAPE 6.3% (97.5% recall), a 25% MAE
reduction over the IR-UWB baseline. This demonstration of transfer learning
between radar systems for heart rate monitoring has the potential to accelerate
its introduction into existing consumer devices.

</details>


### [635] [Explainable Parallel CNN-LSTM Model for Differentiating Ventricular Tachycardia from Supraventricular Tachycardia with Aberrancy in 12-Lead ECGs](https://arxiv.org/abs/2507.14196)
*Zahra Teimouri-Jervekani,Fahimeh Nasimi,Mohammadreza Yazdchi,Ghazal MogharehZadeh,Javad Tezerji,Farzan Niknejad Mazandarani,Maryam Mohebbi*

Main category: eess.SP

TL;DR: The paper presents a computationally efficient deep learning model for classifying wide complex tachycardia, achieving high diagnostic accuracy and offering clinical interpretability.


<details>
  <summary>Details</summary>
Motivation: Current methods for differentiating life-threatening ventricular tachycardia from supraventricular tachycardia with aberrancy face challenges due to ECG morphological similarities and fatal risks associated with misdiagnosis.

Method: The paper introduces a lightweight parallel deep architecture that processes ECG signals using 1D-CNN blocks for feature extraction, LSTM layers for temporal dependencies, and fully connected layers for classification. Explainability is achieved using SHAP analysis.

Result: The proposed model achieved 95.63% accuracy, 95.10% sensitivity, 96.06% specificity, and 95.12% F1-score, outperforming existing methods in accuracy and computational efficiency. SHAP demonstrated interpretable feature contributions.

Conclusion: The framework provides high-precision classification for wide complex tachycardia with low computational requirements, supports clinical trust through interpretable decision-making, and holds promise for practical ECG diagnosis.

Abstract: Background and Objective: Differentiating wide complex tachycardia (WCT) is
clinically critical yet challenging due to morphological similarities in
electrocardiogram (ECG) signals between life-threatening ventricular
tachycardia (VT) and supraventricular tachycardia with aberrancy (SVT-A).
Misdiagnosis carries fatal risks. We propose a computationally efficient deep
learning solution to improve diagnostic accuracy and provide model
interpretability for clinical deployment.
  Methods: A novel lightweight parallel deep architecture is introduced. Each
pipeline processes individual ECG leads using two 1D-CNN blocks to extract
local features. Feature maps are concatenated across leads, followed by LSTM
layers to capture temporal dependencies. Final classification employs fully
connected layers. Explainability is achieved via Shapley Additive Explanations
(SHAP) for local/global interpretation. The model was evaluated on a 35-subject
ECG database using standard performance metrics.
  Results: The model achieved $95.63\%$ accuracy ($95\%$ CI: $93.07-98.19\%$),
with sensitivity=$95.10\%$, specificity=$96.06\%$, and F1-score=$95.12\%$. It
outperformed state-of-the-art methods in both accuracy and computational
efficiency, requiring minimal CNN blocks per pipeline. SHAP analysis
demonstrated clinically interpretable feature contributions.
  Conclusions: Our end-to-end framework delivers high-precision WCT
classification with minimal computational overhead. The integration of SHAP
enhances clinical trust by elucidating decision logic, supporting rapid,
informed diagnosis. This approach shows significant promise for real-world ECG
analysis tools.

</details>


### [636] [Age of Information Minimization in UAV-Enabled Integrated Sensing and Communication Systems](https://arxiv.org/abs/2507.14299)
*Yu Bai,Yifan Zhang,Boxuan Xie,Zheng Chang,Yanru Zhang,Riku Jantti,Zhu Han*

Main category: eess.SP

TL;DR: The paper introduces a UAV system that optimizes sensing and communication tasks by minimizing Age of Information (AoI) using a deep reinforcement learning algorithm.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for flexible and efficient UAVs in wireless networks necessitates systems that can handle both sensing and communication tasks under resource constraints.

Method: The study employs a DRL-based algorithm integrating Kalman filter for target prediction, regularized zero-forcing for interference reduction, and Soft Actor-Critic for continuous action decisions.

Result: Simulation results reveal the proposed UAV system achieves lower average AoI compared to existing baseline approaches.

Conclusion: The proposed UAV-ISAC system effectively balances sensing accuracy and communication quality, demonstrating its potential in future wireless networks.

Abstract: Unmanned aerial vehicles (UAVs) equipped with integrated sensing and
communication (ISAC) capabilities are envisioned to play a pivotal role in
future wireless networks due to their enhanced flexibility and efficiency.
However, jointly optimizing UAV trajectory planning, multi-user communication,
and target sensing under stringent resource constraints and time-critical
conditions remains a significant challenge. To address this, we propose an Age
of Information (AoI)-centric UAV-ISAC system that simultaneously performs
target sensing and serves multiple ground users, emphasizing information
freshness as the core performance metric. We formulate a long-term average AoI
minimization problem that jointly optimizes the UAV's flight trajectory and
beamforming. To tackle the high-dimensional, non-convexity of this problem, we
develop a deep reinforcement learning (DRL)-based algorithm capable of
providing real-time decisions on UAV movement and beamforming for both radar
sensing and multi-user communication. Specifically, a Kalman filter is employed
for accurate target state prediction, regularized zero-forcing is utilized to
mitigate inter-user interference, and the Soft Actor-Critic algorithm is
applied for training the DRL agent on continuous actions. The proposed
framework adaptively balances the trade-offs between sensing accuracy and
communication quality. Extensive simulation results demonstrate that our
proposed method consistently achieves lower average AoI compared to baseline
approaches.

</details>


### [637] [Recursive KalmanNet: Analyse des capacités de généralisation d'un réseau de neurones récurrent guidé par un filtre de Kalman](https://arxiv.org/abs/2507.14144)
*Cyril Falcon,Hassan Mortada,Mathéo Clavaud,Jean-Philippe Michel*

Main category: eess.SP

TL;DR: The paper studies Recursive KalmanNet, a recurrent neural network guided by a Kalman filter for state estimation, focusing on its performance in out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of estimating stochastic system states using noisy data in cases with unknown noise characteristics.

Method: Recursive KalmanNet leverages a recurrent neural network guided by a Kalman filter to estimate state variables and error covariance without requiring prior noise knowledge.

Result: The paper evaluates how the Recursive KalmanNet generalizes when the temporal dynamics in the test data differ significantly from training conditions.

Conclusion: Recursive KalmanNet demonstrates promising generalization capabilities in scenarios with unseen measurement dynamics, highlighting its adaptability beyond training conditions.

Abstract: The Recursive KalmanNet, recently introduced by the authors, is a recurrent
neural network guided by a Kalman filter, capable of estimating the state
variables and error covariance of stochastic dynamic systems from noisy
measurements, without prior knowledge of the noise characteristics. This paper
explores its generalization capabilities in out-of-distribution scenarios,
where the temporal dynamics of the test measurements differ from those
encountered during training.
  Le Recursive KalmanNet, r\'ecemment introduit par les auteurs, est un
r\'eseau de neurones r\'ecurrent guid\'e par un filtre de Kalman, capable
d'estimer les variables d'\'etat et la covariance des erreurs des syst\`emes
dynamiques stochastiques \`a partir de mesures bruit\'ees, sans connaissance
pr\'ealable des caract\'eristiques des bruits. Cet article explore ses
capacit\'es de g\'en\'eralisation dans des sc\'enarios hors distribution, o\`u
les dynamiques temporelles des mesures de test diff\`erent de celles
rencontr\'ees \`a l'entra\^inement.

</details>


### [638] [Machine learning-enabled river water quality monitoring using lithography-free 3D-printed sensors](https://arxiv.org/abs/2507.14152)
*Frank Efe Erukainure,Feidra Gjata,Matin Ataei Kachouei,Henry Cox,Md. Azahar Ali*

Main category: eess.SP

TL;DR: The paper presents a lithography-free sensor detecting phosphate levels in river water at ppb precision, offering rapid results with high accuracy.


<details>
  <summary>Details</summary>
Motivation: To address ecological and health problems caused by excessive phosphate contamination in river water, which depletes dissolved oxygen and leads to eutrophication.

Method: Developed a sensor using 3D-printed polymer patterns coated with a phosphate ion-selective membrane, and coupled its data analysis with a feed-forward neural network for enhanced accuracy.

Result: The sensor effectively detected phosphate levels as low as 1 ppb, validated on river water samples with high correlation to commercial meters and achieved robust neural network prediction performance.

Conclusion: The developed sensor provides a viable solution for continuous monitoring of river phosphate levels, offering utility in public health and ecological conservation efforts.

Abstract: River water quality monitoring is important for aquatic life, livestock, and
humans because clean water is critical to meeting food demand during the global
food crisis. Excessive contaminants, including phosphate, deplete dissolved
oxygen and trigger eutrophication, leading to serious health and ecological
problems. Continuous sensors that track phosphate levels can therefore help
prevent eutrophication. In this work we present a lithography-free phosphate
sensor (P-sensor) that detects phosphate in river water at parts-per-billion
levels. The device uses a solid-state indicator electrode formed by 3D-printed
periodic polymer patterns (8 um feature size) coated with a thin phosphate
ion-selective membrane. The P-sensor detects as little as 1 ppb phosphate
across 0 - 475 ppm with a response time under 30 seconds. We validated the
sensor on Rappahannock River water, Virginia (less than 0.8 ppm phosphate) at
sites upstream and downstream of a sewage treatment plant and benchmarked the
results against a commercial phosphate meter. A feed-forward neural network was
trained to predict phosphate levels, achieving a mean-squared error below 1e-3,
zero standard deviation, and a Pearson correlation coefficient of 0.997 for
river samples. These results demonstrate a practical tool for continuous
water-quality monitoring that can inform stakeholders and policymakers and
ultimately improve public health.

</details>


### [639] [Automated Vigilance State Classification in Rodents Using Machine Learning and Feature Engineering](https://arxiv.org/abs/2507.14166)
*Sankalp Jajee,Gaurav Kumar,Homayoun Valafar*

Main category: eess.SP

TL;DR: This paper introduces an automated machine learning model to classify EEG recordings from small rodents into wakefulness, slow wave sleep (SWS), and paradoxical (REM) sleep, achieving impressive accuracy and outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: The need to alleviate the constraints of manual vigilance state classification and improve throughput and reproducibility in preclinical sleep research.

Method: An XGBoost model trained using engineered features derived from signal processing techniques across time and frequency domains, focusing on spectral power, temporal dynamics, and cross-frequency coupling metrics.

Result: The model achieved 91.5% accuracy, 86.8% precision, 81.2% recall, and an F1 score of 83.5%, performing better than existing baseline methods.

Conclusion: This automated system marks a significant step forward in sleep state classification and offers potential benefits for advancing sleep science and treatments for sleep disorders.

Abstract: Preclinical sleep research remains constrained by labor intensive, manual
vigilance state classification and inter rater variability, limiting throughput
and reproducibility. This study presents an automated framework developed by
Team Neural Prognosticators to classify electroencephalogram (EEG) recordings
of small rodents into three critical vigilance states paradoxical sleep (REM),
slow wave sleep (SWS), and wakefulness. The system integrates advanced signal
processing with machine learning, leveraging engineered features from both time
and frequency domains, including spectral power across canonical EEG bands
(delta to gamma), temporal dynamics via Maximum-Minimum Distance, and
cross-frequency coupling metrics. These features capture distinct
neurophysiological signatures such as high frequency desynchronization during
wakefulness, delta oscillations in SWS, and REM specific bursts. Validated
during the 2024 Big Data Health Science Case Competition (University of South
Carolina Big Data Health Science Center, 2024), our XGBoost model achieved
91.5% overall accuracy, 86.8% precision, 81.2% recall, and an F1 score of
83.5%, outperforming all baseline methods. Our approach represents a critical
advancement in automated sleep state classification and a valuable tool for
accelerating discoveries in sleep science and the development of targeted
interventions for chronic sleep disorders. As a publicly available code (BDHSC)
resource is set to contribute significantly to advancements.

</details>


### [640] [Attention-Based Fusion of IQ and FFT Spectrograms with AoA Features for GNSS Jammer Localization](https://arxiv.org/abs/2507.14167)
*Lucas Heublein,Christian Wielenberg,Thorsten Nowak,Tobias Feigl,Christopher Mutschler,Felix Ott*

Main category: eess.SP

TL;DR: The paper addresses GNSS signal disruption by jamming devices and proposes a novel method combining attention-based fusion of IQ samples, spectrograms, and AoA features to enhance localization accuracy.


<details>
  <summary>Details</summary>
Motivation: The need to detect and localize GNSS jamming devices is crucial to maintain accurate positioning and mitigate their negative impact, especially in challenging multipath environments.

Method: The authors introduce an attention-based fusion framework integrating IQ samples, FFT spectrograms, and AoA features. They benchmark 128 models and present a novel dataset of moving jamming devices recorded indoors.

Result: The paper demonstrates that the proposed method outperforms existing techniques for localization and classification tasks in dynamic multipath environments.

Conclusion: The proposed approach significantly boosts the reliability of detecting and localizing GNSS jamming devices, laying the groundwork for improved situational awareness and counter-measures.

Abstract: Jamming devices disrupt signals from the global navigation satellite system
(GNSS) and pose a significant threat by compromising the reliability of
accurate positioning. Consequently, the detection and localization of these
interference signals are essential to achieve situational awareness, mitigating
their impact, and implementing effective counter-measures. Classical Angle of
Arrival (AoA) methods exhibit reduced accuracy in multipath environments due to
signal reflections and scattering, leading to localization errors.
Additionally, AoA-based techniques demand substantial computational resources
for array signal processing. In this paper, we propose a novel approach for
detecting and classifying interference while estimating the distance, azimuth,
and elevation of jamming sources. Our benchmark study evaluates 128 vision
encoder and time-series models to identify the highest-performing methods for
each task. We introduce an attention-based fusion framework that integrates
in-phase and quadrature (IQ) samples with Fast Fourier Transform (FFT)-computed
spectrograms while incorporating 22 AoA features to enhance localization
accuracy. Furthermore, we present a novel dataset of moving jamming devices
recorded in an indoor environment with dynamic multipath conditions and
demonstrate superior performance compared to state-of-the-art methods.

</details>


### [641] [Enhancing Generalization in PPG-Based Emotion Measurement with a CNN-TCN-LSTM Model](https://arxiv.org/abs/2507.14173)
*Karim Alghoul,Hussein Al Osman,Abdulmotaleb El Saddik*

Main category: eess.SP

TL;DR: The paper proposes a hybrid architecture combining CNN, LSTM, and TCN for emotion recognition using PPG signals to address generalization challenges across individuals.


<details>
  <summary>Details</summary>
Motivation: To improve emotion recognition systems by overcoming generalization issues in models based on PPG signals, which are commonly collected from wearable devices.

Method: A hybrid approach integrating CNN for feature extraction, LSTM and TCN for sequence processing; their outputs are combined for emotion classification in terms of valence and arousal.

Result: The hybrid model significantly outperformed standalone CNN and CNN-LSTM models in generalization and achieved better metrics such as AUC and F1 Score on the PPGE dataset.

Conclusion: The proposed hybrid architecture enhances emotion recognition accuracy and robustness, addressing individual variability in PPG-based models effectively.

Abstract: Human computer interaction has become integral to modern life, driven by
advancements in machine learning technologies. Affective computing, in
particular, has focused on systems that recognize, interpret, and respond to
human emotions, often using wearable devices, which provide continuous data
streams of physiological signals. Among various physiological signals, the
photoplethysmogram (PPG) has gained prominence due to its ease of acquisition
from widely available devices. However, the generalization of PPG-based emotion
recognition models across individuals remains an unresolved challenge. This
paper introduces a novel hybrid architecture that combines Convolutional Neural
Networks (CNNs), Long Short-Term Memory networks (LSTMs), and Temporal
Convolutional Networks (TCNs) to address this issue. The proposed model
integrates the strengths of these architectures to improve robustness and
generalization. Raw PPG signals are fed into the CNN for feature extraction.
These features are processed separately by LSTM and TCN. The outputs from these
components are concatenated to generate a final feature representation, which
serves as the input for classifying valence and arousal, the primary dimensions
of emotion. Experiments using the Photoplethysmogram Dataset for Emotional
Analysis (PPGE) demonstrate that the proposed hybrid model achieves better
model generalization than standalone CNN and LSTM architectures. Our results
show that the proposed solution outperforms the state-of-the-art CNN
architecture, as well as a CNN-LSTM model, in emotion recognition tasks with
PPG signals. Using metrics such as Area Under the Curve (AUC) and F1 Score, we
highlight the model's effectiveness in handling subject variability.

</details>


### [642] [Latent Sensor Fusion: Multimedia Learning of Physiological Signals for Resource-Constrained Devices](https://arxiv.org/abs/2507.14185)
*Abdullah Ahmed,Jeremy Gummeson*

Main category: eess.SP

TL;DR: This paper introduces a unified encoder for analyzing multimodal physiological signals using latent spaces and sensor-latent fusion.


<details>
  <summary>Details</summary>
Motivation: The research aims to streamline biosignal analysis by leveraging latent spaces to preserve metadata while enabling efficient computation across multiple modalities.

Method: The authors propose a compressed sensing approach utilizing an autoencoder for latent space fusion to achieve modality-agnostic signal analysis.

Result: The experimental results demonstrate that the unified encoder is faster, lighter, and more scalable compared to other modality-specific solutions, maintaining representational accuracy.

Conclusion: The proposed encoder effectively addresses computational constraints in multimodal biosignal analysis, offering a practical solution for resource-limited devices.

Abstract: Latent spaces offer an efficient and effective means of summarizing data
while implicitly preserving meta-information through relational encoding. We
leverage these meta-embeddings to develop a modality-agnostic, unified encoder.
Our method employs sensor-latent fusion to analyze and correlate multimodal
physiological signals. Using a compressed sensing approach with
autoencoder-based latent space fusion, we address the computational challenges
of biosignal analysis on resource-constrained devices. Experimental results
show that our unified encoder is significantly faster, lighter, and more
scalable than modality-specific alternatives, without compromising
representational accuracy.

</details>


### [643] [Boosted Enhanced Quantile Regression Neural Networks with Spatiotemporal Permutation Entropy for Complex System Prognostics](https://arxiv.org/abs/2507.14194)
*David J Poland*

Main category: eess.SP

TL;DR: The paper introduces a new framework combining Spatiotemporal Permutation Entropy and Boosted Enhanced Quantile Regression Neural Networks (BEQRNNs) for pattern prediction and system prognostics, achieving notable improvements in accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenge of understanding complex dynamical patterns in multidimensional systems, which require advanced methods for accurate prediction and uncertainty quantification.

Method: The framework employs two main stages: (1) spatiotemporal entropy extraction to analyze multiscale temporal and spatial data, and (2) integration of BEQRNNs for probabilistic pattern prediction with uncertainty quantification.

Result: The system achieves 81.17% accuracy in spatiotemporal pattern classification up to 200 time steps, along with significant improvements in critical transition detection accuracy (79%) and long-term prediction reliability (81.22%).

Conclusion: The framework proves effective in processing complex multimodal entropy features, showing promise for real-time prognostic and predictive applications across various domains.

Abstract: This paper presents a novel framework for pattern prediction and system
prognostics centered on Spatiotemporal Permutation Entropy analysis integrated
with Boosted Enhanced Quantile Regression Neural Networks (BEQRNNs). We address
the challenge of understanding complex dynamical patterns in multidimensional
systems through an approach that combines entropy-based complexity measures
with advanced neural architectures. The system leverages dual computational
stages: first implementing spatiotemporal entropy extraction optimized for
multiscale temporal and spatial data streams, followed by an integrated BEQRNN
layer that enables probabilistic pattern prediction with uncertainty
quantification. This architecture achieves 81.17% accuracy in spatiotemporal
pattern classification with prediction horizons up to 200 time steps and
maintains robust performance across diverse regimes. Field testing across
chaotic attractors, reaction-diffusion systems, and industrial datasets shows a
79% increase in critical transition detection accuracy and 81.22% improvement
in long-term prediction reliability. The framework's effectiveness in
processing complex, multimodal entropy features demonstrates significant
potential for real-time prognostic applications.

</details>


### [644] [Distributed Machine Learning Approach for Low-Latency Localization in Cell-Free Massive MIMO Systems](https://arxiv.org/abs/2507.14216)
*Manish Kumar,Tzu-Hsuan Chou,Byunghyun Lee,Nicolò Michelusi,David J. Love,Yaguang Zhang,James V. Krogmeier*

Main category: eess.SP

TL;DR: The paper introduces a decentralized machine learning framework for low-latency, precise localization in cell-free massive MIMO systems for 6G networks, achieving comparable accuracy to centralized methods while reducing latency.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the need for low-latency and high-accuracy localization in cellular networks to support real-time applications in emerging 6G architectures.

Method: The proposed framework employs distributed Gaussian process regression models at access points (APs) to independently process angle-of-arrival and received signal strength fingerprints, with final location estimates fused at the user equipment (UE).

Result: Simulation results confirm that the distributed approach achieves localization accuracy similar to centralized schemes while minimizing latency and computational overhead, as shown by reduced uncertainty in position estimates.

Conclusion: The study highlights that distributed machine learning can effectively support low-latency and high-accuracy localization for 6G networks, showcasing its potential for real-time applications without centralized data aggregation.

Abstract: Low-latency localization is critical in cellular networks to support
real-time applications requiring precise positioning. In this paper, we propose
a distributed machine learning (ML) framework for fingerprint-based
localization tailored to cell-free massive multiple-input multiple-output
(MIMO) systems, an emerging architecture for 6G networks. The proposed
framework enables each access point (AP) to independently train a Gaussian
process regression model using local angle-of-arrival and received signal
strength fingerprints. These models provide probabilistic position estimates
for the user equipment (UE), which are then fused by the UE with minimal
computational overhead to derive a final location estimate. This decentralized
approach eliminates the need for fronthaul communication between the APs and
the central processing unit (CPU), thereby reducing latency. Additionally,
distributing computational tasks across the APs alleviates the processing
burden on the CPU compared to traditional centralized localization schemes.
Simulation results demonstrate that the proposed distributed framework achieves
localization accuracy comparable to centralized methods, despite lacking the
benefits of centralized data aggregation. Moreover, it effectively reduces
uncertainty of the location estimates, as evidenced by the 95\% covariance
ellipse. The results highlight the potential of distributed ML for enabling
low-latency, high-accuracy localization in future 6G networks.

</details>


### [645] [Advanced Space Mapping Technique Integrating a Shared Coarse Model for Multistate Tuning-Driven Multiphysics Optimization of Tunable Filters](https://arxiv.org/abs/2507.14220)
*Haitian Hu,Wei Zhang,Feng Feng,Zhiguo Zhang,Qi-Jun Zhang*

Main category: eess.SP

TL;DR: A space mapping technique using electromagnetic (EM)-based coarse models for optimizing tunable filters, combining computational efficiency and precision in simulations.


<details>
  <summary>Details</summary>
Motivation: To enhance the accuracy and computational efficiency of multiphysics optimization in tunable filter design by leveraging EM-based approaches.

Method: Developed a surrogate model composed of a shared EM coarse model and mapping neural networks, enabling efficiency and precision across multiple tuning states.

Result: Improved multiphysics modeling accuracy while reducing training samples and computational costs compared to existing methods.

Conclusion: The proposed space mapping approach successfully balances high modeling accuracy and efficiency for multistate tuning-driven multiphysics optimization.

Abstract: This article introduces an advanced space mapping (SM) technique that applies
a shared electromagnetic (EM)-based coarse model for multistate tuning-driven
multiphysics optimization of tunable filters. The SM method combines the
computational efficiency of EM single-physics simulations with the precision of
multiphysics simulations. The shared coarse model is based on EM single-physics
responses corresponding to various nontunable design parameters values.
Conversely, the fine model is implemented to delineate the behavior of
multiphysics responses concerning both nontunable and tunable design parameter
values. The proposed overall surrogate model comprises multiple subsurrogate
models, each consisting of one shared coarse model and two distinct mapping
neural networks. The responses from the shared coarse model in the EM
single-physics filed offer a suitable approximation for the fine responses in
the multiphysics filed, whereas the mapping neural networks facilitate
transition from the EM single-physics field to the multiphysics field. Each
subsurrogate model maintains consistent nontunable design parameter values but
possesses unique tunable design parameter values. By developing multiple
subsurrogate models, optimization can be simultaneously performed for each
tuning state. Nontunable design parameter values are constrained by all tuning
states, whereas tunable design parameter values are confined to their
respective tuning states. This optimization technique simultaneously accounts
for all the tuning states to fulfill the necessary multiple tuning state
requirements. Multiple EM and multiphysics training samples are generated
concurrently to develop the surrogate model. Compared with existing direct
multiphysics parameterized modeling techniques, our proposed method achieves
superior multiphysics modeling accuracy with fewer training samples and reduced
computational costs.

</details>


### [646] [Diffusion-based translation between unpaired spontaneous premature neonatal EEG and fetal MEG](https://arxiv.org/abs/2507.14224)
*Benoît Brebion,Alban Gallard,Katrin Sippel,Amer Zaylaa,Hubert Preissl,Sahar Moghimi,Fabrice Wallois,Yaël Frégier*

Main category: eess.SP

TL;DR: The study improves understanding of prenatal brain development by transferring knowledge from EEG to fMEG using a novel artificial intelligence method. It exhibits significant advancements over existing approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to bridge the gap in knowledge about fetal brain development since traditional EEG studies focus on premature newborns, leaving the fetal stage insufficiently understood. By leveraging AI, the research aims to enhance the capabilities of fMEG for studying prenatal brain activity.

Method: The method involves creating an unpaired diffusion translation model using dual diffusion bridges with numerical integration enhancements. The model was trained on EEG and fMEG datasets from 30 premature newborns and 44 fetuses, respectively.

Result: The method outperformed prior GANs-based solutions, achieving a 5% improvement in mean squared error and eliminating the mode collapse issue in frequency domain, resulting in high signal fidelity.

Conclusion: This research sets a new benchmark in EEG-to-fMEG unpaired translation, enabling advanced analysis of early brain activity and presenting the method as adaptable for other similar applications.

Abstract: Background and objective: Brain activity in premature newborns has
traditionally been studied using electroencephalography (EEG), leading to
substantial advances in our understanding of early neural development. However,
since brain development takes root at the fetal stage, a critical window of
this process remains largely unknown. The only technique capable of recording
neural activity in the intrauterine environment is fetal magnetoencephalography
(fMEG), but this approach presents challenges in terms of data quality and
scarcity. Using artificial intelligence, the present research aims to transfer
the well-established knowledge from EEG studies to fMEG to improve
understanding of prenatal brain development, laying the foundations for better
detection and treatment of potential pathologies. Methods: We developed an
unpaired diffusion translation method based on dual diffusion bridges, which
notably includes numerical integration improvements to obtain more qualitative
results at a lower computational cost. Models were trained on our unpaired
dataset of bursts of spontaneous activity from 30 high-resolution premature
newborns EEG recordings and 44 fMEG recordings. Results: We demonstrate that
our method achieves significant improvement upon previous results obtained with
Generative Adversarial Networks (GANs), by almost 5% on the mean squared error
in the time domain, and completely eliminating the mode collapse problem in the
frequency domain, thus achieving near-perfect signal fidelity. Conclusion: We
set a new state of the art in the EEG-fMEG unpaired translation problem, as our
developed tool completely paves the way for early brain activity analysis.
Overall, we also believe that our method could be reused for other unpaired
signal translation applications.

</details>


### [647] [MEETI: A Multimodal ECG Dataset from MIMIC-IV-ECG with Signals, Images, Features and Interpretations](https://arxiv.org/abs/2507.15255)
*Deyun Zhang,Xiang Lan,Shijia Geng,Qinghao Zhao,Sumei Fan,Mengling Feng,Shenda Hong*

Main category: eess.SP

TL;DR: This paper introduces MEETI, a multimodal ECG dataset with raw waveforms, images, and interpretation texts, supporting advanced AI development for cardiovascular health.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive multimodal ECG datasets that integrate raw signals, diagnostic images, and interpretation texts, which limits the development of deployable AI systems.

Method: The creation of MEETI, a large-scale synchronized dataset, including raw waveforms, high-resolution images, extracted ECG parameters, and detailed textual interpretations aligned using consistent identifiers.

Result: MEETI provides a unified data structure enabling transformer-based multimodal learning and interpretable cardiac health analysis.

Conclusion: MEETI bridges traditional ECG analysis methods with explainable multimodal AI, offering a new benchmark for cardiovascular research and advancing deployable AI systems.

Abstract: Electrocardiogram (ECG) plays a foundational role in modern cardiovascular
care, enabling non-invasive diagnosis of arrhythmias, myocardial ischemia, and
conduction disorders. While machine learning has achieved expert-level
performance in ECG interpretation, the development of clinically deployable
multimodal AI systems remains constrained, primarily due to the lack of
publicly available datasets that simultaneously incorporate raw signals,
diagnostic images, and interpretation text. Most existing ECG datasets provide
only single-modality data or, at most, dual modalities, making it difficult to
build models that can understand and integrate diverse ECG information in
real-world settings. To address this gap, we introduce MEETI (MIMIC-IV-Ext
ECG-Text-Image), the first large-scale ECG dataset that synchronizes raw
waveform data, high-resolution plotted images, and detailed textual
interpretations generated by large language models. In addition, MEETI includes
beat-level quantitative ECG parameters extracted from each lead, offering
structured parameters that support fine-grained analysis and model
interpretability. Each MEETI record is aligned across four components: (1) the
raw ECG waveform, (2) the corresponding plotted image, (3) extracted feature
parameters, and (4) detailed interpretation text. This alignment is achieved
using consistent, unique identifiers. This unified structure supports
transformer-based multimodal learning and supports fine-grained, interpretable
reasoning about cardiac health. By bridging the gap between traditional signal
analysis, image-based interpretation, and language-driven understanding, MEETI
established a robust foundation for the next generation of explainable,
multimodal cardiovascular AI. It offers the research community a comprehensive
benchmark for developing and evaluating ECG-based AI systems.

</details>


### [648] [Optimal Transceiver Design in Over-the-Air Federated Distillation](https://arxiv.org/abs/2507.15256)
*Zihao Hu,Jia Yan,Ying-Jun Angela Zhang,Jun Zhang,Khaled B. Letaief*

Main category: eess.SP

TL;DR: The paper presents a communication-efficient solution for federated learning by proposing over-the-air federated distillation, where devices share model outputs instead of parameters, reducing overhead.


<details>
  <summary>Details</summary>
Motivation: With the rise of large AI models, conventional federated learning methods face inefficiencies due to high communication demands. This paper aims to address this bottleneck.

Method: The authors propose a framework combining federated learning and knowledge distillation. It uses over-the-air aggregation of model outputs through wireless channels, optimizing transmitter and receiver designs for better learning performance.

Result: The proposed method significantly reduces communication overhead, with only a minor compromise in testing accuracy compared to standard federated learning benchmarks.

Conclusion: Over-the-air federated distillation is efficient for handling large AI models in federated learning, offering a better balance between performance and communication efficiency.

Abstract: The rapid proliferation and growth of artificial intelligence (AI) has led to
the development of federated learning (FL). FL allows wireless devices (WDs) to
cooperatively learn by sharing only local model parameters, without needing to
share the entire dataset. However, the emergence of large AI models has made
existing FL approaches inefficient, due to the significant communication
overhead required. In this paper, we propose a novel over-the-air federated
distillation (FD) framework by synergizing the strength of FL and knowledge
distillation to avoid the heavy local model transmission. Instead of sharing
the model parameters, only the WDs' model outputs, referred to as knowledge,
are shared and aggregated over-the-air by exploiting the superposition property
of the multiple-access channel. We shall study the transceiver design in
over-the-air FD, aiming to maximize the learning convergence rate while meeting
the power constraints of the transceivers. The main challenge lies in the
intractability of the learning performance analysis, as well as the non-convex
nature and the optimization spanning the whole FD training period. To tackle
this problem, we first derive an analytical expression of the convergence rate
in over-the-air FD. Then, the closed-form optimal solutions of the WDs'
transmit power and the estimator for over-the-air aggregation are obtained
given the receiver combining strategy. Accordingly, we put forth an efficient
approach to find the optimal receiver beamforming vector via semidefinite
relaxation. We further prove that there is no optimality gap between the
original and relaxed problem for the receiver beamforming design. Numerical
results will show that the proposed over-the-air FD approach achieves a
significant reduction in communication overhead, with only a minor compromise
in testing accuracy compared to conventional FL benchmarks.

</details>


### [649] [EEG-based Epileptic Prediction via a Two-stage Channel-aware Set Transformer Network](https://arxiv.org/abs/2507.15364)
*Ruifeng Zheng,Cong Chen,Shuang Wang,Yiming Liu,Lin You,Jindong Lu,Ruizhe Zhu,Guodao Zhang,Kejie Huang*

Main category: eess.SP

TL;DR: A novel two-stage Set Transformer Network for seizure prediction using fewer EEG channels was proposed, achieving improved sensitivity and reduced channels.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of current wearable seizure-predicting devices due to the large size of EEG sensors, and thus propose a more efficient method using fewer sensors.

Method: The method involves a two-stage channel-aware Set Transformer Network and a seizure-independent division strategy tested on a public EEG dataset with 22 patients.

Result: Before channel selection, sensitivity was 76.4% with a 0.09/hour false predicting rate. After reducing channel sensors to an average of 2.8, sensitivity increased to 80.1%, with a false predicting rate of 0.11/hour. This method identifies dominant EEG channels in 20 of 22 patients.

Conclusion: The proposed method enables more efficient seizure prediction with fewer EEG channels while maintaining or improving prediction performance. It emphasizes the need for rigorous seizure-independent data division methods.

Abstract: Epilepsy is a chronic, noncommunicable brain disorder, and sudden seizure
onsets can significantly impact patients' quality of life and health. However,
wearable seizure-predicting devices are still limited, partly due to the bulky
size of EEG-collecting devices. To relieve the problem, we proposed a novel
two-stage channel-aware Set Transformer Network that could perform seizure
prediction with fewer EEG channel sensors. We also tested a seizure-independent
division method which could prevent the adjacency of training and test data.
Experiments were performed on the CHB-MIT dataset which includes 22 patients
with 88 merged seizures. The mean sensitivity before channel selection was
76.4% with a false predicting rate (FPR) of 0.09/hour. After channel selection,
dominant channels emerged in 20 out of 22 patients; the average number of
channels was reduced to 2.8 from 18; and the mean sensitivity rose to 80.1%
with an FPR of 0.11/hour. Furthermore, experimental results on the
seizure-independent division supported our assertion that a more rigorous
seizure-independent division should be used for patients with abundant EEG
recordings.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [650] [What do Large Language Models know about materials?](https://arxiv.org/abs/2507.14586)
*Adrian Ehrenhofer,Thomas Wallmersperger,Gianaurelio Cuniberti*

Main category: physics.app-ph

TL;DR: This study explores Large Language Models (LLMs) in material science, focusing on their knowledge of the Periodic Table and their ability to generate accurate material-related information.


<details>
  <summary>Details</summary>
Motivation: To investigate the capability of LLMs in handling engineering-specific knowledge due to the predominantly non-scientific content on the accessible internet.

Method: Analyzed vocabulary, tokenization, and fact-generation of multiple advanced open-state models to assess their ability to produce accurate material-related data.

Result: Developed a material knowledge benchmark to understand LLMs' applicability in the Processing-Structure-Property-Performance (PSPP) chain and identify areas requiring specialized models.

Conclusion: LLMs show potential for certain tasks in material science but need specialized models for specific steps in the PSPP chain.

Abstract: Large Language Models (LLMs) are increasingly applied in the fields of
mechanical engineering and materials science. As models that establish
connections through the interface of language, LLMs can be applied for
step-wise reasoning through the Processing-Structure-Property-Performance chain
of material science and engineering. Current LLMs are built for adequately
representing a dataset, which is the most part of the accessible internet.
However, the internet mostly contains non-scientific content. If LLMs should be
applied for engineering purposes, it is valuable to investigate models for
their intrinsic knowledge -- here: the capacity to generate correct information
about materials. In the current work, for the example of the Periodic Table of
Elements, we highlight the role of vocabulary and tokenization for the
uniqueness of material fingerprints, and the LLMs' capabilities of generating
factually correct output of different state-of-the-art open models. This leads
to a material knowledge benchmark for an informed choice, for which steps in
the PSPP chain LLMs are applicable, and where specialized models are required.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [651] [Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining](https://arxiv.org/abs/2507.14619)
*Van-Hoang Le,Duc-Vu Nguyen,Kiet Van Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.IR

TL;DR: The paper proposes a two-stage framework using a Bi-Encoder for retrieval and a Cross-Encoder for re-ranking to improve legal document search.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle in specialized domains like law, necessitating precision-focused frameworks.

Method: Combines Bi-Encoder for candidate retrieval and Cross-Encoder for final ranking, leveraging negative example mining and the Exist@m metric.

Result: Achieved top-three performance in the SoICT Hackathon with fewer parameters compared to ensemble models.

Conclusion: Optimized data processing and balanced negative sampling are crucial for robust legal document retrieval systems.

Abstract: Large Language Models (LLMs) face significant challenges in specialized
domains like law, where precision and domain-specific knowledge are critical.
This paper presents a streamlined two-stage framework consisting of Retrieval
and Re-ranking to enhance legal document retrieval efficiency and accuracy. Our
approach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,
followed by a Cross-Encoder for precise re-ranking, both optimized through
strategic negative example mining. Key innovations include the introduction of
the Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard
negatives to mitigate training bias, which significantly improved re-ranking
performance. Evaluated on the SoICT Hackathon 2024 for Legal Document
Retrieval, our team, 4Huiter, achieved a top-three position. While
top-performing teams employed ensemble models and iterative self-training on
large bge-m3 architectures, our lightweight, single-pass approach offered a
competitive alternative with far fewer parameters. The framework demonstrates
that optimized data processing, tailored loss functions, and balanced negative
sampling are pivotal for building robust retrieval-augmented systems in legal
contexts.

</details>


### [652] [GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou](https://arxiv.org/abs/2507.15267)
*Ninglu Shao,Jinshan Wang,Chenxu Wang,Qingbiao Li,Xiaoxue Zang,Han Li*

Main category: cs.IR

TL;DR: The paper addresses the lack of research in query recommendation for video-related search and introduces a novel LLM-based framework called GREAT. The method uses a query-based trie to guide query generation, improving relevance through extensive experiments.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of research and datasets in the emerging field of query recommendation for video-related searches, where users need accurate and relevant queries while browsing short videos.

Method: The paper introduces the GREAT framework, which leverages a query-based trie for training and inference. This trie is built from high-quality queries and helps guide the LLM in generating and refining queries. Post-processing ensures relevance and quality.

Result: The proposed GREAT framework shows improved performance for item-to-query recommendations based on extensive offline and online experiments, highlighting its effectiveness.

Conclusion: The GREAT framework provides a robust solution for query recommendation in video-related search, addressing challenges in semantic matching and improving user experience through guided query generation.

Abstract: Currently, short video platforms have become the primary place for
individuals to share experiences and obtain information. To better meet users'
needs for acquiring information while browsing short videos, some apps have
introduced a search entry at the bottom of videos, accompanied with recommended
relevant queries. This scenario is known as query recommendation in
video-related search, where core task is item-to-query (I2Q) recommendation. As
this scenario has only emerged in recent years, there is a notable scarcity of
academic research and publicly available datasets in this domain. To address
this gap, we systematically examine the challenges associated with this
scenario for the first time. Subsequently, we release a large-scale dataset
derived from real-world data pertaining to the query recommendation in
video-\textit{\textbf{r}}elated \textit{\textbf{s}}earch on the
\textit{\textbf{Kuai}}shou app (\textbf{KuaiRS}). Presently, existing methods
rely on embeddings to calculate similarity for matching short videos with
queries, lacking deep interaction between the semantic content and the query.
In this paper, we introduce a novel LLM-based framework named \textbf{GREAT},
which \textit{\textbf{g}}uides que\textit{\textbf{r}}y
g\textit{\textbf{e}}ner\textit{\textbf{a}}tion with a \textit{\textbf{t}}rie to
address I2Q recommendation in related search. Specifically, we initially gather
high-quality queries with high exposure and click-through rate to construct a
query-based trie. During training, we enhance the LLM's capability to generate
high-quality queries using the query-based trie. In the inference phase, the
query-based trie serves as a guide for the token generation. Finally, we
further refine the relevance and literal quality between items and queries via
a post-processing module. Extensive offline and online experiments demonstrate
the effectiveness of our proposed method.

</details>


### [653] [A Reproducibility Study of Product-side Fairness in Bundle Recommendation](https://arxiv.org/abs/2507.14352)
*Huy-Son Nguyen,Yuanna Liu,Masoud Mansoury,Mohammad Alian Nejadi,Alan Hanjalic,Maarten de Rijke*

Main category: cs.IR

TL;DR: This paper investigates fairness issues in bundle recommender systems (BR) by assessing exposure disparities at both bundle and item levels, finding notable differences and highlighting the need for new fairness interventions and metrics.


<details>
  <summary>Details</summary>
Motivation: To address unexplored fairness issues in bundle recommendation, particularly on the product side where exposure discrepancies exist, and to adapt traditional fairness frameworks to the multi-layered settings of BR.

Method: A reproducibility study was conducted using three datasets and four state-of-the-art BR methods. Multiple fairness metrics were applied to analyze exposure disparities at the bundle and item levels.

Result: Exposure patterns diverge significantly between bundles and their constituent items, and user behavior influences fairness outcomes. Different fairness metrics yield varied assessments, necessitating a multi-faceted evaluation.

Conclusion: Fairness interventions in BR must consider both bundle and item levels, as well as user interaction patterns. The study lays groundwork for developing fairer BR systems and highlights gaps for further research in this area.

Abstract: Recommender systems are known to exhibit fairness issues, particularly on the
product side, where products and their associated suppliers receive unequal
exposure in recommended results. While this problem has been widely studied in
traditional recommendation settings, its implications for bundle recommendation
(BR) remain largely unexplored. This emerging task introduces additional
complexity: recommendations are generated at the bundle level, yet user
satisfaction and product (or supplier) exposure depend on both the bundle and
the individual items it contains. Existing fairness frameworks and metrics
designed for traditional recommender systems may not directly translate to this
multi-layered setting. In this paper, we conduct a comprehensive
reproducibility study of product-side fairness in BR across three real-world
datasets using four state-of-the-art BR methods. We analyze exposure
disparities at both the bundle and item levels using multiple fairness metrics,
uncovering important patterns. Our results show that exposure patterns differ
notably between bundles and items, revealing the need for fairness
interventions that go beyond bundle-level assumptions. We also find that
fairness assessments vary considerably depending on the metric used,
reinforcing the need for multi-faceted evaluation. Furthermore, user behavior
plays a critical role: when users interact more frequently with bundles than
with individual items, BR systems tend to yield fairer exposure distributions
across both levels. Overall, our findings offer actionable insights for
building fairer bundle recommender systems and establish a vital foundation for
future research in this emerging domain.

</details>


### [654] [Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module](https://arxiv.org/abs/2507.14612)
*Pei-Xuan Li,Wei-Yun Liang,Fandel Lin,Hsun-Ping Hsieh*

Main category: cs.IR

TL;DR: This paper introduces GDPW, a new framework for next POI recommendations that incorporates POI categories, time factors, and various weights, achieving superior performance over existing models by 3% to 11%.


<details>
  <summary>Details</summary>
Motivation: Current POI recommendation methods neglect the relationship between POI categories and time while also overlooking important factors like POI popularity, transitions, and distances, leading to limitations in prediction accuracy.

Method: The proposed GDPW framework constructs Global Category and Global Category-Time Graphs to learn category and time representations. These are then disentangled through contrastive learning, and final recommendations utilize weighted predictions based on transition and distance factors.

Result: Experiments on two real-world datasets demonstrate improved recommendation performance of GDPW over existing models, with gains ranging from 3% to 11%.

Conclusion: GDPW effectively enhances next POI recommendation by integrating temporal, categorical, and weighted POI information, offering better predictive accuracy than current methods.

Abstract: Next point of interest (POI) recommendation primarily predicts future
activities based on users' past check-in data and current status, providing
significant value to users and service providers. We observed that the popular
check-in times for different POI categories vary. For example, coffee shops are
crowded in the afternoon because people like to have coffee to refresh after
meals, while bars are busy late at night. However, existing methods rarely
explore the relationship between POI categories and time, which may result in
the model being unable to fully learn users' tendencies to visit certain POI
categories at different times. Additionally, existing methods for modeling time
information often convert it into time embeddings or calculate the time
interval and incorporate it into the model, making it difficult to capture the
continuity of time. Finally, during POI prediction, various weighting
information is often ignored, such as the popularity of each POI, the
transition relationships between POIs, and the distances between POIs, leading
to suboptimal performance. To address these issues, this paper proposes a novel
next POI recommendation framework called Graph Disentangler with POI Weighted
Module (GDPW). This framework aims to jointly consider POI category information
and multiple POI weighting factors. Specifically, the proposed GDPW learns
category and time representations through the Global Category Graph and the
Global Category-Time Graph. Then, we disentangle category and time information
through contrastive learning. After prediction, the final POI recommendation
for users is obtained by weighting the prediction results based on the
transition weights and distance relationships between POIs. We conducted
experiments on two real-world datasets, and the results demonstrate that the
proposed GDPW outperforms other existing models, improving performance by 3% to
11%.

</details>


### [655] [LOVO: Efficient Complex Object Query in Large-Scale Video Datasets](https://arxiv.org/abs/2507.14301)
*Yuxin Liu,Yuezhang Peng,Hefeng Zhou,Hongze Liu,Xinyu Lu,Jiong Lou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.IR

TL;DR: The paper introduces LOVO, an innovative system for querying objects in large-scale video datasets with high efficiency, low latency, and adaptability.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations in existing video analysis methods that struggle with adaptability to unseen object classes and suffer from high query latency.

Method: LOVO uses feature extraction with pre-trained visual encoders to create compact visual embeddings, organizes data in an inverted multi-index structure within a vector database, performs approximate nearest-neighbor searches, and refines results with cross-modal reranking.

Result: LOVO achieves superior performance over existing methods, offering near-optimal accuracy, up to 85x lower latency, and reduced index construction costs.

Conclusion: LOVO sets a new standard in video analysis for complex object queries by delivering scalable, efficient, and accurate solutions for dynamic data environments.

Abstract: The widespread deployment of cameras has led to an exponential increase in
video data, creating vast opportunities for applications such as traffic
management and crime surveillance. However, querying specific objects from
large-scale video datasets presents challenges, including (1) processing
massive and continuously growing data volumes, (2) supporting complex query
requirements, and (3) ensuring low-latency execution. Existing video analysis
methods struggle with either limited adaptability to unseen object classes or
suffer from high query latency. In this paper, we present LOVO, a novel system
designed to efficiently handle comp$\underline{L}$ex $\underline{O}$bject
queries in large-scale $\underline{V}$ide$\underline{O}$ datasets. Agnostic to
user queries, LOVO performs one-time feature extraction using pre-trained
visual encoders, generating compact visual embeddings for key frames to build
an efficient index. These visual embeddings, along with associated bounding
boxes, are organized in an inverted multi-index structure within a vector
database, which supports queries for any objects. During the query phase, LOVO
transforms object queries to query embeddings and conducts fast approximate
nearest-neighbor searches on the visual embeddings. Finally, a cross-modal
rerank is performed to refine the results by fusing visual features with
detailed textual features. Evaluation on real-world video datasets demonstrates
that LOVO outperforms existing methods in handling complex queries, with
near-optimal query accuracy and up to 85x lower search latency, while
significantly reducing index construction costs. This system redefines the
state-of-the-art object query approaches in video analysis, setting a new
benchmark for complex object queries with a novel, scalable, and efficient
approach that excels in dynamic environments.

</details>


### [656] [U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs](https://arxiv.org/abs/2507.14902)
*Xiaojie Li,Chu Li,Shi-Zhe Chen,Xi Chen*

Main category: cs.IR

TL;DR: This paper investigates universal multimodal retrieval (UMR) tasks using multi-language large models (MLLMs), addressing key factors for effective embedding learning and proposing the U-MARVEL framework, which advances state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the limitations of current MLLM-based retrieval systems, where the mechanisms behind their performance and generalization are insufficiently understood, leading to possible suboptimal outcomes.

Method: The authors implemented a general MLLM-based embedding learning pipeline and carried out a systematic analysis of factors such as embedding generation, progressive transition, hard negative mining, and re-ranker distillation to optimize performance.

Result: The study introduced the U-MARVEL framework, which surpasses existing methods on the M-BEIR benchmark in supervised settings and demonstrates significant zero-shot effectiveness across various tasks like composed image retrieval and text-to-video retrieval.

Conclusion: The research highlights often-overlooked factors in UMR and positions U-MARVEL as a highly generalized and effective framework for diverse embedding-based retrieval tasks.

Abstract: Universal multimodal retrieval (UMR), which aims to address complex retrieval
tasks where both queries and candidates span diverse modalities, has been
significantly advanced by the emergence of MLLMs. While state-of-the-art
MLLM-based methods in the literature predominantly adopt contrastive learning
principles, they often differ in their specific training recipes. Despite their
success, the mechanisms underlying their retrieval capabilities remain largely
unexplored, potentially resulting in suboptimal performance and limited
generalization ability. To address these issues, we present a comprehensive
study aimed at uncovering the key factors that drive effective embedding
learning for UMR using MLLMs. We begin by implementing a general MLLM-based
embedding learning pipeline, and systematically analyze the primary
contributors to high-performing universal retrieval systems. Based on this, we
explore various aspects of the details in embedding generation and training
strategies, including progressive transition, hard negative mining and
re-ranker distillation. Notably, our findings reveal that often-overlooked
factors can have a substantial impact on model performance. Building on these
discoveries, we introduce a unified framework termed U-MARVEL
(\textbf{U}niversal \textbf{M}ultimod\textbf{A}l \textbf{R}etrie\textbf{V}al
via \textbf{E}mbedding \textbf{L}earning), which outperforms state-of-the-art
competitors on the M-BEIR benchmark by a large margin in supervised settings,
and also exihibits strong zero-shot performance on several tasks such as
composed image retrieval and text-to-video retrieval. These results underscore
the generalization potential of our framework across various embedding-based
retrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL

</details>


### [657] [Understanding Matching Mechanisms in Cross-Encoders](https://arxiv.org/abs/2507.14604)
*Mathias Vast,Basile Van Cooten,Laure Soulier,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: This paper explores simple methods to provide insights into the behavior of neural IR architectures like cross-encoders, focusing on attention processes and matching detection.


<details>
  <summary>Details</summary>
Motivation: To understand and explain the internal mechanisms of highly effective neural IR architectures, particularly cross-encoders, since current explanations focus on high-level processes but neglect the matching process itself.

Method: Investigates the attention process of neural IR models to extract causal insights and interprets the mechanism behind matching detection using straightforward methods.

Result: Identified the critical role of specific attention heads in the attention process and provided an interpretation of how matching detection operates within these models.

Conclusion: The study shows that simpler interpretability methods can offer meaningful insights into neural IR mechanisms, eliminating the need to rely solely on complex mechanistic interpretability approaches.

Abstract: Neural IR architectures, particularly cross-encoders, are highly effective
models whose internal mechanisms are mostly unknown. Most works trying to
explain their behavior focused on high-level processes (e.g., what in the input
influences the prediction, does the model adhere to known IR axioms) but fall
short of describing the matching process. Instead of Mechanistic
Interpretability approaches which specifically aim at explaining the hidden
mechanisms of neural models, we demonstrate that more straightforward methods
can already provide valuable insights. In this paper, we first focus on the
attention process and extract causal insights highlighting the crucial roles of
some attention heads in this process. Second, we provide an interpretation of
the mechanism underlying matching detection.

</details>


### [658] [FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall in Patent Retrieval](https://arxiv.org/abs/2507.14946)
*Amna Ali,Liyanage C. De Silva,Pg Emeroylariffion Abas*

Main category: cs.IR

TL;DR: The FullRecall approach achieves 100% recall in patent retrieval tasks, significantly outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: The intricate nature of patent data makes retrieval challenging, necessitating methods to maximize recall and ensure reliability.

Method: FullRecall uses IPC-guided keyphrase generation and a multi-phase process combining retrieval and ranking to balance recall and precision effectively.

Result: Experiments demonstrated 100% recall across five test cases, outperforming baseline methods HRR2 and ReQ-ReC.

Conclusion: The approach strengthens patent verification processes and mitigates legal risks by ensuring all relevant documents are retrieved.

Abstract: Patent examiners and inventors face significant pressure to verify the
originality and non-obviousness of inventions, and the intricate nature of
patent data intensifies the challenges of patent retrieval. Therefore, there is
a pressing need to devise cutting-edge retrieval strategies that can reliably
achieve the desired recall. This study introduces FullRecall, a novel patent
retrieval approach that effectively manages the complexity of patent data while
maintaining the reliability of relevance matching and maximising recall. It
leverages IPC-guided knowledge to generate informative phrases, which are
processed to extract key information in the form of noun phrases characterising
the query patent under observation. From these, the top k keyphrases are
selected to construct a query for retrieving a focused subset of the dataset.
This initial retrieval step achieves complete recall, successfully capturing
all relevant documents. To further refine the results, a ranking scheme is
applied to the retrieved subset, reducing its size while maintaining 100%
recall. This multi-phase process demonstrates an effective strategy for
balancing precision and recall in patent retrieval tasks. Comprehensive
experiments were conducted, and the results were compared with baseline
studies, namely HRR2 [1] and ReQ-ReC [2]. The proposed approach yielded
superior results, achieving 100% recall in all five test cases. However,
HRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and
14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the
second test case, and 0% for the third, fourth, and fifth test cases. The 100%
recall ensures that no relevant prior art is overlooked, thereby strengthening
the patent pre-filing and examination processes, hence reducing potential legal
risks.

</details>


### [659] [SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search](https://arxiv.org/abs/2507.15245)
*Xiaofeng Shi,Yuduo Li,Qian Kou,Longbin Yu,Jinxin Xie,Hua Zhou*

Main category: cs.IR

TL;DR: SPAR, a multi-agent framework, is introduced to enhance academic literature retrieval using query decomposition and evolution. It outperforms baselines in benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing academic literature retrieval systems are rigid and lack advanced reasoning capabilities, necessitating a more flexible and effective solution.

Method: SPAR is based on query decomposition and query evolution using a multi-agent framework. It includes the creation of a new benchmark, SPARBench, with expert-annotated labels for evaluation.

Result: SPAR surpasses existing baselines, improving F1 scores by up to 56% on AutoScholar and 23% on SPARBench.

Conclusion: SPAR and SPARBench provide a robust and interpretable foundation for advancing research in scholarly literature retrieval.

Abstract: Recent advances in large language models (LLMs) have opened new opportunities
for academic literature retrieval. However, existing systems often rely on
rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,
a multi-agent framework that incorporates RefChain-based query decomposition
and query evolution to enable more flexible and effective search. To facilitate
systematic evaluation, we also construct SPARBench, a challenging benchmark
with expert-annotated relevance labels. Experimental results demonstrate that
SPAR substantially outperforms strong baselines, achieving up to +56% F1 on
AutoScholar and +23% F1 on SPARBench over the best-performing baseline.
Together, SPAR and SPARBench provide a scalable, interpretable, and
high-performing foundation for advancing research in scholarly retrieval. Code
and data will be available at: https://github.com/xiaofengShi/SPAR

</details>


### [660] [Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation](https://arxiv.org/abs/2507.15826)
*Alessandro B. Melchiorre,Elena V. Epure,Shahed Masoudian,Gustavo Escobedo,Anna Hausberger,Manuel Moussallam,Markus Schedl*

Main category: cs.IR

TL;DR: JAM (Just Ask for Music) is a lightweight framework for natural language music recommendation that uses vector translations in a shared latent space and incorporates multimodal item features for accurate and practical recommendations.


<details>
  <summary>Details</summary>
Motivation: To improve natural language interfaces for music recommendation by addressing high costs and latency in LLMs and the limitations of current retrieval-based approaches like single-modal representations and lack of long-term user preferences.

Method: JAM employs vector translations in a shared latent space (inspired by knowledge graph embeddings), uses cross-attention and sparse mixture-of-experts to aggregate multimodal item features, and introduces the JAMSessions dataset with over 100k user-query-item triples.

Result: JAM achieves accurate music recommendations, generates intuitive representations suitable for real-world applications, and can be integrated with existing recommendation systems.

Conclusion: JAM is an effective, cost-efficient, and practical solution for conversational music recommendation that addresses scalability and preference complexity challenges.

Abstract: Natural language interfaces offer a compelling approach for music
recommendation, enabling users to express complex preferences conversationally.
While Large Language Models (LLMs) show promise in this direction, their
scalability in recommender systems is limited by high costs and latency.
Retrieval-based approaches using smaller language models mitigate these issues
but often rely on single-modal item representations, overlook long-term user
preferences, and require full model retraining, posing challenges for
real-world deployment. In this paper, we present JAM (Just Ask for Music), a
lightweight and intuitive framework for natural language music recommendation.
JAM models user-query-item interactions as vector translations in a shared
latent space, inspired by knowledge graph embedding methods like TransE. To
capture the complexity of music and user intent, JAM aggregates multimodal item
features via cross-attention and sparse mixture-of-experts. We also introduce
JAMSessions, a new dataset of over 100k user-query-item triples with anonymized
user/item embeddings, uniquely combining conversational queries and user
long-term preferences. Our results show that JAM provides accurate
recommendations, produces intuitive representations suitable for practical use
cases, and can be easily integrated with existing music recommendation stacks.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [661] [Collusion-Resilient Hierarchical Secure Aggregation with Heterogeneous Security Constraints](https://arxiv.org/abs/2507.14768)
*Zhou Li,Xiang Zhang,Jiawen Lv,Jihao Fan,Haiqiang Chen,Giuseppe Caire*

Main category: cs.IT

TL;DR: The paper studies weakly-secure hierarchical secure aggregation (WS-HSA) in federated learning to address heterogeneous security needs and introduces a method to optimize key rates for these settings.


<details>
  <summary>Details</summary>
Motivation: Federated learning environments often involve hierarchical network setups where users have varying security requirements. Current approaches in hierarchical secure aggregation (HSA) struggle with these heterogeneous security demands, motivating a more flexible framework.

Method: The authors propose WS-HSA, a flexible framework focused on protecting inputs selectively, based on predefined collections of security input sets and collusion sets. They analyze the optimal total key rate and provide bounds for scenarios where optimality isn't fully characterized.

Result: The paper offers a full characterization of the optimal key rate for many parameter settings and establishes constant-factor gap bounds for the remaining cases, improving efficiency and adaptability in secure aggregation.

Conclusion: WS-HSA provides a flexible and efficient method for addressing diverse security needs in hierarchical federated learning setups, optimizing secret key usage and maintaining security guarantees.

Abstract: Motivated by federated learning (FL), secure aggregation (SA) aims to
securely compute, as efficiently as possible, the sum of a set of inputs
distributed across many users. To understand the impact of network topology,
hierarchical secure aggregation (HSA) investigated the communication and secret
key generation efficiency in a 3-layer relay network, where clusters of users
are connected to the aggregation server through an intermediate layer of
relays. Due to the pre-aggregation of the messages at the relays, HSA reduces
the communication burden on the relay-to-server links and is able to support a
large number of users. However, as the number of users increases, a practical
challenge arises from heterogeneous security requirements--for example, users
in different clusters may require varying levels of input protection. Motivated
by this, we study weakly-secure HSA (WS-HSA) with collusion resilience, where
instead of protecting all the inputs from any set of colluding users, only the
inputs belonging to a predefined collection of user groups (referred to as
security input sets) need to be protected against another predefined collection
of user groups (referred to as collusion sets). Since the security input sets
and collusion sets can be arbitrarily defined, our formulation offers a
flexible framework for addressing heterogeneous security requirements in HSA.
We characterize the optimal total key rate, i.e., the total number of
independent key symbols required to ensure both server and relay security, for
a broad range of parameter configurations. For the remaining cases, we
establish lower and upper bounds on the optimal key rate, providing
constant-factor gap optimality guarantees.

</details>


### [662] [A DPI-PAC-Bayesian Framework for Generalization Bounds](https://arxiv.org/abs/2507.14795)
*Muhan Guan,Farhad Farokhi,Jingge Zhu*

Main category: cs.IT

TL;DR: The paper introduces a unified DPI-PAC-Bayesian framework to derive tighter generalization error bounds by integrating the Data Processing Inequality.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve the generalization error bounds in supervised learning models by combining data-processing principles and PAC-Bayesian techniques.

Method: The authors embed the Data Processing Inequality into the change-of-measure approach to calculate explicit bounds using various divergence measures between prior and posterior distributions.

Result: The framework yields explicit error bounds using Renyi, Hellinger, and Chi-Squared divergences, recovering classical bounds, and eliminating extraneous slack factors for tighter results.

Conclusion: The DPI-PAC-Bayesian framework complements data-processing and PAC-Bayesian methods, offering a flexible tool for constructing precise generalization guarantees.

Abstract: We develop a unified Data Processing Inequality PAC-Bayesian framework --
abbreviated DPI-PAC-Bayesian -- for deriving generalization error bounds in the
supervised learning setting. By embedding the Data Processing Inequality (DPI)
into the change-of-measure technique, we obtain explicit bounds on the binary
Kullback-Leibler generalization gap for both R\'enyi divergence and any
$f$-divergence measured between a data-independent prior distribution and an
algorithm-dependent posterior distribution. We present three bounds derived
under our framework using R\'enyi, Hellinger \(p\) and Chi-Squared divergences.
Additionally, our framework also demonstrates a close connection with other
well-known bounds. When the prior distribution is chosen to be uniform, our
bounds recover the classical Occam's Razor bound and, crucially, eliminate the
extraneous \(\log(2\sqrt{n})/n\) slack present in the PAC-Bayes bound, thereby
achieving tighter results. The framework thus bridges data-processing and
PAC-Bayesian perspectives, providing a flexible, information-theoretic tool to
construct generalization guarantees.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [663] [Numerical Artifacts in Learning Dynamical Systems](https://arxiv.org/abs/2507.14491)
*Bing-Ze Lu,Richard Tsai*

Main category: math.NA

TL;DR: This paper investigates how numerical schemes can lead to incorrect identifications of dynamical systems.


<details>
  <summary>Details</summary>
Motivation: To examine the effects of numerical integration schemes on the learning outcomes of dynamical systems from finite data points.

Method: Analyzed the impact of numerical schemes during optimization of dynamical systems by assessing misinterpretations in oscillatory behavior when fitting sampled data.

Result: Demonstrated that damped oscillatory systems risk being misinterpreted as 'anti-damping' with reversed oscillation directions during numerical optimization.

Conclusion: Numerical schemes can significantly distort the learning process, thus careful consideration is necessary to ensure correct identification of dynamical systems.

Abstract: In many applications, one needs to learn a dynamical system from its
solutions sampled at a finite number of time points. The learning problem is
often formulated
  as an optimization problem over a chosen function class. However, in the
optimization procedure, it is necessary to employ a numerical scheme to
integrate candidate dynamical systems and assess how their solutions fit the
data.
  This paper reveals potentially serious effects of a chosen numerical scheme
on the learning outcome. In particular, our analysis demonstrates that a damped
oscillatory system may be incorrectly identified as having "anti-damping" and
exhibiting a reversed oscillation direction, despite adequately fitting the
given data points.

</details>


### [664] [Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy iteration](https://arxiv.org/abs/2507.15455)
*Hee Jun Yang,Min Jung Kim,Yeoneung Kim*

Main category: math.NA

TL;DR: The paper presents a framework combining policy iteration and physics-informed neural networks (PINNs) to solve complex HJI equations, validated numerically and theoretically.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving high-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations which are important in stochastic games, robust control, and other domains, and for which traditional methods struggle due to dimensionality and computational complexity.

Method: The authors developed a mesh-free policy iteration framework integrating PINNs. The method alternates between solving linear second-order PDEs with fixed feedback policies and updating controls via automatic differentiation-based minimax optimization. They also provided a theoretical proof of local uniform convergence and equi-Lipschitz regularity under standard assumptions.

Result: The method demonstrated high accuracy and scalability in numerical experiments, including matching finite-difference benchmarks in 2D and outperforming direct PINN solvers in 5D and 10D problems, with improvements in value function smoothness and residuals.

Conclusion: Integrating PINNs with policy iteration is both theoretically sound and practically effective for solving high-dimensional, nonconvex HJI equations. This approach has promising applications in robotics, finance, and multi-agent reinforcement learning.

Abstract: We propose a mesh-free policy iteration framework that combines classical
dynamic programming with physics-informed neural networks (PINNs) to solve
high-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in
stochastic differential games and robust control. The method alternates between
solving linear second-order PDEs under fixed feedback policies and updating the
controls via pointwise minimax optimization using automatic differentiation.
Under standard Lipschitz and uniform ellipticity assumptions, we prove that the
value function iterates converge locally uniformly to the unique viscosity
solution of the HJI equation. The analysis establishes equi-Lipschitz
regularity of the iterates, enabling provable stability and convergence without
requiring convexity of the Hamiltonian. Numerical experiments demonstrate the
accuracy and scalability of the method. In a two-dimensional stochastic
path-planning game with a moving obstacle, our method matches finite-difference
benchmarks with relative $L^2$-errors below %10^{-2}%. In five- and
ten-dimensional publisher-subscriber differential games with anisotropic noise,
the proposed approach consistently outperforms direct PINN solvers, yielding
smoother value functions and lower residuals. Our results suggest that
integrating PINNs with policy iteration is a practical and theoretically
grounded method for solving high-dimensional, nonconvex HJI equations, with
potential applications in robotics, finance, and multi-agent reinforcement
learning.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [665] [Transaction Profiling and Address Role Inference in Tokenized U.S. Treasuries](https://arxiv.org/abs/2507.14808)
*Junliang Luo,Katrin Tinn,Samuel Ferreira Duran,Di Wu,Xue Liu*

Main category: q-fin.CP

TL;DR: This paper studies transaction-level behaviors of tokenized U.S. Treasuries on blockchains, analyzing their issuance, redemption, and usage patterns, and models economic roles with advanced learning frameworks.


<details>
  <summary>Details</summary>
Motivation: Tokenized Treasuries are rapidly expanding as a financial mechanism on blockchains, offering sovereign debt-backed and yield-bearing instruments. However, little empirical work exists at the transaction level to understand their functionality and participant behavior.

Method: The authors analyze decoded contract calls of Treasury-backed tokens like BUIDL, BENJI, and USDY across blockchains including Ethereum, using both segmentation of user behavior and a curvature-aware Poincaré embedding framework for role modeling.

Result: Their method outperforms baseline models in inferring economic roles for blockchain participants, and it generalizes to tasks like anomaly detection and wallet classification. Segmentation patterns between different user types are also uncovered.

Conclusion: This research advances understanding of tokenized Treasury markets by dissecting functional primitives and participant roles, enabling better insights into on-chain financialization dynamics.

Abstract: Tokenized U.S. Treasuries have emerged as a prominent subclass of real-world
assets (RWAs), offering cryptographically enforced, yield-bearing instruments
collateralized by sovereign debt and deployed across multiple blockchain
networks. While the market has expanded rapidly, empirical analyses of
transaction-level behaviour remain limited. This paper conducts a quantitative,
function-level dissection of U.S. Treasury-backed RWA tokens including BUIDL,
BENJI, and USDY, across multi-chain: mostly Ethereum and Layer-2s. We analyze
decoded contract calls to isolate core functional primitives such as issuance,
redemption, transfer, and bridge activity, revealing segmentation in behaviour
between institutional actors and retail users. To model address-level economic
roles, we introduce a curvature-aware representation learning framework using
Poincar\'e embeddings and liquidity-based graph features. Our method
outperforms baseline models on our RWA Treasury dataset in role inference and
generalizes to downstream tasks such as anomaly detection and wallet
classification in broader blockchain transaction networks. These findings
provide a structured understanding of functional heterogeneity and participant
roles in tokenized Treasury in a transaction-level perspective, contributing
new empirical evidence to the study of on-chain financialization.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [666] [Neural Brownian Motion](https://arxiv.org/abs/2507.14499)
*Qian Qi*

Main category: math.PR

TL;DR: The paper introduces Neural-Brownian Motion (NBM), a stochastic process leveraging neural networks for learned uncertainty modeling through a unique axiom of non-linear martingale behavior.


<details>
  <summary>Details</summary>
Motivation: To redefine stochastic processes for learned uncertainty by replacing classical linear martingale properties with a novel axiom using non-linear neural expectations.

Method: The paper proposes using a Neural Expectation Operator derived from a Backward Stochastic Differential Equation (BSDE) where its driver is neural-network parameterized. A representation theorem is developed under structural constraints.

Result: Canonical Neural-Brownian Motion exists uniquely as a strong solution to a specific stochastic differential equation, with volatility implicitly defined by an algebraic constraint.

Conclusion: NBM introduces a new paradigm in stochastic calculus, allowing the measure's drift and character to be discovered based on learned features, with applications where attitudes toward uncertainty are endogenous.

Abstract: This paper introduces the Neural-Brownian Motion (NBM), a new class of
stochastic processes for modeling dynamics under learned uncertainty. The NBM
is defined axiomatically by replacing the classical martingale property with
respect to linear expectation with one relative to a non-linear Neural
Expectation Operator, $\varepsilon^\theta$, generated by a Backward Stochastic
Differential Equation (BSDE) whose driver $f_\theta$ is parameterized by a
neural network. Our main result is a representation theorem for a canonical
NBM, which we define as a continuous $\varepsilon^\theta$-martingale with zero
drift under the physical measure. We prove that, under a key structural
assumption on the driver, such a canonical NBM exists and is the unique strong
solution to a stochastic differential equation of the form ${\rm d} M_t =
\nu_\theta(t, M_t) {\rm d} W_t$. Crucially, the volatility function
$\nu_\theta$ is not postulated a priori but is implicitly defined by the
algebraic constraint $g_\theta(t, M_t, \nu_\theta(t, M_t)) = 0$, where
$g_\theta$ is a specialization of the BSDE driver. We develop the stochastic
calculus for this process and prove a Girsanov-type theorem for the quadratic
case, showing that an NBM acquires a drift under a new, learned measure. The
character of this measure, whether pessimistic or optimistic, is endogenously
determined by the learned parameters $\theta$, providing a rigorous foundation
for models where the attitude towards uncertainty is a discoverable feature.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [667] [Remote Assistance or Remote Driving: The Impact of Operational Design Domains on ADS-Supporting Systems Selection](https://arxiv.org/abs/2507.14347)
*Ole Hans,Benedikt Walter*

Main category: eess.SY

TL;DR: This paper focuses on a structured approach to choosing between Remote Driving Systems (RDS) and Remote Assistance Systems (RAS) for high-level Automated Driving Systems (ADS) based on use case analysis and the Operational Design Domain (ODD).


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of determining the most suitable remote support system (RDS or RAS) for Level 4 ADS, especially as current evaluation methods overlook critical factors like use cases and ODD complementarity.

Method: The proposed method combines the use of the PEGASUS framework to systematically describe and analyze ODD, alongside a structured framework that evaluates RDS and RAS suitability based on predefined criteria.

Result: The outcome is a structured and criteria-driven framework that aids developers in making informed decisions about the appropriate remote support system for a specific ADS application within its ODD.

Conclusion: The paper provides a practical, systematic approach to streamline development efforts and enhance the compatibility between automated driving systems and their respective remote support systems, focusing on the context of ODD and use cases.

Abstract: High level Automated Driving Systems (ADS) can handle many situations, but
they still encounter situations where human intervention is required. In
systems where a physical driver is present in the vehicle, typically SAE Level
3 systems, this intervention is relatively straightforward and is handled by
the in-vehicle driver. However, the complexity increases for Level 4 systems,
where, in most cases, no physical driver remains in the vehicle. The two common
industry solutions for this challenge are the integration of a remote support
system, such as a Remote Driving System (RDS) or Remote Assistance System
(RAS). While it is clear that ADS will require one of these systems, it is less
clear how the suitability of either system for a particular ADS application
should be evaluated. Currently, the selection process often focuses on system
architecture as well as its design and integration challenges. Furthermore,
since many ADS developers choose to develop remote system solutions in-house,
it is advantageous to select the simpler approach to streamline development and
integration efforts. While these decision points are certainly relevant, this
approach overlooks the most critical factors: the use cases and the
complementarity of the ADS and the remote support system within the context of
the Operational Design Design Domain (ODD). This paper proposes a structured
approach for selecting between RDS and RAS as an ADS support system, based on
the defined ODD and use case analysis. To achieve this, the paper applies the
PEGASUS framework to systematically describe and analyze the ODD. A structured
framework is introduced to evaluate and select the most suitable remote support
system for an ADS based on clearly defined criteria.

</details>


### [668] [Improving Functional Reliability of Near-Field Monitoring for Emergency Braking in Autonomous Vehicles](https://arxiv.org/abs/2507.15594)
*Junnan Pan,Prodromos Sotiriadis,Vladislav Nenchev,Ferdinand Englberger*

Main category: eess.SY

TL;DR: The paper proposes three enhanced strategies for near-field hazard detection in autonomous vehicles and reports improved reliability through simulation experiments.


<details>
  <summary>Details</summary>
Motivation: To address the safety risks posed by undetected near-field obstacles and to reduce false positives in near-field monitoring systems for autonomous vehicles.

Method: Three novel near-field monitoring strategies are developed, focusing on dynamic spatial properties, object size relevance, and motion-aware predictions, and their performance is analyzed in simulation.

Result: Simulation experiments show that the improved strategies enhance the reliability of near-field monitoring systems compared to the initial approach.

Conclusion: The presented strategies offer a promising solution to improving safety in autonomous vehicles by better detecting near-field hazards while reducing false positives.

Abstract: Autonomous vehicles require reliable hazard detection. However, primary
sensor systems may miss near-field obstacles, resulting in safety risks.
Although a dedicated fast-reacting near-field monitoring system can mitigate
this, it typically suffers from false positives. To mitigate these, in this
paper, we introduce three monitoring strategies based on dynamic spatial
properties, relevant object sizes, and motion-aware prediction. In experiments
in a validated simulation, we compare the initial monitoring strategy against
the proposed improvements. The results demonstrate that the proposed strategies
can significantly improve the reliability of near-field monitoring systems.

</details>


### [669] [Large Language Model as An Operator: An Experience-Driven Solution for Distribution Network Voltage Control](https://arxiv.org/abs/2507.14800)
*Xu Yang,Chenhui Lin,Haotian Liu,Qi Wang,Wenchuan Wu*

Main category: eess.SY

TL;DR: The paper introduces an LLM-based voltage control solution for power distribution networks that uses experience-driven methods and demonstrates its effectiveness through experiments.


<details>
  <summary>Details</summary>
Motivation: To explore how large language models (LLMs) can autonomously generate better dispatch strategies for voltage control in power systems.

Method: The method includes the use of LLMs in a modular framework with components for experience storage, retrieval, generation, and modification to evolve voltage control strategies.

Result: Experiments confirm the effectiveness of the LLM-based method and its applicability for solving power system dispatch issues.

Conclusion: The integration of LLMs into power systems offers a promising approach for autonomous and adaptable voltage control strategies.

Abstract: With the advanced reasoning and information analysis capabilities, large
language models (LLMs) can offer a novel approach for the autonomous generation
of dispatch strategies in power systems. This letter proposes an LLM-based
experience-driven voltage control solution for distribution networks, which
enables the self-evolution of LLM-based voltage control strategies through the
collaboration and interaction of multiple modules-specifically, experience
storage, experience retrieval, experience generation, and experience
modification. Comprehensive experimental results validate the effectiveness of
the proposed method and highlight the applicability of LLM in addressing power
system dispatch challenges.

</details>


### [670] [Physics-Informed Learning of Proprietary Inverter Models for Grid Dynamic Studies](https://arxiv.org/abs/2507.15259)
*Kyung-Bin Kwon,Sayak Mukherjee,Ramij R. Hossain,Marcelo Elizondo*

Main category: eess.SY

TL;DR: This paper introduces a physics-informed neural ODE framework to improve grid dynamic simulations by emulating inverter dynamics without requiring proprietary details.


<details>
  <summary>Details</summary>
Motivation: Dynamic simulations in grids face inaccuracies due to the lack of disclosed internal controls and parameters from original equipment manufacturers (OEMs).

Method: The authors proposed a Physics-Informed Latent Neural ODE Model (PI-LNM) that merges physics-based modeling and neural network layers to emulate unmodeled inverter behaviors.

Result: The model showed improved dynamic simulation accuracy compared to purely data-driven methods during a case study on grid-forming inverters.

Conclusion: Integrating physics into neural learning layers enhances the modeling of proprietary inverter dynamics, addressing key challenges in dynamic grid simulations.

Abstract: This letter develops a novel physics-informed neural ordinary differential
equations-based framework to emulate the proprietary dynamics of the inverters
-- essential for improved accuracy in grid dynamic simulations. In current
industry practice, the original equipment manufacturers (OEMs) often do not
disclose the exact internal controls and parameters of the inverters, posing
significant challenges in performing accurate dynamic simulations and other
relevant studies, such as gain tunings for stability analysis and controls. To
address this, we propose a Physics-Informed Latent Neural ODE Model (PI-LNM)
that integrates system physics with neural learning layers to capture the
unmodeled behaviors of proprietary units. The proposed method is validated
using a grid-forming inverter (GFM) case study, demonstrating improved dynamic
simulation accuracy over approaches that rely solely on data-driven learning
without physics-based guidance.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [671] [U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model](https://arxiv.org/abs/2507.14237)
*Louis Bahrman,Mathieu Fontaine,Gaël Richard*

Main category: cs.SD

TL;DR: The paper proposes a sequential learning strategy for dereverberation using Bayesian formulation and deep neural networks, eliminating the need for paired dry and reverberant data.


<details>
  <summary>Details</summary>
Motivation: Paired dry and reverberant data are hard to obtain for typical dereverberation tasks, prompting the need for an alternative method that relies only on reverberant signals and acoustic models.

Method: A sequential learning strategy leveraging a Bayesian approach is designed, where acoustic parameters and dry signals are estimated from reverberant inputs using deep neural networks and a reverberation matching loss.

Result: The proposed data-efficient method outperformed an unsupervised baseline using only 100 samples with reverberation-parameter labels, showing effectiveness in low-resource settings.

Conclusion: The strategy is a practical solution for dereverberation in scenarios with limited resources, providing a viable alternative to current deep learning methods requiring paired data.

Abstract: This paper explores the outcome of training state-ofthe-art dereverberation
models with supervision settings ranging from weakly-supervised to fully
unsupervised, relying solely on reverberant signals and an acoustic model for
training. Most of the existing deep learning approaches typically require
paired dry and reverberant data, which are difficult to obtain in practice. We
develop instead a sequential learning strategy motivated by a bayesian
formulation of the dereverberation problem, wherein acoustic parameters and dry
signals are estimated from reverberant inputs using deep neural networks,
guided by a reverberation matching loss. Our most data-efficient variant
requires only 100 reverberation-parameter-labelled samples to outperform an
unsupervised baseline, demonstrating the effectiveness and practicality of the
proposed method in low-resource scenarios.

</details>


### [672] [Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems](https://arxiv.org/abs/2507.15214)
*Natalia Tomashenko,Emmanuel Vincent,Marc Tommasi*

Main category: cs.SD

TL;DR: The paper explores how speech dynamics like rhythm and intonation reveal speaker identity. It proposes duration embeddings for improved speech temporal analysis and highlights vulnerabilities in verification and anonymization systems.


<details>
  <summary>Details</summary>
Motivation: Understanding temporal dynamics in speech could enhance speaker verification and voice anonymization, revealing challenges and opportunities in the field.

Method: The authors extracted context-dependent duration embeddings from speech and used them in novel attack models to analyze vulnerabilities.

Result: Attack models based on duration embeddings significantly outperformed simpler representations in speaker verification for both original and anonymized data.

Conclusion: The study identified improved methods to analyze speaker characteristics and demonstrated vulnerabilities in current speech verification and anonymization systems.

Abstract: The temporal dynamics of speech, encompassing variations in rhythm,
intonation, and speaking rate, contain important and unique information about
speaker identity. This paper proposes a new method for representing speaker
characteristics by extracting context-dependent duration embeddings from speech
temporal dynamics. We develop novel attack models using these representations
and analyze the potential vulnerabilities in speaker verification and voice
anonymization systems.The experimental results show that the developed attack
models provide a significant improvement in speaker verification performance
for both original and anonymized data in comparison with simpler
representations of speech temporal dynamics reported in the literature.

</details>


### [673] [A2TTS: TTS for Low Resource Indian Languages](https://arxiv.org/abs/2507.15272)
*Ayush Singh Bhadoriya,Abhishek Nikunj Shinde,Isha Pandey,Ganesh Ramakrishnan*

Main category: cs.SD

TL;DR: The paper develops a diffusion-based TTS system for speaker-conditioned, zero-shot voice synthesis in Indian languages.


<details>
  <summary>Details</summary>
Motivation: There is a need to generate natural, speaker-specific, and expressive speech for unseen speakers across diverse Indian languages.

Method: Utilized a DDPM-based TTS system with a speaker encoder for embedding extraction, cross-attention for duration prediction, and classifier-free guidance for better zero-shot performance.

Result: Achieved improved speaker resemblance, duration modeling, and expressiveness in Indian languages like Bengali, Hindi, and Tamil using the IndicSUPERB dataset.

Conclusion: The developed system enhances TTS performance for unseen speakers and diverse languages with promising naturalness and expressiveness.

Abstract: We present a speaker conditioned text-to-speech (TTS) system aimed at
addressing challenges in generating speech for unseen speakers and supporting
diverse Indian languages. Our method leverages a diffusion-based TTS
architecture, where a speaker encoder extracts embeddings from short reference
audio samples to condition the DDPM decoder for multispeaker generation. To
further enhance prosody and naturalness, we employ a cross-attention based
duration prediction mechanism that utilizes reference audio, enabling more
accurate and speaker consistent timing. This results in speech that closely
resembles the target speaker while improving duration modeling and overall
expressiveness. Additionally, to improve zero-shot generation, we employed
classifier free guidance, allowing the system to generate speech more near
speech for unknown speakers. Using this approach, we trained language-specific
speaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian
languages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and
Tamil.

</details>


### [674] [Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation](https://arxiv.org/abs/2507.15396)
*Hui-Guan Yuan,Ryandhimas E. Zezario,Shafique Ahmed,Hsin-Min Wang,Kai-Lung Hua,Yu Tsao*

Main category: cs.SD

TL;DR: The paper presents Neuro-MSBG, a lightweight and efficient hearing loss simulation model that improves runtime while maintaining sound quality and intelligibility.


<details>
  <summary>Details</summary>
Motivation: Current hearing loss simulation models suffer from high computational complexity, latency, and lack of integration with speech processing systems, restricting their real-time application.

Method: The authors designed Neuro-MSBG, an end-to-end lightweight model incorporating a personalized audiogram encoder for precise time-frequency simulation, enabling parallel inference.

Result: Neuro-MSBG achieved a runtime reduction by a factor of 46 (from 0.970s to 0.021s for a 1s input) while maintaining high intelligibility (STOI SRCC of 0.9247) and perceptual quality (PESQ SRCC of 0.8671).

Conclusion: Neuro-MSBG is an efficient, real-time model that overcomes challenges in existing hearing loss simulation systems by combining computational efficiency and perceptual accuracy, supporting practical usability and deployment.

Abstract: Hearing loss simulation models are essential for hearing aid deployment.
However, existing models have high computational complexity and latency, which
limits real-time applications and lack direct integration with speech
processing systems. To address these issues, we propose Neuro-MSBG, a
lightweight end-to-end model with a personalized audiogram encoder for
effective time-frequency modeling. Experiments show that Neuro-MSBG supports
parallel inference and retains the intelligibility and perceptual quality of
the original MSBG, with a Spearman's rank correlation coefficient (SRCC) of
0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for
Perceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation
runtime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second
input), further demonstrating its efficiency and practicality.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [675] [Approximate Revenue Maximization for Diffusion Auctions](https://arxiv.org/abs/2507.14470)
*Yifan Huang,Dong Hao,Zhiyi Fan,Yuhang Guo,Bin Li*

Main category: econ.TH

TL;DR: The paper explores designing near-optimal network auctions using reserve prices, aimed at increasing revenue while expanding reach to bidders in economic networks.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitation of traditional auction designs which assume direct bidder reachability, extending to broader economic networks.

Method: Using Bayesian approximation analysis, the paper proposes a tailored reserve price function for network auctions that balances high revenue and successful sale probability.

Result: The reserve price method achieves a $(1 - 1 / \rho)$ approximation to the theoretical upper revenue bound for networks of any size and structure.

Conclusion: The proposed auction design preserves incentive compatibility while enabling sellers to achieve significant revenue gains beyond traditional methods.

Abstract: Reserve prices are widely used in practice. The problem of designing
revenue-optimal auctions based on reserve price has drawn much attention in the
auction design community. Although they have been extensively studied, most
developments rely on the significant assumption that the target audience of the
sale is directly reachable by the auctioneer, while a large portion of bidders
in the economic network unaware of the sale are omitted. This work follows the
diffusion auction design, which aims to extend the target audience of optimal
auction theory to all entities in economic networks. We investigate the design
of simple and provably near-optimal network auctions via reserve price. Using
Bayesian approximation analysis, we provide a simple and explicit form of the
reserve price function tailored to the most representative network auction. We
aim to balance setting a sufficiently high reserve price to induce high revenue
in a successful sale, and attracting more buyers from the network to increase
the probability of a successful sale. This reserve price function preserves
incentive compatibility for network auctions, allowing the seller to extract
additional revenue beyond that achieved by the Myerson optimal auction.
Specifically, if the seller has $\rho$ direct neighbours in a network of size
$n$, this reserve price guarantees a $1-{1 \over \rho}$ approximation to the
theoretical upper bound, i.e., the maximum possible revenue from any network of
size $n$. This result holds for any size and any structure of the networked
market.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [676] [Geometric design of the tangent term in landing algorithms for orthogonality constraints](https://arxiv.org/abs/2507.15638)
*Florentin Goyens,P. -A. Absil,Florian Feppon*

Main category: math.OC

TL;DR: The paper introduces a metric family for full-rank matrices, extending the $\beta$-metric from the Stiefel manifold, and applies it to optimization under orthogonality constraints.


<details>
  <summary>Details</summary>
Motivation: Orthogonality constraints appear commonly in optimization problems, requiring advanced methods to effectively address them.

Method: The authors extend the $\beta$-metric defined on the Stiefel manifold to full-rank $n \times p$ matrices and integrate it within a landing framework for optimization.

Result: The paper demonstrates how these new metrics can be applied to optimization problems under orthogonality constraints.

Conclusion: The proposed metrics provide a natural and effective extension for solving optimization problems involving orthogonality constraints.

Abstract: We propose a family a metrics over the set of full-rank $n\times p$ real
matrices, and apply them to the landing framework for optimization under
orthogonality constraints. The family of metrics we propose is a natural
extension of the $\beta$-metric, defined on the Stiefel manifold.

</details>


### [677] [On exploration of an interior mirror descent flow for stochastic nonconvex constrained problem](https://arxiv.org/abs/2507.15264)
*Kuangyu Ding,Kim-Chuan Toh*

Main category: math.OC

TL;DR: The paper explores nonsmooth, nonconvex optimization problems involving intersection constraints by introducing a Riemannian subgradient flow and deriving insights into related methods like the Hessian barrier and mirror descent schemes.


<details>
  <summary>Details</summary>
Motivation: To address growing challenges in understanding the convergence behavior and limitations of iterative optimization methods like Hessian barrier and mirror descent for solving nonsmooth nonconvex problems with complex feasible sets.

Method: The authors introduce a Riemannian subgradient flow defined via a barrier-induced Riemannian metric, establishing a connection between the continuous flow and discrete optimization methods. They analyze the trajectory behavior and propose strategies to avoid spurious stationary points, including random perturbations and strict complementarity conditions.

Result: The analysis reveals that Hessian barrier and mirror descent methods can be understood as approximations of the newly introduced continuous flow. It also provides a clearer interpretation of common issues, like spurious stationary points, while suggesting methods to overcome them. New iterative methods generalizing existing schemes are also introduced.

Conclusion: The paper offers a unified theoretical framework for nonsmooth, nonconvex optimization over complex constraints, making iterative methods more interpretable and reliable through continuous dynamics insights and new generalizations.

Abstract: We study a nonsmooth nonconvex optimization problem defined over nonconvex
constraints, where the feasible set is given by the intersection of the closure
of an open set and a smooth manifold. By endowing the open set with a
Riemannian metric induced by a barrier function, we obtain a Riemannian
subgradient flow formulated as a differential inclusion, which remains strictly
within the interior of the feasible set. This continuous dynamical system
unifies two classes of iterative optimization methods, namely the Hessian
barrier method and mirror descent scheme, by revealing that these methods can
be interpreted as discrete approximations of the continuous flow. We explore
the long-term behavior of the trajectories generated by this dynamical system
and show that the existing deficient convergence properties of the Hessian
barrier and mirror descent scheme can be unifily and more insightfully
interpreted through these of the continuous trajectory. For instance, the
notorious spurious stationary points \cite{chen2024spurious} observed in
Hessian barrier method and mirror descent scheme are interpreted as stable
equilibria of the dynamical system that do not correspond to real stationary
points of the original optimization problem. We provide two sufficient
condition such that these spurious stationary points can be avoided if the
strict complementarity conditions holds. In the absence of these regularity
condition, we propose a random perturbation strategy that ensures the
trajectory converges (subsequentially) to an approximate stationary point.
Building on these insights, we introduce two iterative Riemannian subgradient
methods, form of interior point methods, that generalizes the existing Hessian
barrier method and mirror descent scheme for solving nonsmooth nonconvex
optimization problems.

</details>


### [678] [Information Preserving Line Search via Bayesian Optimization](https://arxiv.org/abs/2507.15485)
*Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: math.OC

TL;DR: The paper introduces a line search method using Bayesian optimization for improved step-length choices in iterative optimization, outpacing traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional line search methods discard valuable data during interval refinements, leading to suboptimal step-length choices.

Method: The authors implement a Bayesian optimization-based line search method that utilizes discarded information for enhanced performance.

Result: The method demonstrates superior performance in empirical tests on complex optimization problems from the CUTEst test set.

Conclusion: Bayesian optimization-based line search provides a convergent and superior alternative to existing methods, leveraging discarded data for optimization improvements.

Abstract: Line search is a fundamental part of iterative optimization methods for
unconstrained and bound-constrained optimization problems to determine suitable
step lengths that provide sufficient improvement in each iteration. Traditional
line search methods are based on iterative interval refinement, where valuable
information about function value and gradient is discarded in each iteration.
We propose a line search method via Bayesian optimization, preserving and
utilizing otherwise discarded information to improve step-length choices. Our
approach is guaranteed to converge and shows superior performance compared to
state-of-the-art methods based on empirical tests on the challenging
unconstrained and bound-constrained optimization problems from the CUTEst test
set.

</details>


### [679] [Multi-beam Beamforming in RIS-aided MIMO Subject to Reradiation Mask Constraints -- Optimization and Machine Learning Design](https://arxiv.org/abs/2507.15367)
*Shumin Wang,Hajar El Hassani,Marco Di Renzo,Marios Poulakis*

Main category: math.OC

TL;DR: This paper designs an RIS-aided MIMO system optimizing transmit precoding and RIS settings for better spectral efficiency, using max-min problem solving, neural networks, and discrete phase shifts.


<details>
  <summary>Details</summary>
Motivation: To improve spectral efficiency and reduce power consumption in future multi-user wireless systems using RIS technology.

Method: The authors propose a max-min optimization problem with constraints, leveraging Arimoto-Blahut algorithm simplification, alternating optimization, neural networks, and greedy search for discrete RIS phase shifts.

Result: The proposed methods effectively shape radiation patterns satisfying constraints, reduce execution time using neural networks, and perform well with limited discrete phase shift levels.

Conclusion: The methods are efficient in enhancing spectral efficiency, achieving computational gains, and delivering robust performance under practical RIS hardware limitations.

Abstract: Reconfigurable intelligent surfaces (RISs) are an emerging technology for
improving spectral efficiency and reducing power consumption in future wireless
systems. This paper investigates the joint design of the transmit precoding
matrices and the RIS phase shift vector in a multi-user RIS-aided
multiple-input multiple-output (MIMO) communication system. We formulate a
max-min optimization problem to maximize the minimum achievable rate while
considering transmit power and reradiation mask constraints. The achievable
rate is simplified using the Arimoto-Blahut algorithm, and the problem is
broken into quadratic programs with quadratic constraints (QPQC) sub-problems
using an alternating optimization approach. To improve efficiency, we develop a
model-based neural network optimization that utilizes the one-hot encoding for
the angles of incidence and reflection. We address practical RIS limitations by
using a greedy search algorithm to solve the optimization problem for discrete
phase shifts. Simulation results demonstrate that the proposed methods
effectively shape the multi-beam radiation pattern towards desired directions
while satisfying reradiation mask constraints. The neural network design
reduces the execution time, and the discrete phase shift scheme performs well
with a small reduction of the beamforming gain by using only four phase shift
levels.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [680] [Self-Supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making](https://arxiv.org/abs/2507.14542)
*Yipeng Zhang,Yuanyi Ding,Chenda Duan,Atsuro Daida,Hiroki Nariai,Vwani Roychowdhury*

Main category: cs.CE

TL;DR: The paper tackles inefficiencies in detecting pathological high-frequency oscillations (HFOs) in epilepsy treatment by proposing a self-supervised framework, SS2LD, utilizing a VAE for morphological pre-training and clustering to enhance detection precision.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the precision of detecting pathological HFOs in intracranial EEG, addressing the limitations of traditional rule-based methods and supervised learning approaches, which struggle due to false positives, expensive expert labeling, and inconsistent practices.

Method: The SS2LD framework uses a variational autoencoder (VAE) for pre-training to learn latent representations of detected events, which are then clustered for weak supervision. A classifier refines detection boundaries based on real and VAE-augmented datasets.

Result: The SS2LD framework outperformed state-of-the-art methods in identifying pathological HFOs when evaluated on large multi-institutional interictal EEG datasets.

Conclusion: SS2LD provides a scalable, label-efficient, and clinically effective means of identifying pathological HFOs, leveraging legacy detectors and removing the dependence on expensive labeled datasets.

Abstract: High-frequency oscillations (HFOs) in intracranial Electroencephalography
(iEEG) are critical biomarkers for localizing the epileptogenic zone in
epilepsy treatment. However, traditional rule-based detectors for HFOs suffer
from unsatisfactory precision, producing false positives that require
time-consuming manual review. Supervised machine learning approaches have been
used to classify the detection results, yet they typically depend on labeled
datasets, which are difficult to acquire due to the need for specialized
expertise. Moreover, accurate labeling of HFOs is challenging due to low
inter-rater reliability and inconsistent annotation practices across
institutions. The lack of a clear consensus on what constitutes a pathological
HFO further challenges supervised refinement approaches. To address this, we
leverage the insight that legacy detectors reliably capture clinically relevant
signals despite their relatively high false positive rates. We thus propose the
Self-Supervised to Label Discovery (SS2LD) framework to refine the large set of
candidate events generated by legacy detectors into a precise set of
pathological HFOs. SS2LD employs a variational autoencoder (VAE) for
morphological pre-training to learn meaningful latent representation of the
detected events. These representations are clustered to derive weak supervision
for pathological events. A classifier then uses this supervision to refine
detection boundaries, trained on real and VAE-augmented data. Evaluated on
large multi-institutional interictal iEEG datasets, SS2LD outperforms
state-of-the-art methods. SS2LD offers a scalable, label-efficient, and
clinically effective strategy to identify pathological HFOs using legacy
detectors.

</details>


### [681] [DiffuMeta: Algebraic Language Models for Inverse Design of Metamaterials via Diffusion Transformers](https://arxiv.org/abs/2507.15753)
*Li Zheng,Siddhant Kumar,Dennis M. Kochmann*

Main category: cs.CE

TL;DR: DiffuMeta is an advanced framework combining generative machine learning with a mathematical language representation to efficiently design 3D metamaterials, enabling precise control, diverse solutions, and experimental validation.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome limitations in the inverse design of 3D metamaterials due to computational challenges and a lack of expressive representations.

Method: The authors introduce DiffuMeta, which integrates diffusion transformers with a novel algebraic language representation to encode 3D geometries as mathematical sentences.

Result: DiffuMeta successfully generates diverse shell structures with targeted mechanical responses and demonstrates controlled performance across multiple structural objectives.

Conclusion: Experimental results confirm DiffuMeta's ability to accelerate material discovery and tailor properties in 3D metamaterials, representing significant progress in generative design frameworks.

Abstract: Generative machine learning models have revolutionized material discovery by
capturing complex structure-property relationships, yet extending these
approaches to the inverse design of three-dimensional metamaterials remains
limited by computational complexity and underexplored design spaces due to the
lack of expressive representations. Here, we present DiffuMeta, a generative
framework integrating diffusion transformers with a novel algebraic language
representation, encoding 3D geometries as mathematical sentences. This compact,
unified parameterization spans diverse topologies while enabling direct
application of transformers to structural design. DiffuMeta leverages diffusion
models to generate novel shell structures with precisely targeted stress-strain
responses under large deformations, accounting for buckling and contact while
addressing the inherent one-to-many mapping by producing diverse solutions.
Uniquely, our approach enables simultaneous control over multiple mechanical
objectives, including linear and nonlinear responses beyond training domains.
Experimental validation of fabricated structures further confirms the efficacy
of our approach for accelerated design of metamaterials and structures with
tailored properties.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [682] [Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification](https://arxiv.org/abs/2506.23298)
*Xing Shen,Justin Szeto,Mingyang Li,Hengguan Huang,Tal Arbel*

Main category: eess.IV

TL;DR: This paper highlights the calibration biases and demographic unfairness issues in multimodal large language models (MLLMs) for medical image classification, proposing the CALIN method to address these challenges.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to ensure safe deployment of MLLMs in real-world clinical applications by addressing accuracy and calibration biases, especially across demographic subgroups.

Method: The authors propose CALIN, a bi-level inference-time calibration method using calibration matrices from population and subgroup levels to calibrate confidence scores during inference.

Result: Through experiments on three datasets (PAPILA, HAM10000, MIMIC-CXR), CALIN demonstrated improved prediction accuracies and fairness while minimizing fairness-utility trade-offs.

Conclusion: CALIN effectively mitigates calibration biases, ensures demographic fairness in confidence scores, and enhances prediction accuracy for medical image classification tasks.

Abstract: Multimodal large language models (MLLMs) have enormous potential to perform
few-shot in-context learning in the context of medical image analysis. However,
safe deployment of these models into real-world clinical practice requires an
in-depth analysis of the accuracies of their predictions, and their associated
calibration errors, particularly across different demographic subgroups. In
this work, we present the first investigation into the calibration biases and
demographic unfairness of MLLMs' predictions and confidence scores in few-shot
in-context learning for medical image classification. We introduce CALIN, an
inference-time calibration method designed to mitigate the associated biases.
Specifically, CALIN estimates the amount of calibration needed, represented by
calibration matrices, using a bi-level procedure: progressing from the
population level to the subgroup level prior to inference. It then applies this
estimation to calibrate the predicted confidence scores during inference.
Experimental results on three medical imaging datasets: PAPILA for fundus image
classification, HAM10000 for skin cancer classification, and MIMIC-CXR for
chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair
confidence calibration in its prediction, while improving its overall
prediction accuracies and exhibiting minimum fairness-utility trade-off. Our
codebase can be found at
https://github.com/xingbpshen/medical-calibration-fairness-mllm.

</details>


### [683] [MiDeSeC: A Dataset for Mitosis Detection and Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14271)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi,Zeynep Yildirim*

Main category: eess.IV

TL;DR: This paper describes the creation of the MiDeSeC dataset, consisting of high-resolution images of invasive breast carcinoma slides for mitosis detection.


<details>
  <summary>Details</summary>
Motivation: The motivation is to solve the challenge of mitosis detection by providing a dataset that accounts for several mitosis shapes and variations, which is essential for accurate computational pathology.

Method: The study used H&E stained slides of 25 patients, scanned with high-resolution imaging devices, selecting regions of 1024x1024 pixels with over 500 mitoses in total. Two-thirds of the regions are allocated for training and one-third for testing.

Result: The result is a dataset comprising 50 regions from 25 patient slides, offering diverse mitosis shapes, aimed at enhancing the accuracy and scope of mitosis detection algorithms.

Conclusion: The MiDeSeC dataset addresses the need for comprehensive datasets in computational pathology, providing varied examples of mitotic figures for use in training and testing purposes.

Abstract: The MiDeSeC dataset is created through H&E stained invasive breast carcinoma,
no special type (NST) slides of 25 different patients captured at 40x
magnification from the Department of Medical Pathology at Ankara University.
The slides have been scanned by 3D Histech Panoramic p250 Flash-3 scanner and
Olympus BX50 microscope. As several possible mitosis shapes exist, it is
crucial to have a large dataset to cover all the cases. Accordingly, a total of
50 regions is selected from glass slides for 25 patients, each of regions with
a size of 1024*1024 pixels. There are more than 500 mitoses in total in these
50 regions. Two-thirds of the regions are reserved for training, the other
third for testing.

</details>


### [684] [NuSeC: A Dataset for Nuclei Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14272)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi*

Main category: eess.IV

TL;DR: The NuSeC dataset contains 100 images from 25 patients, divided into 75% for training and 25% for testing.


<details>
  <summary>Details</summary>
Motivation: To create a standardized dataset for consistent comparative analysis in future research regarding nuclei structures.

Method: Randomly select one image per patient for the testing set and reserve the remaining images for training, ensuring a balanced dataset.

Result: The dataset is divided into 75 images for training containing ~30,000 nuclei structures, and 25 images for testing containing ~6,000 nuclei structures.

Conclusion: The NuSeC dataset provides a structured framework to facilitate reliable comparisons of developed methods for analyzing nuclei structures.

Abstract: The NuSeC dataset is created by selecting 4 images with the size of 1024*1024
pixels from the slides of each patient among 25 patients. Therefore, there are
a total of 100 images in the NuSeC dataset. To carry out a consistent
comparative analysis between the methods that will be developed using the NuSeC
dataset by the researchers in the future, we divide the NuSeC dataset 75% as
the training set and 25% as the testing set. In detail, an image is randomly
selected from 4 images of each patient among 25 patients to build the testing
set, and then the remaining images are reserved for the training set. While the
training set includes 75 images with around 30000 nuclei structures, the
testing set includes 25 images with around 6000 nuclei structures.

</details>


### [685] [Self-Supervised Joint Reconstruction and Denoising of T2-Weighted PROPELLER MRI of the Lungs at 0.55T](https://arxiv.org/abs/2507.14308)
*Jingjia Chen,Haoyang Pei,Christoph Maier,Mary Bruno,Qiuting Wen,Seon-Hi Shin,William Moore,Hersh Chandarana,Li Feng*

Main category: eess.IV

TL;DR: This study explores a self-supervised model to enhance T2-weighted PROPELLER lung MRI quality and reduce scan times.


<details>
  <summary>Details</summary>
Motivation: Improve clarity, structural integrity, and efficiency of T2-weighted lung MRI at 0.55T for post-COVID patient assessments.

Method: The model employs a self-supervised learning framework, splitting blades of PROPELLER acquisition, training on one subset, and calculating loss on the other, using matched noise statistics for denoising.

Result: The reconstructed lung images showed improved visual quality, alignment with CT scans, and reduced scan times. Reader evaluations confirmed superiority compared to MPPCA-denoised images.

Conclusion: The self-supervised model effectively reconstructs high-quality lung MRI images while suppressing noise, leveraging intrinsic k-space redundancies.

Abstract: Purpose: This study aims to improve 0.55T T2-weighted PROPELLER lung MRI
through a self-supervised joint reconstruction and denoising model.
  Methods: T2-weighted 0.55T lung MRI dataset including 44 patients with
previous covid infection were used. A self-supervised learning framework was
developed, where each blade of the PROPELLER acquisition was split along the
readout direction into two partitions. One subset trains the unrolled
reconstruction network, while the other subset is used for loss calculation,
enabling self-supervised training without clean targets and leveraging matched
noise statistics for denoising. For comparison, Marchenko-Pastur Principal
Component Analysis (MPPCA) was performed along the coil dimension, followed by
conventional parallel imaging reconstruction. The quality of the reconstructed
lung MRI was assessed visually by two experienced radiologists independently.
  Results: The proposed self-supervised model improved the clarity and
structural integrity of the lung images. For cases with available CT scans, the
reconstructed images demonstrated strong alignment with corresponding CT
images. Additionally, the proposed model enables further scan time reduction by
requiring only half the number of blades. Reader evaluations confirmed that the
proposed method outperformed MPPCA-denoised images across all categories
(Wilcoxon signed-rank test, p<0.001), with moderate inter-reader agreement
(weighted Cohen's kappa=0.55; percentage of exact and within +/-1 point
agreement=91%).
  Conclusion: By leveraging intrinsic structural redundancies between two
disjoint splits of k-space subsets, the proposed self-supervised learning model
effectively reconstructs the image while suppressing the noise for 0.55T
T2-weighted lung MRI with PROPELLER sampling.

</details>


### [686] [Classification of Histopathology Slides with Persistence Homology Convolutions](https://arxiv.org/abs/2507.14378)
*Shrunal Pothagoni,Benjamin Schweinhart*

Main category: eess.IV

TL;DR: This paper introduces Persistent Homology Convolutions to improve CNN performance in histopathology by capturing local topological features, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: Standard CNNs often lose topological information, critical in domains like histopathology where tissue and cell shapes are key to diagnostics.

Method: A modified convolution operator, Persistent Homology Convolutions, was developed to capture local and translation-invariant topological features.

Result: Models employing persistent homology convolutions outperformed conventional models in histopathology tasks and demonstrated greater robustness to hyperparameter changes.

Conclusion: Persistent Homology Convolutions successfully extract local geometric information from histopathology slides, enhancing diagnostic accuracy and stability.

Abstract: Convolutional neural networks (CNNs) are a standard tool for computer vision
tasks such as image classification. However, typical model architectures may
result in the loss of topological information. In specific domains such as
histopathology, topology is an important descriptor that can be used to
distinguish between disease-indicating tissue by analyzing the shape
characteristics of cells. Current literature suggests that reintroducing
topological information using persistent homology can improve medical
diagnostics; however, previous methods utilize global topological summaries
which do not contain information about the locality of topological features. To
address this gap, we present a novel method that generates local persistent
homology-based data using a modified version of the convolution operator called
Persistent Homology Convolutions. This method captures information about the
locality and translation invariance of topological features. We perform a
comparative study using various representations of histopathology slides and
find that models trained with persistent homology convolutions outperform
conventionally trained models and are less sensitive to hyperparameters. These
results indicate that persistent homology convolutions extract meaningful
geometric information from the histopathology slides.

</details>


### [687] [QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems](https://arxiv.org/abs/2507.14760)
*Cassandra Tong Ye,Shamus Li,Tyler King,Kristina Monakhova*

Main category: eess.IV

TL;DR: The paper presents QUTCC, a method for quantile-based uncertainty prediction, to improve reliability in imaging tasks like MRI, providing tighter uncertainty bounds than prior techniques.


<details>
  <summary>Details</summary>
Motivation: Deep learning hallucinations can mislead critical applications in areas like medical imaging. Reliable uncertainty quantification is crucial to mitigate these risks.

Method: The paper introduces QUTCC, a technique combining quantile embedding in a U-Net architecture and iterative calibration to predict conditional distributions and refine uncertainty bounds.

Result: QUTCC consistently outperforms prior methods by achieving tighter uncertainty intervals while maintaining statistical coverage.

Conclusion: Using QUTCC improves the reliability of imaging tasks by effectively detecting hallucinations and providing more informative uncertainty bounds.

Abstract: Deep learning models often hallucinate, producing realistic artifacts that
are not truly present in the sample. This can have dire consequences for
scientific and medical inverse problems, such as MRI and microscopy denoising,
where accuracy is more important than perceptual quality. Uncertainty
quantification techniques, such as conformal prediction, can pinpoint outliers
and provide guarantees for image regression tasks, improving reliability.
However, existing methods utilize a linear constant scaling factor to calibrate
uncertainty bounds, resulting in larger, less informative bounds. We propose
QUTCC, a quantile uncertainty training and calibration technique that enables
nonlinear, non-uniform scaling of quantile predictions to enable tighter
uncertainty estimates. Using a U-Net architecture with a quantile embedding,
QUTCC enables the prediction of the full conditional distribution of quantiles
for the imaging task. During calibration, QUTCC generates uncertainty bounds by
iteratively querying the network for upper and lower quantiles, progressively
refining the bounds to obtain a tighter interval that captures the desired
coverage. We evaluate our method on several denoising tasks as well as
compressive MRI reconstruction. Our method successfully pinpoints
hallucinations in image estimates and consistently achieves tighter uncertainty
intervals than prior methods while maintaining the same statistical coverage.

</details>


### [688] [PET Image Reconstruction Using Deep Diffusion Image Prior](https://arxiv.org/abs/2507.15078)
*Fumio Hashimoto,Kuang Gong*

Main category: eess.IV

TL;DR: This paper proposes a novel PET image reconstruction method utilizing diffusion models guided by anatomical priors, showing effective cross-tracer and scanner generalization.


<details>
  <summary>Details</summary>
Motivation: PET image reconstruction faces challenges due to tracer-specific contrast variability and high computational demands, which current methods struggle to address.

Method: The authors based their method on the deep diffusion image prior (DDIP) framework, integrating an alternating process of diffusion sampling and model fine-tuning guided by PET sinograms. Additionally, the half-quadratic splitting (HQS) algorithm was employed to enhance computational efficiency.

Result: The proposed method demonstrated strong performance in simulations and clinical datasets, including robust generalization across tracer distributions and scanner types when tested on out-of-distribution (OOD) scenarios.

Conclusion: This study presents an efficient and versatile approach to PET reconstruction, highlighting its potential to improve low-dose PET imaging through cross-tracer generalization and computational efficiency.

Abstract: Diffusion models have shown great promise in medical image denoising and
reconstruction, but their application to Positron Emission Tomography (PET)
imaging remains limited by tracer-specific contrast variability and high
computational demands. In this work, we proposed an anatomical prior-guided PET
image reconstruction method based on diffusion models, inspired by the deep
diffusion image prior (DDIP) framework. The proposed method alternated between
diffusion sampling and model fine-tuning guided by the PET sinogram, enabling
the reconstruction of high-quality images from various PET tracers using a
score function pretrained on a dataset of another tracer. To improve
computational efficiency, the half-quadratic splitting (HQS) algorithm was
adopted to decouple network optimization from iterative PET reconstruction. The
proposed method was evaluated using one simulation and two clinical datasets.
For the simulation study, a model pretrained on [$^{18}$F]FDG data was tested
on amyloid-negative PET data to assess out-of-distribution (OOD) performance.
For the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one
[$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from
another tracer. Experiment results show that the proposed PET reconstruction
method can generalize robustly across tracer distributions and scanner types,
providing an efficient and versatile reconstruction framework for low-dose PET
imaging.

</details>


### [689] [Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection](https://arxiv.org/abs/2507.15151)
*Sebastian A. Cruz Romero,Wilfredo E. Lugo Beauchamp*

Main category: eess.IV

TL;DR: This paper explores anemia detection through deep learning on conjunctival images using MobileNet, achieving 93% accuracy. Quantization effects on model performance for edge deployment are investigated.


<details>
  <summary>Details</summary>
Motivation: Anemia is prevalent among young children in low-resource areas, and traditional detection is expensive and expertise-reliant. The paper addresses this gap with affordable, automated solutions.

Method: Using the CP-AnemiC dataset of conjunctival images paired with demographic data, the study employed the MobileNet architecture, fine-tuned with data augmentation and cross-validation. Quantization strategies were tested for edge deployment.

Result: The model achieved an accuracy of 93.13%, precision of 93.74%, and F1 score of 97.73%. FP16 quantization preserved most performance, but more aggressive quantization (INT8, INT4) degraded accuracy.

Conclusion: The study demonstrates deep learning's potential for affordable anemia detection. It also highlights the trade-offs in quantization schemes, important for edge-device deployment in mobile healthcare.

Abstract: Anemia is a widespread global health issue, particularly among young children
in low-resource settings. Traditional methods for anemia detection often
require expensive equipment and expert knowledge, creating barriers to early
and accurate diagnosis. To address these challenges, we explore the use of deep
learning models for detecting anemia through conjunctival pallor, focusing on
the CP-AnemiC dataset, which includes 710 images from children aged 6-59
months. The dataset is annotated with hemoglobin levels, gender, age and other
demographic data, enabling the development of machine learning models for
accurate anemia detection. We use the MobileNet architecture as a backbone,
known for its efficiency in mobile and embedded vision applications, and
fine-tune our model end-to-end using data augmentation techniques and a
cross-validation strategy. Our model implementation achieved an accuracy of
0.9313, a precision of 0.9374, and an F1 score of 0.9773 demonstrating strong
performance on the dataset. To optimize the model for deployment on edge
devices, we performed post-training quantization, evaluating the impact of
different bit-widths (FP32, FP16, INT8, and INT4) on model performance.
Preliminary results suggest that while FP16 quantization maintains high
accuracy (0.9250), precision (0.9370), and F1 Score (0.9377), more aggressive
quantization (INT8 and INT4) leads to significant performance degradation.
Overall, our study supports further exploration of quantization schemes and
hardware optimizations to assess trade-offs between model size, inference time,
and diagnostic accuracy in mobile healthcare applications.

</details>


### [690] [A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT](https://arxiv.org/abs/2507.15193)
*Tanjin Taher Toma,Tejas Sudharshan Mathai,Bikash Santra,Pritam Mukherjee,Jianfei Liu,Wesley Jong,Darwish Alabyad,Vivek Batheja,Abhishek Jha,Mayank Patel,Darko Pucar,Jayadira del Rivero,Karel Pacak,Ronald M. Summers*

Main category: eess.IV

TL;DR: The study explores integrating anatomical priors into deep learning models for accurate segmentation of pheochromocytoma in abdominal CT scans, demonstrating superior performance with Tumor + Kidney + Aorta (TKA) contextual information.


<details>
  <summary>Details</summary>
Motivation: Accurate PCC segmentation aids in tumor burden estimation, prognosis, treatment planning, and genetic inference, addressing the limitations associated with expensive testing.

Method: The analysis used the nnU-Net framework to evaluate 11 annotation strategies, comparing priors derived from neighboring organs against a broad body-region prior for 3D tumor segmentation. The study included 105 contrast-enhanced CT scans.

Result: The TKA annotation achieved the highest segmentation accuracy, outperforming the Tumor + Body (TB) approach across DSC, NSD, and F1, along with superior tumor burden quantification and robustness over cross-validation folds.

Conclusion: Incorporating organ-specific anatomical context (TKA) significantly improves PCC segmentation performance, ensuring precision and supporting clinical workflows including tumor monitoring and genetic subtype segmentation.

Abstract: Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is
essential for tumor burden estimation, prognosis, and treatment planning. It
may also help infer genetic clusters, reducing reliance on expensive testing.
This study systematically evaluates anatomical priors to identify
configurations that improve deep learning-based PCC segmentation. We employed
the nnU-Net framework to evaluate eleven annotation strategies for accurate 3D
segmentation of pheochromocytoma, introducing a set of novel multi-class
schemes based on organ-specific anatomical priors. These priors were derived
from adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen,
kidney, aorta, adrenal gland, and pancreas), and were compared against a broad
body-region prior used in previous work. The framework was trained and tested
on 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center.
Performance was measured using Dice Similarity Coefficient (DSC), Normalized
Surface Distance (NSD), and instance-wise F1 score. Among all strategies, the
Tumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation
accuracy, significantly outperforming the previously used Tumor + Body (TB)
annotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84%
improvement at an IoU threshold of 0.5), measured on a 70-30 train-test split.
The TKA model also showed superior tumor burden quantification (R^2 = 0.968)
and strong segmentation across all genetic subtypes. In five-fold
cross-validation, TKA consistently outperformed TB across IoU thresholds (0.1
to 0.5), reinforcing its robustness and generalizability. These findings
highlight the value of incorporating relevant anatomical context in deep
learning models to achieve precise PCC segmentation, supporting clinical
assessment and longitudinal monitoring.

</details>


### [691] [Personalized 3D Myocardial Infarct Geometry Reconstruction from Cine MRI with Explicit Cardiac Motion Modeling](https://arxiv.org/abs/2507.15194)
*Yilin Lyu,Fan Yang,Xiaoyue Liu,Zichen Jiang,Joshua Dillon,Debbie Zhao,Martyn Nash,Charlene Mauger,Alistair Young,Ching-Hui Sia,Mark YY Chan,Lei Li*

Main category: eess.IV

TL;DR: This study proposes a contrast-free method to reconstruct high-fidelity 3D myocardial infarct geometry using standard cine MRI, leveraging cardiac motion analysis.


<details>
  <summary>Details</summary>
Motivation: To eliminate the limitations associated with Late Gadolinium Enhancement (LGE) MRI, such as the need for contrast agents and reduced spatial resolution from 2D slices.

Method: The paper introduces a two-step framework: (1) utilizing a deep shape fitting model (biv-me) for 4D biventricular mesh reconstruction, and (2) applying CMotion2Infarct-Net to analyze cardiac motion dynamics for infarct localization.

Result: On a dataset of 205 cine MRI scans from 126 MI patients, the method shows reasonable agreement with manual infarct delineation.

Conclusion: The approach provides a promising avenue for contrast-free and motion-driven reconstruction of 3D myocardial infarct geometry, supporting personalized cardiac modeling.

Abstract: Accurate representation of myocardial infarct geometry is crucial for
patient-specific cardiac modeling in MI patients. While Late gadolinium
enhancement (LGE) MRI is the clinical gold standard for infarct detection, it
requires contrast agents, introducing side effects and patient discomfort.
Moreover, infarct reconstruction from LGE often relies on sparsely sampled 2D
slices, limiting spatial resolution and accuracy. In this work, we propose a
novel framework for automatically reconstructing high-fidelity 3D myocardial
infarct geometry from 2D clinically standard cine MRI, eliminating the need for
contrast agents. Specifically, we first reconstruct the 4D biventricular mesh
from multi-view cine MRIs via an automatic deep shape fitting model, biv-me.
Then, we design a infarction reconstruction model, CMotion2Infarct-Net, to
explicitly utilize the motion patterns within this dynamic geometry to localize
infarct regions. Evaluated on 205 cine MRI scans from 126 MI patients, our
method shows reasonable agreement with manual delineation. This study
demonstrates the feasibility of contrast-free, cardiac motion-driven 3D infarct
reconstruction, paving the way for efficient digital twin of MI.

</details>


### [692] [Personalized 4D Whole Heart Geometry Reconstruction from Cine MRI for Cardiac Digital Twins](https://arxiv.org/abs/2507.15203)
*Xiaoyue Liu,Xicheng Sheng,Xiahai Zhuang,Vicente Grau,Mark YY Chan,Ching-Hui Sia,Lei Li*

Main category: eess.IV

TL;DR: This paper introduces a method to directly create personalized 4D heart models from cine MRIs using a weakly supervised learning approach, enhancing precision cardiology.


<details>
  <summary>Details</summary>
Motivation: Current cardiac digital twins (CDTs) lack comprehensive models that simulate the full electromechanics of the heart's four chambers effectively, limiting their potential in precision medicine.

Method: The authors propose a weakly supervised learning model that translates multi-view cine MRIs into dynamic 4D heart meshes through self-supervised mapping techniques.

Result: The model generates personalized 4D heart meshes, enabling the extraction of key cardiac variables like ejection fraction and chamber dynamics with high temporal resolution.

Conclusion: This work demonstrates the feasibility of inferring accurate 4D heart models from MRIs, marking a step forward in CDT development for precision medicine.

Abstract: Cardiac digital twins (CDTs) provide personalized in-silico cardiac
representations and hold great potential for precision medicine in cardiology.
However, whole-heart CDT models that simulate the full organ-scale
electromechanics of all four heart chambers remain limited. In this work, we
propose a weakly supervised learning model to reconstruct 4D (3D+t) heart mesh
directly from multi-view 2D cardiac cine MRIs. This is achieved by learning a
self-supervised mapping between cine MRIs and 4D cardiac meshes, enabling the
generation of personalized heart models that closely correspond to input cine
MRIs. The resulting 4D heart meshes can facilitate the automatic extraction of
key cardiac variables, including ejection fraction and dynamic chamber volume
changes with high temporal resolution. It demonstrates the feasibility of
inferring personalized 4D heart models from cardiac MRIs, paving the way for an
efficient CDT platform for precision medicine. The code will be publicly
released once the manuscript is accepted.

</details>


### [693] [EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro](https://arxiv.org/abs/2507.15292)
*An Wanga,Rulin Zhou,Mengya Xu,Yiru Ye,Longfei Gou,Yiting Chang,Hao Chen,Chwee Ming Lim,Jiankun Wang,Hongliang Ren*

Main category: eess.IV

TL;DR: This paper presents EndoControlMag, a mask-conditioned vascular motion magnification framework tailored for endoscopic surgeries, which improves precision and robustness in challenging conditions.


<details>
  <summary>Details</summary>
Motivation: The need for precise visualization of subtle vascular motions during endoscopic surgeries for better surgical decision-making, given the challenging dynamics of surgical environments.

Method: The framework introduces a Periodic Reference Resetting (PRR) scheme to avoid error accumulation and a Hierarchical Tissue-aware Magnification (HTM) module utilizing dual-mode mask dilation strategies for various surgical scenarios.

Result: EndoControlMag significantly outperformed existing methods in magnification accuracy and visual quality across diverse surgical scenarios when evaluated on the EndoVMM24 dataset.

Conclusion: EndoControlMag offers a robust, training-free method for vascular motion magnification tailored to endoscopic environments, addressing dynamic challenges in surgical settings with high reliability.

Abstract: Visualizing subtle vascular motions in endoscopic surgery is crucial for
surgical precision and decision-making, yet remains challenging due to the
complex and dynamic nature of surgical scenes. To address this, we introduce
EndoControlMag, a training-free, Lagrangian-based framework with
mask-conditioned vascular motion magnification tailored to endoscopic
environments. Our approach features two key modules: a Periodic Reference
Resetting (PRR) scheme that divides videos into short overlapping clips with
dynamically updated reference frames to prevent error accumulation while
maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification
(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores
using a pretrained visual tracking model to maintain accurate localization
despite occlusions and view changes. It then applies one of two adaptive
softening strategies to surrounding tissues: motion-based softening that
modulates magnification strength proportional to observed tissue displacement,
or distance-based exponential decay that simulates biomechanical force
attenuation. This dual-mode approach accommodates diverse surgical
scenarios-motion-based softening excels with complex tissue deformations while
distance-based softening provides stability during unreliable optical flow
conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four
different surgery types and various challenging scenarios, including
occlusions, instrument disturbance, view changes, and vessel deformations.
Quantitative metrics, visual assessments, and expert surgeon evaluations
demonstrate that EndoControlMag significantly outperforms existing methods in
both magnification accuracy and visual quality while maintaining robustness
across challenging surgical conditions. The code, dataset, and video results
are available at https://szupc.github.io/EndoControlMag/.

</details>


### [694] [MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis](https://arxiv.org/abs/2507.15340)
*Marc Boubnovski Martell,Kristofer Linton-Reid,Mitchell Chen,Sumeet Hindocha,Benjamin Hunter,Marco A. Calzado,Richard Lee,Joram M. Posma,Eric O. Aboagye*

Main category: eess.IV

TL;DR: The paper introduces TVSRN-V2, a transformer-based super-resolution network designed to improve low-dose lung CT imaging while maintaining compatibility with clinical workflows.


<details>
  <summary>Details</summary>
Motivation: To address limitations in high-resolution lung CT imaging due to radiation exposure and hardware cost, and to improve the integration of super-resolution methods into clinical pipelines.

Method: The authors propose TVSRN-V2, incorporating Through-Plane Attention Blocks (TAB) and Swin Transformer V2. They also introduce pseudo-low-resolution augmentation to simulate diverse scanner conditions.

Result: TVSRN-V2 improves segmentation accuracy by 4% Dice, enhances radiomic reproducibility, and boosts predictive performance by 0.06 in C-index and AUC, demonstrating its effectiveness across multiple clinical tasks.

Conclusion: TVSRN-V2 significantly enhances CT imaging quality and clinical decision-making, offering a reliable, dose-efficient, and integrative solution for real-world applications.

Abstract: High-resolution volumetric computed tomography (CT) is essential for accurate
diagnosis and treatment planning in thoracic diseases; however, it is limited
by radiation dose and hardware costs. We present the Transformer Volumetric
Super-Resolution Network (\textbf{TVSRN-V2}), a transformer-based
super-resolution (SR) framework designed for practical deployment in clinical
lung CT analysis. Built from scalable components, including Through-Plane
Attention Blocks (TAB) and Swin Transformer V2 -- our model effectively
reconstructs fine anatomical details in low-dose CT volumes and integrates
seamlessly with downstream analysis pipelines. We evaluate its effectiveness on
three critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis
-- across multiple clinical cohorts. To enhance robustness across variable
acquisition protocols, we introduce pseudo-low-resolution augmentation,
simulating scanner diversity without requiring private data. TVSRN-V2
demonstrates a significant improvement in segmentation accuracy (+4\% Dice),
higher radiomic feature reproducibility, and enhanced predictive performance
(+0.06 C-index and AUC). These results indicate that SR-driven recovery of
structural detail significantly enhances clinical decision support, positioning
TVSRN-V2 as a well-engineered, clinically viable system for dose-efficient
imaging and quantitative analysis in real-world CT workflows.

</details>


### [695] [Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation](https://arxiv.org/abs/2507.15361)
*Muhammad Aqeel,Maham Nazir,Zanxi Ruan,Francesco Setti*

Main category: eess.IV

TL;DR: SynDiff introduces a novel framework combining text-guided synthetic data generation and efficient segmentation, addressing data scarcity in polyp detection with promising performance metrics.


<details>
  <summary>Details</summary>
Motivation: The paper focuses on overcoming data scarcity challenges in medical image segmentation, especially in polyp detection, which demands specialized expertise for annotation.

Method: The method integrates latent diffusion models for text-conditioned inpainting of synthetic polyps and introduces direct latent estimation for single-step inference, ensuring computational efficiency.

Result: SynDiff achieves notable performance metrics, including a 96.0% Dice score and 92.9% IoU on CVC-ClinicDB, with real-time capabilities for clinical environments.

Conclusion: Controlled synthetic augmentation via SynDiff enhances segmentation robustness while avoiding distribution shifts, making it suitable for resource-limited medical settings and bridging the gap between deep learning needs and clinical constraints.

Abstract: Medical image segmentation suffers from data scarcity, particularly in polyp
detection where annotation requires specialized expertise. We present SynDiff,
a framework combining text-guided synthetic data generation with efficient
diffusion-based segmentation. Our approach employs latent diffusion models to
generate clinically realistic synthetic polyps through text-conditioned
inpainting, augmenting limited training data with semantically diverse samples.
Unlike traditional diffusion methods requiring iterative denoising, we
introduce direct latent estimation enabling single-step inference with T x
computational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%
IoU while maintaining real-time capability suitable for clinical deployment.
The framework demonstrates that controlled synthetic augmentation improves
segmentation robustness without distribution shift. SynDiff bridges the gap
between data-hungry deep learning models and clinical constraints, offering an
efficient solution for deployment in resourcelimited medical settings.

</details>


### [696] [A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization](https://arxiv.org/abs/2507.15476)
*Cong Chen,Ming Chen,Hoileong Lee,Yan Li,Jiyang Yu*

Main category: eess.IV

TL;DR: The paper proposes a YOLOv9s-based framework using deep learning with enhancements like C3Ghost and SCConv modules, and CARAFE upsamples for improving steel surface defect detection, focusing on accuracy, efficiency, and robustness in handling small and multi-scale defects.


<details>
  <summary>Details</summary>
Motivation: To address limitations in traditional steel surface defect detection methods, such as low accuracy and high miss-detection rates, especially for small and multi-scale defects.

Method: The study utilizes YOLOv9s as the backbone and integrates the innovative C3Ghost module for feature extraction efficiency, SCConv module for feature redundancy reduction, and CARAFE upsampling operator for precise restoration of high-resolution defect regions.

Result: The proposed framework showed improved accuracy and robustness in detecting steel surface defects, surpassing the performance of existing detection methods.

Conclusion: This study successfully enhances steel surface defect detection using deep learning, recommending scalable and effective solutions to overcome traditional detection bottlenecks in industrial applications.

Abstract: Surface defect detection of steel, especially the recognition of multi-scale
defects, has always been a major challenge in industrial manufacturing. Steel
surfaces not only have defects of various sizes and shapes, which limit the
accuracy of traditional image processing and detection methods in complex
environments. However, traditional defect detection methods face issues of
insufficient accuracy and high miss-detection rates when dealing with small
target defects. To address this issue, this study proposes a detection
framework based on deep learning, specifically YOLOv9s, combined with the
C3Ghost module, SCConv module, and CARAFE upsampling operator, to improve
detection accuracy and model performance. First, the SCConv module is used to
reduce feature redundancy and optimize feature representation by reconstructing
the spatial and channel dimensions. Second, the C3Ghost module is introduced to
enhance the model's feature extraction ability by reducing redundant
computations and parameter volume, thereby improving model efficiency. Finally,
the CARAFE upsampling operator, which can more finely reorganize feature maps
in a content-aware manner, optimizes the upsampling process and ensures
detailed restoration of high-resolution defect regions. Experimental results
demonstrate that the proposed model achieves higher accuracy and robustness in
steel surface defect detection tasks compared to other methods, effectively
addressing defect detection problems.

</details>


### [697] [DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI Lesion Classification](https://arxiv.org/abs/2507.15487)
*Dezhen Wang,Sheng Miao,Rongxin Chai,Jiufa Cui*

Main category: eess.IV

TL;DR: This paper introduces DeSamba, a novel framework integrating spatial and spectral features for robust 3D lesion classification using multi-sequence MRI data. It outperforms state-of-the-art methods on two clinical datasets.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of integrating multi-sequence MRI data to improve the accuracy and robustness of 3D lesion classification in medical imaging.

Method: The proposed approach involves two key components: the Decoupled Representation Learning Module (DRLM) for feature extraction via self- and cross-reconstruction, and the Spectral Adaptive Modulation Block (SAMB) for dynamic fusion of features based on lesion characteristics.

Result: DeSamba achieves superior classification performance on two datasets: a six-class spinal metastasis dataset and a binary spondylitis dataset, outperforming all state-of-the-art baselines.

Conclusion: DeSamba is demonstrated to be a generalizable and effective framework for 3D lesion classification in multi-sequence medical imaging, with significant improvements attributed to its novel DRLM and SAMB modules.

Abstract: Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency
domain information, which is crucial for accurate lesion classification in
medical imaging. However, effectively integrating multi-sequence MRI data for
robust 3D lesion classification remains a challenge. In this paper, we propose
DeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel
framework designed to extract decoupled representations and adaptively fuse
spatial and spectral features for lesion classification. DeSamba introduces a
Decoupled Representation Learning Module (DRLM) that decouples features from
different MRI sequences through self-reconstruction and cross-reconstruction,
and a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet,
enabling dynamic fusion of spectral and spatial information based on lesion
characteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On
a six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1
accuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external
validation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On
a spondylitis dataset (n=251) involving a challenging binary classification
task, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal
and external validation sets, respectively. Ablation studies demonstrate that
both DRLM and SAMB significantly contribute to overall performance, with over
10% relative improvement compared to the baseline. Our results highlight the
potential of DeSamba as a generalizable and effective solution for 3D lesion
classification in multi-sequence medical imaging.

</details>


### [698] [RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation](https://arxiv.org/abs/2507.15524)
*Simon Winther Albertsen,Hjalte Svaneborg Bjørnstrup,Mostafa Mehdipour Ghazi*

Main category: eess.IV

TL;DR: RARE-UNet is a segmentation model designed to adapt dynamically to input resolution, offering robust performance across varying resolutions.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing segmentation models that perform poorly on lower-resolution data encountered in real-world clinical scenarios.

Method: Introduced a resolution-aware architecture integrating multi-scale blocks, routing mechanisms, and consistency-driven training to improve segmentation performance across different input resolutions.

Result: RARE-UNet outperforms standard UNet, its augmented variant, and nnUNet, achieving the highest Dice scores (0.84, 0.65) and faster inference times at lower resolutions.

Conclusion: RARE-UNet is effective and scalable, making it suitable for resolution-robust segmentation in clinical applications.

Abstract: Accurate segmentation is crucial for clinical applications, but existing
models often assume fixed, high-resolution inputs and degrade significantly
when faced with lower-resolution data in real-world scenarios. To address this
limitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation
architecture that dynamically adapts its inference path to the spatial
resolution of the input. Central to our design are multi-scale blocks
integrated at multiple encoder depths, a resolution-aware routing mechanism,
and consistency-driven training that aligns multi-resolution features with
full-resolution representations. We evaluate RARE-UNet on two benchmark brain
imaging tasks for hippocampus and tumor segmentation. Compared to standard
UNet, its multi-resolution augmented variant, and nnUNet, our model achieves
the highest average Dice scores of 0.84 and 0.65 across resolution, while
maintaining consistent performance and significantly reduced inference time at
lower resolutions. These results highlight the effectiveness and scalability of
our architecture in achieving resolution-robust segmentation. The codes are
available at: https://github.com/simonsejse/RARE-UNet.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [699] [Identifying Algorithmic and Domain-Specific Bias in Parliamentary Debate Summarisation](https://arxiv.org/abs/2507.14221)
*Eoghan Cunningham,James Cross,Derek Greene*

Main category: cs.CY

TL;DR: This paper examines using large language models (LLMs) to summarize European Parliament debates while addressing biases, proposing a multi-stage framework to enhance summary quality and fairness.


<details>
  <summary>Details</summary>
Motivation: The paper aims to make parliamentary debates more accessible to the public by using summarization techniques, while ensuring equitable representation of all speakers.

Method: A multi-stage summarization framework is proposed, enabling systematic bias analysis based on speaker attributes like political affiliation and speaking order. Proprietary and open-weight LLMs were used and compared.

Result: Evidence of positional and partisan biases was found, where certain speakers were under-represented or misattributed. Hierarchical approaches showed the best results in reducing these disparities.

Conclusion: The study highlights the need for domain-sensitive metrics and ethical considerations in LLM usage for summarization in democratic contexts.

Abstract: The automated summarisation of parliamentary debates using large language
models (LLMs) offers a promising way to make complex legislative discourse more
accessible to the public. However, such summaries must not only be accurate and
concise but also equitably represent the views and contributions of all
speakers. This paper explores the use of LLMs to summarise plenary debates from
the European Parliament and investigates the algorithmic and representational
biases that emerge in this context. We propose a structured, multi-stage
summarisation framework that improves textual coherence and content fidelity,
while enabling the systematic analysis of how speaker attributes -- such as
speaking order or political affiliation -- influence the visibility and
accuracy of their contributions in the final summaries. Through our experiments
using both proprietary and open-weight LLMs, we find evidence of consistent
positional and partisan biases, with certain speakers systematically
under-represented or misattributed. Our analysis shows that these biases vary
by model and summarisation strategy, with hierarchical approaches offering the
greatest potential to reduce disparity. These findings underscore the need for
domain-sensitive evaluation metrics and ethical oversight in the deployment of
LLMs for democratic applications.

</details>


### [700] [Cognitive Castes: Artificial Intelligence, Epistemic Stratification, and the Dissolution of Democratic Discourse](https://arxiv.org/abs/2507.14218)
*Craig S Wright*

Main category: cs.CY

TL;DR: AI is exacerbating cognitive stratification in democratic societies, benefiting those with advanced reasoning skills while pacifying others through engagement-focused interfaces.


<details>
  <summary>Details</summary>
Motivation: To investigate how contemporary AI systems influence cognitive capacities and reshape power structures in liberal-democratic societies.

Method: Analyzed AI's role through a synthesis of formal epistemology, political theory, algorithmic architecture, and economic frameworks.

Result: AI amplifies reasoning for the cognitively skilled, while eroding interpretive agency and fostering a technocratic shift in power dynamics.

Conclusion: The solution lies in fostering rational autonomy as a civic prerequisite through education, epistemic rights, and cognitive infrastructure.

Abstract: Artificial intelligence functions not as an epistemic leveller, but as an
accelerant of cognitive stratification, entrenching and formalising
informational castes within liberal-democratic societies. Synthesising formal
epistemology, political theory, algorithmic architecture, and economic
incentive structures, the argument traces how contemporary AI systems
selectively amplify the reasoning capacity of individuals equipped with
recursive abstraction, symbolic logic, and adversarial interrogation, whilst
simultaneously pacifying the cognitively untrained through engagement-optimised
interfaces. Fluency replaces rigour, immediacy displaces reflection, and
procedural reasoning is eclipsed by reactive suggestion. The result is a
technocratic realignment of power: no longer grounded in material capital
alone, but in the capacity to navigate, deconstruct, and manipulate systems of
epistemic production. Information ceases to be a commons; it becomes the
substrate through which consent is manufactured and autonomy subdued.
Deliberative democracy collapses not through censorship, but through the
erosion of interpretive agency. The proposed response is not technocratic
regulation, nor universal access, but the reconstruction of rational autonomy
as a civic mandate, codified in education, protected by epistemic rights, and
structurally embedded within open cognitive infrastructure.

</details>


### [701] [Culling Misinformation from Gen AI: Toward Ethical Curation and Refinement](https://arxiv.org/abs/2507.14242)
*Prerana Khatiwada,Grace Donaher,Jasymyn Navarro,Lokesh Bhatta*

Main category: cs.CY

TL;DR: The paper analyzes the benefits and risks of new AI technologies, particularly ChatGPT and deepfakes, and proposes guidelines to mitigate harm while fostering innovation.


<details>
  <summary>Details</summary>
Motivation: To address the risks of equity concerns and misinformation brought by AI technologies like ChatGPT and deepfakes, and to promote responsible use and development.

Method: Analyzes academic sources to assess the impact of AI in fields like healthcare, education, and finance, and collaborates with stakeholders to propose guidelines and policy recommendations.

Result: Identifies risks such as equity concerns and misinformation due to AI, and shapes actionable proposals to mitigate these issues while preserving innovation.

Conclusion: Understanding and addressing the risks of AI requires collaboration among users, developers, and governing entities to responsibly innovate and avoid harmful consequences.

Abstract: While Artificial Intelligence (AI) is not a new field, recent developments,
especially with the release of generative tools like ChatGPT, have brought it
to the forefront of the minds of industry workers and academic folk alike.
There is currently much talk about AI and its ability to reshape many everyday
processes as we know them through automation. It also allows users to expand
their ideas by suggesting things they may not have thought of on their own and
provides easier access to information. However, not all of the changes this
technology will bring or has brought so far are positive; this is why it is
extremely important for all modern people to recognize and understand the risks
before using these tools and allowing them to cause harm. This work takes a
position on better understanding many equity concerns and the spread of
misinformation that result from new AI, in this case, specifically ChatGPT and
deepfakes, and encouraging collaboration with law enforcement, developers, and
users to reduce harm. Considering many academic sources, it warns against these
issues, analyzing their cause and impact in fields including healthcare,
education, science, academia, retail, and finance. Lastly, we propose a set of
future-facing guidelines and policy considerations to solve these issues while
still enabling innovation in these fields, this responsibility falling upon
users, developers, and government entities.

</details>


### [702] [Bridging MOOCs, Smart Teaching, and AI: A Decade of Evolution Toward a Unified Pedagogy](https://arxiv.org/abs/2507.14266)
*Bo Yuan,Jiazi Hu*

Main category: cs.CY

TL;DR: This paper reviews three educational paradigms—MOOCs, Smart Teaching, and AI-enhanced learning—and proposes a unified framework combining their strengths.


<details>
  <summary>Details</summary>
Motivation: Traditional education faces challenges in scalability, real-time insights, and personalization, prompting the need for integrated solutions.

Method: The authors develop a three-layer instructional framework combining MOOCs, Smart Teaching, and generative AI, and test it via a project-based course curriculum.

Result: The framework is shown to improve learner engagement, support educators, and enable personalized, scalable learning.

Conclusion: Integrating these paradigms creates a complementary model that addresses the limitations of traditional and isolated education technologies.

Abstract: Over the past decade, higher education has evolved through three distinct
paradigms: the emergence of Massive Open Online Courses (MOOCs), the
integration of Smart Teaching technologies into classrooms, and the rise of
AI-enhanced learning. Each paradigm is intended to address specific challenges
in traditional education: MOOCs enable ubiquitous access to learning resources;
Smart Teaching supports real-time interaction with data-driven insights; and
generative AI offers personalized feedback and on-demand content generation.
However, these paradigms are often implemented in isolation due to their
disparate technological origins and policy-driven adoption. This paper examines
the origins, strengths, and limitations of each paradigm, and advocates a
unified pedagogical perspective that synthesizes their complementary
affordances. We propose a three-layer instructional framework that combines the
scalability of MOOCs, the responsiveness of Smart Teaching, and the adaptivity
of AI. To demonstrate its feasibility, we present a curriculum design for a
project-based course. The findings highlight the framework's potential to
enhance learner engagement, support instructors, and enable personalized yet
scalable learning.

</details>


### [703] [Fiduciary AI for the Future of Brain-Technology Interactions](https://arxiv.org/abs/2507.14339)
*Abhishek Bhattacharjee,Jack Pilkington,Nita Farahany*

Main category: cs.CY

TL;DR: The paper discusses the transformative potentials and risks of brain foundation models, emphasizing the need for fiduciary principles in their development.


<details>
  <summary>Details</summary>
Motivation: To address the ethical and technical challenges of brain foundation models to ensure their benefits do not compromise user autonomy or cognitive freedom.

Method: The paper draws upon legal frameworks and AI alignment methods to propose architectural and governance mechanisms embedding fiduciary duties in BCIs.

Result: Proposes specific technical and governance designs to embed duties like care, loyalty, and confidentiality in brain foundation models.

Conclusion: Embedding fiduciary principles into brain foundation models is critical for responsible development, securing cognitive liberty, and safeguarding user trust.

Abstract: Brain foundation models represent a new frontier in AI: instead of processing
text or images, these models interpret real-time neural signals from EEG, fMRI,
and other neurotechnologies. When integrated with brain-computer interfaces
(BCIs), they may enable transformative applications-from thought controlled
devices to neuroprosthetics-by interpreting and acting on brain activity in
milliseconds. However, these same systems pose unprecedented risks, including
the exploitation of subconscious neural signals and the erosion of cognitive
liberty. Users cannot easily observe or control how their brain signals are
interpreted, creating power asymmetries that are vulnerable to manipulation.
This paper proposes embedding fiduciary duties-loyalty, care, and
confidentiality-directly into BCI-integrated brain foundation models through
technical design. Drawing on legal traditions and recent advancements in AI
alignment techniques, we outline implementable architectural and governance
mechanisms to ensure these systems act in users' best interests. Placing brain
foundation models on a fiduciary footing is essential to realizing their
potential without compromising self-determination.

</details>


### [704] [Mining Voter Behaviour and Confidence: A Rule-Based Analysis of the 2022 U.S. Elections](https://arxiv.org/abs/2507.14236)
*Md Al Jubair,Mohammad Shamsul Arefin,Ahmed Wasif Reza*

Main category: cs.CY

TL;DR: This research connects voter trust with their election experience using the data mining method and highlights the impact of better voting access.


<details>
  <summary>Details</summary>
Motivation: To understand how voter trust relates to their experiences, particularly focusing on marginalized communities.

Method: Applied the Apriori algorithm (rule-based data mining) to the 2022 SPAE survey with parameters like support >= 3%, confidence >= 60%, and lift > 1.5.

Result: Identified connections between accessibility and voter trust, such as Black voters with smooth polling access showing higher registration ease and confidence in vote counting associated with Democratic support.

Conclusion: Improving voting accessibility and providing targeted support can enhance trust in electoral systems, especially among marginalized groups.

Abstract: This study explores the relationship between voter trust and their
experiences during elections by applying a rule-based data mining technique to
the 2022 Survey of the Performance of American Elections (SPAE). Using the
Apriori algorithm and setting parameters to capture meaningful associations
(support >= 3%, confidence >= 60%, and lift > 1.5), the analysis revealed a
strong connection between demographic attributes and voting-related challenges,
such as registration hurdles, accessibility issues, and queue times. For
instance, respondents who indicated that accessing polling stations was "very
easy" and who reported moderate confidence were found to be over six times more
likely (lift = 6.12) to trust their county's election outcome and experience no
registration issues. A further analysis, which adjusted the support threshold
to 2%, specifically examined patterns among minority voters. It revealed that
98.16 percent of Black voters who reported easy access to polling locations
also had smooth registration experiences. Additionally, those who had high
confidence in the vote-counting process were almost two times as likely to
identify as Democratic Party supporters. These findings point to the important
role that enhancing voting access and offering targeted support can play in
building trust in the electoral system, particularly among marginalized
communities.

</details>


### [705] [Unequal Voices: How LLMs Construct Constrained Queer Narratives](https://arxiv.org/abs/2507.15585)
*Atreya Ghosal,Ashim Gupta,Vivek Srikumar*

Main category: cs.CY

TL;DR: The paper examines how Large Language Models (LLMs) narratively marginalize queer individuals by limiting their representation. Results show significant stereotyping and constrained portrayals in LLM outputs.


<details>
  <summary>Details</summary>
Motivation: The motivation is to identify and critique the biased portrayal of marginalized groups, specifically queer individuals, by LLMs in generated discourse.

Method: Hypotheses were formulated to test for harmful representations, narrow portrayals, and discursive othering.

Result: Results confirm that queer individuals are significantly limited in the complexity and diversity of portrayal by LLMs.

Conclusion: LLMs exhibit stereotyping and contribute to the marginalization of queer individuals in discourse, requiring improved model representation.

Abstract: One way social groups are marginalized in discourse is that the narratives
told about them often default to a narrow, stereotyped range of topics. In
contrast, default groups are allowed the full complexity of human existence. We
describe the constrained representations of queer people in LLM generations in
terms of harmful representations, narrow representations, and discursive
othering and formulate hypotheses to test for these phenomena. Our results show
that LLMs are significantly limited in their portrayals of queer personas.

</details>


### [706] [Why can't Epidemiology be automated (yet)?](https://arxiv.org/abs/2507.15617)
*David Bann,Ed Lowther,Liam Wright,Yevgeniya Kovalchuk*

Main category: cs.CY

TL;DR: The paper explores how generative AI can assist epidemiological research, identifying tasks where AI is beneficial and its current limitations.


<details>
  <summary>Details</summary>
Motivation: To assess whether generative AI tools can accelerate or automate epidemiological research, taking advantage of the discipline's reliance on secondary data analysis.

Method: The authors mapped various epidemiological tasks and demonstrated examples of AI-generated epidemiological outputs to assess AI utility and limitations.

Result: AI tools were found to boost productivity, primarily in coding and administrative tasks, but their effectiveness is limited by issues like hallucinations and dataset access barriers.

Conclusion: Realizing AI's full potential in epidemiology requires collaboration between epidemiologists and engineers for empirical testing and system improvements.

Abstract: Recent advances in artificial intelligence (AI) - particularly generative AI
- present new opportunities to accelerate, or even automate, epidemiological
research. Unlike disciplines based on physical experimentation, a sizable
fraction of Epidemiology relies on secondary data analysis and thus is
well-suited for such augmentation. Yet, it remains unclear which specific tasks
can benefit from AI interventions or where roadblocks exist. Awareness of
current AI capabilities is also mixed. Here, we map the landscape of
epidemiological tasks using existing datasets - from literature review to data
access, analysis, writing up, and dissemination - and identify where existing
AI tools offer efficiency gains. While AI can increase productivity in some
areas such as coding and administrative tasks, its utility is constrained by
limitations of existing AI models (e.g. hallucinations in literature reviews)
and human systems (e.g. barriers to accessing datasets). Through examples of
AI-generated epidemiological outputs, including fully AI-generated papers, we
demonstrate that recently developed agentic systems can now design and execute
epidemiological analysis, albeit to varied quality (see
https://github.com/edlowther/automated-epidemiology). Epidemiologists have new
opportunities to empirically test and benchmark AI systems; realising the
potential of AI will require two-way engagement between epidemiologists and
engineers.

</details>


### [707] [Left Leaning Models: AI Assumptions on Economic Policy](https://arxiv.org/abs/2507.15771)
*Maxim Chupilkin*

Main category: cs.CY

TL;DR: This study investigates how large language models (LLMs) evaluate economic policy and finds that they prioritize unemployment, inequality, financial stability, and environmental harm over traditional concerns like growth, inflation, and debt.


<details>
  <summary>Details</summary>
Motivation: To understand the priorities and assumptions of LLMs when assessing economic policies, which remain opaque.

Method: A conjoint experiment was conducted to discern which factors most influence LLMs' evaluations of economic policies.

Result: LLMs prioritize unemployment, inequality, financial stability, and environmental harm, showing less sensitivity to growth, inflation, and debt, with results consistent across different models and scenarios.

Conclusion: LLMs exhibit unique prioritization that diverges from traditional macroeconomic focuses, illustrating the need for further examination of their decision-making in economic contexts.

Abstract: How does AI think about economic policy? While the use of large language
models (LLMs) in economics is growing exponentially, their assumptions on
economic issues remain a black box. This paper uses a conjoint experiment to
tease out the main factors influencing LLMs' evaluation of economic policy. It
finds that LLMs are most sensitive to unemployment, inequality, financial
stability, and environmental harm and less sensitive to traditional
macroeconomic concerns such as economic growth, inflation, and government debt.
The results are remarkably consistent across scenarios and across models.

</details>
