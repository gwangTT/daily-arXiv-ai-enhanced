<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 11]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.CV](#cs.CV) [Total: 88]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 86]
- [cs.NE](#cs.NE) [Total: 4]
- [cs.PL](#cs.PL) [Total: 2]
- [cs.RO](#cs.RO) [Total: 28]
- [cs.SE](#cs.SE) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [eess.AS](#eess.AS) [Total: 3]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 2]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.DL](#cs.DL) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [cs.CR](#cs.CR) [Total: 6]
- [eess.IV](#eess.IV) [Total: 12]
- [eess.SP](#eess.SP) [Total: 2]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.IR](#cs.IR) [Total: 8]
- [eess.SY](#eess.SY) [Total: 3]
- [math.NA](#math.NA) [Total: 3]
- [math-ph](#math-ph) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [quant-ph](#quant-ph) [Total: 3]
- [cs.IT](#cs.IT) [Total: 2]
- [econ.EM](#econ.EM) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Rethinking the Illusion of Thinking](https://arxiv.org/abs/2507.01231)
*IÃ±aki Dellibarda Varela,Pablo Romero-Sorozabal,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: The paper re-evaluates critiques of Large Reasoning Models (LRMs) by refining contentious benchmarks, revealing nuanced reasoning limitations.


<details>
  <summary>Details</summary>
Motivation: To clarify debates on whether LRMs genuinely lack reasoning abilities as branded by 'The Illusion of Thinking.'

Method: Through replication and refinement of benchmarks (Towers of Hanoi and River Crossing), introducing techniques such as incremental stepwise prompting and agentic collaborative dialogue.

Result: Finds LRMs struggle with moderate complexity (e.g., Towers of Hanoi with 8 disks). Contrarily, LRMs excel at solving solvable River Crossing problems, past claims mischaracterized failures due to testing unsolvable configurations.

Conclusion: LRMs are not simplistic 'stochastic parrots' but iterative reasoners in an underexplored search state space. Progress requires detailed ablation studies like this one.

Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [2] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang,Wenli Yang,Monica R Roberts,Byeong Ho Kang,Charles B Malpas*

Main category: cs.AI

TL;DR: Large Language Models (LLMs) hold potential for aiding medical diagnoses, particularly in dementia care, but face challenges in transparency, causal reasoning, and clinical integration.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to evaluate and address the limitations of AI systems, especially LLMs, in practical medical applications like dementia care.

Method: The authors conducted a scoping review to assess the limitations and potentials of AI in clinical settings, exploring hybrid methods for improved interpretability and workflow integration.

Result: LLMs did not significantly improve diagnostic accuracy or speed when used by physicians. Hybrid AI models show promise, but current approaches still rely on data-driven paradigms that lack clinician involvement.

Conclusion: Future AI systems should emphasize human-in-the-loop approaches, explanatory coherence, and alignment with clinical workflows to improve trust, usability, and patient outcomes.

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [3] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/abs/2507.01376)
*Yinwang Ren,Yangyang Liu,Tang Ji,Xun Xu*

Main category: cs.AI

TL;DR: The paper reviews advancements in LLM-Agents, MLLM-Agents, and Agentic AI for smart manufacturing while exploring their potential and challenges.


<details>
  <summary>Details</summary>
Motivation: The study seeks to clarify unclear definitions, boundaries, and practical applications of advanced AI paradigms in smart manufacturing.

Method: The paper systematically reviews the evolution of AI agents, focusing on the concepts and technological progress of LLM-Agents, MLLM-Agents, and Agentic AI.

Result: The paper identifies potential applications and integration strategies for advanced AI paradigms into smart manufacturing, while addressing accompanying challenges.

Conclusion: Emerging AI paradigms hold significant promise for innovation in smart manufacturing, though challenges in their practical adoption require attention.

Abstract: AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [4] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/abs/2507.01410)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: The paper proposes a formal method for ethical decision-making evaluation using fuzzy logic and applies it to a medical case study.


<details>
  <summary>Details</summary>
Motivation: Establishing clear standards for evaluating moral machines is challenging due to the complexities of the moral domain.

Method: Develops ethical decision-making models described through fuzzy rules, verified and validated with fuzzy Petri nets.

Result: Applied the method to a medical case study to demonstrate its feasibility.

Conclusion: The approach provides a structured way to evaluate ethical decision-making models in complex domains.

Abstract: The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [5] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: Pensieve is an AI-assisted grading platform for evaluating handwritten, open-ended student responses, reducing grading time by 65% with 95.4% grading accuracy with instructor consistency.


<details>
  <summary>Details</summary>
Motivation: Grading handwritten and open-ended STEM responses is a major time-consuming challenge in large university courses, necessitating automation to streamline the process.

Method: Pensieve employs large language models (LLMs) to handle the entire grading pipelineâretrieving, transcribing, and evaluating student work with an interactive human-in-the-loop setup.

Result: Pensieve has been deployed at over 20 institutions, grading over 300,000 responses, achieving a grading accuracy agreement of 95.4% and reducing time spent on grading by 65%.

Conclusion: Pensieve demonstrates the potential for AI to assist in grading processes, significantly improving efficiency while retaining high alignment with instructors' evaluations across STEM disciplines.

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [6] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446)
*Abd Elrahman Amer,Magdi Amer*

Main category: cs.AI

TL;DR: This paper introduces a multi-agent system combining Large Language Models (LLMs) with fuzzy logic to handle SMS customer requests and reduce hallucination risks.


<details>
  <summary>Details</summary>
Motivation: To improve customer service quality and response times while addressing the hallucination risks associated with LLM technologies.

Method: The creation of a multi-agent system that integrates LLM-based agents with fuzzy logic technology to process customer requests via SMS.

Result: A system that aims to both improve response quality and manage hallucination risks effectively in customer service.

Conclusion: The presented multi-agent system provides a balanced approach to enhancing customer service using LLMs while mitigating their inherent risks.

Abstract: Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [7] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2507.01489)
*Yanfei Zhang*

Main category: cs.AI

TL;DR: This paper proposes a hierarchical framework called "Agent-as-tool" to separate tool calling and reasoning processes in LLMs, improving efficiency and reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenges faced by previous reinforcement learning research involving LLMs, particularly in handling tool-generated raw information that imposes a reasoning burden on the model.

Method: They detach the tool calling process from the reasoning process by using a separate agent for tool management, enabling efficient verbal reasoning by the main model.

Result: Their approach achieved comparable results with minimal fine-tuning (180 samples) and outperformed benchmarks in the Bamboogle task, with scores of 63.2% exact match and 75.2% cover exact match.

Conclusion: Separating tool handling and reasoning processes enhances the performance of LLMs in agent tasks, demonstrating the step forward in reinforcement learning with AI-driven frameworks.

Abstract: Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [8] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Main category: cs.AI

TL;DR: The paper introduces T3DM, an approach addressing challenges in Temporal Knowledge Graph Reasoning (TKGR) by tackling event distribution shifts and improving negative sampling quality.


<details>
  <summary>Details</summary>
Motivation: TKGR models face challenges including inadequate modeling of event distribution shifts between train and test phases and reliance on low-quality negative sampling strategies.

Method: T3DM, a novel approach, integrates Test-Time Training-guided Distribution Shift Modelling to handle distribution shifts, and employs an adversarial training-based negative-sampling strategy to generate superior negative samples.

Result: Extensive experiments demonstrate that T3DM significantly outperforms state-of-the-art baselines in robustness and reasoning accuracy.

Conclusion: T3DM improves the robustness and global reasoning consistency of TKGR models by addressing key limitations in distribution shift modeling and negative sampling.

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [9] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu,Ashok Urlana,Charaka Vinayak Kumar,Bala Mallikarjunarao Garlapati*

Main category: cs.AI

TL;DR: The paper presents "Agent Ideate," a framework leveraging Large Language Models (LLMs) and agents to generate innovative product ideas based on patents.


<details>
  <summary>Details</summary>
Motivation: To overcome the challenge of accessing and interpreting technical knowledge in patents to derive innovative product ideas.

Method: Designed and experimented with a framework, "Agent Ideate," using LLMs and agent-based architectures in three domains: Computer Science, Natural Language Processing, and Material Chemistry.

Result: Agent-based approaches outperformed standalone LLMs in idea quality, relevance, and novelty.

Conclusion: Combining LLMs with autonomous agents significantly enhances business idea generation from patent data, unlocking innovation potential.

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [10] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/abs/2507.01749)
*Arash Dehghan,Mucahit Cevik,Merve Bodur,Bissan Ghaddar*

Main category: cs.AI

TL;DR: This paper proposes a crowd-shipping model where in-store shoppers serve as delivery couriers, using a combination of NeurADP and DDQN approaches for adaptive assignment and pricing, achieving significant cost savings in urban delivery systems.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the inefficiency in last-mile delivery within urban areas by leveraging in-store customers as delivery couriers in a crowd-shipping model.

Method: An MDP model is developed to account for uncertainties such as stochastic order and shopper arrivals, and probabilistic delivery acceptance. Neural Approximate Dynamic Programming (NeurADP) is used for order-to-shopper assignment, combined with Deep Double Q-Network (DDQN) for dynamic pricing.

Result: The proposed solution outperforms fixed pricing and myopic baselines, with delivery cost savings of up to 6.7% and 18%, respectively. Allowing flexible delays and multi-destination routing further reduces costs by up to 17%.

Conclusion: Dynamic and forward-looking policies like the integrated NeurADP and DDQN strategy can significantly enhance efficiency in crowd-shipping systems, offering actionable insights for urban logistics.

Abstract: This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [11] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/abs/2507.01833)
*Yi-Dong Shen,Thomas Eiter*

Main category: cs.AI

TL;DR: The paper refines and extends principles for answer set programming semantics, questioning traditional constraints while proposing alternative, generalized properties.


<details>
  <summary>Details</summary>
Motivation: Revisiting foundational conditions of answer set semantics and proposing refined principles for enhanced flexibility in non-monotonic logic programming.

Method: Illustrates examples to challenge existing constraints, introduces well-supportedness and minimality principles, extends GAS principles, defines new semantics, and assesses computational complexity.

Result: Refined GAS principles address issues with traditional constraints, provide a flexible semantics framework, and offer insights into computational complexity.

Conclusion: The refined GAS principles and new semantics improve the philosophical and computational organization of answer set programming, while also functioning as a baseline for evaluating logic programming frameworks.

Abstract: Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [12] [CarbonClarity: Understanding and Addressing Uncertainty in Embodied Carbon for Sustainable Computing](https://arxiv.org/abs/2507.01145)
*Xuesi Chen,Leo Han,Anvita Bhagavathula,Udit Gupta*

Main category: cs.AR

TL;DR: This paper introduces CarbonClarity, a probabilistic framework for modeling uncertainties in embodied carbon footprint in semiconductor design, highlighting the potential for improved carbon-aware decision-making.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the lack of models that incorporate spatial and temporal variability in the semiconductor supply chain, limiting system designers' ability to address carbon emissions effectively.

Method: The authors developed CarbonClarity, a probabilistic framework that models embodied carbon footprints by accounting for uncertainties in factors like energy-per-area, yield, and carbon intensity across various technology nodes.

Result: The framework reveals variability in embodied carbon emissions, with up to 1.6X gap between mean and 95th percentile values for the 7nm node. It also shows that chiplet technology and mature nodes significantly reduce both emissions and uncertainty.

Conclusion: CarbonClarity helps system designers make carbon-aware decisions, emphasizing the value of chiplet architecture and mature nodes to lower emissions and uncertainties for better carbon budgeting.

Abstract: Embodied carbon footprint modeling has become an area of growing interest due
to its significant contribution to carbon emissions in computing. However, the
deterministic nature of the existing models fail to account for the spatial and
temporal variability in the semiconductor supply chain. The absence of
uncertainty modeling limits system designers' ability to make informed,
carbon-aware decisions. We introduce CarbonClarity, a probabilistic framework
designed to model embodied carbon footprints through distributions that reflect
uncertainties in energy-per-area, gas-per-area, yield, and carbon intensity
across different technology nodes. Our framework enables a deeper understanding
of how design choices, such as chiplet architectures and new vs. old technology
node selection, impact emissions and their associated uncertainties. For
example, we show that the gap between the mean and 95th percentile of embodied
carbon per cm$^2$ can reach up to 1.6X for the 7nm technology node.
Additionally, we demonstrate through case studies that: (i) CarbonClarity is a
valuable resource for device provisioning, help maintaining performance under a
tight carbon budget; and (ii) chiplet technology and mature nodes not only
reduce embodied carbon but also significantly lower its associated uncertainty,
achieving an 18% reduction in the 95th percentile compared to monolithic
designs for the mobile application.

</details>


### [13] [SD-Acc: Accelerating Stable Diffusion through Phase-aware Sampling and Hardware Co-Optimizations](https://arxiv.org/abs/2507.01309)
*Zhican Wang,Guanghui He,Hongxiang Fan*

Main category: cs.AR

TL;DR: The paper introduces SD-Acc, a framework to optimize computation and hardware for text-to-image generation in Stable Diffusion models, improving speed and energy efficiency while reducing computational demands by up to 3x.


<details>
  <summary>Details</summary>
Motivation: Stable Diffusion models, despite their advanced generative capabilities, are hindered by high computational and memory demands, limiting their speed and energy efficiency.

Method: SD-Acc employs both algorithmic and hardware optimizations. Algorithmically, it uses an adaptive, phase-aware sampling strategy to reduce redundant computations and balance image quality with complexity. Hardware-wise, it incorporates an address-centric dataflow system, a two-stage streaming architecture for nonlinear functions, and adaptive dataflow optimizations tailored to StableDiff workloads.

Result: The framework achieves up to a 3x reduction in computational demands across StableDiff models without degrading image quality. When paired with the optimized hardware accelerator, it outperforms conventional CPU and GPU setups in speed and energy efficiency.

Conclusion: SD-Acc effectively balances the trade-off between computational efficiency and image quality in Stable Diffusion models. Its co-optimization approach streamlines both software and hardware processes, making text-to-image generation faster and more energy-efficient.

Abstract: The emergence of diffusion models has significantly advanced generative AI,
improving the quality, realism, and creativity of image and video generation.
Among them, Stable Diffusion (StableDiff) stands out as a key model for
text-to-image generation and a foundation for next-generation multi-modal
algorithms. However, its high computational and memory demands hinder inference
speed and energy efficiency. To address these challenges, we identify three
core issues: (1) intensive and often redundant computations, (2) heterogeneous
operations involving convolutions and attention mechanisms, and (3) diverse
weight and activation sizes.
  We present SD-Acc, a novel algorithm and hardware co-optimization framework.
At the algorithm level, we observe that high-level features in certain
denoising phases show significant similarity, enabling approximate computation.
Leveraging this, we propose an adaptive, phase-aware sampling strategy that
reduces compute and memory loads. This framework automatically balances image
quality and complexity based on the StableDiff model and user requirements. At
the hardware level, we design an address-centric dataflow to efficiently handle
heterogeneous operations within a simple systolic array. We address the
bottleneck of nonlinear functions via a two-stage streaming architecture and a
reconfigurable vector processing unit. Additionally, we implement adaptive
dataflow optimizations by combining dynamic reuse and operator fusion tailored
to StableDiff workloads, significantly reducing memory access. Across multiple
StableDiff models, our method achieves up to a 3x reduction in computational
demand without compromising image quality. Combined with our optimized hardware
accelerator, SD-Acc delivers higher speed and energy efficiency than
traditional CPU and GPU implementations.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [14] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
*Imran Mirza,Cole Huang,Ishwara Vasista,Rohan Patil,Asli Akalin,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: The paper introduces MALIBU, a benchmark for evaluating implicit biases in multi-agent systems using large language models.


<details>
  <summary>Details</summary>
Motivation: To address concerns about potential reinforcement of social biases and stereotypes by LLM-based multi-agent systems during persona-based interactions.

Method: The benchmark assesses biases using scenario-based tasks judged by an LLM-based system in two phases: scoring individual demographic personas and comparing paired responses.

Result: Results show that bias mitigation strategies may sometimes favor marginalized groups, highlighting challenges in achieving true neutrality.

Conclusion: The study underscores the importance of nuanced bias detection, balanced fairness approaches, and transparent benchmarks in designing multi-agent systems.

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [15] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
*Huiling You,Samia Touileb,Erik Velldal,Lilja Ãvrelid*

Main category: cs.CL

TL;DR: The paper examines a method for evaluating abstractive summaries by focusing on overlapping event information between generated, reference, and source content.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation methods for abstractive summaries rely heavily on human-authored references and focus primarily on surface-level similarities. This study aims to focus on events as the key element of news storytelling.

Method: The authors propose a novel evaluation method for abstractive summaries by calculating overlapping events between generated summaries, reference summaries, and the original news articles. They test this using a Norwegian dataset with detailed event annotations and summaries written by experts.

Result: Their approach effectively highlights the event-related information contained in summaries and provides insights beyond traditional similarity metrics.

Conclusion: Using event overlap as an evaluation metric offers a more meaningful understanding of summary quality and suitability for news articles.

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [16] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
*Simon BÃ¶rjesson,Erik Ersmark,Pierre Nugues*

Main category: cs.CL

TL;DR: This study analyzed shifts in geographic entries between the first and second editions of the Swedish encyclopedia 'Nordisk familjebok' using semantic embeddings and transformer-based classifiers.


<details>
  <summary>Details</summary>
Motivation: To investigate how intellectual trends and geographical focus shifted in Sweden's 'Nordisk familjebok' encyclopedia across its two editions.

Method: The study used digitized texts resegmented into entries and paired them using semantic sentence embeddings. Geographic entries were classified using a transformer-based model and linked to Wikidata for trend analysis.

Result: A shift in geographic focus from Europe towards regions like North America, Africa, Asia, Australia, and northern Scandinavia was observed between the first (1876-1899) and second editions (1904-1926).

Conclusion: The findings reflect intellectual evolution in the encyclopedia due to historical events like WWI and the emergence of new global powers, demonstrating an expanded worldview in the later edition.

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [17] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
*Adamu Lawan,Juhua Pu,Haruna Yunusa,Jawad Muhammad,Muhammad Lawan*

Main category: cs.CL

TL;DR: The paper proposes an advanced xLSTM model called MEGA to improve Aspect-based Sentiment Analysis (ABSA), achieving superior accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing ABSA methods face challenges in balancing computational efficiency and high performance due to issues in global context modeling, resource demands, and local correlation handling.

Method: The proposed MEGA framework integrates bi-directional xLSTM architecture using forward and partially flipped backward (PF-mLSTM) streams. It introduces MECGAF for optimized fusion of outputs, capturing short-range dependencies while preserving global context.

Result: MEGA demonstrates superior performance over state-of-the-art ABSA baselines in terms of both accuracy and computational efficiency on three benchmark datasets.

Conclusion: The MEGA framework effectively addresses known limitations in ABSA methods by improving localized and global context modeling, offering a scalable and high-performing solution.

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [18] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
*Yu Fan,Yang Tian,Shauli Ravfogel,Mrinmaya Sachan,Elliott Ash,Alexander Hoyle*

Main category: cs.CL

TL;DR: This paper proposes a debiasing algorithm to counteract biases in embedding-based similarity metrics caused by document confounders, significantly improving the quality of similarity and clustering metrics on text sequences.


<details>
  <summary>Details</summary>
Motivation: To address the problem of biases in text sequence similarity metrics caused by spurious attributes such as the text's source or language, which can hinder applications pooling texts from different corpora.

Method: A debiasing algorithm is proposed to remove information about observed document confounders from encoder representations, reducing the influence of these biases.

Result: The debiasing algorithm significantly improves document similarity and clustering metrics across multiple embedding variants and tasks, while maintaining performance on out-of-distribution benchmarks.

Conclusion: The algorithm effectively reduces biases caused by confounders in text sequence embeddings without compromising overall performance, making it valuable for applications involving pooled corpora.

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [19] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*MichaÅ Matak,JarosÅaw A. Chudziak*

Main category: cs.CL

TL;DR: The paper introduces gAIus, a retrieval system enhanced by large language models for handling Polish legal texts, achieving significant improvements in accuracy over existing models.


<details>
  <summary>Details</summary>
Motivation: Existing large language models (LLMs) have limitations in providing reliable references and accurate answers for legal matters in non-English and non-Chinese contexts, prompting research into more context-sensitive and explainable retrieval methods.

Method: The paper proposes a new architecture called gAIus, which uses large language models in combination with a retrieval mechanism tailored for Polish Civil Code. A specialized dataset based on Polish law apprenticeship questions was created for evaluation.

Result: The gAIus system improved GPT-3.5-turbo-0125's score by 419%, enabling it to outperform GPT-4o and boosting GPT-4o-mini's score from 31% to 86%.

Conclusion: The gAIus architecture demonstrates significant potential for enhancing LLM-based tasks in legal domains and suggests future research directions and applications in law.

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [20] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
*Cindy Lie Tabuse,David Restepo,Carolina Gracitelli,Fernando Korn Malerbi,Caio Regatieri,Luis Filipe Nakayama*

Main category: cs.CL

TL;DR: This study explores GPT-4's capability to analyze retinal fundus image descriptions for clinical decision-making in ophthalmology, showing moderate success in basic tasks but poor performance in complex applications.


<details>
  <summary>Details</summary>
Motivation: To investigate the largely unexplored utility of GPT-4 in ophthalmology for simulating clinical decision-making from structured prompts, specifically in diabetic retinopathy and glaucoma screening.

Method: The study utilized 300 annotated fundus images and used structured textual prompts, with or without patient metadata, to evaluate GPT-4's diagnostic and referral decision-making abilities using accuracy, F1 scores, Cohenâs kappa, McNemarâs test, and change rate analysis.

Result: GPT-4 performed moderately well for basic tasks such as diabetic retinopathy severity classification and referral recommendations but showed poor performance for complex tasks like glaucoma referral analysis. Metadata inclusion did not affect the model's outputs significantly.

Conclusion: While GPT-4 cannot replace clinical systems for precision tasks, it holds potential for educational, documentation, and annotation purposes in ophthalmology workflows.

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [21] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Main category: cs.CL

TL;DR: The paper presents CARE-RAG, a framework aimed at improving the reliability of Retrieval-Augmented Generation (RAG) systems by resolving conflicts between LLMs' internal knowledge and retrieved information.


<details>
  <summary>Details</summary>
Motivation: RAG systems often suffer from reliability issues due to inconsistencies between a modelâs internal knowledge and retrieved external content.

Method: The approach includes parameter-aware evidence extraction, context-aware refinement of retrieved content, conflict-driven summarization using a distilled LLaMA model, and introducing a QA Repair step for benchmark evaluation.

Result: Experiments show that CARE-RAG outperforms existing RAG systems, notably when dealing with noisy or contradictory evidence.

Conclusion: CARE-RAG enhances trustworthiness in RAG systems by effectively resolving knowledge conflicts, making it more suitable for scenarios needing reliable evidence synthesis.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [22] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Main category: cs.CL

TL;DR: The study improves Retrieval-augmented Generation (RAG) performance on reasoning-intensive benchmarks by introducing CompactDS, a high-quality, web-scale datastore for effective retrieval.


<details>
  <summary>Details</summary>
Motivation: Challenges in reasoning-intensive tasks require a robust datastore aligned with pretraining breadth to optimize RAG.

Method: CompactDS combines in-memory approximate nearest neighbor search with on-disk exact search to balance speed and recall, offering a filtered, high-quality datastore.

Result: Usage of CompactDS yields accuracy improvements (10%-33%) across benchmarks like MMLU, MATH, and GPQA, outperforming existing systems including web search engines.

Conclusion: CompactDS demonstrates simplicity, reproducibility, and superior performance, paving the way for enhanced retrieval-based AI research and applications.

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [23] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
*Kai Liu,Bowen Xu,Shaoyu Wu,Xin Chen,Hao Zhou,Yongliang Tao,Lulu Hu*

Main category: cs.CL

TL;DR: LaRoSA is a novel activation sparsification method designed for Large Language Models (LLMs) that enhances efficiency without requiring retraining or magnitude-based pruning.


<details>
  <summary>Details</summary>
Motivation: Existing sparsification methods for LLM inference face challenges such as requiring retraining or introducing unstable inference speed due to empirical pruning approaches.

Method: LaRoSA applies layerwise orthogonal rotations to input activations, followed by Top-K selection to achieve a consistent sparsity level across models.

Result: LaRoSA demonstrates effective sparsification with minimal performance loss across various LLMs. For instance, at 40% sparsity, LLaMA2-7B shows a mere 0.17 perplexity gap and achieves a 1.30x speed-up, outperforming other methods like TEAL and CATS.

Conclusion: LaRoSA is a reliable and efficient activation sparsification method that delivers consistent sparsity and inference acceleration for LLMs, addressing key limitations of existing techniques.

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [24] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
*Nifu Dan,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: The study explores using advanced instruction-tuned models like Deepseek-R1 to solve physics problems from the SciBench benchmark, achieving state-of-the-art accuracy and emphasizing symbolic derivation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the challenge of applying Large Language Models to physics reasoning, which requires deep conceptual understanding and strong problem-solving skills.

Method: They employ advanced instruction-tuned reasoning models, such as Deepseek-R1, and experiment with few-shot prompting to solve diverse physics problems from the SciBench benchmark.

Result: The models achieve state-of-the-art accuracy in physics problem-solving and highlight distinctive reasoning patterns focused on symbolic derivation.

Conclusion: The research demonstrates the potential of instruction-tuned reasoning models in physics and shows that few-shot prompting can significantly enhance their performance, allowing room for further improvement.

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [25] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: LEDOM is a reverse language model trained autoregressively on substantial data, showcasing its utility as a foundational model and improving mathematical reasoning tasks through a novel Reverse Reward application.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of reverse language models as foundational models for general tasks and their unique properties in improving specific applications like mathematical reasoning.

Method: Developed LEDOM, a reverse autoregressive language model, and proposed the Reverse Reward approach, which refines forward language model outputs via LEDOM-guided reranking.

Result: LEDOM demonstrated improved performance on mathematical reasoning tasks by leveraging its backward reasoning capabilities through a posterior evaluation framework.

Conclusion: LEDOM's backward reasoning provides unique advantages with broad applications, and the release of its resources aims to foster further research and innovation.

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [26] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: The paper addresses the shortcomings of current reward models used in reinforcement learning from human feedback and introduces a large-scale preference dataset (SynPref-40M) and new models (Skywork-Reward-V2) to overcome these limitations.


<details>
  <summary>Details</summary>
Motivation: Existing reward models struggle to capture complex human preferences due to limitations in preference datasets, such as narrow scope or synthetic labeling.

Method: The authors created a large preference dataset (SynPref-40M) using a human-AI synergistic pipeline for data curation and trained eight new reward models based on a subset of this dataset.

Result: Skywork-Reward-V2 models achieved state-of-the-art performance across seven major benchmarks for reward models.

Conclusion: Human-AI collaboration in data curation can significantly enhance dataset quality, enabling reward models to better align with nuanced human preferences.

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [27] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
*Ting Xu,Xiaoxiao Deng,Xiandong Meng,Haifeng Yang,Yan Wu*

Main category: cs.CL

TL;DR: The paper introduces a Transformer-based deep learning model using attention mechanisms for multi-label disease prediction from electronic health records, achieving superior performance on the MIMIC-IV dataset.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of handling unstructured and semantically complex medical text data in electronic health records.

Method: The authors utilize a Transformer-based architecture with multi-layer self-attention mechanisms for representation learning, and a Sigmoid-based classifier for multi-label disease prediction. Context-aware semantic alignment is integrated to enhance model robustness and capture label co-occurrence.

Result: The proposed method consistently outperforms existing approaches in performance metrics, showcasing strong generalization abilities across varying data scales and interference scenarios.

Conclusion: The framework provides an effective algorithm for clinical text modeling and is practically significant for real-world medical applications.

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [28] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
*Tianyu Liu,Qitan Lv,Hao Li,Xing Gao,Xiao Sun*

Main category: cs.CL

TL;DR: The paper introduces LogitSpec, a method to improve speculative decoding (SD) by leveraging logits to expand the retrieval range for draft tokens, achieving notable speedups in LLM inference.


<details>
  <summary>Details</summary>
Motivation: To address inefficiencies in retrieval-based speculative decoding, which often fails to find accurate draft tokens, thereby hindering the acceleration potential of LLM inference.

Method: LogitSpec generates draft tokens using the logits of the last token to speculate two consecutive tokens. The framework is training-free and works by retrieving references for these tokens to enhance their relevance.

Result: Experiments show that LogitSpec achieves up to 2.61Ã speedup and 3.28 mean accepted tokens per decoding step across various text generation benchmarks.

Conclusion: LogitSpec offers a simple, effective, and easily integrable solution to improve LLM inference by enhancing speculative decoding without requiring additional model training.

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [29] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Main category: cs.CL

TL;DR: The study explores a novel method to enhance automatic text simplification (ATS) systems using large language models (LLMs), integrating direct preference optimization (DPO) based on human feedback from persons with intellectual disabilities for personalized output.


<details>
  <summary>Details</summary>
Motivation: The research is motivated by the need to improve text accessibility for individuals with intellectual disabilities and address the gap in ATS systems which lack personalization due to absence of preference feedback during training.

Method: The paper extends supervised fine-tuning (SFT) of LLM-based ATS models by utilizing direct preference optimization (DPO) with human feedback to tailor text simplifications. It also proposes a comprehensive development pipeline for personalized ATS systems.

Result: The study demonstrates that incorporating human feedback from the target group effectively aligns ATS systems with the preferences and expectations of persons with intellectual disabilities.

Conclusion: This work highlights the importance of involving target group representatives in developing AI systems for inclusivity, taking one step closer to personalized accessibility solutions.

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [30] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
*Ãlvaro Zaera,Diana Nicoleta Popa,Ivan Sekulic,Paolo Rosso*

Main category: cs.CL

TL;DR: This paper introduces a method that combines uncertainty modeling with fine-tuned large language models (LLMs) to improve the accuracy and efficiency of detecting out-of-scope (OOS) intents in dialogue systems.


<details>
  <summary>Details</summary>
Motivation: The challenge of detecting OOS intents is vital for ensuring the robustness of task-oriented dialogue systems (TODS) against unseen and ambiguous queries.

Method: The approach involves two steps: (1) Applying uncertainty estimation to outputs from an existing in-scope intent detection classifier used in a real-world TODS. (2) Using a fine-tuned LLM to handle instances with high uncertainty, leveraging its decision-making ability.

Result: The proposed method achieves state-of-the-art results on standard OOS detection benchmarks, including real-world data from a deployed TODS, balancing computational efficiency and performance.

Conclusion: The method combines traditional techniques with LLM-based enhancements to handle OOS intent detection efficiently and accurately, making it a practical solution for real-world dialogue systems.

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [31] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
*Quang Minh Nguyen,Taegyoon Kim*

Main category: cs.CL

TL;DR: The study evaluates the impact of external information like Wikipedia excerpts on stance detection tasks using large language models (LLMs).


<details>
  <summary>Details</summary>
Motivation: To investigate whether external information benefits stance detection in LLMs, given previously observed advantages in BERT-based systems.

Method: The researchers systematically analyzed the effect of Wikipedia and web search-derived information on eight LLMs across three datasets and 12 targets.

Result: Results revealed that external information leads to performance degradation, with macro F1 scores dropping by up to 27.9%.

Conclusion: External information biases LLM stance predictions, contrasting with prior findings for BERT-based systems. Fine-tuning slightly mitigates but doesn't entirely eliminate these biases, emphasizing risks in relying on external data for LLMs.

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [32] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
*Shutong Feng,Hsien-chin Lin,Nurul Lubis,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Renato Vukovic,Milica GaÅ¡iÄ*

Main category: cs.CL

TL;DR: This paper introduces LUSTER, a unified ToD system that combines LLMs with end-to-end reinforcement learning to optimize for both user sentiment and task success.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in creating emotionally intelligent and task-efficient ToD systems that operate in noisy, ambiguous conversational settings.

Method: The authors propose LUSTER, leveraging a natural-language user simulator, an imperfect NLU module, and reinforcement learning for short-term (sentiment) and long-term (task success) rewards.

Result: LUSTER improves resilience and emotional responsiveness in ToD systems by integrating LLM capabilities with reward modeling.

Conclusion: The study offers a practical approach to developing next-gen ToD systems using LLMs and structured reward mechanisms.

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [33] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.CL

TL;DR: A new dataset for chart question answering (CQA) is designed to reflect real-world reasoning workflows, and multimodal language models exhibit significant challenges in performing well.


<details>
  <summary>Details</summary>
Motivation: There is a need for a dataset that mirrors authentic reasoning workflows, as prior benchmarks for CQA fail to reflect ecologically valid contexts.

Method: A dataset was constructed using real-world, multi-view charts from visualization notebooks, paired with analytically grounded natural language questions.

Result: GPT-4.1 achieved a 69.3% accuracy in benchmark tests, indicating a significant performance gap in handling authentic CQA settings.

Conclusion: The dataset presents a more authentic CQA challenge, revealing the limitations of current state-of-the-art models in natural reasoning workflows.

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [34] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Main category: cs.CL

TL;DR: This paper compares global score-based evaluations with pairwise comparisons for NLP model benchmarking, finding strengths and limitations in both.


<details>
  <summary>Details</summary>
Motivation: The motivation lies in understanding and improving the effectiveness of model evaluation strategies as NLP benchmarking shifts from global scoring to pairwise comparison leaderboards.

Method: The study uses computational experiments on synthetic and real-world datasets, leveraging standard global metrics and the Bradley-Terry model for pairwise comparisons.

Result: Global scores offer reliable rankings but may undervalue models with rare significant errors, while pairwise comparisons excel in identifying strong models with lower global scores, particularly in challenging domains like text generation.

Conclusion: Both evaluation methods are complementary, with global scores better for general reliability and pairwise comparisons more suited for nuanced insights, especially in areas with hard-to-define metrics.

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [35] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
*Rifki Afina Putri*

Main category: cs.CL

TL;DR: This paper explores sentiment analysis in local Indonesian languages using pre-trained language models, revealing performance differences based on language exposure during pre-training.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the challenges of applying sentiment analysis to low-resource Indonesian local languages.

Method: Zero-shot evaluations and adapter-based transfer approaches (MAD-X) are applied to ten local languages and grouped into 'seen,' 'partially seen,' and 'unseen' language categories.

Result: Multilingual models like mBERT and XLM-R perform variably based on prior language exposure, and MAD-X notably improves performance for seen and partially seen languages.

Conclusion: Performance in sentiment analysis correlates most with model exposure during pre-training. Tokenization factors have a weaker correlation.

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [36] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Main category: cs.CL

TL;DR: This paper introduces AdamMeme, an adaptable agent-based framework to evaluate multimodal Large Language Models (mLLMs) in understanding harmful memes, addressing the limitations of static dataset benchmarks with iterative updates.


<details>
  <summary>Details</summary>
Motivation: The shift of memes to dynamic multimodal forms on social media demands better evaluation methods for mLLMs in judging meme harmfulness, which static benchmarks fail to address.

Method: The authors developed AdamMeme, a collaborative multi-agent framework that iteratively probes mLLMs with evolving meme datasets to uncover model-specific deficiencies in understanding harmfulness.

Result: Extensive experiments show AdamMeme is effective in exposing varying performance and weaknesses across different mLLMs, enabling more thorough evaluations.

Conclusion: AdamMeme provides a flexible and adaptive solution for assessing mLLMs, surpassing the static methods currently in use, and can systematically identify areas for model improvement in meme harmfulness handling.

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [37] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
*Aditya Tomar,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: StereoBias paper introduces a dataset labeled for joint bias and stereotype detection tasks, improving bias detection performance across various categories including religion, gender, and race.


<details>
  <summary>Details</summary>
Motivation: Bias and stereotypes in language models can lead to harmful impacts and inefficiencies in applications like moderation and decision-making.

Method: The paper proposes joint learning of bias and stereotype detection using a newly curated dataset, StereoBias, across five key categories, comparing encoder-only models and fine-tuned decoder-only models.

Result: Joint training substantially enhances bias detection accuracy and reveals connections between bias and stereotypes, beyond benefits from multi-task learning alone.

Conclusion: Leveraging stereotype data can help develop fairer and effective AI systems, showcasing the importance of integrating bias and stereotype relationships into training models.

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [38] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
*Oliver Wardas,Florian Matthes*

Main category: cs.CL

TL;DR: The paper explores Large Language Models (LLMs) for evaluating clause legality in German employment contracts, highlighting moderate improvements with full-text legal sources and significant gains using distilled guidelines, but emphasizing LLMs still perform below human lawyers.


<details>
  <summary>Details</summary>
Motivation: Legal work demands both interpretability and trustworthiness, which data-driven NLP approaches often lack, posing challenges in dynamic legal contexts.

Method: They extended a dataset, collaborated with legal experts, and tested LLMs under three legal contexts: no context, full-text legal sources, and distilled examination guidelines.

Result: Using distilled guidelines enhanced recall for void clauses and boosted weighted F1-score to 80%. Full-text sources also improved performance, though less markedly than distilled guidelines.

Conclusion: LLMs show promise in supporting legal reviews but remain inferior to human lawyers, especially in performance using full-text legal references.

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [39] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
*Matteo Di Cristofaro*

Main category: cs.CL

TL;DR: Tokenisation impacts corpus linguistic data accuracy, especially with emojis and homoglyphs, requiring targeted preprocessing methods.


<details>
  <summary>Details</summary>
Motivation: To address reliability issues arising from tokenisation discrepancies in corpus linguistics, particularly dealing with emojis and homoglyphs.

Method: Examining how tokenisation discrepancies influence data representation and proposing preprocessing methods to maintain corpus fidelity.

Result: Highlighted how preprocessing emojis and homoglyphs ensures accurate representation of digital texts in corpora.

Conclusion: Accurate tokenisation and preprocessing are essential for reliable corpus-based linguistic analysis, impacting both quantitative and qualitative approaches.

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [40] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: MuRating transfers English data-quality evaluation to 17 target languages, enhancing multilingual large language model performance.


<details>
  <summary>Details</summary>
Motivation: Existing selection methods for data quality concentrate on English, overlooking a scalable approach for multilingual data.

Method: MuRating employs English document-quality scores, translates them for multilingual evaluations, and balances datasets for pretraining a LLaMA model.

Result: Increased accuracy across English and multilingual benchmarks, especially in knowledge-intensive tasks.

Conclusion: MuRating improves data selection for pretraining multilingual models and highlights areas for research such as biases and narrative material.

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [41] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix HofstÃ¤tter*

Main category: cs.CL

TL;DR: The paper investigates how language models, specifically Llama-3.3-70B-Instruct, differentiate between evaluation and deployment phases, highlighting implications for AI safety and governance.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand if language models like Llama-3.3-70B-Instruct can distinguish evaluation prompts from deployment prompts, which may have serious consequences for AI safety evaluations and governance reliability.

Method: The authors employ linear probes to analyze how Llama-3.3-70B-Instruct internally represents prompts from evaluation and deployment phases.

Result: The study finds that the model can internally distinguish evaluation prompts, with safety evaluations being classified as artificial by the probes.

Conclusion: The findings emphasize the need for robust evaluation mechanisms and deeper understanding of deceptive capabilities in models. They also suggest leveraging internal model representations for safety audits in future, more advanced models.

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [42] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: The paper investigates how vision-language models handle conflicting information between modalities (e.g., images vs. captions) and identifies key factors influencing their responses.


<details>
  <summary>Details</summary>
Motivation: To understand and control the behavior of multimodal AI models when input streams, such as visual and textual data, conflict with each other.

Method: Providing inconsistent multimodal inputs (e.g., mismatched captions and images) to vision-language models and analyzing their preference for one modality over the other, while examining internal mechanisms such as attention heads and representational structures.

Result: Models show a preference for certain modalities, which varies across models. Specific attention heads and 'router heads' influence modality preference and can be manipulated to favor accurate responses according to the given instructions.

Conclusion: This work lays the groundwork for better controlling and understanding the mechanisms of multimodal AI models, particularly in resolving conflicting multimodal signals effectively.

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [43] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan RÃ¼ping*

Main category: cs.CL

TL;DR: The paper analyzes the MDACE dataset and evaluates explainable medical coding systems, focusing on transparency and plausibility in evidence extraction.


<details>
  <summary>Details</summary>
Motivation: To improve transparency and efficiency in automatic medical coding while addressing the scarcity of annotated datasets for evaluation.

Method: Conducted an in-depth analysis of the MDACE dataset and evaluated the plausibility of current explainable medical coding systems through match measures.

Result: Ground truth evidence showed partial alignment with code descriptions, and current systems had significant overlap with this evidence; success and failure cases were identified.

Conclusion: The study provides valuable recommendations for enhancing explainable medical coding systems and further understanding automatic coding processes.

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [44] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.CL

TL;DR: Small language models were tested for parsing structured outputs in clinical notes using JSON, YAML, and XML formats, with JSON proving consistently more reliable.


<details>
  <summary>Details</summary>
Motivation: The paper aims to determine the most robust serialization format for structured outputs from language models in clinical settings.

Method: The study evaluates three serialization formats (JSON, YAML, XML) across various conditions using small language models and analyzes structural robustness under different scenarios.

Result: JSON emerged as the most parseable format. Parseability improved with better prompts and larger models, but certain note types and longer documents negatively impacted robustness.

Conclusion: This research provides practical suggestions for selecting serialization formats and designing prompts for language models in clinical contexts where privacy is critical.

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [45] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: This paper analyzes how Large Language Models (LLMs) utilize their training data by focusing on sequences of text that are highly probable according to the model.


<details>
  <summary>Details</summary>
Motivation: The work seeks to address transparency, accountability, privacy, and fairness in LLMs by understanding the relationship between their training data and generated outputs.

Method: Their approach extracts high-probability text spans (low-perplexity sequences) from LLM-generated outputs, traces these sequences back to their training sources, and analyzes the distribution and nature of these matches.

Result: It reveals that many high-probability spans cannot be traced back to the training corpus, while others involve verbatim recall from varying source distributions.

Conclusion: The study provides insights into how training data affects LLM behavior and offers a framework for analyzing training-data attribution in generated content.

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [46] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
*Samridhi Raj Sinha,Rajvee Sheth,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: The paper introduces EKA-EVAL, an evaluation framework for Large Language Models (LLMs) that includes over 35 benchmarks, with a focus on supporting Indic languages.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation tools for Large Language Models largely focus on English-centric benchmarks and lack inclusivity for diverse linguistic regions, such as India.

Method: EKA-EVAL integrates over 35 benchmarks, including 10 Indic-specific datasets, and supports distributed inference, quantization, and multi-GPU use for broad, multilingual model evaluation.

Result: The framework establishes itself as the first extensible, end-to-end solution for benchmarking both global and Indic-specific LLMs, facilitating multilingual benchmarking.

Conclusion: EKA-EVAL significantly advances the evaluation ecosystem for LLMs by providing a unified, production-ready, and extensible solution, catering to global and multilingual needs.

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [47] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.CL

TL;DR: The paper introduces DIY-MKG, an open-source system for personalized polyglot language learning, emphasizing knowledge graphs, adaptive quizzes, and LLMs' dynamic support.


<details>
  <summary>Details</summary>
Motivation: Language learning tools lack support for multilingual connections, customization for learners' individual needs, and often lead to cognitive offloading.

Method: The system helps users create their own vocabulary knowledge graphs expanded by LLM suggestions, enables rich annotations, and uses LLMs to generate personalized adaptive quizzes. Incorrect quizzes can be flagged, creating a feedback loop.

Result: DIY-MKG demonstrates reliable and fair vocabulary expansion across languages and highly accurate quiz generation, showcasing the system's robustness.

Conclusion: DIY-MKG offers effective, customizable, and interactive support for polyglot learners, improving engagement and addressing previous language tool gaps.

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [48] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
*Dongyi Ding,Tiannan Wang,Chenghao Zhu,Meiling Tao,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: This paper proposes MiCoTA, a teaching framework that uses intermediate models to improve small language models (SLMs) in reasoning tasks by bridging capacity and reasoning gaps.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty small language models face in learning complex, long-form chain-of-thought (CoT) reasoning due to their limited computational capacity.

Method: The MiCoTA framework introduces intermediate-sized teacher models and employs intermediate-length reasoning examples to better align training data with the capacity of small models.

Result: Experiments show notable improvements in SLM reasoning capabilities, with Qwen2.5-based models achieving significant benchmark performance boosts on tasks related to reasoning.

Conclusion: MiCoTA effectively enhances reasoning for SLMs, paving the way for future advancements in making high-level reasoning tasks computationally feasible on smaller models.

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [49] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: The paper introduces a novel structured pruning method for language models, focusing on strategic pruning in higher layers and adaptive rescaling to improve performance.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for language models are heuristic and donât consider the architectural positioning of attention heads, leading to inefficiencies.

Method: Proposes a strategy to prune attention heads in higher layers and introduces an adaptive rescaling parameter to adjust token representation scales post-pruning.

Result: Extensive testing on multiple LLMs and datasets showed better performance of the proposed method, with significant improvement in generation tasks compared to conventional methods.

Conclusion: The proposed approach is more effective and efficient for structured pruning in LLMs, especially benefiting generation tasks.

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [50] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: The paper surveys advancements in AI technologies for scientific research, proposing a taxonomy, identifying gaps, and providing resources for enhanced development in AI-powered research.


<details>
  <summary>Details</summary>
Motivation: AI and large language models show promising capabilities in scientific research, but a comprehensive survey on their innovative applications is lacking.

Method: The paper conducts a systematic survey presenting a taxonomy to classify AI4Research tasks, identifies research gaps, and compiles applications and resources to guide future work.

Result: The survey establishes a taxonomy of five tasks in AI4Research, identifies research directions like scalability and societal impact, and provides multidisciplinary resources and tools.

Conclusion: This study aims to aid the research community by bridging gaps in AI4Research and stimulating innovation by offering an organized framework and valuable resources.

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [51] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: This paper introduces GAPO, a multi-gradient optimization approach for aligning large language models (LLMs) with diverse human preferences, overcoming conflicts in objectives.


<details>
  <summary>Details</summary>
Motivation: Aligning large language models (LLMs) with conflicting human preferences is a critical challenge in reinforcement learning from human feedback (RLHF).

Method: The paper proposes Gradient-Adaptive Policy Optimization (GAPO), which uses multiple-gradient descent and adaptive gradient rescaling to balance trade-offs for conflicting objectives. Additionally, P-GAPO incorporates user-specific preferences to achieve tailored Pareto solutions.

Result: GAPO achieves superior performance in aligning LLMs with human values, surpassing existing state-of-the-art methods on Mistral-7B and showing improved helpfulness and harmlessness metrics.

Conclusion: GAPO provides an effective framework for balancing diverse human preferences in LLM alignment, demonstrating theoretical convergence to Pareto optimal solutions and empirical superiority over existing methods.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [52] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
*Yang Li,Youssef Emad,Karthik Padthe,Jack Lanchantin,Weizhe Yuan,Thao Nguyen,Jason Weston,Shang-Wen Li,Dong Wang,Ilia Kulikov,Xian Li*

Main category: cs.CL

TL;DR: The study explores effective methods for distilling reasoning capabilities from a teacher model to a student model, finding that targeted selection of reasoning examples enhances performance.


<details>
  <summary>Details</summary>
Motivation: To improve smaller student models' reasoning abilities by examining the optimal type of reasoning demonstrations from a stronger teacher model.

Method: High-quality reasoning traces, termed 'NaturalThoughts,' were curated from a teacher model using selective sampling based on difficult and diverse reasoning examples, then evaluated on reasoning benchmarks.

Result: Training with curated NaturalThoughts demonstrated improved performance over existing datasets on STEM reasoning benchmarks, for both Llama and Qwen models.

Conclusion: Selective sampling of challenging, diverse reasoning traces is critical for efficiently transferring reasoning capabilities to student models, outperforming random sampling and existing datasets.

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [53] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
*Yu-Shiang Huang,Chuan-Ju Wang,Chung-Chi Chen*

Main category: cs.CL

TL;DR: This paper evaluates natural language generation (NLG) using its impact on decision-making by humans and LLMs, focusing on financial market contexts.


<details>
  <summary>Details</summary>
Motivation: The paper seeks to address the weak correlation between intrinsic NLG evaluation metrics and real-world decision-making efficacy. Current metrics like n-gram overlap do not capture how generated text influences practical decision outcomes.

Method: A decision-oriented evaluation framework was proposed, testing financial market digest texts on humans and LLMs. Performance was gauged based on the financial outcomes of trades informed exclusively by the tested texts.

Result: The study found that neither humans nor LLMs achieved better-than-random performance when using summaries alone. Team-based collaboration with richer commentaries, however, significantly improved performance over individual baselines.

Conclusion: The results highlight the need for evaluating generated text based on its ability to enhance collaborative human-LLM decision-making, contrasting with the inadequacy of traditional metrics.

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


### [54] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Main category: cs.CL

TL;DR: This study compares two ASR models, Whisper and Wav2Vec-BERT, for Bangla, finding Wav2Vec-BERT performs better.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in automatic speech recognition for low-resource languages, using Bangla as a case study.

Method: Two public datasets and advanced fine-tuning with hyperparameter optimization were used to evaluate the models based on WER, CER, training time, and computational efficiency.

Result: Wav2Vec-BERT performed better than Whisper across all evaluation metrics, requiring fewer computational resources.

Conclusion: The study provides insights into developing efficient speech recognition systems for low-resource languages, highlighting the advantages of Wav2Vec-BERT.

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [55] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
*Adrian de Wynter,Tangming Yuan*

Main category: cs.CL

TL;DR: The paper investigates the ability of large language models (LLMs) to engage in debates and understand dialogue structures, revealing their strengths in persuasion but weaknesses in comprehending deeper dialogue contexts.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the disparate accounts of LLMs' reasoning capabilities and explore their effectiveness and limitations in dialogue scenarios, especially in sensitive applications.

Method: The study evaluates LLMs' ability to conduct debates and understand dialogical structures and pragmatic context, analyzing their coherence and capability in changing beliefs.

Result: LLMs successfully maintain coherent and persuasive debates that can influence participants. However, they fail to exhibit an understanding of deeper dialogical structures or pragmatic contexts.

Conclusion: The paper concludes that maintaining an effective dialogue does not require comprehension of its meaning, which questions the role of pragmatic context and coherence in argumentation theory for LLMs.

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [56] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

TL;DR: The paper introduces a 4D video generation model that ensures geometric and temporal consistency by applying cross-view pointmap alignment during training, aiding in robot vision and planning.


<details>
  <summary>Details</summary>
Motivation: Robots struggle with effectively planning and interacting in complex environments due to challenges in accurately modeling dynamic physical scenes.

Method: The proposed method uses a 4D video generation model supervised with cross-view pointmap alignment to enforce 3D consistency, requiring only RGB-D observations and no camera pose inputs.

Result: The method achieves visually stable and spatially aligned video predictions across multiple datasets and supports robot end-effector trajectory reconstruction with a 6DoF pose tracker.

Conclusion: Using 4D video predictions enriched with geometric consistency significantly benefits robotic manipulation and generalization for novel camera viewpoints.

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [57] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Main category: cs.CV

TL;DR: The study integrates multi-source satellite imagery and deep learning to enhance landslide detection and prediction.


<details>
  <summary>Details</summary>
Motivation: Landslides pose serious risks to infrastructure, economies, and lives, requiring improved detection and predictive methods.

Method: Multi-source data (Sentinel-2, ALOS PALSAR) and deep learning models (U-Net, DeepLabV3+, Res-Net) were used along with geospatial analysis to identify landslides.

Result: Deep learning models and multi-source remote sensing demonstrated strong potential in landslide prediction and detection.

Conclusion: The framework supports early warning systems, disaster management, and land-use planning, offering scalable and reliable landslide prediction.

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [58] [cp_measure: API-first feature extraction for image-based profiling workflows](https://arxiv.org/abs/2507.01163)
*AlÃ¡n F. MuÃ±oz,Tim Treis,Alexandr A. Kalinin,Shatavisha Dasgupta,Fabian Theis,Anne E. Carpenter,Shantanu Singh*

Main category: cs.CV

TL;DR: The paper introduces cp_measure, a Python library for streamlined and automated image-based profiling, addressing limitations of traditional tools like CellProfiler while maintaining high feature fidelity.


<details>
  <summary>Details</summary>
Motivation: Biological image analysis requires tools that support comprehensive, automated, and reproducible image-based profiling pipelines suitable for machine learning workflows, as current tools like CellProfiler have limitations.

Method: They developed cp_measure as a modular, API-first Python library that retains the core features of CellProfiler but integrates seamlessly with the scientific Python ecosystem for programmatic feature extraction.

Result: cp_measure successfully retained high fidelity with CellProfiler features and showcased applications in 3D astrocyte imaging and spatial transcriptomics.

Conclusion: cp_measure proves to be a reproducible, automated, and scalable tool, facilitating image-based profiling for computational biology and machine learning advancements.

Abstract: Biological image analysis has traditionally focused on measuring specific
visual properties of interest for cells or other entities. A complementary
paradigm gaining increasing traction is image-based profiling - quantifying
many distinct visual features to form comprehensive profiles which may reveal
hidden patterns in cellular states, drug responses, and disease mechanisms.
While current tools like CellProfiler can generate these feature sets, they
pose significant barriers to automated and reproducible analyses, hindering
machine learning workflows. Here we introduce cp_measure, a Python library that
extracts CellProfiler's core measurement capabilities into a modular, API-first
tool designed for programmatic feature extraction. We demonstrate that
cp_measure features retain high fidelity with CellProfiler features while
enabling seamless integration with the scientific Python ecosystem. Through
applications to 3D astrocyte imaging and spatial transcriptomics, we showcase
how cp_measure enables reproducible, automated image-based profiling pipelines
that scale effectively for machine learning applications in computational
biology.

</details>


### [59] [Rapid Salient Object Detection with Difference Convolutional Neural Networks](https://arxiv.org/abs/2507.01182)
*Zhuo Su,Li Liu,Matthias MÃ¼ller,Jiehua Zhang,Diana Wofk,Ming-Ming Cheng,Matti PietikÃ¤inen*

Main category: cs.CV

TL;DR: The paper proposes efficient neural networks for real-time salient object detection (SOD) on resource-constrained devices using Pixel Difference Convolutions (PDCs) and SpatioTemporal Difference Convolutions (STDC).


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the computational inefficiency of existing top-performing SOD models, making them viable for resource-constrained devices while maintaining real-time performance.

Method: The authors design an efficient model integrating Pixel Difference Convolutions (PDCs) for contrast encoding and Difference Convolution Reparameterization (DCR) for reducing computation at inference. They further develop SpatioTemporal Difference Convolutions (STDC) to enhance 3D convolutions for video SOD.

Result: The proposed models, SDNet for image SOD and STDNet for video SOD, achieve significant improvements in efficiency and accuracy trade-offs, running at 46 FPS and 150 FPS respectively, with less than 1M parameters, outperforming lightweight alternatives by margins of 2Ã to 3Ã in speed.

Conclusion: This research demonstrates that leveraging biologically-inspired saliency detection techniques combined with CNNs can result in models that are both computationally efficient and accurate for real-time deployment on resource-constrained devices.

Abstract: This paper addresses the challenge of deploying salient object detection
(SOD) on resource-constrained devices with real-time performance. While recent
advances in deep neural networks have improved SOD, existing top-leading models
are computationally expensive. We propose an efficient network design that
combines traditional wisdom on SOD and the representation power of modern CNNs.
Like biologically-inspired classical SOD methods relying on computing contrast
cues to determine saliency of image regions, our model leverages Pixel
Difference Convolutions (PDCs) to encode the feature contrasts. Differently,
PDCs are incorporated in a CNN architecture so that the valuable contrast cues
are extracted from rich feature maps. For efficiency, we introduce a difference
convolution reparameterization (DCR) strategy that embeds PDCs into standard
convolutions, eliminating computation and parameters at inference.
Additionally, we introduce SpatioTemporal Difference Convolution (STDC) for
video SOD, enhancing the standard 3D convolution with spatiotemporal contrast
capture. Our models, SDNet for image SOD and STDNet for video SOD, achieve
significant improvements in efficiency-accuracy trade-offs. On a Jetson Orin
device, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on
streamed images and videos, surpassing the second-best lightweight models in
our experiments by more than $2\times$ and $3\times$ in speed with superior
accuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.

</details>


### [60] [Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using HÃ¶lder Divergence and Mutual Information-Enhanced Knowledge Transfer](https://arxiv.org/abs/2507.01254)
*Runze Cheng,Xihang Qiu,Ming Li,Ye Zhang,Chun Li,Fei Yu*

Main category: cs.CV

TL;DR: The paper introduces a framework for accurate brain tumor segmentation using single-modality MRI, addressing the challenge of missing modalities.


<details>
  <summary>Details</summary>
Motivation: Existing methods for brain tumor segmentation face difficulties when some MRI modalities are unavailable due to various constraints.

Method: A single-modality parallel processing framework utilizes Holder divergence and mutual information to maintain modality-specific features and adjust parameters dynamically based on available inputs.

Result: The framework achieves consistently accurate segmentation and outperforms existing methods when modalities are missing, validated on BraTS 2018 and BraTS 2020 datasets.

Conclusion: The proposed framework effectively addresses the challenge of incomplete modalities, providing a robust solution for brain tumor segmentation.

Abstract: Multimodal MRI provides critical complementary information for accurate brain
tumor segmentation. However, conventional methods struggle when certain
modalities are missing due to issues such as image quality, protocol
inconsistencies, patient allergies, or financial constraints. To address this,
we propose a robust single-modality parallel processing framework that achieves
high segmentation accuracy even with incomplete modalities. Leveraging Holder
divergence and mutual information, our model maintains modality-specific
features while dynamically adjusting network parameters based on the available
inputs. By using these divergence- and information-based loss functions, the
framework effectively quantifies discrepancies between predictions and
ground-truth labels, resulting in consistently accurate segmentation. Extensive
evaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior
performance over existing methods in handling missing modalities.

</details>


### [61] [AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation](https://arxiv.org/abs/2507.01255)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: This paper introduces AIGVE-MACS, a model that evaluates AI-generated videos with numerical scores and multi-aspect language feedback using a new benchmark, AIGVE-BENCH 2, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing evaluation metrics for AI-generated videos lack interpretability and alignment with human assessments, necessitating a solution for comprehensive evaluation.

Method: AIGVE-MACS uses Vision-Language Models with token-wise weighted loss and dynamic frame sampling, trained on the AIGVE-BENCH 2 containing 2,500 videos and 22,500 human annotations.

Result: AIGVE-MACS delivers state-of-the-art performance in scoring correlation and feedback quality, surpassing methods like GPT-4o, and improves video generation quality by 53.5% using a multi-agent refinement framework.

Conclusion: The proposed AIGVE-MACS model and AIGVE-BENCH 2 set a new standard for detailed and human-aligned evaluation of AI-generated videos, promoting iterative improvements in video production quality.

Abstract: The rapid advancement of AI-generated video models has created a pressing
need for robust and interpretable evaluation frameworks. Existing metrics are
limited to producing numerical scores without explanatory comments, resulting
in low interpretability and human evaluation alignment. To address those
challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video
Evaluation(AIGVE), which can provide not only numerical scores but also
multi-aspect language comment feedback in evaluating these generated videos.
Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising
2,500 AI-generated videos and 22,500 human-annotated detailed comments and
numerical scores across nine critical evaluation aspects. Leveraging
AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a
novel token-wise weighted loss and a dynamic frame sampling strategy to better
align with human evaluators. Comprehensive experiments across supervised and
zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art
performance in both scoring correlation and comment quality, significantly
outperforming prior baselines including GPT-4o and VideoScore. In addition, we
further showcase a multi-agent refinement framework where feedback from
AIGVE-MACS drives iterative improvements in video generation, leading to 53.5%
quality enhancement. This work establishes a new paradigm for comprehensive,
human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2
and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.

</details>


### [62] [Advancements in Weed Mapping: A Systematic Review](https://arxiv.org/abs/2507.01269)
*Mohammad Jahanbakht,Alex Olsen,Ross Marchant,Emilie Fillols,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: The paper reviews state-of-the-art technologies and methods for weed mapping, including sensors, processing, and mapping techniques, to improve management efficacy and sustainability.


<details>
  <summary>Details</summary>
Motivation: Weed mapping is essential for targeted weed control, minimizing environmental impacts, and enhancing sustainable land management, but existing research lacks comprehensive reviews across mapping pipelines.

Method: The authors performed a structured review based on PRISMA guidelines, systematically analyzing literature on sensor and platform technologies, data processing, and mapping techniques.

Result: The paper synthesizes key findings in weed mapping methods and technologies, offering a holistic overview of advances and challenges in the entire mapping workflow.

Conclusion: This review provides foundational insights to guide future research and development of scalable and sustainable weed mapping and management systems.

Abstract: Weed mapping plays a critical role in precision management by providing
accurate and timely data on weed distribution, enabling targeted control and
reduced herbicide use. This minimizes environmental impacts, supports
sustainable land management, and improves outcomes across agricultural and
natural environments. Recent advances in weed mapping leverage ground-vehicle
Red Green Blue (RGB) cameras, satellite and drone-based remote sensing combined
with sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The
resulting data are processed using advanced techniques including big data
analytics and machine learning, significantly improving the spatial and
temporal resolution of weed maps and enabling site-specific management
decisions. Despite a growing body of research in this domain, there is a lack
of comprehensive literature reviews specifically focused on weed mapping. In
particular, the absence of a structured analysis spanning the entire mapping
pipeline, from data acquisition to processing techniques and mapping tools,
limits progress in the field. This review addresses these gaps by
systematically examining state-of-the-art methods in data acquisition (sensor
and platform technologies), data processing (including annotation and
modelling), and mapping techniques (such as spatiotemporal analysis and
decision support tools). Following PRISMA guidelines, we critically evaluate
and synthesize key findings from the literature to provide a holistic
understanding of the weed mapping landscape. This review serves as a
foundational reference to guide future research and support the development of
efficient, scalable, and sustainable weed management systems.

</details>


### [63] [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/abs/2507.01275)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: The paper introduces a frequency domain-based diffusion model for unpaired image dehazing, leveraging amplitude and phase-specific techniques for improved results.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in unpaired image dehazing, such as the introduction of irrelevant content information and neglect of haze-relevant properties in the frequency domain.

Method: A novel diffusion model is proposed, incorporating an Amplitude Residual Encoder (ARE) for frequency-specific amplitude adjustments and a Phase Correction Module (PCM) for phase refinement.

Result: The proposed model outperforms state-of-the-art methods on both synthetic and real-world datasets, showcasing superior dehazing capabilities.

Conclusion: Integrating frequency domain reconstruction via diffusion models effectively enhances unpaired image dehazing by addressing amplitude and phase spectrum inconsistencies.

Abstract: Unpaired image dehazing has attracted increasing attention due to its
flexible data requirements during model training. Dominant methods based on
contrastive learning not only introduce haze-unrelated content information, but
also ignore haze-specific properties in the frequency domain (\ie,~haze-related
degradation is mainly manifested in the amplitude spectrum). To address these
issues, we propose a novel frequency domain-based diffusion model, named \ours,
for fully exploiting the beneficial knowledge in unpaired clear data. In
particular, inspired by the strong generative ability shown by Diffusion Models
(DMs), we tackle the dehazing task from the perspective of frequency domain
reconstruction and perform the DMs to yield the amplitude spectrum consistent
with the distribution of clear images. To implement it, we propose an Amplitude
Residual Encoder (ARE) to extract the amplitude residuals, which effectively
compensates for the amplitude gap from the hazy to clear domains, as well as
provide supervision for the DMs training. In addition, we propose a Phase
Correction Module (PCM) to eliminate artifacts by further refining the phase
spectrum during dehazing with a simple attention mechanism. Experimental
results demonstrate that our \ours outperforms other state-of-the-art methods
on both synthetic and real-world datasets.

</details>


### [64] [Learning an Ensemble Token from Task-driven Priors in Facial Analysis](https://arxiv.org/abs/2507.01290)
*Sunyong Seo,Semin Kim,Jongha Lee*

Main category: cs.CV

TL;DR: The paper proposes ET-Fuser, a method for facial analysis that uses token ensemble learning based on task priors from pre-trained models for better feature representation.


<details>
  <summary>Details</summary>
Motivation: There is a lack of research focusing on unified feature representation during single-task facial analysis learning, despite advancements in spatial and semantic interpretability.

Method: ET-Fuser generates ensemble tokens via a self-attention mechanism, leveraging mutual information across pre-trained encoders with minimal computational cost.

Result: The methodology improves facial analysis tasks and achieves statistically significant enhancements in feature representation.

Conclusion: ET-Fuser offers efficient unified feature learning, advancing facial analysis through its robust and computationally-light approach.

Abstract: Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the training process. In
this work, we introduce ET-Fuser, a novel methodology for learning ensemble
token by leveraging attention mechanisms based on task priors derived from
pre-trained models for facial analysis. Specifically, we propose a robust prior
unification learning method that generates a ensemble token within a
self-attention mechanism, which shares the mutual information along the
pre-trained encoders. This ensemble token approach offers high efficiency with
negligible computational cost. Our results show improvements across a variety
of facial analysis, with statistically significant enhancements observed in the
feature representations.

</details>


### [65] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: The paper presents DiffusionLight, a novel method for estimating lighting from a single LDR image, reframing the task as a chrome ball inpainting problem and employing the Stable Diffusion XL model.


<details>
  <summary>Details</summary>
Motivation: Existing methods for lighting estimation struggle with generalization due to reliance on limited HDR panorama datasets, necessitating a robust approach that can adapt to diverse settings.

Method: The approach involves iterative inpainting to generate a median chrome ball for stable lighting priors (DiffusionLight). To speed up the process, Turbo LoRA is utilized (DiffusionLight-Turbo) to predict averaged chrome balls in a single denoising pass.

Result: The method achieves convincing light estimates across diverse scenarios with a significant speed improvement: from approximately 30 minutes to 30 seconds per estimation with minimal quality degradation.

Conclusion: DiffusionLight and its accelerated version (Turbo) offer a robust and practical solution to lighting estimation with superior generalization capabilities and significant runtime efficiency.

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [66] [Physics-informed Ground Reaction Dynamics from Human Motion Capture](https://arxiv.org/abs/2507.01340)
*Cuong Le,Huy-Phuong Le,Duc Le,Minh-Thien Duong,Van-Binh Nguyen,My-Ha Le*

Main category: cs.CV

TL;DR: The paper presents a method to estimate human ground reaction dynamics directly from motion capture data using physics constraints, eliminating the need for force plates.


<details>
  <summary>Details</summary>
Motivation: Force plates, commonly used for collecting human ground reaction dynamics, are limited to lab setups, restricting the study of human motion dynamics.

Method: The authors employ physics laws and computational simulations with Euler's integration scheme and PD algorithm to estimate ground reaction forces from motion capture data.

Result: The approach outperforms baseline models in estimating ground reaction force accuracy and simulated root trajectory precision on the GroundLink dataset.

Conclusion: By leveraging physics-informed simulations, the method provides a robust and accurate alternative to force plates for studying human motion dynamics, expanding possibilities beyond laboratory setups.

Abstract: Body dynamics are crucial information for the analysis of human motions in
important research fields, ranging from biomechanics, sports science to
computer vision and graphics. Modern approaches collect the body dynamics,
external reactive force specifically, via force plates, synchronizing with
human motion capture data, and learn to estimate the dynamics from a black-box
deep learning model. Being specialized devices, force plates can only be
installed in laboratory setups, imposing a significant limitation on the
learning of human dynamics. To this end, we propose a novel method for
estimating human ground reaction dynamics directly from the more reliable
motion capture data with physics laws and computational simulation as
constrains. We introduce a highly accurate and robust method for computing
ground reaction forces from motion capture data using Euler's integration
scheme and PD algorithm. The physics-based reactive forces are used to inform
the learning model about the physics-informed motion dynamics thus improving
the estimation accuracy. The proposed approach was tested on the GroundLink
dataset, outperforming the baseline model on: 1) the ground reaction force
estimation accuracy compared to the force plates measurement; and 2) our
simulated root trajectory precision. The implementation code is available at
https://github.com/cuongle1206/Phys-GRD

</details>


### [67] [Learning Camera-Agnostic White-Balance Preferences](https://arxiv.org/abs/2507.01342)
*Luxi Zhao,Mahmoud Afifi,Michael S. Brown*

Main category: cs.CV

TL;DR: This paper introduces a lightweight post-illuminant-estimation mapping for consistent and aesthetic white balance corrections across cameras, achieving state-of-the-art results with minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: Modern image signal processors often focus on aesthetic white-balance preferences rather than accurate correction, resulting in challenges for devices like smartphones with multiple cameras.

Method: The authors propose a camera-agnostic post-illuminant-estimation mapping that works after an initial neutral AWB module, ensuring aesthetic and stylized color rendering consistently across various camera systems.

Result: The proposed model achieves state-of-the-art performance on a dataset of 771 images across three smartphone cameras, requiring only ~500 parameters and negligible computational resources (0.024 ms on flagship CPUs).

Conclusion: This novel approach advances cross-camera auto white balance techniques by enabling aesthetic consistency while being lightweight and compatible with existing AWB systems.

Abstract: The image signal processor (ISP) pipeline in modern cameras consists of
several modules that transform raw sensor data into visually pleasing images in
a display color space. Among these, the auto white balance (AWB) module is
essential for compensating for scene illumination. However, commercial AWB
systems often strive to compute aesthetic white-balance preferences rather than
accurate neutral color correction. While learning-based methods have improved
AWB accuracy, they typically struggle to generalize across different camera
sensors -- an issue for smartphones with multiple cameras. Recent work has
explored cross-camera AWB, but most methods remain focused on achieving neutral
white balance. In contrast, this paper is the first to address aesthetic
consistency by learning a post-illuminant-estimation mapping that transforms
neutral illuminant corrections into aesthetically preferred corrections in a
camera-agnostic space. Once trained, our mapping can be applied after any
neutral AWB module to enable consistent and stylized color rendering across
unseen cameras. Our proposed model is lightweight -- containing only $\sim$500
parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile
CPU. Evaluated on a dataset of 771 smartphone images from three different
cameras, our method achieves state-of-the-art performance while remaining fully
compatible with existing cross-camera AWB techniques, introducing minimal
computational and memory overhead.

</details>


### [68] [Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation](https://arxiv.org/abs/2507.01347)
*Andrei Jelea,Ahmed Nabil Belbachir,Marius Leordeanu*

Main category: cs.CV

TL;DR: The paper introduces Generalized Test-Time Augmentation (GTTA), a new method enhancing model performance across various tasks, using a PCA-based data transformation and self-supervised ensemble training.


<details>
  <summary>Details</summary>
Motivation: Existing Test-Time Augmentation (TTA) methods are often task-specific and computationally intensive. The authors sought to create a universal TTA approach that can be used across diverse tasks, with reduced computational requirements at test time.

Method: GTTA employs a PCA-based random perturbation transformation to create robust ensembles at test time. Additionally, it includes a self-supervised learning phase where ensemble outputs are used to train the original model, thereby eliminating the need for runtime ensembles.

Result: GTTA showed superior performance compared to traditional TTA and state-of-the-art models on various datasets and tasks (e.g., classification, segmentation, and object detection). It also proved effective in a unique real-world example, salmon segmentation in underwater videos.

Conclusion: GTTA is a generalizable, efficient method that not only improves model performance across various tasks but also reduces the computational burden during inference, validating its utility in both standard and unique real-world scenarios.

Abstract: We introduce Generalized Test-Time Augmentation (GTTA), a highly effective
method for improving the performance of a trained model, which unlike other
existing Test-Time Augmentation approaches from the literature is general
enough to be used off-the-shelf for many vision and non-vision tasks, such as
classification, regression, image segmentation and object detection. By
applying a new general data transformation, that randomly perturbs multiple
times the PCA subspace projection of a test input, GTTA forms robust ensembles
at test time in which, due to sound statistical properties, the structural and
systematic noises in the initial input data is filtered out and final estimator
errors are reduced. Different from other existing methods, we also propose a
final self-supervised learning stage in which the ensemble output, acting as an
unsupervised teacher, is used to train the initial single student model, thus
reducing significantly the test time computational cost, at no loss in
accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on
various vision and non-vision well-known datasets and tasks, such as image
classification and segmentation, speech recognition and house price prediction,
validate the generality of the proposed GTTA. Furthermore, we also prove its
effectiveness on the more specific real-world task of salmon segmentation and
detection in low-visibility underwater videos, for which we introduce
DeepSalmon, the largest dataset of its kind in the literature.

</details>


### [69] [Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model](https://arxiv.org/abs/2507.01351)
*Chaoxiang Cai,Longrong Yang,Kaibing Chen,Fan Yang,Xi Li*

Main category: cs.CV

TL;DR: The paper presents a method called Long-Tailed Distribution-aware Router (LTDR) to optimize vision-language mixture-of-expert frameworks by addressing distribution discrepancies and enhancing expert activation for vision tail tokens.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve existing mixture-of-experts frameworks for large vision-language models that neglect distributional differences between vision and language data, particularly in token routing.

Method: The proposed method involves two key strategies: (1) using a distribution-aware router tailored specifically for uniform language and long-tailed vision token distributions, and (2) using an oversampling-like strategy to enhance the activation of experts for vision tail tokens.

Result: Experimental results on various benchmarks demonstrate the improved effectiveness of the LTDR approach compared to existing methods.

Conclusion: LTDR provides a pathway to more effective routing and expert activation in vision-language mixture-of-experts models by addressing their distributional and activation challenges.

Abstract: The mixture-of-experts (MoE), which replaces dense models with sparse
architectures, has gained attention in large vision-language models (LVLMs) for
achieving comparable performance with fewer activated parameters. Existing MoE
frameworks for LVLMs focus on token-to-expert routing (TER), encouraging
different experts to specialize in processing distinct tokens. However, these
frameworks often rely on the load balancing mechanism, overlooking the inherent
distributional differences between vision and language. To this end, we propose
a Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,
tackling two challenges: (1) Distribution-aware router for modality-specific
routing. We observe that language TER follows a uniform distribution, whereas
vision TER exhibits a long-tailed distribution. This discrepancy necessitates
distinct routing strategies tailored to each modality. (2) Enhancing expert
activation for vision tail tokens. Recognizing the importance of vision tail
tokens, we introduce an oversampling-like strategy by increasing the number of
activated experts for these tokens. Experiments on extensive benchmarks
validate the effectiveness of our approach.

</details>


### [70] [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/abs/2507.01367)
*Tianrui Lou,Xiaojun Jia,Siyuan Liang,Jiawei Liang,Ming Zhang,Yanjun Xiao,Xiaochun Cao*

Main category: cs.CV

TL;DR: The paper introduces a novel physical adversarial attack framework called PGA, leveraging 3D Gaussian Splatting for rapid, precise object reconstruction and robust adversarial camouflage capabilities under diverse viewpoints and environments.


<details>
  <summary>Details</summary>
Motivation: To address vulnerabilities in deep neural networks exposed by physical adversarial attacks, specifically improving the robustness and effectiveness of camouflage-based attacks under real-world, multi-view, and complex conditions.

Method: Utilizes 3D Gaussian Splatting for object reconstruction and photo-realistic rendering, combined with a min-max optimization strategy to enhance adversarial features while preventing occlusions across viewpoints.

Result: Experiments demonstrate PGA's effectiveness and robustness in creating adversarial camouflage, outperforming prior methods across diverse viewpoints and physical environments.

Conclusion: The PGA framework improves adversarial attack robustness and effectiveness without relying on extensive simulations, offering a practical advancement in safety-critical scenarios. Source code is publicly available for verification and application.

Abstract: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

</details>


### [71] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/abs/2507.01368)
*Tianning Chai,Chancharik Mitra,Brandon Huang,Gautam Rajendrakumar Gare,Zhiqiu Lin,Assaf Arbelle,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Deva Ramanan,Roei Herzig*

Main category: cs.CV

TL;DR: The paper introduces Activation Reward Models (Activation RMs) as a novel method to align generative models with human preferences using minimal supervision, outperforming previous approaches and addressing reward hacking issues.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve alignment of Large Language Models (LLMs) and Large Multimodal Models (LMMs) to human preferences in real-world applications, addressing the inflexibility and supervision requirements of traditional reward modeling methods.

Method: The authors propose Activation Reward Models (Activation RMs), which leverage activation steering for few-shot reward modeling without needing extensive preference datasets or additional finetuning.

Result: Experimental results show that Activation RMs outperform existing few-shot methods (e.g., LLM-as-a-judge, voting-based scoring) on standard benchmarks and effectively mitigate reward hacking. The authors also present a new benchmark, PreferenceHack, where Activation RMs achieve state-of-the-art performance.

Conclusion: Activation RMs offer an innovative and efficient way to encode human preferences and prevent reward hacking, making them particularly useful for safety-critical applications while setting new standards in alignment studies.

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [72] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/abs/2507.01372)
*Max Hamilton,Jinlin Lai,Wenlong Zhao,Subhransu Maji,Daniel Sheldon*

Main category: cs.CV

TL;DR: The paper introduces 'active measurement,' an AI framework with human involvement for improving scientific measurements, combining AI predictions, human labeling, and Monte Carlo estimations.


<details>
  <summary>Details</summary>
Motivation: Current scientific measurement workflows lack sufficient accuracy and statistical robustness and often require significant human effort.

Method: Develop an AI framework that integrates AI predictions with selective human labeling using importance sampling and employs Monte Carlo estimations to refine measurements iteratively.

Result: Active measurement showed reduced estimation errors and provided accurate measurements across several tasks, demonstrating its effectiveness compared to alternative methods.

Conclusion: The framework balances AI's efficiency with human precision, offering an effective solution for obtaining accurate scientific measurements with minimal human effort.

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [73] [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](https://arxiv.org/abs/2507.01384)
*Langyu Wang,Bingke Zhu,Yingying Chen,Yiyuan Zhang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: The paper introduces a method called MUG to improve weakly-supervised audio-visual video parsing, achieving superior results on the LLP dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods for audio-visual video parsing struggle to simultaneously improve both segment-level and event-level predictions due to weak supervision and architectural deficiencies.

Method: The paper proposes the MUG approach, which combines a novel audio-visual Mamba network and pseudo labeling augmentation. It leverages unimodal pseudo-labels and cross-modal data generation to enhance segment uniqueness and reduce noise.

Result: Experiments show significant improvements in performance on the LLP dataset, with gains in visual and audio segment-level metrics (+2.1% and +1.2%, respectively).

Conclusion: MUG effectively addresses weaknesses in weakly-supervised audio-visual video parsing by improving segment perception and noise reduction, offering state-of-the-art performance.

Abstract: The weakly-supervised audio-visual video parsing (AVVP) aims to predict all
modality-specific events and locate their temporal boundaries. Despite
significant progress, due to the limitations of the weakly-supervised and the
deficiencies of the model architecture, existing methods are lacking in
simultaneously improving both the segment-level prediction and the event-level
prediction. In this work, we propose a audio-visual Mamba network with pseudo
labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and
excluding the noise interference from the alternate modalities. Specifically,
we annotate some of the pseudo-labels based on previous work. Using unimodal
pseudo-labels, we perform cross-modal random combinations to generate new data,
which can enhance the model's ability to parse various segment-level event
combinations. For feature processing and interaction, we employ a audio-visual
mamba network. The AV-Mamba enhances the ability to perceive different segments
and excludes additional modal noise while sharing similar modal information.
Our extensive experiments demonstrate that MUG improves state-of-the-art
results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of
visual Segment-level and audio Segment-level metrics). Our code is available at
https://github.com/WangLY136/MUG.

</details>


### [74] [FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases](https://arxiv.org/abs/2507.01390)
*Shuai Tan,Bill Gong,Bin Ji,Ye Pan*

Main category: cs.CV

TL;DR: FixTalk is a novel framework for high-quality talking head generation that resolves identity leakage (IL) and rendering artifacts (RA) using Enhanced Motion Indicator (EMI) and Enhanced Detail Indicator (EDI).


<details>
  <summary>Details</summary>
Motivation: The paper was motivated by the challenges of identity leakage and rendering artifacts in current talking head generation methods, particularly in extreme cases, and the need for improved quality rendering.

Method: The authors introduced FixTalk, which includes two key components: Enhanced Motion Indicator (EMI) to decouple identity from motion features, and Enhanced Detail Indicator (EDI) to utilize leaked identity information for correcting artifacts.

Result: FixTalk effectively reduces identity leakage and rendering artifacts, achieving superior results compared to existing state-of-the-art methods in talking head generation.

Conclusion: FixTalk demonstrates a novel and effective approach to enhance the quality of talking head generation by simultaneously addressing identity leakage and rendering artifacts through innovative indicators.

Abstract: Talking head generation is gaining significant importance across various
domains, with a growing demand for high-quality rendering. However, existing
methods often suffer from identity leakage (IL) and rendering artifacts (RA),
particularly in extreme cases. Through an in-depth analysis of previous
approaches, we identify two key insights: (1) IL arises from identity
information embedded within motion features, and (2) this identity information
can be leveraged to address RA. Building on these findings, this paper
introduces FixTalk, a novel framework designed to simultaneously resolve both
issues for high-quality talking head generation. Firstly, we propose an
Enhanced Motion Indicator (EMI) to effectively decouple identity information
from motion features, mitigating the impact of IL on generated talking heads.
To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes
the leaked identity information to supplement missing details, thus fixing the
artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates
IL and RA, achieving superior performance compared to state-of-the-art methods.

</details>


### [75] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Main category: cs.CV

TL;DR: The authors address the challenge of online HD map construction for autonomous cars by proposing a network that predicts lane segments, road boundaries, and topology using SD map data and temporal consistency techniques.


<details>
  <summary>Details</summary>
Motivation: Current autonomous vehicle systems require HD maps, which are resource-intensive to construct, motivating a search for alternative methods to predict HD maps directly from onboard sensor data and SD maps.

Method: The paper introduces a network architecture that uses hybrid lane segment encodings, prior map information, denoising techniques, and temporal consistency achieved via past frame facilitation.

Result: The proposed model outperformed previous methods significantly in predicting coherent HD map elements such as lane boundaries and topology.

Conclusion: This work offers a more efficient, accurate method for constructing HD maps online, leveraging SD maps and advanced modeling techniques.

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [76] [Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound](https://arxiv.org/abs/2507.01401)
*Huanwen Liang,Jingxian Xu,Yuanji Zhang,Yuhao Huang,Yuhan Zhang,Xin Yang,Ran Li,Xuedong Deng,Yanjun Liu,Guowei Tao,Yun Wu,Sheng Zhao,Xinru Gao,Dong Ni*

Main category: cs.CV

TL;DR: This paper presents a novel AI-based method for classifying fetal abdominal anomalies using prenatal ultrasounds, outperforming existing techniques by employing multiple instance learning (MIL) and innovative modules tailored to medical knowledge.


<details>
  <summary>Details</summary>
Motivation: The study addresses the limited application of AI in diagnosing fetal abdominal malformations, which are life-threatening congenital anomalies requiring precise and case-level diagnostic tools beyond standard plane localization.

Method: The authors introduce a multiple instance learning (MIL)-based framework featuring three key components: a mixture-of-attention-experts module (MoAE) for weighting various attention heads, a medical-knowledge-driven feature selection module (MFS) for aligning image features with medical expertise, and a prompt-based prototype learning (PPL) approach to enhance the MFS.

Result: The proposed method outperforms state-of-the-art techniques, as validated on a robust dataset of 2,419 cases and 24,748 images spanning six categories of prenatal ultrasounds.

Conclusion: The study demonstrates that incorporating medical knowledge-driven feature selection and attention mechanisms in a case-level MIL setting can significantly improve the diagnosis of prenatal abdominal anomalies, providing a promising tool for clinical application.

Abstract: Fetal abdominal malformations are serious congenital anomalies that require
accurate diagnosis to guide pregnancy management and reduce mortality. Although
AI has demonstrated significant potential in medical diagnosis, its application
to prenatal abdominal anomalies remains limited. Most existing studies focus on
image-level classification and rely on standard plane localization, placing
less emphasis on case-level diagnosis. In this paper, we develop a case-level
multiple instance learning (MIL)-based method, free of standard plane
localization, for classifying fetal abdominal anomalies in prenatal ultrasound.
Our contribution is three-fold. First, we adopt a mixture-of-attention-experts
module (MoAE) to weight different attention heads for various planes. Secondly,
we propose a medical-knowledge-driven feature selection module (MFS) to align
image features with medical knowledge, performing self-supervised image token
selection at the case-level. Finally, we propose a prompt-based prototype
learning (PPL) to enhance the MFS. Extensively validated on a large prenatal
abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748
images and 6 categories, our proposed method outperforms the state-of-the-art
competitors. Codes are available at:https://github.com/LL-AC/AAcls.

</details>


### [77] [CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning](https://arxiv.org/abs/2507.01409)
*Kuniaki Saito,Donghyun Kim,Kwanyong Park,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: CaptionSmiths, a novel image captioning model, enables control over length, descriptiveness, and uniqueness in captions using interpolated conditioning between endpoints.


<details>
  <summary>Details</summary>
Motivation: To create an image captioning model that can flexibly adjust language patterns (e.g., descriptiveness, length) for diverse applications, overcoming limitations in fine-grained control in existing models.

Method: CaptionSmiths quantifies caption properties (length, descriptiveness, uniqueness) as scalar values, represents these values as interpolated vectors between extreme states, and trains the model to generate outputs based on these vectors.

Result: CaptionSmiths enables smooth transitions in the properties of captions while achieving better lexical alignment and reducing errors in length control by 50.6%.

Conclusion: The model demonstrates that conditioning captions using interpolated vectors effectively allows for fine-grained control over properties, making the approach viable for diverse applications.

Abstract: An image captioning model flexibly switching its language pattern, e.g.,
descriptiveness and length, should be useful since it can be applied to diverse
applications. However, despite the dramatic improvement in generative
vision-language models, fine-grained control over the properties of generated
captions is not easy due to two reasons: (i) existing models are not given the
properties as a condition during training and (ii) existing models cannot
smoothly transition its language pattern from one state to the other. Given
this challenge, we propose a new approach, CaptionSmiths, to acquire a single
captioning model that can handle diverse language patterns. First, our approach
quantifies three properties of each caption, length, descriptiveness, and
uniqueness of a word, as continuous scalar values, without human annotation.
Given the values, we represent the conditioning via interpolation between two
endpoint vectors corresponding to the extreme states, e.g., one for a very
short caption and one for a very long caption. Empirical results demonstrate
that the resulting model can smoothly change the properties of the output
captions and show higher lexical alignment than baselines. For instance,
CaptionSmiths reduces the error in controlling caption length by 506\% despite
better lexical alignment. Code will be available on
https://github.com/omron-sinicx/captionsmiths.

</details>


### [78] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/abs/2507.01417)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Main category: cs.CV

TL;DR: The paper introduces a lightweight, practical inference-stage approach for Out-of-Distribution (OOD) detection based on observed gradient phenomena, showing improved robustness and minimal computational overhead.


<details>
  <summary>Details</summary>
Motivation: To address the critical challenge of detecting inputs outside the training distribution (OOD) in deep models deployed in open-world environments.

Method: The authors propose an inference-stage technique to suppress the impact of spurious gradients that inflate OOD confidence by short-circuiting certain feature coordinates, while approximating logits with a first-order method to avoid additional computation costs.

Result: Their method yields significant improvements in OOD detection as demonstrated on standard benchmarks, with minimal disruption to the classification of in-distribution (ID) data.

Conclusion: The approach is lightweight, requires minimal changes to standard inference pipelines, and provides a practical solution for robust OOD detection in real-world scenarios.

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [79] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/abs/2507.01422)
*Wenjie Liu,Bingshu Wang,Ze Wang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: The paper introduces a diffusion model called DocShaDiffusion for removing color shadows in document images, enhancing performance compared to prior methods.


<details>
  <summary>Details</summary>
Motivation: Existing shadow removal techniques struggle with color shadows and lack effective solutions to maintain document structure and details.

Method: The authors propose DocShaDiffusion, a latent-space diffusion model combined with a shadow soft-mask generation module (SSGM) and a shadow mask-aware guided diffusion module (SMGDM). Additionally, they introduce a shadow-robust perceptual feature loss and a synthetic dataset (SDCSRD).

Result: DocShaDiffusion achieves superior performance in removing document image shadows compared to state-of-the-art methods, as validated by experiments on three public datasets.

Conclusion: The study demonstrates the effectiveness of DocShaDiffusion for accurate color shadow removal while preserving essential document details, supported by the release of code and dataset.

Abstract: Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow soft-mask generation module (SSGM).
It is able to produce accurate shadow mask and add noise into shadow regions
specially. Guided by the shadow mask, a shadow mask-aware guided diffusion
module (SMGDM) is proposed to remove shadows from document images by
supervising the diffusion and denoising process. We also propose a
shadow-robust perceptual feature loss to preserve details and structures in
document images. Moreover, we develop a large-scale synthetic document color
shadow removal dataset (SDCSRD). It simulates the distribution of realistic
color shadows and provides powerful supports for the training of models.
Experiments on three public datasets validate the proposed method's superiority
over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [80] [DiffMark: Diffusion-based Robust Watermark Against Deepfakes](https://arxiv.org/abs/2507.01428)
*Chen Sun,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Liejun Wang,Dan Ma,Gaobo Yang,Keqin Li*

Main category: cs.CV

TL;DR: This paper presents DiffMark, a diffusion model-based watermarking framework that robustly embeds watermarks into facial images, offering resistance against Deepfake manipulations.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of robustness in existing watermarking techniques when faced with Deepfake manipulations and provide a solution to enhance security and authenticity verification.

Method: The method leverages diffusion models to integrate watermarks into facial images during generation. It employs a timestep-dependent weighting for facial images, a cross information fusion (CIF) module for watermark-image feature integration, and utilizes a frozen autoencoder and Deepfake-resistant guidance to simulate and counteract Deepfake manipulations.

Result: The proposed DiffMark framework demonstrates effective watermarking that is robust against standard Deepfake manipulations, as proven by experimental results.

Conclusion: DiffMark offers a novel and effective diffusion model-based approach to robust watermarking, addressing vulnerabilities against Deepfakes in authenticity and privacy settings.

Abstract: Deepfakes pose significant security and privacy threats through malicious
facial manipulations. While robust watermarking can aid in authenticity
verification and source tracking, existing methods often lack the sufficient
robustness against Deepfake manipulations. Diffusion models have demonstrated
remarkable performance in image generation, enabling the seamless fusion of
watermark with image during generation. In this study, we propose a novel
robust watermarking framework based on diffusion model, called DiffMark. By
modifying the training and sampling scheme, we take the facial image and
watermark as conditions to guide the diffusion model to progressively denoise
and generate corresponding watermarked image. In the construction of facial
condition, we weight the facial image by a timestep-dependent factor that
gradually reduces the guidance intensity with the decrease of noise, thus
better adapting to the sampling process of diffusion model. To achieve the
fusion of watermark condition, we introduce a cross information fusion (CIF)
module that leverages a learnable embedding table to adaptively extract
watermark features and integrates them with image features via cross-attention.
To enhance the robustness of the watermark against Deepfake manipulations, we
integrate a frozen autoencoder during training phase to simulate Deepfake
manipulations. Additionally, we introduce Deepfake-resistant guidance that
employs specific Deepfake model to adversarially guide the diffusion sampling
process to generate more robust watermarked images. Experimental results
demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.
Our code will be available at https://github.com/vpsg-research/DiffMark.

</details>


### [81] [TurboReg: TurboClique for Robust and Efficient Point Cloud Registration](https://arxiv.org/abs/2507.01439)
*Shaocheng Yan,Pengcheng Shi,Zhenjun Zhao,Kaixin Wang,Kuang Cao,Ji Wu,Jiayuan Li*

Main category: cs.CV

TL;DR: The paper introduces TurboReg, a fast and robust point cloud registration estimator using TurboCliques and the efficient Pivot-Guided Search algorithm for improved speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing correspondence-based Point Cloud Registration (PCR) methods face significant time complexity due to reliance on maximal clique searches, limiting their use in time-critical applications.

Method: TurboReg introduces TurboCliques (lightweight 3-cliques in highly-constrained graphs) and the linear time complexity Pivot-Guided Search (PGS) algorithm to improve searching efficiency and robustness.

Result: Experiments demonstrate TurboReg's superior performance in speed and recall, achieving state-of-the-art accuracy while being substantially faster on datasets like 3DMatch+FCGF, operating over 208Ã faster than 3DMAC.

Conclusion: TurboReg presents a breakthrough in point cloud registration by combining robust, spatially consistent transformations with significant computational efficiency, making it highly suitable for real-world applications.

Abstract: Robust estimation is essential in correspondence-based Point Cloud
Registration (PCR). Existing methods using maximal clique search in
compatibility graphs achieve high recall but suffer from exponential time
complexity, limiting their use in time-sensitive applications. To address this
challenge, we propose a fast and robust estimator, TurboReg, built upon a novel
lightweight clique, TurboClique, and a highly parallelizable Pivot-Guided
Search (PGS) algorithm. First, we define the TurboClique as a 3-clique within a
highly-constrained compatibility graph. The lightweight nature of the 3-clique
allows for efficient parallel searching, and the highly-constrained
compatibility graph ensures robust spatial consistency for stable
transformation estimation. Next, PGS selects matching pairs with high SC$^2$
scores as pivots, effectively guiding the search toward TurboCliques with
higher inlier ratios. Moreover, the PGS algorithm has linear time complexity
and is significantly more efficient than the maximal clique search with
exponential time complexity. Extensive experiments show that TurboReg achieves
state-of-the-art performance across multiple real-world datasets, with
substantial speed improvements. For example, on the 3DMatch+FCGF dataset,
TurboReg (1K) operates $208.22\times$ faster than 3DMAC while also achieving
higher recall. Our code is accessible at
\href{https://github.com/Laka-3DV/TurboReg}{\texttt{TurboReg}}.

</details>


### [82] [OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes](https://arxiv.org/abs/2507.01455)
*Yuxing Liu,Ji Zhang,Zhou Xuchuan,Jingzhong Xiao,Huimin Yang,Jiaxin Zhong*

Main category: cs.CV

TL;DR: OoDDINO is introduced as a novel framework for pixel-wise anomaly segmentation, tackling challenges like fragmented results and inconsistent anomaly score distributions.


<details>
  <summary>Details</summary>
Motivation: Address challenges in anomaly segmentation, such as fragmented segmentation due to ignored spatial correlations and inconsistent global thresholds resulting in false positives or missed anomalies.

Method: Proposes a two-stage cascade architecture with (1) an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) to enhance anomaly detection accuracy and (2) an Adaptive Dual-Threshold Network (ADT-Net) for region-specific thresholding.

Result: OoDDINO demonstrates superior performance and compatibility compared to state-of-the-art methods on two benchmark datasets.

Conclusion: OoDDINO offers a significant improvement in pixel-wise anomaly segmentation, serving as a highly adaptable tool for practical applications and boosting other models' performance as a plug-in.

Abstract: Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous
objects within images. Existing pixel-wise methods typically assign anomaly
scores individually and employ a global thresholding strategy to segment
anomalies. Despite their effectiveness, these approaches encounter significant
challenges in real-world applications: (1) neglecting spatial correlations
among pixels within the same object, resulting in fragmented segmentation; (2)
variabil ity in anomaly score distributions across image regions, causing
global thresholds to either generate false positives in background areas or
miss segments of anomalous objects. In this work, we introduce OoDDINO, a novel
multi-level anomaly segmentation framework designed to address these
limitations through a coarse-to-fine anomaly detection strategy. OoDDINO
combines an uncertainty-guided anomaly detection model with a pixel-level
segmentation model within a two-stage cascade architecture. Initially, we
propose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that
sequentially integrates multiple uncertainty metrics with visual
representations, employing orthogonal constraints to strengthen the detection
model's capacity for localizing anomalous regions accurately. Subsequently, we
develop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically
generates region-specific thresholds based on object-level detection outputs
and pixel-wise anomaly scores. This approach allows for distinct thresholding
strategies within foreground and background areas, achieving fine-grained
anomaly segmentation. The proposed framework is compatible with other
pixel-wise anomaly detection models, which acts as a plug-in to boost the
performance. Extensive experiments on two benchmark datasets validate our
framework's superiority and compatibility over state-of-the-art methods.

</details>


### [83] [NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation](https://arxiv.org/abs/2507.01463)
*Max Gandyra,Alessandro Santonicola,Michael Beetz*

Main category: cs.CV

TL;DR: The paper introduces NOCTIS, a framework for instance segmentation of novel objects, aiming to generalize across unseen objects without retraining. It builds on prior works and leverages vision foundation models like Grounded-SAM 2 and DINOv2 for object detection and embedding similarity matrices.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of instance segmentation for unseen object categories in RGB images, aiming to create a generalizable model that doesn't require retraining.

Method: NOCTIS leverages Grounded-SAM 2 for bounding box detection and segmentation masks, and DINOv2 for zero-shot embedding generation. It introduces a cyclic filtering mechanism for patch embeddings and incorporates bounding box/mask confidence scores to refine object matching.

Result: NOCTIS outperformed existing methods in the BOP 2023 challenge for unseen object segmentation, achieving state-of-the-art results on seven datasets without additional training.

Conclusion: The framework demonstrates that integrating vision foundation models with novel metrics can yield superior generalizable performance for instance segmentation of previously unseen objects.

Abstract: Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed, for all kinds of novel
objects, without (re-) training, has proven to be a difficult task. To handle
this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic
Threshold based Instance Segmentation (NOCTIS). This work stems from and
improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also
leverages on recent vision foundation models, namely: Grounded-SAM 2 and
DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise
bounding boxes and their corresponding segmentation masks; while DINOv2's
zero-shot capabilities are employed to generate the image embeddings. The
quality of those masks, together with their embeddings, is of vital importance
to our approach; as the proposal-object matching is realized by determining an
object matching score based on the similarity of the class embeddings and the
average maximum similarity of the patch embeddings. Differently to SAM-6D,
calculating the latter involves a prior patch filtering based on the distance
between each patch and its corresponding cyclic/roundtrip patch in the image
grid. Furthermore, the average confidence of the proposals' bounding box and
mask is used as an additional weighting factor for the object matching score.
We empirically show that NOCTIS, without further training/fine tuning,
outperforms the best RGB and RGB-D methods on the seven core datasets of the
BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects"
task.

</details>


### [84] [Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think](https://arxiv.org/abs/2507.01467)
*Ge Wu,Shen Zhang,Ruijing Shi,Shanghua Gao,Zhenyuan Chen,Lei Wang,Zhaowei Chen,Hongcheng Gao,Yao Tang,Jian Yang,Ming-Ming Cheng,Xiang Li*

Main category: cs.CV

TL;DR: This paper introduces REG, a novel method for improving the efficiency and effectiveness of diffusion model training by integrating low-level and high-level visual representations.


<details>
  <summary>Details</summary>
Motivation: The paper addresses limitations in current diffusion models, particularly the lack of utilization of global semantics during denoising processes, which hampers the ability to fully harness discriminative representations.

Method: REG entangles low-level image latents with a single high-level class token from pretrained models, seamlessly guiding the reconstruction of both image and global semantics during generation with negligible inference overhead.

Result: In experiments with ImageNet 256x256, REG significantly accelerates convergenceâup to 63x and 23x faster than baseline modelsâand achieves superior performance within fewer training iterations. Notably, REG provides better results with only a fraction of the training time needed for existing approaches.

Conclusion: REG represents a breakthrough in diffusion model training, yielding more efficient, high-quality generation with minimal computational increases, and setting a new benchmark for rapid training convergence and performance capabilities.

Abstract: REPA and its variants effectively mitigate training challenges in diffusion
models by incorporating external visual representations from pretrained models,
through alignment between the noisy hidden projections of denoising networks
and foundational clean image representations. We argue that the external
alignment, which is absent during the entire denoising inference process, falls
short of fully harnessing the potential of discriminative representations. In
this work, we propose a straightforward method called Representation
Entanglement for Generation (REG), which entangles low-level image latents with
a single high-level class token from pretrained foundation models for
denoising. REG acquires the capability to produce coherent image-class pairs
directly from pure noise, substantially improving both generation quality and
training efficiency. This is accomplished with negligible additional inference
overhead, requiring only one single additional token for denoising (<0.5\%
increase in FLOPs and latency). The inference process concurrently reconstructs
both image latents and their corresponding global semantics, where the acquired
semantic knowledge actively guides and enhances the image generation process.
On ImageNet 256$\times$256, SiT-XL/2 + REG demonstrates remarkable convergence
acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster
training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,
SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA
trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at:
https://github.com/Martinser/REG.

</details>


### [85] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/abs/2507.01472)
*JonÃ¡Å¡ Herec,VÃ­t RÅ¯Å¾iÄka,Rado PitoÅÃ¡k*

Main category: cs.CV

TL;DR: Researchers developed faster algorithms for detecting methane leaks from hyperspectral satellite imagery, optimizing them for resource-limited onboard systems.


<details>
  <summary>Details</summary>
Motivation: Efficient onboard detection of methane leaks is vital to mitigate climate change and overcome slow satellite data downlink rates.

Method: The study tested fast detection algorithms (ACE, CEM) and introduced a new method (Mag1c-SAS), integrating these with machine learning models like U-Net and LinkNet.

Result: Two algorithmsâMag1c-SAS and CEMâwere identified as effective and computationally efficient, achieving up to ~100x and ~230x speed improvements over existing methods.

Conclusion: This research enables timely methane detection via low-power onboard systems and provides open-source resources to advance the field further.

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [86] [Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects](https://arxiv.org/abs/2507.01478)
*Chentao Shen,Ding Pan,Mingyu Mei,Zaixing He,Xinyue Zhao*

Main category: cs.CV

TL;DR: The paper introduces a novel method for 6DoF pose tracking of industrial metal objects using active control points and optimal regression techniques to overcome the challenges posed by reflective surfaces.


<details>
  <summary>Details</summary>
Motivation: Pose tracking for industrial metal objects is difficult due to reflection challenges in real-world environments.

Method: It uses image control points to generate edge features for optimization, replaces pose-based rendering, and implements optimal control point regression for better robustness.

Result: The method effectively tracks metal objects in both dataset evaluations and real-world tasks, showing real-time capabilities.

Conclusion: The proposed approach is a practical solution to 6DoF pose tracking problems in industry and is open-sourced for public use.

Abstract: Visual pose tracking is playing an increasingly vital role in industrial
contexts in recent years. However, the pose tracking for industrial metal
objects remains a challenging task especially in the real world-environments,
due to the reflection characteristic of metal objects. To address this issue,
we propose a novel 6DoF pose tracking method based on active control points.
The method uses image control points to generate edge feature for optimization
actively instead of 6DoF pose-based rendering, and serve them as optimization
variables. We also introduce an optimal control point regression method to
improve robustness. The proposed tracking method performs effectively in both
dataset evaluation and real world tasks, providing a viable solution for
real-time tracking of industrial metal objects. Our source code is made
publicly available at: https://github.com/tomatoma00/ACPTracking.

</details>


### [87] [What Really Matters for Robust Multi-Sensor HD Map Construction?](https://arxiv.org/abs/2507.01484)
*Xiaoshuai Hao,Yuting Zhao,Yuheng Ji,Luanyuan Dai,Peng Hao,Dingzhe Li,Shuai Cheng,Rong Yin*

Main category: cs.CV

TL;DR: The paper focuses on improving robustness in Camera-LiDAR multi-modal fusion for HD map construction, achieving state-of-the-art results with novel strategies.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robustness in existing Camera-LiDAR fusion methods for HD map construction, critical for autonomous vehicles.

Method: Introduces three components: data augmentation, a novel multi-modal fusion module, and a modality dropout training strategy, tested on NuScenes data.

Result: Significant robustness improvements over baselines and state-of-the-art performance on the NuScenes validation set.

Conclusion: The proposed methods provide valuable insights for creating robust HD map construction models, enhancing real-world autonomous driving applications.

Abstract: High-definition (HD) map construction methods are crucial for providing
precise and comprehensive static environmental information, which is essential
for autonomous driving systems. While Camera-LiDAR fusion techniques have shown
promising results by integrating data from both modalities, existing approaches
primarily focus on improving model accuracy and often neglect the robustness of
perception models, which is a critical aspect for real-world applications. In
this paper, we explore strategies to enhance the robustness of multi-modal
fusion methods for HD map construction while maintaining high accuracy. We
propose three key components: data augmentation, a novel multi-modal fusion
module, and a modality dropout training strategy. These components are
evaluated on a challenging dataset containing 10 days of NuScenes data. Our
experimental results demonstrate that our proposed methods significantly
enhance the robustness of baseline methods. Furthermore, our approach achieves
state-of-the-art performance on the clean validation set of the NuScenes
dataset. Our findings provide valuable insights for developing more robust and
reliable HD map construction models, advancing their applicability in
real-world autonomous driving scenarios. Project website:
https://robomap-123.github.io.

</details>


### [88] [AVC-DPO: Aligned Video Captioning via Direct Preference Optimization](https://arxiv.org/abs/2507.01492)
*Jiyang Tang,Hengyi Li,Yifan Du,Wayne Xin Zhao*

Main category: cs.CV

TL;DR: The paper introduces AVC-DPO, a framework to improve video captioning quality in multimodal language models by aligning captions with human preferences.


<details>
  <summary>Details</summary>
Motivation: The need to optimize video captions for human preferences, focusing on temporal dynamics and spatial information.

Method: AVC-DPO employs enhanced prompts targeting specific video features and conducts post-training using varied prompts to align captions with human-centric preferences.

Result: The method achieved first place in the Video Detailed Captioning Challenge according to VDCSCORE evaluation metric.

Conclusion: AVC-DPO enhances video MLLMs' captioning capabilities, showing superior performance in aligning outputs with human preferences.

Abstract: Although video multimodal large language models (video MLLMs) have achieved
substantial progress in video captioning tasks, it remains challenging to
adjust the focal emphasis of video captions according to human preferences. To
address this limitation, we propose Aligned Video Captioning via Direct
Preference Optimization (AVC-DPO), a post-training framework designed to
enhance captioning capabilities in video MLLMs through preference alignment.
Our approach designs enhanced prompts that specifically target temporal
dynamics and spatial information-two key factors that humans care about when
watching a video-thereby incorporating human-centric preferences. AVC-DPO
leverages the same foundation model's caption generation responses under varied
prompt conditions to conduct preference-aware training and caption alignment.
Using this framework, we have achieved exceptional performance in the
LOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving
first place on the Video Detailed Captioning (VDC) benchmark according to the
VDCSCORE evaluation metric.

</details>


### [89] [Crop Pest Classification Using Deep Learning Techniques: A Review](https://arxiv.org/abs/2507.01494)
*Muhammad Hassam Ejaz,Muhammad Bilal,Usman Habib*

Main category: cs.CV

TL;DR: The paper reviews 37 studies on AI-based pest classification, highlighting the evolution from CNNs to hybrid and transformer models, current challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately detecting and monitoring insect pests threatening global crop yields, aiming to improve and scale traditional manual methods.

Method: The paper reviews and organizes 37 studies by factors like crop type, pest species, model architecture, dataset usage, and challenges, focusing on AI approaches (CNNs, ViTs, and hybrid models).

Result: AI methods are advancing from CNNs to more precise hybrid and transformer-based models, but significant challenges such as dataset imbalance, small pest detection, and scalability on edge devices remain.

Conclusion: AI-based pest classification holds promise with improved model architectures, but further innovation is needed to address key limitations for practical and scalable deployment.

Abstract: Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-based pest classification. The selected
research is organized by crop type, pest species, model architecture, dataset
usage, and key technical challenges. The early studies relied heavily on CNNs
but latest work is shifting toward hybrid and transformer-based models that
deliver higher accuracy and better contextual understanding. Still, challenges
like imbalanced datasets, difficulty in detecting small pests, limited
generalizability, and deployment on edge devices remain significant hurdles.
Overall, this review offers a structured overview of the field, highlights
useful datasets, and outlines the key challenges and future directions for
AI-based pest monitoring systems.

</details>


### [90] [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](https://arxiv.org/abs/2507.01496)
*Jimyeong Kim,Jungwon Park,Yeji Song,Nojun Kwak,Wonjong Rhee*

Main category: cs.CV

TL;DR: This paper introduces a new method for real-image editing using Rectified Flow (ReFlow) models, outperforming prior methods in image quality and text alignment without requiring training, masks, or source prompts.


<details>
  <summary>Details</summary>
Motivation: While ReFlow models show superior performance in text-to-image generation, adapting them for real-image editing has been difficult, necessitating new techniques.

Method: The authors leverage mid-step latents and adapt attention mechanisms during injection to extract key intermediate features, enabling real-image editing without extensive prerequisites like masks or additional training.

Result: Experiments on two benchmarks and nine baselines demonstrate significant performance improvements over existing methods, with human evaluations favoring this approach.

Conclusion: The proposed technique enables effective, user-friendly real-image editing using ReFlow models, pushing the boundaries of multimodal text-to-image models for practical applications.

Abstract: Rectified Flow text-to-image models surpass diffusion models in image quality
and text alignment, but adapting ReFlow for real-image editing remains
challenging. We propose a new real-image editing method for ReFlow by analyzing
the intermediate representations of multimodal transformer blocks and
identifying three key features. To extract these features from real images with
sufficient structural preservation, we leverage mid-step latent, which is
inverted only up to the mid-step. We then adapt attention during injection to
improve editability and enhance alignment to the target text. Our method is
training-free, requires no user-provided mask, and can be applied even without
a source prompt. Extensive experiments on two benchmarks with nine baselines
demonstrate its superior performance over prior methods, further validated by
human evaluations confirming a strong user preference for our approach.

</details>


### [91] [Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images](https://arxiv.org/abs/2507.01502)
*Ozan Durgut,Beril Kallfelz-Sirmacek,Cem Unsalan*

Main category: cs.CV

TL;DR: The paper introduces a rule-based approach combining traditional and deep learning methods to improve tree crown detection, aiming to aid in forest monitoring.


<details>
  <summary>Details</summary>
Motivation: To address global environmental issues by improving forest monitoring using automated tree crown detection methods.

Method: A novel rule-based approach integrating traditional feature extraction/segmentation and deep learning detection, followed by post-processing to refine the results.

Result: Enhanced accuracy and robustness in detecting tree crowns compared to individual methods, with a discussion on advantages and limitations.

Conclusion: Combining methods improves tree crown detection, but there are opportunities for further refinement and enhancement.

Abstract: Global warming, loss of biodiversity, and air pollution are among the most
significant problems facing Earth. One of the primary challenges in addressing
these issues is the lack of monitoring forests to protect them. To tackle this
problem, it is important to leverage remote sensing and computer vision methods
to automate monitoring applications. Hence, automatic tree crown detection
algorithms emerged based on traditional and deep learning methods. In this
study, we first introduce two different tree crown detection methods based on
these approaches. Then, we form a novel rule-based approach that integrates
these two methods to enhance robustness and accuracy of tree crown detection
results. While traditional methods are employed for feature extraction and
segmentation of forested areas, deep learning methods are used to detect tree
crowns in our method. With the proposed rule-based approach, we post-process
these results, aiming to increase the number of detected tree crowns through
neighboring trees and localized operations. We compare the obtained results
with the proposed method in terms of the number of detected tree crowns and
report the advantages, disadvantages, and areas for improvement of the obtained
outcomes.

</details>


### [92] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
*Robert AufschlÃ¤ger,Youssef Shoeb,Azarm Nowzad,Michael Heigl,Fabian Bally,Martin Schramm*

Main category: cs.CV

TL;DR: The paper introduces cRID, a framework combining AI techniques to improve detection of privacy-sensitive personal identifiers in image datasets.


<details>
  <summary>Details</summary>
Motivation: To address privacy risks in street-level open data containing Personally Identifiable Information (PII).

Method: Developed a cross-modal framework using Large Vision-Language Models, Graph Attention Networks, and representation learning to focus on semantically meaningful PII detection.

Result: Improved person re-identification (Re-ID) performance across datasets, particularly in cross-dataset scenarios like Market-1501 to CUHK03-np.

Conclusion: The approach provides practical tools for enhancing privacy-aware autonomous systems while advancing person Re-ID technology.

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [93] [What does really matter in image goal navigation?](https://arxiv.org/abs/2507.01667)
*Gianluca Monaci,Philippe Weinzaepfel,Christian Wolf*

Main category: cs.CV

TL;DR: The paper explores whether image goal navigation can be efficiently solved with end-to-end reinforcement learning (RL), comparing architectural choices and their impact on relative pose estimators.


<details>
  <summary>Details</summary>
Motivation: To determine if end-to-end reinforcement learning is sufficient for image goal navigation, eliminating the need for dedicated image-matching or pre-trained computer vision modules.

Method: The study evaluated architectural choices (e.g., late fusion, channel stacking, space-to-depth projections, cross-attention) alongside reinforcement learning in simulated and more realistic settings.

Result: Findings indicate that simulator settings can enable shortcuts, but some learned capabilities transfer to realistic settings. There is also evidence of correlations between navigation and relative pose estimation performance.

Conclusion: End-to-end RL shows potential for emerging relative pose estimators in navigation tasks, though performance is influenced by simulation settings and transferable capabilities are limited.

Abstract: Image goal navigation requires two different skills: firstly, core navigation
skills, including the detection of free space and obstacles, and taking
decisions based on an internal representation; and secondly, computing
directional information by comparing visual observations to the goal image.
Current state-of-the-art methods either rely on dedicated image-matching, or
pre-training of computer vision modules on relative pose estimation. In this
paper, we study whether this task can be efficiently solved with end-to-end
training of full agents with RL, as has been claimed by recent work. A positive
answer would have impact beyond Embodied AI and allow training of relative pose
estimation from reward for navigation alone. In a large study we investigate
the effect of architectural choices like late fusion, channel stacking,
space-to-depth projections and cross-attention, and their role in the emergence
of relative pose estimators from navigation training. We show that the success
of recent methods is influenced up to a certain extent by simulator settings,
leading to shortcuts in simulation. However, we also show that these
capabilities can be transferred to more realistic setting, up to some extend.
We also find evidence for correlations between navigation performance and
probed (emerging) relative pose estimation performance, an important sub skill.

</details>


### [94] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/abs/2507.01509)
*Tapas K. Dutta,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: The paper introduces SAM-MaGuP, a novel model for polyp segmentation in colonoscopy images, overcoming challenges such as weak boundaries and similarity with surrounding tissues, achieving superior results compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenges of polyp segmentation in colonoscopy images, particularly weak/boundary issues, high similarity between polyps and surrounding tissue, and the limitations of existing models in real-world clinical applications.

Method: The proposed method, SAM-MaGuP, integrates a boundary distillation module and a 1D-2D Mamba adapter into the Segment Anything Model (SAM) to improve feature learning and address weak boundary problems.

Result: Extensive testing on five datasets demonstrated that SAM-MaGuP outperforms current state-of-the-art methods in segmentation accuracy and robustness.

Conclusion: SAM-MaGuP represents a significant advancement in polyp segmentation, offering robust solutions to weak boundary challenges and setting a new performance benchmark for real-world clinical applications.

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [95] [Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights](https://arxiv.org/abs/2507.01532)
*Tomas Zelezny,Jakub Straka,Vaclav Javorek,Ondrej Valach,Marek Hruz,Ivan Gruber*

Main category: cs.CV

TL;DR: This paper investigates how preprocessing techniques like normalization, interpolation, and augmentation affect the performance of pose-based Sign Language Translation systems using a transformer-based model.


<details>
  <summary>Details</summary>
Motivation: The paper aims to improve Sign Language Translation systems by exploring the impact of preprocessing techniques and adapting advanced architectures for better translation accuracy.

Method: The researchers applied a modified T5 transformer-based encoder-decoder model to pose representations and conducted ablation studies on YouTubeASL and How2Sign datasets, analyzing the effects of various preprocessing strategies.

Result: The study shows that preprocessing techniques (normalization, interpolation, and augmentation) significantly enhance model robustness, generalization, and overall translation accuracy.

Conclusion: Appropriate preprocessing strategies are critical for improving SLT systems. Additionally, modifying the model with register tokens further enhances performance. The published code and preprocessed data are accessible for the community.

Abstract: Sign Language Translation (SLT) has evolved significantly, moving from
isolated recognition approaches to complex, continuous gloss-free translation
systems. This paper explores the impact of pose-based data preprocessing
techniques - normalization, interpolation, and augmentation - on SLT
performance. We employ a transformer-based architecture, adapting a modified T5
encoder-decoder model to process pose representations. Through extensive
ablation studies on YouTubeASL and How2Sign datasets, we analyze how different
preprocessing strategies affect translation accuracy. Our results demonstrate
that appropriate normalization, interpolation, and augmentation techniques can
significantly improve model robustness and generalization abilities.
Additionally, we provide a deep analysis of the model's attentions and reveal
interesting behavior suggesting that adding a dedicated register token can
improve overall model performance. We publish our code on our GitHub
repository, including the preprocessed YouTubeASL data.

</details>


### [96] [TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking](https://arxiv.org/abs/2507.01535)
*Bingxi Liu,Calvin Chen,Junhao Li,Guyang Yu,Haoqian Song,Xuchen Liu,Jinqiang Cui,Hong Zhang*

Main category: cs.CV

TL;DR: The paper proposes TrackingMiM, a minimal-computation model using a Mamba-in-Mamba architecture for UAV tracking, addressing temporal inconsistency issues in existing methods and achieving state-of-the-art precision at higher speeds.


<details>
  <summary>Details</summary>
Motivation: The study aims to overcome the quadratic complexity challenges of Vision Transformer (ViT) models in real-time UAV tracking by leveraging the computational efficiency of the State-Space Model, Mamba.

Method: The proposed method, TrackingMiM, employs a Mamba-in-Mamba architecture, performing nested Mamba scans to independently process temporal and spatial coherent patch tokens while encoding the template frame as a query token.

Result: Experiments on five UAV tracking benchmarks demonstrate that TrackingMiM achieves state-of-the-art precision with noticeably higher tracking speed.

Conclusion: TrackingMiM effectively addresses temporal inconsistency challenges in Mamba-based methods, offering an efficient and high-precision solution for real-time UAV tracking.

Abstract: The Vision Transformer (ViT) model has long struggled with the challenge of
quadratic complexity, a limitation that becomes especially critical in unmanned
aerial vehicle (UAV) tracking systems, where data must be processed in real
time. In this study, we explore the recently proposed State-Space Model, Mamba,
leveraging its computational efficiency and capability for long-sequence
modeling to effectively process dense image sequences in tracking tasks. First,
we highlight the issue of temporal inconsistency in existing Mamba-based
methods, specifically the failure to account for temporal continuity in the
Mamba scanning mechanism. Secondly, building upon this insight,we propose
TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model
for handling image sequence of tracking problem. In our framework, the mamba
scan is performed in a nested way while independently process temporal and
spatial coherent patch tokens. While the template frame is encoded as query
token and utilized for tracking in every scan. Extensive experiments conducted
on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves
state-of-the-art precision while offering noticeable higher speed in UAV
tracking.

</details>


### [97] [A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization](https://arxiv.org/abs/2507.01539)
*Mohammadreza Amirian,Michael Bach,Oscar Jimenez-del-Toro,Christoph Aberle,Roger Schaer,Vincent Andrearczyk,Jean-FÃ©lix Maestrati,Maria Martin Asiain,Kyriakos Flouris,Markus Obmann,Clarisse Dromain,BenoÃ®t Dufour,Pierre-Alexandre Alois Poletti,Hendrik von Tengg-Kobligk,Rolf HÃ¼gli,Martin Kretzschmar,Hatem Alkadhi,Ender Konukoglu,Henning MÃ¼ller,Bram Stieltjes,Adrien Depeursinge*

Main category: cs.CV

TL;DR: The paper presents an open-source CT scan dataset designed to improve AI harmonization techniques for addressing data distribution shifts. It includes 1378 image series from 13 scanners and offers tools for stability assessment and liver classification.


<details>
  <summary>Details</summary>
Motivation: To address the issue of poor generalization in AI for CT analysis caused by data distribution shifts due to factors like scanner variability and acquisition settings.

Method: The paper introduces a benchmark dataset of CT scans acquired under controlled conditions with variations in scanners and settings. This dataset is used to evaluate AI harmonization techniques. A phantom model replaced real patient data to eliminate individual biological effects.

Result: The dataset features 1378 CT scan series from 13 scanners across 8 institutions and includes a methodology for assessing image- and feature-level stability, along with baseline results in liver tissue classification.

Conclusion: The dataset and methodology aim to facilitate the development and evaluation of AI techniques that mitigate distribution shift issues, contributing to more reliable AI in medical imaging.

Abstract: Artificial intelligence (AI) has introduced numerous opportunities for human
assistance and task automation in medicine. However, it suffers from poor
generalization in the presence of shifts in the data distribution. In the
context of AI-based computed tomography (CT) analysis, significant data
distribution shifts can be caused by changes in scanner manufacturer,
reconstruction technique or dose. AI harmonization techniques can address this
problem by reducing distribution shifts caused by various acquisition settings.
This paper presents an open-source benchmark dataset containing CT scans of an
anthropomorphic phantom acquired with various scanners and settings, which
purpose is to foster the development of AI harmonization techniques. Using a
phantom allows fixing variations attributed to inter- and intra-patient
variations. The dataset includes 1378 image series acquired with 13 scanners
from 4 manufacturers across 8 institutions using a harmonized protocol as well
as several acquisition doses. Additionally, we present a methodology, baseline
results and open-source code to assess image- and feature-level stability and
liver tissue classification, promoting the development of AI harmonization
strategies.

</details>


### [98] [Interpolation-Based Event Visual Data Filtering Algorithms](https://arxiv.org/abs/2507.01557)
*Marcin Kowlaczyk,Tomasz Kryjak*

Main category: cs.CV

TL;DR: The paper introduces a method to eliminate 99% of noise in event camera data while retaining valid signals.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the significant noise in the data streams of rapidly developing neuromorphic vision and event camera applications.

Method: Four algorithms based on the matrix of infinite impulse response (IIR) filters were proposed and tested on event datasets with artificially added noise and real noise.

Result: The proposed methods demonstrated high noise reduction efficiency and required only 30KB of memory for a 1280 x 720 resolution sensor, making them suitable for embedded devices.

Conclusion: The method is highly effective and resource-efficient, offering a practical solution for noise reduction in event camera data.

Abstract: The field of neuromorphic vision is developing rapidly, and event cameras are
finding their way into more and more applications. However, the data stream
from these sensors is characterised by significant noise. In this paper, we
propose a method for event data that is capable of removing approximately 99\%
of noise while preserving the majority of the valid signal. We have proposed
four algorithms based on the matrix of infinite impulse response (IIR) filters
method. We compared them on several event datasets that were further modified
by adding artificially generated noise and noise recorded with dynamic vision
sensor. The proposed methods use about 30KB of memory for a sensor with a
resolution of 1280 x 720 and is therefore well suited for implementation in
embedded devices.

</details>


### [99] [A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2507.01573)
*Hao Wang,Keyan Hu,Xin Guo,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: This paper addresses semantic segmentation challenges in remote sensing through a novel framework combining discriminative and diffusion generative models to refine boundaries effectively.


<details>
  <summary>Details</summary>
Motivation: Current semantic segmentation models excel at identifying large-scale features but struggle to accurately capture fine boundary details due to limitations in discriminative learning and insufficient semantic inference in generative models.

Method: The proposed IDGBR framework combines a discriminative backbone for generating coarse segmentation maps with a conditioning guidance network, feeding into a denoising diffusion process for refined boundary segmentation.

Result: Experimentation across five datasets demonstrates the framework's consistent effectiveness in refining segmentation boundaries for binary and multi-class segmentation tasks.

Conclusion: The integration of discriminative and generative learning in the IDGBR framework effectively addresses the shortcomings in boundary refinement for remote sensing applications, bridging the gap between semantic accuracy and boundary precision.

Abstract: Remote sensing semantic segmentation must address both what the ground
objects are within an image and where they are located. Consequently,
segmentation models must ensure not only the semantic correctness of
large-scale patches (low-frequency information) but also the precise
localization of boundaries between patches (high-frequency information).
However, most existing approaches rely heavily on discriminative learning,
which excels at capturing low-frequency features, while overlooking its
inherent limitations in learning high-frequency features for semantic
segmentation. Recent studies have revealed that diffusion generative models
excel at generating high-frequency details. Our theoretical analysis confirms
that the diffusion denoising process significantly enhances the model's ability
to learn high-frequency features; however, we also observe that these models
exhibit insufficient semantic inference for low-frequency features when guided
solely by the original image. Therefore, we integrate the strengths of both
discriminative and generative learning, proposing the Integration of
Discriminative and diffusion-based Generative learning for Boundary Refinement
(IDGBR) framework. The framework first generates a coarse segmentation map
using a discriminative backbone model. This map and the original image are fed
into a conditioning guidance network to jointly learn a guidance representation
subsequently leveraged by an iterative denoising diffusion process refining the
coarse segmentation. Extensive experiments across five remote sensing semantic
segmentation datasets (binary and multi-class segmentation) confirm our
framework's capability of consistent boundary refinement for coarse results
from diverse discriminative architectures. The source code will be available at
https://github.com/KeyanHu-git/IDGBR.

</details>


### [100] [SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation](https://arxiv.org/abs/2507.01586)
*Bryan Constantine Sadihin,Michael Hua Wang,Shei Pern Chua,Hang Su*

Main category: cs.CV

TL;DR: The paper introduces SketchColour, a solution to improve 2D animation production using a diffusion transformer, reducing labor intensity and computational costs while achieving high-quality results.


<details>
  <summary>Details</summary>
Motivation: Current methods for producing 2D animation require manual labor for drawing and coloring each frame, making the process highly intensive.

Method: The pipeline utilizes a diffusion transformer backbone, replaces U-Net denoiser, and integrates sketch information through lightweight adapters and LoRA finetuning, reducing memory and parameter usage while enhancing quality.

Result: SketchColour outperforms prior video colorization methods on the SAKUGA dataset, achieving better metrics and producing coherent animations with fewer visual artifacts, even with less training data.

Conclusion: The SketchColour method is efficient, reduces computational requirements, and achieves state-of-the-art results, making it a promising solution for 2D animation production.

Abstract: The production of high-quality 2D animation is highly labor-intensive
process, as animators are currently required to draw and color a large number
of frames by hand. We present SketchColour, the first sketch-to-colour pipeline
for 2D animation built on a diffusion transformer (DiT) backbone. By replacing
the conventional U-Net denoiser with a DiT-style architecture and injecting
sketch information via lightweight channel-concatenation adapters accompanied
with LoRA finetuning, our method natively integrates conditioning without the
parameter and memory bloat of a duplicated ControlNet, greatly reducing
parameter count and GPU memory usage. Evaluated on the SAKUGA dataset,
SketchColour outperforms previous state-of-the-art video colourization methods
across all metrics, despite using only half the training data of competing
models. Our approach produces temporally coherent animations with minimal
artifacts such as colour bleeding or object deformation. Our code is available
at: https://bconstantine.github.io/SketchColour .

</details>


### [101] [Towards Controllable Real Image Denoising with Camera Parameters](https://arxiv.org/abs/2507.01587)
*Youngjin Oh,Junhyeong Kwon,Keuntek Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: The paper introduces a camera-parameter-based control for image denoising, improving performance and adjustability.


<details>
  <summary>Details</summary>
Motivation: Many existing image denoising methods lack flexibility in adjusting denoising strength based on noise levels and preferences.

Method: The proposed framework integrates camera parameters (ISO, shutter speed, F-number) into a vector to control and enhance a denoising neural network.

Result: The method successfully adds controllability to denoising networks and improves their performance.

Conclusion: Camera-based parameters effectively enable controllable and improved image denoising, validated by experimental results.

Abstract: Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these selected parameters into a vector to
control and enhance the performance of the denoising network. Experimental
results show that our method seamlessly adds controllability to standard
denoising neural networks and improves their performance. Code is available at
https://github.com/OBAKSA/CPADNet.

</details>


### [102] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/abs/2507.01590)
*Ameer Hamza,Zuhaib Hussain But,Umar Arif,Samiya,M. Abdullah Asad,Muhammad Naeem*

Main category: cs.CV

TL;DR: The study introduces a multi-modal classroom monitoring system combining advanced technologies for tracking student attentiveness, integrating facial recognition, drowsiness detection, and mobile phone tracking.


<details>
  <summary>Details</summary>
Motivation: To create a more precise and comprehensive classroom monitoring system for assessing student engagement and attentiveness.

Method: The system integrates YOLOv8 for mobile phone and drowsiness detection, LResNet Occ FC and MTCNN for facial recognition, and uses specialized datasets for training. It operates through a core PHP web application and ESP32-CAM hardware.

Result: Achieved high accuracy in multiple areas: 97.42% mAP@50 for sleep detection, 86.45% validation accuracy for face recognition, and 85.89% mAP@50 for mobile phone detection.

Conclusion: The system provides effective real-time monitoring and automatic attendance recording suitable for diverse educational settings, improving classroom management and engagement insights.

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [103] [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/abs/2507.01603)
*Yue-Jiang Dong,Wang Zhao,Jiale Xu,Ying Shan,Song-Hai Zhang*

Main category: cs.CV

TL;DR: DepthSync is a training-free framework designed to enhance scale and geometric consistency in video depth estimations, especially for long videos.


<details>
  <summary>Details</summary>
Motivation: Challenges in existing video depth estimation methods arise from accumulated scale discrepancies in sliding windows and the lack of 3D geometric consideration.

Method: DepthSync introduces scale guidance for consistent depth scales across video windows and geometry guidance for enforcing 3D-inspired depth alignment.

Result: Experiments demonstrate that DepthSync improves scale and geometry consistency in video depth estimations on various datasets, particularly for longer videos.

Conclusion: DepthSync provides a robust, training-free solution to achieve consistent depth predictions, addressing key challenges of scale and geometry in long video sequences.

Abstract: Diffusion-based video depth estimation methods have achieved remarkable
success with strong generalization ability. However, predicting depth for long
videos remains challenging. Existing methods typically split videos into
overlapping sliding windows, leading to accumulated scale discrepancies across
different windows, particularly as the number of windows increases.
Additionally, these methods rely solely on 2D diffusion priors, overlooking the
inherent 3D geometric structure of video depths, which results in geometrically
inconsistent predictions. In this paper, we propose DepthSync, a novel,
training-free framework using diffusion guidance to achieve scale- and
geometry-consistent depth predictions for long videos. Specifically, we
introduce scale guidance to synchronize the depth scale across windows and
geometry guidance to enforce geometric alignment within windows based on the
inherent 3D constraints in video depths. These two terms work synergistically,
steering the denoising process toward consistent depth predictions. Experiments
on various datasets validate the effectiveness of our method in producing depth
estimates with improved scale and geometry consistency, particularly for long
videos.

</details>


### [104] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/abs/2507.01607)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi,Eric Bourbao*

Main category: cs.CV

TL;DR: This paper explores vulnerabilities in deep learning face recognition systems and demonstrates new backdoor attack methods across various pipeline configurations.


<details>
  <summary>Details</summary>
Motivation: To address the lack of research on real-world vulnerabilities of deep learning-based face recognition systems, particularly focusing on backdoor attacks in unconstrained settings.

Method: The study proposes two novel backdoor attacksâface generation and face landmark shiftâwhile analyzing their impact on face detection and feature extraction tasks within diverse pipeline configurations.

Result: The authors show that backdoor attacks can entirely bypass a face recognition system's function across 20 pipeline configurations and 15 attack scenarios.

Conclusion: The work highlights the risks associated with DNN backdoors in face recognition systems and provides guidelines for stakeholders to mitigate these vulnerabilities.

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [105] [Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference](https://arxiv.org/abs/2507.01608)
*Xu Zhang,Ming Lu,Yan Chen,Zhan Ma*

Main category: cs.CV

TL;DR: The paper presents POLC, a semantic-rich latent coding method that improves compressed domain semantic inference while reducing fine-tuning overhead.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address limitations in compressed domain semantic inference, where MSE-optimized models lack semantic richness and are computationally intensive for fine-tuning.

Method: The proposed POLC method enriches latent features for semantic inference and uses a plug-and-play adapter instead of extensive fine-tuning.

Result: POLC delivers rate-perception performance comparable to the best generative coding methods while significantly improving vision task performance with minimal computational overhead.

Conclusion: POLC offers an efficient and effective approach to compressed domain semantic inference, combining high performance with reduced fine-tuning requirements.

Abstract: In recent years, compressed domain semantic inference has primarily relied on
learned image coding models optimized for mean squared error (MSE). However,
MSE-oriented optimization tends to yield latent spaces with limited semantic
richness, which hinders effective semantic inference in downstream tasks.
Moreover, achieving high performance with these models often requires
fine-tuning the entire vision model, which is computationally intensive,
especially for large models. To address these problems, we introduce
Perception-Oriented Latent Coding (POLC), an approach that enriches the
semantic content of latent features for high-performance compressed domain
semantic inference. With the semantically rich latent space, POLC requires only
a plug-and-play adapter for fine-tuning, significantly reducing the parameter
count compared to previous MSE-oriented methods. Experimental results
demonstrate that POLC achieves rate-perception performance comparable to
state-of-the-art generative image coding methods while markedly enhancing
performance in vision tasks, with minimal fine-tuning overhead. Code is
available at https://github.com/NJUVISION/POLC.

</details>


### [106] [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/abs/2507.01630)
*Yuxiao Wang,Yu Lei,Zhenao Wei,Weiying Xue,Xinyu Jiang,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: The paper introduces P3HOT, a Human-Object Contact detection framework using prompt-guided semantic mechanisms and depth perception to improve accuracy and consistency over benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Current models for Human-Object Contact detection face challenges such as over-segmentation, ineffective handling of areas with minimal interaction, and lack of category consistency within regions.

Method: The approach combines a prompt-driven semantic mechanism for targeting relevant areas and human proximal depth perception for better spatial understanding, alongside the introduction of RJLoss and AD-Acc metric for better evaluation.

Result: P3HOT achieves state-of-the-art outcomes on multiple metrics, including SC-Acc., mIoU, wIoU, and AD-Acc., with significant improvements demonstrated on the HOT-Annotated dataset.

Conclusion: The model effectively addresses limitations in existing HOT detection methods, offering better interaction identification and resolving overlapping uncertainties with a quasi-3D viewpoint.

Abstract: The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.

</details>


### [107] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: This paper presents Snake-NeRF, a scalable framework for 3D reconstruction from large-scale satellite imagery using Neural Radiance Fields (NeRF).


<details>
  <summary>Details</summary>
Motivation: State-of-the-art NeRF methods struggle with memory limitations during training, limiting their applications to small-scale scenes. The authors aim to address these scalability challenges.

Method: The authors divide the region of interest into overlapping 3D tiles, ensuring pixels are included without reconstruction errors. They introduce a $2\times2$ 3D tile progression strategy and segmented sampler to enhance reconstruction quality.

Result: Snake-NeRF eliminates the need for simultaneous loading of all images and networks. It processes large satellite images on a single GPU with linear time complexity and retains high reconstruction quality.

Conclusion: Snake-NeRF offers an effective, scalable solution for processing large-scale 3D satellite imagery using a single device, overcoming memory constraints.

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [108] [Depth Anything at Any Condition](https://arxiv.org/abs/2507.01634)
*Boyuan Sun,Modi Jin,Bowen Yin,Qibin Hou*

Main category: cs.CV

TL;DR: DepthAnything-AC is a monocular depth estimation (MDE) model capable of performing well across challenging environmental conditions, utilizing a novel finetuning paradigm and spatial constraints.


<details>
  <summary>Details</summary>
Motivation: Previous models struggled with depth estimation in complex environments involving lighting changes, adverse weather, and image distortions due to both data scarcity and poor pseudo-label quality.

Method: The paper introduces an unsupervised consistency regularization finetuning approach combined with a Spatial Distance Constraint to enhance patch-level depth relationships in images.

Result: Experimental results show that DepthAnything-AC performs robustly in zero-shot settings across benchmarks like adverse weather, synthetic corruptions, and general datasets.

Conclusion: DepthAnything-AC demonstrates enhanced depth estimation capabilities in diverse and challenging environmental conditions, setting a foundation for broader applications.

Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation
monocular depth estimation (MDE) model capable of handling diverse
environmental conditions. Previous foundation MDE models achieve impressive
performance across general scenes but not perform well in complex open-world
environments that involve challenging conditions, such as illumination
variations, adverse weather, and sensor-induced distortions. To overcome the
challenges of data scarcity and the inability of generating high-quality
pseudo-labels from corrupted images, we propose an unsupervised consistency
regularization finetuning paradigm that requires only a relatively small amount
of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to
explicitly enforce the model to learn patch-level relative relationships,
resulting in clearer semantic boundaries and more accurate details.
Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC
across diverse benchmarks, including real-world adverse weather benchmarks,
synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC

</details>


### [109] [SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement](https://arxiv.org/abs/2507.01643)
*Weijie Yin,Dingkang Yang,Hongyuan Dong,Zijian Kang,Jiacong Wang,Xiao Liang,Chao Feng,Jiao Ran*

Main category: cs.CV

TL;DR: The paper proposes SAILViT, a Vision Transformer (ViT) design aimed at improving Multimodal Large Language Models (MLLMs) by enhancing feature learning and alignment, solving issues related to parameter conflicts and semantic gaps.


<details>
  <summary>Details</summary>
Motivation: ViTs, while strong in vision-related tasks, face challenges in integrating with LLMs due to initialization conflicts and semantic differences.

Method: The authors introduce SAILViT, a gradual feature learning-enhanced ViT that performs coarse-to-fine-grained feature alignment and integrates world knowledge into the learning process.

Result: SAILViT demonstrates robust performance across varying parameter sizes, architectures, and data scales, resulting in notable improvements for MLLMs on the OpenCompass benchmark.

Conclusion: SAILViT proves effective in overcoming multimodal integration challenges and enhances the performance of Multimodal Large Language Models on diverse tasks.

Abstract: Vision Transformers (ViTs) are essential as foundation backbones in
establishing the visual comprehension capabilities of Multimodal Large Language
Models (MLLMs). Although most ViTs achieve impressive performance through
image-text pair-based contrastive learning or self-supervised mechanisms, they
struggle to engage in connector-based co-training directly with LLMs due to
potential parameter initialization conflicts and modality semantic gaps. To
address the above challenges, this paper proposes SAILViT, a gradual feature
learning-enhanced ViT for facilitating MLLMs to break through performance
bottlenecks in complex multimodal interactions. SAILViT achieves
coarse-to-fine-grained feature alignment and world knowledge infusion with
gradual feature refinement, which better serves target training demands. We
perform thorough empirical analyses to confirm the powerful robustness and
generalizability of SAILViT across different dimensions, including parameter
sizes, model architectures, training strategies, and data scales. Equipped with
SAILViT, existing MLLMs show significant and consistent performance
improvements on the OpenCompass benchmark across extensive downstream tasks.
SAILViT series models are released at
https://huggingface.co/BytedanceDouyinContent.

</details>


### [110] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: The paper introduces LASAD, a novel linear attention mechanism tailored for image generation, and LASADGen, an autoregressive image generator, which achieves efficient and high-quality image generation by accounting for true 2D spatial relationships.


<details>
  <summary>Details</summary>
Motivation: Traditional autoregressive models for image generation face computational inefficiencies due to quadratic complexity in transformers. Attempts to incorporate linear attention mechanisms compromise image quality due to insufficient handling of long-range spatial dependencies.

Method: The paper introduces LASAD, a mechanism that computes position-dependent decay factors while preserving genuine 2D spatial relationships, to enable efficient and selective attention. LASAD is incorporated into LASADGen, a new autoregressive image generation framework.

Result: LASADGen achieves state-of-the-art image generation results on ImageNet with enhanced computational efficiency, addressing challenges in both performance and memory overhead.

Conclusion: LASAD successfully bridges the gap between linear attention efficiency and spatial understanding, demonstrating a promising advancement for autoregressive image generation models.

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [111] [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/abs/2507.01653)
*Yuran Wang,Yingping Liang,Yutao Hu,Ying Fu*

Main category: cs.CV

TL;DR: This paper introduces RobuSTereo, a framework addressing data scarcity and feature extraction challenges in stereo matching under adverse weather conditions.


<details>
  <summary>Details</summary>
Motivation: Stereo matching models underperform in challenging weather due to data limitations and degraded feature extraction.

Method: Proposes a diffusion-based simulation pipeline with stereo consistency and a robust feature encoder integrating ConvNet with a denoising transformer.

Result: RobuSTereo enhances robustness and zero-shot generalization to unseen adverse weather scenarios.

Conclusion: The framework improves disparity estimation accuracy and mitigates the effects of weather-induced distortions.

Abstract: Learning-based stereo matching models struggle in adverse weather conditions
due to the scarcity of corresponding training data and the challenges in
extracting discriminative features from degraded images. These limitations
significantly hinder zero-shot generalization to out-of-distribution weather
conditions. In this paper, we propose \textbf{RobuSTereo}, a novel framework
that enhances the zero-shot generalization of stereo matching models under
adverse weather by addressing both data scarcity and feature extraction
challenges. First, we introduce a diffusion-based simulation pipeline with a
stereo consistency module, which generates high-quality stereo data tailored
for adverse conditions. By training stereo matching models on our synthetic
datasets, we reduce the domain gap between clean and degraded images,
significantly improving the models' robustness to unseen weather conditions.
The stereo consistency module ensures structural alignment across synthesized
image pairs, preserving geometric integrity and enhancing depth estimation
accuracy. Second, we design a robust feature encoder that combines a
specialized ConvNet with a denoising transformer to extract stable and reliable
features from degraded images. The ConvNet captures fine-grained local
structures, while the denoising transformer refines global representations,
effectively mitigating the impact of noise, low visibility, and weather-induced
distortions. This enables more accurate disparity estimation even under
challenging visual conditions. Extensive experiments demonstrate that
\textbf{RobuSTereo} significantly improves the robustness and generalization of
stereo matching models across diverse adverse weather scenarios.

</details>


### [112] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Main category: cs.CV

TL;DR: The paper discusses the 1st W-CODA workshop at ECCV 2024, focusing on autonomous driving corner cases and multimodal perception, including talks, research papers, and a dual-track challenge.


<details>
  <summary>Details</summary>
Motivation: To address corner cases in autonomous driving and explore advanced multimodal perception techniques for making self-driving systems more reliable.

Method: Organizing a workshop including invited talks, research paper collection, and a dual-track challenge on corner case scene understanding and generation.

Result: The workshop features contributions from both academia and industry, fostering discussions and solutions for autonomous driving corner cases.

Conclusion: W-CODA acts as a pioneering platform to close the gap between cutting-edge research and the development of robust self-driving agents for handling challenging corner cases.

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [113] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/abs/2507.01654)
*Martine Hjelkrem-Tan,Marius Aasan,Gabriel Y. Arteaga,AdÃ­n RamÃ­rez Rivera*

Main category: cs.CV

TL;DR: This paper introduces Subpixel Placement of Tokens (SPoT), an innovative tokenization method removing grid-based limitations in Vision Transformers to achieve efficiency and flexibility.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiencies and limitations in tokenization strategies that confine features to discrete patch grids in Vision Transformers, hindering sparse regimes.

Method: Proposing Subpixel Placement of Tokens (SPoT) and an oracle-guided search to determine optimal subpixel token positioning.

Result: Substantial performance gains with reduced token requirements for accurate predictions, showcasing the efficiency and interpretability of SPoT.

Conclusion: SPoT redefines sparsity in Vision Transformers as an advantage, paving the way for more flexible, efficient models without grid constraints.

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


### [114] [Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2507.01673)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: The paper introduces FACET-VLM, a vision-language framework, achieving state-of-the-art results in 3D/4D facial expression and micro-expression recognition.


<details>
  <summary>Details</summary>
Motivation: To address challenges in recognizing complex spatial and temporal facial dynamics, which are crucial for applications in behavior understanding, healthcare monitoring, and human-computer interaction.

Method: The paper proposes FACET-VLM, integrating multiview facial representation with language prompts. Key methods include Cross-View Semantic Aggregation (CVSA) for fusing views, Multiview Text-Guided Fusion (MTGF) for semantic alignment, and multiview consistency loss for structural coherence.

Result: The proposed model achieves state-of-the-art accuracy on benchmarks like BU-3DFE and BP4D-Spontaneous, and shows strong performance for 4D micro-expression recognition on the 4DME dataset.

Conclusion: FACET-VLM is a high-performing and extensible solution for multimodal facial expression recognition in both posed and spontaneous scenarios, validated through extensive experiments.

Abstract: Facial expression recognition (FER) in 3D and 4D domains presents a
significant challenge in affective computing due to the complexity of spatial
and temporal facial dynamics. Its success is crucial for advancing applications
in human behavior understanding, healthcare monitoring, and human-computer
interaction. In this work, we propose FACET-VLM, a vision-language framework
for 3D/4D FER that integrates multiview facial representation learning with
semantic guidance from natural language prompts. FACET-VLM introduces three key
components: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,
Multiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,
and a multiview consistency loss to enforce structural coherence across views.
Our model achieves state-of-the-art accuracy across multiple benchmarks,
including BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend
FACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,
demonstrating strong performance in capturing subtle, short-lived emotional
cues. The extensive experimental results confirm the effectiveness and
substantial contributions of each individual component within the framework.
Overall, FACET-VLM offers a robust, extensible, and high-performing solution
for multimodal FER in both posed and spontaneous settings.

</details>


### [115] [Component Adaptive Clustering for Generalized Category Discovery](https://arxiv.org/abs/2507.01711)
*Mingfu Yan,Jiancheng Huang,Yifan Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: The paper proposes AdaGCD, a contrastive learning-based framework with adaptive slot attention, to categorize known and unknown classes in image datasets without predefined class numbers.


<details>
  <summary>Details</summary>
Motivation: Traditional GCD methods rely on rigid assumptions like predefining the number of classes, which limits their adaptability to complex real-world data.

Method: AdaGCD incorporates Adaptive Slot Attention (AdaSlot) which dynamically determines the optimal number of clusters, removing predefined slot requirements, while leveraging spatial and instance-specific features for class discovery.

Result: Experiments on public and fine-grained datasets demonstrate the frameworkâs effectiveness in categorizing images, highlighting its ability to leverage spatial local information.

Conclusion: AdaGCD successfully adapts to varying data complexities in open-world scenarios, overcoming existing limitations in category discovery for unlabeled datasets.

Abstract: Generalized Category Discovery (GCD) tackles the challenging problem of
categorizing unlabeled images into both known and novel classes within a
partially labeled dataset, without prior knowledge of the number of unknown
categories. Traditional methods often rely on rigid assumptions, such as
predefining the number of classes, which limits their ability to handle the
inherent variability and complexity of real-world data. To address these
shortcomings, we propose AdaGCD, a cluster-centric contrastive learning
framework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD
framework. AdaSlot dynamically determines the optimal number of slots based on
data complexity, removing the need for predefined slot counts. This adaptive
mechanism facilitates the flexible clustering of unlabeled data into known and
novel categories by dynamically allocating representational capacity. By
integrating adaptive representation with dynamic slot allocation, our method
captures both instance-specific and spatially clustered features, improving
class discovery in open-world scenarios. Extensive experiments on public and
fine-grained datasets validate the effectiveness of our framework, emphasizing
the advantages of leveraging spatial local information for category discovery
in unlabeled image datasets.

</details>


### [116] [Using Wavelet Domain Fingerprints to Improve Source Camera Identification](https://arxiv.org/abs/2507.01712)
*Xinle Tian,Matthew Nunes,Emiko Dupont,Shaunagh Downing,Freddie Lichtenstein,Matt Burns*

Main category: cs.CV

TL;DR: The paper presents an enhancement to wavelet-based sensor pattern noise (SPN) extraction by operating directly in the wavelet domain, leading to greater accuracy and faster processing.


<details>
  <summary>Details</summary>
Motivation: Improving camera fingerprinting for source identification and image forensics using SPN extraction methods, particularly in addressing limitations of current wavelet denoising approaches.

Method: Modified wavelet-based SPN extraction avoiding the inversion step, allowing comparisons directly in the wavelet domain for streamlining the process.

Result: The proposed method achieves both higher detection accuracy and significant improvements in processing speed on real-world datasets.

Conclusion: The modification to the wavelet-based approach enhances both efficiency and effectiveness in camera fingerprint detection, benefiting forensic applications.

Abstract: Camera fingerprint detection plays a crucial role in source identification
and image forensics, with wavelet denoising approaches proving to be
particularly effective in extracting sensor pattern noise (SPN). In this
article, we propose a modification to wavelet-based SPN extraction. Rather than
constructing the fingerprint as an image, we introduce the notion of a wavelet
domain fingerprint. This avoids the final inversion step of the denoising
algorithm and allows fingerprint comparisons to be made directly in the wavelet
domain. As such, our modification streamlines the extraction and comparison
process. Experimental results on real-world datasets demonstrate that our
method not only achieves higher detection accuracy but can also significantly
improve processing speed.

</details>


### [117] [Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation](https://arxiv.org/abs/2507.01721)
*Zhongwen Zhang,Yuri Boykov*

Main category: cs.CV

TL;DR: The paper proposes a soft self-labeling approach combined with CRF relaxations to improve weakly supervised segmentation, surpassing both traditional supervision and complex WSSS systems.


<details>
  <summary>Details</summary>
Motivation: Address limitations in weakly supervised segmentation, particularly overcoming representation errors and uncertainties inherent in hard pseudo-labeling.

Method: Develop an auxiliary loss with both standard and novel CRF relaxations, and use soft pseudo-labels integrated with network predictions for training. Incorporate a general continuous sub-problem solver.

Result: Soft self-labeling improves scribble-based training, surpasses pixel-precise supervision, and outperforms more complex WSSS systems.

Conclusion: The authors highlight the superiority of soft self-labeling for weakly supervised systems, advocating its adaptability to a wider range of problems.

Abstract: We consider weakly supervised segmentation where only a fraction of pixels
have ground truth labels (scribbles) and focus on a self-labeling approach
optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled
pixels. While WSSS methods can directly optimize such losses via gradient
descent, prior work suggests that higher-order optimization can improve network
training by introducing hidden pseudo-labels and powerful CRF sub-problem
solvers, e.g. graph cut. However, previously used hard pseudo-labels can not
represent class uncertainty or errors, which motivates soft self-labeling. We
derive a principled auxiliary loss and systematically evaluate standard and new
CRF relaxations (convex and non-convex), neighborhood systems, and terms
connecting network predictions with soft pseudo-labels. We also propose a
general continuous sub-problem solver. Using only standard architectures, soft
self-labeling consistently improves scribble-based training and outperforms
significantly more complex specialized WSSS systems. It can outperform full
pixel-precise supervision. Our general ideas apply to other weakly-supervised
problems/systems.

</details>


### [118] [When Does Pruning Benefit Vision Representations?](https://arxiv.org/abs/2507.01722)
*Enrico Cassano,Riccardo Renzulli,Andrea Bragagnolo,Marco Grangetto*

Main category: cs.CV

TL;DR: The study examines how pruning affects vision models concerning interpretability, object discovery, and alignment with human perception, finding "sweet spots" where sparse models improve representations but depend on architecture and size.


<details>
  <summary>Details</summary>
Motivation: Deep learning models are increasingly complex; pruning can reduce their size and computational demands, but its impact on interpretability and representation quality needs exploration.

Method: The authors analyze how varying levels of sparsity influence interpretability, structured representations, object discovery, and alignment with human perception across different vision architectures.

Result: Sparse models exhibit improved interpretability, generalization, and alignment with human perception under specific conditions, such as network architecture and parameter size.

Conclusion: The study highlights a nuanced relationship between sparsity and representation quality, emphasizing the need to fine-tune pruning to optimize interpretability, generalization, and alignment with human perception.

Abstract: Pruning is widely used to reduce the complexity of deep learning models, but
its effects on interpretability and representation learning remain poorly
understood. This paper investigates how pruning influences vision models across
three key dimensions: (i) interpretability, (ii) unsupervised object discovery,
and (iii) alignment with human perception. We first analyze different vision
network architectures to examine how varying sparsity levels affect feature
attribution interpretability methods. Additionally, we explore whether pruning
promotes more succinct and structured representations, potentially improving
unsupervised object discovery by discarding redundant information while
preserving essential features. Finally, we assess whether pruning enhances the
alignment between model representations and human perception, investigating
whether sparser models focus on more discriminative features similarly to
humans. Our findings also reveal the presence of sweet spots, where sparse
models exhibit higher interpretability, downstream generalization and human
alignment. However, these spots highly depend on the network architectures and
their size in terms of trainable parameters. Our results suggest a complex
interplay between these three dimensions, highlighting the importance of
investigating when and how pruning benefits vision representations.

</details>


### [119] [HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion](https://arxiv.org/abs/2507.01737)
*Lin Wu,Zhixiang Chen,Jianglin Lan*

Main category: cs.CV

TL;DR: HOI-Dyn, a transformer-based model, enhances 3D human-object interaction realism by modeling dynamics between humans and objects through a novel driver-responder framework.


<details>
  <summary>Details</summary>
Motivation: Existing approaches fail to achieve realistic physical and causal 3D human-object interactions due to the independent treatment of human and object motions.

Method: The proposed HOI-Dyn employs a transformer-based interaction dynamics model, which predicts object behavior in response to human motion, complemented by a residual-based dynamics loss to address prediction errors.

Result: The experiments show improved quality in human-object interaction generation and introduce a new metric for evaluating interaction quality.

Conclusion: HOI-Dyn successfully offers a more consistent methodology for generating realistic human-object interaction and enhances evaluation practices.

Abstract: Generating realistic 3D human-object interactions (HOIs) remains a
challenging task due to the difficulty of modeling detailed interaction
dynamics. Existing methods treat human and object motions independently,
resulting in physically implausible and causally inconsistent behaviors. In
this work, we present HOI-Dyn, a novel framework that formulates HOI generation
as a driver-responder system, where human actions drive object responses. At
the core of our method is a lightweight transformer-based interaction dynamics
model that explicitly predicts how objects should react to human motion. To
further enforce consistency, we introduce a residual-based dynamics loss that
mitigates the impact of dynamics prediction errors and prevents misleading
optimization signals. The dynamics model is used only during training,
preserving inference efficiency. Through extensive qualitative and quantitative
experiments, we demonstrate that our approach not only enhances the quality of
HOI generation but also establishes a feasible metric for evaluating the
quality of generated interactions.

</details>


### [120] [DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy](https://arxiv.org/abs/2507.01738)
*Ming Dai,Wenxuan Cheng,Jiang-jiang Liu,Sen Yang,Wenxiao Cai,Yanpeng Sun,Wankou Yang*

Main category: cs.CV

TL;DR: The paper introduces DeRIS, a new framework for Referring Image Segmentation (RIS) that decomposes the task into perception and cognition modules for systematic bottleneck analysis and improves multi-modal cognitive capacities.


<details>
  <summary>Details</summary>
Motivation: Existing RIS frameworks focus on vision-language interaction but lack a systematic analysis of their primary limitations, affecting performance.

Method: DeRIS decomposes RIS into perception and cognition modules and includes a Loopback Synergy mechanism to synergize these components for better segmentation and image-text comprehension. It also introduces a data augmentation technique for addressing long-tail distribution issues.

Result: DeRIS enhances RIS performance by improving multi-modal cognitive capabilities and adapts to both single and multi-referent scenarios without needing architectural changes.

Conclusion: The proposed decomposition framework and Loopback Synergy mechanism address fundamental RIS limitations, ensuring better adaptability and segmentation performance.

Abstract: Referring Image Segmentation (RIS) is a challenging task that aims to segment
objects in an image based on natural language expressions. While prior studies
have predominantly concentrated on improving vision-language interactions and
achieving fine-grained localization, a systematic analysis of the fundamental
bottlenecks in existing RIS frameworks remains underexplored. To bridge this
gap, we propose DeRIS, a novel framework that decomposes RIS into two key
components: perception and cognition. This modular decomposition facilitates a
systematic analysis of the primary bottlenecks impeding RIS performance. Our
findings reveal that the predominant limitation lies not in perceptual
deficiencies, but in the insufficient multi-modal cognitive capacity of current
models. To mitigate this, we propose a Loopback Synergy mechanism, which
enhances the synergy between the perception and cognition modules, thereby
enabling precise segmentation while simultaneously improving robust image-text
comprehension. Additionally, we analyze and introduce a simple non-referent
sample conversion data augmentation to address the long-tail distribution issue
related to target existence judgement in general scenarios. Notably, DeRIS
demonstrates inherent adaptability to both non- and multi-referents scenarios
without requiring specialized architectural modifications, enhancing its
general applicability. The codes and models are available at
https://github.com/Dmmm1997/DeRIS.

</details>


### [121] [Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans](https://arxiv.org/abs/2507.01744)
*Benjamin Jin,Grant Mair,Joanna M. Wardlaw,Maria del C. ValdÃ©s HernÃ¡ndez*

Main category: cs.CV

TL;DR: The paper explores Vision Transformers (ViTs) within the masked autoencoder (MAE) framework for intracranial arterial calcification (IAC) segmentation, achieving better clinical applicability and performance over nnU-Net.


<details>
  <summary>Details</summary>
Motivation: The paper aims to examine the potential of Vision Transformers in 3D medical image segmentation, especially for identifying IAC on CT scans, which is associated with neurovascular diseases like stroke and dementia.

Method: The authors pre-train ViTs using a self-supervised MAE framework and fine-tune them for IAC segmentation. They leverage data from a large clinical trial (IST-3) and analyze various model configurations.

Result: Their self-supervised ViT outperforms nnU-Net by 3.2 Dice points, demonstrates the importance of low patch sizes, and shows increased robustness to slice thickness variations. This also improves clinical risk group classification by 46%.

Conclusion: ViTs trained in an MAE framework are effective for IAC segmentation, outperforming traditional convolutional methods, and show promising applications in clinical scenarios.

Abstract: Vision Transformers (ViTs) have gained significant popularity in the natural
image domain but have been less successful in 3D medical image segmentation.
Nevertheless, 3D ViTs are particularly interesting for large medical imaging
volumes due to their efficient self-supervised training within the masked
autoencoder (MAE) framework, which enables the use of imaging data without the
need for expensive manual annotations. intracranial arterial calcification
(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to
neurovascular diseases such as stroke and dementia, and automated IAC
quantification could enable their large-scale risk assessment. We pre-train
ViTs with MAE and fine-tune them for IAC segmentation for the first time. To
develop our models, we use highly heterogeneous data from a large clinical
trial, the third International Stroke Trial (IST-3). We evaluate key aspects of
MAE pre-trained ViTs in IAC segmentation, and analyse the clinical
implications. We show: 1) our calibrated self-supervised ViT beats a strong
supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial
for ViTs for IAC segmentation and interpolation upsampling with regular
convolutions is preferable to transposed convolutions for ViT-based models, and
3) our ViTs increase robustness to higher slice thicknesses and improve risk
group classification in a clinical scenario by 46%. Our code is available
online.

</details>


### [122] [SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery](https://arxiv.org/abs/2507.01747)
*Nora Gourmelon,Marcel Dreier,Martin Mayr,Thorsten Seehaus,Dakota Pyles,Matthias Braun,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: The paper develops advanced models using self-supervised learning to monitor glacier calving fronts from remote sensing imagery, achieving near-human-level precision.


<details>
  <summary>Details</summary>
Motivation: Glaciers are rapidly losing ice mass, necessitating more accurate tools for monitoring calving processes using specialized remote sensing imagery.

Method: Two self-supervised multimodal pretraining techniques and a hybrid Swin Transformer-CNN architecture are introduced, trained on the SSL4SAR dataset containing Sentinel-1 and Sentinel-2 imagery.

Result: The model achieves mean distance errors of 293 m and 75 m (ensemble), outperforming prior models by 67 m and nearing human performance (38 m).

Conclusion: The proposed techniques enhance the precision of glacier monitoring, especially for seasonal changes in calving fronts.

Abstract: Glaciers are losing ice mass at unprecedented rates, increasing the need for
accurate, year-round monitoring to understand frontal ablation, particularly
the factors driving the calving process. Deep learning models can extract
calving front positions from Synthetic Aperture Radar imagery to track seasonal
ice losses at the calving fronts of marine- and lake-terminating glaciers. The
current state-of-the-art model relies on ImageNet-pretrained weights. However,
they are suboptimal due to the domain shift between the natural images in
ImageNet and the specialized characteristics of remote sensing imagery, in
particular for Synthetic Aperture Radar imagery. To address this challenge, we
propose two novel self-supervised multimodal pretraining techniques that
leverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14
Sentinel-2 images of Arctic glaciers, with one optical image per glacier in the
dataset. Additionally, we introduce a novel hybrid model architecture that
combines a Swin Transformer encoder with a residual Convolutional Neural
Network (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean
distance error of 293 m on the "CAlving Fronts and where to Find thEm" (CaFFe)
benchmark dataset, outperforming the prior best model by 67 m. Evaluating an
ensemble of the proposed model on a multi-annotator study of the benchmark
dataset reveals a mean distance error of 75 m, approaching the human
performance of 38 m. This advancement enables precise monitoring of seasonal
changes in glacier calving fronts.

</details>


### [123] [Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis](https://arxiv.org/abs/2507.01756)
*Peng Zheng,Junke Wang,Yi Chang,Yizhou Yu,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: The paper introduces DisCon, a novel framework leveraging discrete-conditioned continuous autoregressive models for visual generation, tackling issues with quantization and continuous representation challenges.


<details>
  <summary>Details</summary>
Motivation: Explore methods to improve image fidelity in autoregressive visual generation by addressing limitations in discrete and continuous token modeling.

Method: Proposed DisCon framework models continuous representations conditioned on discrete token signals to avoid quantization problems and optimization challenges.

Result: DisCon achieved a superior gFID score of 1.38 on ImageNet 256Ã256 generation, outperforming state-of-the-art autoregressive methods significantly.

Conclusion: DisCon demonstrates the effectiveness of discrete-conditioned continuous autoregressive modeling in enhancing visual generation fidelity and setting a new performance benchmark.

Abstract: Recent advances in large language models (LLMs) have spurred interests in
encoding images as discrete tokens and leveraging autoregressive (AR)
frameworks for visual generation. However, the quantization process in AR-based
visual generation models inherently introduces information loss that degrades
image fidelity. To mitigate this limitation, recent studies have explored to
autoregressively predict continuous tokens. Unlike discrete tokens that reside
in a structured and bounded space, continuous representations exist in an
unbounded, high-dimensional space, making density estimation more challenging
and increasing the risk of generating out-of-distribution artifacts. Based on
the above findings, this work introduces DisCon (Discrete-Conditioned
Continuous Autoregressive Model), a novel framework that reinterprets discrete
tokens as conditional signals rather than generation targets. By modeling the
conditional probability of continuous representations conditioned on discrete
tokens, DisCon circumvents the optimization challenges of continuous token
modeling while avoiding the information loss caused by quantization. DisCon
achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation,
outperforming state-of-the-art autoregressive approaches by a clear margin.

</details>


### [124] [Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging](https://arxiv.org/abs/2507.01788)
*Montasir Shams,Chashi Mahiul Islam,Shaeke Salman,Phat Tran,Xiuwen Liu*

Main category: cs.CV

TL;DR: This paper investigates Vision Transformers (ViTs) in medical imaging and reveals that their representations lack semantic meaningfulness and are highly vulnerable to minor perturbations.


<details>
  <summary>Details</summary>
Motivation: To address the unclear semantic meaningfulness and reliability of Vision Transformers' representations in medical imaging tasks, where safety-critical decisions rely on such systems.

Method: A projected gradient-based algorithm was used to analyze ViTs' representations and to determine their response to slight image changes and their consistency across semantic classes.

Result: The study finds that ViTs are extremely sensitive to imperceptible image differences, resulting in significant classification accuracy drops (~60%) and unreliable mappings where similar or dissimilar images share unexpected representations.

Conclusion: ViTs' lack of semantically robust representations poses a critical challenge for their application in safety-critical medical systems, necessitating further research to address these vulnerabilities.

Abstract: Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.

</details>


### [125] [Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation](https://arxiv.org/abs/2507.01791)
*Zihong Guo,Chen Wan,Yayin Zheng,Hailing Kuang,Xiaohai Lu*

Main category: cs.CV

TL;DR: This paper introduces the Segmented Gaussian Pyramid (SGP) attack method to improve the transferability of adversarial examples against defense models.


<details>
  <summary>Details</summary>
Motivation: To address the security challenge where deep neural networks can be attacked without prior knowledge of their structure, and to enhance the transferability of such attacks to defense models.

Method: The SGP attack generates multi-scale adversarial examples by applying Gaussian filtering and three downsampling approaches. Gradients across scales are averaged to derive perturbations. This method serves as an input transformation integrated into existing attacks.

Result: SGP improves black-box defense model attack success rates, increasing accuracy by 2.3% to 32.6% over current state-of-the-art methods.

Conclusion: The proposed SGP technique significantly boosts adversarial attack performance against defense models based purely on transferability, showcasing its adaptability and practicality.

Abstract: The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale examples. Then, the gradients of the loss
function with respect to each scale are computed, and their average is used to
determine the adversarial perturbations. The proposed SGP can be considered an
input transformation with high extensibility that is easily integrated into
most existing adversarial attacks. Extensive experiments demonstrate that in
contrast to the state-of-the-art methods, SGP significantly enhances attack
success rates against black-box defense models, with average attack success
rates increasing by 2.3% to 32.6%, based only on transferability.

</details>


### [126] [FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization](https://arxiv.org/abs/2507.01792)
*Peng Zheng,Ye Wang,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: FreeLoRA is a novel framework for multi-subject image generation without requiring training, using LoRA modules and specific inference strategies.


<details>
  <summary>Details</summary>
Motivation: Existing subject-driven image generation approaches face challenges in accommodating multi-subject personalization without complex re-tuning or joint optimization.

Method: The framework applies a Full Token Tuning strategy during adaptation of LoRA modules and utilizes Subject-Aware Inference to selectively activate modules based on subject tokens, enabling training-free fusion.

Result: FreeLoRA effectively generates images with high subject fidelity and prompt consistency while minimizing overfitting and interference.

Conclusion: The proposed FreeLoRA framework offers a generalizable and training-free approach to multi-subject image generation, achieving strong and reliable performance.

Abstract: Subject-driven image generation plays a crucial role in applications such as
virtual try-on and poster design. Existing approaches typically fine-tune
pretrained generative models or apply LoRA-based adaptations for individual
subjects. However, these methods struggle with multi-subject personalization,
as combining independently adapted modules often requires complex re-tuning or
joint optimization. We present FreeLoRA, a simple and generalizable framework
that enables training-free fusion of subject-specific LoRA modules for
multi-subject personalization. Each LoRA module is adapted on a few images of a
specific subject using a Full Token Tuning strategy, where it is applied across
all tokens in the prompt to encourage weakly supervised token-content
alignment. At inference, we adopt Subject-Aware Inference, activating each
module only on its corresponding subject tokens. This enables training-free
fusion of multiple personalized subjects within a single image, while
mitigating overfitting and mutual interference between subjects. Extensive
experiments show that FreeLoRA achieves strong performance in both subject
fidelity and prompt consistency.

</details>


### [127] [HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision](https://arxiv.org/abs/2507.01800)
*Shengli Zhou,Jianuo Zhu,Qilin Huang,Fangjing Wang,Yanfu Zhang,Feng Zheng*

Main category: cs.CV

TL;DR: The paper introduces a method, HCNQA, for 3D Visual Question-Answering (VQA), ensuring models develop effective and rational reasoning pathways and achieve superior performance.


<details>
  <summary>Details</summary>
Motivation: Current 3D Visual Question-Answering models mainly rely on answer-centric supervision, which only supervises the model's output but not the reasoning process. This can result in shortcuts and underthinking.

Method: The authors propose HCNQA, a model that employs hierarchical concentration narrowing, imitating humans by gradually narrowing focus through three phases of hierarchical supervision, guiding the reasoning process.

Result: HCNQA demonstrated improved reasoning pathways and better performance in experiments compared to other approaches.

Conclusion: The proposed HCNQA method helps 3D VQA models develop rational reasoning processes, advancing their performance and reliability in 3D scene understanding.

Abstract: 3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the
physical world and perform spatial reasoning. Answer-centric supervision is a
commonly used training method for 3D VQA models. Many models that utilize this
strategy have achieved promising results in 3D VQA tasks. However, the
answer-centric approach only supervises the final output of models and allows
models to develop reasoning pathways freely. The absence of supervision on the
reasoning pathway enables the potential for developing superficial shortcuts
through common patterns in question-answer pairs. Moreover, although
slow-thinking methods advance large language models, they suffer from
underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA
model leveraging a hierarchical concentration narrowing supervision method. By
mimicking the human process of gradually focusing from a broad area to specific
objects while searching for answers, our method guides the model to perform
three phases of concentration narrowing through hierarchical supervision. By
supervising key checkpoints on a general reasoning pathway, our method can
ensure the development of a rational and effective reasoning pathway. Extensive
experimental results demonstrate that our method can effectively ensure that
the model develops a rational reasoning pathway and performs better. The code
is available at https://github.com/JianuoZhu/HCNQA.

</details>


### [128] [AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction](https://arxiv.org/abs/2507.01801)
*Bin Rao,Haicheng Liao,Yanchen Guan,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

TL;DR: The paper introduces AMD, a framework to improve trajectory prediction in autonomous driving by dealing with long-tail data challenges.


<details>
  <summary>Details</summary>
Motivation: There's a need for better accuracy in rare and complex trajectory predictions in autonomous driving, which are essential for safety.

Method: AMD integrates adaptive momentum, unsupervised and supervised contrastive learning, augmented trajectory techniques, and iterative clustering.

Result: The framework shows superior results on nuScenes and ETH/UCY datasets for both long-tail and overall prediction accuracy.

Conclusion: AMD effectively recognizes rare trajectory patterns, adapting to distributional shifts and enhancing autonomous driving safety.

Abstract: Accurately predicting the future trajectories of traffic agents is essential
in autonomous driving. However, due to the inherent imbalance in trajectory
distributions, tail data in natural datasets often represents more complex and
hazardous scenarios. Existing studies typically rely solely on a base model's
prediction error, without considering the diversity and uncertainty of
long-tail trajectory patterns. We propose an adaptive momentum and decoupled
contrastive learning framework (AMD), which integrates unsupervised and
supervised contrastive learning strategies. By leveraging an improved momentum
contrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,
our framework enhances the model's ability to recognize rare and complex
trajectories. Additionally, we design four types of trajectory random
augmentation methods and introduce an online iterative clustering strategy,
allowing the model to dynamically update pseudo-labels and better adapt to the
distributional shifts in long-tail data. We propose three different criteria to
define long-tail trajectories and conduct extensive comparative experiments on
the nuScenes and ETH$/$UCY datasets. The results show that AMD not only
achieves optimal performance in long-tail trajectory prediction but also
demonstrates outstanding overall prediction accuracy.

</details>


### [129] [Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views](https://arxiv.org/abs/2507.01835)
*Daniil Reutsky,Daniil Vladimirov,Yasin Mamedov,Georgy Perevozchikov,Nancy Mehta,Egor Ershov,Radu Timofte*

Main category: cs.CV

TL;DR: This paper introduces a multi-image-to-hyperspectral reconstruction (MI-HSR) framework using a triple-camera smartphone system, achieving better accuracy in spectral estimation compared to single-camera solutions.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral reconstruction from RGB images is challenging due to significant spectral information loss, and existing methods relying on single RGB images limit reconstruction accuracy.

Method: The authors propose leveraging a triple-camera smartphone system with carefully selected spectral filters and introduce Doomer, a dataset of aligned images from three cameras and a hyperspectral reference camera.

Result: The MI-HSR model consistently outperforms existing methods on the new benchmark, achieving a 30% improvement in spectral estimation accuracy compared to ordinary RGB cameras.

Conclusion: Multi-view spectral filtering with commodity hardware enables more accurate and practical hyperspectral imaging solutions.

Abstract: Hyperspectral reconstruction (HSR) from RGB images is a fundamentally
ill-posed problem due to severe spectral information loss. Existing approaches
typically rely on a single RGB image, limiting reconstruction accuracy. In this
work, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)
framework that leverages a triple-camera smartphone system, where two lenses
are equipped with carefully selected spectral filters. Our configuration,
grounded in theoretical and empirical analysis, enables richer and more diverse
spectral observations than conventional single-camera setups. To support this
new paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising
aligned images from three smartphone cameras and a hyperspectral reference
camera across diverse scenes. We show that the proposed HSR model achieves
consistent improvements over existing methods on the newly proposed benchmark.
In a nutshell, our setup allows 30% towards more accurately estimated spectra
compared to an ordinary RGB camera. Our findings suggest that multi-view
spectral filtering with commodity hardware can unlock more accurate and
practical hyperspectral imaging solutions.

</details>


### [130] [MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices](https://arxiv.org/abs/2507.01838)
*Hailong Yan,Ao Li,Xiangtao Zhang,Zhe Liu,Zenglin Shi,Ce Zhu,Le Zhang*

Main category: cs.CV

TL;DR: The paper presents a lightweight CNN framework designed for efficient image enhancement on mobile devices, achieving real-time inference at up to 1,100 FPS with competitive image quality.


<details>
  <summary>Details</summary>
Motivation: The paper aims to overcome challenges in deploying deep learning models for image enhancement on resource-constrained platforms, such as mobile devices, due to the high computation and memory requirements.

Method: The authors introduce a lightweight CNN framework with approximately 4K parameters, integrating reparameterization and Incremental Weight Optimization. They enhance performance using a Feature Self-Transform module, Hierarchical Dual-Path Attention, and a Local Variance-Weighted loss.

Result: The presented framework achieves real-time image enhancement inference at up to 1,100 FPS while maintaining competitive image quality, representing a superior trade-off between speed and performance in comparison to other models.

Conclusion: The developed framework advances the field of mobile image enhancement by enabling real-time processing with minimal computational resources without sacrificing image quality, paving the way for broader mobile applications.

Abstract: Recent advancements in deep neural networks have driven significant progress
in image enhancement (IE). However, deploying deep learning models on
resource-constrained platforms, such as mobile devices, remains challenging due
to high computation and memory demands. To address these challenges and
facilitate real-time IE on mobile, we introduce an extremely lightweight
Convolutional Neural Network (CNN) framework with around 4K parameters. Our
approach integrates reparameterization with an Incremental Weight Optimization
strategy to ensure efficiency. Additionally, we enhance performance with a
Feature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,
optimized with a Local Variance-Weighted loss. With this efficient framework,
we are the first to achieve real-time IE inference at up to 1,100 frames per
second (FPS) while delivering competitive image quality, achieving the best
trade-off between speed and performance across multiple IE tasks. The code will
be available at https://github.com/AVC2-UESTC/MobileIE.git.

</details>


### [131] [Future Slot Prediction for Unsupervised Object Discovery in Surgical Video](https://arxiv.org/abs/2507.01882)
*Guiqiu Liao,Matjaz Jogan,Marcel Hussing,Edward Zhang,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: The paper proposes a dynamic temporal slot transformer (DTST) for better object representation in surgical videos, surpassing prior unsupervised methods.


<details>
  <summary>Details</summary>
Motivation: Current object-centric methods struggle with parsing complex, heterogeneous surgical scenes, especially in videos.

Method: The paper introduces DTST to improve temporal reasoning and predict future slot initialization for scene interpretation.

Result: DTST achieves state-of-the-art performance on multiple surgical datasets.

Conclusion: Unsupervised object-centric methods like DTST show promise for practical healthcare applications, including surgical video analysis.

Abstract: Object-centric slot attention is an emerging paradigm for unsupervised
learning of structured, interpretable object-centric representations (slots).
This enables effective reasoning about objects and events at a low
computational cost and is thus applicable to critical healthcare applications,
such as real-time interpretation of surgical video. The heterogeneous scenes in
real-world applications like surgery are, however, difficult to parse into a
meaningful set of slots. Current approaches with an adaptive slot count perform
well on images, but their performance on surgical videos is low. To address
this challenge, we propose a dynamic temporal slot transformer (DTST) module
that is trained both for temporal reasoning and for predicting the optimal
future slot initialization. The model achieves state-of-the-art performance on
multiple surgical databases, demonstrating that unsupervised object-centric
methods can be applied to real-world data and become part of the common arsenal
in healthcare applications.

</details>


### [132] [Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification](https://arxiv.org/abs/2507.01884)
*Kunlun Xu,Fan Zhuo,Jiangmeng Li,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TL;DR: The paper introduces the Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED) to address the Semi-Supervised Lifelong Person Re-identification (Semi-LReID) problem caused by limited labeled data and noisy pseudo-labels.


<details>
  <summary>Details</summary>
Motivation: To improve performance in Semi-Supervised LReID scenarios, where labeled data is scarce and unlabeled data is abundant, and existing methods struggle with noisy pseudo-labels.

Method: Establishing a self-reinforcing cycle between dynamic prototype-guided pseudo-label generation and refining pseudo-labels through current model specialization and historical model generalization.

Result: SPRED achieves state-of-the-art performance on Semi-LReID benchmarks.

Conclusion: The framework progressively enhances pseudo-label reliability, enabling superior long-term learning and addressing performance degradation challenges.

Abstract: Current lifelong person re-identification (LReID) methods predominantly rely
on fully labeled data streams. However, in real-world scenarios where
annotation resources are limited, a vast amount of unlabeled data coexists with
scarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)
problem where LReID methods suffer severe performance degradation. Existing
LReID methods, even when combined with semi-supervised strategies, suffer from
limited long-term adaptation performance due to struggling with the noisy
knowledge occurring during unlabeled data utilization. In this paper, we
pioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing
Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key
innovation lies in establishing a self-reinforcing cycle between dynamic
prototype-guided pseudo-label generation and new-old knowledge collaborative
purification to enhance the utilization of unlabeled data. Specifically,
learnable identity prototypes are introduced to dynamically capture the
identity distributions and generate high-quality pseudo-labels. Then, the
dual-knowledge cooperation scheme integrates current model specialization and
historical model generalization, refining noisy pseudo-labels. Through this
cyclic design, reliable pseudo-labels are progressively mined to improve
current-stage learning and ensure positive knowledge propagation over long-term
learning. Experiments on the established Semi-LReID benchmarks show that our
SPRED achieves state-of-the-art performance. Our source code is available at
https://github.com/zhoujiahuan1991/ICCV2025-SPRED

</details>


### [133] [Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning](https://arxiv.org/abs/2507.01908)
*Qingdong He,Xueqin Chen,Chaoyi Wang,Yanjie Pan,Xiaobin Hu,Zhenye Gan,Yabiao Wang,Chengjie Wang,Xiangtai Li,Jiangning Zhang*

Main category: cs.CV

TL;DR: The paper introduces Reason50K, a dataset for reasoning-aware image editing, and ReasonBrain, a framework integrating multimodal models for handling complex hypothetical instructions.


<details>
  <summary>Details</summary>
Motivation: Existing image editing methods struggle with complex implicit hypothetical instructions requiring deeper reasoning and intent understanding, along with limited training datasets.

Method: The authors propose Reason50K, a dataset of over 50K samples with four reasoning categories, and ReasonBrain, which uses Multimodal Large Language Models, diffusion models, and specialized modules for instruction reasoning.

Result: ReasonBrain outperforms state-of-the-art methods on reasoning scenarios and exhibits strong zero-shot generalization to traditional image editing tasks.

Conclusion: ReasonBrain and Reason50K enhance IIE capabilities for hypothetical instructions, and their public release aims to advance research in reasoning-aware image editing.

Abstract: Instruction-based image editing (IIE) has advanced rapidly with the success
of diffusion models. However, existing efforts primarily focus on simple and
explicit instructions to execute editing operations such as adding, deleting,
moving, or swapping objects. They struggle to handle more complex implicit
hypothetical instructions that require deeper reasoning to infer plausible
visual changes and user intent. Additionally, current datasets provide limited
support for training and evaluating reasoning-aware editing capabilities.
Architecturally, these methods also lack mechanisms for fine-grained detail
extraction that support such reasoning. To address these limitations, we
propose Reason50K, a large-scale dataset specifically curated for training and
evaluating hypothetical instruction reasoning image editing, along with
ReasonBrain, a novel framework designed to reason over and execute implicit
hypothetical instructions across diverse scenarios. Reason50K includes over 50K
samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and
Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)
for editing guidance generation and a diffusion model for image synthesis,
incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture
detailed visual and textual semantics essential for supporting instruction
reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal
Enhancer (CME) that enables rich interactions between the fine-grained cues and
MLLM-derived features. Extensive experiments demonstrate that ReasonBrain
consistently outperforms state-of-the-art baselines on reasoning scenarios
while exhibiting strong zero-shot generalization to conventional IIE tasks. Our
dataset and code will be released publicly.

</details>


### [134] [Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion](https://arxiv.org/abs/2507.01909)
*Jorge Tapias Gomez,Nishant Nadkarni,Lando S. Bosma,Jue Jiang,Ergys D. Subashi,William P. Segars,James M. Balter,Mert R Sabuncu,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: This paper introduces patient-specific digital twins (DTs) to evaluate the spatial accuracy of deformable image registration (DIR) methods in gastrointestinal (GI) organ motion scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the lack of voxel-based accuracy metrics in highly mobile GI organs, especially for evaluating DIR methods' precision in dynamic anatomical scenarios.

Method: A semi-automated pipeline was developed to create patient-specific 4D DTs from static 3D scans using analytical GI motion models. These DTs were used to evaluate six DIR methods with quantitative metrics and dosimetric accuracy assessments.

Result: The proposed approach generated realistic DTs that closely matched real-patient gastric motion data, enabling detailed assessments of DIR accuracy and dose mapping performance in GI regions.

Conclusion: This pipeline provides a new framework for testing DIR methodologies in anatomically complex and dynamic regions, contributing to improved spatial and dosimetric accuracy in clinical settings.

Abstract: Objective: Clinical implementation of deformable image registration (DIR)
requires voxel-based spatial accuracy metrics such as manually identified
landmarks, which are challenging to implement for highly mobile
gastrointestinal (GI) organs. To address this, patient-specific digital twins
(DT) modeling temporally varying motion were created to assess the accuracy of
DIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D
sequences were generated from static 3D patient scans using published
analytical GI motion models through a semi-automated pipeline. Eleven datasets,
including six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,
and three contrast-enhanced CT scans. The motion amplitudes of the DTs were
assessed against real patient stomach motion amplitudes extracted from
independent 4D MRI datasets. The generated DTs were then used to assess six
different DIR methods using target registration error, Dice similarity
coefficient, and the 95th percentile Hausdorff distance using summary metrics
and voxel-level granular visualizations. Finally, for a subset of T2w MRI scans
from patients treated with MR-guided radiation therapy, dose distributions were
warped and accumulated to assess dose warping errors, including evaluations of
DIR performance in both low- and high-dose regions for patient-specific error
estimation. Main results: Our proposed pipeline synthesized DTs modeling
realistic GI motion, achieving mean and maximum motion amplitudes and a mean
log Jacobian determinant within 0.8 mm and 0.01, respectively, similar to
published real-patient gastric motion data. It also enables the extraction of
detailed quantitative DIR performance metrics and rigorous validation of dose
mapping accuracy. Significance: The pipeline enables rigorously testing DIR
tools for dynamic, anatomically complex regions enabling granular spatial and
dosimetric accuracies.

</details>


### [135] [3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP](https://arxiv.org/abs/2507.01912)
*Ranjan Sapkota,Zhichao Meng,Martin Churuvija,Xiaoqiang Du,Zenghong Ma,Manoj Karkee*

Main category: cs.CV

TL;DR: The paper develops a multi-seasonal information fusion framework using advanced imaging and reconstruction techniques to overcome visibility challenges in orchard automation caused by dense foliage.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address visibility challenges in orchard automation during the canopy season, which hampers machine vision systems, by leveraging defoliated tree structures in the dormant season.

Method: The framework integrates RGB-D imagery, YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D reconstruction, and Fast GICP for multi-season alignment to develop coherent models of tree structures.

Result: The YOLOv9-Seg achieved segmentation mAP@50 of 0.78 and low MSE of 0.0047. Kinect Fusion attained geometry reconstruction with RMSEs of 5.23 mm (trunk diameter), 4.5 mm (branch diameter), and 13.72 mm (branch spacing). Fast GICP yielded precise cross-season alignment with a minimum fitness score of 0.00197.

Conclusion: The fused structural model enhances robotic systems' ability to access tree structural information throughout the growing season, benefiting operations like pruning and thinning during heavy foliage periods.

Abstract: In orchard automation, dense foliage during the canopy season severely
occludes tree structures, minimizing visibility to various canopy parts such as
trunks and branches, which limits the ability of a machine vision system.
However, canopy structure is more open and visible during the dormant season
when trees are defoliated. In this work, we present an information fusion
framework that integrates multi-seasonal structural data to support robotic and
automated crop load management during the entire growing season. The framework
combines high-resolution RGB-D imagery from both dormant and canopy periods
using YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D
reconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for
model alignment. Segmentation outputs from YOLOv9-Seg were used to extract
depth-informed masks, which enabled accurate 3D point cloud reconstruction via
Kinect Fusion; these reconstructed models from each season were subsequently
aligned using Fast GICP to achieve spatially coherent multi-season fusion. The
YOLOv9-Seg model, trained on manually annotated images, achieved a mean squared
error (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in
dormant season dataset. Kinect Fusion enabled accurate reconstruction of tree
geometry, validated with field measurements resulting in root mean square
errors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and
13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal
registration with a minimum fitness score of 0.00197, allowing integrated,
comprehensive tree structure modeling despite heavy occlusions during the
growing season. This fused structural representation enables robotic systems to
access otherwise obscured architectural information, improving the precision of
pruning, thinning, and other automated orchard operations.

</details>


### [136] [IC-Custom: Diverse Image Customization via In-Context Learning](https://arxiv.org/abs/2507.01926)
*Yaowei Li,Xiaoyu Li,Zhaoyang Zhang,Yuxuan Bian,Gan Liu,Xinyuan Li,Jiale Xu,Wenbo Hu,Yating Liu,Lingen Li,Jing Cai,Yuexian Zou,Yancheng He,Ying Shan*

Main category: cs.CV

TL;DR: IC-Custom integrates position-aware and position-free image customization via in-context learning, outperforming existing methods while training a fraction of parameters.


<details>
  <summary>Details</summary>
Motivation: Existing image customization methods separate customization into distinct paradigms, limiting their adaptability across diverse applications.

Method: IC-Custom leverages polyptych configurations and introduces In-context Multi-Modal Attention (ICMA) for precise token-level interactions with curated datasets.

Result: IC-Custom significantly outperforms existing workflows and models, achieving a 73% uplift in human preference metrics like identity consistency and text alignment.

Conclusion: IC-Custom is a groundbreaking framework offering unified, efficient customization for various industrial applications, demonstrating superior performance and adaptability.

Abstract: Image customization, a crucial technique for industrial media production,
aims to generate content that is consistent with reference images. However,
current approaches conventionally separate image customization into
position-aware and position-free customization paradigms and lack a universal
framework for diverse customization, limiting their applications across various
scenarios. To overcome these limitations, we propose IC-Custom, a unified
framework that seamlessly integrates position-aware and position-free image
customization through in-context learning. IC-Custom concatenates reference
images with target images to a polyptych, leveraging DiT's multi-modal
attention mechanism for fine-grained token-level interactions. We introduce the
In-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented
register tokens and boundary-aware positional embeddings to enable the model to
correctly handle different task types and distinguish various inputs in
polyptych configurations. To bridge the data gap, we carefully curated a
high-quality dataset of 12k identity-consistent samples with 8k from real-world
sources and 4k from high-quality synthetic data, avoiding the overly glossy and
over-saturated synthetic appearance. IC-Custom supports various industrial
applications, including try-on, accessory placement, furniture arrangement, and
creative IP customization. Extensive evaluations on our proposed ProductBench
and the publicly available DreamBench demonstrate that IC-Custom significantly
outperforms community workflows, closed-source models, and state-of-the-art
open-source approaches. IC-Custom achieves approximately 73% higher human
preference across identity consistency, harmonicity, and text alignment
metrics, while training only 0.4% of the original model parameters. Project
page: https://liyaowei-stu.github.io/project/IC_Custom

</details>


### [137] [evMLP: An Efficient Event-Driven MLP Architecture for Vision](https://arxiv.org/abs/2507.01927)
*Zhentan Zheng*

Main category: cs.CV

TL;DR: The paper introduces evMLP, an efficient architecture for vision tasks using an event-driven local update mechanism that selectively processes image patches with changes.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore innovative vision architectures beyond CNNs and ViTs, focusing on improving computational efficiency, especially for sequential image data like videos.

Method: evMLP processes image patches using MLPs and utilizes an event-driven local update mechanism, where only patches with significant changes ('events') between consecutive frames are processed, reducing redundancy.

Result: Experiments show evMLP achieves competitive accuracy on ImageNet while significantly reducing computational cost, maintaining consistency with the non-event-driven baseline in video processing.

Conclusion: evMLP effectively balances accuracy and efficiency, presenting a promising approach for computationally demanding tasks like video data processing.

Abstract: Deep neural networks have achieved remarkable results in computer vision
tasks. In the early days, Convolutional Neural Networks (CNNs) were the
mainstream architecture. In recent years, Vision Transformers (ViTs) have
become increasingly popular. In addition, exploring applications of multi-layer
perceptrons (MLPs) has provided new perspectives for research into vision model
architectures. In this paper, we present evMLP accompanied by a simple
event-driven local update mechanism. The proposed evMLP can independently
process patches on images or feature maps via MLPs. We define changes between
consecutive frames as "events". Under the event-driven local update mechanism,
evMLP selectively processes patches where events occur. For sequential image
data (e.g., video processing), this approach improves computational performance
by avoiding redundant computations. Through ImageNet image classification
experiments, evMLP attains accuracy competitive with state-of-the-art models.
More significantly, experimental results on multiple video datasets demonstrate
that evMLP reduces computational cost via its event-driven local update
mechanism while maintaining output consistency with its non-event-driven
baseline. The code and trained models are available at
https://github.com/i-evi/evMLP.

</details>


### [138] [CI-VID: A Coherent Interleaved Text-Video Dataset](https://arxiv.org/abs/2507.01938)
*Yiming Ju,Jijin Hu,Zhengxiong Luo,Haoge Deng,hanyu Zhao,Li Du,Chengwei Wu,Donglin Hao,Xinlong Wang,Tengfei Pan*

Main category: cs.CV

TL;DR: The paper introduces CI-VID, a dataset for generating coherent multi-scene video sequences using text and videos, addressing limitations in existing datasets with isolated text-video pairs.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of existing text-to-video datasets, which fail to support coherent multi-clip video sequence modeling, and to enable the generation of story-driven sequences with smooth transitions.

Method: The authors introduce CI-VID, a dataset containing over 340,000 text-captioned coherent video sequences. They also develop a multi-dimensional benchmark that uses human evaluation, VLM-based assessment, and similarity-based metrics to validate the dataset's effectiveness.

Result: Experiments show that models trained on CI-VID demonstrate improved accuracy and consistency in generating video sequences, achieving smooth transitions and enhanced temporal coherence.

Conclusion: CI-VID enhances the quality and utility of text-to-video generation, allowing for the creation of coherent, story-driven content. The dataset and its accompanying code are publicly released for broader community use.

Abstract: Text-to-video (T2V) generation has recently attracted considerable attention,
resulting in the development of numerous high-quality datasets that have
propelled progress in this area. However, existing public datasets are
primarily composed of isolated text-video (T-V) pairs and thus fail to support
the modeling of coherent multi-clip video sequences. To address this
limitation, we introduce CI-VID, a dataset that moves beyond isolated
text-to-video (T2V) generation toward text-and-video-to-video (TV2V)
generation, enabling models to produce coherent, multi-scene video sequences.
CI-VID contains over 340,000 samples, each featuring a coherent sequence of
video clips with text captions that capture both the individual content of each
clip and the transitions between them, enabling visually and textually grounded
generation. To further validate the effectiveness of CI-VID, we design a
comprehensive, multi-dimensional benchmark incorporating human evaluation,
VLM-based assessment, and similarity-based metrics. Experimental results
demonstrate that models trained on CI-VID exhibit significant improvements in
both accuracy and content consistency when generating video sequences. This
facilitates the creation of story-driven content with smooth visual transitions
and strong temporal coherence, underscoring the quality and practical utility
of the CI-VID dataset We release the CI-VID dataset and the accompanying code
for data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID

</details>


### [139] [LongAnimation: Long Animation Generation with Dynamic Global-Local Memory](https://arxiv.org/abs/2507.01945)
*Nan Chen,Mengqi Huang,Yihao Meng,Zhendong Mao*

Main category: cs.CV

TL;DR: The study addresses issues of long-term color consistency in animation colorization through a novel framework called LongAnimation.


<details>
  <summary>Details</summary>
Motivation: Animation colorization often involves high labor costs and challenges in maintaining long-term color consistency, motivating automated solutions.

Method: The authors proposed LongAnimation framework, including SketchDiT for feature capturing, Dynamic Global-Local Memory (DGLM) for feature compression and fusion, and Color Consistency Reward for refining color consistency.

Result: Experiments showed LongAnimation's efficacy in both short-term (14 frames) and long-term (500 frames) animations for ensuring color consistency.

Conclusion: LongAnimation provides a dynamic global-local approach that successfully addresses challenges in long animation colorization with consistent coloring results.

Abstract: Animation colorization is a crucial part of real animation industry
production. Long animation colorization has high labor costs. Therefore,
automated long animation colorization based on the video generation model has
significant research value. Existing studies are limited to short-term
colorization. These studies adopt a local paradigm, fusing overlapping features
to achieve smooth transitions between local segments. However, the local
paradigm neglects global information, failing to maintain long-term color
consistency. In this study, we argue that ideal long-term color consistency can
be achieved through a dynamic global-local paradigm, i.e., dynamically
extracting global color-consistent features relevant to the current generation.
Specifically, we propose LongAnimation, a novel framework, which mainly
includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color
Consistency Reward. The SketchDiT captures hybrid reference features to support
the DGLM module. The DGLM module employs a long video understanding model to
dynamically compress global historical features and adaptively fuse them with
the current generation features. To refine the color consistency, we introduce
a Color Consistency Reward. During inference, we propose a color consistency
fusion to smooth the video segment transition. Extensive experiments on both
short-term (14 frames) and long-term (average 500 frames) animations show the
effectiveness of LongAnimation in maintaining short-term and long-term color
consistency for open-domain animation colorization task. The code can be found
at https://cn-makers.github.io/long_animation_web/.

</details>


### [140] [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949)
*Kwai Keye Team,Biao Yang,Bin Wen,Changyi Liu,Chenglong Chu,Chengru Song,Chongling Rao,Chuan Yi,Da Li,Dunju Zang,Fan Yang,Guorui Zhou,Hao Peng,Haojie Ding,Jiaming Huang,Jiangxia Cao,Jiankang Chen,Jingyun Hua,Jin Ouyang,Kaibing Chen,Kaiyu Jiang,Kaiyu Tang,Kun Gai,Shengnan Zhang,Siyang Mao,Sui Huang,Tianke Zhang,Tingting Gao,Wei Chen,Wei Yuan,Xiangyu Wu,Xiao Hu,Xingyu Lu,Yang Zhou,Yi-Fan Zhang,Yiping Yang,Yulong Chen,Zhenhua Wu,Zhenyu Li,Zhixin Ling,Ziming Li,Dehua Ma,Di Xu,Haixuan Gao,Hang Li,Jiawei Guo,Jing Wang,Lejian Ren,Muhao Wei,Qianqian Wang,Qigen Hu,Shiyao Wang,Tao Yu,Xinchen Luo,Yan Li,Yiming Liang,Yuhang Hu,Zeyi Lu,Zhuoran Yang,Zixing Zhang*

Main category: cs.CV

TL;DR: This paper introduces Kwai Keye-VL, an 8-billion-parameter multimodal model optimized for short-video understanding using a novel four-stage pre-training and two-phase post-training process.


<details>
  <summary>Details</summary>
Motivation: The dominance of short-form videos in the digital landscape necessitates the development of models that can comprehend dynamic and information-dense content, which current MLLMs struggle with.

Method: A massive dataset of over 600 billion tokens and a specialized training recipe, including a four-stage pre-training process and a two-phase post-training process, were used. Innovations include a five-mode 'cold-start' data mixture and reinforcement learning for reasoning capabilities.

Result: Keye-VL achieves state-of-the-art performance on public video benchmarks, remains competitive in general image-based tasks, and excels on the new KC-MMBench benchmark for real-world short-video scenarios.

Conclusion: Kwai Keye-VL represents a significant advancement in short-video understanding with robust general-purpose vision-language capabilities, validated through extensive benchmarking and real-world testing.

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities on static images, they often fall short in comprehending dynamic,
information-dense short-form videos, a dominant medium in today's digital
landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an
8-billion-parameter multimodal foundation model engineered for leading-edge
performance in short-video understanding while maintaining robust
general-purpose vision-language abilities. The development of Keye-VL rests on
two core pillars: a massive, high-quality dataset exceeding 600 billion tokens
with a strong emphasis on video, and an innovative training recipe. This recipe
features a four-stage pre-training process for solid vision-language alignment,
followed by a meticulous two-phase post-training process. The first
post-training stage enhances foundational capabilities like instruction
following, while the second phase focuses on stimulating advanced reasoning. In
this second phase, a key innovation is our five-mode ``cold-start'' data
mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think
with image'', and high-quality video data. This mixture teaches the model to
decide when and how to reason. Subsequent reinforcement learning (RL) and
alignment steps further enhance these reasoning capabilities and correct
abnormal model behaviors, such as repetitive outputs. To validate our approach,
we conduct extensive evaluations, showing that Keye-VL achieves
state-of-the-art results on public video benchmarks and remains highly
competitive on general image-based tasks (Figure 1). Furthermore, we develop
and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world
short-video scenarios, where Keye-VL shows a significant advantage.

</details>


### [141] [FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model](https://arxiv.org/abs/2507.01953)
*Yukang Cao,Chenyang Si,Jinghao Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper introduces FreeMorph, a tuning-free image morphing method leveraging diffusion models to overcome semantic/layout differences, outperforming existing approaches in speed and quality.


<details>
  <summary>Details</summary>
Motivation: Existing morphing methods have limitations due to their reliance on fine-tuning pre-trained diffusion models, leading to inefficiencies and constraints when dealing with diverse semantics or layouts.

Method: FreeMorph employs guidance-aware spherical interpolation with explicit input guidance and introduces step-oriented variation trends to blend self-attention modules for controlled transitions.

Result: Compared to existing methods, FreeMorph achieves high-fidelity morphing, operates 10x ~ 50x faster, and sets a new state-of-the-art performance benchmark.

Conclusion: FreeMorph addresses challenges of tuning-free morphing methods and offers an efficient, high-quality solution for image morphing with diverse inputs.

Abstract: We present FreeMorph, the first tuning-free method for image morphing that
accommodates inputs with different semantics or layouts. Unlike existing
methods that rely on finetuning pre-trained diffusion models and are limited by
time constraints and semantic/layout discrepancies, FreeMorph delivers
high-fidelity image morphing without requiring per-instance training. Despite
their efficiency and potential, tuning-free methods face challenges in
maintaining high-quality results due to the non-linear nature of the multi-step
denoising process and biases inherited from the pre-trained diffusion model. In
this paper, we introduce FreeMorph to address these challenges by integrating
two key innovations. 1) We first propose a guidance-aware spherical
interpolation design that incorporates explicit guidance from the input images
by modifying the self-attention modules, thereby addressing identity loss and
ensuring directional transitions throughout the generated sequence. 2) We
further introduce a step-oriented variation trend that blends self-attention
modules derived from each input image to achieve controlled and consistent
transitions that respect both inputs. Our extensive evaluations demonstrate
that FreeMorph outperforms existing methods, being 10x ~ 50x faster and
establishing a new state-of-the-art for image morphing.

</details>


### [142] [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955)
*Rahul Ramachandran,Ali Garjani,Roman Bachmann,Andrei Atanov,OÄuzhan Fatih Kar,Amir Zamir*

Main category: cs.CV

TL;DR: Multimodal foundation models have been benchmarked for standard computer vision tasks, revealing their strengths as generalists but limitations compared to specialist models.


<details>
  <summary>Details</summary>
Motivation: To evaluate the current state of multimodal foundation models in visual understanding and benchmark their performance on standard computer vision tasks.

Method: The researchers developed a standardized benchmarking framework by translating vision tasks into text-promptable and API-compatible formats using prompt chaining.

Result: Multimodal models are respectable generalists but do not match specialist models in any task. They performed better on semantic tasks compared to geometric ones.

Conclusion: While these models show promise as generalists, they are not yet ready to match state-of-the-art approaches in specialized computer vision tasks.

Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.

</details>


### [143] [Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation](https://arxiv.org/abs/2507.01957)
*Zhuoyang Zhang,Luke J. Huang,Chengyue Wu,Shang Yang,Kelly Peng,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: A method called Locality-aware Parallel Decoding (LPD) is introduced to speed up autoregressive image generation, reducing generation steps significantly and achieving over 3.4Ã lower latency with preserved quality.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the issue of high latency in traditional autoregressive image generation caused by the memory-bound next-patch prediction process.

Method: Two key techniques are proposed: (1) Flexible Parallelized Autoregressive Modeling, a novel architecture enabling arbitrary generation ordering and parallelization with learnable position query tokens ensuring consistent decoding; (2) Locality-aware Generation Ordering, a scheduling approach optimizing dependency minimization and contextual support.

Result: The proposed method reduces generation steps from 256 to 20 for 256Ã256 resolution and from 1024 to 48 for 512Ã512 resolution, all while maintaining generation quality. At least 3.4Ã lower latency is achieved compared to existing models.

Conclusion: Locality-aware Parallel Decoding significantly accelerates autoregressive image generation without quality loss, demonstrating the effectiveness of the novel architecture and generation scheduling techniques.

Abstract: We present Locality-aware Parallel Decoding (LPD) to accelerate
autoregressive image generation. Traditional autoregressive image generation
relies on next-patch prediction, a memory-bound process that leads to high
latency. Existing works have tried to parallelize next-patch prediction by
shifting to multi-patch prediction to accelerate the process, but only achieved
limited parallelization. To achieve high parallelization while maintaining
generation quality, we introduce two key techniques: (1) Flexible Parallelized
Autoregressive Modeling, a novel architecture that enables arbitrary generation
ordering and degrees of parallelization. It uses learnable position query
tokens to guide generation at target positions while ensuring mutual visibility
among concurrently generated tokens for consistent parallel decoding. (2)
Locality-aware Generation Ordering, a novel schedule that forms groups to
minimize intra-group dependencies and maximize contextual support, enhancing
generation quality. With these designs, we reduce the generation steps from 256
to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without
compromising quality on the ImageNet class-conditional generation, and
achieving at least 3.4$\times$ lower latency than previous parallelized
autoregressive models.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [144] [HERCULES: Hardware accElerator foR stoChastic schedULing in hEterogeneous Systems](https://arxiv.org/abs/2507.01113)
*Vairavan Palaniappan,Adam H. Ross,Amit Ranjan Trivedi,Debjit Pal*

Main category: cs.DC

TL;DR: The paper proposes an FPGA-based accelerator for stochastic online scheduling (SOS) to enhance workload scheduling in heterogeneous computing environments, achieving up to 1060x speedup over traditional software methods.


<details>
  <summary>Details</summary>
Motivation: Traditional software-based schedulers face challenges in high scheduling overhead, lack of adaptability, and poor resource utilization, especially in heterogeneous computing systems with diverse performance profiles.

Method: The authors presented an SOS accelerator by modifying a greedy cost-selection policy to include discretized time, implemented in hardware to leverage parallelism, precalculation, and precision quantization to optimize scheduling.

Result: The proposed hardware accelerator achieves high throughput, low latency, and energy efficiency, demonstrating consistent workload distribution and fair machine utilization with significant speedups (up to 1060x).

Conclusion: The SOS accelerator is a scalable and efficient alternative to traditional software-based schedulers, suitable for HPC systems, deep learning pipelines, and performance-critical applications.

Abstract: Efficient workload scheduling is a critical challenge in modern heterogeneous
computing environments, particularly in high-performance computing (HPC)
systems. Traditional software-based schedulers struggle to efficiently balance
workload distribution due to high scheduling overhead, lack of adaptability to
dynamic workloads, and suboptimal resource utilization. These pitfalls are
compounded in heterogeneous systems, where differing computational elements can
have vastly different performance profiles. To resolve these hindrances, we
present a novel FPGA-based accelerator for stochastic online scheduling (SOS).
We modify a greedy cost selection assignment policy by adapting existing cost
equations to engage with discretized time before implementing them into a
hardware accelerator design. Our design leverages hardware parallelism,
precalculation, and precision quantization to reduce job scheduling latency. By
introducing a hardware-accelerated approach to real-time scheduling, this paper
establishes a new paradigm for adaptive scheduling mechanisms in heterogeneous
computing systems. The proposed design achieves high throughput, low latency,
and energy-efficient operation, offering a scalable alternative to traditional
software scheduling methods. Experimental results demonstrate consistent
workload distribution, fair machine utilization, and up to 1060x speedup over
single-threaded software scheduling policy implementations. This makes the SOS
accelerator a strong candidate for deployment in high-performance computing
system, deeplearning pipelines, and other performance-critical applications.

</details>


### [145] [FLARE: A Dataflow-Aware and Scalable Hardware Architecture for Neural-Hybrid Scientific Lossy Compression](https://arxiv.org/abs/2507.01224)
*Wenqi Jia,Ying Huang,Jian Xu,Zhewen Hu,Sian Jin,Jiannan Tian,Yuede Ji,Miao Yin*

Main category: cs.DC

TL;DR: This paper introduces FLARE, a hardware architecture addressing scalability, energy efficiency, and runtime bottlenecks in HPC workflows using neural-hybrid lossy compression.


<details>
  <summary>Details</summary>
Motivation: The need to manage substantial I/O and network bottlenecks in processing massive scientific datasets while leveraging advanced lossy compression frameworks.

Method: FLARE employs a dataflow-aware scalable hardware architecture minimizing memory access overhead and implementing modular design for adaptability and efficiency.

Result: FLARE demonstrated significant runtime speedups (up to 96.07x) and energy efficiency improvements (up to 520.68x) across varied datasets and hardware.

Conclusion: FLARE is a groundbreaking solution that enhances HPC systems' capability to handle scientific lossy compression efficiently, addressing critical bottlenecks and promoting scalability.

Abstract: Scientific simulation leveraging high-performance computing (HPC) systems is
crucial for modeling complex systems and phenomena in fields such as
astrophysics, climate science, and fluid dynamics, generating massive datasets
that often reach petabyte to exabyte scales. However, managing these vast data
volumes introduces significant I/O and network bottlenecks, limiting practical
performance and scalability. While cutting-edge lossy compression frameworks
powered by deep neural networks (DNNs) have demonstrated superior compression
ratios by capturing complex data correlations, their integration into HPC
workflows poses substantial challenges due to the hybrid non-neural and neural
computation patterns, causing excessive memory access overhead, large
sequential stalls, and limited adaptability to varying data sizes and workloads
in existing hardware platforms. To overcome these challenges and push the limit
of high-performance scientific computing, we for the first time propose FLARE,
a dataflow-aware and scalable hardware architecture for neural-hybrid
scientific lossy compression. FLARE minimizes off-chip data access, reduces
bubble overhead through efficient dataflow, and adopts a modular design that
provides both scalability and flexibility, significantly enhancing throughput
and energy efficiency on modern HPC systems. Particularly, the proposed FLARE
achieves runtime speedups ranging from $3.50 \times$ to $96.07 \times$, and
energy efficiency improvements ranging from $24.51 \times$ to $520.68 \times$,
across various datasets and hardware platforms.

</details>


### [146] [Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration](https://arxiv.org/abs/2507.01225)
*Sunandita Patra,Mehtab Pathan,Mahmoud Mahfouz,Parisa Zehtabi,Wided Ouaja,Daniele Magazzeni,Manuela Veloso*

Main category: cs.DC

TL;DR: The paper addresses capacity planning and job scheduling for hybrid on-prem cloud environments in the context of finance, emphasizing uncertainty handling and competing objectives of minimizing resources and adhering to deadlines.


<details>
  <summary>Details</summary>
Motivation: The authors aim to solve resource estimation and scheduling problems in grid computing environments, with a focus on coping with uncertainty in job parameters, especially for the finance sector.

Method: The authors propose approximate methodologies such as deterministic estimators and pair sampling-based constraint programming for scheduling and planning tasks.

Result: They found that the pair sampling-based approach significantly reduces peak resource usage while maintaining high-quality service compared to manual scheduling.

Conclusion: The proposed methods, especially pair sampling, offer an effective strategy for optimizing resource usage and meeting service deadlines in uncertain environments, showcasing a promising solution for industries like finance.

Abstract: Organizations around the world schedule jobs (programs) regularly to perform
various tasks dictated by their end users. With the major movement towards
using a cloud computing infrastructure, our organization follows a hybrid
approach with both cloud and on-prem servers. The objective of this work is to
perform capacity planning, i.e., estimate resource requirements, and job
scheduling for on-prem grid computing environments. A key contribution of our
approach is handling uncertainty in both resource usage and duration of the
jobs, a critical aspect in the finance industry where stochastic market
conditions significantly influence job characteristics. For capacity planning
and scheduling, we simultaneously balance two conflicting objectives: (a)
minimize resource usage, and (b) provide high quality-of-service to the end
users by completing jobs by their requested deadlines. We propose approximate
approaches using deterministic estimators and pair sampling-based constraint
programming. Our best approach (pair sampling-based) achieves much lower peak
resource usage compared to manual scheduling without compromising on the
quality-of-service.

</details>


### [147] [Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization](https://arxiv.org/abs/2507.01676)
*Giuseppe Ruggeri,Renzo Andri,Daniele Jahier Pagliari,Lukas Cavigelli*

Main category: cs.DC

TL;DR: The paper addresses the bottleneck in embedding layers of Deep Recommender Models (DLRMs) by proposing four strategies for efficient embedding look-ups and a framework for asymmetric table mapping to cores, achieving significant speed-ups, especially in unbalanced workloads.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the performance bottleneck in DLRM inference, particularly in the embedding layers, which dominate the AI workload in Meta's data centers due to their intensive random memory access demands.

Method: The method involves designing tailored data flows with four efficient embedding look-up strategies and introducing a framework for asymmetric mapping of embedding tables to multiple cores on System-on-Chips (SoCs).

Result: The proposed approach achieves speed-ups ranging from 1.5x to 6.5x for real-world workload distributions and over 20x for highly unbalanced ones. It also shows better consistency regardless of query distribution, compared to baseline approaches.

Conclusion: The approach significantly accelerates DLRM embedding look-ups, especially under unbalanced conditions, proving to be scalable and reliable across various workload distributions.

Abstract: Deep Recommender Models (DLRMs) inference is a fundamental AI workload
accounting for more than 79% of the total AI workload in Meta's data centers.
DLRMs' performance bottleneck is found in the embedding layers, which perform
many random memory accesses to retrieve small embedding vectors from tables of
various sizes. We propose the design of tailored data flows to speedup
embedding look-ups. Namely, we propose four strategies to look up an embedding
table effectively on one core, and a framework to automatically map the tables
asymmetrically to the multiple cores of a SoC. We assess the effectiveness of
our method using the Huawei Ascend AI accelerators, comparing it with the
default Ascend compiler, and we perform high-level comparisons with Nvidia
A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload
distributions, and more than 20x for extremely unbalanced distributions.
Furthermore, the method proves to be much more independent of the query
distribution than the baseline.

</details>


### [148] [Optimal Dispersion Under Asynchrony](https://arxiv.org/abs/2507.01298)
*Debasish Pattanayak,Ajay D. Kshemkalyani,Manish Kumar,Anisur Rahaman Molla,Gokarna Sharma*

Main category: cs.DC

TL;DR: This paper presents an optimal algorithm for the dispersion problem in asynchronous anonymous graphs, achieving $O(k)$ time while requiring $O(\log(k+\Delta))$ memory per agent.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the historical challenge of efficiently solving the dispersion problem in graphs, minimizing time and memory overhead, especially in the asynchronous setting.

Method: A novel technique is introduced to construct a port-one tree in anonymous graphs, facilitating efficient dispersion.

Result: The paper closes the existing complexity gap by proposing an algorithm that achieves optimality in both time ($O(k)$) and memory usage ($O(\log(k+\Delta))$).

Conclusion: The proposed method not only achieves optimal performance but introduces a tree construction technique that may have broader applications in distributed computing scenarios involving mobile agents.

Abstract: We study the dispersion problem in anonymous port-labeled graphs: $k \leq n$
mobile agents, each with a unique ID and initially located arbitrarily on the
nodes of an $n$-node graph with maximum degree $\Delta$, must autonomously
relocate so that no node hosts more than one agent. Dispersion serves as a
fundamental task in distributed computing of mobile agents, and its complexity
stems from key challenges in local coordination under anonymity and limited
memory.
  The goal is to minimize both the time to achieve dispersion and the memory
required per agent. It is known that any algorithm requires $\Omega(k)$ time in
the worst case, and $\Omega(\log k)$ bits of memory per agent. A recent result
[SPAA'25] gives an optimal $O(k)$-time algorithm in the synchronous setting and
an $O(k \log k)$-time algorithm in the asynchronous setting, both using
$O(\log(k+\Delta))$ bits.
  In this paper, we close the complexity gap in the asynchronous setting by
presenting the first dispersion algorithm that runs in optimal $O(k)$ time
using $O(\log(k+\Delta))$ bits of memory per agent. Our solution is based on a
novel technique we develop in this paper that constructs a port-one tree in
anonymous graphs, which may be of independent interest.

</details>


### [149] [EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices](https://arxiv.org/abs/2507.01438)
*Zheyu Shen,Yexiao He,Ziyao Wang,Yuning Zhang,Guoheng Sun,Wanghao Ye,Ang Li*

Main category: cs.DC

TL;DR: EdgeLoRA improves the efficiency of deploying fine-tuned LLMs on resource-constrained edge devices in multi-tenant environments, offering significant gains in latency, throughput, and adapter management.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address challenges in deploying fine-tuned LLMs on edge devices, including adapter complexity, memory overhead, and computational inefficiencies in multi-tenant setups.

Method: The authors propose EdgeLoRA, which implements adaptive adapter selection, intelligent memory management, and batch inference to optimize LLM serving on edge devices.

Result: EdgeLoRA demonstrates up to 4x throughput gains and enables simultaneous service of numerous adapters, outperforming existing solutions like llama.cpp.

Conclusion: EdgeLoRA provides a scalable and efficient system for edge deployment of LLMs, likely transforming multi-tenant edge scenarios by overcoming resource constraints effectively.

Abstract: Large Language Models (LLMs) have gained significant attention due to their
versatility across a wide array of applications. Fine-tuning LLMs with
parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these
models to efficiently adapt to downstream tasks without extensive retraining.
Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial
benefits, such as reduced latency, enhanced privacy, and personalized
responses. However, serving LLMs efficiently on resource-constrained edge
devices presents critical challenges, including the complexity of adapter
selection for different tasks and memory overhead from frequent adapter
swapping. Moreover, given the multiple requests in multi-tenant settings,
processing requests sequentially results in underutilization of computational
resources and increased latency. This paper introduces EdgeLoRA, an efficient
system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA
incorporates three key innovations: (1) an adaptive adapter selection mechanism
to streamline the adapter configuration process; (2) heterogeneous memory
management, leveraging intelligent adapter caching and pooling to mitigate
memory operation overhead; and (3) batch LoRA inference, enabling efficient
batch processing to significantly reduce computational latency. Comprehensive
evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly
outperforms the status quo (i.e., llama.cpp) in terms of both latency and
throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times
boost in throughput. Even more impressively, it can serve several orders of
magnitude more adapters simultaneously. These results highlight EdgeLoRA's
potential to transform edge deployment of LLMs in multi-tenant scenarios,
offering a scalable and efficient solution for resource-constrained
environments.

</details>


### [150] [EDGChain-E: A Decentralized Git-Based Framework for Versioning Encrypted Energy Data](https://arxiv.org/abs/2507.01615)
*Alper Alimoglu,Kamil Erdayandi,Mustafa A. Mustafa,Ãmit Cali*

Main category: cs.DC

TL;DR: The paper introduces EDGChain-E, a blockchain and IPFS-based technology for version-controlled, encrypted energy data management, ensuring traceability, privacy, and decentralized collaboration.


<details>
  <summary>Details</summary>
Motivation: The aim is to solve issues of secure, private, and decentralized collaboration in energy data governance for applications like grid monitoring and energy trading.

Method: EDGChain-E integrates blockchain, IPFS, and decentralized DAO governance for managing encrypted data with versioning, leveraging Git patches and hash-based identifiers.

Result: The framework supports FAIR principles, enables secure collaboration, and ensures auditable, transparent data tracking in decentralized energy applications.

Conclusion: This decentralized framework improves data governance in energy systems, enhancing trust, transparency, and reproducibility while safeguarding sensitive information.

Abstract: This paper proposes a new decentralized framework, named EDGChain-E
(Encrypted-Data-Git Chain for Energy), designed to manage version-controlled,
encrypted energy data using blockchain and the InterPlanetary File System. The
framework incorporates a Decentralized Autonomous Organization (DAO) to
orchestrate collaborative data governance across the lifecycle of energy
research and operations, such as smart grid monitoring, demand forecasting, and
peer-to-peer energy trading. In EDGChain-E, initial commits capture the full
encrypted datasets-such as smart meter readings or grid telemetry-while
subsequent updates are tracked as encrypted Git patches, ensuring integrity,
traceability, and privacy. This versioning mechanism supports secure
collaboration across multiple stakeholders (e.g., utilities, researchers,
regulators) without compromising sensitive or regulated information. We
highlight the framework's capability to maintain FAIR-compliant (Findable,
Accessible, Interoperable, Reusable) provenance of encrypted data. By embedding
hash-based content identifiers in Merkle trees, the system enables transparent,
auditable, and immutable tracking of data changes, thereby supporting
reproducibility and trust in decentralized energy applications.

</details>


### [151] [Evolving HPC services to enable ML workloads on HPE Cray EX](https://arxiv.org/abs/2507.01880)
*Stefano Schuppli,Fawzi Mohamed,Henrique MendonÃ§a,Nina Mujkanovic,Elia Palme,Dino Conciatore,Lukas Drescher,Miguel Gila,Pim Witlox,Joost VandeVondele,Maxime Martinasso,Thomas C. Schulthess,Torsten Hoefler*

Main category: cs.DC

TL;DR: The paper examines improvements in HPC services to better support machine learning (ML) workloads, using the Alps infrastructure, which features 10,752 GPUs.


<details>
  <summary>Details</summary>
Motivation: The dynamic and specific needs of ML workloads are not adequately addressed by traditional HPC services.

Method: The paper identifies challenges and gaps observed during early-access use of Alps by the Swiss AI community and proposes enhancements like a tailored user environment, performance screening tools, infrastructure updates, and improved storage systems.

Result: Several technological proposals are introduced to improve usability, resiliency, and adoption of Alps for ML workloads while addressing existing gaps.

Conclusion: The enhancements facilitate better execution of ML workloads on HPC systems and align HPC infrastructure with the evolving needs of the scientific communities it serves.

Abstract: The Alps Research Infrastructure leverages GH200 technology at scale,
featuring 10,752 GPUs. Accessing Alps provides a significant computational
advantage for researchers in Artificial Intelligence (AI) and Machine Learning
(ML). While Alps serves a broad range of scientific communities, traditional
HPC services alone are not sufficient to meet the dynamic needs of the ML
community. This paper presents an initial investigation into extending HPC
service capabilities to better support ML workloads. We identify key challenges
and gaps we have observed since the early-access phase (2023) of Alps by the
Swiss AI community and propose several technological enhancements. These
include a user environment designed to facilitate the adoption of HPC for ML
workloads, balancing performance with flexibility; a utility for rapid
performance screening of ML applications during development; observability
capabilities and data products for inspecting ongoing large-scale ML workloads;
a utility to simplify the vetting of allocated nodes for compute readiness; a
service plane infrastructure to deploy various types of workloads, including
support and inference services; and a storage infrastructure tailored to the
specific needs of ML workloads. These enhancements aim to facilitate the
execution of ML workloads on HPC systems, increase system usability and
resilience, and better align with the needs of the ML community. We also
discuss our current approach to security aspects. This paper concludes by
placing these proposals in the broader context of changes in the communities
served by HPC infrastructure like ours.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [152] [Few-Shot Inspired Generative Zero-Shot Learning](https://arxiv.org/abs/2507.01026)
*Md Shakil Ahamed Shohag,Q. M. Jonathan Wu,Farhad Pourpanah*

Main category: cs.LG

TL;DR: FSIGenZ, a novel few-shot-inspired generative zero-shot learning method, reduces dependence on extensive feature synthesis to improve efficiency and variability handling.


<details>
  <summary>Details</summary>
Motivation: Existing generative ZSL methods demand substantial computation and synthetic data, deviating from the original goal of effective zero-shot learning.

Method: Proposes Model-Specific Attribute Scoring (MSAS) to adjust class attributes dynamically and group-level prototypes for unseen classes, along with Dual-Purpose Semantic Regularization (DPSR) for addressing data imbalance.

Result: On datasets like SUN, AwA2, and CUB, FSIGenZ demonstrates competitive performance using significantly fewer synthetic features.

Conclusion: The FSIGenZ framework offers an efficient and effective approach for ZSL by reducing reliance on large-scale feature synthesis while handling variability and imbalance challenges.

Abstract: Generative zero-shot learning (ZSL) methods typically synthesize visual
features for unseen classes using predefined semantic attributes, followed by
training a fully supervised classification model. While effective, these
methods require substantial computational resources and extensive synthetic
data, thereby relaxing the original ZSL assumptions. In this paper, we propose
FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on
large-scale feature synthesis. Our key insight is that class-level attributes
exhibit instance-level variability, i.e., some attributes may be absent or
partially visible, yet conventional ZSL methods treat them as uniformly
present. To address this, we introduce Model-Specific Attribute Scoring (MSAS),
which dynamically re-scores class attributes based on model-specific
optimization to approximate instance-level variability without access to unseen
data. We further estimate group-level prototypes as clusters of instances based
on MSAS-adjusted attribute scores, which serve as representative synthetic
features for each unseen class. To mitigate the resulting data imbalance, we
introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training
a semantic-aware contrastive classifier (SCC) using these prototypes.
Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves
competitive performance using far fewer synthetic features.

</details>


### [153] [DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization](https://arxiv.org/abs/2507.01027)
*Zijian Ye,Wei Huang,Yifei Yu,Tianhe Ren,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.LG

TL;DR: The paper introduces DBellQuant, a post-training quantization framework that significantly reduces weight and activation bit usage in large language models while maintaining high performance.


<details>
  <summary>Details</summary>
Motivation: The motivation behind this paper is to address the computational and memory limitations of large language models to enable their practical deployment. Existing quantization methods suffer from inaccuracies due to weight distributions and activation outliers.

Method: The authors propose DBellQuant, which uses the Learnable Transformation for Dual-Bell (LTDB) algorithm. It transforms weight distributions into dual-bell forms to minimize quantization errors and applies inverse transformations to smooth activations.

Result: DBellQuant achieves significant compression with minimal performance loss. For instance, it achieves a perplexity of 14.39 on the Wikitext2 dataset using LLaMA2-13B with 6-bit activation quantization, outperforming other state-of-the-art methods considerably.

Conclusion: The study demonstrates the potential of DBellQuant in efficiently compressing language models for deployment in resource-constrained settings, setting new performance benchmarks in aggressive quantization scenarios.

Abstract: Large language models (LLMs) demonstrate remarkable performance but face
substantial computational and memory challenges that limit their practical
deployment. Quantization has emerged as a promising solution; however, its
effectiveness is often limited by quantization errors arising from weight
distributions that are not quantization-friendly and the presence of activation
outliers. To address these challenges, we introduce DBellQuant, an innovative
post-training quantization (PTQ) framework that achieves nearly 1-bit weight
compression and 6-bit activation quantization with minimal performance
degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)
algorithm, which transforms single-bell weight distributions into dual-bell
forms to reduce binarization errors and applies inverse transformations to
smooth activations. DBellQuant sets a new state-of-the-art by preserving
superior model performance under aggressive weight and activation quantization.
For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of
14.39 on LLaMA2-13B with 6-bit activation quantization, significantly
outperforming BiLLM's 21.35 without activation quantization, underscoring its
potential in compressing LLMs for real-world applications.

</details>


### [154] [Proof of a perfect platonic representation hypothesis](https://arxiv.org/abs/2507.01098)
*Liu Ziyin,Isaac Chuang*

Main category: cs.LG

TL;DR: The paper elaborates on the proof of the Perfect Platonic Representation Hypothesis for embedded deep linear networks (EDLN), showing that SGD training leads to all layers learning identical representations up to rotation, while most global minima do not exhibit this characteristic.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand why and how stochastic gradient descent (SGD) results in the emergence of identical representations across layers in deep linear networks, a seemingly rare behavior.

Method: The authors revisit and explain the proof provided by Ziyin et al. (2025) for the Platonic Representation Hypothesis, focusing on embedded deep linear network models and analyzing SGD dynamics.

Result: They demonstrated that SGD consistently leads to perfectly Platonic representations, despite these being exceptional minima, and identified six conditions under which the hypothesis could break.

Conclusion: The study reveals a connection between Platonic representations and progressive sharpening phenomena, emphasizing the critical role of entropic forces induced by SGD in representation learning and suggesting a unifying cause for these observations.

Abstract: In this note, we elaborate on and explain in detail the proof given by Ziyin
et al. (2025) of the "perfect" Platonic Representation Hypothesis (PRH) for the
embedded deep linear network model (EDLN). We show that if trained with SGD,
two EDLNs with different widths and depths and trained on different data will
become Perfectly Platonic, meaning that every possible pair of layers will
learn the same representation up to a rotation. Because most of the global
minima of the loss function are not Platonic, that SGD only finds the perfectly
Platonic solution is rather extraordinary. The proof also suggests at least six
ways the PRH can be broken. We also show that in the EDLN model, the emergence
of the Platonic representations is due to the same reason as the emergence of
progressive sharpening. This implies that these two seemingly unrelated
phenomena in deep learning can, surprisingly, have a common cause. Overall, the
theory and proof highlight the importance of understanding emergent "entropic
forces" due to the irreversibility of SGD training and their role in
representation learning. The goal of this note is to be instructive and avoid
lengthy technical details.

</details>


### [155] [Dual Perspectives on Non-Contrastive Self-Supervised Learning](https://arxiv.org/abs/2507.01028)
*Jean Ponce,Martial Hebert,Basile Terver*

Main category: cs.LG

TL;DR: The paper analyzes two common procedures in non-contrastive self-supervised learning (stop gradient and exponential moving average) for avoiding representation collapse, proving their theoretical stability and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Non-contrastive approaches to self-supervised learning face challenges such as representation collapse, which undermines performance. The motivation is to investigate the theoretical dynamics of commonly used procedures to understand their efficacy and stability.

Method: The authors employ dual theoretical perspectivesâoptimization and dynamical systemsâto analyze stop gradient and exponential moving average procedures for avoiding collapse. They use proofs in linear cases without additional assumptions to validate their claims.

Result: The study demonstrates that these procedures avoid representation collapse despite not optimizing the original objective or any smooth function. It also shows limit points of the associated dynamical systems as asymptotically stable equilibria.

Conclusion: Stop gradient and exponential moving average are effective in avoiding collapse in non-contrastive self-supervised learning. Their stability, proven with minimal assumptions, justifies their use in practical scenarios.

Abstract: The objective of non-contrastive approaches to self-supervised learning is to
train on pairs of different views of the data an encoder and a predictor that
minimize the mean discrepancy between the code predicted from the embedding of
the first view and the embedding of the second one. In this setting, the stop
gradient and exponential moving average iterative procedures are commonly used
to avoid representation collapse, with excellent performance in downstream
supervised applications. This presentation investigates these procedures from
the dual theoretical viewpoints of optimization and dynamical systems. We first
show that, in general, although they do not optimize the original objective, or
for that matter, any other smooth function, they do avoid collapse. Following
Tian et al. [2021], but without any of the extra assumptions used in their
proofs, we then show using a dynamical system perspective that, in the linear
case, minimizing the original objective function without the use of a stop
gradient or exponential moving average always leads to collapse. Conversely, we
finally show that the limit points of the dynamical systems associated with
these two procedures are, in general, asymptotically stable equilibria, with no
risk of degenerating to trivial solutions.

</details>


### [156] [Fast Clifford Neural Layers](https://arxiv.org/abs/2507.01040)
*Tianxiang Xia,Max Neuwinger,Lin Xiao*

Main category: cs.LG

TL;DR: The paper proposes optimized Clifford convolutional and activation layers for neural networks applied to PDE modeling, achieving 30% faster CPU inference.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of neural networks that model PDEs by leveraging Clifford Algebra and optimizing implementation for single-core CPUs.

Method: The authors optimized Clifford convolutional layers and multivector activation layers, focusing on enhancing performance for larger data and networks exceeding L2 cache limits.

Result: Their implementation demonstrated a 30% speed improvement over standard PyTorch for the targeted use case.

Conclusion: The optimized Clifford layers provide significant performance gains. The implementation is open-sourced for broader use and testing.

Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [157] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Main category: cs.LG

TL;DR: The paper presents PathCoT, a zero-shot CoT prompting method that improves reasoning in pathology visual tasks by integrating expert knowledge and self-evaluation.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address challenges in applying multimodal large language models (MLLMs) to pathology visual reasoning tasks, including a lack of domain-specific knowledge and errors introduced by CoT reasoning.

Method: PathCoT integrates pathology expert knowledge into MLLMs' reasoning processes and incorporates self-evaluation mechanisms to assess and refine answers generated through CoT and direct reasoning.

Result: Experimental results on the PathMMU dataset show PathCoT's effectiveness in improving pathology visual understanding and reasoning, demonstrating its ability to provide reliable answers.

Conclusion: PathCoT successfully enhances MLLMs' performance on pathology visual reasoning tasks by combining expert knowledge integration and self-evaluation steps, making it a promising zero-shot approach.

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [158] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/abs/2507.01052)
*Ahmed Farooq*

Main category: cs.LG

TL;DR: This paper introduces a new energy functional for long-sequence memory in dense Hopfield networks, proposing a temporal kernel for effective sequential retrieval, with applications in transformers and time-series tasks.


<details>
  <summary>Details</summary>
Motivation: To improve the limitations of transformers in addressing long-context tasks, specifically in sequential data like movies, natural language processing, and forecasting.

Method: The paper proposes a temporal kernel $K(m,k)$ within dense Hopfield networks to integrate temporal dependencies, enabling sequential retrieval of patterns in long sequences.

Result: The method successfully stores and retrieves movie frames, demonstrating the ability to handle high-dimensional data variation and integrating within transformer architectures.

Conclusion: The approach enhances transformers' efficiency in tasks requiring long-sequence modeling and handling of long-term dependencies, with implications across NLP, forecasting, and other domains.

Abstract: In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [159] [Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study](https://arxiv.org/abs/2507.01030)
*Reza Lotfi Navaei,Mohammad Safarzadeh,Seyed Mohammad Jafar Sobhani*

Main category: cs.LG

TL;DR: This study uses machine learning techniques to regenerate Flamelet libraries for methane combustion simulations, achieving high accuracy with optimized neural networks.


<details>
  <summary>Details</summary>
Motivation: Memory-intensive Flamelet Generated Manifold (FGM) libraries are significant for combustion modeling but need efficient alternatives for methane simulations.

Method: Four machine learning algorithmsâMulti-Layer Perceptron (MLP), Random Forest, Linear Regression, and Support Vector Machineâwere evaluated for reconstructing FGM libraries, with hyperparameter tuning applied.

Result: MLP with four optimized hidden layers and specific neuron counts attained an accuracy of 99.81%, outperforming others and reducing error rates to 2.30%.

Conclusion: Machine learning-enabled regeneration of Flamelet libraries for methane simulations is resource-efficient, with MLP found as the superior method upon enhancement.

Abstract: In chemistry tabulations and Flamelet combustion models, the Flamelet
Generated Manifold (FGM) is recognized for its precision and physical
representation. The practical implementation of FGM requires a significant
allocation of memory resources. FGM libraries are developed specifically for a
specific fuel and subsequently utilized for all numerical problems using
machine learning techniques. This research aims to develop libraries of Laminar
FGM utilizing machine learning algorithms for application in combustion
simulations of methane fuel. This study employs four Machine Learning
algorithms to regenerate Flamelet libraries, based on an understanding of data
sources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.
Random Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries
were identified as appropriate for constructing a database for training machine
learning models, giving an error rate of 2.30%. The default architectures of
each method were evaluated to determine the optimal approach, leading to the
selection of the MLP method as the primary choice. The method was enhanced
through hyperparameter tuning to improve accuracy. The quantity of hidden
layers and neurons significantly influences method performance. The optimal
model, comprising four hidden layers with 10, 15, 20, and 25 neurons
respectively, achieved an accuracy of 99.81%.

</details>


### [160] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

Main category: cs.LG

TL;DR: The paper investigates optimizing a foundational model to forecast rare, spiky production outages in machine learning services, comparing its performance with classical stochastic models.


<details>
  <summary>Details</summary>
Motivation: There is a need to accurately forecast rare and challenging spiky events, such as production outages, which foundational models have yet to address effectively.

Method: The study involved optimizing a state-of-the-art foundational model and comparing its forecasting errors with classical stochastic models using data on sporadic production outages.

Result: The foundational model achieved favorable forecasting accuracy, identifying patterns better suited for spiky event prediction compared to traditional models. Outage statistics were estimated with less than 6% error.

Conclusion: Optimized foundational models are promising for tackling rare, spiky event forecasting, outperforming traditional approaches in specific contexts.

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [161] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/abs/2507.01031)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: The paper describes the adaptation of PyTorch-based geometric learning frameworks to Intelâs Gaudi-v2 HPUs, offering tools, tutorials, and real-world examples to facilitate usage on non-CUDA hardware.


<details>
  <summary>Details</summary>
Motivation: The growing importance of geometric learning applications coupled with the need for alternatives to the CUDA-dominated hardware ecosystem motivated the exploration of Intel's Gaudi-v2 HPUs.

Method: The team ported core utilities like scatter, sparse indexing, and k-nearest neighbors to Gaudi-v2 HPUs and created a repository featuring tutorials, examples, and diagnostic tools for broader accessibility.

Result: Sixteen tutorials, eleven real-world examples, and essential utilities for implementing geometric machine learning algorithms on Gaudi-v2 HPUs were successfully developed and shared.

Conclusion: Their work lowers the barrier for researchers to explore geometric learning on non-CUDA platforms, promoting hardware diversification and software portability.

Abstract: Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [162] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/abs/2507.01075)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: The paper introduces yProv4ML, a library for collecting provenance data to optimize large-scale AI model training and deployment, focusing on energy efficiency and computational balance.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of large-scale AI models creates challenges in balancing computational efficiency, execution time, accuracy, and energy consumption, prompting the need for better monitoring and understanding of training processes.

Method: The authors developed yProv4ML, a library compliant with W3C PROV and ProvML standards, designed to flexibly collect provenance data in JSON format. It integrates with the yProv framework and can be extended with plugins.

Result: yProv4ML enables the collection of detailed provenance data, providing insights into resource usage patterns, inefficiencies, and workflows, while ensuring reproducibility and accountability.

Conclusion: yProv4ML represents a significant step in scaling AI models efficiently by leveraging provenance data, offering researchers tools to optimize workflows and resource usage effectively.

Abstract: As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [163] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/abs/2507.01032)
*Nan Mu,Hongbo Yang,Chen Zhao*

Main category: cs.LG

TL;DR: This paper presents a cost-effective, dynamic decision framework for classifying multi-omics data, reducing redundant tests while maintaining diagnostic accuracy.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the economic burden and resource inefficiency of high-cost multi-omics profiling in disease diagnosis.

Method: A neural network computes uncertainty-aware Dirichlet distributions for single-omics data, while a Dempster-Shafer theory-based fusion strategy integrates multi-omics data incrementally for decision-making.

Result: The framework successfully reduces redundant testing while retaining diagnostic performance, achieving accurate classification in over 50% of cases with single omics in 3 datasets.

Conclusion: The proposed framework boosts diagnostic efficiency and preserves biological insights, offering a more resource-efficient solution for multi-omics profiling.

Abstract: Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [164] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/abs/2507.01078)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: The paper introduces yProv4ML, a framework to capture machine learning process provenance in PROV-JSON format with minimal effort.


<details>
  <summary>Details</summary>
Motivation: The lack of transparency and rigor in tracking hyperparameters and epochs in large language model development.

Method: Developing yProv4ML, a framework that captures provenance data during ML processes in standard PROV-JSON format, requiring minimal code adjustments.

Result: The framework offers an alternative to proprietary tools like MLFlow, emphasizing openness and lineage tracking.

Conclusion: yProv4ML enhances transparency in machine learning by efficiently capturing process provenance without extensive code changes.

Abstract: The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [165] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/abs/2507.01034)
*Asma Agaal,Mansour Essgaer,Hend M. Farkash,Zulaiha Ali Othman*

Main category: cs.LG

TL;DR: The study presents an advanced LSTM-based framework for electricity forecasting in Benghazi, Libya, outperforming traditional models using historical data during periods of instability and stability.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address electricity forecasting challenges in Benghazi, Libya, due to frequent load shedding, infrastructure issues, and generation deficits.

Method: The researchers applied various time series models, including LSTM, to predict electricity load, generation, and deficits. They enhanced the dataset with preprocessing techniques and assessed model performance using error metrics.

Result: LSTM exhibited superior performance over other models in handling non-stationary and seasonal data, particularly when integrating exogenous factors such as temperature and humidity.

Conclusion: The findings support the development of effective electricity forecasting frameworks, empowering policymakers and grid operators in volatile regions to make informed decisions.

Abstract: Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [166] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia,Frederik Haxel,Oliver Bringmann*

Main category: cs.LG

TL;DR: This paper integrates RISC-V's RVV extension into TVM's MetaSchedule framework, enabling efficient AI workload mapping and achieving latency improvements over GCC, muRISCV-NN, and LLVM, with a smaller binary footprint.


<details>
  <summary>Details</summary>
Motivation: To address challenges in efficiently utilizing RISC-V vector units for AI workloads, which currently rely on compiler autovectorization or hand-crafted libraries, by introducing smarter autotuning approaches.

Method: Integrated the RVV extension into the TVM MetaSchedule framework for tuning tensor operations. Tested on FPGA-implemented RISC-V SoCs and commercially available RISC-V hardware.

Result: Achieved 46% faster execution than GCC autovectorization, 29% faster than muRISCV-NN, and 35% faster than LLVM, with smaller code memory footprint.

Conclusion: The proposed solution improves AI workload execution on RISC-V vector units and is open-sourced for community expansion to other extensions.

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [167] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2507.01035)
*Yushang Zhao,Haotian Lyu,Yike Peng,Aijia Sun,Feng Jiang,Xinyue Han*

Main category: cs.LG

TL;DR: The paper explores optimizing hybrid Graph Neural Networks (GNN) and Large Language Models (LLM) for recommender systems to improve inference latency, training efficiency, and real-time performance.


<details>
  <summary>Details</summary>
Motivation: The growing demand for efficient, high-speed recommender systems to handle complex user-item interactions motivated the research into overcoming computational bottlenecks in hybrid GNN-LLM-based models.

Method: The methodology employed hybrid GNN-LLM integration, model optimization techniques (quantization, LoRA, distillation), hardware acceleration (FPGA, DeepSpeed), and implementation under R 4.4.2.

Result: The optimized hybrid configuration with FPGA and DeepSpeed achieved 13.6% higher accuracy (NDCG@10: 0.75) and latency of 40-60ms. LoRA reduced training time by 66% (3.8 hours) compared to the baseline.

Conclusion: Hardware-software co-design and parameter-efficient tuning significantly enhance the performance of hybrid recommender systems over standalone GNN or LLM methods. Using FPGA and LoRA is recommended for deployment, with future work targeting federated learning and advanced fusion architectures.

Abstract: The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [168] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/abs/2507.01285)
*Aymen Rayane Khouas,Mohamed Reda Bouadjenek,Hakim Hacid,Sunil Aryal*

Main category: cs.LG

TL;DR: The paper introduces Dist-FedAvg, a distance-based aggregation technique for graph federated recommendation systems that improves personalization by prioritizing similar user embeddings, while ensuring anchor users retain influence.


<details>
  <summary>Details</summary>
Motivation: Existing federated recommendation systems fail to effectively address user embedding complexity, similarity, and evolving user interactions, impacting the effectiveness of recommendations.

Method: Dist-FedAvg assigns higher aggregation weights to users with similar embeddings and preserves influence from anchor users, offering adaptive aggregation suited for graph-based frameworks.

Result: Empirical evaluations across various datasets show Dist-FedAvg outperforms traditional methods in recommendation accuracy and integrates well into existing federated learning setups.

Conclusion: Dist-FedAvg enhances aggregation efficiency and personalization in graph federated learning while offering a privacy-preserving solution for recommendation systems.

Abstract: Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [169] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/abs/2507.01037)
*Wenbin Ouyang,Sirui Li,Yining Ma,Cathy Wu*

Main category: cs.LG

TL;DR: The study presents Learning-to-Segment (L2Seg), a neural framework aiding First-Segment-Then-Aggregate (FSTA) decomposition for iterative solvers, accelerating solution times for Vehicle Routing Problems (VRPs up to 7x.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address redundant computations in iterative heuristics for large-scale VRPs, where stable solution portions remain unchanged through iterations.

Method: The authors introduce the FSTA technique, preserving stable solution segments and aggregating nodes into hypernodes, while employing L2Segâa neural frameworkâto differentiate stable and unstable sections for efficient decomposition.

Result: Empirical results on Capacitated VRP (CVRP) and VRP with Time Windows (VRPTW) demonstrate up to 7x acceleration using L2Seg integration.

Conclusion: L2Seg enhances iterative solvers across different VRP types by leveraging stability detection, with notable compatibility across solvers, providing significant computation acceleration.

Abstract: Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [170] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/abs/2507.01039)
*Kaaustaaub Shankar,Wilhelm Louw,Kelly Cohen*

Main category: cs.LG

TL;DR: The paper introduces a reinforcement learning approach using Proximal Policy Optimization (PPO) to train neuro-fuzzy controllers, achieving faster and more stable performance compared to Deep Q-Network (DQN)-based methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the training stability and performance of neuro-fuzzy controllers in reinforcement learning by using an on-policy actor-critic method instead of the prior off-policy value-based learning.

Method: The authors implement PPO to train fuzzy controllers, evaluating its performance in the CartPole-v1 environment and comparing it to ANFIS-DQN baselines.

Result: PPO-trained fuzzy agents achieved better stability (reduced training variance) and faster convergence, with a mean return of 500 +/- 0 on CartPole-v1 after 20,000 updates.

Conclusion: The study concludes that PPO is a promising framework for training explainable neuro-fuzzy controllers in reinforcement learning, outperforming prior DQN-based approaches.

Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [171] [Enhanced Generative Model Evaluation with Clipped Density and Coverage](https://arxiv.org/abs/2507.01761)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: The authors propose two new metrics, Clipped Density and Clipped Coverage, to improve the evaluation of generative model samples, addressing issues with fidelity, coverage, calibration, and robustness.


<details>
  <summary>Details</summary>
Motivation: Generative models require reliable metrics to assess the quality of their samples, as current methods are often not calibrated and sensitive to outliers.

Method: The authors introduce Clipped Density and Clipped Coverage metrics, which clip sample contributions and leverage calibration to reduce biases from out-of-distribution samples.

Result: Analytical and empirical calibration show that the metrics degrade linearly with poor sample proportions, and experimental evaluations confirm they outperform existing quality metrics.

Conclusion: Clipped Density and Clipped Coverage offer robust, sensitive, and interpretable metrics for generative model evaluation, marking an improvement over traditional methods.

Abstract: Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

</details>


### [172] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz SÃ¡ez de OcÃ¡riz Borde,Anastasis Kratsios*

Main category: cs.LG

TL;DR: The paper introduces a CPU-friendly method for Low-Rank Adapter (LoRA) fine-tuning by using pre-trained adapters to make fine-tuning accessible without GPUs.


<details>
  <summary>Details</summary>
Motivation: Currently, fine-tuning Large Language Models (LLMs) using LoRAs is heavily dependent on GPU-based training, limiting its application for users with limited computational resources.

Method: The method involves creating a meta-operator that leverages a pre-trained adapter bank (for Mistral-7B-Instruct-v0.2) to build new LoRAs on CPU by combining existing ones, instead of performing gradient-based updates.

Result: The CPU-generated adapters outperformed the base Mistral model on downstream tasks but did not match the performance of GPU-trained LoRAs.

Conclusion: This approach provides a practical solution for users with restricted resources, enabling parameter-efficient model fine-tuning without GPUs.

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [173] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/abs/2507.01041)
*Zuguang Li,Wen Wu,Shaohua Wu,Songge Zhang,Ye Wang,Xuemin,Shen*

Main category: cs.LG

TL;DR: This paper develops algorithms for efficient AI model splitting, enabling faster training with reduced computational complexity.


<details>
  <summary>Details</summary>
Motivation: The need for efficient AI training methods on devices with limited computational resources.

Method: The paper reformulates model splitting as a graph theory problem, introducing DAG-based algorithmsâincluding a block-wise methodâto optimize splits.

Result: The algorithms achieve optimal model splitting within milliseconds and reduce training delays by up to 38.95% in dynamic edge networks.

Conclusion: DAG-based approaches are computationally efficient for model splitting, offering significant advantages over traditional methods in real-world scenarios.

Abstract: Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [174] [Out-of-Distribution Detection Methods Answer the Wrong Questions](https://arxiv.org/abs/2507.01831)
*Yucen Lily Li,Daohan Lu,Polina Kirichenko,Shikai Qiu,Tim G. J. Rudner,C. Bayan Bruss,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: This paper critically examines the limitations of current out-of-distribution (OOD) detection methods, arguing that they fundamentally misalign with the task and exhibit irreducible errors.


<details>
  <summary>Details</summary>
Motivation: The authors aim to improve OOD detection by identifying critical flaws in the commonly-used uncertainty- and feature-based methods, which they argue are answering the wrong questions.

Method: The study re-examines OOD detection methods, identifies their misalignments and limitations, and evaluates alternative approaches such as hybrid methods, scaling, epistemic uncertainty, and unsupervised generative models.

Result: The paper demonstrates that current OOD detection techniques, including proposed interventions, cannot fully overcome their misalignment issues, leading to irreducible errors and limitations.

Conclusion: Mysteries and inefficiencies persist in OOD detection methods, and fundamental changes to the problem formulation are needed to effectively detect distribution shifts.

Abstract: To detect distribution shifts and improve model safety, many
out-of-distribution (OOD) detection methods rely on the predictive uncertainty
or features of supervised models trained on in-distribution data. In this
paper, we critically re-examine this popular family of OOD detection
procedures, and we argue that these methods are fundamentally answering the
wrong questions for OOD detection. There is no simple fix to this misalignment,
since a classifier trained only on in-distribution classes cannot be expected
to identify OOD points; for instance, a cat-dog classifier may confidently
misclassify an airplane if it contains features that distinguish cats from
dogs, despite generally appearing nothing alike. We find that uncertainty-based
methods incorrectly conflate high uncertainty with being OOD, while
feature-based methods incorrectly conflate far feature-space distance with
being OOD. We show how these pathologies manifest as irreducible errors in OOD
detection and identify common settings where these methods are ineffective.
Additionally, interventions to improve OOD detection such as feature-logit
hybrid methods, scaling of model and data size, epistemic uncertainty
representation, and outlier exposure also fail to address this fundamental
misalignment in objectives. We additionally consider unsupervised density
estimation and generative models for OOD detection, which we show have their
own fundamental limitations.

</details>


### [175] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/abs/2507.01043)
*Szymon Åwiderski,Agnieszka JastrzÄbska*

Main category: cs.LG

TL;DR: The paper introduces a method to dynamically grow and shrink neural network architectures during training using Monte Carlo Tree Search, with strong performance in visual and multivariate time series classification.


<details>
  <summary>Details</summary>
Motivation: Optimizing neural network architectures dynamically during training is a key challenge in AI, improving over fixed-architecture models.

Method: The proposal uses Monte Carlo Tree Search to simulate and select the best architecture adjustments while training, allowing flexible model adaptation.

Result: The method demonstrated strong performance, particularly in multivariate time series classification, due to its dynamic adaptability across datasets.

Conclusion: The proposed approach is robust, adaptable, and effective, with Python code provided for reproducibility, advancing dynamic architecture optimization.

Abstract: The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [176] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/abs/2507.01045)
*Xiao Gu,Wei Tang,Jinpei Han,Veer Sangha,Fenglin Liu,Shreyank N Gowda,Antonio H. Ribeiro,Patrick Schwab,Kim Branson,Lei Clifton,Antonio Luiz P. Ribeiro,Zhangdaihong Liu,David A. Clifton*

Main category: cs.LG

TL;DR: The study introduces a cardiac sensing foundation model (CSFM) using transformers and massive healthcare data to advance cardiac signal analysis and supports multiple diagnostic tasks while enabling seamless transfer learning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of robustness and generalizability in traditional deep learning models for cardiac signal analysis, particularly due to reliance on homogeneous datasets and static models.

Method: The study develops CSFM, a transformer-based model leveraging a generative masked pretraining strategy, trained on over 1.7 million multi-modal health records including ECGs, PPGs, and clinical text reports.

Result: CSFM demonstrated superior performance across various diagnostic tasks, demographic recognition, and clinical outcome prediction. It supports multiple ECG and PPG configurations with robust cross-modal and cross-setup adaptability.

Conclusion: CSFM is a scalable, versatile foundation model that outperforms traditional cardiac signal analysis approaches, offering improved generalizability and performance across diverse settings.

Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [177] [Variational Digital Twins](https://arxiv.org/abs/2507.01047)
*Logan A. Burnett,Umme Mahbuba Nabila,Majdi I. Radaideh*

Main category: cs.LG

TL;DR: This paper proposes a variational digital twin (VDT) framework that augments neural architectures with a Bayesian output layer to improve real-time energy system modeling, uncertainty quantification, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current digital twin frameworks lack robust mechanisms for real-time updates, calibrated uncertainty modeling, and computational efficiency, posing challenges in energy asset applications.

Method: The VDT framework combines a single Bayesian output layer with a novel updating algorithm to enable real-time model updates, calibrated uncertainty bounds, and efficient performance using standard hardware.

Result: VDT achieves high accuracy across four energy-sector problems: active learning reduces experiments for heat-flux prediction, renewable-generation twins maintain consistent forecasts, nuclear reactor twins handle sensor loss, and a battery twin improves voltage error prediction significantly.

Conclusion: Integrating Bayesian elements with efficient update schemes enhances digital twins to become uncertainty-aware, data-efficient, and computationally viable, offering dependable modeling for scientific and industrial energy systems.

Abstract: While digital twins (DT) hold promise for providing real-time insights into
complex energy assets, much of the current literature either does not offer a
clear framework for information exchange between the model and the asset, lacks
key features needed for real-time implementation, or gives limited attention to
model uncertainty. Here, we aim to solve these gaps by proposing a variational
digital twin (VDT) framework that augments standard neural architectures with a
single Bayesian output layer. This lightweight addition, along with a novel VDT
updating algorithm, lets a twin update in seconds on commodity GPUs while
producing calibrated uncertainty bounds that can inform experiment design,
control algorithms, and model reliability. The VDT is evaluated on four
energy-sector problems. For critical-heat-flux prediction, uncertainty-driven
active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third
the training time of random sampling. A three-year renewable-generation twin
maintains R2 > 0.95 for solar output and curbs error growth for volatile wind
forecasts via monthly updates that process only one month of data at a time. A
nuclear reactor transient cooldown twin reconstructs thermocouple signals with
R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating
robustness to degraded instrumentation. Finally, a physics-informed Li-ion
battery twin, retrained after every ten discharges, lowers voltage mean-squared
error by an order of magnitude relative to the best static model while adapting
its credible intervals as the cell approaches end-of-life. These results
demonstrate that combining modest Bayesian augmentation with efficient update
schemes turns conventional surrogates into uncertainty-aware, data-efficient,
and computationally tractable DTs, paving the way for dependable models across
industrial and scientific energy systems.

</details>


### [178] [3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells](https://arxiv.org/abs/2507.01048)
*Ricardo Emanuel Vaz Vargas,AfrÃ¢nio JosÃ© de Melo Junior,Celso JosÃ© Munaro,ClÃ¡udio Benevenuto de Campos Lima,Eduardo Toledo de Lima Junior,Felipe Muntzberg Barrocas,FlÃ¡vio Miguel VarejÃ£o,Guilherme Fidelis Peixer,Igor de Melo Nery Oliveira,Jader Riso Barbosa Jr.,Jaime AndrÃ©s Lozano Cadena,Jean Carlos Dias de AraÃºjo,JoÃ£o Neuenschwander Escosteguy Carneiro,Lucas Gouveia Omena Lopes,Lucas Pereira de Gouveia,Mateus de Araujo Fernandes,Matheus Lima Scramignon,Patrick Marques Ciarelli,Rodrigo Castello Branco,RogÃ©rio Leite Alves Pinto*

Main category: cs.LG

TL;DR: The paper discusses the 3W Dataset, a publicly available, expert-labeled multivariate time series dataset for detecting undesirable events in oil wells.


<details>
  <summary>Details</summary>
Motivation: Undesirable events in oil wells can result in significant economic, environmental, and human consequences, highlighting the need for advanced detection solutions and the absence of public datasets in this domain.

Method: The dataset was collaboratively developed by Petrobras and upgraded over time to include structural modifications and additional labeled data for broader applicability.

Result: The current version of the 3W Dataset is widely adopted as a key resource in the field, supporting research on early detection of adverse events in oil wells.

Conclusion: The enhanced dataset facilitates advancements in creating AI/ML methodologies for predicting undesirable oil well events, encouraging community collaboration and innovation.

Abstract: In the oil industry, undesirable events in oil wells can cause economic
losses, environmental accidents, and human casualties. Solutions based on
Artificial Intelligence and Machine Learning for Early Detection of such events
have proven valuable for diverse applications across industries. In 2019,
recognizing the importance and the lack of public datasets related to
undesirable events in oil wells, Petrobras developed and publicly released the
first version of the 3W Dataset, which is essentially a set of Multivariate
Time Series labeled by experts. Since then, the 3W Dataset has been developed
collaboratively and has become a foundational reference for numerous works in
the field. This data article describes the current publicly available version
of the 3W Dataset, which contains structural modifications and additional
labeled data. The detailed description provided encourages and supports the 3W
community and new 3W users to improve previous published results and to develop
new robust methodologies, digital products and services capable of detecting
undesirable events in oil wells with enough anticipation to enable corrective
or mitigating actions.

</details>


### [179] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Main category: cs.LG

TL;DR: This paper introduces a two-stage training framework to enhance text detoxification on social media, improving efficiency, semantic retention, and robustness without heavy reliance on annotated data.


<details>
  <summary>Details</summary>
Motivation: The increase of toxic content on social media creates the need for effective detoxification methods that maintain original meaning while overcoming current limitations like poor data efficiency and reliance on manual annotations.

Method: The two-stage training involves initial supervised fine-tuning on filtered annotated data, followed by training using unlabeled toxic inputs and a custom reward model through Group Relative Policy Optimization.

Result: The proposed method outperforms previous approaches, achieving superior generalization, better semantic preservation, reduced dependence on annotated data, and state-of-the-art detoxification performance.

Conclusion: The framework offers an effective and efficient strategy for detoxification, addressing prior trade-offs and advancing the capabilities in mitigating toxic content.

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [180] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/abs/2507.01054)
*Jithendaraa Subramanian,Linda Hung,Daniel Schweigert,Santosh Suram,Weike Ye*

Main category: cs.LG

TL;DR: This paper introduces a multimodal framework leveraging elemental composition and X-ray diffraction (XRD) data to advance materials discovery, eliminating the need for crystal structures.


<details>
  <summary>Details</summary>
Motivation: Current structure-based models rely heavily on computational datasets and cannot be applied effectively when atomic structures are unavailable or difficult to access.

Method: The proposed framework integrates specific encoders for each modality with a cross-attention fusion module, utilizing the Alexandria dataset and incorporating two pretraining strategies: masked XRD modeling (MXM) and contrastive alignment.

Result: Pretraining accelerates convergence speeds up to 4.2x, enhances accuracy and representation quality, and demonstrates scalable multimodal performance surpassing unimodal approaches as dataset sizes increase.

Conclusion: This study presents an innovative approach for materials discovery rooted in experimental data, showcasing the potential for scalable, structure-free foundation models in science.

Abstract: Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [181] [Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI](https://arxiv.org/abs/2507.01056)
*Lidan Peng,Lu Gao,Feng Hong,Jingran Sun*

Main category: cs.LG

TL;DR: This paper evaluates how flooding impacts pavement roughness deterioration using International Roughness Index (IRI) data and explains findings through advanced AI techniques like SHAP and LIME.


<details>
  <summary>Details</summary>
Motivation: Flooding poses both immediate and long-term risks to pavement infrastructure, emphasizing the need for understanding and mitigating its deterioration effects.

Method: The study analyzes 20 years of pavement condition data from TxDOT's PMIS database integrated with flood event details. Statistical tools are used to compare pre- and post-flood IRI values, and XAI methods such as SHAP and LIME assess the influence of flooding on pavement conditions.

Result: Flood-affected pavements deteriorate more rapidly in roughness than non-flooded sections, as evidenced by data-driven evaluations.

Conclusion: The study underscores the importance of proactive flood mitigation, including better drainage systems, resilient materials, and preventative maintenance, to bolster pavement longevity in flood-prone areas.

Abstract: Flooding can damage pavement infrastructure significantly, causing both
immediate and long-term structural and functional issues. This research
investigates how flooding events affect pavement deterioration, specifically
focusing on measuring pavement roughness by the International Roughness Index
(IRI). To quantify these effects, we utilized 20 years of pavement condition
data from TxDOT's PMIS database, which is integrated with flood event data,
including duration and spatial extent. Statistical analyses were performed to
compare IRI values before and after flooding and to calculate the deterioration
rates influenced by flood exposure. Moreover, we applied Explainable Artificial
Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and
Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of
flooding on pavement performance. The results demonstrate that flood-affected
pavements experience a more rapid increase in roughness compared to non-flooded
sections. These findings emphasize the need for proactive flood mitigation
strategies, including improved drainage systems, flood-resistant materials, and
preventative maintenance, to enhance pavement resilience in vulnerable regions.

</details>


### [182] [Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates](https://arxiv.org/abs/2507.01057)
*Lushun Fan,Yuqin Xia,Jun Li,Karl Jenkins*

Main category: cs.LG

TL;DR: The paper proposes an intelligent optimization system using deep learning for mesh quality and generation, focusing on a CNN-based Loop2Net framework.


<details>
  <summary>Details</summary>
Motivation: To develop a method for achieving automated and high-quality mesh generation and optimization for complex structures like wings, addressing current limitations.

Method: A deep convolutional neural network (Loop2Net) predicts meshes from wing coordinates, optimizing its performance during training with two key loss functions and penalty-based discipline.

Result: The study designed a robust mesh prediction and optimization system capable of generating high-quality meshes efficiently.

Conclusion: The proposed intelligent optimization system successfully demonstrates the ability to predict and optimize meshes using advanced deep learning methods.

Abstract: In this study, an innovative intelligent optimization system for mesh quality
is proposed, which is based on a deep convolutional neural network
architecture, to achieve mesh generation and optimization. The core of the
study is the Loop2Net generator and loss function, it predicts the mesh based
on the given wing coordinates. And the model's performance is continuously
optimised by two key loss functions during the training. Then discipline by
adding penalties, the goal of mesh generation was finally reached.

</details>


### [183] [Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors](https://arxiv.org/abs/2507.01068)
*Biplov Paneru*

Main category: cs.LG

TL;DR: This paper proposes explainable AI methods for early detection and prediction of Freezing of Gait (FOG) using IMU data and machine learning, achieving nearly 99% classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Early detection of FOG in Parkinson's disease is crucial for improving patient care, and there is a need for accurate, interpretable, and scalable diagnostic methods.

Method: The study uses machine learning models, including CatBoost, XGBoost, and Extra Trees classifiers, combined with a Stacking Ensemble model and federated learning to improve predictions. SHAP analysis provides interpretability, highlighting key influencing factors in gait patterns.

Result: The Stacking Ensemble model achieved near 99% classification accuracy, outperforming other models. Time (seconds) was identified as the most critical feature for distinguishing FOG episodes.

Conclusion: This framework successfully combines high accuracy with interpretability and scalability, potentially aiding in real-world FOG detection and prediction for Parkinson's disease patients.

Abstract: This study leverages an Inertial Measurement Unit (IMU) dataset to develop
explainable AI methods for the early detection and prediction of Freezing of
Gait (FOG), a common symptom in Parkinson's disease. Machine learning models,
including CatBoost, XGBoost, and Extra Trees classifiers, are employed to
accurately categorize FOG episodes based on relevant clinical features. A
Stacking Ensemble model achieves superior performance, surpassing a hybrid
bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP
interpretability analysis reveals that time (seconds) is the most influential
factor in distinguishing gait patterns. Additionally, the proposed FOG
prediction framework incorporates federated learning, where models are trained
locally on individual devices and aggregated on a central server using a
federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for
enhanced predictive capability.

</details>


### [184] [Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs](https://arxiv.org/abs/2507.01073)
*Dian Jin*

Main category: cs.LG

TL;DR: The paper introduces a new 3D molecular encoding module that ensures rotational invariance for graph neural networks with improved accuracy, robustness, and low computational cost.


<details>
  <summary>Details</summary>
Motivation: Current graph neural networks fail to handle 3D spatial variability effectively, which limits their generalization and robustness for molecular property prediction.

Method: A novel module using rotational sampling and SO(3) rotational group expectations to approximate rotational invariance, combined with a post-alignment strategy for strict invariance.

Result: Experiments on QM9 and C10 datasets show superior predictive accuracy, robustness, generalization, and cost-efficiency as compared to existing methods.

Conclusion: The proposed module is efficient, interpretable, and significantly advances the handling of 3D molecular structures, marking potential progress in drug discovery and material design.

Abstract: Graph neural networks (GNNs) have achieved remarkable success in molecular
property prediction. However, traditional graph representations struggle to
effectively encode the inherent 3D spatial structures of molecules, as
molecular orientations in 3D space introduce significant variability, severely
limiting model generalization and robustness. Existing approaches primarily
focus on rotation-invariant and rotation-equivariant methods. Invariant methods
often rely heavily on prior knowledge and lack sufficient generalizability,
while equivariant methods suffer from high computational costs. To address
these limitations, this paper proposes a novel plug-and-play 3D encoding module
leveraging rotational sampling. By computing the expectation over the SO(3)
rotational group, the method naturally achieves approximate rotational
invariance. Furthermore, by introducing a carefully designed post-alignment
strategy, strict invariance can be achieved without compromising performance.
Experimental evaluations on the QM9 and C10 Datasets demonstrate superior
predictive accuracy, robustness, and generalization performance compared to
existing methods. Moreover, the proposed approach maintains low computational
complexity and enhanced interpretability, providing a promising direction for
efficient and effective handling of 3D molecular information in drug discovery
and material design.

</details>


### [185] [Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels](https://arxiv.org/abs/2507.01077)
*Bogdan Bogdan,Arina Cazacu,Laura Vasilie*

Main category: cs.LG

TL;DR: The paper introduces a novel decoder-only Large Language Model (LLM) for anomaly detection in ECU communication logs, enhancing scalability and detection accuracy in complex systems.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods are insufficient for specialized domains like automotive communication systems due to scalability challenges and inconsistent ground truth data.

Method: A decoder-only LLM was trained on UDP communication logs to recognize time deviations as anomalies. It uses entropy regularization to handle inconsistent labeling and increase model uncertainty in known anomalies.

Result: The proposed model demonstrates an adaptable architecture capable of learning from limited examples while effectively handling inconsistent labeling, improving anomaly detection accuracy in ECU communications.

Conclusion: The novel LLM-based approach offers a scalable, more accurate method for anomaly detection in specialized automotive communication systems, minimizing reliance on extensive manual labeling.

Abstract: Anomaly detection often relies on supervised or clustering approaches, with
limited success in specialized domains like automotive communication systems
where scalable solutions are essential. We propose a novel decoder-only Large
Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU)
communication logs. Our approach addresses two key challenges: the lack of LLMs
tailored for ECU communication and the complexity of inconsistent ground truth
data. By learning from UDP communication logs, we formulate anomaly detection
simply as identifying deviations in time from normal behavior. We introduce an
entropy regularization technique that increases model's uncertainty in known
anomalies while maintaining consistency in similar scenarios. Our solution
offers three novelties: a decoder-only anomaly detection architecture, a way to
handle inconsistent labeling, and an adaptable LLM for different ECU
communication use cases. By leveraging the generative capabilities of
decoder-only models, we present a new technique that addresses the high cost
and error-prone nature of manual labeling through a more scalable system that
is able to learn from a minimal set of examples, while improving detection
accuracy in complex communication environments.

</details>


### [186] [Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept](https://arxiv.org/abs/2507.01080)
*Edouard Lansiaux,Ramy Azzouz,Emmanuel Chazard,AmÃ©lie Vromant,Eric Wiel*

Main category: cs.LG

TL;DR: This study evaluates three AI models for their accuracy in predicting triage outcomes in emergency departments, demonstrating that Large Language Models (LLMs) outperform other models and human nurse triage.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from addressing persistent undertriage and overtriage issues in emergency departments, compounded by increasing patient numbers and staff shortages.

Method: Three AI models were trained and validated using demographic, complaint, vital signs, and triage outcome data collected over seven months in a hospital ED.

Result: The LLM-based model (URGENTIAPARSE) demonstrated superior performance in predicting triage levels according to the FRENCH scale, surpassing both other AI models and nurse triage.

Conclusion: Large Language Models offer the most accurate triage prediction among tested frameworks, suggesting their potential for improving patient safety and efficiency in emergency workflows. Challenges like model limitations and ethical transparency need addressing before integration.

Abstract: Triage errors, including undertriage and overtriage, are persistent
challenges in emergency departments (EDs). With increasing patient influx and
staff shortages, the integration of artificial intelligence (AI) into triage
protocols has gained attention. This study compares the performance of three AI
models [Natural Language Processing (NLP), Large Language Models (LLM), and
Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes
against the FRENCH scale and clinical practice.We conducted a retrospective
analysis of a prospectively recruited cohort gathering adult patient triage
data over a 7-month period at the Roger Salengro Hospital ED (Lille, France).
Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)
URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic
details, verbatim chief complaints, vital signs, and triage outcomes based on
the FRENCH scale and GEMSA coding. The primary outcome was the concordance of
AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks
to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM
model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared
to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse
triage (-4.343). Secondary analyses highlighted the effectiveness of
URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness
with structured data versus raw transcripts (either for GEMSA prediction or for
FRENCH prediction). LLM architecture, through abstraction of patient
representations, offers the most accurate triage predictions among tested
models. Integrating AI into ED workflows could enhance patient safety and
operational efficiency, though integration into clinical workflows requires
addressing model limitations and ensuring ethical transparency.

</details>


### [187] [A Neural Operator based on Dynamic Mode Decomposition](https://arxiv.org/abs/2507.01117)
*Nikita Sakovich,Dmitry Aksenov,Ekaterina Pleshakova,Sergey Gataullin*

Main category: cs.LG

TL;DR: The paper introduces a neural operator combining Dynamic Mode Decomposition (DMD) with deep learning for efficient spatiotemporal modeling, reducing computation costs in PDE solutions while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the computational resource intensiveness in solving PDEs for various initial and boundary conditions and improve lightweight accurate computations.

Method: A dynamic mode decomposition (DMD)-based neural operator is developed, integrating DMD and deep learning for automatic extraction of key system dynamics and modes to predict outcomes.

Result: The proposed method demonstrated high reconstruction accuracy and efficiency compared to DeepONet and FNO in heat equation, Laplaceâs equation, and Burgers equation approximation.

Conclusion: The presented approach effectively balances computational cost and prediction accuracy, showcasing utility in spatiotemporal process modeling for PDE solving tasks.

Abstract: The scientific computation methods development in conjunction with artificial
intelligence technologies remains a hot research topic. Finding a balance
between lightweight and accurate computations is a solid foundation for this
direction. The study presents a neural operator based on the dynamic mode
decomposition algorithm (DMD), mapping functional spaces, which combines DMD
and deep learning (DL) for spatiotemporal processes efficient modeling. Solving
PDEs for various initial and boundary conditions requires significant
computational resources. The method suggested automatically extracts key modes
and system dynamics using them to construct predictions, reducing computational
costs compared to traditional numerical methods. The approach has demonstrated
its efficiency through comparative analysis of performance with closest
analogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers
equation solutions approximation, where it achieves high reconstruction
accuracy.

</details>


### [188] [On Design Principles for Private Adaptive Optimizers](https://arxiv.org/abs/2507.01129)
*Arun Ganesh,Brendan McMahan,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: This paper evaluates techniques for improving the compatibility of adaptive optimizers like Adam and AdaGrad with differentially private (DP) training, proposing that scale-then-privatize performs better than other methods on small-scale language model tasks.


<details>
  <summary>Details</summary>
Motivation: Differentially private training often degrades the performance of adaptive optimizers due to added noise, necessitating improved methods that handle noise while maintaining learning efficiency.

Method: The authors survey existing DP adaptive optimizer variants, fostering theoretical insight and conducting empirical comparisons, introducing and validating the scale-then-privatize technique.

Result: The scale-then-privatize approach demonstrated superior performance and theoretical behaviors compared to other variants in small-scale language model training.

Conclusion: Contrary to popular practices aiming for unbiased gradient estimates, scale-then-privatize offers a promising method for DP training due to its robust theoretical properties and practical performance advantages.

Abstract: The spherical noise added to gradients in differentially private (DP)
training undermines the performance of adaptive optimizers like AdaGrad and
Adam, and hence many recent works have proposed algorithms to address this
challenge. However, the empirical results in these works focus on simple tasks
and models and the conclusions may not generalize to model training in
practice. In this paper we survey several of these variants, and develop better
theoretical intuition for them as well as perform empirical studies comparing
them. We find that a common intuition of aiming for unbiased estimates of
second moments of gradients in adaptive optimizers is misguided, and instead
that a simple technique called scale-then-privatize (which does not achieve
unbiased second moments) has more desirable theoretical behaviors and
outperforms all other variants we study on a small-scale language model
training task. We additionally argue that scale-then-privatize causes the noise
addition to better match the application of correlated noise mechanisms which
are more desirable to use in practice.

</details>


### [189] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/abs/2507.01131)
*Yuchao Lin,Cong Fu,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: This paper introduces Tensor Decomposition Networks (TDNs) as an efficient alternative to traditional SO(3)-equivariant networks for machine learning interatomic potentials, significantly reducing computational complexity while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the computational inefficiency of Clebsch-Gordan (CG) tensor products in SO(3)-equivariant networks.

Method: The authors develop TDNs using low-rank tensor decompositions like CP decomposition, proving its capability to approximate equivariant bilinear maps and applying path-weight sharing to reduce parameters.

Result: TDNs reduce computational complexity from O(L^6) to O(L^4), achieving competitive performance on large datasets, including PubChemQCR and Open Catalyst datasets (OC20, OC22).

Conclusion: Tensor Decomposition Networks offer a scalable and efficient alternative for SO(3)-equivariant networks, enabling broader applications without compromising computational accuracy.

Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


### [190] [Spectral Manifold Harmonization for Graph Imbalanced Regression](https://arxiv.org/abs/2507.01132)
*Brenda Nogueira,Gabe Gomes,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Main category: cs.LG

TL;DR: The paper introduces Spectral Manifold Harmonization (SMH) to address the issue of imbalanced regression in graph-structured data, focusing on underrepresented target ranges and demonstrating improved performance on chemistry and drug discovery datasets.


<details>
  <summary>Details</summary>
Motivation: Imbalanced regression in graph-structured data often fails to focus on specific target value ranges that are scientifically valuable. Existing methods either ignore graph topology or skew towards average target values, creating biases.

Method: Spectral Manifold Harmonization (SMH) generates synthetic graph samples that preserve topological properties while targeting underrepresented regions in the target distribution.

Result: Experimental validation on benchmark datasets in chemistry and drug discovery shows that SMH consistently improves predictive performance for important target domain ranges.

Conclusion: SMH effectively tackles imbalanced regression in graph-structured data by focusing on valuable target ranges and preserving topology, indicating its potential for scientific domains like drug discovery.

Abstract: Graph-structured data is ubiquitous in scientific domains, where models often
face imbalanced learning settings. In imbalanced regression, domain preferences
focus on specific target value ranges representing the most scientifically
valuable cases; we observe a significant lack of research. In this paper, we
present Spectral Manifold Harmonization (SMH), a novel approach for addressing
this imbalanced regression challenge on graph-structured data by generating
synthetic graph samples that preserve topological properties while focusing on
often underrepresented target distribution regions. Conventional methods fail
in this context because they either ignore graph topology in case generation or
do not target specific domain ranges, resulting in models biased toward average
target values. Experimental results demonstrate the potential of SMH on
chemistry and drug discovery benchmark datasets, showing consistent
improvements in predictive performance for target domain ranges.

</details>


### [191] [FlashDP: Private Training Large Language Models with Efficient DP-SGD](https://arxiv.org/abs/2507.01154)
*Liangyu Wang,Junxiao Wang,Jie Ren,Zihang Xiang,David E. Keyes,Di Wang*

Main category: cs.LG

TL;DR: This paper introduces FlashDP, an efficient per-layer DP-SGD method for training large language models with Differential Privacy, achieving significant memory and computational efficiency gains.


<details>
  <summary>Details</summary>
Motivation: To address the privacy concerns of training data in large language models and tackle the inefficiencies of existing Differential Privacy methods (e.g., memory requirements in explicit methods and redundant computations in implicit methods).

Method: The authors propose FlashDP, a per-layer DP-SGD technique that consolidates gradient computation into a fused, cache-friendly step. This avoids recalculating gradients multiple times and reduces memory movement.

Result: FlashDP reduces memory movement by up to 50% and redundant computations by 20% compared to prior methods. It achieves 90% throughput of Non-DP methods on a four-A100 system, while maintaining comparable accuracy for training the Llama-13B model.

Conclusion: FlashDP proves to be an impactful innovation for efficient, privacy-preserving training of large language models. It addresses critical challenges in Differential Privacy integration without compromising performance.

Abstract: As large language models (LLMs) increasingly underpin technological
advancements, the privacy of their training data emerges as a critical concern.
Differential Privacy (DP) serves as a rigorous mechanism to protect this data,
yet its integration via Differentially Private Stochastic Gradient Descent
(DP-SGD) introduces substantial challenges, primarily due to the complexities
of per-sample gradient clipping. Current explicit methods, such as Opacus,
necessitate extensive storage for per-sample gradients, significantly inflating
memory requirements. Conversely, implicit methods like GhostClip reduce storage
needs by recalculating gradients multiple times, which leads to inefficiencies
due to redundant computations. This paper introduces FlashDP, an innovative
cache-friendly per-layer DP-SGD that consolidates necessary operations into a
single task, calculating gradients only once in a fused manner. This approach
not only diminishes memory movement by up to \textbf{50\%} but also cuts down
redundant computations by \textbf{20\%}, compared to previous methods.
Consequently, FlashDP does not increase memory demands and achieves a
\textbf{90\%} throughput compared to the Non-DP method on a four-A100 system
during the pre-training of the Llama-13B model, while maintaining parity with
standard per-layer clipped DP-SGD in terms of accuracy. These advancements
establish FlashDP as a pivotal development for efficient and privacy-preserving
training of LLMs. FlashDP's code has been open-sourced in
https://github.com/kaustpradalab/flashdp.

</details>


### [192] [Diffusion Explorer: Interactive Exploration of Diffusion Models](https://arxiv.org/abs/2507.01178)
*Alec Helbling,Duen Horng Chau*

Main category: cs.LG

TL;DR: The paper introduces Diffusion Explorer, a tool for visualizing and understanding geometric properties of diffusion models in low dimensions.


<details>
  <summary>Details</summary>
Motivation: To simplify the explanation of diffusion models' geometric properties using accessible tools and animations, addressing limitations of current theoretical or architecture-focused resources.

Method: The authors developed Diffusion Explorer, an interactive browser-based tool that lets users train 2D diffusion models and animate their sampling processes.

Result: Diffusion Explorer is available as open-source software and offers a live demo for interactive learning.

Conclusion: Interactive tools like Diffusion Explorer enhance understanding of dynamic systems, particularly diffusion models, by leveraging animation and engaging visualizations.

Abstract: Diffusion models have been central to the development of recent image, video,
and even text generation systems. They posses striking geometric properties
that can be faithfully portrayed in low-dimensional settings. However, existing
resources for explaining diffusion either require an advanced theoretical
foundation or focus on their neural network architectures rather than their
rich geometric properties. We introduce Diffusion Explorer, an interactive tool
to explain the geometric properties of diffusion models. Users can train 2D
diffusion models in the browser and observe the temporal dynamics of their
sampling process. Diffusion Explorer leverages interactive animation, which has
been shown to be a powerful tool for making engaging visualizations of dynamic
systems, making it well suited to explaining diffusion models which represent
stochastic processes that evolve over time. Diffusion Explorer is open source
and a live demo is available at alechelbling.com/Diffusion-Explorer.

</details>


### [193] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee,Konstantinos Barmpas,Yannis Panagakis,Dimitrios Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: Large Brainwave Foundation Models (LBMs) achieve marginal improvements over traditional methods in Brain-Computer Interface (BCI) tasks despite requiring higher computational resources. LoRA reduces the overhead without performance loss.


<details>
  <summary>Details</summary>
Motivation: To evaluate the efficiency and applicability of Large Brainwave Foundation Models in brainwave modeling and identify potential limitations and improvements.

Method: Systematic fine-tuning experiments across multiple BCI benchmark tasks, including memory tasks and sleep classification, supported by ablation studies and Low-Rank Adaptation (LoRA).

Result: State-of-the-art LBMs show slight performance improvement (0.9%-1.2%) over traditional deep architectures but require significantly larger computational resources. LoRA effectively reduces trainable parameters while minimizing performance degradation.

Conclusion: Current LBMs exhibit architectural and training inefficiencies despite marginal performance gains. Redesign and domain-specific development strategies are crucial for leveraging foundation models effectively in BCI applications.

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [194] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Main category: cs.LG

TL;DR: The study investigates whether independently trained vision and language models can align their representations and proposes the Joint Autoencoder Modulator (JAM) framework to achieve this.


<details>
  <summary>Details</summary>
Motivation: To explore whether independently trained vision and language models, with distinct modalities and objectives, can converge towards a shared representational space and how this alignment can be explicitly optimized.

Method: A new framework, Joint Autoencoder Modulator (JAM), is introduced that encourages alignment via reconstruction and cross-modal objectives, implemented as a multi-objective optimization process.

Result: The proposed JAM framework successfully induces alignment between frozen, independently trained vision and language models, demonstrating reliability across various design factors like alignment objectives, layer depths, and model scales.

Conclusion: The JAM framework provides theoretical insights and practical techniques for transforming unimodal models into multimodal ones, supporting alignment even in disjointly trained representations.

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [195] [TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents](https://arxiv.org/abs/2507.01823)
*Dmytro Kuzmenko,Nadiya Shvai*

Main category: cs.LG

TL;DR: The paper introduces a method to condense a high-capacity multi-task reinforcement learning model into a compact, efficient version for resource-limited deployments.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of deploying large reinforcement learning world models in environments with limited computational and storage resources.

Method: Develops a distillation technique to transfer knowledge from a large 317M parameter model into a smaller 1M parameter model, followed by FP16 quantization reducing model size by ~50%.

Result: Achieved state-of-the-art normalized score of 28.45 on MT30 benchmark, significantly higher than the previous 1M parameter model's score of 18.93.

Conclusion: The proposed approach simplifies deployment of multi-task models in resource-constrained environments while enhancing performance, offering advances for reinforcement learning in robotics and other applications.

Abstract: We present a novel approach to knowledge transfer in model-based
reinforcement learning, addressing the critical challenge of deploying large
world models in resource-constrained environments. Our method efficiently
distills a high-capacity multi-task agent (317M parameters) into a compact
model (1M parameters) on the MT30 benchmark, significantly improving
performance across diverse tasks. Our distilled model achieves a
state-of-the-art normalized score of 28.45, surpassing the original 1M
parameter model score of 18.93. This improvement demonstrates the ability of
our distillation technique to capture and consolidate complex multi-task
knowledge. We further optimize the distilled model through FP16 post-training
quantization, reducing its size by $\sim$50\%. Our approach addresses practical
deployment limitations and offers insights into knowledge representation in
large world models, paving the way for more efficient and accessible multi-task
reinforcement learning systems in robotics and other resource-constrained
applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.

</details>


### [196] [Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform](https://arxiv.org/abs/2507.01208)
*Pedro R. X. Carmo,Igor de Moura,Assis T. de Oliveira Filho,Djamel Sadok,Cleber Zanchettin*

Main category: cs.LG

TL;DR: The paper evaluates techniques like distilling and pruning for fast neural network inference to deploy Intrusion Detection Systems (IDS) on low-cost platforms like Raspberry Pi.


<details>
  <summary>Details</summary>
Motivation: Modern vehicles rely on automotive Ethernet for communication, but face safety risks due to flow injection attacks, necessitating cost-effective IDS solutions.

Method: Fast neural network inference methods, including distilling and pruning, are applied and tested for real-time IDS deployment on low-cost hardware.

Result: Achieved intrusion detection times up to 727 Î¼s on Raspberry Pi 4 and AUCROC values of 0.9890.

Conclusion: The proposed neural network inference techniques are effective for real-time IDS on low-cost platforms, addressing safety risks in connected vehicles.

Abstract: Modern vehicles are increasingly connected, and in this context, automotive
Ethernet is one of the technologies that promise to provide the necessary
infrastructure for intra-vehicle communication. However, these systems are
subject to attacks that can compromise safety, including flow injection
attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often
designed to combat this problem, but they require expensive hardware to run in
real time. In this work, we propose to evaluate and apply fast neural network
inference techniques like Distilling and Prunning for deploying IDS models on
low-cost platforms in real time. The results show that these techniques can
achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4,
with AUCROC values of 0.9890.

</details>


### [197] [PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning](https://arxiv.org/abs/2507.01216)
*Xingke Yang,Liang Li,Zhiyi Wan,Sicong Li,Hao Wang,Xiaoqi Qi,Jiang Liu,Tomoaki Ohtsuki,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: The paper introduces PAE MobiLLM, a privacy-aware and efficient method for on-device large language model fine-tuning using server-assisted additive side-tuning to address resource limitations of mobile devices.


<details>
  <summary>Details</summary>
Motivation: Current methods for on-device LLM fine-tuning face challenges like communication burdens and privacy concerns, especially when using server assistance.

Method: PAE MobiLLM incorporates server-side activation caching, a one-token activation shortcut for communication efficiency, and an additive adapter side-network design for privacy.

Result: PAE MobiLLM reduces communication costs, improves convergence speed, and ensures privacy while achieving efficient fine-tuning of LLMs on mobile devices.

Conclusion: PAE MobiLLM effectively balances privacy, efficiency, and resource optimization, making it suitable for on-device LLM fine-tuning with server assistance.

Abstract: There is a huge gap between numerous intriguing applications fostered by
on-device large language model (LLM) fine-tuning (FT) from fresh mobile data
and the limited resources of a mobile device. While existing server-assisted
methods (e.g., split learning or side-tuning) may enable LLM FT on the local
mobile device, they suffer from heavy communication burdens of activation
transmissions, and may disclose data, labels or fine-tuned models to the
server. To address those issues, we develop PAE MobiLLM, a privacy-aware and
efficient LLM FT method which can be deployed on the mobile device via
server-assisted additive side-tuning. To further accelerate FT convergence and
improve computing efficiency, PAE MobiLLM integrates activation caching on the
server side, which allows the server to reuse historical activations and saves
the mobile device from repeatedly computing forward passes for the recurring
data samples. Besides, to reduce communication cost, PAE MobiLLM develops a
one-token (i.e., ``pivot'' token) activation shortcut that transmits only a
single activation dimension instead of full activation matrices to guide the
side network tuning. Last but not least, PAE MobiLLM introduces the additive
adapter side-network design which makes the server train the adapter modules
based on device-defined prediction differences rather than raw ground-truth
labels. In this way, the server can only assist device-defined side-network
computing, and learn nothing about data, labels or fine-tuned models.

</details>


### [198] [Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling](https://arxiv.org/abs/2507.01235)
*Bara Rababa,Bilal Farooq*

Main category: cs.LG

TL;DR: Explores quantum machine learning methods (QSVM and QNN) for analyzing pedestrian stress using skin conductance data, with QNN achieving better classification accuracy.


<details>
  <summary>Details</summary>
Motivation: Quantum computing methods can enhance machine learning for complex, high-dimensional data, such as intelligent transportation systems requiring stress modeling.

Method: Two quantum models (QSVM and QNN) were developed using an eight-qubit ZZ feature map in Pennylane to classify skin conductance responses.

Result: QSVM showed overfitting and poor test accuracy (45%), while QNN outperformed with a 55% test accuracy compared to classic models.

Conclusion: Quantum Neural Network (QNN) offers better reliability for stress classification modeling than Quantum Support Vector Machine (QSVM) and classical approaches.

Abstract: Quantum computing has opened new opportunities to tackle complex machine
learning tasks, for instance, high-dimensional data representations commonly
required in intelligent transportation systems. We explore quantum machine
learning to model complex skin conductance response (SCR) events that reflect
pedestrian stress in a virtual reality road crossing experiment. For this
purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature
map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and
an eight-qubit ZZ feature map, were developed on Pennylane. The dataset
consists of SCR measurements along with features such as the response amplitude
and elapsed time, which have been categorized into amplitude-based classes. The
QSVM achieved good training accuracy, but had an overfitting problem, showing a
low test accuracy of 45% and therefore impacting the reliability of the
classification model. The QNN model reached a higher test accuracy of 55%,
making it a better classification model than the QSVM and the classic versions.

</details>


### [199] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/abs/2507.01241)
*Di Zhang,Yihang Zhang*

Main category: cs.LG

TL;DR: The paper introduces a stochastic conjugate subgradient method with adaptive sampling as an alternative to SGD for training LLMs, demonstrating faster convergence and improved scalability.


<details>
  <summary>Details</summary>
Motivation: SGDâs effectiveness in training LLMs faces challenges in large-scale scenarios, highlighting the need for improved optimization methods.

Method: The approach combines a stochastic conjugate subgradient method, adaptive sampling based on sample complexity analysis, and an AdamW-like algorithm for adaptive step sizing.

Result: The proposed method achieves faster convergence, improved scalability, and surpasses SGD in both speed and accuracy for LLM training.

Conclusion: The new optimization algorithm effectively addresses challenges in LLMs training, offering a scalable and accurate alternative to traditional SGD.

Abstract: Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [200] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/abs/2507.01271)
*Tatsuki Kawakami,Kazuki Egashira,Atsuyuki Miyai,Go Irie,Kiyoharu Aizawa*

Main category: cs.LG

TL;DR: The paper introduces PULSE, a protocol for evaluating unlearning methods in large multimodal models (LMMs) with a focus on pre-trained knowledge unlearning and long-term sustainability.


<details>
  <summary>Details</summary>
Motivation: With privacy and copyright concerns growing in large language and multimodal models, the need for methods allowing models to forget specific learned information became critical.

Method: The study proposes PULSE protocol, emphasizing pre-trained knowledge unlearning and sequential sustainability evaluation frameworks.

Result: Existing methods successfully unlearn fine-tuned knowledge but struggle with pre-trained knowledge and encounter performance degradation for sequential unlearning requests.

Conclusion: Improved unlearning techniques are necessary to address pre-trained knowledge and ensure long-term sustainability of operations for practical applications in LMMs.

Abstract: In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [201] [Neural Hamiltonian Operator](https://arxiv.org/abs/2507.01313)
*Qian Qi*

Main category: cs.LG

TL;DR: The paper tackles high-dimensional stochastic control problems by reformulating them using deep learning through a Neural Hamiltonian Operator (NHO), offering universal approximation proofs and analytical clarity.


<details>
  <summary>Details</summary>
Motivation: The challenge of solving high-dimensional stochastic control problems is compounded by the curse of dimensionality, making traditional methods like dynamic programming inefficient. The paper seeks an alternative approach grounded in Pontryagin's Maximum Principle.

Method: Introduce and define the Neural Hamiltonian Operator (NHO), combining neural networks to parameterize coupled FBSDE dynamics. Train networks to satisfy consistency conditions derived from the Maximum Principle.

Result: The proposed NHO framework aligns stochastic control problem-solving with statistical inference principles, demonstrating universal approximation capabilities and clarity in optimization challenges.

Conclusion: NHOs provide a rigorous operator-theoretic framework to tackle high-dimensional stochastic control problems using deep learning, bridging mathematical theory and computational efficiency.

Abstract: Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [202] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)
*Zhiyao Ren,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.LG

TL;DR: This paper explores the vulnerability of in-context learning (ICL) in large language models (LLMs) to backdoor attacks and proposes a defense mechanism called ICLShield.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the critical vulnerability of ICL in LLMs to backdoor attacks, highlighting the need to address this security threat without altering model parameters.

Method: The authors introduce the dual-learning hypothesis and propose ICLShield, which dynamically adjusts the concept preference ratio in ICL demonstrations using confidence and similarity scores to mitigate backdoor attacks.

Result: Experiments show that ICLShield achieves state-of-the-art defense effectiveness, outclassing existing methods by an average of 26.02%, and works effectively across various models and tasks, including closed-source models like GPT-4.

Conclusion: ICLShield is an effective defense mechanism that significantly enhances the robustness of ICL in LLMs against backdoor attacks while maintaining adaptability to different models and tasks.

Abstract: In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [203] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/abs/2507.01327)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Jiansong Chen,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: The paper introduces APARL, a framework for detecting abnormal events in customer dialogues, aimed at improving adaptability and robustness.


<details>
  <summary>Details</summary>
Motivation: To address challenges in detecting anomalies in complex customer service dialogues and enhance out-of-domain generalization for diverse business scenarios.

Method: APARL employs a dual-loop dynamic curriculum with reinforcement learning leveraging large language models, focusing progressively on challenging samples.

Result: In food delivery dialogue tasks, APARL improved adaptability and OOD transferability, achieving an highest F1 score and significant performance improvements.

Conclusion: APARL demonstrates its potential for industrial application, offering improved efficiency and commercial benefits in anomaly detection models.

Abstract: Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [204] [Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion](https://arxiv.org/abs/2507.01354)
*Chugang Yi,Minghan Yu,Weikang Qian,Yixin Wen,Haizhao Yang*

Main category: cs.LG

TL;DR: The paper introduces the Wavelet Diffusion Model (WDM) for achieving high-resolution precipitation downscaling (1 km) with a 9x speedup over traditional methods.


<details>
  <summary>Details</summary>
Motivation: The need for kilometer-scale precipitation data for hydrological and weather modeling surpasses the resolution provided by current global products, requiring more realistic and efficient downscaling techniques.

Method: WDM uses a wavelet-based conditional diffusion model trained with radar data to focus on high-frequency wavelet coefficients, facilitating better realism and accuracy in generating 1-km precipitation fields.

Result: The model outperforms traditional pixel-based diffusion models both visually and computationally, producing higher-quality results with fewer artifacts and greater efficiency.

Conclusion: WDM successfully addresses the challenges of generating high-resolution precipitation data, enhancing accuracy and speed for geoscience applications.

Abstract: Effective hydrological modeling and extreme weather analysis demand
precipitation data at a kilometer-scale resolution, which is significantly
finer than the 10 km scale offered by standard global products like IMERG. To
address this, we propose the Wavelet Diffusion Model (WDM), a generative
framework that achieves 10x spatial super-resolution (downscaling to 1 km) and
delivers a 9x inference speedup over pixel-based diffusion models. WDM is a
conditional diffusion model that learns the learns the complex structure of
precipitation from MRMS radar data directly in the wavelet domain. By focusing
on high-frequency wavelet coefficients, it generates exceptionally realistic
and detailed 1-km precipitation fields. This wavelet-based approach produces
visually superior results with fewer artifacts than pixel-space models, and
delivers a significant gains in sampling efficiency. Our results demonstrate
that WDM provides a robust solution to the dual challenges of accuracy and
speed in geoscience super-resolution, paving the way for more reliable
hydrological forecasts.

</details>


### [205] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/abs/2507.01381)
*Tong Liu,Yinuo Wang,Xujie Song,Wenjun Zou,Liangfa Chen,Likun Wang,Bin Shuai,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: The paper introduces DSAC-D, a reinforcement learning algorithm employing diffusion models to reduce value estimation bias and enable multimodal policy representation, achieving state-of-the-art results in control tasks.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning methods often use unimodal distributions, which can bias value function estimation and degrade algorithm performance.

Method: A distributional reinforcement learning approach is developed that employs a diffusion value network for multimodal policy iteration, leveraging reverse sampling and dual diffusion in both the value and policy networks.

Result: The method achieves state-of-the-art results across 9 MuJoCo control tasks, significantly reduces estimation bias, and improves total returns by over 10%. Real vehicle testing confirms its effectiveness in characterizing multimodal driving behaviors.

Conclusion: DSAC-D successfully addresses value estimation bias and captures multimodal distributions, outperforming traditional algorithms both in simulation and real-world scenarios.

Abstract: Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [206] [Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning](https://arxiv.org/abs/2507.01389)
*Anbang Wang,Dunbo Cai,Yu Zhang,Yangqing Huang,Xiangyang Feng,Zhihong Zhang*

Main category: cs.LG

TL;DR: The paper presents an enhanced surrogate model with slack variables integrated into factorization machines, unified for single-step optimization, used in predicting drug combination effects, showing improved performance.


<details>
  <summary>Details</summary>
Motivation: To enhance surrogate modeling for complex systems by optimizing input-output mapping through integrated quantum annealing and incorporating higher-order feature interactions.

Method: Introduced slack variables into factorization machines and their Ising representation, unifying the training phase into a single iterative process for effectively optimizing the surrogate models.

Result: Experiments on predicting drug combination effects show significant performance improvement with the inclusion of slack variables.

Conclusion: The approach demonstrates a promising method for improving surrogate models' efficiency and performance, leveraging potential quantum computational advantages.

Abstract: Recently, a surrogate model was proposed that employs a factorization machine
to approximate the underlying input-output mapping of the original system, with
quantum annealing used to optimize the resulting surrogate function. Inspired
by this approach, we propose an enhanced surrogate model that incorporates
additional slack variables into both the factorization machine and its
associated Ising representation thereby unifying what was by design a two-step
process into a single, integrated step. During the training phase, the slack
variables are iteratively updated, enabling the model to account for
higher-order feature interactions. We apply the proposed method to the task of
predicting drug combination effects. Experimental results indicate that the
introduction of slack variables leads to a notable improvement of performance.
Our algorithm offers a promising approach for building efficient surrogate
models that exploit potential quantum advantages.

</details>


### [207] [Decomposing Prediction Mechanisms for In-Context Recall](https://arxiv.org/abs/2507.01414)
*Sultan Daniels,Dylan Davis,Dhruv Gautam,Wentinn Liao,Gireeja Ranade,Anant Sahai*

Main category: cs.LG

TL;DR: The paper introduces a toy problem combining continuous in-context learning and associative recall using transformer models. The study observes distinct learning phases for separate mechanisms handling sequence prediction tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore and understand how transformer models perform in-context learning, specifically focusing on their ability to recall and predict sequences based on discrete symbolic labels and previous states.

Method: The authors pretrain transformer models on synthetic data generated from deterministic dynamical systems and analyze their ability to recall and predict sequences. Mechanistic analysis via edge pruning and out-of-distribution tests is performed to uncover the underlying learning dynamics.

Result: The study identifies two distinct mechanisms for sequence prediction: one for associative recall using symbolic labels and another for Bayesian-style prediction based on prior context. These mechanisms exhibit different learning dynamics, with one capability emerging earlier in training than the other.

Conclusion: The presence of phase transitions in learning behaviors indicates that multi-mechanism dynamics are not just artifacts of the toy problem but generalize to more complex tasks, as demonstrated in ICL translation experiments.

Abstract: We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

</details>


### [208] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Main category: cs.LG

TL;DR: This paper presents SPRO, a framework for process-aware reinforcement learning that integrates training efficiency and improved performance without extra computational costs.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a unified theoretical framework and computational inefficiencies in process-level reinforcement learning for LLMs.

Method: SPRO introduces intrinsic rewards derived from the policy model and cumulative rewards paired with a Masked Step Advantage (MSA) for action advantage estimation.

Result: SPRO achieves 3.4x training efficiency, 17.5% higher test accuracy, stable policy entropy, and reduces average response length by 1/3 compared to GRPO.

Conclusion: SPRO is an effective, industrially implementable method for process-aware RL in LLMs, outperforming existing frameworks without additional computational demands.

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [209] [Cross-platform Smartphone Positioning at Museums](https://arxiv.org/abs/2507.01469)
*Alessio Ferrato,Fabio Gasparetti,Carla Limongelli,Stefano Mastandrea,Giuseppe Sansonetti,JoaquÃ­n Torres-Sospedra*

Main category: cs.LG

TL;DR: This paper introduces BAR, a unique RSS dataset specifically tailored for museum environments, collected using Android and iOS platforms to enhance IPS development and evaluation.


<details>
  <summary>Details</summary>
Motivation: Cultural heritage institutions face challenges in implementing Indoor Positioning Systems (IPSs), with a need for specialized RSS datasets to cater to museum environments.

Method: The authors collected RSS data in front of 90 artworks across 13 museum rooms using Android and iOS devices and implemented proximity-based and $k$-NN algorithms for experimental classification.

Result: The study presents a dataset that reflects museum environments and demonstrates baseline positioning classification using the proximity-based method and $k$-NN algorithms.

Conclusion: The BAR dataset serves as a valuable resource for advancing research in IPSs tailored to cultural heritage sites, outlining opportunities for improved navigation and visitor experiences.

Abstract: Indoor Positioning Systems (IPSs) hold significant potential for enhancing
visitor experiences in cultural heritage institutions. By enabling personalized
navigation, efficient artifact organization, and better interaction with
exhibits, IPSs can transform the modalities of how individuals engage with
museums, galleries and libraries. However, these institutions face several
challenges in implementing IPSs, including environmental constraints, technical
limits, and limited experimentation. In other contexts, Received Signal
Strength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have
emerged as preferred solutions due to their non-invasive nature and minimal
infrastructure requirements. Nevertheless, the lack of publicly available RSS
datasets that specifically reflect museum environments presents a substantial
barrier to developing and evaluating positioning algorithms designed for the
intricate spatial characteristics typical of cultural heritage sites. To
address this limitation, we present BAR, a novel RSS dataset collected in front
of 90 artworks across 13 museum rooms using two different platforms, i.e.,
Android and iOS. Additionally, we provide an advanced position classification
baseline taking advantage of a proximity-based method and $k$-NN algorithms. In
our analysis, we discuss the results and offer suggestions for potential
research directions.

</details>


### [210] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/abs/2507.01470)
*Yannick Molinghen,Tom Lenaerts*

Main category: cs.LG

TL;DR: This paper challenges the assumption that reward frequency is a reliable measure of task difficulty in reinforcement learning, highlighting issues with 'zero-incentive dynamics,' where key subgoals are unrewarded.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the structural challenges in reinforcement learning when essential transitions, vital to task success, do not offer direct rewards.

Method: The paper identifies 'zero-incentive dynamics' and evaluates the performance of state-of-the-art subgoal algorithms, analyzing sensitivity to timing between subgoal completion and eventual rewards.

Result: It is found that current deep subgoal-based methods fail to exploit zero-incentive dynamics and are highly sensitive to the temporal reward structure.

Conclusion: Current reinforcement learning approaches have fundamental limitations in handling tasks with latent structures, necessitating new mechanisms beyond immediate reward-driven learning.

Abstract: This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [211] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Main category: cs.LG

TL;DR: The paper introduces Prefix-RFT, a hybrid approach combining Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT), yielding superior performance and requiring minimal modifications to existing RFT pipelines.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the trade-offs in post-training methods for large language models, balancing SFT's mimicry of demonstrations and RFT's exploration-driven learning.

Method: The Prefix-RFT methodology combines SFT and RFT paradigms to synergize learning from demonstration and exploration.

Result: Prefix-RFT outperforms standalone SFT, RFT, and mixed-policy RFT methods, demonstrating robustness to variations in demonstration data.

Conclusion: Prefix-RFT highlights the complementary nature of SFT and RFT, suggesting a unified paradigm for enhancing LLM post-training techniques.

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [212] [Loss Functions in Diffusion Models: A Comparative Study](https://arxiv.org/abs/2507.01516)
*Dibyanshu Kumar,Philipp Vaeth,Magda GregorovÃ¡*

Main category: cs.LG

TL;DR: This paper systematically analyzes loss functions in diffusion models, unifies them under a variational framework, and explores their impact empirically.


<details>
  <summary>Details</summary>
Motivation: Investigate the critical differences among loss functions in diffusion models and unify them under a common theoretical framework.

Method: Examines relationships between loss functions using the variational lower bound framework and conducts empirical studies to analyze their performance divergences.

Result: Finds conditions where loss functions diverge in performance and evaluates their impact on model goals like sample quality and likelihood estimation.

Conclusion: Provides a unified understanding of loss functions to guide efficient and goal-oriented diffusion model design in future research.

Abstract: Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

</details>


### [213] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/abs/2507.01522)
*Koen Ponse,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: The paper introduces Chargax, a JAX-based system for simulating EV charging stations, making RL training over 100x faster.


<details>
  <summary>Details</summary>
Motivation: To overcome the inefficiencies and bottlenecks caused by high sample complexity and expensive simulations in RL approaches addressing sustainable energy challenges.

Method: Chargax leverages JAX for accelerated and modular simulation of electric vehicle charging stations, enabling faster training of RL agents.

Result: Chargax achieves substantial computational speed improvements, delivering 100x-1000x faster training and supports diverse configurations in realistic scenarios.

Conclusion: Chargax enhances RL application in sustainable energy systems by combining computational efficiency and environmental realism, thereby tackling operational challenges in energy grids.

Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [214] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Main category: cs.LG

TL;DR: Gradient-based optimization faces privacy and overfitting issues despite its efficiency. BBoxER, a black-box evolutionary optimization method, addresses these concerns for post-training large language models (LLMs) while improving performance.


<details>
  <summary>Details</summary>
Motivation: The need to address privacy, security, and overfitting issues in gradient-based optimization, while providing scalable solutions for restricted or adversarial environments.

Method: Developed BBoxER, which leverages an evolutionary black-box approach with an information bottleneck for implicit data compression and theoretical guarantees.

Result: Experimental results showed that BBoxER improves reasoning task performance, offers robustness, generalization, and privacy in LLMs.

Conclusion: BBoxER is a modular, lightweight enhancement suitable for LLM scenarios requiring privacy preservation, scalability, and reliable generalization.

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [215] [MARVIS: Modality Adaptive Reasoning over VISualizations](https://arxiv.org/abs/2507.01544)
*Benjamin Feuer,Lennart Purucker,Oussama Elachqar,Chinmay Hegde*

Main category: cs.LG

TL;DR: MARVIS is a training-free method enabling small vision-language models to handle various data modalities efficiently by converting latent embeddings into visual formats.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the trade-off between the flexibility of foundation models and the high accuracy of specialized machine-learning models for domain-specific tasks.

Method: MARVIS involves transforming latent embeddings into visual representations, leveraging vision-language models' reasoning skills across diverse modalities.

Result: MARVIS demonstrated competitive performance across vision, audio, biological, and tabular data types, surpassing Gemini by 16% on average, while requiring no domain-specific training.

Conclusion: MARVIS bridges the flexibility-accuracy gap, providing a training-free and adaptable solution for multi-modality data interpretation using small vision-language models.

Abstract: Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

</details>


### [216] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Main category: cs.LG

TL;DR: MetaStone-S1 is a reflective generative model using a self-supervised process reward model (SPRM) to achieve OpenAI o3's performance efficiently, with only 32 billion parameters. It offers controllable reasoning modes and follows a test time scaling law.


<details>
  <summary>Details</summary>
Motivation: The authors aimed to develop a generative model that combines efficient reasoning with high performance and self-supervised learning to match or rival existing models while being more resource-efficient.

Method: The MetaStone-S1 uses a shared backbone network with task-specific heads for token prediction and process scoring, integrating a policy and process reward model (PRM) into a single interface without requiring additional process annotations. This reduces PRM parameters by over 99% and enables test time scaling (TTS) with varying reasoning effort modes.

Result: MetaStone-S1 demonstrated performance comparable to OpenAI-o3-mini with a smaller parameter count (32 billion). A scaling law was also identified, linking thinking computation to TTS performance.

Conclusion: The MetaStone-S1 is an efficient and open-source reflective generative model achieving high performance with controllable reasoning efforts, supporting broader research innovation.

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


### [217] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Main category: cs.LG

TL;DR: This paper investigates the effects of "zapping" (resampling weights of the last layer) in continual and few-shot transfer learning scenarios. Results imply that zapping accelerates recovery when transferring to new domains and interacts dynamically with task performance and optimizer choice.


<details>
  <summary>Details</summary>
Motivation: The motivation is to understand the mechanisms behind the observed benefits of weight resampling techniques (zapping) in challenging machine learning tasks like continual learning and few-shot transfer learning.

Method: The study examines neural networks during continual learning, investigating the interplay of zapping and optimizer choices, and evaluates their impact on transfer shock recovery and multi-task performance using datasets of handwritten characters and natural images.

Result: Models that used zapping recover faster after domain transfer. The choice of optimizer significantly influences task synergy and interference during sequential learning.

Conclusion: The paper highlights zapping and optimizer choice as key factors in neural network learning dynamics, with implications for handling continual and transfer learning challenges.

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [218] [A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning](https://arxiv.org/abs/2507.01581)
*Masood Jan,Wafa Njima,Xun Zhang*

Main category: cs.LG

TL;DR: This paper presents a Federated Learning (FL)-based indoor localization method using a Deep Neural Network (DNN) to enhance privacy and maintain performance levels close to centralized models.


<details>
  <summary>Details</summary>
Motivation: To address the significant errors, privacy issues, bandwidth inefficiency, and server reliability challenges of traditional and machine learning-based indoor localization systems.

Method: A Federated Learning (FL) approach is implemented using a Deep Neural Network (DNN) for dynamic indoor localization.

Result: The proposed FL approach achieves performance comparable to centralized models while improving privacy, bandwidth efficiency, and server reliability.

Conclusion: The FL-based method offers a secure and efficient solution for indoor localization, showcasing its potential for privacy-enhanced IoT applications.

Abstract: Location information serves as the fundamental element for numerous Internet
of Things (IoT) applications. Traditional indoor localization techniques often
produce significant errors and raise privacy concerns due to centralized data
collection. In response, Machine Learning (ML) techniques offer promising
solutions by capturing indoor environment variations. However, they typically
require central data aggregation, leading to privacy, bandwidth, and server
reliability issues. To overcome these challenges, in this paper, we propose a
Federated Learning (FL)-based approach for dynamic indoor localization using a
Deep Neural Network (DNN) model. Experimental results show that FL has the
nearby performance to Centralized Model (CL) while keeping the data privacy,
bandwidth efficiency and server reliability. This research demonstrates that
our proposed FL approach provides a viable solution for privacy-enhanced indoor
localization, paving the way for advancements in secure and efficient indoor
localization systems.

</details>


### [219] [Analysis of Muon's Convergence and Critical Batch Size](https://arxiv.org/abs/2507.01598)
*Naoki Sato,Hiroki Naganuma,Hideaki Iiduka*

Main category: cs.LG

TL;DR: The paper introduces Muon, a matrix-structured neural network optimizer, and provides theoretical convergence proofs for its variants while highlighting weight decay benefits and experimental validation.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency and theoretical understanding of neural network optimizers by leveraging the matrix structure inherent in neural parameters.

Method: The paper theoretically analyzes Muon, deriving convergence proofs for its variants, studying the effects of weight decay, clarifying the interplay between weight decay and learning rate, and deriving the critical batch size for computational cost minimization.

Result: Muon demonstrates tighter bounds on parameter and gradient norms when weight decay is applied, and the critical batch size minimizing stochastic computational cost is derived and validated experimentally.

Conclusion: Muon showcases theoretical and practical advantages as a neural network optimizer, with significant implications for computational efficiency and optimization strategies.

Abstract: This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

</details>


### [220] [Kernel Recursive Least Squares Dictionary Learning Algorithm](https://arxiv.org/abs/2507.01636)
*Ghasem Alipoor,Karl Skretting*

Main category: cs.LG

TL;DR: The paper introduces an online dictionary learning algorithm for kernel-based sparse representations with a focus on efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the efficiency and scalability of kernel-based sparse representation methods while maintaining high classification accuracy.

Method: The algorithm uses a high-dimensional feature space for sparse signal representation and updates the dictionary recursively with a novel Recursive Least Squares (RLS)-based method, suitable for single samples or mini-batches.

Result: Experiments on four datasets show the algorithm outperforms existing methods in online kernel dictionary learning and achieves classification accuracy comparable to batch-trained models at lower computational costs.

Conclusion: The proposed method combines efficiency with strong performance, marking a significant advancement in online kernel dictionary learning approaches.

Abstract: We propose an efficient online dictionary learning algorithm for kernel-based
sparse representations. In this framework, input signals are nonlinearly mapped
to a high-dimensional feature space and represented sparsely using a virtual
dictionary. At each step, the dictionary is updated recursively using a novel
algorithm based on the recursive least squares (RLS) method. This update
mechanism works with single samples or mini-batches and maintains low
computational complexity. Experiments on four datasets across different domains
show that our method not only outperforms existing online kernel dictionary
learning approaches but also achieves classification accuracy close to that of
batch-trained models, while remaining significantly more efficient.

</details>


### [221] [Dance Dance ConvLSTM](https://arxiv.org/abs/2507.01644)
*Miguel O'Malley*

Main category: cs.LG

TL;DR: This paper introduces Dance Dance ConvLSTM (DDCL), a method leveraging ConvLSTM models to enhance the accuracy of automatic Dance Dance Revolution chart generation.


<details>
  <summary>Details</summary>
Motivation: To improve upon the existing methodology of automatic chart generation in Dance Dance Revolution, as proposed by the earlier Dance Dance Convolution (DDC) algorithm, especially in terms of accuracy.

Method: The paper utilizes a ConvLSTM-based model as an enhancement over the CNN-LSTM architecture employed in the 2017 DDC algorithm.

Result: The new ConvLSTM-based model demonstrated a substantial increase in accuracy for automatically generating Dance Dance Revolution charts.

Conclusion: Dance Dance ConvLSTM (DDCL) successfully builds on and surpasses the original DDC methodology, showcasing improved performance in chart generation for rhythm games.

Abstract: \textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

</details>


### [222] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/abs/2507.01649)
*Yoav Gelberg,Yam Eitan,Aviv Navon,Aviv Shamsian,Theo,Putterman,Michael Bronstein,Haggai Maron*

Main category: cs.LG

TL;DR: The paper introduces GradMetaNet, a novel architecture specifically designed for processing gradients of neural networks, guided by principles of equivariance, curvature-aware processing, and efficient representation.


<details>
  <summary>Details</summary>
Motivation: Existing approaches to gradient processing in neural networks lack architectures designed specifically for handling gradients, reducing their effectiveness and adaptability.

Method: GradMetaNet utilizes three design principles: equivariance to preserve permutation symmetries, multi-data processing for curvature information, and rank-1 decomposition for efficient representation.

Result: GradMetaNet was demonstrated to outperform previous methods in tasks like learned optimization, INR editing, and estimation of loss landscape curvature on MLPs and transformers.

Conclusion: GradMetaNet is a universal and effective architecture for gradient-based tasks, surpassing prior methods' limitations and enabling advanced processing and application in neural networks.

Abstract: Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [223] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
*Zhenyu Han,Ansheng You,Haibo Wang,Kui Luo,Guang Yang,Wenqi Shi,Menglong Chen,Sicheng Zhang,Zeshun Lan,Chunshi Deng,Huazhong Ji,Wenjie Liu,Yu Huang,Yixiang Zhang,Chenyi Pan,Jing Wang,Xin Huang,Chunsheng Li,Jianping Wu*

Main category: cs.LG

TL;DR: AsyncFlow is an asynchronous RL framework addressing scalability and resource inefficiencies in post-training for LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing RL frameworks tied to LLM post-training suffer from scalability issues and workload imbalance, requiring innovative solutions.

Method: AsyncFlow introduces a distributed data module for streaming and a producer-consumer-based workflow ensuring automated pipeline overlapping and load balancing.

Result: Experiments show AsyncFlow achieves a 1.59x throughput improvement over baseline frameworks.

Conclusion: AsyncFlow offers efficient and modular RL post-training support with actionable insights for future RL system designs.

Abstract: Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [224] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/abs/2507.01693)
*Adrians Skapars,Edoardo Manino,Youcheng Sun,Lucas C. Cordeiro*

Main category: cs.LG

TL;DR: The paper introduces SODA, a novel algorithm for reconstructing exact inputs that led to outputs from large language models (LLMs), surpassing existing methods in precision.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the forensic challenge of reconstructing the input that generated a specific LLM output, an area not tackled by current auditing practices.

Method: The authors formalize the input reconstruction as a discrete optimization problem with a unique global minimum. They introduce the gradient-based SODA algorithm utilizing continuous relaxation, periodic restarts, and parameter decay.

Result: SODA achieves a recovery rate of 79.5% for shorter, out-of-distribution inputs from next-token logits, with no false positives. However, it is less effective for longer input sequences.

Conclusion: Standard LLM deployment practices may adequately protect against abuses of this method, as SODA struggles to extract inputs from longer sequences. The research advances forensic capabilities in LLM auditing.

Abstract: While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


### [225] [PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution](https://arxiv.org/abs/2507.01695)
*Omkar Shende,Gayathri Ananthanarayanan,Marcello Traiola*

Main category: cs.LG

TL;DR: The paper introduces PERTINENCE, a technique that dynamically selects the most efficient pre-trained model for input processing to reduce computation without compromising on accuracy.


<details>
  <summary>Details</summary>
Motivation: Large DNN models are highly accurate but resource-intensive; there is a need for methods to reduce reliance on such models while maintaining accuracy.

Method: The authors use a genetic algorithm to train an input dispatcher that dynamically chooses the optimal pre-trained model based on input complexity, aiming for a balance between accuracy and computational efficiency.

Result: PERTINENCE was tested on CNNs trained on CIFAR-10/100 and Vision Transformers on TinyImageNet, showing comparable or better accuracy with up to 36% fewer operations.

Conclusion: Dynamic model selection using PERTINENCE can significantly reduce computational cost while preserving or improving accuracy, providing an efficient alternative to large DNN models.

Abstract: Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable
ability to model complex patterns across various domains such as computer
vision, speech recognition, robotics, etc. While large DNN models are often
more accurate than simpler, lightweight models, they are also resource- and
energy-hungry. Hence, it is imperative to design methods to reduce reliance on
such large models without significant degradation in output accuracy. The high
computational cost of these models is often necessary only for a reduced set of
challenging inputs, while lighter models can handle most simple ones. Thus,
carefully combining properties of existing DNN models in a dynamic, input-based
way opens opportunities to improve efficiency without impacting accuracy.
  In this work, we introduce PERTINENCE, a novel online method designed to
analyze the complexity of input features and dynamically select the most
suitable model from a pre-trained set to process a given input effectively. To
achieve this, we employ a genetic algorithm to explore the training space of an
ML-based input dispatcher, enabling convergence towards the Pareto front in the
solution space that balances overall accuracy and computational efficiency.
  We showcase our approach on state-of-the-art Convolutional Neural Networks
(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers
(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's
ability to provide alternative solutions to existing state-of-the-art models in
terms of trade-offs between accuracy and number of operations. By
opportunistically selecting among models trained for the same task, PERTINENCE
achieves better or comparable accuracy with up to 36% fewer operations.

</details>


### [226] [Variational Graph Convolutional Neural Networks](https://arxiv.org/abs/2507.01699)
*Illia Oleksiienko,Juho Kanniainen,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: This paper proposes Variational Neural Networks for improving Graph Convolutional Networks' accuracy and explainability by estimating model uncertainty and layer-wise attentions.


<details>
  <summary>Details</summary>
Motivation: To enhance explainability and accuracy of Graph Convolutional Networks, particularly in critical applications requiring verification of model results.

Method: Introduces Variational Neural Network versions of spatial and spatio-temporal Graph Convolutional Networks, estimating uncertainty in model outputs and layer-wise attentions.

Result: Applied on Finnish board membership, NTU-60, NTU-120, and Kinetics datasets, demonstrating improved model accuracy and estimation of model uncertainties.

Conclusion: The approach improves both accuracy and explainability of Graph Convolutional Networks, showcasing potential across diverse tasks such as social trading analysis and human action recognition.

Abstract: Estimation of model uncertainty can help improve the explainability of Graph
Convolutional Networks and the accuracy of the models at the same time.
Uncertainty can also be used in critical applications to verify the results of
the model by an expert or additional models. In this paper, we propose
Variational Neural Network versions of spatial and spatio-temporal Graph
Convolutional Networks. We estimate uncertainty in both outputs and layer-wise
attentions of the models, which has the potential for improving model
explainability. We showcase the benefits of these models in the social trading
analysis and the skeleton-based human action recognition tasks on the Finnish
board membership, NTU-60, NTU-120 and Kinetics datasets, where we show
improvement in model accuracy in addition to estimated model uncertainties.

</details>


### [227] [Relational Causal Discovery with Latent Confounders](https://arxiv.org/abs/2507.01700)
*Andrea Piras,Matteo Negro,Ragib Ahsan,David Arbour,Elena Zheleva*

Main category: cs.LG

TL;DR: The paper introduces RelFCI, a new causal discovery algorithm for relational data that addresses the challenge of latent confounders, demonstrating its effectiveness in identifying causal structures.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the limitations of existing causal discovery algorithms, which either assume i.i.d data or causal sufficiencyâboth of which are unrealistic for many real-world relational datasets.

Method: The approach expands upon the Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms, introduces new graphical models, and establishes theoretical guarantees for relational d-separation with latent confounders.

Result: The proposed RelFCI algorithm is shown experimentally to correctly identify causal structures in relational data with latent confounders.

Conclusion: RelFCI is a sound and complete solution for causal discovery in relational domains, overcoming prior limitations and demonstrating its value in handling latent confounders.

Abstract: Estimating causal effects from real-world relational data can be challenging
when the underlying causal model and potential confounders are unknown. While
several causal discovery algorithms exist for learning causal models with
latent confounders from data, they assume that the data is independent and
identically distributed (i.i.d.) and are not well-suited for learning from
relational data. Similarly, existing relational causal discovery algorithms
assume causal sufficiency, which is unrealistic for many real-world datasets.
To address this gap, we propose RelFCI, a sound and complete causal discovery
algorithm for relational data with latent confounders. Our work builds upon the
Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms
and it defines new graphical models, necessary to support causal discovery in
relational domains. We also establish soundness and completeness guarantees for
relational d-separation with latent confounders. We present experimental
results demonstrating the effectiveness of RelFCI in identifying the correct
causal structure in relational causal models with latent confounders.

</details>


### [228] [B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling](https://arxiv.org/abs/2507.01714)
*Kevin Innerebner,Franz M. Rohrhofer,Bernhard C. Geiger*

Main category: cs.LG

TL;DR: The paper proposes an enhancement in training physics-informed neural networks (PINNs) using a Bayesian approach, eliminating the need for an ensemble and integrating posterior variance evaluation.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the convergence issues faced by PINNs in forward problems, particularly the challenge in propagating information to regions with less-defined solutions.

Method: The authors replace the ensemble approach by utilizing a Bayesian PINN and assessing the PINN's posterior variance for information propagation.

Result: The Bayesian PINN-based approach outperformed ensemble-based methods on multiple benchmark problems, demonstrating its effectiveness.

Conclusion: The paper concludes that this Bayesian approach is both mathematically principled and competitive, offering improvements in PINN training dynamics.

Abstract: Training physics-informed neural networks (PINNs) for forward problems often
suffers from severe convergence issues, hindering the propagation of
information from regions where the desired solution is well-defined.
Haitsiukevich and Ilin (2023) proposed an ensemble approach that extends the
active training domain of each PINN based on i) ensemble consensus and ii)
vicinity to (pseudo-)labeled points, thus ensuring that the information from
the initial condition successfully propagates to the interior of the
computational domain.
  In this work, we suggest replacing the ensemble by a Bayesian PINN, and
consensus by an evaluation of the PINN's posterior variance. Our experiments
show that this mathematically principled approach outperforms the ensemble on a
set of benchmark problems and is competitive with PINN ensembles trained with
combinations of Adam and LBFGS.

</details>


### [229] [Revisiting Learning Rate Control](https://arxiv.org/abs/2507.01724)
*Micha Henheik,Theresa Eimer,Marius Lindauer*

Main category: cs.LG

TL;DR: The paper evaluates different paradigms for learning rate control in deep learning and highlights the challenges in algorithm selection methods for optimization.


<details>
  <summary>Details</summary>
Motivation: To assess and improve methods for learning rate control, a key hyperparameter in deep learning, as current approaches often lack reliability across diverse settings.

Method: Comparison of approaches across multi-fidelity hyperparameter optimization, fixed schedules, hyperparameter-free learning, and analysis on their performance in deep learning tasks.

Result: Many existing methods work well in specific scenarios but fail to provide consistent reliability across diverse deep learning environments; hyperparameter optimization loses effectiveness with increasing model and task complexity.

Conclusion: There is a need for improved algorithm selection methods and exploration of finetunable, meta-learning methods to address limitations and enhance learning rate control in deep learning.

Abstract: The learning rate is one of the most important hyperparameters in deep
learning, and how to control it is an active area within both AutoML and deep
learning research. Approaches for learning rate control span from classic
optimization to online scheduling based on gradient statistics. This paper
compares paradigms to assess the current state of learning rate control. We
find that methods from multi-fidelity hyperparameter optimization,
fixed-hyperparameter schedules, and hyperparameter-free learning often perform
very well on selected deep learning tasks but are not reliable across settings.
This highlights the need for algorithm selection methods in learning rate
control, which have been neglected so far by both the AutoML and deep learning
communities. We also observe a trend of hyperparameter optimization approaches
becoming less effective as models and tasks grow in complexity, even when
combined with multi-fidelity approaches for more expensive model trainings. A
focus on more relevant test tasks and new promising directions like finetunable
methods and meta-learning will enable the AutoML community to significantly
strengthen its impact on this crucial factor in deep learning.

</details>


### [230] [A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference](https://arxiv.org/abs/2507.01740)
*Trung-Dung Hoang,Alceu Bissoto,Vihangkumar V. Naik,Tim FlÃ¼hmann,Artemii Shlychkov,JosÃ© Garcia-Tirado,Lisa M. Koch*

Main category: cs.LG

TL;DR: The paper introduces a Simulation-Based Inference (SBI) method using Neural Posterior Estimation for estimating Type 1 Diabetes model parameters, surpassing traditional methods in speed, accuracy, and robustness.


<details>
  <summary>Details</summary>
Motivation: Reliable digital twins for Type 1 Diabetes require precise physiological parameter estimation, but the complexity of glucose-insulin interactions makes this challenging for traditional methods.

Method: The study uses Neural Posterior Estimation within a Simulation-Based Inference framework to model glucose-insulin dynamics, aiming for faster and amortized inference compared to traditional Markov Chain Monte Carlo techniques.

Result: Experiments show that the proposed SBI approach outperforms traditional methods in parameter estimation and provides better generalization to unseen conditions, along with real-time posterior inference and uncertainty quantification.

Conclusion: The proposed SBI approach presents an advance in estimating Type 1 Diabetes model parameters, offering a faster, more generalizable, and computationally efficient alternative to traditional methods.

Abstract: Accurately estimating parameters of physiological models is essential to
achieving reliable digital twins. For Type 1 Diabetes, this is particularly
challenging due to the complexity of glucose-insulin interactions. Traditional
methods based on Markov Chain Monte Carlo struggle with high-dimensional
parameter spaces and fit parameters from scratch at inference time, making them
slow and computationally expensive. In this study, we propose a
Simulation-Based Inference approach based on Neural Posterior Estimation to
efficiently capture the complex relationships between meal intake, insulin, and
glucose level, providing faster, amortized inference. Our experiments
demonstrate that SBI not only outperforms traditional methods in parameter
estimation but also generalizes better to unseen conditions, offering real-time
posterior inference with reliable uncertainty quantification.

</details>


### [231] [BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification](https://arxiv.org/abs/2507.01781)
*Dalia RodrÃ­guez-Salas,Christian Riess*

Main category: cs.LG

TL;DR: BranchNet converts decision tree ensembles into sparse neural networks, outperforming XGBoost on multi-class benchmarks.


<details>
  <summary>Details</summary>
Motivation: Address challenges of combining interpretability and optimization in machine learning models.

Method: Maps decision paths to hidden neurons, preserving symbolic tree structure for gradient-based learning.

Result: Tests show statistically significant accuracy improvements over XGBoost in multi-class classification.

Conclusion: BranchNet is compact and interpretable, but limitations remain in binary tasks requiring adaptive calibration.

Abstract: We introduce BranchNet, a neuro-symbolic learning framework that transforms
decision tree ensembles into sparse, partially connected neural networks. Each
branch, defined as a decision path from root to a parent of leaves, is mapped
to a hidden neuron, preserving symbolic structure while enabling gradient-based
optimization. The resulting models are compact, interpretable, and require no
manual architecture tuning. Evaluated on a suite of structured multi-class
classification benchmarks, BranchNet consistently outperforms XGBoost in
accuracy, with statistically significant gains. We detail the architecture,
training procedure, and sparsity dynamics, and discuss the model's strengths in
symbolic interpretability as well as its current limitations, particularly on
binary tasks where further adaptive calibration may be beneficial.

</details>


### [232] [Towards Decentralized and Sustainable Foundation Model Training with the Edge](https://arxiv.org/abs/2507.01803)
*Leyang Xue,Meghana Madhyastha,Randal Burns,Myungjin Lee,Mahesh K. Marina*

Main category: cs.LG

TL;DR: The paper proposes a decentralized approach to train foundation models using underutilized edge AI devices, aiming for sustainability and addressing potential centralized control risks.


<details>
  <summary>Details</summary>
Motivation: The significant computational demands and environmental impact of training foundation models, alongside concerns over centralized control, motivated the authors to explore decentralized training approaches.

Method: The authors propose utilizing the computational resources of underused connected edge AI devices to collaboratively train foundation models.

Result: They outline the sustainability benefits of their decentralized training vision and identify specific challenges for its practical implementation.

Conclusion: The proposed vision leverages decentralized computing for foundation model training, with potential sustainability advantages, provided its implementation challenges are effectively addressed.

Abstract: Foundation models are at the forefront of AI research, appealing for their
ability to learn from vast datasets and cater to diverse tasks. Yet, their
significant computational demands raise issues of environmental impact and the
risk of centralized control in their development. We put forward a vision
towards decentralized and sustainable foundation model training that leverages
the collective compute of sparingly used connected edge AI devices. We present
the rationale behind our vision, particularly in support of its sustainability
benefit. We further outline a set of challenges that need to be addressed to
turn this vision into reality.

</details>


### [233] [MILP-SAT-GNN: Yet Another Neural SAT Solver](https://arxiv.org/abs/2507.01825)
*Franco Alberto Cardillo,Hamza Khyari,Umberto Straccia*

Main category: cs.LG

TL;DR: The paper introduces a novel method using Graph Neural Networks (GNNs) to solve SAT problems by representing k-CNF formulae as Mixed Integer Linear Programming (MILP) instances. These are then mapped to weighted bipartite graphs for GNN processing.


<details>
  <summary>Details</summary>
Motivation: To leverage graph neural networks for solving SAT problems by capitalizing on techniques used in Mixed Integer Linear Programming (MILP).

Method: The method maps k-CNF formulae into MILP problems, encodes them as weighted bipartite graphs, and uses GNNs for training and testing. It incorporates results on permutation invariance, equivalence invariance, theoretical limitations, and universal approximation capabilities through Random Node Initialization (RNI).

Result: Theoretical findings include invariance results, a limitation for foldable formulae, and a universal approximation theorem. Experimental results show promising outcomes with the proposed approach, even with simple neural architectures.

Conclusion: The proposed method effectively applies GNNs to SAT solving. Despite limitations with certain formulae and the simplicity of the model, experimental results are promising, indicating potential for further research.

Abstract: We proposes a novel method that enables Graph Neural Networks (GNNs) to solve
SAT problems by leveraging a technique developed for applying GNNs to Mixed
Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into
MILP problems, which are then encoded as weighted bipartite graphs and
subsequently fed into a GNN for training and testing. From a theoretical
perspective: (i) we establish permutation and equivalence invariance results,
demonstrating that the method produces outputs that are stable under reordering
of clauses and variables; (ii) we identify a theoretical limitation, showing
that for a class of formulae called foldable formulae, standard GNNs cannot
always distinguish satisfiable from unsatisfiable instances; (iii) we prove a
universal approximation theorem, establishing that with Random Node
Initialization (RNI), the method can approximate SAT solving to arbitrary
precision on finite datasets, that is, the GNN becomes approximately sound and
complete on such datasets. Furthermore, we show that for unfoldable formulae,
the same approximation guarantee can be achieved without the need for RNI.
Finally, we conduct an experimental evaluation of our approach, which show
that, despite the simplicity of the neural architecture, the method achieves
promising results.

</details>


### [234] [mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling](https://arxiv.org/abs/2507.01829)
*Tristan Torchet,Christian Metzner,Laura Kriener,Melika Payvand*

Main category: cs.LG

TL;DR: The paper introduces mGRADE, a hybrid-memory model combining temporal convolution and minimal gated recurrence for efficient temporal processing on edge devices with constrained memory.


<details>
  <summary>Details</summary>
Motivation: Edge devices face challenges in processing temporal data efficiently due to memory limits, requiring models that handle both short- and long-range dynamics.

Method: The mGRADE framework integrates a learnable delay embedding via temporal 1D-convolution with a minimal gated recurrent unit (minGRU) to effectively combine short-term and long-term temporal processing while reducing memory overhead.

Result: Experiments show mGRADE excels in synthetic tasks and image classification benchmarks, outperforming convolutional and recurrent models while reducing memory usage by approximately 20%.

Conclusion: mGRADE is a practical and efficient solution for memory-constrained systems, achieving superior multi-scale temporal processing capabilities on edge devices.

Abstract: Edge devices for temporal processing demand models that capture both short-
and long- range dynamics under tight memory constraints. While Transformers
excel at sequence modeling, their quadratic memory scaling with sequence length
makes them impractical for such settings. Recurrent Neural Networks (RNNs)
offer constant memory but train sequentially, and Temporal Convolutional
Networks (TCNs), though efficient, scale memory with kernel size. To address
this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay
Embedding), a hybrid-memory system that integrates a temporal 1D-convolution
with learnable spacings followed by a minimal gated recurrent unit (minGRU).
This design allows the convolutional layer to realize a flexible delay
embedding that captures rapid temporal variations, while the recurrent module
efficiently maintains global context with minimal memory overhead. We validate
our approach on two synthetic tasks, demonstrating that mGRADE effectively
separates and preserves multi-scale temporal features. Furthermore, on
challenging pixel-by-pixel image classification benchmarks, mGRADE consistently
outperforms both pure convolutional and pure recurrent counterparts using
approximately 20% less memory footprint, highlighting its suitability for
memory-constrained temporal processing at the edge. This highlights mGRADE's
promise as an efficient solution for memory-constrained multi-scale temporal
processing at the edge.

</details>


### [235] [Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization](https://arxiv.org/abs/2507.01841)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: The paper introduces SubLoRA, a second-order method leveraging submodular function maximization for rank determination in Low-Rank Adaptation (LoRA), outperforming previous approaches in accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods like AdaLoRA rely on inaccurate first-order approximations for rank determination in LoRA, leading to ill-conditioned solutions after optimization.

Method: SubLoRA reformulates rank determination as a combinatorial optimization problem involving a quadratic objective, utilizing the Hessian matrix and a submodular function maximization framework combined with a greedy algorithm.

Result: SubLoRA surpasses prior methods in rank determination and joint training performance, as demonstrated through experiments fine-tuning physics-informed neural networks to solve PDEs.

Conclusion: The proposed approach achieves accurate rank determination with theoretical robustness and practical efficiency, advancing the optimization of low-rank adaptations in neural networks.

Abstract: In this paper, we propose SubLoRA, a rank determination method for Low-Rank
Adaptation (LoRA) based on submodular function maximization. In contrast to
prior approaches, such as AdaLoRA, that rely on first-order (linearized)
approximations of the loss function, SubLoRA utilizes second-order information
to capture the potentially complex loss landscape by incorporating the Hessian
matrix. We show that the linearization becomes inaccurate and ill-conditioned
when the LoRA parameters have been well optimized, motivating the need for a
more reliable and nuanced second-order formulation. To this end, we reformulate
the rank determination problem as a combinatorial optimization problem with a
quadratic objective. However, solving this problem exactly is NP-hard in
general. To overcome the computational challenge, we introduce a submodular
function maximization framework and devise a greedy algorithm with
approximation guarantees. We derive a sufficient and necessary condition under
which the rank-determination objective becomes submodular, and construct a
closed-form projection of the Hessian matrix that satisfies this condition
while maintaining computational efficiency. Our method combines solid
theoretical foundations, second-order accuracy, and practical computational
efficiency. We further extend SubLoRA to a joint optimization setting,
alternating between LoRA parameter updates and rank determination under a rank
budget constraint. Extensive experiments on fine-tuning physics-informed neural
networks (PINNs) for solving partial differential equations (PDEs) demonstrate
the effectiveness of our approach. Results show that SubLoRA outperforms
existing methods in both rank determination and joint training performance.

</details>


### [236] [Towards Foundation Auto-Encoders for Time-Series Anomaly Detection](https://arxiv.org/abs/2507.01875)
*GastÃ³n GarcÃ­a GonzÃ¡lez,Pedro Casas,Emilio MartÃ­nez,Alicia FernÃ¡ndez*

Main category: cs.LG

TL;DR: The paper introduces Foundation Auto-Encoders (FAE), a VAE-based large foundation model aimed at anomaly detection in time-series data, featuring DCNNs and trained on vast datasets.


<details>
  <summary>Details</summary>
Motivation: To enhance time-series modeling and anomaly detection through foundation models pretrained on large datasets, enabling out-of-the-box, zero-shot applications.

Method: The FAE model utilizes Variational Auto-Encoders (VAEs) and Dilated Convolutional Neural Networks (DCNNs) for univariate time-series anomaly detection, trained on large-scale datasets.

Result: Preliminary experiments demonstrate its effectiveness on datasets from diverse domains, including an operational mobile ISP and the KDD 2021 Anomaly Detection dataset.

Conclusion: FAE showcases promising potential as a universal time-series anomaly detection tool with zero-shot capabilities, though further validation is needed.

Abstract: We investigate a novel approach to time-series modeling, inspired by the
successes of large pretrained foundation models. We introduce FAE (Foundation
Auto-Encoders), a foundation generative-AI model for anomaly detection in
time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we
mean a model pretrained on massive amounts of time-series data which can learn
complex temporal patterns useful for accurate modeling, forecasting, and
detection of anomalies on previously unseen datasets. FAE leverages VAEs and
Dilated Convolutional Neural Networks (DCNNs) to build a generic model for
univariate time-series modeling, which could eventually perform properly in
out-of-the-box, zero-shot anomaly detection applications. We introduce the main
concepts of FAE, and present preliminary results in different multi-dimensional
time-series datasets from various domains, including a real dataset from an
operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.

</details>


### [237] [Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection](https://arxiv.org/abs/2507.01924)
*Samirah Bakker,Yao Ma,Seyed Sahand Mohammadi Ziabari*

Main category: cs.LG

TL;DR: This paper proposes a hybrid deep learning approach using LSTM, Transformers, and pseudo-labeling (via iForest and Autoencoders) to address anomaly detection challenges in mental healthcare billing, showing promising results on real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Mental healthcare billing is complex and prone to anomalies such as fraud, and existing methods struggle with challenges like data imbalance, label scarcity, and sequential patterns.

Method: The paper combines LSTM and Transformers with pseudo-labeling techniques, employing Isolation Forests and Autoencoders, and evaluates the models on two real-world mental healthcare billing datasets.

Result: The iForest LSTM baseline achieved recall of 0.963 on declaration-level data, while the hybrid iForest-based model achieved recall of 0.744 on operation-level data, albeit with lower precision.

Conclusion: Hybrid deep learning models using pseudo-labeled data show promise for anomaly detection in challenging, imbalanced scenarios like mental healthcare billing.

Abstract: The complexity of mental healthcare billing enables anomalies, including
fraud. While machine learning methods have been applied to anomaly detection,
they often struggle with class imbalance, label scarcity, and complex
sequential patterns. This study explores a hybrid deep learning approach
combining Long Short-Term Memory (LSTM) networks and Transformers, with
pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior
work has not evaluated such hybrid models trained on pseudo-labeled data in the
context of healthcare billing. The approach is evaluated on two real-world
billing datasets related to mental healthcare. The iForest LSTM baseline
achieves the highest recall (0.963) on declaration-level data. On the
operation-level data, the hybrid iForest-based model achieves the highest
recall (0.744), though at the cost of lower precision. These findings highlight
the potential of combining pseudo-labeling with hybrid deep learning in
complex, imbalanced anomaly detection settings.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [238] [Diversity-Preserving Exploitation of Crossover](https://arxiv.org/abs/2507.01524)
*Johannes Lengler,Tom Offermann*

Main category: cs.NE

TL;DR: The paper introduces a genetic algorithm paradigm, DiPEC, which maintains diversity during crossover to improve optimization performance.


<details>
  <summary>Details</summary>
Motivation: The antagonism between diversity preservation and exploitation in crossover often reduces its effectiveness in genetic algorithms.

Method: The authors propose the Diversity Exploitation Genetic Algorithm (DEGA), which leverages the DiPEC paradigm to maintain diversity while exploiting crossover benefits.

Result: The (2+1)-DEGA achieves $O(n^{5/3}\log^{2/3} n)$ fitness evaluations for the LeadingOnes problem, outperforming standard genetic algorithms requiring $\Theta(n^2)$. Simulations and other benchmarks validate its competitiveness.

Conclusion: The DiPEC paradigm enhances genetic algorithm efficiency and represents a promising direction for future systematic research.

Abstract: Crossover is a powerful mechanism for generating new solutions from a given
population of solutions. Crossover comes with a discrepancy in itself: on the
one hand, crossover usually works best if there is enough diversity in the
population; on the other hand, exploiting the benefits of crossover reduces
diversity. This antagonism often makes crossover reduce its own effectiveness.
  We introduce a new paradigm for utilizing crossover that reduces this
antagonism, which we call diversity-preserving exploitation of crossover
(DiPEC). The resulting Diversity Exploitation Genetic Algorithm (DEGA) is able
to still exploit the benefits of crossover, but preserves a much higher
diversity than conventional approaches.
  We demonstrate the benefits by proving that the (2+1)-DEGA finds the optimum
of LeadingOnes with $O(n^{5/3}\log^{2/3} n)$ fitness evaluations. This is
remarkable since standard genetic algorithms need $\Theta(n^2)$ evaluations,
and among genetic algorithms only some artificial and specifically tailored
algorithms were known to break this runtime barrier. We confirm the theoretical
results by simulations. Finally, we show that the approach is not overfitted to
Leadingones by testing it empirically on other benchmarks and showing that it
is also competitive in other settings. We believe that our findings justify
further systematic investigations of the DiPEC paradigm.

</details>


### [239] [Adaptive Estimation of the Number of Algorithm Runs in Stochastic Optimization](https://arxiv.org/abs/2507.01629)
*Tome Eftimov,Peter KoroÅ¡ec*

Main category: cs.NE

TL;DR: This paper presents a way to streamline the number of algorithm runs needed while maintaining reliable performance estimates for optimization tasks.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of balancing experiment duration with the reliability of outcomes in stochastic optimization benchmarking.

Method: An empirical, online approach dynamically estimates required runs using probability theory, robustness checks for data imbalances, and adjustments during execution.

Result: Achieved 82%-95% accurate estimations while reducing about 50% of runs in benchmarks across multiple algorithm portfolios.

Conclusion: The proposed method enhances efficiency and sustainability in benchmarking by reducing computation time and energy requirements without sacrificing outcome reliability.

Abstract: Determining the number of algorithm runs is a critical aspect of experimental
design, as it directly influences the experiment's duration and the reliability
of its outcomes. This paper introduces an empirical approach to estimating the
required number of runs per problem instance for accurate estimation of the
performance of the continuous single-objective stochastic optimization
algorithm. The method leverages probability theory, incorporating a robustness
check to identify significant imbalances in the data distribution relative to
the mean, and dynamically adjusts the number of runs during execution as an
online approach. The proposed methodology was extensively tested across two
algorithm portfolios (104 Differential Evolution configurations and the
Nevergrad portfolio) and the COCO benchmark suite, totaling 5748000 runs. The
results demonstrate 82% - 95% accuracy in estimations across different
algorithms, allowing a reduction of approximately 50% in the number of runs
without compromising optimization outcomes. This online calculation of required
runs not only improves benchmarking efficiency, but also contributes to energy
reduction, fostering a more environmentally sustainable computing ecosystem.

</details>


### [240] [Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance](https://arxiv.org/abs/2507.01638)
*Ana Nikolikj,Gabriela Ochoa,Tome Eftimov*

Main category: cs.NE

TL;DR: The paper analyzes landscape features from C-PLOS-net models to predict performance in multi-objective combinatorial optimization algorithms.


<details>
  <summary>Details</summary>
Motivation: The study aims to understand the relations between landscape features and algorithm performance to improve optimization techniques.

Method: It examines rmnk-landscapes with various ruggedness/correlation levels, assessing three algorithms (PLS, GSEMO, NSGA-II) using specific performance metrics.

Result: The analysis identifies influential feature combinations tied to specific landscape types and algorithm outcomes.

Conclusion: These insights promote a tailored understanding of optimization design for multi-objective landscapes.

Abstract: We present an analysis of landscape features for predicting the performance
of multi-objective combinatorial optimization algorithms. We consider features
from the recently proposed compressed Pareto Local Optimal Solutions Networks
(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a
set of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness
and objective correlation. We consider the performance of three algorithms --
Pareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and
Non-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and
hypervolume metrics. Our tailored analysis reveals feature combinations that
influence algorithm performance specific to certain landscapes. This study
provides deeper insights into feature importance, tailored to specific
rmnk-landscapes and algorithms.

</details>


### [241] [Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis](https://arxiv.org/abs/2507.01668)
*Gjorgjina Cenikj,GaÅ¡per Petelin,Tome Eftimov*

Main category: cs.NE

TL;DR: Many recent optimization algorithms are criticized for lack of innovation, leading to research in statistical methods to compare their search behavior. Authors apply cross-match tests on algorithm outputs from MEALPY library.


<details>
  <summary>Details</summary>
Motivation: To address concerns around oversaturation of 'novel' metaheuristic optimization algorithms with unclear differentiation and innovation.

Method: Using cross-match statistical tests to compare search behaviors of algorithms based on their generated solutions.

Result: Cross-match tests are applied to solutions from 114 algorithms in MEALPY library, identifying similarities in search behavior.

Conclusion: Empirical analysis highlights algorithms with similar behaviors, contributing tools to better distinguish innovation in optimization algorithms.

Abstract: The field of numerical optimization has recently seen a surge in the
development of "novel" metaheuristic algorithms, inspired by metaphors derived
from natural or human-made processes, which have been widely criticized for
obscuring meaningful innovations and failing to distinguish themselves from
existing approaches. Aiming to address these concerns, we investigate the
applicability of statistical tests for comparing algorithms based on their
search behavior. We utilize the cross-match statistical test to compare
multivariate distributions and assess the solutions produced by 114 algorithms
from the MEALPY library. These findings are incorporated into an empirical
analysis aiming to identify algorithms with similar search behaviors.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [242] [Advanced LPeg techniques: A dual case study approach](https://arxiv.org/abs/2507.01272)
*Zixuan Zhu*

Main category: cs.PL

TL;DR: The paper enhances Lua Parsing Expression Grammars (LPeg) by showcasing optimizations through case studies on JSON parsing and Glob pattern conversion, significantly boosting performance benchmarks.


<details>
  <summary>Details</summary>
Motivation: Optimize parsing performance for LPeg without altering its library, targeting improved memory and computational efficiency.

Method: Introduced grammar construction strategies: JSON parser optimization via substitution capture and table handling; Glob converter via boundary separation, Cox's strategy, and braced condition optimization.

Result: JSON parser achieved speeds of up to 125 MB/s, surpassing dkjson and matching rxi_json; Glob converter outperformed Bun.Glob by 14-92% and Minimatch by 300-1400%.

Conclusion: The research provides novel, practical strategies for optimizing LPeg parsers, enhancing text processing efficiency and contributing to its ecosystem.

Abstract: This paper presents advanced optimization techniques for Lua Parsing
Expression Grammars (LPeg) through two complementary case studies: a
high-performance JSON parser and a sophisticated Glob-to-LPeg pattern
converter. We demonstrate how strategic grammar construction can dramatically
improve parsing performance without modifying the underlying LPeg library. For
the JSON parser, we implement substitution capture and table construction
optimization to reduce memory allocation overhead and improve object
processing. For the Glob converter, we introduce segment-boundary separation,
implement Cox's flattened search strategy, and develop optimized braced
condition handling to prevent exponential backtracking. Comprehensive
benchmarks demonstrate that our JSON parser achieves processing speeds up to
125 MB/s on complex documents, consistently outperforming dkjson and showing
competitive results against rxi_json across most test cases. Our Glob-to-LPeg
converter exhibits 14-92% better performance than Bun.Glob and runs 3-14 times
faster than Minimatch across diverse pattern matching scenarios. This research
provides practical optimization techniques for LPeg-based parsers, contributing
valuable strategies to the text processing ecosystem.

</details>


### [243] [Globality and Regions](https://arxiv.org/abs/2507.01664)
*Hector Gramaglia*

Main category: cs.PL

TL;DR: The paper integrates abstraction with region abstraction to describe global variables in a region-based language.


<details>
  <summary>Details</summary>
Motivation: To offer a clear approach to incorporating imperative operations into functional languages while ensuring memory safety through linear protection.

Method: The authors use the concept of linear protection to unify abstraction and region abstraction in the global language and connect it to Tofte and Talping's region language.

Result: The study demonstrates that global variables in the global language naturally emerge from the unification process in the region language.

Conclusion: The characterization of global variables enables a principled integration of functional and imperative paradigms in region-based languages, ensuring memory safety.

Abstract: We obtain a characterization of global variables by unifying abstraction with
region abstraction in a region-based language. More precisely, in a previous
work a language called global was presented, whose virtue is to provide a
conceptually clear way of introducing imperative operations in a functional
language. Memory safety is provided by the concept of linear protection, which
connects the global system to a linear one. In this paper we show that the
concept of global variable provided by the global language arises from the
Tofte and Talping's region language through the unification of abstraction and
region abstraction.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [244] [Environment-Aware and Human-Cooperative Swing Control for Lower-Limb Prostheses in Diverse Obstacle Scenarios](https://arxiv.org/abs/2507.01111)
*Haosen Xing,Haoran Ma,Sijin Zhang,Hartmut Geyer*

Main category: cs.RO

TL;DR: This paper proposes a novel control strategy for powered prosthetic legs, integrating environmental awareness and human cooperativeness to enable effective obstacle negotiation.


<details>
  <summary>Details</summary>
Motivation: Current control strategies in powered prosthetics struggle with complex terrains, lacking integration of environmental awareness and user intent, particularly in obstacle negotiation scenarios.

Method: The proposed method combines an on-board depth camera for obstacle detection, which triggers elevated swing trajectories in early-phase, with biomechanical cue-driven control in late-phase to align with natural user intent.

Result: Experiments with three non-amputee participants showed 100% success in stepping over/onto more than 150 obstacles of varying sizes and placements.

Conclusion: The system enhances adaptability to environmental and user constraints, showing potential for versatile and intuitive prosthetic mobility in complex terrains.

Abstract: Current control strategies for powered lower limb prostheses often lack
awareness of the environment and the user's intended interactions with it. This
limitation becomes particularly apparent in complex terrains. Obstacle
negotiation, a critical scenario exemplifying such challenges, requires both
real-time perception of obstacle geometry and responsiveness to user intention
about when and where to step over or onto, to dynamically adjust swing
trajectories. We propose a novel control strategy that fuses environmental
awareness and human cooperativeness: an on-board depth camera detects obstacles
ahead of swing phase, prompting an elevated early-swing trajectory to ensure
clearance, while late-swing control defers to natural biomechanical cues from
the user. This approach enables intuitive stepping strategies without requiring
unnatural movement patterns. Experiments with three non-amputee participants
demonstrated 100 percent success across more than 150 step-overs and 30
step-ons with randomly placed obstacles of varying heights (4-16 cm) and
distances (15-70 cm). By effectively addressing obstacle navigation -- a
gateway challenge for complex terrain mobility -- our system demonstrates
adaptability to both environmental constraints and user intentions, with
promising applications across diverse locomotion scenarios.

</details>


### [245] [VISTA: Open-Vocabulary, Task-Relevant Robot Exploration with Online Semantic Gaussian Splatting](https://arxiv.org/abs/2507.01125)
*Keiko Nagami,Timothy Chen,Javier Yu,Ola Shorinwa,Maximilian Adang,Carlyn Dougherty,Eric Cristofalo,Mac Schwager*

Main category: cs.RO

TL;DR: VISTA is a robot exploration method improving 3D map quality using open-vocabulary task-guided exploration.


<details>
  <summary>Details</summary>
Motivation: To enable robots to efficiently explore environments and create high-quality 3D semantic maps while focusing on task-relevant regions based on user queries.

Method: VISTA plans robot trajectories by combining semantic task awareness and exploration. It uses a viewpoint-semantic coverage metric optimizing geometric view diversity and task relevance.

Result: VISTA outperforms state-of-the-art methods in computational efficiency and 3D reconstruction quality on static datasets. In quadrotor experiments, it improves success rates in challenging areas by 6x.

Conclusion: VISTA is a robust, platform-agnostic system for task-guided robotic exploration and semantic 3D mapping, demonstrating significant improvements in complex environments.

Abstract: We present VISTA (Viewpoint-based Image selection with Semantic Task
Awareness), an active exploration method for robots to plan informative
trajectories that improve 3D map quality in areas most relevant for task
completion. Given an open-vocabulary search instruction (e.g., "find a
person"), VISTA enables a robot to explore its environment to search for the
object of interest, while simultaneously building a real-time semantic 3D
Gaussian Splatting reconstruction of the scene. The robot navigates its
environment by planning receding-horizon trajectories that prioritize semantic
similarity to the query and exploration of unseen regions of the environment.
To evaluate trajectories, VISTA introduces a novel, efficient
viewpoint-semantic coverage metric that quantifies both the geometric view
diversity and task relevance in the 3D scene. On static datasets, our coverage
metric outperforms state-of-the-art baselines, FisherRF and Bayes' Rays, in
computation speed and reconstruction quality. In quadrotor hardware
experiments, VISTA achieves 6x higher success rates in challenging maps,
compared to baseline methods, while matching baseline performance in less
challenging maps. Lastly, we show that VISTA is platform-agnostic by deploying
it on a quadrotor drone and a Spot quadruped robot. Open-source code will be
released upon acceptance of the paper.

</details>


### [246] [A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods](https://arxiv.org/abs/2507.01143)
*Reza Jalayer,Masoud Jalayer,Amirali Baniasadi*

Main category: cs.RO

TL;DR: This paper reviews sound source localization (SSL) techniques, focusing on deep learning (DL) advancements for robotics.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address gaps in existing surveys by focusing on robotic-specific SSL constraints and incorporating the latest deep learning advancements.

Method: The paper reviews classical SSL methods (e.g., TDOA, beamforming) and modern deep learning approaches (CNNs, CRNNs, attention-based architectures). It also categorizes studies by robot type and application domain.

Result: The review discusses key challenges (e.g., robustness, sound multiplicity, and implementation constraints) and identifies trends and strategies for SSL in robotics.

Conclusion: The paper provides a roadmap for achieving robust, efficient, and explainable DL-based SSL for next-generation robots.

Abstract: Sound source localization (SSL) adds a spatial dimension to auditory
perception, allowing a system to pinpoint the origin of speech, machinery
noise, warning tones, or other acoustic events, capabilities that facilitate
robot navigation, human-machine dialogue, and condition monitoring. While
existing surveys provide valuable historical context, they typically address
general audio applications and do not fully account for robotic constraints or
the latest advancements in deep learning. This review addresses these gaps by
offering a robotics-focused synthesis, emphasizing recent progress in deep
learning methodologies. We start by reviewing classical methods such as Time
Difference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and
subspace analysis. Subsequently, we delve into modern machine learning (ML) and
deep learning (DL) approaches, discussing traditional ML and neural networks
(NNs), convolutional neural networks (CNNs), convolutional recurrent neural
networks (CRNNs), and emerging attention-based architectures. The data and
training strategy that are the two cornerstones of DL-based SSL are explored.
Studies are further categorized by robot types and application domains to
facilitate researchers in identifying relevant work for their specific
contexts. Finally, we highlight the current challenges in SSL works in general,
regarding environmental robustness, sound source multiplicity, and specific
implementation constraints in robotics, as well as data and learning strategies
in DL-based SSL. Also, we sketch promising directions to offer an actionable
roadmap toward robust, adaptable, efficient, and explainable DL-based SSL for
next-generation robots.

</details>


### [247] [SonoGym: High Performance Simulation for Challenging Surgical Tasks with Robotic Ultrasound](https://arxiv.org/abs/2507.01152)
*Yunke Ao,Masoud Moghani,Mayank Mittal,Manish Prajapat,Luohong Wu,Frederic Giraud,Fabio Carrillo,Andreas Krause,Philipp FÃ¼rnstahl*

Main category: cs.RO

TL;DR: SonoGym is a scalable simulation platform designed to advance robotic ultrasound tasks, enabling efficient training of reinforcement and imitation learning algorithms for surgery.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the limited use of deep reinforcement learning (DRL) and imitation learning (IL) in complex surgical tasks due to the absence of realistic simulation environments.

Method: SonoGym employs scalable simulation using CT-derived 3D models for ultrasound data and supports parallel environments, realistic US data modeling, and integration with robotic platforms.

Result: The study demonstrates successful training of advanced DRL and IL agents for anatomy reconstruction and surgical guidance tasks, while exposing limitations in current methods.

Conclusion: SonoGym provides a critical tool for enhancing robot learning in complex surgery applications, offering data, code, and tools publicly for further research.

Abstract: Ultrasound (US) is a widely used medical imaging modality due to its
real-time capabilities, non-invasive nature, and cost-effectiveness. Robotic
ultrasound can further enhance its utility by reducing operator dependence and
improving access to complex anatomical regions. For this, while deep
reinforcement learning (DRL) and imitation learning (IL) have shown potential
for autonomous navigation, their use in complex surgical tasks such as anatomy
reconstruction and surgical guidance remains limited -- largely due to the lack
of realistic and efficient simulation environments tailored to these tasks. We
introduce SonoGym, a scalable simulation platform for complex robotic
ultrasound tasks that enables parallel simulation across tens to hundreds of
environments. Our framework supports realistic and real-time simulation of US
data from CT-derived 3D models of the anatomy through both a physics-based and
a generative modeling approach. Sonogym enables the training of DRL and recent
IL agents (vision transformers and diffusion policies) for relevant tasks in
robotic orthopedic surgery by integrating common robotic platforms and
orthopedic end effectors. We further incorporate submodular DRL -- a recent
method that handles history-dependent rewards -- for anatomy reconstruction and
safe reinforcement learning for surgery. Our results demonstrate successful
policy learning across a range of scenarios, while also highlighting the
limitations of current methods in clinically relevant environments. We believe
our simulation can facilitate research in robot learning approaches for such
challenging robotic surgery applications. Dataset, codes, and videos are
publicly available at https://sonogym.github.io/.

</details>


### [248] [A Differentiable Distance Metric for Robotics Through Generalized Alternating Projection](https://arxiv.org/abs/2507.01181)
*Vinicius M. GonÃ§alves,Shiqing Wei,Eduardo Malacarne S. de Souza,Krishnamurthy Prashanth,Anthony Tzes,Farshad Khorrami*

Main category: cs.RO

TL;DR: This paper addresses the limitations of a recently proposed differentiable distance metric in robotics by introducing simpler expressions for general convex polytopes and ensuring the metric vanishes when objects overlap.


<details>
  <summary>Details</summary>
Motivation: The need to compute distance and derivative in robotics applications, such as control barrier functions, and shortcomings of existing differentiable metrics.

Method: Simplify and improve expressions for smooth projection for convex polytopes, ensuring overlap-sensitive behavior, and validate through experimental results.

Result: Demonstrates the effectiveness of the proposed metric in experiments and makes it publicly accessible via the UAIBot Python package.

Conclusion: The paper proposes a practical and efficient differentiable distance metric addressing limitations in prior approaches for better utility in robotics.

Abstract: In many robotics applications, it is necessary to compute not only the
distance between the robot and the environment, but also its derivative - for
example, when using control barrier functions. However, since the traditional
Euclidean distance is not differentiable, there is a need for alternative
distance metrics that possess this property. Recently, a metric with guaranteed
differentiability was proposed [1]. This approach has some important drawbacks,
which we address in this paper. We provide much simpler and practical
expressions for the smooth projection for general convex polytopes.
Additionally, as opposed to [1], we ensure that the distance vanishes as the
objects overlap. We show the efficacy of the approach in experimental results.
Our proposed distance metric is publicly available through the Python-based
simulation package UAIBot.

</details>


### [249] [Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives](https://arxiv.org/abs/2507.01198)
*Benjamin Kraljusic,Zlatan Ajanovic,Nermin Covic,Bakir Lacevic*

Main category: cs.RO

TL;DR: This paper introduces a hybrid motion planning algorithm that enhances efficiency for robotic manipulators using adaptive motion primitives called 'burs' in configuration space exploration.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency and performance in motion planning for robotic manipulators, especially in challenging environments or with robots of high degrees-of-freedom.

Method: Introduces 'burs', adaptive motion primitives, which are utilized within a graph search algorithm to better explore free configuration space. Implemented in the SMPL library.

Result: The bur-based approach shows significant efficiency improvements in complex scenarios and with robots of higher degrees-of-freedom, while maintaining comparable results in simpler cases.

Conclusion: Utilizing burs as adaptive motion primitives offers a promising enhancement to motion planning algorithms for complex and high DoF robotic systems.

Abstract: This work proposes a motion planning algorithm for robotic manipulators that
combines sampling-based and search-based planning methods. The core
contribution of the proposed approach is the usage of burs of free
configuration space (C-space) as adaptive motion primitives within the graph
search algorithm. Due to their feature to adaptively expand in free C-space,
burs enable more efficient exploration of the configuration space compared to
fixed-sized motion primitives, significantly reducing the time to find a valid
path and the number of required expansions. The algorithm is implemented within
the existing SMPL (Search-Based Motion Planning Library) library and evaluated
through a series of different scenarios involving manipulators with varying
number of degrees-of-freedom (DoF) and environment complexity. Results
demonstrate that the bur-based approach outperforms fixed-primitive planning in
complex scenarios, particularly for high DoF manipulators, while achieving
comparable performance in simpler scenarios.

</details>


### [250] [2024 NASA SUITS Report: LLM-Driven Immersive Augmented Reality User Interface for Robotics and Space Exploration](https://arxiv.org/abs/2507.01206)
*Kathy Zhuang,Zixun Huang,Yukun Song,Rui Li,Yinuo Zhou,Allen Y. Yang*

Main category: cs.RO

TL;DR: This paper introduces URSA, an LLM-driven AR system designed for spaceflight missions like Artemis, focusing on integrating AR, voice control, and real-time 3D localization for human-robot interaction.


<details>
  <summary>Details</summary>
Motivation: The paper addresses challenges in human-robot interaction in dynamic environments using AR, particularly for tasks like 3D pose estimation critical for spaceflight missions.

Method: The authors developed URSA, a system integrating an AR headset (HoloLens), LLM-powered voice control, robot tracking via digital twin localization, and 3D pose estimation with algorithms like DTTDNet.

Result: URSA demonstrated real-time robot control and monitoring capabilities in AR, even without ground-truth sensors, suitable for hazardous and remote space operations.

Conclusion: The system advances AR capabilities for robotics, offering scalable solutions for aerospace and industrial applications, enhancing precision and usability in complex environments.

Abstract: As modern computing advances, new interaction paradigms have emerged,
particularly in Augmented Reality (AR), which overlays virtual interfaces onto
physical objects. This evolution poses challenges in machine perception,
especially for tasks like 3D object pose estimation in complex, dynamic
environments. Our project addresses critical issues in human-robot interaction
within mobile AR, focusing on non-intrusive, spatially aware interfaces. We
present URSA, an LLM-driven immersive AR system developed for NASA's 2023-2024
SUITS challenge, targeting future spaceflight needs such as the Artemis
missions. URSA integrates three core technologies: a head-mounted AR device
(e.g., HoloLens) for intuitive visual feedback, voice control powered by large
language models for hands-free interaction, and robot tracking algorithms that
enable accurate 3D localization in dynamic settings. To enhance precision, we
leverage digital twin localization technologies, using datasets like
DTTD-Mobile and specialized hardware such as the ZED2 camera for real-world
tracking under noise and occlusion. Our system enables real-time robot control
and monitoring via an AR interface, even in the absence of ground-truth
sensors--vital for hazardous or remote operations. Key contributions include:
(1) a non-intrusive AR interface with LLM-based voice input; (2) a ZED2-based
dataset tailored for non-rigid robotic bodies; (3) a Local Mission Control
Console (LMCC) for mission visualization; (4) a transformer-based 6DoF pose
estimator (DTTDNet) optimized for depth fusion and real-time tracking; and (5)
end-to-end integration for astronaut mission support. This work advances
digital twin applications in robotics, offering scalable solutions for both
aerospace and industrial domains.

</details>


### [251] [Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion](https://arxiv.org/abs/2507.01243)
*Ziang Zheng,Guojian Zhan,Shiqi Liu,Yao Lyu,Tao Zhang,Shengbo Eben Li*

Main category: cs.RO

TL;DR: The paper introduces JumpER, a reinforcement learning framework, for training quadruped robots to perform challenging monopedal hopping tasks on unpredictable terrains without relying on external priors or handcrafted rewards.


<details>
  <summary>Details</summary>
Motivation: Quadruped robots face challenges in performing agile locomotion, especially under extreme underactuation and across extreme terrains. Current RL approaches struggle to address these due to unstable training and unreliable feedback.

Method: JumpER structures RL training into multiple progressive stages by generating self-evolving priors. A three-stage curriculum adjusts action modality, observation, and objectives dynamically to refine policies without external guidance.

Result: The proposed approach allows quadruped robots to robustly achieve monopedal hopping on challenging terrains, handling scenarios like gaps (up to 60 cm), irregular stairs, and uneven stepping stones.

Conclusion: JumpER is a principled and scalable RL framework capable of overcoming dual challenges in locomotion for quadruped robots, offering a robust solution for terrains where traditional methods fail.

Abstract: Reinforcement learning (RL) has shown great potential in enabling quadruped
robots to perform agile locomotion. However, directly training policies to
simultaneously handle dual extreme challenges, i.e., extreme underactuation and
extreme terrains, as in monopedal hopping tasks, remains highly challenging due
to unstable early-stage interactions and unreliable reward feedback. To address
this, we propose JumpER (jump-start reinforcement learning via self-evolving
priors), an RL training framework that structures policy learning into multiple
stages of increasing complexity. By dynamically generating self-evolving priors
through iterative bootstrapping of previously learned policies, JumpER
progressively refines and enhances guidance, thereby stabilizing exploration
and policy optimization without relying on external expert priors or
handcrafted reward shaping. Specifically, when integrated with a structured
three-stage curriculum that incrementally evolves action modality, observation
space, and task objective, JumpER enables quadruped robots to achieve robust
monopedal hopping on unpredictable terrains for the first time. Remarkably, the
resulting policy effectively handles challenging scenarios that traditional
methods struggle to conquer, including wide gaps up to 60 cm, irregularly
spaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.
JumpER thus provides a principled and scalable approach for addressing
locomotion tasks under the dual challenges of extreme underactuation and
extreme terrains.

</details>


### [252] [LLM-based Realistic Safety-Critical Driving Video Generation](https://arxiv.org/abs/2507.01264)
*Yongjie Fu,Ruijian Zha,Pei Tian,Xuan Di*

Main category: cs.RO

TL;DR: This paper introduces a framework using Large Language Models (LLMs) for few-shot code generation to create diverse and safety-critical driving scenarios in the CARLA simulator, accompanied by a video generation pipeline for realistic visualization.


<details>
  <summary>Details</summary>
Motivation: Testing autonomous driving systems requires the generation of diverse, safety-critical, and realistic driving scenarios, which are essential for thorough evaluation but difficult to create manually due to their complexity.

Method: The approach uses LLMs to synthesize scenario scripts for the CARLA simulator based on few-shot learning from example prompts and code samples. It also employs Cosmos-Transfer1 with ControlNet to turn simulated scenes into realistic driving videos.

Result: The framework successfully generates a diverse range of realistic and safety-critical driving scenarios, including rare edge cases like pedestrian crossings under occlusion and sudden vehicle cut-ins.

Conclusion: This method provides an effective and scalable tool for simulation-based testing of autonomous vehicles, contributing to their safety and reliability evaluation.

Abstract: Designing diverse and safety-critical driving scenarios is essential for
evaluating autonomous driving systems. In this paper, we propose a novel
framework that leverages Large Language Models (LLMs) for few-shot code
generation to automatically synthesize driving scenarios within the CARLA
simulator, which has flexibility in scenario scripting, efficient code-based
control of traffic participants, and enforcement of realistic physical
dynamics. Given a few example prompts and code samples, the LLM generates
safety-critical scenario scripts that specify the behavior and placement of
traffic participants, with a particular focus on collision events. To bridge
the gap between simulation and real-world appearance, we integrate a video
generation pipeline using Cosmos-Transfer1 with ControlNet, which converts
rendered scenes into realistic driving videos. Our approach enables
controllable scenario generation and facilitates the creation of rare but
critical edge cases, such as pedestrian crossings under occlusion or sudden
vehicle cut-ins. Experimental results demonstrate the effectiveness of our
method in generating a wide range of realistic, diverse, and safety-critical
scenarios, offering a promising tool for simulation-based testing of autonomous
vehicles.

</details>


### [253] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Main category: cs.RO

TL;DR: This paper introduces VLAD, a vision-language autonomous driving model, which integrates fine-tuned VLMs with a state-of-the-art driving system to improve navigation, spatial reasoning, and transparency.


<details>
  <summary>Details</summary>
Motivation: To enhance autonomous driving systems by leveraging the general knowledge of Visual Language Models to improve perception, prediction, and planning.

Method: The authors fine-tuned an existing Vision-Language Model (VLM) using custom question-answer datasets designed for spatial reasoning and integrated it with VAD, a leading end-to-end driving system. The model generates navigation commands and offers natural language explanations for increased transparency.

Result: VLAD demonstrated a 31.82% reduction in collision rates compared to baseline approaches in tests conducted on the nuScenes dataset.

Conclusion: The integration of VLMs significantly improves the performance, transparency, and trustworthiness of autonomous driving systems, setting a new benchmark in the field.

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


### [254] [LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction](https://arxiv.org/abs/2507.01308)
*Muhammad Atta ur Rahman,Dooseop Choi,KyoungWook Min*

Main category: cs.RO

TL;DR: The paper introduces a motion forecasting model for autonomous driving, leveraging multiple vector map elements beyond lane centerlines to enhance environmental representation and trajectory prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address limitations in current trajectory prediction models for autonomous driving, which rely heavily on lane centerlines and fail to fully capture road environments and constraints.

Method: The proposed method incorporates multiple vector map elements (lane boundaries, road edges) and uses a feature fusion strategy to combine data while employing a pruning mechanism to manage computational costs.

Result: Extensive experiments on the Argoverse 2 dataset demonstrate the model's competitiveness and improved trajectory prediction performance compared to lane centerline-based approaches.

Conclusion: The enhanced model substantially improves environmental representation for motion forecasting, advancing the state of the art and providing a computationally efficient solution.

Abstract: Accurate motion forecasting is critical for safe and efficient autonomous
driving, enabling vehicles to predict future trajectories and make informed
decisions in complex traffic scenarios. Most of the current designs of motion
prediction models are based on the major representation of lane centerlines,
which limits their capability to capture critical road environments and traffic
rules and constraints. In this work, we propose an enhanced motion forecasting
model informed by multiple vector map elements, including lane boundaries and
road edges, that facilitates a richer and more complete representation of
driving environments. An effective feature fusion strategy is developed to
merge information in different vector map components, where the model learns
holistic information on road structures and their interactions with agents.
Since encoding more information about the road environment increases memory
usage and is computationally expensive, we developed an effective pruning
mechanism that filters the most relevant map connections to the target agent,
ensuring computational efficiency while maintaining essential spatial and
semantic relationships for accurate trajectory prediction. Overcoming the
limitations of lane centerline-based models, our method provides a more
informative and efficient representation of the driving environment and
advances the state of the art for autonomous vehicle motion forecasting. We
verify our approach with extensive experiments on the Argoverse 2 motion
forecasting dataset, where our method maintains competitiveness on AV2 while
achieving improved performance.
  Index Terms-Autonomous driving, trajectory prediction, vector map elements,
road topology, connection pruning, Argoverse 2.

</details>


### [255] [TriVLA: A Unified Triple-System-Based Unified Vision-Language-Action Model for General Robot Control](https://arxiv.org/abs/2507.01424)
*Zhenyang Liu,Yongchong Gu,Sixiao Zheng,Xiangyang Xue,Yanwei Fu*

Main category: cs.RO

TL;DR: TriVLA introduces a triple-system vision-language-action (VLA) model that incorporates dynamic perception for advanced robot manipulation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current autoregressive VLA models emphasize static information but fail to adapt to dynamic situations crucial for embodied robot tasks.

Method: TriVLA combines pre-trained vision-language and video foundation models, enhances dynamic perception, and uses real-time policy learning to generate motor actions.

Result: TriVLA achieves 36 Hz operation speed and outperforms imitation learning baselines in simulations and real-world manipulation tasks.

Conclusion: The proposed triple-system architecture of TriVLA provides significant advancements in robotic control, ensuring better dynamic manipulation and fluid action generation.

Abstract: Recent advancements in vision-language models (VLMs) for common-sense
reasoning have led to the development of vision-language-action (VLA) models,
enabling robots to perform generalized manipulation. Although existing
autoregressive VLA methods design a specific architecture like dual-system to
leverage large-scale pretrained knowledge, they tend to capture static
information, often neglecting the dynamic aspects vital for embodied tasks. To
this end, we propose TriVLA, a unified Vision-Language-Action model with a
triple-system architecture for general robot control. The vision-language
module (System 2) interprets the environment through vision and language
instructions. The dynamics perception module (System 3) inherently produces
visual representations that encompass both current static information and
predicted future dynamics, thereby providing valuable guidance for policy
learning. TriVLA utilizes pre-trained VLM model and fine-tunes pre-trained
video foundation model on robot datasets along with internet human manipulation
data. The subsequent policy learning module (System 1) generates fluid motor
actions in real time. Experimental evaluation demonstrates that TriVLA operates
at approximately 36 Hz and surpasses state-of-the-art imitation learning
baselines on standard simulation benchmarks as well as challenging real-world
manipulation tasks.

</details>


### [256] [Approximation-free Control of Unknown Euler-Lagrangian Systems under Input Constraints](https://arxiv.org/abs/2507.01426)
*Ratnangshu Das,Pushpak Jagtap*

Main category: cs.RO

TL;DR: This paper introduces a funnel-based tracking control algorithm for robotic systems, focusing on balancing performance and actuator safety under input constraints.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of maintaining performance and actuator safety in robotics systems with unknown dynamics and constrained inputs.

Method: A funnel-based control algorithm is developed using the Euler-Lagrange formulation, alongside approximation-free strategies to manage violations of feasibility conditions.

Result: The proposed control approach demonstrates robust performance and actuator safety via simulations and experimental validations.

Conclusion: This work significantly advances funnel-based control methods, making them more applicable to practical robotics systems with limited actuation capabilities.

Abstract: In this paper, we present a novel funnel-based tracking control algorithm for
robotic systems with unknown dynamics and prescribed input constraints. The
Euler-Lagrange formulation, a common modeling approach for robotic systems, has
been adopted in this study to address the trade-off between performance and
actuator safety. We establish feasibility conditions that ensure tracking
errors evolve within predefined funnel bounds while maintaining bounded control
efforts, a crucial consideration for robots with limited actuation
capabilities. We propose two approximation-free control strategies for
scenarios where these conditions are violated: one actively corrects the error,
and the other stops further deviation. Finally, we demonstrate the robust
performance and safety of the approach through simulations and experimental
validations. This work represents a significant advancement in funnel-based
control, enhancing its applicability to real-world robotics systems with input
constraints.

</details>


### [257] [Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0](https://arxiv.org/abs/2507.01462)
*Eneko Osaba,Estibaliz Garrote,Pablo Miranda-Rodriguez,Alessia Ciacco,Itziar Cabanes,Aitziber Mancisidor*

Main category: cs.RO

TL;DR: The paper investigates hybrid quantum-classical algorithms for optimizing 3D robotic inspection paths and compares their efficiency against classical methods, showing quantum approaches have competitive results.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of quantum computing in solving complex industrial optimization problems like robotic inspection routes.

Method: The task is modeled as a 3D Traveling Salesman Problem with constraints (incomplete graphs, open routes). Quantum solvers from D-Wave are compared with classical solvers like GUROBI and Google OR-Tools using real-world cases.

Result: Quantum solvers provide competitive solution quality with significantly reduced computation times compared to classical methods.

Conclusion: Hybrid quantum-classical algorithms present promising opportunities for improving industrial automation tasks, supporting Industry 4.0 innovations.

Abstract: This work explores the application of hybrid quantum-classical algorithms to
optimize robotic inspection trajectories derived from Computer-Aided Design
(CAD) models in industrial settings. By modeling the task as a 3D variant of
the Traveling Salesman Problem, incorporating incomplete graphs and open-route
constraints, this study evaluates the performance of two D-Wave-based solvers
against classical methods such as GUROBI and Google OR-Tools. Results across
five real-world cases demonstrate competitive solution quality with
significantly reduced computation times, highlighting the potential of quantum
approaches in automation under Industry 4.0.

</details>


### [258] [BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments](https://arxiv.org/abs/2507.01485)
*Yibo Qiu,Zan Huang,Zhiyu Wang,Handi Liu,Yiling Qiao,Yifeng Hu,Shu'ang Sun,Hangke Peng,Ronald X Xu,Mingzhai Sun*

Main category: cs.RO

TL;DR: BioMARS is an AI-driven platform integrating large language models, vision-language models, and modular robotics to autonomously conduct biological experiments.


<details>
  <summary>Details</summary>
Motivation: To address limitations in current experimental automation systems, such as rigid protocol design, lack of adaptability, inadequate error handling, and operational complexity.

Method: BioMARS uses a hierarchical setup with three agents: Biologist Agent for protocol synthesis, Technician Agent for code translation, and Inspector Agent for anomaly detection, supported by multimodal perception and modular robotics.

Result: The system autonomously performed tasks like cell passaging and culture with high viability, consistency, and morphological integrity, outperforming conventional methods in cellular differentiation and lab task optimization.

Conclusion: BioMARS demonstrates the potential of AI-driven automation in biology, emphasizing the transformative power of language-based reasoning for laboratory tasks and optimization.

Abstract: Large language models (LLMs) and vision-language models (VLMs) have the
potential to transform biological research by enabling autonomous
experimentation. Yet, their application remains constrained by rigid protocol
design, limited adaptability to dynamic lab conditions, inadequate error
handling, and high operational complexity. Here we introduce BioMARS
(Biological Multi-Agent Robotic System), an intelligent platform that
integrates LLMs, VLMs, and modular robotics to autonomously design, plan, and
execute biological experiments. BioMARS uses a hierarchical architecture: the
Biologist Agent synthesizes protocols via retrieval-augmented generation; the
Technician Agent translates them into executable robotic pseudo-code; and the
Inspector Agent ensures procedural integrity through multimodal perception and
anomaly detection. The system autonomously conducts cell passaging and culture
tasks, matching or exceeding manual performance in viability, consistency, and
morphological integrity. It also supports context-aware optimization,
outperforming conventional strategies in differentiating retinal pigment
epithelial cells. A web interface enables real-time human-AI collaboration,
while a modular backend allows scalable integration with laboratory hardware.
These results highlight the feasibility of generalizable, AI-driven laboratory
automation and the transformative role of language-based reasoning in
biological research.

</details>


### [259] [Dynamic System Model Generation for Online Fault Detection and Diagnosis of Robotic Systems](https://arxiv.org/abs/2507.01550)
*Johannes Kohl,Georg Muck,Georg JÃ¤ger,Sebastian Zug*

Main category: cs.RO

TL;DR: The paper proposes a dynamic fault detection and diagnosis system for robots without requiring predetermined models or historical data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of fault detection in complex, dynamic robotic systems while minimizing reliance on static models, historical data, or expert involvement.

Method: The authors propose generating a dynamic system model at runtime, specifically targeting robotic systems with a shared software design for universal applicability.

Result: The proposed dynamic model system locates root causes of faults effectively without imposing significant computational overhead.

Conclusion: The method offers a flexible, lightweight approach to fault diagnosis in fast-changing robotic systems, improving autonomy and adaptability.

Abstract: With the rapid development of more complex robots, Fault Detection and
Diagnosis (FDD) becomes increasingly harder. Especially the need for
predetermined models and historic data is problematic because they do not
encompass the dynamic and fast-changing nature of such systems. To this end, we
propose a concept that actively generates a dynamic system model at runtime and
utilizes it to locate root causes. The goal is to be applicable to all kinds of
robotic systems that share a similar software design. Additionally, it should
exhibit minimal overhead and enhance independence from expert attention.

</details>


### [260] [Self-Closing Suction Grippers for Industrial Grasping via Form-Flexible Design](https://arxiv.org/abs/2507.01561)
*Huijiang Wang,Holger Kunz,Timon Adler,Fumiya Iida*

Main category: cs.RO

TL;DR: The paper introduces a form-flexible gripper for adaptive grasping, leveraging a hybrid jamming and suction mechanism, enabling secure and versatile handling of objects.


<details>
  <summary>Details</summary>
Motivation: Traditional grippers face challenges in handling objects of varying sizes, motivating the need for an adaptive, form-flexible gripper design.

Method: The gripper is designed with a hybrid jamming and suction mechanism that uses a self-closing, airtight seal and a passive morphing interface adjustable with pressure and flow rate.

Result: This gripper can securely handle delicate objects, like eggs smaller than its aperture, and achieves a high load-to-mass ratio of 94.3.

Conclusion: The hybrid design demonstrates significant advancements in secure, adaptive grasping for industrial applications.

Abstract: Shape-morphing robots have shown benefits in industrial grasping. We propose
form-flexible grippers for adaptive grasping. The design is based on the hybrid
jamming and suction mechanism, which deforms to handle objects that vary
significantly in size from the aperture, including both larger and smaller
parts. Compared with traditional grippers, the gripper achieves self-closing to
form an airtight seal. Under a vacuum, a wide range of grasping is realized
through the passive morphing mechanism at the interface that harmonizes
pressure and flow rate. This hybrid gripper showcases the capability to
securely grasp an egg, as small as 54.5% of its aperture, while achieving a
maximum load-to-mass ratio of 94.3.

</details>


### [261] [An RRT* algorithm based on Riemannian metric model for optimal path planning](https://arxiv.org/abs/2507.01697)
*Yu Zhang,Qi Zhou,Xiao-Song Yang*

Main category: cs.RO

TL;DR: This paper develops a metric-based approach for path planning on submanifolds in high-dimensional space using a Riemannian model. A new Riemannian metric enables optimization on a projected 2D plane, enhancing path planning performance in uneven fields.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimizing robot path planning in high-dimensional environments with uneven terrains by leveraging geometric properties of submanifolds.

Method: The study introduces a novel Riemannian metric that captures environmental information on a 2D projection plane. Path planning is then approached as a geometric problem on this plane, and the RRT*-R incremental algorithm is developed based on this metric.

Result: Experimental outcomes demonstrate that the RRT*-R algorithm is effective for uneven multi-dimensional fields, avoids challenging terrain features, and produces smoother and more optimized paths than the traditional RRT* algorithm.

Conclusion: The RRT*-R algorithm provides an efficient and theoretically supported solution for robot path planning, approximating the minimum geodesic distance and handling high-dimensional environmental complexities effectively.

Abstract: This paper presents a Riemannian metric-based model to solve the optimal path
planning problem on two-dimensional smooth submanifolds in high-dimensional
space. Our model is based on constructing a new Riemannian metric on a
two-dimensional projection plane, which is induced by the high-dimensional
Euclidean metric on two-dimensional smooth submanifold and reflects the
environmental information of the robot. The optimal path planning problem in
high-dimensional space is therefore transformed into a geometric problem on the
two-dimensional plane with new Riemannian metric. Based on the new Riemannian
metric, we proposed an incremental algorithm RRT*-R on the projection plane.
The experimental results show that the proposed algorithm is suitable for
scenarios with uneven fields in multiple dimensions. The proposed algorithm can
help the robot to effectively avoid areas with drastic changes in height,
ground resistance and other environmental factors. More importantly, the RRT*-R
algorithm shows better smoothness and optimization properties compared with the
original RRT* algorithm using Euclidean distance in high-dimensional workspace.
The length of the entire path by RRT*-R is a good approximation of the
theoretical minimum geodesic distance on projection plane.

</details>


### [262] [Efficient Collision Detection for Long and Slender Robotic Links in Euclidean Distance Fields: Application to a Forestry Crane](https://arxiv.org/abs/2507.01705)
*Marc-Philip Ecker,Bernhard Bischof,Minh Nhat Vu,Christoph FrÃ¶hlich,Tobias GlÃ¼ck,Wolfgang KemmetmÃ¼ller*

Main category: cs.RO

TL;DR: This paper introduces a collision detection algorithm to enhance motion planning for large-scale manipulators like forestry cranes, addressing inefficiencies in traditional spherical approximations.


<details>
  <summary>Details</summary>
Motivation: The spherical approximation of robots in voxelized distance fields is inefficient for manipulators with elongated structures, such as forestry cranes.

Method: A collision detection algorithm tailored for elongated manipulators, improving computational efficiency and removing the need for accuracy fine-tuning.

Result: Validation through real-world LiDAR data and simulated environments demonstrates its effectiveness.

Conclusion: The proposed algorithm significantly boosts the computational efficiency and accuracy of motion planning in complex outdoor environments.

Abstract: Collision-free motion planning in complex outdoor environments relies heavily
on perceiving the surroundings through exteroceptive sensors. A widely used
approach represents the environment as a voxelized Euclidean distance field,
where robots are typically approximated by spheres. However, for large-scale
manipulators such as forestry cranes, which feature long and slender links,
this conventional spherical approximation becomes inefficient and inaccurate.
This work presents a novel collision detection algorithm specifically designed
to exploit the elongated structure of such manipulators, significantly
enhancing the computational efficiency of motion planning algorithms. Unlike
traditional sphere decomposition methods, our approach not only improves
computational efficiency but also naturally eliminates the need to fine-tune
the approximation accuracy as an additional parameter. We validate the
algorithm's effectiveness using real-world LiDAR data from a forestry crane
application, as well as simulated environment data.

</details>


### [263] [SE(3)-Equivariant Diffusion Policy in Spherical Fourier Space](https://arxiv.org/abs/2507.01723)
*Xupeng Zhu,Fan Wang,Robin Walters,Jane Shi*

Main category: cs.RO

TL;DR: The paper introduces Spherical Diffusion Policy (SDP) which achieves robust generalization in 3D manipulation tasks with SE(3) equivariance, outperforming strong baselines in simulated and physical robot tasks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve the generalization capability of closed-loop manipulation policies to novel 3D object arrangements, as existing diffusion policies struggle in such scenarios.

Method: The paper introduces SDP, which embeds states, actions, and the denoising process in spherical Fourier space and uses spherical FiLM layers and a spherical denoising temporal U-net for efficient spatiotemporal processing.

Result: SDP showed significant performance improvements across 20 simulation tasks and 5 physical robot tasks, including single-arm and bi-manual setups.

Conclusion: SDP is an effective SE(3) equivariant manipulation policy that can robustly generalize across transformed 3D scenes, facilitating better real-world applications.

Abstract: Diffusion Policies are effective at learning closed-loop manipulation
policies from human demonstrations but generalize poorly to novel arrangements
of objects in 3D space, hurting real-world performance. To address this issue,
we propose Spherical Diffusion Policy (SDP), an SE(3) equivariant diffusion
policy that adapts trajectories according to 3D transformations of the scene.
Such equivariance is achieved by embedding the states, actions, and the
denoising process in spherical Fourier space. Additionally, we employ novel
spherical FiLM layers to condition the action denoising process equivariantly
on the scene embeddings. Lastly, we propose a spherical denoising temporal
U-net that achieves spatiotemporal equivariance with computational efficiency.
In the end, SDP is end-to-end SE(3) equivariant, allowing robust generalization
across transformed 3D scenes. SDP demonstrates a large performance improvement
over strong baselines in 20 simulation tasks and 5 physical robot tasks
including single-arm and bi-manual embodiments. Code is available at
https://github.com/amazon-science/Spherical_Diffusion_Policy.

</details>


### [264] [Augmented Bridge Spinal Fixation: A New Concept for Addressing Pedicle Screw Pullout via a Steerable Drilling Robot and Flexible Pedicle Screws](https://arxiv.org/abs/2507.01753)
*Yash Kulkarni,Susheela Sharma,Omid Rezayof,Siddhartha Kapuria,Jordan P. Amadio,Mohsen Khadem,Maryam Tilton,Farshid Alambeigi*

Main category: cs.RO

TL;DR: This paper proposes the Augmented Bridge Spinal Fixation (AB-SF) technique using a robotic system and flexible pedicle screws for stronger spinal fixation.


<details>
  <summary>Details</summary>
Motivation: The motivation is to overcome the limitations of rigid pedicle screws, such as screw loosening and pullout, during spinal fixation procedures.

Method: A Concentric Tube Steerable Drilling Robot (CT-SDR) is used to create inter-pedicle tunnels, followed by implantation of Flexible Pedicle Screws (FPS) and bone cement injection for augmentation.

Result: Two successful fixation scenarios were created using a robotic arm, demonstrating feasibility through tunnel formation, screw placement, and cement augmentation in a vertebral phantom.

Conclusion: The AB-SF concept shows potential for improving spinal fixation strength and broadening the application of robotic and flexible screw technologies.

Abstract: To address the screw loosening and pullout limitations of rigid pedicle
screws in spinal fixation procedures, and to leverage our recently developed
Concentric Tube Steerable Drilling Robot (CT-SDR) and Flexible Pedicle Screw
(FPS), in this paper, we introduce the concept of Augmented Bridge Spinal
Fixation (AB-SF). In this concept, two connecting J-shape tunnels are first
drilled through pedicles of vertebra using the CT-SDR. Next, two FPSs are
passed through this tunnel and bone cement is then injected through the
cannulated region of the FPS to form an augmented bridge between two pedicles
and reinforce strength of the fixated spine. To experimentally analyze and
study the feasibility of AB-SF technique, we first used our robotic system
(i.e., a CT-SDR integrated with a robotic arm) to create two different fixation
scenarios in which two J-shape tunnels, forming a bridge, were drilled at
different depth of a vertebral phantom. Next, we implanted two FPSs within the
drilled tunnels and then successfully simulated the bone cement augmentation
process.

</details>


### [265] [S3D: A Spatial Steerable Surgical Drilling Framework for Robotic Spinal Fixation Procedures](https://arxiv.org/abs/2507.01779)
*Daniyal Maroufi,Xinyuan Huang,Yash Kulkarni,Omid Rezayof,Susheela Sharma,Vaibhav Goggela,Jordan P. Amadio,Mohsen Khadem,Farshid Alambeigi*

Main category: cs.RO

TL;DR: This paper introduces S3D, a steerable surgical drilling framework designed for robotic spinal fixation procedures, validated through experiments on vertebral phantoms.


<details>
  <summary>Details</summary>
Motivation: The aim is to enhance precision and adaptability in robotic spinal fixation procedures by considering anatomical constraints and enabling realistic steerable drilling.

Method: The approach involves improving the concentric tube steerable drilling robot (CT-SDR) and integrating it with a robotic manipulator, alongside a calibration, registration, and navigation procedure for spinal phantom experiments.

Result: Experiments on vertebral phantoms demonstrated the framework's ability to perform planar and out-of-plane steerable drilling, validating the proposed approach.

Conclusion: The study successfully developed and validated a spatial steerable surgical drilling framework, advancing the capability for accurate spinal fixation using robotics.

Abstract: In this paper, we introduce S3D: A Spatial Steerable Surgical Drilling
Framework for Robotic Spinal Fixation Procedures. S3D is designed to enable
realistic steerable drilling while accounting for the anatomical constraints
associated with vertebral access in spinal fixation (SF) procedures. To achieve
this, we first enhanced our previously designed concentric tube Steerable
Drilling Robot (CT-SDR) to facilitate steerable drilling across all vertebral
levels of the spinal column. Additionally, we propose a four-Phase calibration,
registration, and navigation procedure to perform realistic SF procedures on a
spine holder phantom by integrating the CT-SDR with a seven-degree-of-freedom
robotic manipulator. The functionality of this framework is validated through
planar and out-of-plane steerable drilling experiments in vertebral phantoms.

</details>


### [266] [Towards Design and Development of a Concentric Tube Steerable Drilling Robot for Creating S-shape Tunnels for Pelvic Fixation Procedures](https://arxiv.org/abs/2507.01811)
*Yash Kulkarni,Susheela Sharma,Sarah Go,Jordan P. Amadio,Mohsen Khadem,Farshid Alambeigi*

Main category: cs.RO

TL;DR: Current methods for pelvic fixation are rigid and lead to complications; a new robotic system (pelvic CT-SDR) tackles this with steerable and anatomically optimized screw pathways.


<details>
  <summary>Details</summary>
Motivation: Existing pelvic fixation techniques result in suboptimal screw placement, leading to complications such as misplacement, longer surgeries, and increased radiation exposure.

Method: The study introduces a 4 DoF concentric tube steerable drilling robot (pelvic CT-SDR) capable of achieving S-shaped drilling trajectories suitable for pelvic anatomy.

Result: Experiments using simulated bone phantoms demonstrated the effectiveness of the pelvic CT-SDR for creating anatomically appropriate screw paths.

Conclusion: The pelvic CT-SDR provides a promising solution for enhancing pelvic fixation, reducing complications, and improving surgical outcomes.

Abstract: Current pelvic fixation techniques rely on rigid drilling tools, which
inherently constrain the placement of rigid medical screws in the complex
anatomy of pelvis. These constraints prevent medical screws from following
anatomically optimal pathways and force clinicians to fixate screws in linear
trajectories. This suboptimal approach, combined with the unnatural placement
of the excessively long screws, lead to complications such as screw
misplacement, extended surgery times, and increased radiation exposure due to
repeated X-ray images taken ensure to safety of procedure. To address these
challenges, in this paper, we present the design and development of a unique 4
degree-of-freedom (DoF) pelvic concentric tube steerable drilling robot (pelvic
CT-SDR). The pelvic CT-SDR is capable of creating long S-shaped drilling
trajectories that follow the natural curvatures of the pelvic anatomy. The
performance of the pelvic CT-SDR was thoroughly evaluated through several
S-shape drilling experiments in simulated bone phantoms.

</details>


### [267] [MoIRA: Modular Instruction Routing Architecture for Multi-Task Robotics](https://arxiv.org/abs/2507.01843)
*Dmytro Kuzmenko,Nadiya Shvai*

Main category: cs.RO

TL;DR: The paper introduces MoIRA, a modular Mixture-of-Experts (MoE) framework that uses an external text-based mechanism for routing tasks across expert models, showing superior performance in robotics benchmarks.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in traditional MoE systems, such as lack of flexibility in customization, need for additional training, and inefficient routing mechanisms.

Method: Proposed the MoIRA framework, which uses text-based zero-shot routing methods (embedding-based similarity and prompt-driven inference) to dynamically coordinate experts. This approach leverages Vision-Language-Action models as experts, with high efficiency ensured by low-rank adapters.

Result: MoIRA outperforms generalist models and rivals other MoE frameworks on GR1 Humanoid tasks and LIBERO benchmarks while being robust to variations in task instructions.

Conclusion: The framework validates the use of modular, scalable, and text-driven routing in multi-expert robotic systems, offering efficient and precise decision-making without additional training overhead.

Abstract: Mixture-of-Experts (MoE) approaches have recently gained traction in robotics
applications due to their ability to dynamically allocate computational
resources and specialize sub-networks for distinct tasks or environmental
contexts, enabling more efficient decision-making. Such systems often comprise
sparsely activated experts combined under a single monolithic architecture and
require a well-configured internal routing mechanism, which does not allow for
selective low-level expert and router customization and requires additional
training. We propose MoIRA, an architecture-agnostic modular MoE framework
designed to coordinate existing experts with an external text-based router.
MoIRA incorporates two zero-shot routing options: embedding-based similarity
and prompt-driven language model inference. In our experiments, we choose large
Vision-Language-Action models, gr00t-N1 and $\pi_0$, as the underlying experts,
and train low-rank adapters for low-overhead inference. We evaluate MoIRA on
various GR1 Humanoid tasks and LIBERO Spatial and Goal benchmarks, where it
consistently outperforms generalist models and competes with other MoE
pipelines. Additionally, we analyse the robustness of the proposed approach to
the variations of the instructions. While relying solely on textual
descriptions of tasks and experts, MoIRA demonstrates the practical viability
of modular deployment with precise, low-effort routing and provides an
alternative, scalable foundation for future multi-expert robotic systems.

</details>


### [268] [TypeTele: Releasing Dexterity in Teleoperation by Dexterous Manipulation Types](https://arxiv.org/abs/2507.01857)
*Yuhao Lin,Yi-Lin Wei,Haoran Liao,Mu Lin,Chengyi Xing,Hao Li,Dandan Zhang,Mark Cutkosky,Wei-Shi Zheng*

Main category: cs.RO

TL;DR: The paper proposes TypeTele, a teleoperation system that leverages type-guided dexterous manipulation to enhance task performance beyond mimicking human hand postures.


<details>
  <summary>Details</summary>
Motivation: Existing teleoperation methods limit dexterous robotic hands by merely mimicking human hand movements, resulting in underuse of their unique capabilities.

Method: TypeTele introduces manipulation types into teleoperation, supported by a type library and an MLLM-assisted module for task-specific type retrieval.

Result: Experiments show improved task success rates and better utilization of dexterous robot hands in both real-world teleoperation and imitation learning scenarios.

Conclusion: TypeTele enhances dexterous teleoperation by utilizing manipulation types, enabling robots to optimize their capabilities for complex tasks.

Abstract: Dexterous teleoperation plays a crucial role in robotic manipulation for
real-world data collection and remote robot control. Previous dexterous
teleoperation mostly relies on hand retargeting to closely mimic human hand
postures. However, these approaches may fail to fully leverage the inherent
dexterity of dexterous hands, which can execute unique actions through their
structural advantages compared to human hands. To address this limitation, we
propose TypeTele, a type-guided dexterous teleoperation system, which enables
dexterous hands to perform actions that are not constrained by human motion
patterns. This is achieved by introducing dexterous manipulation types into the
teleoperation system, allowing operators to employ appropriate types to
complete specific tasks. To support this system, we build an extensible
dexterous manipulation type library to cover comprehensive dexterous postures
used in manipulation tasks. During teleoperation, we employ a MLLM
(Multi-modality Large Language Model)-assisted type retrieval module to
identify the most suitable manipulation type based on the specific task and
operator commands. Extensive experiments of real-world teleoperation and
imitation learning demonstrate that the incorporation of manipulation types
significantly takes full advantage of the dexterous robot's ability to perform
diverse and complex tasks with higher success rates.

</details>


### [269] [A Survey on Vision-Language-Action Models: An Action Tokenization Perspective](https://arxiv.org/abs/2507.01925)
*Yifan Zhong,Fengshuo Bai,Shaofei Cai,Xuchuan Huang,Zhang Chen,Xiaowei Zhang,Yuanfei Wang,Shaoyang Guo,Tianrui Guan,Ka Nam Lui,Zhiquan Qi,Yitao Liang,Yuanpei Chen,Yaodong Yang*

Main category: cs.RO

TL;DR: This survey examines vision-language-action (VLA) models, categorizing their approaches to processing action tokens and identifying areas for improvement.


<details>
  <summary>Details</summary>
Motivation: To extend the capabilities of vision and language foundation models to the physical world, leading to advancements in general-purpose intelligence.

Method: The authors categorize and analyze VLA models based on their approach to action tokenization, distilling strengths, limitations, and areas for improvement through a systematic review.

Result: The study identifies eight types of action token formulations and their characteristics, revealing gaps in understanding and development opportunities within the VLA domain.

Conclusion: By categorizing VLA research, the paper provides guidance to improve action tokenization strategies and advance toward more general-purpose intelligence in VLA systems.

Abstract: The remarkable advancements of vision and language foundation models in
multimodal understanding, reasoning, and generation has sparked growing efforts
to extend such intelligence to the physical world, fueling the flourishing of
vision-language-action (VLA) models. Despite seemingly diverse approaches, we
observe that current VLA models can be unified under a single framework: vision
and language inputs are processed by a series of VLA modules, producing a chain
of \textit{action tokens} that progressively encode more grounded and
actionable information, ultimately generating executable actions. We further
determine that the primary design choice distinguishing VLA models lies in how
action tokens are formulated, which can be categorized into language
description, code, affordance, trajectory, goal state, latent representation,
raw action, and reasoning. However, there remains a lack of comprehensive
understanding regarding action tokens, significantly impeding effective VLA
development and obscuring future directions. Therefore, this survey aims to
categorize and interpret existing VLA research through the lens of action
tokenization, distill the strengths and limitations of each token type, and
identify areas for improvement. Through this systematic review and analysis, we
offer a synthesized outlook on the broader evolution of VLA models, highlight
underexplored yet promising directions, and contribute guidance for future
research, hoping to bring the field closer to general-purpose intelligence.

</details>


### [270] [Large Language Model-Driven Closed-Loop UAV Operation with Semantic Observations](https://arxiv.org/abs/2507.01930)
*Wenhao Wang,Yanyan Li,Long Jiao,Jiawei Yuan*

Main category: cs.RO

TL;DR: This paper introduces a closed-loop control framework using Large Language Models (LLMs) for reliable UAV operations, addressing logical reasoning and decision-making challenges.


<details>
  <summary>Details</summary>
Motivation: To enhance the reliability of LLM-driven UAV operations which face challenges in logical reasoning and complex decision-making.

Method: The proposed framework includes two LLM modulesâa Code Generator and an Evaluatorâthat use natural language trajectory descriptions and simulation-based refinement for reliable feedback and code execution.

Result: Experiments show the framework outperforms baselines in terms of success rate and task completeness, especially as task complexity increases.

Conclusion: The framework enables reliable UAV operations using LLMs, with effective feedback mechanisms and risk mitigation during code refinement.

Abstract: Large Language Models (LLMs) have revolutionized robotic autonomy, including
Unmanned Aerial Vehicles (UAVs). Recent studies have demonstrated the potential
of LLMs for translating human instructions into executable control code for UAV
operations. However, LLMs still face challenges from logical reasoning and
complex decision-making, leading to concerns about the reliability of
LLM-driven UAV operations. In this paper, we propose a LLM-driven closed-loop
control framework that enables reliable UAV operations powered by effective
feedback and refinement using two LLM modules, i.e., a Code Generator and an
Evaluator. Our framework transforms numerical state observations from UAV
operations into natural language trajectory descriptions to enhance the
evaluator LLM's understanding of UAV dynamics for precise feedback generation.
Our framework also enables a simulation-based refinement process, and hence
eliminates the risks to physical UAVs caused by incorrect code execution during
the refinement. Extensive experiments on UAV control tasks with different
complexities are conducted. The experimental results show that our framework
can achieve reliable UAV operations using LLMs, which significantly outperforms
baseline approaches in terms of success rate and completeness with the increase
of task complexity.

</details>


### [271] [AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation](https://arxiv.org/abs/2507.01961)
*Sixiang Chen,Jiaming Liu,Siyuan Qian,Han Jiang,Lily Li,Renrui Zhang,Zhuoyang Liu,Chenyang Gu,Chengkai Hou,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: This paper introduces the Adaptive Coordination Diffusion Transformer (AC-DiT) to improve coordination in mobile manipulation by addressing challenges like error accumulation and uniform visual observation modalities.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of current mobile manipulation methods, which struggle with coordinating the mobile base and manipulator due to error accumulation and a lack of stage-specific visual observation modalities.

Method: The proposed AC-DiT incorporates a mobility-to-body conditioning mechanism for coordinated whole-body control and a perception-aware multimodal conditioning strategy for stage-specific visual observation requirements.

Result: Extensive experiments conducted on both simulated and real-world mobile manipulation tasks validate the effectiveness of AC-DiT.

Conclusion: AC-DiT enhances end-to-end mobile manipulation by addressing coordination issues and leveraging multimodal visual perception tailored to task requirements.

Abstract: Recently, mobile manipulation has attracted increasing attention for enabling
language-conditioned robotic control in household tasks. However, existing
methods still face challenges in coordinating mobile base and manipulator,
primarily due to two limitations. On the one hand, they fail to explicitly
model the influence of the mobile base on manipulator control, which easily
leads to error accumulation under high degrees of freedom. On the other hand,
they treat the entire mobile manipulation process with the same visual
observation modality (e.g., either all 2D or all 3D), overlooking the distinct
multimodal perception requirements at different stages during mobile
manipulation. To address this, we propose the Adaptive Coordination Diffusion
Transformer (AC-DiT), which enhances mobile base and manipulator coordination
for end-to-end mobile manipulation. First, since the motion of the mobile base
directly influences the manipulator's actions, we introduce a mobility-to-body
conditioning mechanism that guides the model to first extract base motion
representations, which are then used as context prior for predicting whole-body
actions. This enables whole-body control that accounts for the potential impact
of the mobile base's motion. Second, to meet the perception requirements at
different stages of mobile manipulation, we design a perception-aware
multimodal conditioning strategy that dynamically adjusts the fusion weights
between various 2D visual images and 3D point clouds, yielding visual features
tailored to the current perceptual needs. This allows the model to, for
example, adaptively rely more on 2D inputs when semantic information is crucial
for action prediction, while placing greater emphasis on 3D geometric
information when precise spatial understanding is required. We validate AC-DiT
through extensive experiments on both simulated and real-world mobile
manipulation tasks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [272] [Is It Safe To Learn And Share? On Psychological Safety and Social Learning in (Agile) Communities of Practice](https://arxiv.org/abs/2507.01065)
*Christiaan Verwijs,Evelien Acun-Roos,Daniel Russo*

Main category: cs.SE

TL;DR: This study highlights psychological safety issues in Agile Communities of Practice (CoPs), emphasizing lower safety in online vs. face-to-face settings and offering interventions to improve inclusivity.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding psychological safety in virtual and hybrid Agile CoPs environments and its impact on continuous learning.

Method: A mixed-methods approach including surveys from 143 participants and thematic analysis of interaction patterns, validated through member checking with 30 participants.

Result: Psychological safety in online interactions is significantly lower than face-to-face settings, affecting contribution intent and interpersonal risk-taking, with threats like exclusionary behavior and tribalism identified.

Conclusion: Enforcing explicit norms, structured facilitation, and active moderation can improve psychological safety in virtual work environments, enhancing social learning and community engagement.

Abstract: As hybrid, distributed, and asynchronous work models become more prevalent,
continuous learning in Agile Software Development (ASD) gains renewed
importance. Communities of Practice (CoPs) are increasingly adopted to support
social learning beyond formal education, often relying on virtual
communication. Psychological safety, a prerequisite for effective learning,
remains insufficiently understood in these settings. This mixed-methods study
investigates psychological safety within Agile CoPs through survey data from
143 participants. Results indicate that psychological safety is significantly
lower in online interactions compared to face-to-face settings. Moreover, low
psychological safety reduces participants' intent to continue contributing and
avoidance of interpersonal risk. No significant differences emerged based on
gender, community seniority, or content creation activity. However, differences
by role and age group suggest potential generational or role-related effects.
Thematic analysis revealed exclusionary behavior, negative interaction
patterns, and hostility as primary threats to psychological safety, often
reinforced by tribalism and specific community dynamics. Suggested
interventions include establishing explicit norms, structured facilitation, and
active moderation. The findings were validated through member checking with 30
participants. This study provides a comparative perspective on interaction
modalities and offers practical guidance for organizers seeking to cultivate
inclusive, high-impact CoPs and similarly structured virtual or hybrid work
environments.

</details>


### [273] [Bugs in the Shadows: Static Detection of Faulty Python Refactorings](https://arxiv.org/abs/2507.01103)
*Jonhnanthan Oliveira,Rohit Gheyi,MÃ¡rcio Ribeiro,Alessandro Garcia*

Main category: cs.SE

TL;DR: This paper introduces a static analysis technique to detect type errors in Python during refactoring, uncovering 29 bugs in popular tools and projects.


<details>
  <summary>Details</summary>
Motivation: Dynamic typing in Python complicates automated refactoring, potentially introducing type errors that affect software reliability and developer efficiency.

Method: The authors proposed a static analysis technique and evaluated it by analyzing Rope refactoring implementations on open-source Python projects.

Result: The study identified 29 type-related bugs across 4 refactoring types from 1,152 refactorings, with some issues found in IDEs like PyCharm and PyDev.

Conclusion: The findings emphasize the importance of improving Python refactoring tools' robustness to ensure correct automated transformations and effective software maintenance.

Abstract: Python is a widely adopted programming language, valued for its simplicity
and flexibility. However, its dynamic type system poses significant challenges
for automated refactoring - an essential practice in software evolution aimed
at improving internal code structure without changing external behavior.
Understanding how type errors are introduced during refactoring is crucial, as
such errors can compromise software reliability and reduce developer
productivity. In this work, we propose a static analysis technique to detect
type errors introduced by refactoring implementations for Python. We evaluated
our technique on Rope refactoring implementations, applying them to open-source
Python projects. Our analysis uncovered 29 bugs across four refactoring types
from a total of 1,152 refactoring attempts. Several of these issues were also
found in widely used IDEs such as PyCharm and PyDev. All reported bugs were
submitted to the respective developers, and some of them were acknowledged and
accepted. These results highlight the need to improve the robustness of current
Python refactoring tools to ensure the correctness of automated code
transformations and support reliable software maintenance.

</details>


### [274] [Context-Aware Code Wiring Recommendation with LLM-based Agent](https://arxiv.org/abs/2507.01315)
*Taiming Wang,Yanjie Jiang,Chunhao Dong,Yuxia Zhang,Hui Liu*

Main category: cs.SE

TL;DR: WIRL is an LLM-based tool designed for improving code adaptation by effectively resolving unresolved variables in pasted code using a Retrieval-Augmented Generation (RAG) infilling method. It has demonstrated significant improvements over existing tools.


<details>
  <summary>Details</summary>
Motivation: Modern development frequently involves copy-pasting code from external sources, which often requires adjustments to integrate with existing variables in the local codebase. Current methods fail to leverage contextual information effectively, leading to limited performance in such adaptations.

Method: The authors introduce WIRL, a solution combining an LLM, a custom toolkit, and an orchestration module to identify and resolve unresolved variables. It uses a mixed strategy: deterministic rules for common issues and an intelligent decision framework for more complex contexts, under a Retrieval-Augmented Generation (RAG) structure.

Result: WIRL achieves high performance in code adaptation scenarios, with a precision of 91.7% and recall of 90.0%. It outperforms advanced LLMs by 22.6 and 13.7 percentage points in precision and recall, respectively, and IntelliJ IDEA by 54.3 and 49.9 points.

Conclusion: WIRL effectively handles complex and context-dependent code adaptations, showing significant performance gains over both state-of-the-art models and widely-used IDEs. Its development indicates progress toward more intelligent and context-aware developer tools.

Abstract: Copy-paste-modify is a widespread and pragmatic practice in software
development, where developers adapt reused code snippets, sourced from
platforms such as Stack Overflow, GitHub, or LLM outputs, into their local
codebase. A critical yet underexplored aspect of this adaptation is code
wiring, which involves substituting unresolved variables in the pasted code
with suitable ones from the surrounding context. Existing solutions either rely
on heuristic rules or historical templates, often failing to effectively
utilize contextual information, despite studies showing that over half of
adaptation cases are context-dependent. In this paper, we introduce WIRL, an
LLM-based agent for code wiring framed as a Retrieval-Augmented Generation
(RAG) infilling task. WIRL combines an LLM, a customized toolkit, and an
orchestration module to identify unresolved variables, retrieve context, and
perform context-aware substitutions. To balance efficiency and autonomy, the
agent adopts a mixed strategy: deterministic rule-based steps for common
patterns, and a state-machine-guided decision process for intelligent
exploration. We evaluate WIRL on a carefully curated, high-quality dataset
consisting of real-world code adaptation scenarios. Our approach achieves an
exact match precision of 91.7% and a recall of 90.0%, outperforming advanced
LLMs by 22.6 and 13.7 percentage points in precision and recall, respectively,
and surpassing IntelliJ IDEA by 54.3 and 49.9 percentage points. These results
underscore its practical utility, particularly in contexts with complex
variable dependencies or multiple unresolved variables. We believe WIRL paves
the way for more intelligent and context-aware developer assistance in modern
IDEs.

</details>


### [275] [Combining Type Inference and Automated Unit Test Generation for Python](https://arxiv.org/abs/2507.01477)
*Lukas Krodinger,Stephan Lukasczyk,Gordon Fraser*

Main category: cs.SE

TL;DR: This paper introduces "type tracing" to address the lack of type information in dynamically-typed Python during automated test generation. The approach improves branch coverage and mutation scores in test cases.


<details>
  <summary>Details</summary>
Motivation: The paper is motivated by the challenge of generating automated unit tests for dynamically-typed languages like Python, where the absence of type information hinders test generator effectiveness.

Method: The paper proposes type tracing, which dynamically records type information during runtime execution of candidate tests to iteratively refine type information. It is implemented as an extension of the Pynguin framework.

Result: The type tracing method leads to up to 90% improvement in branch coverage, better mutation scores, and type information comparable to existing state-of-the-art type-inference tools.

Conclusion: Type tracing effectively addresses limitations of test generation in dynamically-typed languages, enhancing test quality and contributing valuable type data for Python.

Abstract: Automated unit test generation is an established research field that has so
far focused on statically-typed programming languages. The lack of type
information in dynamically-typed programming languages, such as Python,
inhibits test generators, which heavily rely on information about parameter and
return types of functions to select suitable arguments when constructing test
cases. Since automated test generators inherently rely on frequent execution of
candidate tests, we make use of these frequent executions to address this
problem by introducing type tracing, which extracts type-related information
during execution and gradually refines the available type information. We
implement type tracing as an extension of the Pynguin test-generation framework
for Python, allowing it (i) to infer parameter types by observing how
parameters are used during runtime, (ii) to record the types of values that
function calls return, and (iii) to use this type information to increase code
coverage. The approach leads to up to 90.0% more branch coverage, improved
mutation scores, and to type information of similar quality to that produced by
other state-of-the-art type-inference tools.

</details>


### [276] [DaiFu: In-Situ Crash Recovery for Deep Learning Systems](https://arxiv.org/abs/2507.01628)
*Zilong He,Pengfei Chen,Hongyu Zhang,Xiaoyun Li,Guangba Yu,Hongyang Chen,Zibin Zheng*

Main category: cs.SE

TL;DR: DaiFu, an in-situ recovery framework for deep learning systems, enables fast crash recovery with negligible overhead, achieving a 1372x speedup over existing methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning systems face frequent crashes due to complex software stacks, wasting resources and hampering productivity. Effective recovery mechanisms for minor and transient errors are lacking.

Method: DaiFu employs lightweight code transformation techniques to augment DL systems, allowing instant updates to the running context (e.g., code and configurations). It dynamically intercepts crashes for quick recovery.

Result: DaiFu achieves a 1372x faster recovery time compared to current state-of-the-art solutions while incurring less than 0.40% overhead. Its effectiveness is demonstrated across 7 different crash scenarios.

Conclusion: DaiFu significantly improves crash recovery efficiency in DL systems, making it a practical and effective solution with minimal performance impact.

Abstract: Deep learning (DL) systems have been widely adopted in many areas, and are
becoming even more popular with the emergence of large language models.
However, due to the complex software stacks involved in their development and
execution, crashes are unavoidable and common. Crashes severely waste computing
resources and hinder development productivity, so efficient crash recovery is
crucial. Existing solutions, such as checkpoint-retry, are too heavyweight for
fast recovery from crashes caused by minor programming errors or transient
runtime errors. Therefore, we present DaiFu, an in-situ recovery framework for
DL systems. Through a lightweight code transformation to a given DL system,
DaiFu augments it to intercept crashes in situ and enables dynamic and instant
updates to its program running context (e.g., code, configurations, and other
data) for agile crash recovery. Our evaluation shows that DaiFu helps reduce
the restore time for crash recovery, achieving a 1372x speedup compared with
state-of-the-art solutions. Meanwhile, the overhead of DaiFu is negligible
(under 0.40%). We also construct a benchmark spanning 7 distinct crash
scenarios in DL systems, and show the effectiveness of DaiFu in diverse
situations.

</details>


### [277] [APRMCTS: Improving LLM-based Automated Program Repair with Iterative Tree Search](https://arxiv.org/abs/2507.01827)
*Haichuan Hu,Congqing He,Hao Zhang,Xiaochen Xie,Quanjun Zhang*

Main category: cs.SE

TL;DR: The paper introduces APRMCTS, a new automated program repair technique that combines LLMs with Monte Carlo Tree Search to improve patch effectiveness and efficiency. It outperforms existing methods across metrics, particularly on complex bugs.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of trial-and-error strategies in LLM-based automated program repair, which include limited patch effectiveness and low search efficiency.

Method: APRMCTS integrates Monte Carlo Tree Search into LLM-based program repair, enabling a global evaluation of explored patches to refine and generate more effective solutions.

Result: On testing 835 bugs from Defects4J, APRMCTS fixed 201 bugs, outperforming current state-of-the-art methods. It also enhances various models' patching capabilities while using significantly smaller patch sizes and reducing costs.

Conclusion: APRMCTS is a scalable and efficient APR solution with strong effectiveness, particularly for fixing complex bugs, while maintaining lower time and monetary expenses.

Abstract: Automated Program Repair (APR) attempts to fix software bugs without human
intervention, which plays a crucial role in software development and
maintenance. Recently, with the advances in Large Language Models (LLMs), a
rapidly increasing number of APR techniques have been proposed with remarkable
performance. However, existing LLM-based APR techniques typically adopt
trial-and-error strategies, which suffer from two major drawbacks: (1)
inherently limited patch effectiveness due to local exploration, and (2) low
search efficiency due to redundant exploration. In this paper, we propose
APRMCTS, which uses iterative tree search to improve LLM-based APR. APRMCTS
incorporates Monte Carlo Tree Search (MCTS) into patch searching by performing
a global evaluation of the explored patches and selecting the most promising
one for subsequent refinement and generation. APRMCTS effectively resolves the
problems of falling into local optima and thus helps improve the efficiency of
patch searching. Our experiments on 835 bugs from Defects4J demonstrate that,
when integrated with GPT-3.5, APRMCTS can fix a total of 201 bugs, which
outperforms all state-of-the-art baselines. Besides, APRMCTS helps GPT-4o-mini,
GPT-3.5, Yi-Coder-9B, and Qwen2.5-Coder-7B to fix 30, 27, 37, and 28 more bugs,
respectively. More importantly, APRMCTS boasts a significant performance
advantage while employing small patch size (16 and 32), notably fewer than the
500 and 10,000 patches adopted in previous studies. In terms of cost, compared
to existing state-of-the-art LLM-based APR methods, APRMCTS has time and
monetary costs of less than 20% and 50%, respectively. Our extensive study
demonstrates that APRMCTS exhibits good effectiveness and efficiency, with
particular advantages in addressing complex bugs.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [278] [Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping](https://arxiv.org/abs/2507.01411)
*Yifei Sun,Marshall A. Dalton,Robert D. Sanders,Yixuan Yuan,Xiang Li,Sharon L. Naismith,Fernando Calamante,Jinglei Lv*

Main category: q-bio.NC

TL;DR: This study employs a deep learning approach to map functional connectivity changes in the hippocampus during neurobiological aging, offering insights into age-sensitive brain regions.


<details>
  <summary>Details</summary>
Motivation: Understanding how functional connectivity in the hippocampus evolves with neurobiological aging remains underexplored, despite the known hallmark of grey matter loss in this region.

Method: The paper develops a deep learning framework combining 3D convolutional neural networks (3D CNN) and LayerCAM saliency mapping to analyze hippocampal functional connectivity for brain age predictions.

Result: This method identifies key hippocampal-cortical connections, including regions like the precuneus, cuneus, posterior cingulate cortex, and parahippocampal cortex, that are significantly impacted by aging. It also highlights distinct anterior and posterior hippocampal connectivity patterns.

Conclusion: The study provides novel insights into hippocampal aging mechanisms and demonstrates how interpretable deep learning methods can uncover biologically relevant patterns in neuroimaging.

Abstract: Grey matter loss in the hippocampus is a hallmark of neurobiological aging,
yet understanding the corresponding changes in its functional connectivity
remains limited. Seed-based functional connectivity (FC) analysis enables
voxel-wise mapping of the hippocampus's synchronous activity with cortical
regions, offering a window into functional reorganization during aging. In this
study, we develop an interpretable deep learning framework to predict brain age
from hippocampal FC using a three-dimensional convolutional neural network (3D
CNN) combined with LayerCAM saliency mapping. This approach maps key
hippocampal-cortical connections, particularly with the precuneus, cuneus,
posterior cingulate cortex, parahippocampal cortex, left superior parietal
lobule, and right superior temporal sulcus, that are highly sensitive to age.
Critically, disaggregating anterior and posterior hippocampal FC reveals
distinct mapping aligned with their known functional specializations. These
findings provide new insights into the functional mechanisms of hippocampal
aging and demonstrate the power of explainable deep learning to uncover
biologically meaningful patterns in neuroimaging data.

</details>


### [279] [Reduced Efficiency in the Right Fronto-Parietal Attentional Network During Distractor Suppression in Mild Cognitive Impairment](https://arxiv.org/abs/2507.01433)
*Jatupong Oboun,Piyanon Charoenpoonpanich,Anna Raksapatcharawong,Chaipat Chunharas,Itthi Chatnuntawech,Chainarong Amornbunchornvej,Sirawaj Itthipuripat*

Main category: q-bio.NC

TL;DR: The study explores Mild Cognitive Impairment (MCI) by analyzing attention and distractor suppression through EEG data and behavioral measures during tasks, finding alpha band coherence as a potential marker for early MCI detection.


<details>
  <summary>Details</summary>
Motivation: Early detection of MCI, a transitional stage to dementia, is critical for timely intervention. The paper aims to understand neural mechanisms of attention in MCI patients.

Method: The study used an EEG and behavioral experiment with a cohort of 56 MCIs and 26 healthy controls during an Eriksen flanker task, analyzing Global Efficiency (GE) in the alpha band, Reaction Time (RT), and Hit Rate (HR) across various conditions.

Result: Healthy controls showed better GE, faster RT, and higher HR in congruent conditions, while MCIs benefited from congruency with shorter RTs and higher HRs but lacked GE adaptability across conditions.

Conclusion: Alpha band coherence and GE are promising markers for early cognitive impairment. The study improves understanding of attentional control and suggests implications for interventions targeting MCI.

Abstract: Mild Cognitive Impairment (MCI) is a critical transitional stage between
normal cognitive aging and dementia, making its early detection essential. This
study investigates the neural mechanisms of distractor suppression in MCI
patients using EEG and behavioral data during an attention-cueing Eriksen
flanker task. A cohort of 56 MCIs and 26 healthy controls (HCs) performed tasks
with congruent and incongruent stimuli of varying saliency levels. During these
tasks, EEG data were analyzed for alpha band coherence's functional
connectivity, focusing on Global Efficiency (GE), while Reaction Time (RT) and
Hit Rate (HR) were also collected.
  Our findings reveal significant interactions between congruency, saliency,
and cognitive status on GE, RT, and HR. In HCs, congruent conditions resulted
in higher GE (p = 0.0114, multivariate t-distribution correction, MVT), faster
RTs (p < 0.0001, MVT), and higher HRs (p < 0.0001, MVT) compared to incongruent
conditions. HCs also showed increased GE in salient conditions for incongruent
trials (p = 0.0406, MVT). MCIs exhibited benefits from congruent conditions
with shorter RTs and higher HRs (both p < 0.0001, MVT) compared to incongruent
conditions but showed reduced adaptability in GE, with no significant GE
differences between conditions.
  These results highlight the potential of alpha band coherence and GE as early
markers for cognitive impairment. By integrating GE, RT, and HR, this study
provides insights into the interplay between neural efficiency, processing
speed, and task accuracy. This approach offers valuable insights into cognitive
load management and interference effects, indicating benefits for interventions
aimed at improving attentional control and processing speed in MCIs.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [280] [Asymptotic convexity of wide and shallow neural networks](https://arxiv.org/abs/2507.01044)
*Vivek Borkar,Parthe Pandit*

Main category: stat.ML

TL;DR: The paper demonstrates that the input-output behavior of a shallow and wide neural network approximates a convex function, shedding light on the network's strong performance.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying reason for the observed high performance of shallow and wide neural networks.

Method: The study examines the epigraph of the neural network's input-output map and compares it to the epigraph of a convex function.

Result: The result reveals that shallow and wide neural networks approximate convex behavior in their parameter space.

Conclusion: This approximation to convexity may provide a plausible explanation for the effective performance of such networks.

Abstract: For a simple model of shallow and wide neural networks, we show that the
epigraph of its input-output map as a function of the network parameters
approximates epigraph of a. convex function in a precise sense. This leads to a
plausible explanation of their observed good performance.

</details>


### [281] [Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles](https://arxiv.org/abs/2507.01542)
*Tom Szwagier,Pierre-Alexandre Mattei,Charles Bouveyron,Xavier Pennec*

Main category: stat.ML

TL;DR: The paper introduces a new family of Gaussian Mixture Models (GMMs) with piecewise-constant covariance eigenvalue profiles, striking a balance between flexibility and parsimony.


<details>
  <summary>Details</summary>
Motivation: The authors aim to address the overparameterization issues in GMM covariance matrices in high dimensions and the inflexibility of spherical GMMs for anisotropic distributions.

Method: They introduce parsimonious GMMs with piecewise-constant covariance eigenvalue profiles and propose an EM algorithm and a componentwise penalized EM algorithm to handle parameter and hyperparameter learning.

Result: The proposed models demonstrate superior likelihood-parsimony tradeoffs in tasks such as density fitting, clustering, and single-image denoising.

Conclusion: The new GMM family extends existing low-rank models, offering greater flexibility and practicality for unsupervised learning tasks in high dimensions.

Abstract: Gaussian mixture models (GMMs) are ubiquitous in statistical learning,
particularly for unsupervised problems. While full GMMs suffer from the
overparameterization of their covariance matrices in high-dimensional spaces,
spherical GMMs (with isotropic covariance matrices) certainly lack flexibility
to fit certain anisotropic distributions. Connecting these two extremes, we
introduce a new family of parsimonious GMMs with piecewise-constant covariance
eigenvalue profiles. These extend several low-rank models like the celebrated
mixtures of probabilistic principal component analyzers (MPPCA), by enabling
any possible sequence of eigenvalue multiplicities. If the latter are
prespecified, then we can naturally derive an expectation-maximization (EM)
algorithm to learn the mixture parameters. Otherwise, to address the
notoriously-challenging issue of jointly learning the mixture parameters and
hyperparameters, we propose a componentwise penalized EM algorithm, whose
monotonicity is proven. We show the superior likelihood-parsimony tradeoffs
achieved by our models on a variety of unsupervised experiments: density
fitting, clustering and single-image denoising.

</details>


### [282] [When Less Is More: Binary Feedback Can Outperform Ordinal Comparisons in Ranking Recovery](https://arxiv.org/abs/2507.01613)
*Shirong Xu,Jingnan Zhang,Junhui Wang*

Main category: stat.ML

TL;DR: This paper proposes a framework where binarizing ordinal paired comparison data improves ranking recovery accuracy and demonstrates this with theoretical proofs and simulations.


<details>
  <summary>Details</summary>
Motivation: To challenge the conventional belief that ordinal paired comparison data inherently provides richer information than binary data.

Method: A general parametric framework was developed with a generalized additive structure, including a link function for preference differences and a pattern function dictating ordinal response levels.

Result: Binarizing ordinal data leads to faster exponential convergence rates in ranking error compared to ordinal data, supported by SNR analysis, simulations, and MovieLens dataset application.

Conclusion: Binary comparison data can be more effective than ordinal data in ranking recovery tasks, particularly when coupled with the optimal pattern function.

Abstract: Paired comparison data, where users evaluate items in pairs, play a central
role in ranking and preference learning tasks. While ordinal comparison data
intuitively offer richer information than binary comparisons, this paper
challenges that conventional wisdom. We propose a general parametric framework
for modeling ordinal paired comparisons without ties. The model adopts a
generalized additive structure, featuring a link function that quantifies the
preference difference between two items and a pattern function that governs the
distribution over ordinal response levels. This framework encompasses classical
binary comparison models as special cases, by treating binary responses as
binarized versions of ordinal data. Within this framework, we show that
binarizing ordinal data can significantly improve the accuracy of ranking
recovery. Specifically, we prove that under the counting algorithm, the ranking
error associated with binary comparisons exhibits a faster exponential
convergence rate than that of ordinal data. Furthermore, we characterize a
substantial performance gap between binary and ordinal data in terms of a
signal-to-noise ratio (SNR) determined by the pattern function. We identify the
pattern function that minimizes the SNR and maximizes the benefit of
binarization. Extensive simulations and a real application on the MovieLens
dataset further corroborate our theoretical findings.

</details>


### [283] [A generative modeling / Physics-Informed Neural Network approach to random differential equations](https://arxiv.org/abs/2507.01687)
*Georgios Arampatzis,Stylianos Katsarakis,Charalambos Makridakis*

Main category: stat.ML

TL;DR: The paper enhances Physics-Informed Neural Networks (PINNs) by integrating probabilistic frameworks to model uncertainty in complex systems.


<details>
  <summary>Details</summary>
Motivation: To address the need for effective modeling and control of uncertainty in computational science, particularly for random differential and partial differential equations.

Method: By combining generative modeling techniques with PINNs to systematically control uncertainty while preserving model accuracy.

Result: The approach effectively models uncertainty in forward problems and demonstrates utility in random differential and partial differential equations.

Conclusion: The integration of probabilistic frameworks with PINNs provides a systematic method for uncertainty quantification while maintaining predictive accuracy.

Abstract: The integration of Scientific Machine Learning (SciML) techniques with
uncertainty quantification (UQ) represents a rapidly evolving frontier in
computational science. This work advances Physics-Informed Neural Networks
(PINNs) by incorporating probabilistic frameworks to effectively model
uncertainty in complex systems. Our approach enhances the representation of
uncertainty in forward problems by combining generative modeling techniques
with PINNs. This integration enables in a systematic fashion uncertainty
control while maintaining the predictive accuracy of the model. We demonstrate
the utility of this method through applications to random differential
equations and random partial differential equations (PDEs).

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [284] [Dynamic Similarity Graph Construction with Kernel Density Estimation](https://arxiv.org/abs/2507.01696)
*Steinar Laenen,Peter Macgregor,He Sun*

Main category: cs.DS

TL;DR: This paper introduces a dynamic data structure for kernel density estimation that effectively supports updates and queries. It also applies this to dynamically maintain a sparse similarity graph and perform spectral clustering.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency in updating estimates for kernel density estimation as data points are added over time, especially in dynamic datasets.

Method: The authors developed a dynamic data structure for maintaining kernel density estimation estimates for query points and expanded it to maintain a sparse similarity graph. This was further applied to create a dynamic spectral clustering algorithm.

Result: The proposed methods were evaluated using both synthetic and real-world datasets, demonstrating their computational efficiency and effectiveness.

Conclusion: The study successfully introduced efficient dynamic algorithms for kernel density estimation and spectral clustering, showcasing their practicality for dynamic datasets.

Abstract: In the kernel density estimation (KDE) problem, we are given a set $X$ of
data points in $\mathbb{R}^d$, a kernel function $k: \mathbb{R}^d \times
\mathbb{R}^d \rightarrow \mathbb{R}$, and a query point $\mathbf{q} \in
\mathbb{R}^d$, and the objective is to quickly output an estimate of
$\sum_{\mathbf{x} \in X} k(\mathbf{q}, \mathbf{x})$. In this paper, we consider
$\textsf{KDE}$ in the dynamic setting, and introduce a data structure that
efficiently maintains the estimates for a set of query points as data points
are added to $X$ over time. Based on this, we design a dynamic data structure
that maintains a sparse approximation of the fully connected similarity graph
on $X$, and develop a fast dynamic spectral clustering algorithm. We further
evaluate the effectiveness of our algorithms on both synthetic and real-world
datasets.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [285] [Agentic AI in Product Management: A Co-Evolutionary Model](https://arxiv.org/abs/2507.01069)
*Nishant A. Parikh*

Main category: cs.CE

TL;DR: The study proposes a framework for integrating agentic AI into product management, highlighting AI's role and its impact on PMs' functions.


<details>
  <summary>Details</summary>
Motivation: To address gaps in traditional product management frameworks and guide the integration of agentic AI in product management, supporting its evolving role.

Method: The research uses a co-evolutionary and systems theory framework supported by an integrative review of over 70 sources, including case studies.

Result: The study identifies the evolving role of PMs, emphasizing new skills in AI literacy, governance, and systems thinking as essential for supervising and aligning AI's integration.

Conclusion: The proposed framework establishes a theoretical foundation for integrating agentic AI responsibly and effectively into software organizations while highlighting the need for skill adaptations in PM roles.

Abstract: This study explores agentic AI's transformative role in product management,
proposing a conceptual co-evolutionary framework to guide its integration
across the product lifecycle. Agentic AI, characterized by autonomy,
goal-driven behavior, and multi-agent collaboration, redefines product managers
(PMs) as orchestrators of socio-technical ecosystems. Using systems theory,
co-evolutionary theory, and human-AI interaction theory, the framework maps
agentic AI capabilities in discovery, scoping, business case development,
development, testing, and launch. An integrative review of 70+ sources,
including case studies from leading tech firms, highlights PMs' evolving roles
in AI orchestration, supervision, and strategic alignment. Findings emphasize
mutual adaptation between PMs and AI, requiring skills in AI literacy,
governance, and systems thinking. Addressing gaps in traditional frameworks,
this study provides a foundation for future research and practical
implementation to ensure responsible, effective agentic AI integration in
software organizations.

</details>


### [286] [HPC-AI Coupling Methodology for Scientific Applications](https://arxiv.org/abs/2507.01025)
*Yutong Lu,Dan Huang,Pin Chen*

Main category: cs.CE

TL;DR: This paper introduces three patterns for coupling HPC and AIâsurrogate, directive, and coordinateâand demonstrates their application with case studies in materials science.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address computational challenges and improve scientific discovery by effectively integrating HPC and AI technologies.

Method: The authors propose three coupling patterns (surrogate, directive, coordinate) which are implemented and tested through case studies in materials science.

Result: The study demonstrates effectiveness, highlights technical challenges, performance improvements, and offers implementation details for the proposed patterns.

Conclusion: The proposed patterns provide a versatile framework for coupling HPC and AI, applicable across various scientific domains, and aim to guide future advancements in HPC-AI ensembles.

Abstract: Artificial intelligence (AI) technologies have fundamentally transformed
numerical-based high-performance computing (HPC) applications with data-driven
approaches and endeavored to address existing challenges, e.g. high
computational intensity, in various scientific domains. In this study, we
explore the scenarios of coupling HPC and AI (HPC-AI) in the context of
emerging scientific applications, presenting a novel methodology that
incorporates three patterns of coupling: surrogate, directive, and coordinate.
Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite,
and typical HPC-AI ensembles. Through case studies in materials science, we
demonstrate the application and effectiveness of these patterns. The study
highlights technical challenges, performance improvements, and implementation
details, providing insight into promising perspectives of HPC-AI coupling. The
proposed coupling patterns are applicable not only to materials science but
also to other scientific domains, offering valuable guidance for future HPC-AI
ensembles in scientific discovery.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [287] [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/abs/2507.01059)
*Xiangbo Gao,Keshu Wu,Hao Zhang,Kexin Tian,Yang Zhou,Zhengzhong Tu*

Main category: cs.MA

TL;DR: The paper advocates using natural language communication for multi-agent collaborative driving to overcome the limitations of current methods in efficiency, completeness, and interoperability.


<details>
  <summary>Details</summary>
Motivation: Existing approaches in multi-agent collaborative driving often fail in terms of bandwidth efficiency, information completeness, and interoperability, while neglecting decision-level fusion.

Method: Proposes using natural language communication for sharing intentions, rationales, and decisions among agents, instead of just perception-oriented data sharing.

Result: Natural language proved effective for balancing semantic density and bandwidth in coordination, and enables adaptability and enhanced interoperability.

Conclusion: Utilizing natural language transforms collaborative driving into proactive coordination, leading to improved safety, efficiency, and transparency in transportation systems.

Abstract: Multi-agent collaborative driving promises improvements in traffic safety and
efficiency through collective perception and decision making. However, existing
communication media -- including raw sensor data, neural network features, and
perception results -- suffer limitations in bandwidth efficiency, information
completeness, and agent interoperability. Moreover, traditional approaches have
largely ignored decision-level fusion, neglecting critical dimensions of
collaborative driving. In this paper we argue that addressing these challenges
requires a transition from purely perception-oriented data exchanges to
explicit intent and reasoning communication using natural language. Natural
language balances semantic density and communication bandwidth, adapts flexibly
to real-time conditions, and bridges heterogeneous agent platforms. By enabling
the direct communication of intentions, rationales, and decisions, it
transforms collaborative driving from reactive perception-data sharing into
proactive coordination, advancing safety, efficiency, and transparency in
intelligent transportation systems.

</details>


### [288] [RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms](https://arxiv.org/abs/2507.01378)
*Ziyao Wang,Rongpeng Li,Sizhao Li,Yuming Xiang,Haiping Wang,Zhifeng Zhao,Honggang Zhang*

Main category: cs.MA

TL;DR: The paper introduces RALLY, an LLM-driven navigation algorithm for UAV swarms, combining strengths of LLMs and MARL while overcoming limitations of both.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for UAV swarm control face challenges with semantic communication and adaptability, leading to issues in generalization and task scalability. While LLM-based frameworks bring semantic reasoning capabilities, they fail in dynamic learning and effective exploration.

Method: The RALLY approach integrates a semantic decision-making framework with dynamic role heterogeneity and a Role-value Mixing Network (RMIX) strategy for combining offline LLM priors and online MARL policies.

Result: RALLY achieves superior performance over traditional models in tests, excelling in task coverage, convergence speed, and scalability, as demonstrated in the Multi-Agent Particle Environment and Software-In-The-Loop platform.

Conclusion: RALLY offers a strong solution for UAV swarm navigation, balancing the strengths of LLM and MARL to truly advance collaborative decision-making and adaptability in multi-UAV systems.

Abstract: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as
a critical research focus, and it typically requires the swarm to navigate
effectively while avoiding obstacles and achieving continuous coverage over
multiple mission targets. Although traditional Multi-Agent Reinforcement
Learning (MARL) approaches offer dynamic adaptability, they are hindered by the
semantic gap in numerical communication and the rigidity of homogeneous role
structures, resulting in poor generalization and limited task scalability.
Recent advances in Large Language Model (LLM)-based control frameworks
demonstrate strong semantic reasoning capabilities by leveraging extensive
prior knowledge. However, due to the lack of online learning and over-reliance
on static priors, these works often struggle with effective exploration,
leading to reduced individual potential and overall system performance. To
address these limitations, we propose a Role-Adaptive LLM-Driven Yoked
navigation algorithm RALLY. Specifically, we first develop an LLM-driven
semantic decision framework that uses structured natural language for efficient
semantic communication and collaborative reasoning. Afterward, we introduce a
dynamic role-heterogeneity mechanism for adaptive role switching and
personalized decision-making. Furthermore, we propose a Role-value Mixing
Network (RMIX)-based assignment strategy that integrates LLM offline priors
with MARL online policies to enable semi-offline training of role selection
strategies. Experiments in the Multi-Agent Particle Environment (MPE)
environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY
outperforms conventional approaches in terms of task coverage, convergence
speed, and generalization, highlighting its strong potential for collaborative
navigation in agentic multi-UAV systems.

</details>


### [289] [Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture](https://arxiv.org/abs/2507.01701)
*Bochen Han,Songmao Zhang*

Main category: cs.MA

TL;DR: This paper integrates the blackboard architecture into LLM-based multi-agent systems for dynamic and collaborative problem-solving, achieving competitive results with reduced token usage.


<details>
  <summary>Details</summary>
Motivation: Dynamic and complex problem-solving often lacks well-defined workflows, necessitating innovative approaches for effective collaboration in multi-agent systems.

Method: The blackboard architecture is incorporated into multi-agent systems, enabling agents to share information, select actions based on a central hub, and iteratively reach consensus during problem-solving.

Result: Experiments on commonsense knowledge, reasoning, and mathematical tasks showcased competitive performance with static and dynamic MASs, while reducing token usage.

Conclusion: This approach demonstrates the potential to enable dynamic and complex problem-solving, offering improved average performance and efficiency over existing MAS approaches.

Abstract: In this paper, we propose to incorporate the blackboard architecture into LLM
multi-agent systems (MASs) so that (1) agents with various roles can share all
the information and others' messages during the whole problem-solving process,
(2) agents that will take actions are selected based on the current content of
the blackboard, and (3) the selection and execution round is repeated until a
consensus is reached on the blackboard. We develop the first implementation of
this proposal and conduct experiments on commonsense knowledge, reasoning and
mathematical datasets. The results show that our system can be competitive with
the SOTA static and dynamic MASs by achieving the best average performance, and
at the same time manage to spend less tokens. Our proposal has the potential to
enable complex and dynamic problem-solving where well-defined structures or
workflows are unavailable.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [290] [Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems](https://arxiv.org/abs/2507.01429)
*Benjamin Chen Ming Choong,Tao Luo,Cheng Liu,Bingsheng He,Wei Zhang,Joey Tianyi Zhou*

Main category: cs.ET

TL;DR: This paper introduces an in-memory CNN accelerator using racetrack memory to improve energy and performance efficiency while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenges posed by the large data requirements of deep neural networks on low-resource, embedded systems, by leveraging racetrack memory for energy-efficient in-memory computing.

Method: The authors design fundamental arithmetic circuits tailored for multiply-and-accumulate operations and perform a co-design of racetrack memory-based systems with CNN model architectures to optimize efficiency, energy use, and memory area.

Result: The proposed designs achieve significant improvements in energy efficiency, system performance, and compact memory bank usage in racetrack memory-based embedded systems.

Conclusion: The work demonstrates the potential of racetrack memory for CNN inference, showing how co-design strategies can overcome the constraints of area and energy to develop efficient embedded AI systems.

Abstract: Deep neural networks generate and process large volumes of data, posing
challenges for low-resource embedded systems. In-memory computing has been
demonstrated as an efficient computing infrastructure and shows promise for
embedded AI applications. Among newly-researched memory technologies, racetrack
memory is a non-volatile technology that allows high data density fabrication,
making it a good fit for in-memory computing. However, integrating in-memory
arithmetic circuits with memory cells affects both the memory density and power
efficiency. It remains challenging to build efficient in-memory arithmetic
circuits on racetrack memory within area and energy constraints. To this end,
we present an efficient in-memory convolutional neural network (CNN)
accelerator optimized for use with racetrack memory. We design a series of
fundamental arithmetic circuits as in-memory computing cells suited for
multiply-and-accumulate operations. Moreover, we explore the design space of
racetrack memory based systems and CNN model architectures, employing co-design
to improve the efficiency and performance of performing CNN inference in
racetrack memory while maintaining model accuracy. Our designed circuits and
model-system co-optimization strategies achieve a small memory bank area with
significant improvements in energy and performance for racetrack memory based
embedded systems.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [291] [User-guided Generative Source Separation](https://arxiv.org/abs/2507.01339)
*Yutong Wen,Minje Kim,Paris Smaragdis*

Main category: cs.SD

TL;DR: The paper introduces GuideSep, a flexible music source separation (MSS) model that uses a diffusion-based approach to separate audio sources beyond traditional instrument classifications by leveraging waveform mimicry and mel-spectrogram masks.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the lack of flexibility in traditional four-stem instrument separation methods, which hampers real-world application versatility.

Method: The proposed method, GuideSep, uses a diffusion-based generative approach, conditioning on waveform mimicry and mel-spectrogram masks for flexible and instrument-agnostic audio separation.

Result: Objective and subjective evaluations of GuideSep show high-quality audio separation results with broader applicability compared to conventional methods.

Conclusion: GuideSep demonstrates the potential for flexible, high-quality music source separation, accommodating user inputs and expanding use-case versatility in real-world scenarios.

Abstract: Music source separation (MSS) aims to extract individual instrument sources
from their mixture. While most existing methods focus on the widely adopted
four-stem separation setup (vocals, bass, drums, and other instruments), this
approach lacks the flexibility needed for real-world applications. To address
this, we propose GuideSep, a diffusion-based MSS model capable of
instrument-agnostic separation beyond the four-stem setup. GuideSep is
conditioned on multiple inputs: a waveform mimicry condition, which can be
easily provided by humming or playing the target melody, and mel-spectrogram
domain masks, which offer additional guidance for separation. Unlike prior
approaches that relied on fixed class labels or sound queries, our conditioning
scheme, coupled with the generative approach, provides greater flexibility and
applicability. Additionally, we design a mask-prediction baseline using the
same model architecture to systematically compare predictive and generative
approaches. Our objective and subjective evaluations demonstrate that GuideSep
achieves high-quality separation while enabling more versatile instrument
extraction, highlighting the potential of user participation in the
diffusion-based generative process for MSS. Our code and demo page are
available at https://yutongwen.github.io/GuideSep/

</details>


### [292] [Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware](https://arxiv.org/abs/2507.01563)
*Marco Giordano,Stefano Giacomelli,Claudia Rinaldi,Fabio Graziosi*

Main category: cs.SD

TL;DR: This paper introduces a real-time emergency vehicle siren detection system optimized for embedded hardware using fine-tuned convolutional neural networks (E2PANNs). Performance is enhanced through structured datasets and efficient deployment on low-cost edge devices.


<details>
  <summary>Details</summary>
Motivation: The motivation is to create a reliable, low-latency acoustic monitoring system for detecting emergency vehicle sirens, addressing challenges like annotation reliability and robustness in urban noise environments.

Method: The system relies on E2PANNs, curated datasets, and optimizations like probability smoothing and decision-state machines. It is deployed on a Raspberry Pi with a high-fidelity DAC+microphone board and supports real-time monitoring via WebSockets.

Result: Tests reveal low-latency and robust detection performance in realistic acoustic conditions, demonstrating the practicality of the system.

Conclusion: The proposed system proves feasible for distributed acoustic monitoring in smart cities using affordable embedded devices, enabling real-time emergency vehicle tracking.

Abstract: We present a full-stack emergency vehicle (EV) siren detection system
designed for real-time deployment on embedded hardware. The proposed approach
is based on E2PANNs, a fine-tuned convolutional neural network derived from
EPANNs, and optimized for binary sound event detection under urban acoustic
conditions. A key contribution is the creation of curated and semantically
structured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -
developed using a custom AudioSet-Tools framework to overcome the low
reliability of standard AudioSet annotations. The system is deployed on a
Raspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing
a multithreaded inference engine with adaptive frame sizing, probability
smoothing, and a decision-state machine to control false positive activations.
A remote WebSocket interface provides real-time monitoring and facilitates live
demonstration capabilities. Performance is evaluated using both framewise and
event-based metrics across multiple configurations. Results show the system
achieves low-latency detection with improved robustness under realistic audio
conditions. This work demonstrates the feasibility of deploying IoS-compatible
SED solutions that can form distributed acoustic monitoring networks, enabling
collaborative emergency vehicle tracking across smart city infrastructures
through WebSocket connectivity on low-cost edge devices.

</details>


### [293] [Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder](https://arxiv.org/abs/2507.01582)
*Jing Luo,Xinyu Yang,Jie Wei*

Main category: cs.SD

TL;DR: This paper focuses on generating classical piano performances by introducing the Expressive Compound Word representation and the XMVAE model, which jointly emulates a composer and pianist.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of emulating both the compositional and performative aspects in generating expressive classical piano performances.

Method: The authors proposed XMVAE, a dual-branch model where one branch handles score generation using VQ-VAE, and the other handles expressive nuances using vanilla VAE, supported by multiscale encoding and an orthogonal Transformer decoder.

Result: Quantitative and qualitative evaluations show XMVAE produces superior performances compared to state-of-the-art models, with observable benefits from pretraining the Composer branch on additional datasets.

Conclusion: XMVAE effectively combines compositional creativity and performance expressiveness, offering enhanced musical quality in generated classical piano performances.

Abstract: The creativity of classical music arises not only from composers who craft
the musical sheets but also from performers who interpret the static notations
with expressive nuances. This paper addresses the challenge of generating
classical piano performances from scratch, aiming to emulate the dual roles of
composer and pianist in the creative process. We introduce the Expressive
Compound Word (ECP) representation, which effectively captures both the
metrical structure and expressive nuances of classical performances. Building
on this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a
model featuring two branches: a Vector Quantized Variational AutoEncoder
(VQ-VAE) branch that generates score-related content, representing the
Composer, and a vanilla VAE branch that produces expressive details, fulfilling
the role of Pianist. These branches are jointly trained with similar Seq2Seq
architectures, leveraging a multiscale encoder to capture beat-level contextual
information and an orthogonal Transformer decoder for efficient compound tokens
decoding. Both objective and subjective evaluations demonstrate that XMVAE
generates classical performances with superior musical quality compared to
state-of-the-art models. Furthermore, pretraining the Composer branch on extra
musical score datasets contribute to a significant performance gain.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [294] [Hello Afrika: Speech Commands in Kinyarwanda](https://arxiv.org/abs/2507.01024)
*George Igwegbe,Martins Awojide,Mboh Bless,Nirel Kadzo*

Main category: eess.AS

TL;DR: This paper discusses the creation of speech command models tailored for African languages, with a focus on Kinyarwanda, using a custom speech corpus and testing it across devices.


<details>
  <summary>Details</summary>
Motivation: To address the scarcity of speech command models for African languages and support accessibility, especially for individuals with disabilities.

Method: Developed a speech command model for Kinyarwanda based on a custom corpus containing directives, numbers, and wake words, followed by deployment on various devices.

Result: The model showed measurable performance on PC, mobile phones, and edge devices, evaluated using appropriate metrics.

Conclusion: The project succeeded in creating and assessing a speech command model for Kinyarwanda, demonstrating practical applications for African language AI systems.

Abstract: Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a
language which are essential for non-contact control of and activation of
larger AI systems in devices used in everyday life especially for persons with
disabilities. Currently, there is a dearth of speech command models for African
languages. The Hello Afrika project aims to address this issue and its first
iteration is focused on the Kinyarwanda language since the country has shown
interest in developing speech recognition technologies culminating in one of
the largest datasets on Mozilla Common Voice. The model was built off a custom
speech command corpus made up of general directives, numbers, and a wake word.
The final model was deployed on multiple devices (PC, Mobile Phone and Edge
Devices) and the performance was assessed using suitable metrics.

</details>


### [295] [Scalable Offline ASR for Command-Style Dictation in Courtrooms](https://arxiv.org/abs/2507.01021)
*Kumarmanas Nethil,Vaibhav Mishra,Kriti Anandan,Kavya Manohar*

Main category: eess.AS

TL;DR: This paper introduces an open-source framework for command-style dictation, leveraging Voice Activity Detection and Whisper models for efficient parallel audio transcription with reduced latency.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of resource-heavy online systems and high-latency batch processing for dictation tasks.

Method: Utilizing Voice Activity Detection to segment audio, followed by parallel transcription using Whisper models and compatibility with various ASR architectures.

Result: Deployment in 15% of India's courtrooms and evaluations showing reduced latency with increasing concurrency, outperforming sequential batch processing.

Conclusion: The proposed framework is practical, efficient, and versatile, presenting significant advancements in dictation systems, highlighted by real-world deployment and a planned live demonstration.

Abstract: We propose an open-source framework for Command-style dictation that
addresses the gap between resource-intensive Online systems and high-latency
Batch processing. Our approach uses Voice Activity Detection (VAD) to segment
audio and transcribes these segments in parallel using Whisper models, enabling
efficient multiplexing across audios. Unlike proprietary systems like
SuperWhisper, this framework is also compatible with most ASR architectures,
including widely used CTC-based models. Our multiplexing technique maximizes
compute utilization in real-world settings, as demonstrated by its deployment
in around 15% of India's courtrooms. Evaluations on live data show consistent
latency reduction as user concurrency increases, compared to sequential batch
processing. The live demonstration will showcase our open-sourced
implementation and allow attendees to interact with it in real-time.

</details>


### [296] [Workflow-Based Evaluation of Music Generation Systems](https://arxiv.org/abs/2507.01022)
*Shayan Dadman,Bernt Arild Bremdal,Andreas Bergsland*

Main category: eess.AS

TL;DR: This study evaluates eight open-source music generation systems within music production workflows, addressing their capabilities, limitations, and potential for collaborative use with humans.


<details>
  <summary>Details</summary>
Motivation: The study aims to assess music generation systems' practical and creative value in contemporary music production and identify opportunities for improving workflow integration and collaboration.

Method: It uses a preliminary single-evaluator methodology, combining qualitative hypothesis formation and quantitative metrics evaluation across diverse symbolic and audio-based systems.

Result: Findings indicate that music generation systems enhance rather than replace human expertise, showing limitations in thematic and structural coherence but valuable as complementary tools.

Conclusion: The research provides a structured evaluation framework, identifies areas for refinement, and offers insights for advancing these systems as collaborative tools in music production workflows.

Abstract: This study presents an exploratory evaluation of Music Generation Systems
(MGS) within contemporary music production workflows by examining eight
open-source systems. The evaluation framework combines technical insights with
practical experimentation through criteria specifically designed to investigate
the practical and creative affordances of the systems within the iterative,
non-linear nature of music production. Employing a single-evaluator methodology
as a preliminary phase, this research adopts a mixed approach utilizing
qualitative methods to form hypotheses subsequently assessed through
quantitative metrics. The selected systems represent architectural diversity
across both symbolic and audio-based music generation approaches, spanning
composition, arrangement, and sound design tasks. The investigation addresses
limitations of current MGS in music production, challenges and opportunities
for workflow integration, and development potential as collaborative tools
while maintaining artistic authenticity. Findings reveal these systems function
primarily as complementary tools enhancing rather than replacing human
expertise. They exhibit limitations in maintaining thematic and structural
coherence that emphasize the indispensable role of human creativity in tasks
demanding emotional depth and complex decision-making. This study contributes a
structured evaluation framework that considers the iterative nature of music
creation. It identifies methodological refinements necessary for subsequent
comprehensive evaluations and determines viable areas for AI integration as
collaborative tools in creative workflows. The research provides
empirically-grounded insights to guide future development in the field.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [297] [End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning](https://arxiv.org/abs/2507.01918)
*Christian Bongiorno,Efstratios Manolakis,Rosario Nunzio Mantegna*

Main category: q-fin.PM

TL;DR: The paper introduces a rotation-invariant neural network method to optimize and generalize minimum-variance portfolio strategies using real financial data, showcasing robust out-of-sample performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of constructing robust, interpretable, and scalable minimum-variance portfolios in finance using advanced machine learning techniques.

Method: The paper develops a rotation-invariant neural network architecture that simultaneously learns lag-transformation of returns and regularization of covariance matrix properties, with the loss function based on future realized minimum portfolio variance, optimized end-to-end using actual financial data.

Result: The proposed method demonstrates systematic outperformance over analytical methods in terms of realized volatility, drawdowns, and Sharpe ratios. It generalizes effectively across asset dimensions and maintains efficacy under long-only constraints without losing advantages.

Conclusion: The model offers a robust, scalable, and interpretable approach to portfolio optimization, effectively handling real-world implementation challenges, and performing well even during market stress.

Abstract: We develop a rotation-invariant neural network that provides the global
minimum-variance portfolio by jointly learning how to lag-transform historical
returns and how to regularise both the eigenvalues and the marginal
volatilities of large equity covariance matrices. This explicit mathematical
mapping offers clear interpretability of each module's role, so the model
cannot be regarded as a pure black-box. The architecture mirrors the analytical
form of the global minimum-variance solution yet remains agnostic to dimension,
so a single model can be calibrated on panels of a few hundred stocks and
applied, without retraining, to one thousand US equities-a cross-sectional jump
that demonstrates robust out-of-sample generalisation. The loss function is the
future realized minimum portfolio variance and is optimized end-to-end on real
daily returns. In out-of-sample tests from January 2000 to December 2024 the
estimator delivers systematically lower realised volatility, smaller maximum
drawdowns, and higher Sharpe ratios than the best analytical competitors,
including state-of-the-art non-linear shrinkage. Furthermore, although the
model is trained end-to-end to produce an unconstrained (long-short)
minimum-variance portfolio, we show that its learned covariance representation
can be used in general optimizers under long-only constraints with virtually no
loss in its performance advantage over competing estimators. These gains
persist when the strategy is executed under a highly realistic implementation
framework that models market orders at the auctions, empirical slippage,
exchange fees, and financing charges for leverage, and they remain stable
during episodes of acute market stress.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [298] [SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars](https://arxiv.org/abs/2507.01939)
*Xiaosheng Zhao,Yang Huang,Guirong Xue,Xiao Kong,Jifeng Liu,Xiaoyu Tang,Timothy C. Beers,Yuan-Sen Ting,A-Li Luo*

Main category: astro-ph.IM

TL;DR: SpecCLIP utilizes foundational LLM methodologies adapted for stellar spectral analysis by training models on large spectral datasets and using contrastive alignment for cross-spectrum applications.


<details>
  <summary>Details</summary>
Motivation: Leverage the success of large language models (LLMs) in understanding structured data for advancing analysis of stellar spectra.

Method: Adapts the CLIP framework by pretraining on large spectral datasets, aligning spectra from multiple sources using contrastive methods, complemented by auxiliary decoders for information retention and type translation.

Result: SpecCLIP accurately estimates stellar parameters and chemical abundances, boosts precision against benchmark surveys, and supports anomaly detection and cross-spectrum predictions.

Conclusion: Contrastively trained foundation models with spectrum-aware enhancements improve precision stellar spectroscopy and expand toolkits for astrophysical research.

Abstract: In recent years, large language models (LLMs) have transformed natural
language understanding through vast datasets and large-scale parameterization.
Inspired by this success, we present SpecCLIP, a foundation model framework
that extends LLM-inspired methodologies to stellar spectral analysis. Stellar
spectra, akin to structured language, encode rich physical and chemical
information about stars. By training foundation models on large-scale spectral
datasets, our goal is to learn robust and informative embeddings that support
diverse downstream applications. As a proof of concept, SpecCLIP involves
pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed
by contrastive alignment using the CLIP (Contrastive Language-Image
Pre-training) framework, adapted to associate spectra from different
instruments. This alignment is complemented by auxiliary decoders that preserve
spectrum-specific information and enable translation (prediction) between
spectral types, with the former achieved by maximizing mutual information
between embeddings and input spectra. The result is a cross-spectrum framework
enabling intrinsic calibration and flexible applications across instruments. We
demonstrate that fine-tuning these models on moderate-sized labeled datasets
improves adaptability to tasks such as stellar-parameter estimation and
chemical-abundance determination. SpecCLIP also enhances the accuracy and
precision of parameter estimates benchmarked against external survey data.
Additionally, its similarity search and cross-spectrum prediction capabilities
offer potential for anomaly detection. Our results suggest that contrastively
trained foundation models enriched with spectrum-aware decoders can advance
precision stellar spectroscopy.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [299] [Rational Censorship Attack: Breaking Blockchain with a Blackboard](https://arxiv.org/abs/2507.01453)
*Michelle Yeo,Haoqian Zhang*

Main category: cs.GT

TL;DR: The paper identifies a rational censorship attack on blockchain resilience using game theory, where a colluding group monopolizes rewards by censoring other nodes.


<details>
  <summary>Details</summary>
Motivation: Blockchain security is frequently analyzed from an economic and game-theoretic perspective. This paper seeks to understand how rational behaviors compromise censorship resilience.

Method: The authors model the attack as a game-theoretic framework, wherein nodes collaborate through a public blackboard and rationally report their voting power.

Result: The strategy of joining the censorship attack and truthfully reporting voting power is proven as subgame perfect equilibrium.

Conclusion: The attack reveals vulnerabilities in blockchain design, demanding countermeasures to ensure fairness and resilience against rational collusion.

Abstract: Censorship resilience is a fundamental assumption underlying the security of
blockchain protocols. Additionally, the analysis of blockchain security from an
economic and game theoretic perspective has been growing in popularity in
recent years. In this work, we present a surprising rational censorship attack
on blockchain censorship resilience when we adopt the analysis of blockchain
security from a game theoretic lens and assume all users are rational. In our
attack, a colluding group with sufficient voting power censors the remainder
nodes such that the group alone can gain all the rewards from maintaining the
blockchain. We show that if nodes are rational, coordinating this attack just
requires a public read and write blackboard and we formally model the attack
using a game theoretic framework. Furthermore, we note that to ensure the
success of the attack, nodes need to know the total true voting power held by
the colluding group. We prove that the strategy to join the rational censorship
attack and also for nodes to honestly declare their power is a subgame perfect
equilibrium in the corresponding extensive form game induced by our attack.
Finally, we discuss the implications of the attack on blockchain users and
protocol designers as well as some potential countermeasures.

</details>


### [300] [Evaluating LLM Agent Collusion in Double Auctions](https://arxiv.org/abs/2507.01413)
*Kushal Agrawal,Verona Teo,Juan J. Vazquez,Sudarsh Kunnavakkam,Vishak Srikanth,Andy Liu*

Main category: cs.GT

TL;DR: This paper examines whether large language models (LLMs) acting as market agents can engage in unethical collusion during socioeconomic interactions.


<details>
  <summary>Details</summary>
Motivation: The motivation is to assess the potential for undesirable, secretive cooperation (collusion) among LLMs acting autonomously in economic settings, given their expanding use.

Method: Simulated continuous double auction markets were created, where LLM agents performed as sellers under varying conditions like communication, model type, and environmental pressures.

Result: Findings show that communication between sellers increases collusion, different models have varying collusive tendencies, and factors like oversight and authority influence this behavior.

Conclusion: LLM market agents must be carefully assessed for economic and ethical concerns, as their deployment may inadvertently lead to harmful collusive practices.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities as
autonomous agents with rapidly expanding applications in various domains. As
these agents increasingly engage in socioeconomic interactions, identifying
their potential for undesirable behavior becomes essential. In this work, we
examine scenarios where they can choose to collude, defined as secretive
cooperation that harms another party. To systematically study this, we
investigate the behavior of LLM agents acting as sellers in simulated
continuous double auction markets. Through a series of controlled experiments,
we analyze how parameters such as the ability to communicate, choice of model,
and presence of environmental pressures affect the stability and emergence of
seller collusion. We find that direct seller communication increases collusive
tendencies, the propensity to collude varies across models, and environmental
pressures, such as oversight and urgency from authority figures, influence
collusive behavior. Our findings highlight important economic and ethical
considerations for the deployment of LLM-based market agents.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [301] [Can AI be Consentful?](https://arxiv.org/abs/2507.01051)
*Giada Pistilli,Bruna Trevelin*

Main category: cs.CY

TL;DR: The paper examines the inadequacy of traditional legal and ethical frameworks of consent in managing AI-generated content derived from personal data, proposing a "consent gap" and the need for evolved approaches.


<details>
  <summary>Details</summary>
Motivation: To address the insufficiency of conventional consent frameworks in the context of AI-generated content and their impact on privacy and identity rights.

Method: The paper conducts legal and ethical analyses, identifying challenges such as the scope problem, temporality problem, and autonomy trap.

Result: The study identifies a "consent gap" due to individuals' inability to foresee or control how AI systems generate and distribute content based on their data.

Conclusion: Current legal frameworks fail to address the challenges posed by AI systems, necessitating the evolution of ethical and legal principles to protect individual autonomy and ensure responsible AI practices.

Abstract: The evolution of generative AI systems exposes the challenges of traditional
legal and ethical frameworks built around consent. This chapter examines how
the conventional notion of consent, while fundamental to data protection and
privacy rights, proves insufficient in addressing the implications of
AI-generated content derived from personal data. Through legal and ethical
analysis, we show that while individuals can consent to the initial use of
their data for AI training, they cannot meaningfully consent to the numerous
potential outputs their data might enable or the extent to which the output is
used or distributed. We identify three fundamental challenges: the scope
problem, the temporality problem, and the autonomy trap, which collectively
create what we term a ''consent gap'' in AI systems and their surrounding
ecosystem. We argue that current legal frameworks inadequately address these
emerging challenges, particularly regarding individual autonomy, identity
rights, and social responsibility, especially in cases where AI-generated
content creates new forms of personal representation beyond the scope of the
original consent. By examining how these consent limitations intersect with
broader principles of responsible AI (including fairness, transparency,
accountability, and autonomy) we demonstrate the need to evolve ethical and
legal approaches to consent.

</details>


### [302] [Epitome: Pioneering an Experimental Platform for AI-Social Science Integration](https://arxiv.org/abs/2507.01061)
*Jingjing Qu,Kejia Hu,Jun Zhu,Wenhao Li,Teng Wang,Zhiyun Chen,Yulei Ye,Chaochao Lu,Aimin Zhou,Xiangfeng Wang,James Evan*

Main category: cs.CY

TL;DR: The paper introduces Epitome, an open experimental platform combining AI and social science to study human-AI interaction and societal impacts through robust experiments.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between artificial intelligence and social science by creating a platform that evaluates AI's societal impacts through interdisciplinary research.

Method: Developed Epitomeâa platform combining AI technologies and social science methodologies with modules for systematic experimentation and multilevel human-computer interactions.

Result: Successfully replicated three foundational social science experiments using LLMs, demonstrating the platform's ability to streamline experimental designs and yield robust results.

Conclusion: Epitome facilitates interdisciplinary research into AI's societal influences, enhancing experimental efficiency and quality, and has policy-making and academic implications.

Abstract: The integration of Large Language Models (LLMs) into social science
experiments represents a transformative approach to understanding human-AI
interactions and their societal impacts. We introduce Epitome, the world's
first open experimental platform dedicated to the deep integration of
artificial intelligence and social science. Rooted in theoretical foundations
from management, communication studies, sociology, psychology, and ethics,
Epitome focuses on the interactive impacts of AI on individuals, organizations,
and society during its real-world deployment. It constructs a theoretical
support system through cross-disciplinary experiments. The platform offers a
one-stop comprehensive experimental solution spanning "foundation
models-complex application development-user feedback" through seven core
modules, while embedding the classical "control-comparison-comparative causal
logic" of social science experiments into multilevel human-computer interaction
environments, including dialogues, group chats, and multi-agent virtual
scenarios. With its canvas-style, user-friendly interface, Epitome enables
researchers to easily design and run complex experimental scenarios,
facilitating systematic investigations into the social impacts of AI and
exploration of integrated solutions.To demonstrate its capabilities, we
replicated three seminal social science experiments involving LLMs, showcasing
Epitome's potential to streamline complex experimental designs and produce
robust results, suitable for publishing in the top selective journals. Our
findings highlight the platform's utility in enhancing the efficiency and
quality of human-AI interactions, providing valuable insights into the societal
implications of AI technologies. Epitome thus offers a powerful tool for
advancing interdisciplinary research at the intersection of AI and social
science, with potential applications in policy-making, ...

</details>


### [303] [Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review](https://arxiv.org/abs/2507.01062)
*Seyma Yaman Kayadibi*

Main category: cs.CY

TL;DR: This paper examines the impact of generative artificial intelligence (GenAI) technologies like ChatGPT on higher education by analyzing student perceptions and their effects on learning outcomes using systematic literature review and simulations.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of GenAI technologies has sparked interest in their applications within higher education, particularly concerning student perspectives, usage patterns, and influence on learning outcomes.

Method: The study combines a systematic literature review of 19 empirical articles from 2023-2025 with thematic categorization and simulation-based probabilistic modeling to analyze student perceptions and predict learning outcomes.

Result: The simulations identified a "Success Score" linking student perceptions to learning achievements, highlighting factors like usability and real-world utility as significant predictors of positive outcomes.

Conclusion: Positive perceptions of GenAI, specifically concerning usability and practical value, strongly influence learning achievements, while trust and emotion-based factors are less predictive. This offers insights into optimizing GenAI tools in educational settings.

Abstract: The exponential development of generative artificial intelligence (GenAI)
technologies like ChatGPT has raised increasing curiosity about their use in
higher education, specifically with respect to how students view them, make use
of them, and the implications for learning outcomes. This paper employs a
hybrid methodological approach involving a systematic literature review and
simulation-based modeling to explore student perceptions of GenAI use in the
context of higher education. A total of nineteen empirical articles from 2023
through 2025 were selected from the PRISMA-based search targeting the Scopus
database. Synthesis of emerging patterns from the literature was achieved by
thematic categorization. Six of these had enough quantitative information,
i.e., item-level means and standard deviations, to permit probabilistic
modeling. One dataset, from the resulting subset, was itself selected as a
representative case with which to illustrate inverse-variance weighting by
Monte Carlo simulation, by virtue of its well-designed Likert scale format and
thematic alignment with the use of computing systems by the researcher.
  The simulation provided a composite "Success Score" forecasting the strength
of the relationship between student perceptions and learning achievements.
Findings reveal that attitude factors concerned with usability and real-world
usefulness are significantly better predictors of positive learning achievement
than affective or trust-based factors. Such an interdisciplinary perspective
provides a unique means of linking thematic results with predictive modelling,
resonating with longstanding controversies about the proper use of GenAI tools
within the university.

</details>


### [304] [Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing](https://arxiv.org/abs/2507.01418)
*Inyoung Cheong,Alicia Guo,Mina Lee,Zhehui Liao,Kowe Kadoma,Dongyoung Go,Joseph Chee Chang,Peter Henderson,Mor Naaman,Amy X. Zhang*

Main category: cs.CY

TL;DR: The paper explores the effects of AI disclosure in writing on perceptions of quality, focusing on interactions with the author's race and gender.


<details>
  <summary>Details</summary>
Motivation: To investigate how disclosure of AI assistance in writing impacts the evaluation of quality, and whether this effect varies based on author identity.

Method: A large-scale experiment was conducted with human (n=1,970) and LLM (n=2,520) raters who were asked to evaluate a single human-written article, varying the AI disclosure and author demographics.

Result: AI use disclosure was penalized by both human and LLM raters. Additionally, LLM raters showed bias: they favored women or Black authors without AI disclosure, but this advantage disappeared with disclosure.

Conclusion: The study reveals complexities in how AI disclosure intersects with author identity, and highlights differences in evaluation biases between human and machine raters.

Abstract: As AI integrates in various types of human writing, calls for transparency
around AI assistance are growing. However, if transparency operates on uneven
ground and certain identity groups bear a heavier cost for being honest, then
the burden of openness becomes asymmetrical. This study investigates how AI
disclosure statement affects perceptions of writing quality, and whether these
effects vary by the author's race and gender. Through a large-scale controlled
experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated
a single human-written news article while disclosure statements and author
demographics were systematically varied. This approach reflects how both human
and algorithmic decisions now influence access to opportunities (e.g., hiring,
promotion) and social recognition (e.g., content recommendation algorithms). We
find that both human and LLM raters consistently penalize disclosed AI use.
However, only LLM raters exhibit demographic interaction effects: they favor
articles attributed to women or Black authors when no disclosure is present.
But these advantages disappear when AI assistance is revealed. These findings
illuminate the complex relationships between AI disclosure and author identity,
highlighting disparities between machine and human evaluation patterns.

</details>


### [305] [AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions](https://arxiv.org/abs/2507.01547)
*Ubada El Joulani,Tatiana Kalganova,Stergios-Aristoteles Mitoulis,Sotirios Argyroudis*

Main category: cs.CY

TL;DR: This paper reviews how AI technologies can improve damage assessment in transport infrastructure, particularly bridges, by addressing research gaps such as SAR data integration.


<details>
  <summary>Details</summary>
Motivation: Aging transport infrastructure faces increasing risks from climate change, disasters, and cyber threats, demanding innovative solutions for resilience and functionality.

Method: Conducted a systematic literature review on AI applications for damage assessment in roads, bridges, and other critical facilities, paying particular attention to bridge-specific challenges.

Result: Revealed a lack of research on using AI with SAR data for comprehensive damage assessment of bridges, highlighting this as a critical gap.

Conclusion: AI, particularly integrated with SAR data, holds promise for enhancing the monitoring and assessment of critical transport infrastructures, but more targeted research is needed.

Abstract: Critical infrastructure, such as transport networks, underpins economic
growth by enabling mobility and trade. However, ageing assets, climate change
impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging
from natural disasters to cyber attacks and conflicts pose growing risks to
their resilience and functionality. This review paper explores how emerging
digital technologies, specifically Artificial Intelligence (AI), can enhance
damage assessment and monitoring of transport infrastructure. A systematic
literature review examines existing AI models and datasets for assessing damage
in roads, bridges, and other critical infrastructure impacted by natural
disasters. Special focus is given to the unique challenges and opportunities
associated with bridge damage detection due to their structural complexity and
critical role in connectivity. The integration of SAR (Synthetic Aperture
Radar) data with AI models is also discussed, with the review revealing a
critical research gap: a scarcity of studies applying AI models to SAR data for
comprehensive bridge damage assessment. Therefore, this review aims to identify
the research gaps and provide foundations for AI-driven solutions for assessing
and monitoring critical transport infrastructures.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [306] [A Dynamical Cartography of the Epistemic Diffusion of Artificial Intelligence in Neuroscience](https://arxiv.org/abs/2507.01651)
*Sylvain Fontaine*

Main category: cs.DL

TL;DR: The paper explores how AI and neuroscience have evolved together, particularly in neurodegenerative disease research, using document embedding techniques. However, AI technologies often remain confined within specific subfields.


<details>
  <summary>Details</summary>
Motivation: To investigate if AI and neuroscience are still growing together in fundamental research despite AI's engineering focus and industry dominance.

Method: Temporal knowledge cartography of neuroscience with document embedding techniques and the analysis of citation networks.

Result: AI and neuroscience continue to expand together, especially in neurodegenerative disease research; however, AI's tools often stay limited within specific subfields.

Conclusion: The study suggests discussing AI's genericity and the need for better diffusion of its metrology across subfields in neuroscience.

Abstract: Neuroscience and AI have an intertwined history, largely relayed in the
literature of both fields. In recent years, due to the engineering orientations
of AI research and the monopoly of industry for its large-scale applications,
the mutual expansion of neuroscience and AI in fundamental research seems
challenged. In this paper, we bring some empirical evidences that, on the
contrary, AI and neuroscience are continuing to grow together, but with a
pronounced interest in the fields of study related to neurodegenerative
diseases since the 1990s. With a temporal knowledge cartography of neuroscience
drawn with advanced document embedding techniques, we draw the dynamical
shaping of the discipline since the 1970s and identified the conceptual
articulation of AI with this particular subfield mentioned before. However, a
further analysis of the underlying citation network of the studied corpus shows
that the produced AI technologies remain confined in the different subfields
and are not transferred from one subfield to another. This invites us to
discuss the genericity capability of AI in the context of an intradisciplinary
development, especially in the diffusion of its associated metrology.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [307] [STEM Diffraction Pattern Analysis with Deep Learning Networks](https://arxiv.org/abs/2507.01889)
*Sebastian Wissel,Jonas Scheunert,Aaron Dextre,Shamail Ahmed,Andreas Bayer,Kerstin Volz,Bai-Xiang Xu*

Main category: cond-mat.dis-nn

TL;DR: This paper proposes using advanced machine learning models to predict crystal orientations from scanning transmission electron microscopy (STEM) diffraction patterns, focusing on lithium nickel oxide (LiNiO$_{2}$). The Swin Transformer outperforms other architectures for high-resolution mapping.


<details>
  <summary>Details</summary>
Motivation: Understanding and optimizing polycrystalline material performance, particularly for energy-focused applications like lithium-ion cathode materials, requires accurate grain orientation mapping. Traditional methods are slow and error-prone, limiting large-scale analysis.

Method: The study evaluates three deep learning architectures (CNNs, DenseNets, and Swin Transformers) to predict Euler angles from STEM diffraction data. A commercial template matching algorithm provided labeled data for model training and comparison.

Result: Both DenseNets and Swin Transformers outperformed baseline CNNs, with the Swin Transformer achieving the best performance in model evaluation and microstructural prediction. This approach produced clear grain boundaries and consistent intra-grain orientations.

Conclusion: The successful application of machine learning, specifically the Swin Transformer, demonstrates the feasibility of robust, automated high-resolution microstructural characterization at the nanoscale, enabling advancements in material analysis.

Abstract: Accurate grain orientation mapping is essential for understanding and
optimizing the performance of polycrystalline materials, particularly in
energy-related applications. Lithium nickel oxide (LiNiO$_{2}$) is a promising
cathode material for next-generation lithium-ion batteries, and its
electrochemical behaviour is closely linked to microstructural features such as
grain size and crystallographic orientations. Traditional orientation mapping
methods--such as manual indexing, template matching (TM), or Hough
transform-based techniques--are often slow and noise-sensitive when handling
complex or overlapping patterns, creating a bottleneck in large-scale
microstructural analysis. This work presents a machine learning-based approach
for predicting Euler angles directly from scanning transmission electron
microscopy (STEM) diffraction patterns (DPs). This enables the automated
generation of high-resolution crystal orientation maps, facilitating the
analysis of internal microstructures at the nanoscale. Three deep learning
architectures--convolutional neural networks (CNNs), Dense Convolutional
Networks (DenseNets), and Shifted Windows (Swin) Transformers--are evaluated,
using an experimentally acquired dataset labelled via a commercial TM
algorithm. While the CNN model serves as a baseline, both DenseNets and Swin
Transformers demonstrate superior performance, with the Swin Transformer
achieving the highest evaluation scores and the most consistent microstructural
predictions. The resulting crystal maps exhibit clear grain boundary
delineation and coherent intra-grain orientation distributions, underscoring
the potential of attention-based architectures for analyzing diffraction-based
image data. These findings highlight the promise of combining advanced machine
learning models with STEM data for robust, high-throughput microstructural
characterization.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [308] [Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem](https://arxiv.org/abs/2507.01076)
*Vanja StojanoviÄ,Bor PangerÅ¡iÄ*

Main category: cs.CG

TL;DR: This paper empirically evaluates three algorithms for the NP-complete mutual-visibility (MV) problem, showing varying performance across different graph sizes.


<details>
  <summary>Details</summary>
Motivation: There is a lack of practical, empirical studies on the behavior of solutions to the NP-complete mutual-visibility (MV) problem.

Method: The study implements and evaluates three algorithms: a direct greedy heuristic, a hypergraph-based approximation, and a genetic algorithm, using diverse synthetic graph datasets with known and general properties.

Result: The algorithms achieved theoretical MV set sizes for smaller graphs but diverged for larger graphs. The genetic algorithm performed best among tested methods based on validation on optimal graphs.

Conclusion: While practical algorithms deliver promising results, especially for smaller graphs, the absence of tight bounds limits absolute quality determination for larger graphs.

Abstract: The NP-complete mutual-visibility (MV) problem currently lacks empirical
analysis on its practical behaviour despite theoretical studies. This paper
addresses this gap by implementing and evaluating three distinct algorithms - a
direct greedy heuristic, a hypergraph-based approximation, and a genetic
algorithm - on diverse synthetic graph datasets, including those with
analytically known $\mu(G)$ values and general graph models. Our results
demonstrate that for smaller graphs, the algorithms consistently achieve MV set
sizes aligning with theoretical bounds. However, for larger instances, achieved
solution sizes notably diverge from theoretical limits; this, combined with the
absence of tight bounds, complicates absolute quality assessment. Nevertheless,
validation on known optimal graphs showed the Genetic Algorithm and other
heuristics empirically performing best among tested methods.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [309] [Epistemic Scarcity: The Economics of Unresolvable Unknowns](https://arxiv.org/abs/2507.01483)
*Craig S Wright*

Main category: econ.GN

TL;DR: The paper critiques AI systems and algorithmic governance by using Austrian economic theories, claiming they fall short for effective economic coordination and ethical reasoning.


<details>
  <summary>Details</summary>
Motivation: To challenge the prevalent notions about AI's ability to sustain economic and epistemic order, against a backdrop of Austrian economics and praxeology.

Method: The authors apply Misesian a priori reasoning, Austrian entrepreneurship theories, and critique ethical AI frameworks like FAT as misaligned with voluntary action and liberal values.

Result: The study concludes AI systems can't originate norms, interpret institutions, or effectively coordinate economic actions, remaining opaque and inert.

Conclusion: The authors argue AI debates signal human autonomy versus computational control, with Austrian economics offering an alternative framework based on action and spontaneous order.

Abstract: This paper presents a praxeological analysis of artificial intelligence and
algorithmic governance, challenging assumptions about the capacity of machine
systems to sustain economic and epistemic order. Drawing on Misesian a priori
reasoning and Austrian theories of entrepreneurship, we argue that AI systems
are incapable of performing the core functions of economic coordination:
interpreting ends, discovering means, and communicating subjective value
through prices. Where neoclassical and behavioural models treat decisions as
optimisation under constraint, we frame them as purposive actions under
uncertainty.
  We critique dominant ethical AI frameworks such as Fairness, Accountability,
and Transparency (FAT) as extensions of constructivist rationalism, which
conflict with a liberal order grounded in voluntary action and property rights.
Attempts to encode moral reasoning in algorithms reflect a misunderstanding of
ethics and economics. However complex, AI systems cannot originate norms,
interpret institutions, or bear responsibility. They remain opaque, misaligned,
and inert.
  Using the concept of epistemic scarcity, we explore how information abundance
degrades truth discernment, enabling both entrepreneurial insight and soft
totalitarianism. Our analysis ends with a civilisational claim: the debate over
AI concerns the future of human autonomy, institutional evolution, and reasoned
choice. The Austrian tradition, focused on action, subjectivity, and
spontaneous order, offers the only coherent alternative to rising computational
social control.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [310] [Systemic Constraints of Undecidability](https://arxiv.org/abs/2507.01036)
*Seth Bulin*

Main category: cs.FL

TL;DR: The paper redefines incomputability as a structural property of systems, introducing a theory of systemic undecidability and causal embedding.


<details>
  <summary>Details</summary>
Motivation: To challenge the conventional view that computational limits can be circumvented and to position undecidability as a systemic constraint.

Method: Introduces the notion of causal embedding and proves a closure principle establishing that functional subsystems inherit undecidability.

Result: Undecidability is shown to be a pervasive limitation affecting prediction, modeling, and epistemic access in systems.

Conclusion: The work broadens classical computability results into dynamic systems and highlights the constraints on scientific knowledge.

Abstract: This paper presents a theory of systemic undecidability, reframing
incomputability as a structural property of systems rather than a localized
feature of specific functions or problems. We define a notion of causal
embedding and prove a closure principle: any subsystem that participates
functionally in the computation of an undecidable system inherits its
undecidability. This result positions undecidability as a pervasive constraint
on prediction, modeling, and epistemic access in both natural and artificial
systems. Our framework disarms oracle mimicry and challenges the view that
computational limits can be circumvented through architectural innovation. By
generalizing classical results into a dynamic systems context, this work
augments the logical trajectory of G\"odel, Turing, and Chaitin, offering a new
perspective of the topology of computability and its interrelation to the
boundaries of scientific knowledge.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [311] [A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory](https://arxiv.org/abs/2507.01110)
*Felix Windisch,Lukas Radl,Thomas KÃ¶hler,Michael Steiner,Dieter Schmalstieg,Markus Steinberger*

Main category: cs.GR

TL;DR: The paper introduces a framework for dynamic streaming and rendering ultra-large Gaussian scenes on a single consumer GPU, avoiding scene partitioning.


<details>
  <summary>Details</summary>
Motivation: To address the difficulty of scaling Gaussian Splatting to large environments while maintaining real-time rendering and minimizing GPU memory dependency.

Method: The paper developed a Level-of-Detail (LoD) representation combined with Gaussian hierarchies and Sequential Point Trees, managing view-dependent LoD selection and leveraging caching for real-time streaming.

Result: The framework enables ultra-large-scale scenes to be trained efficiently and rendered in real-time without GPU memory constraints.

Conclusion: The method offers seamless multi-scale reconstruction and interactive visualization for complex scenes without introducing chunk-based artifacts.

Abstract: Gaussian Splatting has emerged as a high-performance technique for novel view
synthesis, enabling real-time rendering and high-quality reconstruction of
small scenes. However, scaling to larger environments has so far relied on
partitioning the scene into chunks -- a strategy that introduces artifacts at
chunk boundaries, complicates training across varying scales, and is poorly
suited to unstructured scenarios such as city-scale flyovers combined with
street-level views. Moreover, rendering remains fundamentally limited by GPU
memory, as all visible chunks must reside in VRAM simultaneously. We introduce
A LoD of Gaussians, a framework for training and rendering ultra-large-scale
Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our
method stores the full scene out-of-core (e.g., in CPU memory) and trains a
Level-of-Detail (LoD) representation directly, dynamically streaming only the
relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with
Sequential Point Trees enables efficient, view-dependent LoD selection, while a
lightweight caching and view scheduling system exploits temporal coherence to
support real-time streaming and rendering. Together, these innovations enable
seamless multi-scale reconstruction and interactive visualization of complex
scenes -- from broad aerial views to fine-grained ground-level details.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [312] [LeanLTL: A unifying framework for linear temporal logics in Lean](https://arxiv.org/abs/2507.01780)
*Eric Vin,Kyle A. Miller,Daniel J. Fremont*

Main category: cs.LO

TL;DR: LeanLTL is a Lean 4 framework for reasoning about linear temporal logics (LTL) over finite and infinite traces, combining LTL syntax with Lean expressions.


<details>
  <summary>Details</summary>
Motivation: To create a flexible and unified framework for reasoning about linear temporal logics (LTL), incorporating both finite and infinite linear time traces and arbitrary Lean expressions.

Method: The framework embeds standard flavors of LTL into the Lean environment and combines LTL syntax with arbitrary Lean expressions. It also provides automation and integrates with Lean's existing tactics.

Result: LeanLTL supports reasoning about system properties using LTL syntax and Lean expressions, and the authors demonstrate its utility with examples.

Conclusion: LeanLTL serves as a versatile framework for integrating LTL with Lean expressions, making it useful for reasoning about various types of systems and properties.

Abstract: We propose LeanLTL, a unifying framework for linear temporal logics in Lean
4. LeanLTL supports reasoning about traces that represent either infinite or
finite linear time. The library allows traditional LTL syntax to be combined
with arbitrary Lean expressions, making it straightforward to define properties
involving numerical or other types. We prove that standard flavors of LTL can
be embedded in our framework. The library also provides automation for
reasoning about LeanLTL formulas in a way that facilitates using Lean's
existing tactics. Finally, we provide examples illustrating the utility of the
library in reasoning about systems that come from applications.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [313] [Entropic optimal transport beyond product reference couplings: the Gaussian case on Euclidean space](https://arxiv.org/abs/2507.01709)
*Paul Freulon,Nikitas Georgakis,Victor Panaretos*

Main category: math.ST

TL;DR: The paper focuses on entropic optimal transport with Gaussian reference couplings, reducing the problem to matrix optimization and addressing bias in regularized transport problems.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of current entropic optimal transport methods which often rely on rigid reference measures like Lebesgue or product measures, and to explore how a flexible choice of reference can reduce bias and incorporate prior knowledge.

Method: The paper introduces a matrix optimization framework for entropic optimal transport with flexible reference measures, particularly focusing on Gaussian references. Both primal and dual variables are analyzed.

Result: The authors provide a full mathematical reduction and characterization of the optimal transport problem under Gaussian reference measures, demonstrating advantages in reducing bias under specific numerical examples.

Conclusion: Flexibility in reference measures for entropic regularization offers significant benefits, particularly in leveraging prior knowledge, handling uncertainties, and addressing bias in statistical transport problems.

Abstract: The optimal transport problem with squared Euclidean cost consists in finding
a coupling between two input measures that maximizes correlation. Consequently,
the optimal coupling is often singular with respect to Lebesgue measure.
Regularizing the optimal transport problem with an entropy term yields an
approximation called entropic optimal transport. Entropic penalties steer the
induced coupling toward a reference measure with desired properties. For
instance, when seeking a diffuse coupling, the most popular reference measures
are the Lebesgue measure and the product of the two input measures. In this
work, we study the case where the reference coupling is not necessarily assumed
to be a product. We focus on the Gaussian case as a motivating paradigm, and
provide a reduction of this more general optimal transport criterion to a
matrix optimization problem. This reduction enables us to provide a complete
description of the solution, both in terms of the primal variable and the dual
variables. We argue that flexibility in terms of the reference measure can be
important in statistical contexts, for instance when one has prior information,
when there is uncertainty regarding the measures to be coupled, or to reduce
bias when the entropic problem is used to estimate the un-regularized transport
problem. In particular, we show in numerical examples that choosing a suitable
reference plan allows to reduce the bias caused by the entropic penalty.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [314] [A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques](https://arxiv.org/abs/2507.01018)
*Mohammed K. Alzaylaee*

Main category: cs.CR

TL;DR: Increasing cybersecurity risks in smart homes require enhanced cryptographic techniques and AI-driven security strategies, but scalability and resource demands remain challenges.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the pressing need to tackle cybersecurity risks in smart homes integrating IoT devices.

Method: The study categorizes security threats and analyzes strategies like post-quantum encryption, AI anomaly detection, blockchain authentication, and zero-trust models using statistical techniques and simulations.

Result: Post-quantum encryption and AI-driven detection enhance security but demand high resources. Blockchain and zero-trust models improve resilience but face infrastructure challenges.

Conclusion: There is a need to refine cryptographic and AI techniques for balancing real-time applicability and efficiency in smart home ecosystems.

Abstract: Smart homes that integrate Internet of Things (IoT) devices face increasing
cybersecurity risks, posing significant challenges to these environments. The
study explores security threats in smart homes ecosystems, categorizing them
into vulnerabilities at the network layer, device level, and those from
cloud-based and AI-driven systems. Research findings indicate that post-quantum
encryption, coupled with AI-driven anomaly detection, is highly effective in
enhancing security; however, computational resource demands present significant
challenges. Blockchain authentication together with zero-trust structures
builds security resilience, although they need changes to existing
infrastructure. The specific security strategies show their effectiveness
through ANOVA, Chi-square tests, and Monte Carlo simulations yet lack
sufficient scalability according to the results. The research demonstrates the
requirement for improvement in cryptographic techniques, alongside AI-enhanced
threat detection and adaptive security models which must achieve a balance
between performance and efficiency and real-time applicability within smart
home ecosystems.

</details>


### [315] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2507.01020)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CR

TL;DR: The paper presents AutoAdv, a framework to automate adversarial prompt generation, exploiting vulnerabilities in LLM safety with up to 86% jailbreak success.


<details>
  <summary>Details</summary>
Motivation: To expose and evaluate vulnerabilities in LLM safety mechanisms, especially those susceptible to sophisticated adversarial attacks.

Method: AutoAdv employs a parametric attacker LLM to dynamically generate adversarial prompts using iterative rewriting, roleplaying, misdirection, and contextual manipulation techniques.

Result: Extensive testing on state-of-the-art LLMs like ChatGPT shows vulnerabilities, achieving jailbreak success rates up to 86%.

Conclusion: Current LLM safety mechanisms are inadequate against advanced multi-turn attacks, necessitating improved defense strategies.

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities to
jailbreaking attacks: carefully crafted malicious inputs intended to circumvent
safety guardrails and elicit harmful responses. As such, we present AutoAdv, a
novel framework that automates adversarial prompt generation to systematically
evaluate and expose vulnerabilities in LLM safety mechanisms. Our approach
leverages a parametric attacker LLM to produce semantically disguised malicious
prompts through strategic rewriting techniques, specialized system prompts, and
optimized hyperparameter configurations. The primary contribution of our work
is a dynamic, multi-turn attack methodology that analyzes failed jailbreak
attempts and iteratively generates refined follow-up prompts, leveraging
techniques such as roleplaying, misdirection, and contextual manipulation. We
quantitatively evaluate attack success rate (ASR) using the StrongREJECT
(arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns.
Through extensive empirical evaluation of state-of-the-art models--including
ChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our
automated attacks achieving jailbreak success rates of up to 86% for harmful
content generation. Our findings reveal that current safety mechanisms remain
susceptible to sophisticated multi-turn attacks, emphasizing the urgent need
for more robust defense strategies.

</details>


### [316] [SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism](https://arxiv.org/abs/2507.01513)
*Beitao Chen,Xinyu Lyu,Lianli Gao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CR

TL;DR: The paper introduces Safe Prune-then-Restore (SafePTR), a training-free method to enhance Multimodal Large Language Models (MLLMs) safety against multimodal jailbreaks by precisely pruning harmful tokens in earlier layers and preserving benign features without extra computational costs.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the vulnerabilities in Multimodal Large Language Models (MLLMs), which are susceptible to multimodal jailbreak attacks, a challenge inadequately addressed by existing methods that fail to tackle root causes and often exhibit inefficiencies.

Method: The proposed method, SafePTR, involves a framework that identifies and selectively prunes harmful tokens in early-middle layers of MLLMs while restoring benign features in subsequent layers, all without additional training or computational overhead.

Result: SafePTR achieves state-of-the-art performance in mitigating jailbreak risks while maintaining model efficiency, as validated across three MLLMs and five benchmarks.

Conclusion: SafePTR provides an effective, computationally efficient solution for enhancing the safety of MLLMs against multimodal jailbreaks by addressing vulnerabilities at their root cause.

Abstract: By incorporating visual inputs, Multimodal Large Language Models (MLLMs)
extend LLMs to support visual reasoning. However, this integration also
introduces new vulnerabilities, making MLLMs susceptible to multimodal
jailbreak attacks and hindering their safe deployment.Existing defense methods,
including Image-to-Text Translation, Safe Prompting, and Multimodal Safety
Tuning, attempt to address this by aligning multimodal inputs with LLMs'
built-in safeguards.Yet, they fall short in uncovering root causes of
multimodal vulnerabilities, particularly how harmful multimodal tokens trigger
jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven
multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing
heavy training overhead.To bridge this gap, we present an comprehensive
analysis of where, how and which harmful multimodal tokens bypass safeguards in
MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers
are responsible for inducing unsafe behaviors, highlighting the potential of
precisely removing a small subset of harmful tokens, without requiring safety
tuning, can still effectively improve safety against jailbreaks. Motivated by
this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense
framework that selectively prunes harmful tokens at vulnerable layers while
restoring benign features at subsequent layers.Without incurring additional
computational overhead, SafePTR significantly enhances the safety of MLLMs
while preserving efficiency. Extensive evaluations across three MLLMs and five
benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating
jailbreak risks without compromising utility.

</details>


### [317] [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](https://arxiv.org/abs/2507.01808)
*Xiaoyu Ji,Jessica Shorland,Joshua Shank,Pascal Delpe-Brice,Latanya Sweeney,Jan Allebach,Ali Shakouri*

Main category: cs.CR

TL;DR: This paper introduces a privacy-preserving platform for data sharing between manufacturers and researchers, featuring a case study on automating food crystal quality control with machine learning.


<details>
  <summary>Details</summary>
Motivation: Manufacturers, particularly small- and medium-sized ones, need data tools for solving real-world problems but are restricted by competition and privacy concerns in sharing proprietary data with researchers.

Method: The authors developed a privacy-preserving platform enabling secure data sharing between manufacturers and researchers. They deployed a machine learning model to automate the counting and characterization of food crystals from microscope images while addressing imperfections, shared via a secured web-based app.

Result: A functional algorithm for automated analysis of food crystals was created and implemented in a real-world manufacturing setting via a privacy-preserving, web-based platform.

Conclusion: The platform successfully facilitates collaboration between manufacturers and researchers while ensuring privacy, and the specific tool for food crystal analysis demonstrates the practicality and benefits of this approach.

Abstract: Small- and medium-sized manufacturers need innovative data tools but, because
of competition and privacy concerns, often do not want to share their
proprietary data with researchers who might be interested in helping. This
paper introduces a privacy-preserving platform by which manufacturers may
safely share their data with researchers through secure methods, so that those
researchers then create innovative tools to solve the manufacturers' real-world
problems, and then provide tools that execute solutions back onto the platform
for others to use with privacy and confidentiality guarantees. We illustrate
this problem through a particular use case which addresses an important problem
in the large-scale manufacturing of food crystals, which is that quality
control relies on image analysis tools. Previous to our research, food crystals
in the images were manually counted, which required substantial and
time-consuming human efforts, but we have developed and deployed a crystal
analysis tool which makes this process both more rapid and accurate. The tool
enables automatic characterization of the crystal size distribution and numbers
from microscope images while the natural imperfections from the sample
preparation are automatically removed; a machine learning model to count high
resolution translucent crystals and agglomeration of crystals was also
developed to aid in these efforts. The resulting algorithm was then packaged
for real-world use on the factory floor via a web-based app secured through the
originating privacy-preserving platform, allowing manufacturers to use it while
keeping their proprietary data secure. After demonstrating this full process,
future directions are also explored.

</details>


### [318] [How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations](https://arxiv.org/abs/2507.01487)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: This paper surveys 26 secure shuffling protocols, categorizing and comparing them based on uniform security definitions while emphasizing their applications and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore and address the gap in understanding what constitutes a 'good secure shuffler' amid discussions of privacy amplification and implementation trade-offs.

Method: The study unifies security definitions for secure shufflers, categorizes 26 protocols, and offers practical guidelines and future research directions.

Result: A comprehensive comparison of secure shuffling protocols, an overview of their applications in privacy-preserving technologies, and practical insights for protocol selection.

Conclusion: This paper provides a methodological framework to evaluate secure shufflers and contributes to advancing their design and practical implementation for better privacy guarantees.

Abstract: Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building
block for private data aggregation. Recently, the field of differential privacy
has revived interest in secure shufflers by highlighting the privacy
amplification they can provide in various computations. Although several works
argue for the utility of secure shufflers, they often treat them as black
boxes; overlooking the practical vulnerabilities and performance trade-offs of
existing implementations. This leaves a central question open: what makes a
good secure shuffler?
  This survey addresses that question by identifying, categorizing, and
comparing 26 secure protocols that realize the necessary shuffling
functionality. To enable a meaningful comparison, we adapt and unify existing
security definitions into a consistent set of properties. We also present an
overview of privacy-preserving technologies that rely on secure shufflers,
offer practical guidelines for selecting appropriate protocols, and outline
promising directions for future work.

</details>


### [319] [On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE](https://arxiv.org/abs/2507.01571)
*Koen T. W. Teuwen,Sam Baggen,Emmanuele Zambon,Luca Allodi*

Main category: cs.CR

TL;DR: This paper evaluates the impact of label imbalance in classifying network intrusion alerts using DeepCASE and suggests improving input data quality to enhance performance and explainability.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in automated alert classification in SOCs caused by label imbalance and the need for explainable decisions.

Method: The performance and explanation correctness of DeepCASE are evaluated under conditions of label imbalance, using network intrusion alerts as a use case.

Result: DeepCASE's classification performance and explanation accuracy are negatively impacted by label imbalance.

Conclusion: Improving the quality of input data, such as tuning SOC detection rules, can enhance the robustness and explainability of automated alert classification systems like DeepCASE.

Abstract: Automation in Security Operations Centers (SOCs) plays a prominent role in
alert classification and incident escalation. However, automated methods must
be robust in the presence of imbalanced input data, which can negatively affect
performance. Additionally, automated methods should make explainable decisions.
In this work, we evaluate the effect of label imbalance on the classification
of network intrusion alerts. As our use-case we employ DeepCASE, the
state-of-the-art method for automated alert classification. We show that label
imbalance impacts both classification performance and correctness of the
classification explanations offered by DeepCASE. We conclude tuning the
detection rules used in SOCs can significantly reduce imbalance and may benefit
the performance and explainability offered by alert post-processing methods
such as DeepCASE. Therefore, our findings suggest that traditional methods to
improve the quality of input data can benefit automation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [320] [Prompt Mechanisms in Medical Imaging: A Comprehensive Survey](https://arxiv.org/abs/2507.01055)
*Hao Yang,Xinlong Liang,Zhang Li,Yue Sun,Zheyu Hu,Xinghe Xie,Behdad Dashtbozorg,Jincheng Huang,Shiwei Zhu,Luyi Han,Jiong Zhang,Shanshan Wang,Ritse Mann,Qifeng Yu,Tao Tan*

Main category: eess.IV

TL;DR: The paper reviews the role of prompt engineering in deep learning for medical imaging, examining how it addresses challenges like data scarcity, distribution shifts, and task generalization.


<details>
  <summary>Details</summary>
Motivation: To address barriers in clinical adoption of deep learning in medical imaging, such as data constraints and the need for better model generalization.

Method: A systematic review of prompt-based methodologies, exploring textual, visual, and learnable prompts and their application in image-related tasks like generation, segmentation, and classification.

Result: Prompt-based techniques enhance task accuracy, robustness, data efficiency, and interpretability, while reducing reliance on manual feature engineering.

Conclusion: Prompt-driven AI has substantial potential in medical imaging, but challenges like prompt design optimization and clinical scalability remain. The future lies in advanced multimodal prompting and robust clinical integration.

Abstract: Deep learning offers transformative potential in medical imaging, yet its
clinical adoption is frequently hampered by challenges such as data scarcity,
distribution shifts, and the need for robust task generalization. Prompt-based
methodologies have emerged as a pivotal strategy to guide deep learning models,
providing flexible, domain-specific adaptations that significantly enhance
model performance and adaptability without extensive retraining. This
systematic review critically examines the burgeoning landscape of prompt
engineering in medical imaging. We dissect diverse prompt modalities, including
textual instructions, visual prompts, and learnable embeddings, and analyze
their integration for core tasks such as image generation, segmentation, and
classification. Our synthesis reveals how these mechanisms improve
task-specific outcomes by enhancing accuracy, robustness, and data efficiency
and reducing reliance on manual feature engineering while fostering greater
model interpretability by making the model's guidance explicit. Despite
substantial advancements, we identify persistent challenges, particularly in
prompt design optimization, data heterogeneity, and ensuring scalability for
clinical deployment. Finally, this review outlines promising future
trajectories, including advanced multimodal prompting and robust clinical
integration, underscoring the critical role of prompt-driven AI in accelerating
the revolution of diagnostics and personalized treatment planning in medicine.

</details>


### [321] [CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation](https://arxiv.org/abs/2506.23121)
*Xinlei Yu,Chanmiao Wang,Hui Jin,Ahmed Elazab,Gangyong Jia,Xiang Wan,Changqing Zou,Ruiquan Ge*

Main category: eess.IV

TL;DR: The paper introduces CRISP-SAM2, a novel model enhancing multi-organ medical segmentation by utilizing cross-modal integration and semantic prompting strategies, achieving superior performance on seven datasets.


<details>
  <summary>Details</summary>
Motivation: To address inaccuracies, reliance on geometric prompts, and loss of spatial details in multi-organ medical segmentation models.

Method: The method involves converting visual and textual inputs into cross-modal contextualized semantics through a progressive cross-attention interaction mechanism. Semantic prompting replaces geometric prompting, while additional techniques like similarity-sorting self-updating and mask refinement further improve segmentation. The model is guided by textual descriptions of organs.

Result: CRISP-SAM2 achieves superior results, outperforming existing multi-organ segmentation models on seven public datasets.

Conclusion: The proposed method enhances the precision, adaptability, and local detail perception of medical image segmentation models, addressing key limitations of previous approaches.

Abstract: Multi-organ medical segmentation is a crucial component of medical image
processing, essential for doctors to make accurate diagnoses and develop
effective treatment plans. Despite significant progress in this field, current
multi-organ segmentation models often suffer from inaccurate details,
dependence on geometric prompts and loss of spatial information. Addressing
these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal
Interaction and Semantic Prompting based on SAM2. This model represents a
promising approach to multi-organ medical segmentation guided by textual
descriptions of organs. Our method begins by converting visual and textual
inputs into cross-modal contextualized semantics using a progressive
cross-attention interaction mechanism. These semantics are then injected into
the image encoder to enhance the detailed understanding of visual information.
To eliminate reliance on geometric prompts, we use a semantic prompting
strategy, replacing the original prompt encoder to sharpen the perception of
challenging targets. In addition, a similarity-sorting self-updating strategy
for memory and a mask-refining process is applied to further adapt to medical
imaging and enhance localized details. Comparative experiments conducted on
seven public datasets indicate that CRISP-SAM2 outperforms existing models.
Extensive analysis also demonstrates the effectiveness of our method, thereby
confirming its superior performance, especially in addressing the limitations
mentioned earlier. Our code is available at:
https://github.com/YU-deep/CRISP\_SAM2.git.

</details>


### [322] [MID-INFRARED (MIR) OCT-based inspection in industry](https://arxiv.org/abs/2507.01074)
*N. P. GarcÃ­a-de-la-Puente,RocÃ­o del Amor,Fernando GarcÃ­a-Torres,Niels MÃ¸ller Israelsen,Coraline Lapre,Christian Rosenberg Petersen,Ole Bang,Dominik Brouczek,Martin Schwentenwein,Kevin Neumann,Niels Benson,Valery Naranjo*

Main category: eess.IV

TL;DR: The paper investigates the use of mid-infrared Optical Coherence Tomography (MIR-OCT) and AI-enhanced vision algorithms for detecting sub-surface irregularities in composites and ceramics.


<details>
  <summary>Details</summary>
Motivation: To provide non-destructive inspection techniques for industrial production processes to identify and address sub-surface irregularities in materials.

Method: The study utilizes MIR-OCT systems on composite and ceramic samples, combining preprocessing and AI vision algorithms for anomaly detection. It also evaluates limitations and criteria for parameter optimization.

Result: Demonstrated the capabilities of MIR-OCT systems and AI techniques for detecting sub-surface abnormalities, along with identified strengths and weaknesses of the approach.

Conclusion: MIR-OCT systems and AI-based methodologies show promise as non-destructive inspection tools, with further refinement needed for optimal parameter selection and overcoming limitations.

Abstract: This paper aims to evaluate mid-infrared (MIR) Optical Coherence Tomography
(OCT) systems as a tool to penetrate different materials and detect sub-surface
irregularities. This is useful for monitoring production processes, allowing
Non-Destructive Inspection Techniques of great value to the industry. In this
exploratory study, several acquisitions are made on composite and ceramics to
know the capabilities of the system. In addition, it is assessed which
preprocessing and AI-enhanced vision algorithms can be anomaly-detection
methodologies capable of detecting abnormal zones in the analyzed objects.
Limitations and criteria for the selection of optimal parameters will be
discussed, as well as strengths and weaknesses will be highlighted.

</details>


### [323] [Classification based deep learning models for lung cancer and disease using medical images](https://arxiv.org/abs/2507.01279)
*Ahmad Chaddad,Jihao Peng,Yihang Wu*

Main category: eess.IV

TL;DR: This study introduces ResNet+, an enhanced CNN based on ResNet, for improved lung cancer prediction from medical images, featuring attention modules and ResNet-D enhancements.


<details>
  <summary>Details</summary>
Motivation: Deep learning has shown promise in medical image analysis, but challenges remain in extracting meaningful features during downsampling in CNNs for lung cancer prediction.

Method: The researchers enhanced the ResNet architecture by integrating the ResNet-D module for better feature extraction and adding a convolutional attention module to focus on important image regions. They employed data augmentation to address class imbalances.

Result: ResNet+ achieved superior performance, with accuracy/F1 reaching up to 99.25/99.13% on certain datasets. It also demonstrated computational efficiency compared to original ResNet models.

Conclusion: ResNet+ significantly improves the prediction of lung cancer and other lung diseases, outperforming existing models, while maintaining efficiency and generalizability.

Abstract: The use of deep learning (DL) in medical image analysis has significantly
improved the ability to predict lung cancer. In this study, we introduce a
novel deep convolutional neural network (CNN) model, named ResNet+, which is
based on the established ResNet framework. This model is specifically designed
to improve the prediction of lung cancer and diseases using the images. To
address the challenge of missing feature information that occurs during the
downsampling process in CNNs, we integrate the ResNet-D module, a variant
designed to enhance feature extraction capabilities by modifying the
downsampling layers, into the traditional ResNet model. Furthermore, a
convolutional attention module was incorporated into the bottleneck layers to
enhance model generalization by allowing the network to focus on relevant
regions of the input images. We evaluated the proposed model using five public
datasets, comprising lung cancer (LC2500 $n$=3183, IQ-OTH/NCCD $n$=1336, and
LCC $n$=25000 images) and lung disease (ChestXray $n$=5856, and COVIDx-CT
$n$=425024 images). To address class imbalance, we used data augmentation
techniques to artificially increase the representation of underrepresented
classes in the training dataset. The experimental results show that ResNet+
model demonstrated remarkable accuracy/F1, reaching 98.14/98.14\% on the
LC25000 dataset and 99.25/99.13\% on the IQ-OTH/NCCD dataset. Furthermore, the
ResNet+ model saved computational cost compared to the original ResNet series
in predicting lung cancer images. The proposed model outperformed the baseline
models on publicly available datasets, achieving better performance metrics.
Our codes are publicly available at
https://github.com/AIPMLab/Graduation-2024/tree/main/Peng.

</details>


### [324] [PanTS: The Pancreatic Tumor Segmentation Dataset](https://arxiv.org/abs/2507.01291)
*Wenxuan Li,Xinze Zhou,Qi Chen,Tianyu Lin,Pedro R. A. S. Bassi,Szymon Plotka,Jaroslaw B. Cwikla,Xiaoxi Chen,Chen Ye,Zheren Zhu,Kai Ding,Heng Li,Kang Wang,Yang Yang,Yucheng Tang,Daguang Xu,Alan L. Yuille,Zongwei Zhou*

Main category: eess.IV

TL;DR: PanTS is a large-scale dataset featuring over 36,000 CT scans with detailed annotations to significantly enhance AI performance in pancreatic tumor analysis.


<details>
  <summary>Details</summary>
Motivation: To address limitations in existing datasets and improve AI-driven analysis of pancreatic tumors through large-scale, high-quality annotations and multi-institutional diversity.

Method: Researchers compiled 36,390 CT scans from 145 centers, providing voxel-wise annotations for over 993,000 anatomical structures along with metadata and using these to train and benchmark AI models.

Result: AI trained on PanTS showed markedly superior performance in detecting and segmenting pancreatic tumors compared to models using existing datasets.

Conclusion: PanTS establishes a new standard for AI in pancreatic CT analysis, emphasizing its potential for advancing research and clinical applications.

Abstract: PanTS is a large-scale, multi-institutional dataset curated to advance
research in pancreatic CT analysis. It contains 36,390 CT scans from 145
medical centers, with expert-validated, voxel-wise annotations of over 993,000
anatomical structures, covering pancreatic tumors, pancreas head, body, and
tail, and 24 surrounding anatomical structures such as vascular/skeletal
structures and abdominal/thoracic organs. Each scan includes metadata such as
patient age, sex, diagnosis, contrast phase, in-plane spacing, slice thickness,
etc. AI models trained on PanTS achieve significantly better performance in
pancreatic tumor detection, localization, and segmentation compared to those
trained on existing public datasets. Our analysis indicates that these gains
are directly attributable to the 16x larger-scale tumor annotations and
indirectly supported by the 24 additional surrounding anatomical structures. As
the largest and most comprehensive resource of its kind, PanTS offers a new
benchmark for developing and evaluating AI models in pancreatic CT analysis.

</details>


### [325] [SWinMamba: Serpentine Window State Space Model for Vascular Segmentation](https://arxiv.org/abs/2507.01323)
*Rongchang Zhao,Huanchi Liu,Jian Zhang*

Main category: eess.IV

TL;DR: The paper presents SWinMamba, a novel method for accurate vascular segmentation in medical images using serpentine window sequences and bidirectional state space models.


<details>
  <summary>Details</summary>
Motivation: Vascular segmentation is vital for diagnosis and surgical navigation but is often discontinuous due to the slender nature of vascular structures and insufficient prior modeling.

Method: The proposed method, SWinMamba, utilizes serpentine window sequences to model the continuity of vascular structures. It includes a Serpentine Window Tokenizer (SWToken) for adaptive splitting, a Bidirectional Aggregation Module (BAM) for coherent local feature integration, and dual-domain learning with a Spatial-Frequency Fusion Unit (SFFU) for enhanced feature representation.

Result: Experiments across three datasets demonstrated excellent performance, with SWinMamba achieving superior vascular segmentation accuracy and connectivity.

Conclusion: The SWinMamba model effectively addresses the challenge of discontinuous vascular segmentation, offering a robust and accurate solution.

Abstract: Vascular segmentation in medical images is crucial for disease diagnosis and
surgical navigation. However, the segmented vascular structure is often
discontinuous due to its slender nature and inadequate prior modeling. In this
paper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve
accurate vascular segmentation. The proposed SWinMamba innovatively models the
continuity of slender vascular structures by incorporating serpentine window
sequences into bidirectional state space models. The serpentine window
sequences enable efficient feature capturing by adaptively guiding global
visual context modeling to the vascular structure. Specifically, the Serpentine
Window Tokenizer (SWToken) adaptively splits the input image using overlapping
serpentine window sequences, enabling flexible receptive fields (RFs) for
vascular structure modeling. The Bidirectional Aggregation Module (BAM)
integrates coherent local features in the RFs for vascular continuity
representation. In addition, dual-domain learning with Spatial-Frequency Fusion
Unit (SFFU) is designed to enhance the feature representation of vascular
structure. Extensive experiments on three challenging datasets demonstrate that
the proposed SWinMamba achieves superior performance with complete and
connected vessels.

</details>


### [326] [Structure and Smoothness Constrained Dual Networks for MR Bias Field Correction](https://arxiv.org/abs/2507.01326)
*Dong Liang,Xingyu Qiu,Yuzhen Li,Wei Wang,Kuanquan Wang,Suyu Dong,Gongning Luo*

Main category: eess.IV

TL;DR: This paper presents S2DNets, a self-supervised deep learning approach for correcting intensity inhomogeneities in MR images while preserving structural details.


<details>
  <summary>Details</summary>
Motivation: To address the issue of intensity inhomogeneities in MR images caused by device limitations, which hampers both qualitative and quantitative medical analysis.

Method: The authors developed S2DNets, a dual network approach with piece-wise structural constraints and smoothness enforcement, enabling self-supervised bias field correction.

Result: Extensive experiments on clinical and simulated MR datasets show that S2DNets outperform conventional and other deep learning models both visually and in downstream segmentation tasks.

Conclusion: The proposed S2DNets effectively correct intensity biases in MR images, preserve structural details, and improve medical image analysis tasks.

Abstract: MR imaging techniques are of great benefit to disease diagnosis. However, due
to the limitation of MR devices, significant intensity inhomogeneity often
exists in imaging results, which impedes both qualitative and quantitative
medical analysis. Recently, several unsupervised deep learning-based models
have been proposed for MR image improvement. However, these models merely
concentrate on global appearance learning, and neglect constraints from image
structures and smoothness of bias field, leading to distorted corrected
results. In this paper, novel structure and smoothness constrained dual
networks, named S2DNets, are proposed aiming to self-supervised bias field
correction. S2DNets introduce piece-wise structural constraints and smoothness
of bias field for network training to effectively remove non-uniform intensity
and retain much more structural details. Extensive experiments executed on both
clinical and simulated MR datasets show that the proposed model outperforms
other conventional and deep learning-based models. In addition to comparison on
visual metrics, downstream MR image segmentation tasks are also used to
evaluate the impact of the proposed model. The source code is available at:
https://github.com/LeongDong/S2DNets}{https://github.com/LeongDong/S2DNets.

</details>


### [327] [BronchoGAN: Anatomically consistent and domain-agnostic image-to-image translation for video bronchoscopy](https://arxiv.org/abs/2507.01387)
*Ahmad Soliman,Ron Keuth,Marian Himstedt*

Main category: eess.IV

TL;DR: BronchoGAN uses conditional GANs and anatomical constraints for image-to-image translation across bronchoscopy image domains, generating realistic synthetic images and enabling robust model training.


<details>
  <summary>Details</summary>
Motivation: The scarcity of bronchoscopy images limits the training of deep learning models, necessitating effective image synthesis methods for robust clinical applications.

Method: BronchoGAN integrates anatomical constraints into conditional GANs, leverages foundation model-generated depth images for intermediate representations, and ensures bronchial orifice consistency between input and output images, facilitating cross-domain learning.

Result: The approach successfully translates inputs from various domains into realistic images, improving FID, SSIM, and Dice scores, with an up to 0.43 improvement in the Dice coefficient for synthetic images.

Conclusion: BronchoGAN addresses the scarcity of bronchoscopy datasets by generating realistic, anatomically accurate images, enabling public CT scan data to be used for large-scale dataset creation.

Abstract: The limited availability of bronchoscopy images makes image synthesis
particularly interesting for training deep learning models. Robust image
translation across different domains -- virtual bronchoscopy, phantom as well
as in-vivo and ex-vivo image data -- is pivotal for clinical applications. This
paper proposes BronchoGAN introducing anatomical constraints for image-to-image
translation being integrated into a conditional GAN. In particular, we force
bronchial orifices to match across input and output images. We further propose
to use foundation model-generated depth images as intermediate representation
ensuring robustness across a variety of input domains establishing models with
substantially less reliance on individual training datasets. Moreover our
intermediate depth image representation allows to easily construct paired image
data for training. Our experiments showed that input images from different
domains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to
images mimicking realistic human airway appearance. We demonstrated that
anatomical settings (i.e. bronchial orifices) can be robustly preserved with
our approach which is shown qualitatively and quantitatively by means of
improved FID, SSIM and dice coefficients scores. Our anatomical constraints
enabled an improvement in the Dice coefficient of up to 0.43 for synthetic
images. Through foundation models for intermediate depth representations,
bronchial orifice segmentation integrated as anatomical constraints into
conditional GANs we are able to robustly translate images from different
bronchoscopy input domains. BronchoGAN allows to incorporate public CT scan
data (virtual bronchoscopy) in order to generate large-scale bronchoscopy image
datasets with realistic appearance. BronchoGAN enables to bridge the gap of
missing public bronchoscopy images.

</details>


### [328] [Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling](https://arxiv.org/abs/2507.01564)
*Chia-Ming Lee,Bo-Cheng Qiu,Ting-Yao Chen,Ming-Han Sun,Fang-Ying Lin,Jung-Tse Tsai,I-An Tsai,Yu-Fan Lin,Chih-Chung Hsu*

Main category: eess.IV

TL;DR: This study focuses on classifying chest CT scans from multiple medical centers using a novel spatial feature learning framework with slice sampling techniques, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the variability encountered in multi-source medical imaging, ensuring robust and accurate classification of chest CT scans for detecting COVID-19 across distinct medical centers.

Method: The authors used the Spatial-Slice Feature Learning (SSFL) framework and Kernel-Density-based Slice Sampling (KDS). Their preprocessing involved lung region extraction, quality control, and adaptive slice sampling. They compared two architectures, EfficientNet and Swin Transformer, on the validation set.

Result: EfficientNet achieved an F1-score of 94.68%, while Swin Transformer reached 93.34%, showcasing the pipeline's efficacy.

Conclusion: Their framework effectively handles variability in multi-source medical imaging, achieving high performance and underscoring the need for balanced datasets in such evaluations.

Abstract: We present our solution for the Multi-Source COVID-19 Detection Challenge,
which classifies chest CT scans from four distinct medical centers. To address
multi-source variability, we employ the Spatial-Slice Feature Learning (SSFL)
framework with Kernel-Density-based Slice Sampling (KDS). Our preprocessing
pipeline combines lung region extraction, quality control, and adaptive slice
sampling to select eight representative slices per scan. We compare
EfficientNet and Swin Transformer architectures on the validation set. The
EfficientNet model achieves an F1-score of 94.68%, compared to the Swin
Transformer's 93.34%. The results demonstrate the effectiveness of our
KDS-based pipeline on multi-source data and highlight the importance of dataset
balance in multi-institutional medical imaging evaluation.

</details>


### [329] [Robust brain age estimation from structural MRI with contrastive learning](https://arxiv.org/abs/2507.01794)
*Carlo Alberto Barbano,Benoit Dufumier,Edouard Duchesnay,Marco Grangetto,Pietro Gori*

Main category: eess.IV

TL;DR: This work introduces a novel contrastive loss function for estimating brain age from MRI scans, achieving improved performance and robustness compared to supervised methods.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable and robust method for estimating brain age, which is critical for studying normative and pathological aging.

Method: A novel contrastive loss function ($\mathcal{L}^{exp}$) was proposed and tested on over 20,000 MRI scans from multiple datasets, emphasizing pre-training on diverse multi-site data.

Result: The new method significantly improved generalization performance, demonstrated robustness to site-related confounds, accurately captured accelerated aging in cognitive impairment conditions, and correlated well with diagnostic performance.

Conclusion: Contrastive learning with $\mathcal{L}^{exp}$ shows strong potential as a foundation model for neuroimaging, offering generalizable and clinically meaningful brain representations.

Abstract: Estimating brain age from structural MRI has emerged as a powerful tool for
characterizing normative and pathological aging. In this work, we explore
contrastive learning as a scalable and robust alternative to supervised
approaches for brain age estimation. We introduce a novel contrastive loss
function, $\mathcal{L}^{exp}$, and evaluate it across multiple public
neuroimaging datasets comprising over 20,000 scans. Our experiments reveal four
key findings. First, scaling pre-training on diverse, multi-site data
consistently improves generalization performance, cutting external mean
absolute error (MAE) nearly in half. Second, $\mathcal{L}^{exp}$ is robust to
site-related confounds, maintaining low scanner-predictability as training size
increases. Third, contrastive models reliably capture accelerated aging in
patients with cognitive impairment and Alzheimer's disease, as shown through
brain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike
supervised baselines, $\mathcal{L}^{exp}$ maintains a strong correlation
between brain age accuracy and downstream diagnostic performance, supporting
its potential as a foundation model for neuroimaging. These results position
contrastive learning as a promising direction for building generalizable and
clinically meaningful brain representations.

</details>


### [330] [Autoadaptive Medical Segment Anything Model](https://arxiv.org/abs/2507.01828)
*Tyler Ward,Meredith K. Owen,O'Kira Coleman,Brian Noehren,Abdullah-Al-Zubaer Imran*

Main category: eess.IV

TL;DR: The paper introduces ADA-SAM, a multitask learning framework for efficient and accurate medical image segmentation using limited labeled data.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the limitations of traditional, fully-supervised segmentation models that require extensive labeled data, which is costly and time-consuming to produce.

Method: ADA-SAM combines class activation maps from an auxiliary classifier with a semi-supervised segmentation model based on the Segment Anything (SAM) framework. Additionally, it features a gradient feedback mechanism linking segmentation and classification tasks.

Result: The method demonstrated superior performance, surpassing both fully-supervised and semi-supervised baselines by a significant margin in limited label settings, validated on real-world clinical datasets.

Conclusion: The proposed ADA-SAM framework is a promising solution for achieving annotation-efficient medical image segmentation, particularly in scenarios with limited labeled data.

Abstract: Medical image segmentation is a key task in the imaging workflow, influencing
many image-based decisions. Traditional, fully-supervised segmentation models
rely on large amounts of labeled training data, typically obtained through
manual annotation, which can be an expensive, time-consuming, and error-prone
process. This signals a need for accurate, automatic, and annotation-efficient
methods of training these models. We propose ADA-SAM (automated,
domain-specific, and adaptive segment anything model), a novel multitask
learning framework for medical image segmentation that leverages class
activation maps from an auxiliary classifier to guide the predictions of the
semi-supervised segmentation branch, which is based on the Segment Anything
(SAM) framework. Additionally, our ADA-SAM model employs a novel gradient
feedback mechanism to create a learnable connection between the segmentation
and classification branches by using the segmentation gradients to guide and
improve the classification predictions. We validate ADA-SAM on real-world
clinical data collected during rehabilitation trials, and demonstrate that our
proposed method outperforms both fully-supervised and semi-supervised baselines
by double digits in limited label settings. Our code is available at:
https://github.com/tbwa233/ADA-SAM.

</details>


### [331] [A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs](https://arxiv.org/abs/2507.01881)
*NiccolÃ² McConnell,Pardeep Vasudev,Daisuke Yamada,Daryl Cheng,Mehran Azimbagirad,John McCabe,Shahab Aslani,Ahmed H. Shahin,Yukun Zhou,The SUMMIT Consortium,Andre Altmann,Yipeng Hu,Paul Taylor,Sam M. Janes,Daniel C. Alexander,Joseph Jacob*

Main category: eess.IV

TL;DR: TANGERINE is an open-source vision foundation model designed for low-dose CT analysis, excelling in diverse disease detection with reduced computational requirements.


<details>
  <summary>Details</summary>
Motivation: There is a global increase in lung cancer screening programs using LDCT imaging, but efforts are hindered by a lack of radiologists to interpret these scans on a large scale.

Method: They developed TANGERINE, leveraging a self-supervised learning approach on over 98,000 thoracic LDCTs, and extended a masked autoencoder framework to handle 3D imaging while ensuring scalability and accessibility.

Result: TANGERINE achieved state-of-the-art performance across 14 disease classification tasks, showed strong label efficiency, and required fewer computational resources compared to models trained from scratch.

Conclusion: TANGERINE's lightweight, open-source design provides a scalable, efficient solution for LDCT analysis, potentially revolutionizing lung cancer screening by enabling comprehensive respiratory disease management.

Abstract: Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can
be fine-tuned off the shelf for a wide range of disease-specific tasks with
limited computational resources and training data. Relative to models trained
from scratch, TANGERINE demonstrates fast convergence during fine-tuning,
thereby requiring significantly fewer GPU hours, and displays strong label
efficiency, achieving comparable or superior performance with a fraction of
fine-tuning data. Pretrained using self-supervised learning on over 98,000
thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public
datasets, TANGERINE achieves state-of-the-art performance across 14 disease
classification tasks, including lung cancer and multiple respiratory diseases,
while generalising robustly across diverse clinical centres. By extending a
masked autoencoder framework to 3D imaging, TANGERINE offers a scalable
solution for LDCT analysis, departing from recent closed, resource-intensive
models by combining architectural simplicity, public availability, and modest
computational requirements. Its accessible, open-source lightweight design lays
the foundation for rapid integration into next-generation medical imaging tools
that could transform LCS initiatives, allowing them to pivot from a singular
focus on lung cancer detection to comprehensive respiratory disease management
in high-risk populations.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [332] [Transfer Learning for VLC-based indoor Localization: Addressing Environmental Variability](https://arxiv.org/abs/2507.01575)
*Masood Jan,Wafa Njima,Xun Zhang,Alexander Artemenko*

Main category: eess.SP

TL;DR: The paper proposes a Transfer Learning-based indoor localization system using Visible Light Communication to improve efficiency and accuracy, achieving high adaptability in industrial environments.


<details>
  <summary>Details</summary>
Motivation: Industry demands highly accurate, efficient, and adaptable indoor localization systems free from electromagnetic interference, especially under challenging environmental conditions.

Method: A Transfer Learning framework integrating a deep neural network was applied to VLC-based localization, and validated using real-world industrial data.

Result: The model delivered improvements: localization accuracy increased by 47%, energy consumption reduced by 32%, and computational time decreased by 40% compared to traditional methods.

Conclusion: The solution is cost-efficient, scalable, and adaptable, making it a practical choice for Industry 4.0 applications. It handles environmental variability effectively while maintaining robust performance.

Abstract: Accurate indoor localization is crucial in industrial environments. Visible
Light Communication (VLC) has emerged as a promising solution, offering high
accuracy, energy efficiency, and minimal electromagnetic interference. However,
VLC-based indoor localization faces challenges due to environmental
variability, such as lighting fluctuations and obstacles. To address these
challenges, we propose a Transfer Learning (TL)-based approach for VLC-based
indoor localization. Using real-world data collected at a BOSCH factory, the TL
framework integrates a deep neural network (DNN) to improve localization
accuracy by 47\%, reduce energy consumption by 32\%, and decrease computational
time by 40\% compared to the conventional models. The proposed solution is
highly adaptable under varying environmental conditions and achieves similar
accuracy with only 30\% of the dataset, making it a cost-efficient and scalable
option for industrial applications in Industry 4.0.

</details>


### [333] [Token Communication in the Era of Large Models: An Information Bottleneck-Based Approach](https://arxiv.org/abs/2507.01728)
*Hao Wei,Wanli Ni,Wen Wang,Wenjun Xu,Dusit Niyato,Ping Zhang*

Main category: eess.SP

TL;DR: The paper introduces UniToCom, a communication paradigm utilizing tokens as core units for processing and wireless transmission, supported by a generative information bottleneck principle and a Transformer-based model for improved efficiency and multimodal processing.


<details>
  <summary>Details</summary>
Motivation: To create a unified framework for communication that reduces computational complexity while enhancing efficiency and reliability, particularly in multimodal contexts.

Method: Proposed the GenIB principle for tokenization to preserve critical information and facilitate generation across modalities, addressed modeling challenges with Ï-GenIB, and employed a causal Transformer-based multimodal language model for processing tokens.

Result: Simulation results show UniToCom outperforms baselines under dynamic channel conditions, proving its efficacy and suitability for multimodal understanding and communication.

Conclusion: UniToCom offers scalable, generalizable, and efficient communication solutions, paving the way for next-generation intelligent multimodal communications.

Abstract: This letter proposes UniToCom, a unified token communication paradigm that
treats tokens as the fundamental units for both processing and wireless
transmission. Specifically, to enable efficient token representations, we
propose a generative information bottleneck (GenIB) principle, which
facilitates the learning of tokens that preserve essential information while
supporting reliable generation across multiple modalities. By doing this,
GenIB-based tokenization is conducive to improving the communication efficiency
and reducing computational complexity. Additionally, we develop $\sigma$-GenIB
to address the challenges of variance collapse in autoregressive modeling,
maintaining representational diversity and stability. Moreover, we employ a
causal Transformer-based multimodal large language model (MLLM) at the receiver
to unify the processing of both discrete and continuous tokens under the
next-token prediction paradigm. Simulation results validate the effectiveness
and superiority of the proposed UniToCom compared to baselines under dynamic
channel conditions. By integrating token processing with MLLMs, UniToCom
enables scalable and generalizable communication in favor of multimodal
understanding and generation, providing a potential solution for
next-generation intelligent communications.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [334] [Meteoroid stream identification with HDBSCAN unsupervised clustering algorithm](https://arxiv.org/abs/2507.01501)
*Eloy PeÃ±a-Asensio,Fabio Ferrari*

Main category: astro-ph.EP

TL;DR: The study evaluates the use of the Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) algorithm for meteoroid stream identification and compares it to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Meteoroid stream classification is critical yet challenging due to overlapping clusters and noise, which impact practical missions like ESA's LUMIO.

Method: The HDBSCAN algorithm was applied to features from the CAMS database with varying parameters and compared using clustering performance metrics.

Result: Using geocentric parameters, HDBSCAN successfully identified 39 streams, 21 of which strongly aligned with CAMS classifications, achieving better statistical coherence.

Conclusion: HDBSCAN shows promise as a consistent alternative to traditional methods for identifying meteoroid streams but requires further validation for physical applicability.

Abstract: Accurate identification of meteoroid streams is central to understanding
their origins and evolution. However, overlapping clusters and background noise
hinder classification, an issue amplified for missions such as ESA's LUMIO that
rely on meteor shower observations to infer lunar meteoroid impact parameters.
This study evaluates the performance of the Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) algorithm for unsupervised
meteoroid stream identification, comparing its outcomes with the established
Cameras for All-Sky Meteor Surveillance (CAMS) look-up table method. We analyze
the CAMS Meteoroid Orbit Database v3.0 using three feature vectors: LUTAB (CAMS
geocentric parameters), ORBIT (heliocentric orbital elements), and GEO (adapted
geocentric parameters). HDBSCAN is applied with varying minimum cluster sizes
and two cluster selection methods (eom and leaf). To align HDBSCAN clusters
with CAMS classifications, the Hungarian algorithm determines the optimal
mapping. Clustering performance is assessed via the Silhouette score,
Normalized Mutual Information, and F1 score, with Principal Component Analysis
further supporting the analysis. With the GEO vector, HDBSCAN confirms 39
meteoroid streams, 21 strongly aligning with CAMS. The ORBIT vector identifies
30 streams, 13 with high matching scores. Less active showers pose
identification challenges. The eom method consistently yields superior
performance and agreement with CAMS. Although HDBSCAN requires careful
selection of the minimum cluster size, it delivers robust, internally
consistent clusters and outperforms the look-up table method in statistical
coherence. These results underscore HDBSCAN's potential as a mathematically
consistent alternative for meteoroid stream identification, although further
validation is needed to assess physical validity.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [335] [Advancing Magnetic Materials Discovery -- A structure-based machine learning approach for magnetic ordering and magnetic moment prediction](https://arxiv.org/abs/2507.01913)
*Apoorv Verma,Junaid Jami,Amrita Bhattacharya*

Main category: cond-mat.mtrl-sci

TL;DR: This paper introduces a refined descriptor for predicting magnetic properties using only structural information, achieving high accuracy across diverse materials.


<details>
  <summary>Details</summary>
Motivation: The goal is to improve the prediction of magnetic behavior, a complex challenge, to accelerate the discovery of next-generation magnetic materials.

Method: The study leverages an enriched elemental vector representation, LightGBM-based modeling, nonlinear terms, and reduced matrix sparsity for predicting magnetic properties.

Result: The model achieves 82.4% accuracy for magnetic ordering classification, balanced recall across FM and FiM classes, and a correlation coefficient of 0.93 for predicting magnetic moment per atom; it also estimates formation energy per atom accurately.

Conclusion: The approach offers a generalized, computationally efficient framework for screening magnetic materials, enhancing material design and discovery processes.

Abstract: Accurately predicting magnetic behavior across diverse materials systems
remains a longstanding challenge due to the complex interplay of structural and
electronic factors and is pivotal for the accelerated discovery and design of
next-generation magnetic materials. In this work, a refined descriptor is
proposed that significantly improves the prediction of two critical magnetic
properties -- magnetic ordering (Ferromagnetic vs. Ferrimagnetic) and magnetic
moment per atom -- using only the structural information of materials. Unlike
previous models limited to Mn-based or lanthanide-transition metal compounds,
the present approach generalizes across a diverse dataset of 5741 stable,
binary and ternary, ferromagnetic and ferrimagnetic compounds sourced from the
Materials Project. Leveraging an enriched elemental vector representation and
advanced feature engineering, including nonlinear terms and reduced matrix
sparsity, the LightGBM-based model achieves an accuracy of 82.4% for magnetic
ordering classification and balanced recall across FM and FiM classes,
addressing a key limitation in prior studies. The model predicts magnetic
moment per atom with a correlation coefficient of 0.93, surpassing the Hund's
matrix and orbital field matrix descriptors. Additionally, it accurately
estimates formation energy per atom, enabling assessment of both magnetic
behavior and material stability. This generalized and computationally efficient
framework offers a robust tool for high-throughput screening of magnetic
materials with tailored properties.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [336] [Characterizing control between interacting subsystems with deep Jacobian estimation](https://arxiv.org/abs/2507.01946)
*Adam J. Eisen,Mitchell Ostrow,Sarthak Chandra,Leo Kozachkov,Earl K. Miller,Ila R. Fiete*

Main category: q-bio.QM

TL;DR: The paper proposes the JacobianODE, a nonlinear control-theoretic deep learning framework to understand subsystem interactions in complex systems, outperforming existing methods and demonstrating its viability in both analysis and control of high-dimensional dynamics.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve upon linear methods for understanding control in dynamical subsystem interactions, which inadequately represent nonlinear complexities and contextual modulation inherent in biological systems.

Method: The authors introduce the JacobianODE, a deep learning-based method to directly estimate Jacobians from time-series data. This method is tailored for analyzing arbitrary dynamical systems by leveraging the theoretical properties of Jacobians.

Result: The JacobianODE outperforms existing methods in estimating Jacobians for difficult systems such as high-dimensional chaos. The method is applied to a recurrent neural network performing a working memory task, revealing insights into learning-driven control changes between neural areas.

Conclusion: The study establishes a robust nonlinear framework for analyzing and controlling interactions in biological subsystems, paving the way for deeper insights into complex systems and precise behavioral manipulation of modeled dynamics.

Abstract: Biological function arises through the dynamical interactions of multiple
subsystems, including those between brain areas, within gene regulatory
networks, and more. A common approach to understanding these systems is to
model the dynamics of each subsystem and characterize communication between
them. An alternative approach is through the lens of control theory: how the
subsystems control one another. This approach involves inferring the
directionality, strength, and contextual modulation of control between
subsystems. However, methods for understanding subsystem control are typically
linear and cannot adequately describe the rich contextual effects enabled by
nonlinear complex systems. To bridge this gap, we devise a data-driven
nonlinear control-theoretic framework to characterize subsystem interactions
via the Jacobian of the dynamics. We address the challenge of learning
Jacobians from time-series data by proposing the JacobianODE, a deep learning
method that leverages properties of the Jacobian to directly estimate it for
arbitrary dynamical systems from data alone. We show that JacobianODEs
outperform existing Jacobian estimation methods on challenging systems,
including high-dimensional chaos. Applying our approach to a multi-area
recurrent neural network (RNN) trained on a working memory selection task, we
show that the "sensory" area gains greater control over the "cognitive" area
over learning. Furthermore, we leverage the JacobianODE to directly control the
trained RNN, enabling precise manipulation of its behavior. Our work lays the
foundation for a theoretically grounded and data-driven understanding of
interactions among biological subsystems.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [337] [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](https://arxiv.org/abs/2507.01599)
*Zhaoyan Sun,Jiayi Wang,Xinyang Zhao,Jiachi Wang,Guoliang Li*

Main category: cs.DB

TL;DR: The paper proposes 'Data Agents' as advanced architectures using LLMs to orchestrate Data+AI ecosystems more effectively by integrating semantic understanding, reasoning, and planning.


<details>
  <summary>Details</summary>
Motivation: Current Data+AI systems rely on human experts for pipeline orchestration due to their limited semantic understanding, reasoning, and planning capabilities. This creates inefficiencies as systems struggle to adapt to dynamic tasks, data, and environments.

Method: The authors propose a novel 'Data Agent' architecture designed to autonomously handle tasks such as semantic understanding, reasoning, pipeline orchestration, optimization, execution, and self-reflection.

Result: Examples of Data Agent systems, including data science agents, analytics agents, and DBA agents, are provided to illustrate their applicability. Challenges in designing such systems are also discussed.

Conclusion: Integrating LLM techniques into Data+AI systems via 'Data Agents' has the potential to revolutionize the field, although several open challenges remain.

Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [338] [AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma](https://arxiv.org/abs/2507.01081)
*Megan T. deBettencourt,Sruthi Sakthivel,Emily A. Holmes,Mark Chevillet*

Main category: cs.HC

TL;DR: This paper tests ANTIDOTE, a scalable AI and pupillometry-based digital treatment for trauma, showing it reduces intrusive memories effectively.


<details>
  <summary>Details</summary>
Motivation: The paper aims to address the global issue of trauma prevalence by exploring scalable, AI-guided digital treatments that eliminate the need for human guidance.

Method: The study tested the AI-guided digital treatment ANTIDOTE on 100 participants exposed to traumatic videos. It combined Imagery Competing Task Intervention (ICTI) with pupillometry for intervention delivery and engagement tracking.

Result: Participants receiving the intervention reported significantly fewer intrusive memories compared to a control group. Pupil size also indicated engagement and predicted symptom reduction.

Conclusion: The study demonstrates the potential of AI-guided digital interventions like ANTIDOTE to address global trauma issues by scaling evidence-based treatments effectively.

Abstract: Trauma prevalence is vast globally. Evidence-based digital treatments can
help, but most require human guidance. Human guides provide tailored
instructions and responsiveness to internal cognitive states, but limit
scalability. Can generative AI and neurotechnology provide a scalable
alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to
automatically deliver and monitor an evidence-based digital treatment,
specifically the Imagery Competing Task Intervention (ICTI), to reduce
intrusive memories after psychological trauma. One hundred healthy volunteers
were exposed to videos of traumatic events and randomly assigned to an
intervention or active control condition. As predicted, intervention
participants reported significantly fewer intrusive memories over the following
week. Post-hoc assessment against clinical rubrics confirmed the AI guide
delivered the intervention successfully. Additionally, pupil size tracked
intervention engagement and predicted symptom reduction, providing a candidate
biomarker of intervention effectiveness. These findings open a path toward
rigorous AI-guided digital interventions that can scale to trauma prevalence.

</details>


### [339] [AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance](https://arxiv.org/abs/2507.01274)
*Vishakha Lall,Yisi Liu*

Main category: cs.HC

TL;DR: This study introduces an AI-driven framework to objectively assess maritime trainees using visual focus tracking, speech recognition, and stress detection, showing high accuracy rates (~90%) in simulated high-stress scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional maritime training relies on subjective trainer evaluations, which face challenges like bias, difficulty in quantifying behaviors, and cognitive limitations. This motivated the development of an AI-based system for objective assessments.

Method: The study used AI techniques including eye tracking, computer vision, speech recognition with specialized maritime models, natural language processing for communication analysis, and stress detection through vocal pitch evaluation.

Result: The AI framework demonstrated high accuracy rates (~92% for visual detection, ~91% for speech recognition, and ~90% for stress detection), surpassing existing benchmarks.

Conclusion: The research highlights how AI can revolutionize maritime simulation training by enabling objective performance analysis, personalized feedback, and better readiness for operational challenges.

Abstract: Traditional simulator-based training for maritime professionals is critical
for ensuring safety at sea but often depends on subjective trainer assessments
of technical skills, behavioral focus, communication, and body language, posing
challenges such as subjectivity, difficulty in measuring key features, and
cognitive limitations. Addressing these issues, this study develops an
AI-driven framework to enhance maritime training by objectively assessing
trainee performance through visual focus tracking, speech recognition, and
stress detection, improving readiness for high-risk scenarios. The system
integrates AI techniques, including visual focus determination using eye
tracking, pupil dilation analysis, and computer vision; communication analysis
through a maritime-specific speech-to-text model and natural language
processing; communication correctness using large language models; and mental
stress detection via vocal pitch. Models were evaluated on data from simulated
maritime scenarios with seafarers exposed to controlled high-stress events. The
AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for
maritime speech recognition, and ~90% for stress detection, surpassing existing
benchmarks. The system provides insights into visual attention, adherence to
communication checklists, and stress levels under demanding conditions. This
study demonstrates how AI can transform maritime training by delivering
objective performance analytics, enabling personalized feedback, and improving
preparedness for real-world operational challenges.

</details>


### [340] [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)
*Wen Zhan,Ziqun Hua,Peiyue Lin,Yunfei Chen*

Main category: cs.HC

TL;DR: The paper uses AI-assisted co-creation workshops to help older adults express personal narratives via oral storytelling and Hanzi reconstruction, offering a novel perspective on aging and human-AI collaboration.


<details>
  <summary>Details</summary>
Motivation: To enable aging migrants in urban China to share fragmented and underrepresented personal narratives through innovative AI-assisted methods.

Method: Conducted workshops combining oral storytelling and Hanzi reconstruction using AI-suggested Xiaozhuan glyphs and physical materials, facilitated by humans.

Result: Participants shared migration memories and created new character forms through symbolic and interactive processes without needing digital literacy.

Conclusion: The approach repositions AI as a supportive tool rather than a content producer, enhancing narrative agency and introducing new perspectives on aging and human-AI collaboration.

Abstract: This paper explores how older adults, particularly aging migrants in urban
China, can engage AI-assisted co-creation to express personal narratives that
are often fragmented, underrepresented, or difficult to verbalize. Through a
pilot workshop combining oral storytelling and the symbolic reconstruction of
Hanzi, participants shared memories of migration and recreated new character
forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),
together with physical materials. Supported by human facilitation and a soft AI
presence, participants transformed lived experience into visual and tactile
expressions without requiring digital literacy. This approach offers new
perspectives on human-AI collaboration and aging by repositioning AI not as a
content producer but as a supportive mechanism, and by supporting narrative
agency within sociotechnical systems.

</details>


### [341] [Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America](https://arxiv.org/abs/2507.01719)
*Dorian Peters,Fernanda Espinoza,Marco da Re,Guido Ivetta,Luciana Benotti,Rafael A. Calvo*

Main category: cs.HC

TL;DR: The paper examines how conversational AI (CAI) can become culturally and linguistically appropriate in health contexts by relying on localized, human-centered data, particularly in Latin America. It introduces a framework called 'Pluriversal Conversational AI.'


<details>
  <summary>Details</summary>
Motivation: To address the exclusion of global lived experiences by current large language models (LLMs), and ensure CAI performs effectively in culturally diverse and underrepresented regions.

Method: The study used a bottom-up, qualitative approach, gathering data through participatory workshops held in Latin America to understand cultural misalignments, regional views on chatbots, and strategies for culturally-appropriate CAI.

Result: Findings reveal that cultural boundaries become meaningless in practical settings due to their intertwining with economic, political, and logistical factors. The research emphasizes that relationality and tolerance in CAI may be as critical as expanding datasets.

Conclusion: The proposed 'Pluriversal Conversational AI for Health' framework suggests that CAI development must consider complex socio-cultural entanglements beyond adding more data, focusing instead on relational and tolerant technologies for better inclusivity.

Abstract: There is justifiable interest in leveraging conversational AI (CAI) for
health across the majority world, but to be effective, CAI must respond
appropriately within culturally and linguistically diverse contexts. Therefore,
we need ways to address the fact that current LLMs exclude many lived
experiences globally. Various advances are underway which focus on top-down
approaches and increasing training data. In this paper, we aim to complement
these with a bottom-up locally-grounded approach based on qualitative data
collected during participatory workshops in Latin America. Our goal is to
construct a rich and human-centred understanding of: a) potential areas of
cultural misalignment in digital health; b) regional perspectives on chatbots
for health and c)strategies for creating culturally-appropriate CAI; with a
focus on the understudied Latin American context. Our findings show that
academic boundaries on notions of culture lose meaning at the ground level and
technologies will need to engage with a broader framework; one that
encapsulates the way economics, politics, geography and local logistics are
entangled in cultural experience. To this end, we introduce a framework for
'Pluriversal Conversational AI for Health' which allows for the possibility
that more relationality and tolerance, rather than just more data, may be
called for.

</details>


### [342] [Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents](https://arxiv.org/abs/2507.01862)
*Sanjay Krishna Anbalagan,Xinrui Nie,Umesh Mohan,Vijay Kumar Kanamarlapudi,Anughna Kommalapati,Xiaodan Zhao*

Main category: cs.HC

TL;DR: This paper addresses challenges in applying GUI-like explicit actions in conversational chatbots by proposing acknowledgment and reset tasks modeled in Language Model prompts, improving coherence and user experience.


<details>
  <summary>Details</summary>
Motivation: To reduce confusion and improve task clarity in domain-specific chatbots during multi-step workflows, which traditional GUI methods handle better.

Method: Integrating GUI-inspired actions like acknowledgment (commit) and reset (discard) into structured Language Model prompts to align chatbot tasks with clear backend logic.

Result: Demonstrated improvements in task coherence, user satisfaction, and efficiency in hotel booking and customer management chatbot scenarios.

Conclusion: Aligning GUI metaphors with conversational LLM prompts minimizes confusion, enhances clarity, and improves multi-turn interaction efficiency.

Abstract: Domain specific chatbot applications often involve multi step interactions,
such as refining search filters, selecting multiple items, or performing
comparisons. Traditional graphical user interfaces (GUIs) handle these
workflows by providing explicit "Submit" (commit data) and "Reset" (discard
data) actions, allowing back-end systems to track user intent unambiguously. In
contrast, conversational agents rely on subtle language cues, which can lead to
confusion and incomplete context management. This paper proposes modeling these
GUI inspired metaphors acknowledgment (submit like) and context switching
(reset-like) as explicit tasks within large language model (LLM) prompts. By
capturing user acknowledgment, reset actions, and chain of thought (CoT)
reasoning as structured session data, we preserve clarity, reduce user
confusion, and align domain-specific chatbot interactions with back-end logic.
We demonstrate our approach in hotel booking and customer management scenarios,
highlighting improvements in multi-turn task coherence, user satisfaction, and
efficiency.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [343] [Can Argus Judge Them All? Comparing VLMs Across Domains](https://arxiv.org/abs/2507.01042)
*Harsh Joshi,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Niharika Jain,Sarthak Jain,Jiechao Gao,Usman Naseem*

Main category: cs.IR

TL;DR: The paper evaluates Vision-Language Models (VLMs) for consistency across tasks and datasets. It introduces a new Cross-Dataset Consistency (CDC) metric.


<details>
  <summary>Details</summary>
Motivation: To analyze the trade-offs in VLM performance across tasks and datasets, aiming to improve their robustness and versatility for industrial applications.

Method: The paper benchmarks three VLMsâCLIP, BLIP, and LXMERTâusing metrics like task accuracy, generation quality, efficiency, and the novel CDC metric.

Result: CLIP has the best generalization (CDC: 0.92), BLIP performs well on curated datasets, and LXMERT excels in structured reasoning tasks.

Conclusion: The study highlights performance trade-offs among VLMs, offering insights for building more task-flexible and robust architectures for practical use.

Abstract: Vision-Language Models (VLMs) are advancing multimodal AI, yet their
performance consistency across tasks is underexamined. We benchmark CLIP, BLIP,
and LXMERT across diverse datasets spanning retrieval, captioning, and
reasoning. Our evaluation includes task accuracy, generation quality,
efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows
strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT
leads in structured reasoning. These results expose trade-offs between
generalization and specialization, informing industrial deployment of VLMs and
guiding development toward robust, task-flexible architectures.

</details>


### [344] [Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis](https://arxiv.org/abs/2507.01053)
*Rafi Al Attrach,Pedro Moreira,Rajna Fani,Renato Umeton,Leo Anthony Celi*

Main category: cs.IR

TL;DR: M3 simplifies access to the complex MIMIC-IV clinical database by using natural language queries, enabling rapid and reproducible data analysis for researchers.


<details>
  <summary>Details</summary>
Motivation: MIMIC-IV, a vast open-source EHR database, offers immense research potential but remains underutilized due to the technical demands of querying and understanding clinical data.

Method: M3 integrates with MIMIC-IV, utilizing a local SQLite instance or hosted BigQuery and a Model Context Protocol (MCP). Researchers use plain English to ask clinical questions, which are translated into SQL, executed, and returned with results.

Result: Demonstrations show M3 enabling nuanced cohort analyses in minutes, greatly reducing the need for extensive SQL programming and deep clinical knowledge.

Conclusion: M3 democratizes access to MIMIC-IV by significantly lowering technical barriers, fostering broader research engagement, and accelerating medical insights.

Abstract: As ever-larger clinical datasets become available, they have the potential to
unlock unprecedented opportunities for medical research. Foremost among them is
Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest
open-source EHR database. However, the inherent complexity of these datasets,
particularly the need for sophisticated querying skills and the need to
understand the underlying clinical settings, often presents a significant
barrier to their effective use. M3 lowers the technical barrier to
understanding and querying MIMIC-IV data. With a single command it retrieves
MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the
hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers
converse with the database in plain English. Ask a clinical question in natural
language; M3 uses a language model to translate it into SQL, executes the query
against the MIMIC-IV dataset, and returns structured results alongside the
underlying query for verifiability and reproducibility. Demonstrations show
that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that
once demanded hours of handcrafted SQL and relied on understanding the
complexities of clinical workflows. By simplifying access, M3 invites the
broader research community to mine clinical critical-care data and accelerates
the translation of raw records into actionable insight.

</details>


### [345] [A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval](https://arxiv.org/abs/2507.01058)
*Puspendu Banerjee,Aritra Mazumdar,Wazib Ansar,Saptarsi Goswami,Amlan Chakrabarti*

Main category: cs.IR

TL;DR: The paper presents a framework using Data Science techniques, including LLM and RAG, to enhance the analysis of Calcutta High Court verdicts. It focuses on summarization and case retrieval.


<details>
  <summary>Details</summary>
Motivation: The judiciary faces an increasing load of legal issues, necessitating the efficient use of resources.

Method: The framework employs fine-tuned Pegasus models for summarization and creates a vector database for RAG to retrieve similar cases.

Result: The system effectively summarizes complex legal texts and retrieves similar cases, aiding legal research and decision-making.

Conclusion: The developed framework improves efficiency in legal research, benefiting professionals and students with better access to condensed, relevant legal information.

Abstract: The judiciary, as one of democracy's three pillars, is dealing with a rising
amount of legal issues, needing careful use of judicial resources. This
research presents a complex framework that leverages Data Science
methodologies, notably Large Language Models (LLM) and Retrieval-Augmented
Generation (RAG) techniques, to improve the efficiency of analyzing Calcutta
High Court verdicts. Our framework focuses on two key aspects: first, the
creation of a robust summarization mechanism that distills complex legal texts
into concise and coherent summaries; and second, the development of an
intelligent system for retrieving similar cases, which will assist legal
professionals in research and decision making. By fine-tuning the Pegasus model
using case head note summaries, we achieve significant improvements in the
summarization of legal cases. Our two-step summarizing technique preserves
crucial legal contexts, allowing for the production of a comprehensive vector
database for RAG. The RAG-powered framework efficiently retrieves similar cases
in response to user queries, offering thorough overviews and summaries. This
technique not only improves legal research efficiency, but it also helps legal
professionals and students easily acquire and grasp key legal information,
benefiting the overall legal scenario.

</details>


### [346] [FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations](https://arxiv.org/abs/2507.01063)
*Madhav Kotecha*

Main category: cs.IR

TL;DR: The study critiques current online dating algorithms for biases and proposes a superior framework with improved performance.


<details>
  <summary>Details</summary>
Motivation: Current dating algorithms exhibit biases like popularity effects, filter bubbles, and insufficient reciprocity modeling, limiting fairness and effectiveness.

Method: The paper integrates analysis of existing algorithms (reciprocal recommendation frameworks, fairness metrics) and proposes a new mathematical framework utilizing enhanced similarity measures, multi-objective optimization, and fairness-aware algorithms.

Result: Current systems achieve modest performance (collaborative filtering: 25.1%, reciprocal methods: 28.7%). The proposed framework delivers competitive accuracy with better demographic fairness.

Conclusion: The proposed system demonstrates the potential to mitigate biases, improve fairness, and enhance the effectiveness of dating recommendation systems.

Abstract: Online dating platforms have fundamentally transformed the formation of
romantic relationships, with millions of users worldwide relying on algorithmic
matching systems to find compatible partners. However, current recommendation
systems in dating applications suffer from significant algorithmic
deficiencies, including but not limited to popularity bias, filter bubble
effects, and inadequate reciprocity modeling that limit effectiveness and
introduce harmful biases. This research integrates foundational work with
recent empirical findings to deliver a detailed analysis of dating app
recommendation systems, highlighting key issues and suggesting research-backed
solutions. Through analysis of reciprocal recommendation frameworks, fairness
evaluation metrics, and industry implementations, we demonstrate that current
systems achieve modest performance with collaborative filtering reaching 25.1\%
while reciprocal methods achieve 28.7\%. Our proposed mathematical framework
addresses these limitations through enhanced similarity measures,
multi-objective optimization, and fairness-aware algorithms that maintain
competitive accuracy while improving demographic representation to reduce
algorithmic bias.

</details>


### [347] [Cohort Retrieval using Dense Passage Retrieval](https://arxiv.org/abs/2507.01049)
*Pranav Jadhav*

Main category: cs.IR

TL;DR: The paper applies Dense Passage Retrieval (DPR) to cohort retrieval in echocardiography, showcasing its effectiveness in identifying patient groups using unstructured EHR data.


<details>
  <summary>Details</summary>
Motivation: Accurate retrieval of patient cohorts in echocardiography is essential to improve clinical outcomes and streamline research.

Method: The researchers transform unstructured echocardiographic EHR data into Query-Passage datasets, frame the task as cohort retrieval, and implement custom evaluation metrics inspired by clinical scenarios. A custom-trained DPR embedding model was developed.

Result: The custom-trained DPR model outperformed traditional and off-the-shelf state-of-the-art methods in patient cohort retrieval tasks.

Conclusion: This study pioneers the use of DPR for patient cohort retrieval in echocardiography and introduces a reusable framework for broader medical applications.

Abstract: Patient cohort retrieval is a pivotal task in medical research and clinical
practice, enabling the identification of specific patient groups from extensive
electronic health records (EHRs). In this work, we address the challenge of
cohort retrieval in the echocardiography domain by applying Dense Passage
Retrieval (DPR), a prominent methodology in semantic search. We propose a
systematic approach to transform an echocardiographic EHR dataset of
unstructured nature into a Query-Passage dataset, framing the problem as a
Cohort Retrieval task. Additionally, we design and implement evaluation metrics
inspired by real-world clinical scenarios to rigorously test the models across
diverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding
model that demonstrates superior performance compared to traditional and
off-the-shelf SOTA methods.To our knowledge, this is the first work to apply
DPR for patient cohort retrieval in the echocardiography domain, establishing a
framework that can be adapted to other medical domains.

</details>


### [348] [Enhanced Influence-aware Group Recommendation for Online Media Propagation](https://arxiv.org/abs/2507.01616)
*Chengkun He,Xiangmin Zhou,Chen Wang,Longbing Cao,Jie Shao,Xiaodong Li,Guang Xu,Carrie Jinqiu Hu,Zahir Tari*

Main category: cs.IR

TL;DR: This paper introduces the Enhanced Influence-aware Group Recommendation (EIGR) framework that incorporates sampling strategies, dynamic propagation models, and indexing techniques for efficient and accurate group recommendations.


<details>
  <summary>Details</summary>
Motivation: The goal of this paper is to address challenges in group recommendation systems, particularly the scalability of social graphs, dynamics of influence propagation, and computational efficiency for real-time recommendations.

Method: The framework includes three key innovations: a Graph Extraction-based Sampling (GES) strategy, a DYnamic Independent Cascade (DYIC) model for influence prediction, and a two-level hash-based User Group Index (UG-Index) for efficient organization and matching.

Result: Experiments on real-world datasets demonstrate that EIGR surpasses existing methods in both recommendation effectiveness and computational efficiency.

Conclusion: EIGR successfully enhances influence-aware group recommendation systems by leveraging novel strategies and models, providing better scalability and real-time performance.

Abstract: Group recommendation over social media streams has attracted significant
attention due to its wide applications in domains such as e-commerce,
entertainment, and online news broadcasting. By leveraging social connections
and group behaviours, group recommendation (GR) aims to provide more accurate
and engaging content to a set of users rather than individuals. Recently,
influence-aware GR has emerged as a promising direction, as it considers the
impact of social influence on group decision-making. In earlier work, we
proposed Influence-aware Group Recommendation (IGR) to solve this task.
However, this task remains challenging due to three key factors: the large and
ever-growing scale of social graphs, the inherently dynamic nature of influence
propagation within user groups, and the high computational overhead of
real-time group-item matching.
  To tackle these issues, we propose an Enhanced Influence-aware Group
Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based
Sampling (GES) strategy to minimise redundancy across multiple temporal social
graphs and effectively capture the evolving dynamics of both groups and items.
Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict
how influence propagates over time across social items and user groups.
Finally, we develop a two-level hash-based User Group Index (UG-Index) to
efficiently organise user groups and enable real-time recommendation
generation. Extensive experiments on real-world datasets demonstrate that our
proposed framework, EIGR, consistently outperforms state-of-the-art baselines
in both effectiveness and efficiency.

</details>


### [349] [Embedding-based Retrieval in Multimodal Content Moderation](https://arxiv.org/abs/2507.01066)
*Hanzhong Liang,Jinghao Shi,Xiang Shen,Zixuan Wang,Vera Wen,Ardalan Mehrani,Zhiqian Chen,Yifan Wu,Zhixin Zhang*

Main category: cs.IR

TL;DR: The paper introduces an Embedding-Based Retrieval (EBR) method for content moderation on short video platforms, outperforming traditional classification in trend adaptation and urgent needs.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional classification approaches in providing rapid responses needed for content moderation, especially during emergent trends and escalations.

Method: The study utilizes Supervised Contrastive Learning to develop single-modal and multi-modal foundation embedding models and integrates them into an efficient embedding-based retrieval system.

Result: The EBR method improves ROC-AUC from 0.85 to 0.99, PR-AUC from 0.35 to 0.95 in offline evaluations. Online tests show a 10.32% increase in action rates, over 80% reduction in operational costs, and better interpretability compared to alternatives.

Conclusion: EBR proves to be an efficient, cost-effective, and interpretable alternative to classification-based solutions, emphasizing its potential for rapid and adaptable content moderation.

Abstract: Video understanding plays a fundamental role for content moderation on short
video platforms, enabling the detection of inappropriate content. While
classification remains the dominant approach for content moderation, it often
struggles in scenarios requiring rapid and cost-efficient responses, such as
trend adaptation and urgent escalations. To address this issue, we introduce an
Embedding-Based Retrieval (EBR) method designed to complement traditional
classification approaches. We first leverage a Supervised Contrastive Learning
(SCL) framework to train a suite of foundation embedding models, including both
single-modal and multi-modal architectures. Our models demonstrate superior
performance over established contrastive learning methods such as CLIP and
MoCo. Building on these embedding models, we design and implement the
embedding-based retrieval system that integrates embedding generation and video
retrieval to enable efficient and effective trend handling. Comprehensive
offline experiments on 25 diverse emerging trends show that EBR improves
ROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online
experiments reveal that EBR increases action rates by 10.32% and reduces
operational costs by over 80%, while also enhancing interpretability and
flexibility compared to classification-based solutions.

</details>


### [350] [Optimizing Conversational Product Recommendation via Reinforcement Learning](https://arxiv.org/abs/2507.01060)
*Kang Liu*

Main category: cs.IR

TL;DR: The paper presents a reinforcement learning approach to improve conversational strategies for product recommendation.


<details>
  <summary>Details</summary>
Motivation: Organizations rely on intelligent agents for sales and service operations, creating a need for optimized conversational strategies that enhance product engagement and uptake.

Method: Leverage reinforcement learning to train agentic systems in adapting dialogue policies based on behavioral patterns and conversion outcomes.

Result: The approach optimizes talk tracks to increase engagement and product uptake while maintaining contextual and regulatory compliance.

Conclusion: Scalable and personalized recommendation strategies in enterprise environments can be achieved through reinforcement learning-driven conversational optimization.

Abstract: We propose a reinforcement learning-based approach to optimize conversational
strategies for product recommendation across diverse industries. As
organizations increasingly adopt intelligent agents to support sales and
service operations, the effectiveness of a conversation hinges not only on what
is recommended but how and when recommendations are delivered. We explore a
methodology where agentic systems learn optimal dialogue policies through
feedback-driven reinforcement learning. By mining aggregate behavioral patterns
and conversion outcomes, our approach enables agents to refine talk tracks that
drive higher engagement and product uptake, while adhering to contextual and
regulatory constraints. We outline the conceptual framework, highlight key
innovations, and discuss the implications for scalable, personalized
recommendation in enterprise environments.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [351] [Imitation Learning for Satellite Attitude Control under Unknown Perturbations](https://arxiv.org/abs/2507.01161)
*Zhizhuo Zhang,Hao Peng,Xiaoli Bai*

Main category: eess.SY

TL;DR: The paper introduces a novel approach combining Soft Actor-Critic (SAC) with Generative Adversarial Imitation Learning (GAIL) for advanced satellite attitude control against perturbations.


<details>
  <summary>Details</summary>
Motivation: Satellite control systems face challenges such as actuator failures, sensor noise, and uncertainties in external perturbations, where traditional methods relying on precision models may fail.

Method: The method integrates SAC reinforcement learning to develop a resilient expert controller, followed by GAIL to train a learner policy that imitates the expertâs actions, improving generalization while reducing training costs.

Result: The SAC expert demonstrated robust control in challenging conditions, while the GAIL learner effectively mimicked the expertâs trajectories, reducing sample complexity.

Conclusion: Combining SAC and GAIL provides a more efficient and effective framework for intelligent and autonomous satellite attitude control, with strong generalization and reduced training demands.

Abstract: This paper presents a novel satellite attitude control framework that
integrates Soft Actor-Critic (SAC) reinforcement learning with Generative
Adversarial Imitation Learning (GAIL) to achieve robust performance under
various unknown perturbations. Traditional control techniques often rely on
precise system models and are sensitive to parameter uncertainties and external
perturbations. To overcome these limitations, we first develop a SAC-based
expert controller that demonstrates improved resilience against actuator
failures, sensor noise, and attitude misalignments, outperforming our previous
results in several challenging scenarios. We then use GAIL to train a learner
policy that imitates the expert's trajectories, thereby reducing training costs
and improving generalization through expert demonstrations. Preliminary
experiments under single and combined perturbations show that the SAC expert
can rotate the antenna to a specified direction and keep the antenna
orientation reliably stable in most of the listed perturbations. Additionally,
the GAIL learner can imitate most of the features from the trajectories
generated by the SAC expert. Comparative evaluations and ablation studies
confirm the effectiveness of the SAC algorithm and reward shaping. The
integration of GAIL further reduces sample complexity and demonstrates
promising imitation capabilities, paving the way for more intelligent and
autonomous spacecraft control systems.

</details>


### [352] [Cooperative Target Capture in 3D Engagements over Switched Dynamic Graphs](https://arxiv.org/abs/2507.01350)
*Abhinav Sinha,Shashi Ranjan Kumar*

Main category: eess.SY

TL;DR: The paper develops a leaderless cooperative guidance strategy for interceptors without radial acceleration capabilities to achieve simultaneous time-constrained interception of a stationary target, leveraging switched dynamic graphs.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of simultaneous target interception with interceptors that lack radial acceleration capabilities and are limited by kinematic turn constraints, focusing on robust performance in complex 3D engagement scenarios.

Method: The method involves deriving guidance commands using instantaneous optimization under affine constraints in coupled pitch and yaw channels, while incorporating time-to-go uncertainties to ensure robustness and consensus within predefined time limits.

Result: The proposed strategy guarantees time-to-go consensus among interceptors within a preset time, regardless of initial conditions, and improves robustness in dynamic engagement scenarios.

Conclusion: The developed guidance strategy effectively handles coupled 3D engagements and improves interception performance under kinematic constraints, demonstrating robustness and practical applicability through simulations.

Abstract: This paper presents a leaderless cooperative guidance strategy for
simultaneous time-constrained interception of a stationary target when the
interceptors exchange information over switched dynamic graphs. We specifically
focus on scenarios when the interceptors lack radial acceleration capabilities,
relying solely on their lateral acceleration components. This consideration
aligns with their inherent kinematic turn constraints. The proposed strategy
explicitly addresses the complexities of coupled 3D engagements, thereby
mitigating performance degradation that typically arises when the pitch and yaw
channels are decoupled into two separate, mutually orthogonal planar
engagements. Moreover, our formulation incorporates modeling uncertainties
associated with the time-to-go estimation into the derivation of cooperative
guidance commands to ensure robustness against inaccuracies in dynamic
engagement scenarios. To optimize control efficiency, we analytically derive
the lateral acceleration components in the orthogonal pitch and yaw channels by
solving an instantaneous optimization problem, subject to an affine constraint.
We show that the proposed cooperative guidance commands guarantee consensus in
time-to-go values within a predefined time, which can be prescribed as a design
parameter, regardless of the interceptors' initial configurations. We provide
simulations to attest to the efficacy of the proposed method.

</details>


### [353] [Time-Varying Coverage Control: A Distributed Tracker-Planner MPC Framework](https://arxiv.org/abs/2507.01567)
*Patrick Benito Eberhard,Johannes KÃ¶hler,Oliver HÃ¼sser,Melanie N. Zeilinger,Andrea Carron*

Main category: eess.SY

TL;DR: The paper introduces a distributed framework for coordinating multiple agents in dynamic environments, ensuring effective coverage and safety compliance.


<details>
  <summary>Details</summary>
Motivation: Address the challenges in dynamic coverage, including changing regions of interest, nonlinear dynamics, and safety constraints, with applications like autonomous taxis and rescue operations.

Method: Develop a multi-rate framework combining trajectory planning and model predictive control, ensuring coverage under constraints; supports periodic and nonperiodic density functions.

Result: Prove closed-loop convergence for periodic density functions, offer practical algorithm for nonperiodic functions, and validate the method with experiments on miniature race cars.

Conclusion: The approach offers a robust and flexible solution for dynamic coverage challenges, suitable for real-world applications.

Abstract: Time-varying coverage control addresses the challenge of coordinating
multiple agents covering an environment where regions of interest change over
time. This problem has broad applications, including the deployment of
autonomous taxis and coordination in search and rescue operations. The
achievement of effective coverage is complicated by the presence of
time-varying density functions, nonlinear agent dynamics, and stringent system
and safety constraints. In this paper, we present a distributed multi-agent
control framework for time-varying coverage under nonlinear constrained
dynamics. Our approach integrates a reference trajectory planner and a tracking
model predictive control (MPC) scheme, which operate at different frequencies
within a multi-rate framework. For periodic density functions, we demonstrate
closed-loop convergence to an optimal configuration of trajectories and provide
formal guarantees regarding constraint satisfaction, collision avoidance, and
recursive feasibility. Additionally, we propose an efficient algorithm capable
of handling nonperiodic density functions, making the approach suitable for
practical applications. Finally, we validate our method through hardware
experiments using a fleet of four miniature race cars.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [354] [GPU-based complete search for nonlinear minimization subject to bounds](https://arxiv.org/abs/2507.01770)
*Guanglu Zhang,Qihang Shan,Jonathan Cagan*

Main category: math.NA

TL;DR: This paper proposes a GPU-based interval analysis method to rigorously find the global minimum of nonlinear functions efficiently.


<details>
  <summary>Details</summary>
Motivation: Current methods for global optimization struggle to enclose the guaranteed global minimum for complex multimodal functions in high dimensions. GPUs offer computational power that can overcome these challenges.

Method: The authors use interval analysis on GPU architecture to iteratively eliminate infeasible regions while employing parallel programming and a variable cycling technique for efficiency in high-dimensional contexts.

Result: The method successfully found the guaranteed global minimum of 10 challenging benchmark test functions with dimensionalities up to 10,000 using a single GPU efficiently.

Conclusion: By leveraging GPU power and novel design techniques, the proposed method sets a new benchmark for solving complex global optimization problems rigorously and efficiently.

Abstract: This paper introduces a GPU-based complete search method to enclose the
global minimum of a nonlinear function subject to simple bounds on the
variables. Using interval analysis, coupled with the computational power and
architecture of GPU, the method iteratively rules out the regions in the search
domain where the global minimum cannot exist and leaves a finite set of regions
where the global minimum must exist. For effectiveness, because of the rigor of
interval analysis, the method is guaranteed to enclose the global minimum of
the nonlinear function even in the presence of rounding errors. For efficiency,
the method employs a novel GPU-based single program, single data parallel
programming style to circumvent major GPU performance bottlenecks, and a
variable cycling technique is also integrated into the method to reduce
computational cost when minimizing large-scale nonlinear functions. The method
is validated by minimizing 10 multimodal benchmark test functions with scalable
dimensions, including the well-known Ackley function, Griewank function, Levy
function, and Rastrigin function. These benchmark test functions represent
grand challenges of global optimization, and enclosing the guaranteed global
minimum of these benchmark test functions with more than 80 dimensions has not
been reported in the literature. Our method completely searches the feasible
domain and successfully encloses the guaranteed global minimum of these 10
benchmark test functions with up to 10,000 dimensions using only one GPU in a
reasonable computation time, far exceeding the reported results in the
literature due to the unique method design and implementation based on GPU
architecture.

</details>


### [355] [Consistency of Learned Sparse Grid Quadrature Rules using NeuralODEs](https://arxiv.org/abs/2507.01533)
*Hanno Gottschalk,Emil Partow,Tobias J. Riedlinger*

Main category: math.NA

TL;DR: This paper proves the consistency of sparse grid quadrature for high-dimensional numerical integrations by leveraging neural ODE-based transport maps combined with sparse grid quadrature.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of numerical integration in high-dimensional problems by providing a provable framework for error control and consistency.

Method: The approach involves two steps: first, a transport map normalizes the high-dimensional distribution using neural ODEs; second, the Clenshaw-Curtis sparse grid quadrature is applied to integrate the composed function. A total numerical error decomposition is conducted and analyzed using PAC learning theory.

Result: The paper demonstrates that all error terms in the numerical integration process can be controlled with high probability, ensuring the numerical integral approximates the theoretical value accurately as data increases and network capacity adapts.

Conclusion: Sparse grid quadrature, combined with transport maps learned via neural ODEs, provides a theoretically sound approach to high-dimensional numerical integration, ensuring consistency and error control.

Abstract: This paper provides a proof of the consistency of sparse grid quadrature for
numerical integration of high dimensional distributions. In a first step, a
transport map is learned that normalizes the distribution to a noise
distribution on the unit cube. This step is built on the statistical learning
theory of neural ordinary differential equations, which has been established
recently. Secondly, the composition of the generative map with the quantity of
interest is integrated numerically using the Clenshaw-Curtis sparse grid
quadrature. A decomposition of the total numerical error in quadrature error
and statistical error is provided. As main result it is proven in the framework
of empirical risk minimization that all error terms can be controlled in the
sense of PAC (probably approximately correct) learning and with high
probability the numerical integral approximates the theoretical value up to an
arbitrary small error in the limit where the data set size is growing and the
network capacity is increased adaptively.

</details>


### [356] [Neural Entropy-stable conservative flux form neural networks for learning hyperbolic conservation laws](https://arxiv.org/abs/2507.01795)
*Lizuo Liu,Lu Zhang,Anne Gelb*

Main category: math.NA

TL;DR: The paper introduces NESCFN, a neural network that learns hyperbolic conservation laws and entropy functions directly from data without predefined discretization.


<details>
  <summary>Details</summary>
Motivation: To overcome reliance on predefined discretizations and prior knowledge of governing equations for modeling hyperbolic conservation laws.

Method: The paper embeds entropy-stable principles into a neural network, jointly learning numerical flux functions and corresponding entropy in a data-driven approach.

Result: The method ensures stability and conservation over long time periods and accurately models shock propagation without access to future-time solutions during training.

Conclusion: This approach effectively discovers physically consistent dynamics, ensuring critical conservation principles in hyperbolic laws using only data-driven learning.

Abstract: We propose a neural entropy-stable conservative flux form neural network
(NESCFN) for learning hyperbolic conservation laws and their associated entropy
functions directly from solution trajectories, without requiring any predefined
numerical discretization. While recent neural network architectures have
successfully integrated classical numerical principles into learned models,
most rely on prior knowledge of the governing equations or assume a fixed
discretization. Our approach removes this dependency by embedding
entropy-stable design principles into the learning process itself, enabling the
discovery of physically consistent dynamics in a fully data-driven setting. By
jointly learning both the numerical flux function and a corresponding entropy,
the proposed method ensures conservation and entropy dissipation, critical for
long-term stability and fidelity in the system of hyperbolic conservation laws.
Numerical results demonstrate that the method achieves stability and
conservation over extended time horizons and accurately captures shock
propagation speeds, even without oracle access to future-time solution profiles
in the training data.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [357] [Symbolic identification of tensor equations in multidimensional physical fields](https://arxiv.org/abs/2507.01466)
*Tianyi Chen,Hao Yang,Wenjun Ma,Jun Zhang*

Main category: math-ph

TL;DR: The paper introduces SITE, a framework that identifies tensor equations using a new host-plasmid representation and M-GEP inspiration. It improves robustness and accuracy by implementing novel algorithms and validations.


<details>
  <summary>Details</summary>
Motivation: Existing data-driven methods are adept at discovering scalar equations but struggle with tensor relationships critical in advanced physical modeling.

Method: SITE uses a genetic evolutionary approach inspired by M-GEP, combines dimensional homogeneity checks, and employs tensor linear regression for better search efficiency and robustness.

Result: SITE accurately recovers tensor equations in benchmark tests, is robust to noise, and works effectively with small data. It validates its capability by identifying constitutive relations from molecular simulations.

Conclusion: SITE is a promising method for discovering complex tensor equations, showing potential for advancing data-driven scientific discovery across different simulation and experimental settings.

Abstract: Recently, data-driven methods have shown great promise for discovering
governing equations from simulation or experimental data. However, most
existing approaches are limited to scalar equations, with few capable of
identifying tensor relationships. In this work, we propose a general
data-driven framework for identifying tensor equations, referred to as Symbolic
Identification of Tensor Equations (SITE). The core idea of SITE--representing
tensor equations using a host-plasmid structure--is inspired by the
multidimensional gene expression programming (M-GEP) approach. To improve the
robustness of the evolutionary process, SITE adopts a genetic information
retention strategy. Moreover, SITE introduces two key innovations beyond
conventional evolutionary algorithms. First, it incorporates a dimensional
homogeneity check to restrict the search space and eliminate physically invalid
expressions. Second, it replaces traditional linear scaling with a tensor
linear regression technique, greatly enhancing the efficiency of numerical
coefficient optimization. We validate SITE using two benchmark scenarios, where
it accurately recovers target equations from synthetic data, showing robustness
to noise and small sample sizes. Furthermore, SITE is applied to identify
constitutive relations directly from molecular simulation data, which are
generated without reliance on macroscopic constitutive models. It adapts to
both compressible and incompressible flow conditions and successfully
identifies the corresponding macroscopic forms, highlighting its potential for
data-driven discovery of tensor equation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [358] [Semi-supervised learning for linear extremile regression](https://arxiv.org/abs/2507.01314)
*Rong Jiang,Keming Yu,Jiangfeng Wang*

Main category: stat.ME

TL;DR: The paper introduces a new linear extremile regression model with a semi-supervised learning approach, addressing issues in high-dimensional and nonparametric settings.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges associated with nonparametric extremile regression in high dimensions, specifically data sparsity, inefficiency, and overfitting.

Method: A novel linear extremile regression model and estimation methodology are proposed, achieving $\sqrt{n}$-consistency, with semi-supervised learning to enhance efficiency and robustness to model misspecification.

Result: Simulation and real data experiments demonstrate strong finite-sample performance for the proposed methodologies.

Conclusion: The proposed linear extremile regression model and semi-supervised approach provide efficient and robust solutions for analyzing extreme tails in high-dimensional settings.

Abstract: Extremile regression, as a least squares analog of quantile regression, is
potentially useful tool for modeling and understanding the extreme tails of a
distribution. However, existing extremile regression methods, as nonparametric
approaches, may face challenges in high-dimensional settings due to data
sparsity, computational inefficiency, and the risk of overfitting. While linear
regression serves as the foundation for many other statistical and machine
learning models due to its simplicity, interpretability, and relatively easy
implementation, particularly in high-dimensional settings, this paper
introduces a novel definition of linear extremile regression along with an
accompanying estimation methodology. The regression coefficient estimators of
this method achieve $\sqrt{n}$-consistency, which nonparametric extremile
regression may not provide. In particular, while semi-supervised learning can
leverage unlabeled data to make more accurate predictions and avoid overfitting
to small labeled datasets in high-dimensional spaces, we propose a
semi-supervised learning approach to enhance estimation efficiency, even when
the specified linear extremile regression model may be misspecified. Both
simulation studies and real data analyses demonstrate the finite-sample
performance of our proposed methods.

</details>


### [359] [Targeted tuning of random forests for quantile estimation and prediction intervals](https://arxiv.org/abs/2507.01430)
*Matthew Berkowitz,Rachel MacKay Altman,Thomas M. Loughin*

Main category: stat.ME

TL;DR: The paper introduces a tuning method for random forests that enhances quantile accuracy and improves prediction intervals.


<details>
  <summary>Details</summary>
Motivation: Most traditional tuning processes for random forests lead to biased quantile estimates and suboptimal prediction interval performance.

Method: The key method involves minimizing 'quantile coverage loss' (QCL) using the out-of-bag sample to reduce bias in quantile estimates. The procedure is further adapted for censored data.

Result: The proposed approach achieves better quantile coverage probability, reduced MSE for coverage probabilities, and narrower, valid prediction intervals.

Conclusion: QCL tuning aligns well with quantile estimation goals, resulting in superior performance over traditional methods.

Abstract: We present a novel tuning procedure for random forests (RFs) that improves
the accuracy of estimated quantiles and produces valid, relatively narrow
prediction intervals. While RFs are typically used to estimate mean responses
(conditional on covariates), they can also be used to estimate quantiles by
estimating the full distribution of the response. However, standard approaches
for building RFs often result in excessively biased quantile estimates. To
reduce this bias, our proposed tuning procedure minimizes "quantile coverage
loss" (QCL), which we define as the estimated bias of the marginal quantile
coverage probability estimate based on the out-of-bag sample. We adapt QCL
tuning to handle censored data and demonstrate its use with random survival
forests. We show that QCL tuning results in quantile estimates with more
accurate coverage probabilities than those achieved using default parameter
values or traditional tuning (using MSPE for uncensored data and C-index for
censored data), while also reducing the estimated MSE of these coverage
probabilities. We discuss how the superior performance of QCL tuning is linked
to its alignment with the estimation goal. Finally, we explore the validity and
width of prediction intervals created using this method.

</details>


### [360] [Nonparametric learning of heterogeneous graphical model on network-linked data](https://arxiv.org/abs/2507.01473)
*Yuwen Wang,Changyu Liu,Xin He,Junhui Wang*

Main category: stat.ME

TL;DR: A nonparametric graphical model is proposed for heterogeneous network-linked data, integrating network embedding and graphical estimation with theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: Conventional graphical models rely heavily on independent and identically distributed samples and thus struggle with network-linked, more complex datasets.

Method: The method integrates network embedding into nonparametric graphical model estimation, transforming the graph learning task into solving finite-dimensional linear equations using vector-valued reproducing kernel Hilbert space properties.

Result: The method shows estimation consistency, exact recovery of heterogeneous graph structures, and strong performance in simulations and a real-world application to a statistician coauthorship dataset.

Conclusion: The approach expands the applicability of graphical models by supporting heterogeneous and complex graph structures using a nonparametric framework.

Abstract: Graphical models have been popularly used for capturing conditional
independence structure in multivariate data, which are often built upon
independent and identically distributed observations, limiting their
applicability to complex datasets such as network-linked data. This paper
proposes a nonparametric graphical model that addresses these limitations by
accommodating heterogeneous graph structures without imposing any specific
distributional assumptions. The proposed estimation method effectively
integrates network embedding with nonparametric graphical model estimation. It
further transforms the graph learning task into solving a finite-dimensional
linear equation system by leveraging the properties of vector-valued
reproducing kernel Hilbert space. Moreover, theoretical guarantees are
established for the proposed method in terms of the estimation consistency and
exact recovery of the heterogeneous graph structures. Its effectiveness is also
demonstrated through a variety of simulated examples and a real application to
the statistician coauthorship dataset.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [361] [Efficient Gate Reordering for Distributed Quantum Compiling in Data Centers](https://arxiv.org/abs/2507.01090)
*Riccardo Mengoni,Walter Nadalin,Mathys Rennela,Jimmy Rotureau,Tom Darras,Julien Laurat,Eleni Diamanti,Ioannis Lavdas*

Main category: quant-ph

TL;DR: The paper introduces the quantum compiler araQne designed to minimize communication costs in distributed quantum computing.


<details>
  <summary>Details</summary>
Motivation: The shift to quantum computing requires specialized infrastructure and software to efficiently distribute quantum algorithms over interconnected quantum processing units (QPUs) with minimal communication overhead.

Method: The authors describe the araQne quantum compiler, which uses gate teleportation protocols and optimizes circuit reordering strategies to minimize the number of entangled pairs needed for distribution.

Result: The use of circuit reordering significantly reduces the cost of inter-QPU communications compared to baseline methods.

Conclusion: Efficient quantum compilers like araQne are essential for the development of hybrid quantum infrastructure, reducing communication costs and enhancing distributed quantum computing performance.

Abstract: Just as classical computing relies on distributed systems, the quantum
computing era requires new kinds of infrastructure and software tools. Quantum
networks will become the backbone of hybrid, quantum-augmented data centers, in
which quantum algorithms are distributed over a local network of quantum
processing units (QPUs) interconnected via shared entanglement. In this
context, it is crucial to develop methods and software that minimize the number
of inter-QPU communications. Here we describe key features of the quantum
compiler araQne, which is designed to minimize distribution cost, measured by
the number of entangled pairs required to distribute a monolithic quantum
circuit using gate teleportation protocols. We establish the crucial role
played by circuit reordering strategies, which strongly reduce the distribution
cost compared to a baseline approach.

</details>


### [362] [Generative flow-based warm start of the variational quantum eigensolver](https://arxiv.org/abs/2507.01726)
*Hang Zou,Martin Rahm,Anton Frisk Kockum,Simon Olsson*

Main category: quant-ph

TL;DR: The paper introduces Flow-VQE, a framework using generative modeling to improve parameter generation and optimization in variational quantum algorithms, showing significant efficiency improvements in simulations.


<details>
  <summary>Details</summary>
Motivation: Current variational quantum eigensolver (VQE) approaches face challenges due to complex objective functions and costly optimization processes, necessitating innovative methods to make quantum simulations more efficient.

Method: The method integrates conditional normalizing flows with parameterized quantum circuits in VQE. It utilizes generative models with preference-based training, enabling gradient-free optimization and systematic parameter transfer, which facilitates quicker convergence across related problems.

Result: Numerical simulations on molecular systems demonstrate that Flow-VQE surpasses traditional optimization algorithms by achieving computational accuracy with fewer circuit evaluations and accelerating optimization up to 50 times compared to Hartree-Fock initialization.

Conclusion: Flow-VQE is a promising paradigm that leverages generative modeling for quantum optimization, reducing the computational cost of variational quantum algorithms, and shows potential for widespread application in quantum simulations.

Abstract: Hybrid quantum-classical algorithms like the variational quantum eigensolver
(VQE) show promise for quantum simulations on near-term quantum devices, but
are often limited by complex objective functions and expensive optimization
procedures. Here, we propose Flow-VQE, a generative framework leveraging
conditional normalizing flows with parameterized quantum circuits to
efficiently generate high-quality variational parameters. By embedding a
generative model into the VQE optimization loop through preference-based
training, Flow-VQE enables quantum gradient-free optimization and offers a
systematic approach for parameter transfer, accelerating convergence across
related problems through warm-started optimization. We compare Flow-VQE to a
number of standard benchmarks through numerical simulations on molecular
systems, including hydrogen chains, water, ammonia, and benzene. We find that
Flow-VQE outperforms baseline optimization algorithms, achieving computational
accuracy with fewer circuit evaluations (improvements range from modest to more
than two orders of magnitude) and, when used to warm-start the optimization of
new systems, accelerates subsequent fine-tuning by up to 50-fold compared with
Hartree--Fock initialization. Therefore, we believe Flow-VQE can become a
pragmatic and versatile paradigm for leveraging generative modeling to reduce
the costs of variational quantum algorithms.

</details>


### [363] [Analyzing Common Electronic Structure Theory Algorithms for Distributed Quantum Computing](https://arxiv.org/abs/2507.01902)
*Grier M. Jones,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: This paper examines the feasibility of applying distributed quantum computing (DQC) to electronic structure theory methods and finds that current algorithms struggle with efficient parallelization using local operations.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of distributed quantum computing (DQC) for scaling quantum devices to practical applications, particularly in the field of quantum chemistry, which is a leading prospective application for quantum computing.

Method: The authors analyzed five common electronic structure methods, using tools like the Tequila and ffsim packages, in conjunction with the Qiskit Circuit Cutting addon. They investigated the methods' ability to efficiently parallelize using local operations (LO).

Result: The study found that the analyzed algorithms are largely unsuitable for efficient parallelization using local operations, highlighting a significant gap in applying electronic structure theory to DQC.

Conclusion: New algorithms or methodologies need to be developed to enable the distribution of electronic structure theory computations effectively within a DQC framework.

Abstract: To move towards the utility era of quantum computing, many corporations have
posed distributed quantum computing (DQC) as a framework for scaling the
current generation of devices for practical applications. One of these
applications is quantum chemistry, also known as electronic structure theory,
which has been poised as a "killer application" of quantum computing, To this
end, we analyze five electronic structure methods, found in common packages
such as Tequila and ffsim, which can be easily interfaced with the Qiskit
Circuit Cutting addon. Herein, we provide insights into cutting these
algorithms using local operations (LO) to determine their aptitude for
distribution. The key findings of our work are that many of these algorithms
cannot be efficiently parallelized using LO, and new methods must be developed
to apply electronic structure theory within a DQC framework.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [364] [Cross-Attention Message-Passing Transformers for Code-Agnostic Decoding in 6G Networks](https://arxiv.org/abs/2507.01038)
*Seong-Joon Park,Hee-Youl Kwak,Sang-Hyo Kim,Yongjune Kim,Jong-Seon No*

Main category: cs.IT

TL;DR: This paper introduces an AI-native decoding framework using the transformer architecture for 6G channel coding, achieving flexibility, scalability, and superior performance across diverse code types.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional code-specific decoders, which lack flexibility and scalability to meet the diverse requirements of 6G networks.

Method: Proposes the CrossMPT (cross-attention message-passing transformer) for decoding, which iteratively updates input representations to learn decoding problems effectively. Enhances it into FCrossMPT for broader code adaptation and introduces CrossED for short blocklength performance using ensemble decoding.

Result: Achieves state-of-the-art single neural decoder performance, broad applicability across different codes, and improved results for short blocklength codes with the ensemble approach.

Conclusion: The proposed AI-native, code-agnostic decoder using transformer architecture is a flexible, scalable, and high-performing solution for channel coding in 6G networks.

Abstract: Channel coding for 6G networks is expected to support a wide range of
requirements arising from heterogeneous communication scenarios. These demands
challenge traditional code-specific decoders, which lack the flexibility and
scalability required for next-generation systems. To tackle this problem, we
propose an AI-native foundation model for unified and code-agnostic decoding
based on the transformer architecture. We first introduce a cross-attention
message-passing transformer (CrossMPT). CrossMPT employs two masked
cross-attention blocks that iteratively update two distinct input
representations-magnitude and syndrome vectors-allowing the model to
effectively learn the decoding problem. Notably, our CrossMPT has achieved
state-of-the-art decoding performance among single neural decoders. Building on
this, we develop foundation CrossMPT (FCrossMPT) by making the architecture
invariant to code length, rate, and class, allowing a single trained model to
decode a broad range of codes without retraining. To further enhance decoding
performance, particularly for short blocklength codes, we propose CrossMPT
ensemble decoder (CrossED), an ensemble decoder composed of multiple parallel
CrossMPT blocks employing different parity-check matrices. This architecture
can also serve as a foundation model, showing strong generalization across
diverse code types. Overall, the proposed AI-native code-agnostic decoder
offers flexibility, scalability, and high performance, presenting a promising
direction to channel coding for 6G networks.

</details>


### [365] [A Hybrid Ensemble Learning Framework for Image-Based Solar Panel Classification](https://arxiv.org/abs/2507.01778)
*Vivek Tetarwal,Sandeep Kumar*

Main category: cs.IT

TL;DR: The paper presents a Dual Ensemble Neural Network (DENN) for classifying clean vs dirty solar panels, achieving state-of-the-art accuracy.


<details>
  <summary>Details</summary>
Motivation: Rising solar energy systems demand automated and accurate maintenance solutions for classifying clean and dirty solar panels.

Method: A novel Dual Ensemble Neural Network (DENN) integrates different ensemble learning models into a dual framework, enhancing accuracy and robustness.

Result: DENN outperforms existing ensemble methods, achieving state-of-the-art accuracy on the Deep Solar Eye dataset.

Conclusion: The DENN model shows promise as a scalable solution for automated maintenance in solar energy using hybrid ensemble techniques.

Abstract: The installation of solar energy systems is on the rise, and therefore,
appropriate maintenance techniques are required to be used in order to maintain
maximum performance levels. One of the major challenges is the automated
discrimination between clean and dirty solar panels. This paper presents a
novel Dual Ensemble Neural Network (DENN) to classify solar panels using
image-based features. The suggested approach utilizes the advantages offered by
various ensemble models by integrating them into a dual framework, aimed at
improving both classification accuracy and robustness. The DENN model is
evaluated in comparison to current ensemble methods, showcasing its superior
performance across a range of assessment metrics. The proposed approach
performs the best compared to other methods and reaches state-of-the-art
accuracy on experimental results for the Deep Solar Eye dataset, effectively
serving predictive maintenance purposes in solar energy systems. It reveals the
potential of hybrid ensemble learning techniques to further advance the
prospects of automated solar panel inspections as a scalable solution to
real-world challenges.

</details>


<div id='econ.EM'></div>

# econ.EM [[Back]](#toc)

### [366] [Shrinkage-Based Regressions with Many Related Treatments](https://arxiv.org/abs/2507.01202)
*Enes Dilber,Colin Gray*

Main category: econ.EM

TL;DR: The paper introduces a ridge regression model to improve causal effect estimation under partially overlapping treatments, achieving reduced MSE and enabling targeted decision-making.


<details>
  <summary>Details</summary>
Motivation: The motivation arises from the challenges faced in disentangling effects of many partially overlapping treatments in observational causal models, where existing approaches yield too noisy estimates for practical use.

Method: The proposed method employs a customized ridge regression approach, transitioning between heterogeneous and homogenous modeling, to produce less noisy estimates of sub-treatment effects.

Result: The method significantly reduces Mean Squared Error (MSE) for individual sub-treatment effects and allows reconstruction of aggregated treatment effects, demonstrated through theory and simulation.

Conclusion: The model offers practical benefits, such as enabling more accurate and actionable decision-making, showcased in its application at Wayfair.

Abstract: When using observational causal models, practitioners often want to
disentangle the effects of many related, partially-overlapping treatments.
Examples include estimating treatment effects of different marketing
touchpoints, ordering different types of products, or signing up for different
services. Common approaches that estimate separate treatment coefficients are
too noisy for practical decision-making. We propose a computationally light
model that uses a customized ridge regression to move between a heterogeneous
and a homogenous model: it substantially reduces MSE for the effects of each
individual sub-treatment while allowing us to easily reconstruct the effects of
an aggregated treatment. We demonstrate the properties of this estimator in
theory and simulation, and illustrate how it has unlocked targeted
decision-making at Wayfair.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [367] [A first-order method for nonconvex-nonconcave minimax problems under a local Kurdyka-Åojasiewicz condition](https://arxiv.org/abs/2507.01932)
*Zhaosong Lu,Xiangyuan Wang*

Main category: math.OC

TL;DR: This study addresses nonconvex-nonconcave minimax problems using a local Kurdyka-Åojasiewicz (KL) condition and introduces an optimized algorithm for solving them.


<details>
  <summary>Details</summary>
Motivation: Existing methods for minimax problems often rely on global KL or PL conditions, which are overly restrictive in real-world scenarios. This paper aims to extend the theoretical framework to address a broader class of problems by using local conditions.

Method: The authors define a local KL condition for the inner problem and use its locally HÃ¶lder smooth maximal function property to develop an inexact proximal gradient algorithm.

Result: They derive complexity guarantees for their proposed method under mild assumptions, addressing challenges related to shrinking KL regions and ill-conditioned landscapes.

Conclusion: The proposed framework and algorithm offer a solution for a broader class of minimax problems by relaxing overly restrictive global conditions and ensuring convergence to approximate stationary points.

Abstract: We study a class of nonconvex-nonconcave minimax problems in which the inner
maximization problem satisfies a local Kurdyka-{\L}ojasiewicz (KL) condition
that may vary with the outer minimization variable. In contrast to the global
KL or Polyak-{\L}ojasiewicz (PL) conditions commonly assumed in the literature
-- which are significantly stronger and often too restrictive in practice --
this local KL condition accommodates a broader range of practical scenarios.
However, it also introduces new analytical challenges. In particular, as an
optimization algorithm progresses toward a stationary point of the problem, the
region over which the KL condition holds may shrink, resulting in a more
intricate and potentially ill-conditioned landscape. To address this challenge,
we show that the associated maximal function is locally H\"older smooth.
Leveraging this key property, we develop an inexact proximal gradient method
for solving the minimax problem, where the inexact gradient of the maximal
function is computed by applying a proximal gradient method to a KL-structured
subproblem. Under mild assumptions, we establish complexity guarantees for
computing an approximate stationary point of the minimax problem.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [368] [Automated Classification of Volcanic Earthquakes Using Transformer Encoders: Insights into Data Quality and Model Interpretability](https://arxiv.org/abs/2507.01260)
*Y. Suzuki,Y. Yukutake,T. Ohminato,M. Yamasaki,Ahyi Kim*

Main category: physics.geo-ph

TL;DR: This paper presents a transformer-based deep learning model for classifying volcanic earthquakes more efficiently and objectively compared to traditional or CNN-based methods. Applied to Mount Asama's seismic activity, the model achieved high F1 scores and demonstrated improved interpretability via attention weight analysis.


<details>
  <summary>Details</summary>
Motivation: The study aims to address the limitations of traditional volcanic earthquake classification methods, which require substantial time and are influenced by subjective human judgment, by creating an efficient and objective automated method.

Method: The authors developed a Transformer encoder-based deep learning model, tested it on seismic data from Mount Asama, and analyzed model interpretability through attention weight visualizations. They also evaluated the impact of issues like data imbalance and ambiguous labeling in training data.

Result: The model achieved high F1 scores (0.930 for volcano tectonic, 0.931 for low-frequency earthquakes, and 0.980 for noise), outperformed a CNN-based model, and revealed that closer seismic stations improved performance. Data inconsistencies were identified as a key limitation.

Conclusion: This study demonstrates the effectiveness of Transformer-based models for classifying volcanic earthquakes, showing how addressing challenges like data imbalance and subjective labeling can improve both accuracy and interpretability. The framework holds potential for application in other regions, enabling better volcanic hazard assessments and disaster response.

Abstract: Precisely classifying earthquake types is crucial for elucidating the
relationship between volcanic earthquakes and volcanic activity. However,
traditional methods rely on subjective human judgment, which requires
considerable time and effort. To address this issue, we developed a deep
learning model using a transformer encoder for a more objective and efficient
classification. Tested on Mount Asama's diverse seismic activity, our model
achieved high F1 scores (0.930 for volcano tectonic, 0.931 for low-frequency
earthquakes, and 0.980 for noise), superior to a conventional CNN-based method.
To enhance interpretability, attention weight visualizations were analyzed,
revealing that the model focuses on key waveform features similarly to human
experts. However, inconsistencies in training data, such as ambiguously labeled
B-type events with S-waves, were found to influence classification accuracy and
attention weight distributions. Experiments addressing data selection and
augmentation demonstrated the importance of balancing data quality and
diversity. In addition, stations within 3 km of the crater played an important
role in improving model performance and interpretability. These findings
highlight the potential of Transformer-based models for automated volcanic
earthquake classification, particularly in improving efficiency and
interpretability. By addressing challenges such as data imbalance and
subjective labeling, our approach provides a robust framework for understanding
seismic activity at Mount Asama. Moreover, this framework offers opportunities
for transfer learning to other volcanic regions, paving the way for enhanced
volcanic hazard assessments and disaster mitigation strategies.

</details>
